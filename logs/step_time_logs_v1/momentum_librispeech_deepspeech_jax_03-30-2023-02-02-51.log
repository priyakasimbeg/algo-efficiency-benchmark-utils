I0330 02:03:07.205962 140017736374080 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/librispeech_deepspeech_jax.
I0330 02:03:07.257178 140017736374080 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0330 02:03:08.285178 140017736374080 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0330 02:03:08.286165 140017736374080 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0330 02:03:08.292106 140017736374080 submission_runner.py:504] Using RNG seed 2673462327
I0330 02:03:09.652413 140017736374080 submission_runner.py:513] --- Tuning run 1/1 ---
I0330 02:03:09.652609 140017736374080 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1.
I0330 02:03:09.652788 140017736374080 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/hparams.json.
I0330 02:03:09.789485 140017736374080 submission_runner.py:230] Starting train once: RAM USED (GB) 4.652879872
I0330 02:03:09.789648 140017736374080 submission_runner.py:231] Initializing dataset.
I0330 02:03:09.789820 140017736374080 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.652879872
I0330 02:03:09.789878 140017736374080 submission_runner.py:240] Initializing model.
I0330 02:03:27.126570 140017736374080 submission_runner.py:251] After Initializing model: RAM USED (GB) 9.24899328
I0330 02:03:27.126767 140017736374080 submission_runner.py:252] Initializing optimizer.
I0330 02:03:27.762479 140017736374080 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 9.248874496
I0330 02:03:27.762645 140017736374080 submission_runner.py:261] Initializing metrics bundle.
I0330 02:03:27.762694 140017736374080 submission_runner.py:275] Initializing checkpoint and logger.
I0330 02:03:27.764714 140017736374080 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0330 02:03:27.764990 140017736374080 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0330 02:03:27.765055 140017736374080 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0330 02:03:28.899027 140017736374080 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0330 02:03:28.901098 140017736374080 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0330 02:03:28.905124 140017736374080 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 9.245003776
I0330 02:03:28.905329 140017736374080 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 9.245003776
I0330 02:03:28.905391 140017736374080 submission_runner.py:312] Starting training loop.
I0330 02:03:29.125562 140017736374080 input_pipeline.py:20] Loading split = train-clean-100
I0330 02:03:29.176750 140017736374080 input_pipeline.py:20] Loading split = train-clean-360
I0330 02:03:29.573277 140017736374080 input_pipeline.py:20] Loading split = train-other-500
I0330 02:03:32.990937 140017736374080 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 11.817598976
2023-03-30 02:04:21.764767: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-03-30 02:04:21.857253: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0330 02:04:24.952508 139842885248768 logging_writer.py:48] [0] global_step=0, grad_norm=26.552204132080078, loss=32.919620513916016
I0330 02:04:24.964007 140017736374080 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 14.380056576
I0330 02:04:24.964240 140017736374080 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 14.380056576
I0330 02:04:24.964328 140017736374080 spec.py:298] Evaluating on the training split.
I0330 02:04:25.107265 140017736374080 input_pipeline.py:20] Loading split = train-clean-100
I0330 02:04:25.140164 140017736374080 input_pipeline.py:20] Loading split = train-clean-360
I0330 02:04:25.454453 140017736374080 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0330 02:06:37.622760 140017736374080 spec.py:310] Evaluating on the validation split.
I0330 02:06:37.719832 140017736374080 input_pipeline.py:20] Loading split = dev-clean
I0330 02:06:37.725407 140017736374080 input_pipeline.py:20] Loading split = dev-other
I0330 02:07:48.894341 140017736374080 spec.py:326] Evaluating on the test split.
I0330 02:07:48.997131 140017736374080 input_pipeline.py:20] Loading split = test-clean
I0330 02:08:33.263108 140017736374080 submission_runner.py:380] Time since start: 56.06s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.121647, dtype=float32), 'train/wer': 4.430142848218562, 'validation/ctc_loss': DeviceArray(31.168585, dtype=float32), 'validation/wer': 4.037057762255304, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.310596, dtype=float32), 'test/wer': 4.252147949546036, 'test/num_examples': 2472}
I0330 02:08:33.264385 140017736374080 submission_runner.py:390] After eval at step 1: RAM USED (GB) 19.396624384
I0330 02:08:33.279235 139839991174912 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=55.838645, test/ctc_loss=31.310596466064453, test/num_examples=2472, test/wer=4.252148, total_duration=56.058896, train/ctc_loss=32.121646881103516, train/wer=4.430143, validation/ctc_loss=31.1685848236084, validation/num_examples=5348, validation/wer=4.037058
I0330 02:08:33.387256 140017736374080 checkpoints.py:356] Saving checkpoint at step: 1
I0330 02:08:33.792383 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0330 02:08:33.799384 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0330 02:08:33.806159 140017736374080 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 19.382951936
I0330 02:08:33.856224 140017736374080 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 19.381403648
I0330 02:08:52.450006 140017736374080 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 19.717132288
I0330 02:10:52.182885 139843667580672 logging_writer.py:48] [100] global_step=100, grad_norm=2.602379322052002, loss=6.391109943389893
I0330 02:12:53.118260 139843675973376 logging_writer.py:48] [200] global_step=200, grad_norm=6.254496097564697, loss=6.233419895172119
I0330 02:14:54.020689 139843667580672 logging_writer.py:48] [300] global_step=300, grad_norm=2.9836883544921875, loss=5.929876327514648
I0330 02:16:54.959460 139843675973376 logging_writer.py:48] [400] global_step=400, grad_norm=4.674178123474121, loss=5.968515872955322
I0330 02:18:56.016784 139843667580672 logging_writer.py:48] [500] global_step=500, grad_norm=2.3764655590057373, loss=5.873946189880371
I0330 02:20:56.920132 139843675973376 logging_writer.py:48] [600] global_step=600, grad_norm=0.49231308698654175, loss=5.752340793609619
I0330 02:22:58.037756 139843667580672 logging_writer.py:48] [700] global_step=700, grad_norm=2.277876138687134, loss=5.616803169250488
I0330 02:24:59.058369 139843675973376 logging_writer.py:48] [800] global_step=800, grad_norm=1.14108145236969, loss=5.223595142364502
I0330 02:27:00.247179 139843667580672 logging_writer.py:48] [900] global_step=900, grad_norm=0.9609814882278442, loss=4.8731207847595215
I0330 02:29:01.186243 139843675973376 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9680025577545166, loss=4.560356140136719
I0330 02:31:05.383325 139842742638336 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.6241512298583984, loss=4.317497730255127
I0330 02:33:06.262690 139842558097152 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3683836460113525, loss=4.033731937408447
I0330 02:35:06.820545 139842742638336 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2951024770736694, loss=3.8444597721099854
I0330 02:37:06.815560 139842558097152 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2097631692886353, loss=3.6918516159057617
I0330 02:39:06.944563 139842742638336 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.308757781982422, loss=3.617166757583618
I0330 02:41:07.579703 139842558097152 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1010246276855469, loss=3.445377826690674
I0330 02:43:08.041450 139842742638336 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.057004451751709, loss=3.3765981197357178
I0330 02:45:08.270967 139842558097152 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1814932823181152, loss=3.2631993293762207
I0330 02:47:08.430269 139842742638336 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8656036257743835, loss=3.1278293132781982
I0330 02:48:34.767836 140017736374080 submission_runner.py:371] Before eval at step 1973: RAM USED (GB) 20.214444032
I0330 02:48:34.768044 140017736374080 spec.py:298] Evaluating on the training split.
I0330 02:49:05.740378 140017736374080 spec.py:310] Evaluating on the validation split.
I0330 02:49:47.045213 140017736374080 spec.py:326] Evaluating on the test split.
I0330 02:50:07.297435 140017736374080 submission_runner.py:380] Time since start: 2705.86s, 	Step: 1973, 	{'train/ctc_loss': DeviceArray(6.190679, dtype=float32), 'train/wer': 0.9441103169862619, 'validation/ctc_loss': DeviceArray(6.3997364, dtype=float32), 'validation/wer': 0.8958118264527395, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.19479, dtype=float32), 'test/wer': 0.8992545650275222, 'test/num_examples': 2472}
I0330 02:50:07.298687 140017736374080 submission_runner.py:390] After eval at step 1973: RAM USED (GB) 19.027443712
I0330 02:50:07.319540 139842742638336 logging_writer.py:48] [1973] global_step=1973, preemption_count=0, score=2453.180065, test/ctc_loss=6.194789886474609, test/num_examples=2472, test/wer=0.899255, total_duration=2705.858430, train/ctc_loss=6.19067907333374, train/wer=0.944110, validation/ctc_loss=6.399736404418945, validation/num_examples=5348, validation/wer=0.895812
I0330 02:50:07.420264 140017736374080 checkpoints.py:356] Saving checkpoint at step: 1973
I0330 02:50:07.853094 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1973
I0330 02:50:07.862315 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1973.
I0330 02:50:07.866676 140017736374080 submission_runner.py:409] After logging and checkpointing eval at step 1973: RAM USED (GB) 19.001868288
I0330 02:50:41.619575 139842558097152 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8457993865013123, loss=3.1007699966430664
I0330 02:52:45.714835 139842742638336 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2664388418197632, loss=3.1458115577697754
I0330 02:54:46.202138 139842558097152 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.2041985988616943, loss=3.0673928260803223
I0330 02:56:46.668801 139842742638336 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9865245819091797, loss=2.9908602237701416
I0330 02:58:47.298444 139842558097152 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0549943447113037, loss=2.9523515701293945
I0330 03:00:47.409474 139842742638336 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.0135722160339355, loss=2.867793560028076
I0330 03:02:47.772934 139842558097152 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9119589328765869, loss=2.9310457706451416
I0330 03:04:48.088444 139842742638336 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.6693618297576904, loss=2.871940851211548
I0330 03:06:48.440581 139842558097152 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9344242215156555, loss=2.7943241596221924
I0330 03:08:48.758116 139842742638336 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0081111192703247, loss=2.848587989807129
I0330 03:10:49.320298 139842558097152 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0152251720428467, loss=2.6941938400268555
I0330 03:12:52.947324 139842742638336 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.425618290901184, loss=2.644664764404297
I0330 03:14:52.911166 139842558097152 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0529065132141113, loss=2.701486825942993
I0330 03:16:52.657201 139842742638336 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8185006976127625, loss=2.641080379486084
I0330 03:18:52.793492 139842558097152 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9722849130630493, loss=2.6760823726654053
I0330 03:20:53.027007 139842742638336 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8265219926834106, loss=2.5942842960357666
I0330 03:22:53.751304 139842558097152 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.013979434967041, loss=2.602210760116577
I0330 03:24:54.564535 139842742638336 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8051130175590515, loss=2.6078104972839355
I0330 03:26:55.003026 139842558097152 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.77800452709198, loss=2.4836244583129883
I0330 03:28:56.126532 139842742638336 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0507261753082275, loss=2.4972777366638184
I0330 03:30:08.259468 140017736374080 submission_runner.py:371] Before eval at step 3961: RAM USED (GB) 19.591757824
I0330 03:30:08.259722 140017736374080 spec.py:298] Evaluating on the training split.
I0330 03:30:54.999286 140017736374080 spec.py:310] Evaluating on the validation split.
I0330 03:31:38.899573 140017736374080 spec.py:326] Evaluating on the test split.
I0330 03:32:00.647673 140017736374080 submission_runner.py:380] Time since start: 5199.35s, 	Step: 3961, 	{'train/ctc_loss': DeviceArray(1.3463417, dtype=float32), 'train/wer': 0.39810883520230134, 'validation/ctc_loss': DeviceArray(1.7660072, dtype=float32), 'validation/wer': 0.45481384287354437, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3653935, dtype=float32), 'test/wer': 0.39126195844250805, 'test/num_examples': 2472}
I0330 03:32:00.648933 140017736374080 submission_runner.py:390] After eval at step 3961: RAM USED (GB) 18.715799552
I0330 03:32:00.668122 139843218183936 logging_writer.py:48] [3961] global_step=3961, preemption_count=0, score=4850.022254, test/ctc_loss=1.3653935194015503, test/num_examples=2472, test/wer=0.391262, total_duration=5199.350103, train/ctc_loss=1.3463417291641235, train/wer=0.398109, validation/ctc_loss=1.7660071849822998, validation/num_examples=5348, validation/wer=0.454814
I0330 03:32:00.769943 140017736374080 checkpoints.py:356] Saving checkpoint at step: 3961
I0330 03:32:01.210584 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_3961
I0330 03:32:01.219254 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_3961.
I0330 03:32:01.223794 140017736374080 submission_runner.py:409] After logging and checkpointing eval at step 3961: RAM USED (GB) 18.68632064
I0330 03:32:49.389685 139843209791232 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9721238017082214, loss=2.5431814193725586
I0330 03:34:49.790209 139843159435008 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8087365031242371, loss=2.5161900520324707
I0330 03:36:53.852109 139843218183936 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.764542818069458, loss=2.4925994873046875
I0330 03:38:53.747511 139843209791232 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7782896161079407, loss=2.480602502822876
I0330 03:40:53.510061 139843218183936 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8917726874351501, loss=2.4732468128204346
I0330 03:42:53.529808 139843209791232 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9154781699180603, loss=2.4606263637542725
I0330 03:44:53.531626 139843218183936 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8331406712532043, loss=2.512286901473999
I0330 03:46:53.598610 139843209791232 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8873698711395264, loss=2.473708391189575
I0330 03:48:53.987028 139843218183936 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1803.94775390625
I0330 03:50:54.101672 139843209791232 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1770.51123046875
I0330 03:52:54.444766 139843218183936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1754.494140625
I0330 03:54:54.353358 139843209791232 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1892.3582763671875
I0330 03:56:57.904372 139843218183936 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1801.2821044921875
I0330 03:58:57.530753 139843209791232 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1825.1690673828125
I0330 04:00:56.881460 139843218183936 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1769.5322265625
I0330 04:02:56.732002 139843209791232 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1807.51416015625
I0330 04:04:56.700244 139843218183936 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1790.823486328125
I0330 04:06:56.655997 139843209791232 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1758.2296142578125
I0330 04:08:56.487440 139843218183936 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1788.4464111328125
I0330 04:10:56.431149 139843209791232 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1804.8380126953125
I0330 04:12:02.309555 140017736374080 submission_runner.py:371] Before eval at step 5956: RAM USED (GB) 19.663896576
I0330 04:12:02.309783 140017736374080 spec.py:298] Evaluating on the training split.
I0330 04:12:33.777572 140017736374080 spec.py:310] Evaluating on the validation split.
I0330 04:13:11.910384 140017736374080 spec.py:326] Evaluating on the test split.
I0330 04:13:31.077054 140017736374080 submission_runner.py:380] Time since start: 7713.40s, 	Step: 5956, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 04:13:31.078468 140017736374080 submission_runner.py:390] After eval at step 5956: RAM USED (GB) 18.634924032
I0330 04:13:31.098772 139843371783936 logging_writer.py:48] [5956] global_step=5956, preemption_count=0, score=7247.593319, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7713.399761, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0330 04:13:31.208366 140017736374080 checkpoints.py:356] Saving checkpoint at step: 5956
I0330 04:13:31.648426 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_5956
I0330 04:13:31.656978 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_5956.
I0330 04:13:31.660653 140017736374080 submission_runner.py:409] After logging and checkpointing eval at step 5956: RAM USED (GB) 18.637979648
I0330 04:14:25.502040 139843363391232 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1794.3382568359375
I0330 04:16:25.246016 139843304642304 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1806.748779296875
I0330 04:18:28.672583 139843371783936 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1863.024658203125
I0330 04:20:28.459853 139843363391232 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1795.0931396484375
I0330 04:22:28.851854 139843371783936 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1814.43212890625
I0330 04:24:28.797828 139843363391232 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1826.0804443359375
I0330 04:26:28.662601 139843371783936 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1893.3380126953125
I0330 04:28:28.896435 139843363391232 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1818.29833984375
I0330 04:30:29.070630 139843371783936 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1783.5869140625
I0330 04:32:29.698374 139843363391232 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1824.128662109375
I0330 04:34:29.951528 139843371783936 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1813.789306640625
I0330 04:36:29.865619 139843363391232 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1806.4937744140625
I0330 04:38:29.585402 139843371783936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1813.1468505859375
I0330 04:40:33.818201 139843371783936 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1833.2728271484375
I0330 04:42:33.759760 139843363391232 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1835.7706298828125
I0330 04:44:33.466793 139843371783936 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1778.5064697265625
I0330 04:46:33.294416 139843363391232 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1773.209228515625
I0330 04:48:33.122370 139843371783936 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1843.1724853515625
I0330 04:50:33.081447 139843363391232 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1890.54150390625
I0330 04:52:33.019771 139843371783936 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1827.1231689453125
I0330 04:53:32.545862 140017736374080 submission_runner.py:371] Before eval at step 7951: RAM USED (GB) 19.618422784
I0330 04:53:32.546091 140017736374080 spec.py:298] Evaluating on the training split.
I0330 04:54:04.779834 140017736374080 spec.py:310] Evaluating on the validation split.
I0330 04:54:43.056162 140017736374080 spec.py:326] Evaluating on the test split.
I0330 04:55:02.956098 140017736374080 submission_runner.py:380] Time since start: 10203.64s, 	Step: 7951, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 04:55:02.957321 140017736374080 submission_runner.py:390] After eval at step 7951: RAM USED (GB) 18.524819456
I0330 04:55:02.977191 139843801863936 logging_writer.py:48] [7951] global_step=7951, preemption_count=0, score=9644.871195, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10203.635306, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0330 04:55:03.082761 140017736374080 checkpoints.py:356] Saving checkpoint at step: 7951
I0330 04:55:03.538649 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_7951
I0330 04:55:03.547287 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_7951.
I0330 04:55:03.551432 140017736374080 submission_runner.py:409] After logging and checkpointing eval at step 7951: RAM USED (GB) 18.527076352
I0330 04:56:01.735843 140017736374080 submission_runner.py:371] Before eval at step 8000: RAM USED (GB) 19.529244672
I0330 04:56:01.736042 140017736374080 spec.py:298] Evaluating on the training split.
I0330 04:56:32.466553 140017736374080 spec.py:310] Evaluating on the validation split.
I0330 04:57:07.203212 140017736374080 spec.py:326] Evaluating on the test split.
I0330 04:57:24.588477 140017736374080 submission_runner.py:380] Time since start: 10352.83s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 04:57:24.589817 140017736374080 submission_runner.py:390] After eval at step 8000: RAM USED (GB) 19.554197504
I0330 04:57:24.607564 139843801863936 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9702.966830, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10352.829622, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0330 04:57:24.706952 140017736374080 checkpoints.py:356] Saving checkpoint at step: 8000
I0330 04:57:25.141485 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0330 04:57:25.151100 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0330 04:57:25.156838 140017736374080 submission_runner.py:409] After logging and checkpointing eval at step 8000: RAM USED (GB) 19.554566144
I0330 04:57:25.166079 139843793471232 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9702.966830
I0330 04:57:25.240739 140017736374080 checkpoints.py:356] Saving checkpoint at step: 8000
I0330 04:57:25.802492 140017736374080 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0330 04:57:25.811182 140017736374080 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0330 04:57:26.995725 140017736374080 submission_runner.py:543] Tuning trial 1/1
I0330 04:57:26.995961 140017736374080 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0330 04:57:27.000507 140017736374080 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.121647, dtype=float32), 'train/wer': 4.430142848218562, 'validation/ctc_loss': DeviceArray(31.168585, dtype=float32), 'validation/wer': 4.037057762255304, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.310596, dtype=float32), 'test/wer': 4.252147949546036, 'test/num_examples': 2472, 'score': 55.83864498138428, 'total_duration': 56.05889558792114, 'global_step': 1, 'preemption_count': 0}), (1973, {'train/ctc_loss': DeviceArray(6.190679, dtype=float32), 'train/wer': 0.9441103169862619, 'validation/ctc_loss': DeviceArray(6.3997364, dtype=float32), 'validation/wer': 0.8958118264527395, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.19479, dtype=float32), 'test/wer': 0.8992545650275222, 'test/num_examples': 2472, 'score': 2453.1800651550293, 'total_duration': 2705.8584299087524, 'global_step': 1973, 'preemption_count': 0}), (3961, {'train/ctc_loss': DeviceArray(1.3463417, dtype=float32), 'train/wer': 0.39810883520230134, 'validation/ctc_loss': DeviceArray(1.7660072, dtype=float32), 'validation/wer': 0.45481384287354437, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3653935, dtype=float32), 'test/wer': 0.39126195844250805, 'test/num_examples': 2472, 'score': 4850.022253513336, 'total_duration': 5199.350102901459, 'global_step': 3961, 'preemption_count': 0}), (5956, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7247.593319416046, 'total_duration': 7713.399761199951, 'global_step': 5956, 'preemption_count': 0}), (7951, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9644.871194601059, 'total_duration': 10203.6353058815, 'global_step': 7951, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9702.966829776764, 'total_duration': 10352.829621553421, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0330 04:57:27.000648 140017736374080 submission_runner.py:546] Timing: 9702.966829776764
I0330 04:57:27.000706 140017736374080 submission_runner.py:547] ====================
I0330 04:57:27.001230 140017736374080 submission_runner.py:606] Final librispeech_deepspeech score: 9702.966829776764
