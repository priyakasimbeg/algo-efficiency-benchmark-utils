WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 13:47:31.898761 140248354813760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 13:47:31.898786 139660429485888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 13:47:31.898805 139806851123008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 13:47:31.899567 140523644176192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 13:47:31.899584 140537477670720 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 13:47:31.899712 139787276965696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 13:47:31.900005 140071793805120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 13:47:31.900217 139985604192064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 13:47:31.900425 140071793805120 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.900563 139985604192064 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.909466 139660429485888 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.909512 139806851123008 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.909535 140248354813760 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.910196 140523644176192 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.910228 140537477670720 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.910369 139787276965696 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:47:31.916211 139985604192064 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/criteo1tb_pytorch.
W0402 13:47:31.955780 139660429485888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.956062 139806851123008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.956207 139985604192064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.956400 140071793805120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.956907 140523644176192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.957134 139787276965696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.958373 140537477670720 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:47:31.959403 140248354813760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 13:47:31.961344 139985604192064 submission_runner.py:511] Using RNG seed 495839569
I0402 13:47:31.962743 139985604192064 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 13:47:31.962867 139985604192064 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1.
I0402 13:47:31.963090 139985604192064 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/hparams.json.
I0402 13:47:31.964077 139985604192064 submission_runner.py:230] Starting train once: RAM USED (GB) 5.710921728
I0402 13:47:31.964172 139985604192064 submission_runner.py:231] Initializing dataset.
I0402 13:47:31.964390 139985604192064 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.710921728
I0402 13:47:31.964468 139985604192064 submission_runner.py:240] Initializing model.
I0402 13:47:46.957453 139985604192064 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.28113152
I0402 13:47:46.957630 139985604192064 submission_runner.py:252] Initializing optimizer.
I0402 13:47:46.958234 139985604192064 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.28113152
I0402 13:47:46.958353 139985604192064 submission_runner.py:261] Initializing metrics bundle.
I0402 13:47:46.958402 139985604192064 submission_runner.py:276] Initializing checkpoint and logger.
I0402 13:47:46.962157 139985604192064 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 13:47:46.962301 139985604192064 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 13:47:47.563961 139985604192064 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0402 13:47:47.564831 139985604192064 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/flags_0.json.
I0402 13:47:47.608226 139985604192064 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.335272448
I0402 13:47:47.609259 139985604192064 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.335272448
I0402 13:47:47.609388 139985604192064 submission_runner.py:313] Starting training loop.
I0402 13:50:18.392356 139985604192064 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 54.157914112
I0402 13:50:21.840823 139902721193728 logging_writer.py:48] [0] global_step=0, grad_norm=8.637516, loss=0.959413
I0402 13:50:21.846735 139985604192064 submission.py:296] 0) loss = 0.959, grad_norm = 8.638
I0402 13:50:21.847132 139985604192064 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 58.307813376
I0402 13:50:21.847657 139985604192064 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 58.308329472
I0402 13:50:21.847758 139985604192064 spec.py:298] Evaluating on the training split.
I0402 14:00:10.230558 139985604192064 spec.py:310] Evaluating on the validation split.
I0402 14:05:23.487317 139985604192064 spec.py:326] Evaluating on the test split.
I0402 14:10:12.430715 139985604192064 submission_runner.py:382] Time since start: 154.24s, 	Step: 1, 	{'train/loss': 0.9592470192926784, 'validation/loss': 0.9636854831460674, 'validation/num_examples': 89000000, 'test/loss': 0.9619735558263878, 'test/num_examples': 89274637}
I0402 14:10:12.431146 139985604192064 submission_runner.py:396] After eval at step 1: RAM USED (GB) 106.127876096
I0402 14:10:12.442600 139845687019264 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=154.236976, test/loss=0.961974, test/num_examples=89274637, total_duration=154.238847, train/loss=0.959247, validation/loss=0.963685, validation/num_examples=89000000
I0402 14:10:26.750534 139985604192064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_1.
I0402 14:10:26.751000 139985604192064 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 106.003734528
I0402 14:10:26.769890 139985604192064 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.962962944
I0402 14:10:26.773107 139985604192064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773095 140537477670720 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773100 139660429485888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773096 139806851123008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773100 139787276965696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773139 140248354813760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773161 140523644176192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:26.773198 140071793805120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 14:10:28.621872 139845678626560 logging_writer.py:48] [1] global_step=1, grad_norm=8.612964, loss=0.959603
I0402 14:10:28.627036 139985604192064 submission.py:296] 1) loss = 0.960, grad_norm = 8.613
I0402 14:10:28.627579 139985604192064 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 106.186686464
I0402 14:10:30.421680 139845687019264 logging_writer.py:48] [2] global_step=2, grad_norm=8.526674, loss=0.940763
I0402 14:10:30.425489 139985604192064 submission.py:296] 2) loss = 0.941, grad_norm = 8.527
I0402 14:10:32.237497 139845678626560 logging_writer.py:48] [3] global_step=3, grad_norm=8.296876, loss=0.908177
I0402 14:10:32.240697 139985604192064 submission.py:296] 3) loss = 0.908, grad_norm = 8.297
I0402 14:10:34.096082 139845687019264 logging_writer.py:48] [4] global_step=4, grad_norm=7.969845, loss=0.862914
I0402 14:10:34.099466 139985604192064 submission.py:296] 4) loss = 0.863, grad_norm = 7.970
I0402 14:10:35.982334 139845678626560 logging_writer.py:48] [5] global_step=5, grad_norm=7.578357, loss=0.808559
I0402 14:10:35.985949 139985604192064 submission.py:296] 5) loss = 0.809, grad_norm = 7.578
I0402 14:10:37.804799 139845687019264 logging_writer.py:48] [6] global_step=6, grad_norm=7.156106, loss=0.746844
I0402 14:10:37.808259 139985604192064 submission.py:296] 6) loss = 0.747, grad_norm = 7.156
I0402 14:10:39.664272 139845678626560 logging_writer.py:48] [7] global_step=7, grad_norm=6.669709, loss=0.679296
I0402 14:10:39.667940 139985604192064 submission.py:296] 7) loss = 0.679, grad_norm = 6.670
I0402 14:10:41.496510 139845687019264 logging_writer.py:48] [8] global_step=8, grad_norm=6.206096, loss=0.608754
I0402 14:10:41.502285 139985604192064 submission.py:296] 8) loss = 0.609, grad_norm = 6.206
I0402 14:10:43.481675 139845678626560 logging_writer.py:48] [9] global_step=9, grad_norm=5.711784, loss=0.534746
I0402 14:10:43.484805 139985604192064 submission.py:296] 9) loss = 0.535, grad_norm = 5.712
I0402 14:10:45.334079 139845687019264 logging_writer.py:48] [10] global_step=10, grad_norm=5.069868, loss=0.462800
I0402 14:10:45.337227 139985604192064 submission.py:296] 10) loss = 0.463, grad_norm = 5.070
I0402 14:10:47.272501 139845678626560 logging_writer.py:48] [11] global_step=11, grad_norm=4.337245, loss=0.394877
I0402 14:10:47.275575 139985604192064 submission.py:296] 11) loss = 0.395, grad_norm = 4.337
I0402 14:10:49.149394 139845687019264 logging_writer.py:48] [12] global_step=12, grad_norm=3.541814, loss=0.335267
I0402 14:10:49.152642 139985604192064 submission.py:296] 12) loss = 0.335, grad_norm = 3.542
I0402 14:10:51.013752 139845678626560 logging_writer.py:48] [13] global_step=13, grad_norm=2.780677, loss=0.286917
I0402 14:10:51.017053 139985604192064 submission.py:296] 13) loss = 0.287, grad_norm = 2.781
I0402 14:10:52.944310 139845687019264 logging_writer.py:48] [14] global_step=14, grad_norm=2.078669, loss=0.245930
I0402 14:10:52.947364 139985604192064 submission.py:296] 14) loss = 0.246, grad_norm = 2.079
I0402 14:10:54.818029 139845678626560 logging_writer.py:48] [15] global_step=15, grad_norm=1.429431, loss=0.217065
I0402 14:10:54.821388 139985604192064 submission.py:296] 15) loss = 0.217, grad_norm = 1.429
I0402 14:10:56.650341 139845687019264 logging_writer.py:48] [16] global_step=16, grad_norm=0.850520, loss=0.198238
I0402 14:10:56.653446 139985604192064 submission.py:296] 16) loss = 0.198, grad_norm = 0.851
I0402 14:10:58.479488 139845678626560 logging_writer.py:48] [17] global_step=17, grad_norm=0.389431, loss=0.190292
I0402 14:10:58.482841 139985604192064 submission.py:296] 17) loss = 0.190, grad_norm = 0.389
I0402 14:11:00.319645 139845687019264 logging_writer.py:48] [18] global_step=18, grad_norm=0.231559, loss=0.185119
I0402 14:11:00.322745 139985604192064 submission.py:296] 18) loss = 0.185, grad_norm = 0.232
I0402 14:11:02.158952 139845678626560 logging_writer.py:48] [19] global_step=19, grad_norm=0.452749, loss=0.187463
I0402 14:11:02.162027 139985604192064 submission.py:296] 19) loss = 0.187, grad_norm = 0.453
I0402 14:11:03.955560 139845687019264 logging_writer.py:48] [20] global_step=20, grad_norm=0.698297, loss=0.193761
I0402 14:11:03.958946 139985604192064 submission.py:296] 20) loss = 0.194, grad_norm = 0.698
I0402 14:11:05.784199 139845678626560 logging_writer.py:48] [21] global_step=21, grad_norm=0.892274, loss=0.199838
I0402 14:11:05.787178 139985604192064 submission.py:296] 21) loss = 0.200, grad_norm = 0.892
I0402 14:11:07.680779 139845687019264 logging_writer.py:48] [22] global_step=22, grad_norm=1.071383, loss=0.209149
I0402 14:11:07.683715 139985604192064 submission.py:296] 22) loss = 0.209, grad_norm = 1.071
I0402 14:11:09.529318 139845678626560 logging_writer.py:48] [23] global_step=23, grad_norm=1.262018, loss=0.226033
I0402 14:11:09.532328 139985604192064 submission.py:296] 23) loss = 0.226, grad_norm = 1.262
I0402 14:11:11.310168 139845687019264 logging_writer.py:48] [24] global_step=24, grad_norm=1.363637, loss=0.233383
I0402 14:11:11.313228 139985604192064 submission.py:296] 24) loss = 0.233, grad_norm = 1.364
I0402 14:11:13.157412 139845678626560 logging_writer.py:48] [25] global_step=25, grad_norm=1.455657, loss=0.243119
I0402 14:11:13.160609 139985604192064 submission.py:296] 25) loss = 0.243, grad_norm = 1.456
I0402 14:11:14.976227 139845687019264 logging_writer.py:48] [26] global_step=26, grad_norm=1.591181, loss=0.259978
I0402 14:11:14.979312 139985604192064 submission.py:296] 26) loss = 0.260, grad_norm = 1.591
I0402 14:11:16.784666 139845678626560 logging_writer.py:48] [27] global_step=27, grad_norm=1.592653, loss=0.258233
I0402 14:11:16.787582 139985604192064 submission.py:296] 27) loss = 0.258, grad_norm = 1.593
I0402 14:11:18.577484 139845687019264 logging_writer.py:48] [28] global_step=28, grad_norm=1.641296, loss=0.265433
I0402 14:11:18.580511 139985604192064 submission.py:296] 28) loss = 0.265, grad_norm = 1.641
I0402 14:11:20.417946 139845678626560 logging_writer.py:48] [29] global_step=29, grad_norm=1.652477, loss=0.266125
I0402 14:11:20.420973 139985604192064 submission.py:296] 29) loss = 0.266, grad_norm = 1.652
I0402 14:11:22.240312 139845687019264 logging_writer.py:48] [30] global_step=30, grad_norm=1.610458, loss=0.259873
I0402 14:11:22.243393 139985604192064 submission.py:296] 30) loss = 0.260, grad_norm = 1.610
I0402 14:11:24.062259 139845678626560 logging_writer.py:48] [31] global_step=31, grad_norm=1.658445, loss=0.266139
I0402 14:11:24.065488 139985604192064 submission.py:296] 31) loss = 0.266, grad_norm = 1.658
I0402 14:11:25.947427 139845687019264 logging_writer.py:48] [32] global_step=32, grad_norm=1.570164, loss=0.253602
I0402 14:11:25.950503 139985604192064 submission.py:296] 32) loss = 0.254, grad_norm = 1.570
I0402 14:11:27.766861 139845678626560 logging_writer.py:48] [33] global_step=33, grad_norm=1.515364, loss=0.245811
I0402 14:11:27.770159 139985604192064 submission.py:296] 33) loss = 0.246, grad_norm = 1.515
I0402 14:11:29.631203 139845687019264 logging_writer.py:48] [34] global_step=34, grad_norm=1.471661, loss=0.240539
I0402 14:11:29.634625 139985604192064 submission.py:296] 34) loss = 0.241, grad_norm = 1.472
I0402 14:11:31.519095 139845678626560 logging_writer.py:48] [35] global_step=35, grad_norm=1.374328, loss=0.228244
I0402 14:11:31.522519 139985604192064 submission.py:296] 35) loss = 0.228, grad_norm = 1.374
I0402 14:11:33.393758 139845687019264 logging_writer.py:48] [36] global_step=36, grad_norm=1.320784, loss=0.222531
I0402 14:11:33.397320 139985604192064 submission.py:296] 36) loss = 0.223, grad_norm = 1.321
I0402 14:11:35.304826 139845678626560 logging_writer.py:48] [37] global_step=37, grad_norm=1.221653, loss=0.213144
I0402 14:11:35.308702 139985604192064 submission.py:296] 37) loss = 0.213, grad_norm = 1.222
I0402 14:11:37.226863 139845687019264 logging_writer.py:48] [38] global_step=38, grad_norm=1.094861, loss=0.200819
I0402 14:11:37.230189 139985604192064 submission.py:296] 38) loss = 0.201, grad_norm = 1.095
I0402 14:11:39.039306 139845678626560 logging_writer.py:48] [39] global_step=39, grad_norm=0.967328, loss=0.190251
I0402 14:11:39.042628 139985604192064 submission.py:296] 39) loss = 0.190, grad_norm = 0.967
I0402 14:11:40.856451 139845687019264 logging_writer.py:48] [40] global_step=40, grad_norm=0.828295, loss=0.180943
I0402 14:11:40.859699 139985604192064 submission.py:296] 40) loss = 0.181, grad_norm = 0.828
I0402 14:11:42.674154 139845678626560 logging_writer.py:48] [41] global_step=41, grad_norm=0.693470, loss=0.175674
I0402 14:11:42.677408 139985604192064 submission.py:296] 41) loss = 0.176, grad_norm = 0.693
I0402 14:11:44.569294 139845687019264 logging_writer.py:48] [42] global_step=42, grad_norm=0.495865, loss=0.166018
I0402 14:11:44.573187 139985604192064 submission.py:296] 42) loss = 0.166, grad_norm = 0.496
I0402 14:11:46.504039 139845678626560 logging_writer.py:48] [43] global_step=43, grad_norm=0.323859, loss=0.161271
I0402 14:11:46.507684 139985604192064 submission.py:296] 43) loss = 0.161, grad_norm = 0.324
I0402 14:11:48.323455 139845687019264 logging_writer.py:48] [44] global_step=44, grad_norm=0.186592, loss=0.157943
I0402 14:11:48.327123 139985604192064 submission.py:296] 44) loss = 0.158, grad_norm = 0.187
I0402 14:11:50.126598 139845678626560 logging_writer.py:48] [45] global_step=45, grad_norm=0.184171, loss=0.155517
I0402 14:11:50.129458 139985604192064 submission.py:296] 45) loss = 0.156, grad_norm = 0.184
I0402 14:11:52.007091 139845687019264 logging_writer.py:48] [46] global_step=46, grad_norm=0.249696, loss=0.155722
I0402 14:11:52.012707 139985604192064 submission.py:296] 46) loss = 0.156, grad_norm = 0.250
I0402 14:11:53.984004 139845678626560 logging_writer.py:48] [47] global_step=47, grad_norm=0.293059, loss=0.154479
I0402 14:11:53.987882 139985604192064 submission.py:296] 47) loss = 0.154, grad_norm = 0.293
I0402 14:11:55.800319 139845687019264 logging_writer.py:48] [48] global_step=48, grad_norm=0.254936, loss=0.153330
I0402 14:11:55.803990 139985604192064 submission.py:296] 48) loss = 0.153, grad_norm = 0.255
I0402 14:11:57.625600 139845678626560 logging_writer.py:48] [49] global_step=49, grad_norm=0.182094, loss=0.149886
I0402 14:11:57.628910 139985604192064 submission.py:296] 49) loss = 0.150, grad_norm = 0.182
I0402 14:11:59.473968 139845687019264 logging_writer.py:48] [50] global_step=50, grad_norm=0.098086, loss=0.149098
I0402 14:11:59.478532 139985604192064 submission.py:296] 50) loss = 0.149, grad_norm = 0.098
I0402 14:12:01.353737 139845678626560 logging_writer.py:48] [51] global_step=51, grad_norm=0.116301, loss=0.147652
I0402 14:12:01.358841 139985604192064 submission.py:296] 51) loss = 0.148, grad_norm = 0.116
I0402 14:12:03.206570 139845687019264 logging_writer.py:48] [52] global_step=52, grad_norm=0.172244, loss=0.148030
I0402 14:12:03.209575 139985604192064 submission.py:296] 52) loss = 0.148, grad_norm = 0.172
I0402 14:12:05.111331 139845678626560 logging_writer.py:48] [53] global_step=53, grad_norm=0.176109, loss=0.145151
I0402 14:12:05.116269 139985604192064 submission.py:296] 53) loss = 0.145, grad_norm = 0.176
I0402 14:12:06.979665 139845687019264 logging_writer.py:48] [54] global_step=54, grad_norm=0.175671, loss=0.145588
I0402 14:12:06.983395 139985604192064 submission.py:296] 54) loss = 0.146, grad_norm = 0.176
I0402 14:12:08.847010 139845678626560 logging_writer.py:48] [55] global_step=55, grad_norm=0.107316, loss=0.142571
I0402 14:12:08.852103 139985604192064 submission.py:296] 55) loss = 0.143, grad_norm = 0.107
I0402 14:12:10.646518 139845687019264 logging_writer.py:48] [56] global_step=56, grad_norm=0.076269, loss=0.141966
I0402 14:12:10.649960 139985604192064 submission.py:296] 56) loss = 0.142, grad_norm = 0.076
I0402 14:12:12.509833 139845678626560 logging_writer.py:48] [57] global_step=57, grad_norm=0.040643, loss=0.139362
I0402 14:12:12.514770 139985604192064 submission.py:296] 57) loss = 0.139, grad_norm = 0.041
I0402 14:12:14.388951 139845687019264 logging_writer.py:48] [58] global_step=58, grad_norm=0.048930, loss=0.142207
I0402 14:12:14.395091 139985604192064 submission.py:296] 58) loss = 0.142, grad_norm = 0.049
I0402 14:12:16.183524 139845678626560 logging_writer.py:48] [59] global_step=59, grad_norm=0.028568, loss=0.140534
I0402 14:12:16.189821 139985604192064 submission.py:296] 59) loss = 0.141, grad_norm = 0.029
I0402 14:12:17.988500 139845687019264 logging_writer.py:48] [60] global_step=60, grad_norm=0.036023, loss=0.141194
I0402 14:12:17.991948 139985604192064 submission.py:296] 60) loss = 0.141, grad_norm = 0.036
I0402 14:12:19.806599 139845678626560 logging_writer.py:48] [61] global_step=61, grad_norm=0.028298, loss=0.140292
I0402 14:12:19.809575 139985604192064 submission.py:296] 61) loss = 0.140, grad_norm = 0.028
I0402 14:12:21.654984 139845687019264 logging_writer.py:48] [62] global_step=62, grad_norm=0.029198, loss=0.139834
I0402 14:12:21.660676 139985604192064 submission.py:296] 62) loss = 0.140, grad_norm = 0.029
I0402 14:12:23.475503 139845678626560 logging_writer.py:48] [63] global_step=63, grad_norm=0.030360, loss=0.139780
I0402 14:12:23.478635 139985604192064 submission.py:296] 63) loss = 0.140, grad_norm = 0.030
I0402 14:12:25.302028 139845687019264 logging_writer.py:48] [64] global_step=64, grad_norm=0.029389, loss=0.138993
I0402 14:12:25.306012 139985604192064 submission.py:296] 64) loss = 0.139, grad_norm = 0.029
I0402 14:12:27.109489 139845678626560 logging_writer.py:48] [65] global_step=65, grad_norm=0.035407, loss=0.139052
I0402 14:12:27.112524 139985604192064 submission.py:296] 65) loss = 0.139, grad_norm = 0.035
I0402 14:12:28.877895 139845687019264 logging_writer.py:48] [66] global_step=66, grad_norm=0.042280, loss=0.141507
I0402 14:12:28.881980 139985604192064 submission.py:296] 66) loss = 0.142, grad_norm = 0.042
I0402 14:12:30.706391 139845678626560 logging_writer.py:48] [67] global_step=67, grad_norm=0.041939, loss=0.140772
I0402 14:12:30.710370 139985604192064 submission.py:296] 67) loss = 0.141, grad_norm = 0.042
I0402 14:12:32.506331 139845687019264 logging_writer.py:48] [68] global_step=68, grad_norm=0.038050, loss=0.141919
I0402 14:12:32.514910 139985604192064 submission.py:296] 68) loss = 0.142, grad_norm = 0.038
I0402 14:12:34.306715 139845678626560 logging_writer.py:48] [69] global_step=69, grad_norm=0.046787, loss=0.139715
I0402 14:12:34.325746 139985604192064 submission.py:296] 69) loss = 0.140, grad_norm = 0.047
I0402 14:12:36.117401 139845687019264 logging_writer.py:48] [70] global_step=70, grad_norm=0.039277, loss=0.140353
I0402 14:12:36.120964 139985604192064 submission.py:296] 70) loss = 0.140, grad_norm = 0.039
I0402 14:12:37.963839 139845678626560 logging_writer.py:48] [71] global_step=71, grad_norm=0.035454, loss=0.139424
I0402 14:12:37.967902 139985604192064 submission.py:296] 71) loss = 0.139, grad_norm = 0.035
I0402 14:12:39.862268 139845687019264 logging_writer.py:48] [72] global_step=72, grad_norm=0.031878, loss=0.138197
I0402 14:12:39.869502 139985604192064 submission.py:296] 72) loss = 0.138, grad_norm = 0.032
I0402 14:12:41.685131 139845678626560 logging_writer.py:48] [73] global_step=73, grad_norm=0.041937, loss=0.140392
I0402 14:12:41.688172 139985604192064 submission.py:296] 73) loss = 0.140, grad_norm = 0.042
I0402 14:12:43.511917 139845687019264 logging_writer.py:48] [74] global_step=74, grad_norm=0.052639, loss=0.138359
I0402 14:12:43.520558 139985604192064 submission.py:296] 74) loss = 0.138, grad_norm = 0.053
I0402 14:12:45.324337 139845678626560 logging_writer.py:48] [75] global_step=75, grad_norm=0.040913, loss=0.139099
I0402 14:12:45.327371 139985604192064 submission.py:296] 75) loss = 0.139, grad_norm = 0.041
I0402 14:12:47.175592 139845678626560 logging_writer.py:48] [76] global_step=76, grad_norm=0.036133, loss=0.138663
I0402 14:12:47.179051 139985604192064 submission.py:296] 76) loss = 0.139, grad_norm = 0.036
I0402 14:12:49.031911 139845687019264 logging_writer.py:48] [77] global_step=77, grad_norm=0.022191, loss=0.136312
I0402 14:12:49.039057 139985604192064 submission.py:296] 77) loss = 0.136, grad_norm = 0.022
I0402 14:12:50.855197 139845678626560 logging_writer.py:48] [78] global_step=78, grad_norm=0.039165, loss=0.136121
I0402 14:12:50.866324 139985604192064 submission.py:296] 78) loss = 0.136, grad_norm = 0.039
I0402 14:12:52.686111 139845687019264 logging_writer.py:48] [79] global_step=79, grad_norm=0.036421, loss=0.136460
I0402 14:12:52.694662 139985604192064 submission.py:296] 79) loss = 0.136, grad_norm = 0.036
I0402 14:12:54.476019 139845678626560 logging_writer.py:48] [80] global_step=80, grad_norm=0.055343, loss=0.137788
I0402 14:12:54.479108 139985604192064 submission.py:296] 80) loss = 0.138, grad_norm = 0.055
I0402 14:12:56.300269 139845687019264 logging_writer.py:48] [81] global_step=81, grad_norm=0.060809, loss=0.138015
I0402 14:12:56.303572 139985604192064 submission.py:296] 81) loss = 0.138, grad_norm = 0.061
I0402 14:12:58.102416 139845678626560 logging_writer.py:48] [82] global_step=82, grad_norm=0.031186, loss=0.135684
I0402 14:12:58.121540 139985604192064 submission.py:296] 82) loss = 0.136, grad_norm = 0.031
I0402 14:12:59.882190 139845687019264 logging_writer.py:48] [83] global_step=83, grad_norm=0.017362, loss=0.135944
I0402 14:12:59.894508 139985604192064 submission.py:296] 83) loss = 0.136, grad_norm = 0.017
I0402 14:13:01.732511 139845678626560 logging_writer.py:48] [84] global_step=84, grad_norm=0.037271, loss=0.137169
I0402 14:13:01.736393 139985604192064 submission.py:296] 84) loss = 0.137, grad_norm = 0.037
I0402 14:13:03.550573 139845687019264 logging_writer.py:48] [85] global_step=85, grad_norm=0.053558, loss=0.136885
I0402 14:13:03.558879 139985604192064 submission.py:296] 85) loss = 0.137, grad_norm = 0.054
I0402 14:13:05.344789 139845678626560 logging_writer.py:48] [86] global_step=86, grad_norm=0.042158, loss=0.133448
I0402 14:13:05.353888 139985604192064 submission.py:296] 86) loss = 0.133, grad_norm = 0.042
I0402 14:13:07.117756 139845687019264 logging_writer.py:48] [87] global_step=87, grad_norm=0.017980, loss=0.134064
I0402 14:13:07.129927 139985604192064 submission.py:296] 87) loss = 0.134, grad_norm = 0.018
I0402 14:13:08.935844 139845678626560 logging_writer.py:48] [88] global_step=88, grad_norm=0.034260, loss=0.134590
I0402 14:13:08.939850 139985604192064 submission.py:296] 88) loss = 0.135, grad_norm = 0.034
I0402 14:13:10.777513 139845687019264 logging_writer.py:48] [89] global_step=89, grad_norm=0.059177, loss=0.134024
I0402 14:13:10.790777 139985604192064 submission.py:296] 89) loss = 0.134, grad_norm = 0.059
I0402 14:13:12.611885 139845678626560 logging_writer.py:48] [90] global_step=90, grad_norm=0.141855, loss=0.135327
I0402 14:13:12.618787 139985604192064 submission.py:296] 90) loss = 0.135, grad_norm = 0.142
I0402 14:13:14.523789 139845687019264 logging_writer.py:48] [91] global_step=91, grad_norm=0.298794, loss=0.133251
I0402 14:13:14.535803 139985604192064 submission.py:296] 91) loss = 0.133, grad_norm = 0.299
I0402 14:13:16.361663 139845678626560 logging_writer.py:48] [92] global_step=92, grad_norm=0.381671, loss=0.137554
I0402 14:13:16.370242 139985604192064 submission.py:296] 92) loss = 0.138, grad_norm = 0.382
I0402 14:13:18.181868 139845687019264 logging_writer.py:48] [93] global_step=93, grad_norm=0.278829, loss=0.136121
I0402 14:13:18.192738 139985604192064 submission.py:296] 93) loss = 0.136, grad_norm = 0.279
I0402 14:13:20.036778 139845678626560 logging_writer.py:48] [94] global_step=94, grad_norm=0.106141, loss=0.131960
I0402 14:13:20.040987 139985604192064 submission.py:296] 94) loss = 0.132, grad_norm = 0.106
I0402 14:13:21.884703 139845687019264 logging_writer.py:48] [95] global_step=95, grad_norm=0.021866, loss=0.132434
I0402 14:13:21.895913 139985604192064 submission.py:296] 95) loss = 0.132, grad_norm = 0.022
I0402 14:13:23.729309 139845678626560 logging_writer.py:48] [96] global_step=96, grad_norm=0.016574, loss=0.132014
I0402 14:13:23.741951 139985604192064 submission.py:296] 96) loss = 0.132, grad_norm = 0.017
I0402 14:13:25.519734 139845687019264 logging_writer.py:48] [97] global_step=97, grad_norm=0.035323, loss=0.133312
I0402 14:13:25.535323 139985604192064 submission.py:296] 97) loss = 0.133, grad_norm = 0.035
I0402 14:13:27.404710 139845678626560 logging_writer.py:48] [98] global_step=98, grad_norm=0.052850, loss=0.132863
I0402 14:13:27.420648 139985604192064 submission.py:296] 98) loss = 0.133, grad_norm = 0.053
I0402 14:13:29.331509 139845687019264 logging_writer.py:48] [99] global_step=99, grad_norm=0.054593, loss=0.132821
I0402 14:13:29.335179 139985604192064 submission.py:296] 99) loss = 0.133, grad_norm = 0.055
I0402 14:13:31.223202 139845678626560 logging_writer.py:48] [100] global_step=100, grad_norm=0.074430, loss=0.131836
I0402 14:13:31.238010 139985604192064 submission.py:296] 100) loss = 0.132, grad_norm = 0.074
I0402 14:19:26.933585 139985604192064 submission_runner.py:373] Before eval at step 297: RAM USED (GB) 113.710112768
I0402 14:19:26.933957 139985604192064 spec.py:298] Evaluating on the training split.
I0402 14:29:38.892390 139985604192064 spec.py:310] Evaluating on the validation split.
I0402 14:34:22.563343 139985604192064 spec.py:326] Evaluating on the test split.
I0402 14:39:26.189956 139985604192064 submission_runner.py:382] Time since start: 1899.19s, 	Step: 297, 	{'train/loss': 0.12688308502489193, 'validation/loss': 0.1275481797752809, 'validation/num_examples': 89000000, 'test/loss': 0.13000345215629383, 'test/num_examples': 89274637}
I0402 14:39:26.190486 139985604192064 submission_runner.py:396] After eval at step 297: RAM USED (GB) 117.609046016
I0402 14:39:26.201862 139845687019264 logging_writer.py:48] [297] global_step=297, preemption_count=0, score=667.998633, test/loss=0.130003, test/num_examples=89274637, total_duration=1899.188994, train/loss=0.126883, validation/loss=0.127548, validation/num_examples=89000000
I0402 14:39:40.917005 139985604192064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_297.
I0402 14:39:40.917585 139985604192064 submission_runner.py:416] After logging and checkpointing eval at step 297: RAM USED (GB) 117.600354304
I0402 14:45:46.432860 139845678626560 logging_writer.py:48] [500] global_step=500, grad_norm=0.030021, loss=0.126537
I0402 14:45:46.438685 139985604192064 submission.py:296] 500) loss = 0.127, grad_norm = 0.030
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 14:48:42.081791 139985604192064 submission_runner.py:373] Before eval at step 599: RAM USED (GB) 120.230002688
I0402 14:48:42.081997 139985604192064 spec.py:298] Evaluating on the training split.
I0402 14:58:58.518824 139985604192064 spec.py:310] Evaluating on the validation split.
I0402 15:04:03.931778 139985604192064 spec.py:326] Evaluating on the test split.
I0402 15:08:41.612077 139985604192064 submission_runner.py:382] Time since start: 3654.34s, 	Step: 599, 	{'train/loss': 0.12543737370797692, 'validation/loss': 0.12654419101123596, 'validation/num_examples': 89000000, 'test/loss': 0.12886184012151178, 'test/num_examples': 89274637}
I0402 15:08:41.612570 139985604192064 submission_runner.py:396] After eval at step 599: RAM USED (GB) 122.421010432
I0402 15:08:41.621014 139839907297024 logging_writer.py:48] [599] global_step=599, preemption_count=0, score=1168.734004, test/loss=0.128862, test/num_examples=89274637, total_duration=3654.341267, train/loss=0.125437, validation/loss=0.126544, validation/num_examples=89000000
I0402 15:08:55.722762 139985604192064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_599.
I0402 15:08:55.723233 139985604192064 submission_runner.py:416] After logging and checkpointing eval at step 599: RAM USED (GB) 122.343043072
I0402 15:14:55.655654 139985604192064 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 123.794952192
I0402 15:14:55.655852 139985604192064 spec.py:298] Evaluating on the training split.
I0402 15:25:06.465871 139985604192064 spec.py:310] Evaluating on the validation split.
I0402 15:30:00.730312 139985604192064 spec.py:326] Evaluating on the test split.
I0402 15:35:20.624670 139985604192064 submission_runner.py:382] Time since start: 5227.91s, 	Step: 800, 	{'train/loss': 0.12439224209480201, 'validation/loss': 0.12587457303370786, 'validation/num_examples': 89000000, 'test/loss': 0.12822629567230837, 'test/num_examples': 89274637}
I0402 15:35:20.625089 139985604192064 submission_runner.py:396] After eval at step 800: RAM USED (GB) 124.982648832
I0402 15:35:20.633285 139839898904320 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1501.689150, test/loss=0.128226, test/num_examples=89274637, total_duration=5227.912340, train/loss=0.124392, validation/loss=0.125875, validation/num_examples=89000000
I0402 15:35:35.135187 139985604192064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0402 15:35:35.135657 139985604192064 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 124.834250752
I0402 15:35:35.142869 139839907297024 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1501.689150
I0402 15:35:50.302691 139985604192064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0402 15:37:30.308561 139985604192064 submission_runner.py:550] Tuning trial 1/1
I0402 15:37:30.308789 139985604192064 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 15:37:30.314136 139985604192064 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 0.9592470192926784, 'validation/loss': 0.9636854831460674, 'validation/num_examples': 89000000, 'test/loss': 0.9619735558263878, 'test/num_examples': 89274637, 'score': 154.236976146698, 'total_duration': 154.2388472557068, 'global_step': 1, 'preemption_count': 0}), (297, {'train/loss': 0.12688308502489193, 'validation/loss': 0.1275481797752809, 'validation/num_examples': 89000000, 'test/loss': 0.13000345215629383, 'test/num_examples': 89274637, 'score': 667.9986329078674, 'total_duration': 1899.1889941692352, 'global_step': 297, 'preemption_count': 0}), (599, {'train/loss': 0.12543737370797692, 'validation/loss': 0.12654419101123596, 'validation/num_examples': 89000000, 'test/loss': 0.12886184012151178, 'test/num_examples': 89274637, 'score': 1168.7340037822723, 'total_duration': 3654.3412671089172, 'global_step': 599, 'preemption_count': 0}), (800, {'train/loss': 0.12439224209480201, 'validation/loss': 0.12587457303370786, 'validation/num_examples': 89000000, 'test/loss': 0.12822629567230837, 'test/num_examples': 89274637, 'score': 1501.6891496181488, 'total_duration': 5227.912340164185, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0402 15:37:30.315073 139985604192064 submission_runner.py:553] Timing: 1501.6891496181488
I0402 15:37:30.315158 139985604192064 submission_runner.py:554] ====================
I0402 15:37:30.315283 139985604192064 submission_runner.py:613] Final criteo1tb score: 1501.6891496181488
