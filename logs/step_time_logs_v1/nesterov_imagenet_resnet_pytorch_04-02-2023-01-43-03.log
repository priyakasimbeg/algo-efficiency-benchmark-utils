WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 01:43:25.147980 139665536984896 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 01:43:25.147961 140318560528192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 01:43:25.148025 140451136837440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 01:43:25.148051 140628080133952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 01:43:26.134042 139971894908736 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 01:43:26.134119 140219221247808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 01:43:26.134351 140151642752832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 01:43:26.139157 140281312577344 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 01:43:26.139484 140281312577344 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.144626 139971894908736 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.144795 140219221247808 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.144893 140151642752832 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.148031 140318560528192 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.148055 139665536984896 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.148099 140451136837440 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:26.148126 140628080133952 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:43:28.282186 140281312577344 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/imagenet_resnet_pytorch.
W0402 01:43:28.395588 140628080133952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.395781 139665536984896 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.396456 139971894908736 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.397213 140219221247808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.397441 140151642752832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.397855 140318560528192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.397830 140451136837440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:43:28.399038 140281312577344 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 01:43:28.402447 140281312577344 submission_runner.py:511] Using RNG seed 1109928736
I0402 01:43:28.403361 140281312577344 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 01:43:28.403466 140281312577344 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1.
I0402 01:43:28.403633 140281312577344 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/hparams.json.
I0402 01:43:28.404613 140281312577344 submission_runner.py:230] Starting train once: RAM USED (GB) 6.126280704
I0402 01:43:28.404711 140281312577344 submission_runner.py:231] Initializing dataset.
I0402 01:43:32.632390 140281312577344 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 8.140992512
I0402 01:43:32.632563 140281312577344 submission_runner.py:240] Initializing model.
I0402 01:43:37.471223 140281312577344 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.968828416
I0402 01:43:37.471438 140281312577344 submission_runner.py:252] Initializing optimizer.
I0402 01:43:37.708026 140281312577344 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.971396608
I0402 01:43:37.708217 140281312577344 submission_runner.py:261] Initializing metrics bundle.
I0402 01:43:37.708274 140281312577344 submission_runner.py:276] Initializing checkpoint and logger.
I0402 01:43:38.370713 140281312577344 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0402 01:43:38.371615 140281312577344 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0402 01:43:38.417109 140281312577344 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.019332096
I0402 01:43:38.418331 140281312577344 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.019332096
I0402 01:43:38.418490 140281312577344 submission_runner.py:313] Starting training loop.
I0402 01:43:40.909842 140281312577344 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.350648832
I0402 01:43:46.877825 140254472304384 logging_writer.py:48] [0] global_step=0, grad_norm=0.530586, loss=6.931471
I0402 01:43:46.887822 140281312577344 submission.py:139] 0) loss = 6.931, grad_norm = 0.531
I0402 01:43:46.888347 140281312577344 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.198156288
I0402 01:43:46.904711 140281312577344 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.201252864
I0402 01:43:46.904834 140281312577344 spec.py:298] Evaluating on the training split.
I0402 01:44:38.884203 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 01:45:24.694657 140281312577344 spec.py:326] Evaluating on the test split.
I0402 01:45:24.710425 140281312577344 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0402 01:45:24.716814 140281312577344 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0402 01:45:24.786098 140281312577344 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
2023-04-02 01:45:25.372685: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:25.386810: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:25.421277: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:25.447753: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:25.501280: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:25.511063: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:25.526710: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:26.176347: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:26.249657: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:26.798785: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:26.976798: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:27.224022: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:27.282006: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:27.354992: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:27.592598: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:27.903662: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:27.942150: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:28.107480: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:28.414737: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:28.519090: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:28.846178: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:28.857371: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:28.865270: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:29.040481: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:29.394714: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:29.454202: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:29.836101: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:29.958258: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:30.350616: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:30.452372: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:30.963453: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:37.744389: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:38.110048: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:38.450295: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:45:38.792289: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0402 01:45:41.425094 140281312577344 submission_runner.py:382] Time since start: 8.49s, 	Step: 1, 	{'train/accuracy': 0.0008569834183673469, 'train/loss': 6.924480827487245, 'validation/accuracy': 0.00118, 'validation/loss': 6.925161875, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92762734375, 'test/num_examples': 10000}
I0402 01:45:41.425523 140281312577344 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.004163584
I0402 01:45:41.434355 140228970919680 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=8.483572, test/accuracy=0.000600, test/loss=6.927627, test/num_examples=10000, total_duration=8.485729, train/accuracy=0.000857, train/loss=6.924481, validation/accuracy=0.001180, validation/loss=6.925162, validation/num_examples=50000
I0402 01:45:41.751977 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0402 01:45:41.752672 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.00525312
I0402 01:45:41.780002 140281312577344 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.006227968
I0402 01:45:41.786867 140281312577344 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787087 140318560528192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787494 140451136837440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787514 139971894908736 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787537 139665536984896 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787524 140151642752832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787532 140219221247808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:41.787738 140628080133952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:45:42.156652 140228962526976 logging_writer.py:48] [1] global_step=1, grad_norm=0.549166, loss=6.921206
I0402 01:45:42.160266 140281312577344 submission.py:139] 1) loss = 6.921, grad_norm = 0.549
I0402 01:45:42.160920 140281312577344 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.011749376
I0402 01:45:42.532659 140228970919680 logging_writer.py:48] [2] global_step=2, grad_norm=0.539989, loss=6.925491
I0402 01:45:42.536423 140281312577344 submission.py:139] 2) loss = 6.925, grad_norm = 0.540
I0402 01:45:42.918347 140228962526976 logging_writer.py:48] [3] global_step=3, grad_norm=0.544095, loss=6.922989
I0402 01:45:42.922402 140281312577344 submission.py:139] 3) loss = 6.923, grad_norm = 0.544
I0402 01:45:43.297951 140228970919680 logging_writer.py:48] [4] global_step=4, grad_norm=0.534469, loss=6.926891
I0402 01:45:43.302171 140281312577344 submission.py:139] 4) loss = 6.927, grad_norm = 0.534
I0402 01:45:43.678498 140228962526976 logging_writer.py:48] [5] global_step=5, grad_norm=0.528912, loss=6.922511
I0402 01:45:43.682113 140281312577344 submission.py:139] 5) loss = 6.923, grad_norm = 0.529
I0402 01:45:44.061151 140228970919680 logging_writer.py:48] [6] global_step=6, grad_norm=0.551221, loss=6.932734
I0402 01:45:44.064954 140281312577344 submission.py:139] 6) loss = 6.933, grad_norm = 0.551
I0402 01:45:44.442056 140228962526976 logging_writer.py:48] [7] global_step=7, grad_norm=0.552626, loss=6.935263
I0402 01:45:44.446530 140281312577344 submission.py:139] 7) loss = 6.935, grad_norm = 0.553
I0402 01:45:44.823088 140228970919680 logging_writer.py:48] [8] global_step=8, grad_norm=0.543515, loss=6.925544
I0402 01:45:44.828776 140281312577344 submission.py:139] 8) loss = 6.926, grad_norm = 0.544
I0402 01:45:45.212408 140228962526976 logging_writer.py:48] [9] global_step=9, grad_norm=0.557705, loss=6.927385
I0402 01:45:45.216888 140281312577344 submission.py:139] 9) loss = 6.927, grad_norm = 0.558
I0402 01:45:45.594362 140228970919680 logging_writer.py:48] [10] global_step=10, grad_norm=0.542950, loss=6.920057
I0402 01:45:45.598066 140281312577344 submission.py:139] 10) loss = 6.920, grad_norm = 0.543
I0402 01:45:45.973184 140228962526976 logging_writer.py:48] [11] global_step=11, grad_norm=0.545889, loss=6.934439
I0402 01:45:45.977474 140281312577344 submission.py:139] 11) loss = 6.934, grad_norm = 0.546
I0402 01:45:46.355861 140228970919680 logging_writer.py:48] [12] global_step=12, grad_norm=0.529007, loss=6.925824
I0402 01:45:46.359463 140281312577344 submission.py:139] 12) loss = 6.926, grad_norm = 0.529
I0402 01:45:46.737647 140228962526976 logging_writer.py:48] [13] global_step=13, grad_norm=0.543085, loss=6.931474
I0402 01:45:46.741514 140281312577344 submission.py:139] 13) loss = 6.931, grad_norm = 0.543
I0402 01:45:47.116533 140228970919680 logging_writer.py:48] [14] global_step=14, grad_norm=0.555094, loss=6.933800
I0402 01:45:47.120383 140281312577344 submission.py:139] 14) loss = 6.934, grad_norm = 0.555
I0402 01:45:47.498321 140228962526976 logging_writer.py:48] [15] global_step=15, grad_norm=0.528638, loss=6.918306
I0402 01:45:47.503791 140281312577344 submission.py:139] 15) loss = 6.918, grad_norm = 0.529
I0402 01:45:47.879021 140228970919680 logging_writer.py:48] [16] global_step=16, grad_norm=0.533774, loss=6.932906
I0402 01:45:47.882549 140281312577344 submission.py:139] 16) loss = 6.933, grad_norm = 0.534
I0402 01:45:48.261729 140228962526976 logging_writer.py:48] [17] global_step=17, grad_norm=0.545121, loss=6.934043
I0402 01:45:48.265555 140281312577344 submission.py:139] 17) loss = 6.934, grad_norm = 0.545
I0402 01:45:48.650268 140228970919680 logging_writer.py:48] [18] global_step=18, grad_norm=0.540099, loss=6.926542
I0402 01:45:48.654143 140281312577344 submission.py:139] 18) loss = 6.927, grad_norm = 0.540
I0402 01:45:49.033581 140228962526976 logging_writer.py:48] [19] global_step=19, grad_norm=0.542225, loss=6.924323
I0402 01:45:49.037087 140281312577344 submission.py:139] 19) loss = 6.924, grad_norm = 0.542
I0402 01:45:49.413557 140228970919680 logging_writer.py:48] [20] global_step=20, grad_norm=0.544459, loss=6.931523
I0402 01:45:49.417522 140281312577344 submission.py:139] 20) loss = 6.932, grad_norm = 0.544
I0402 01:45:49.793931 140228962526976 logging_writer.py:48] [21] global_step=21, grad_norm=0.535063, loss=6.924804
I0402 01:45:49.800362 140281312577344 submission.py:139] 21) loss = 6.925, grad_norm = 0.535
I0402 01:45:50.176029 140228970919680 logging_writer.py:48] [22] global_step=22, grad_norm=0.542411, loss=6.926868
I0402 01:45:50.180280 140281312577344 submission.py:139] 22) loss = 6.927, grad_norm = 0.542
I0402 01:45:50.555409 140228962526976 logging_writer.py:48] [23] global_step=23, grad_norm=0.538562, loss=6.932288
I0402 01:45:50.559001 140281312577344 submission.py:139] 23) loss = 6.932, grad_norm = 0.539
I0402 01:45:50.975637 140228970919680 logging_writer.py:48] [24] global_step=24, grad_norm=0.547028, loss=6.929432
I0402 01:45:50.979457 140281312577344 submission.py:139] 24) loss = 6.929, grad_norm = 0.547
I0402 01:45:51.353191 140228962526976 logging_writer.py:48] [25] global_step=25, grad_norm=0.535773, loss=6.923200
I0402 01:45:51.356697 140281312577344 submission.py:139] 25) loss = 6.923, grad_norm = 0.536
I0402 01:45:51.729637 140228970919680 logging_writer.py:48] [26] global_step=26, grad_norm=0.547119, loss=6.921103
I0402 01:45:51.733440 140281312577344 submission.py:139] 26) loss = 6.921, grad_norm = 0.547
I0402 01:45:52.113269 140228962526976 logging_writer.py:48] [27] global_step=27, grad_norm=0.527601, loss=6.918691
I0402 01:45:52.116947 140281312577344 submission.py:139] 27) loss = 6.919, grad_norm = 0.528
I0402 01:45:52.494514 140228970919680 logging_writer.py:48] [28] global_step=28, grad_norm=0.546034, loss=6.920803
I0402 01:45:52.497956 140281312577344 submission.py:139] 28) loss = 6.921, grad_norm = 0.546
I0402 01:45:52.871885 140228962526976 logging_writer.py:48] [29] global_step=29, grad_norm=0.544441, loss=6.926776
I0402 01:45:52.875980 140281312577344 submission.py:139] 29) loss = 6.927, grad_norm = 0.544
I0402 01:45:53.253577 140228970919680 logging_writer.py:48] [30] global_step=30, grad_norm=0.543631, loss=6.932536
I0402 01:45:53.257164 140281312577344 submission.py:139] 30) loss = 6.933, grad_norm = 0.544
I0402 01:45:53.635138 140228962526976 logging_writer.py:48] [31] global_step=31, grad_norm=0.526341, loss=6.920537
I0402 01:45:53.638800 140281312577344 submission.py:139] 31) loss = 6.921, grad_norm = 0.526
I0402 01:45:54.014266 140228970919680 logging_writer.py:48] [32] global_step=32, grad_norm=0.514648, loss=6.910658
I0402 01:45:54.018079 140281312577344 submission.py:139] 32) loss = 6.911, grad_norm = 0.515
I0402 01:45:54.401164 140228962526976 logging_writer.py:48] [33] global_step=33, grad_norm=0.554933, loss=6.921939
I0402 01:45:54.405214 140281312577344 submission.py:139] 33) loss = 6.922, grad_norm = 0.555
I0402 01:45:54.781052 140228970919680 logging_writer.py:48] [34] global_step=34, grad_norm=0.538178, loss=6.927245
I0402 01:45:54.784821 140281312577344 submission.py:139] 34) loss = 6.927, grad_norm = 0.538
I0402 01:45:55.161311 140228962526976 logging_writer.py:48] [35] global_step=35, grad_norm=0.530765, loss=6.922366
I0402 01:45:55.165046 140281312577344 submission.py:139] 35) loss = 6.922, grad_norm = 0.531
I0402 01:45:55.541449 140228970919680 logging_writer.py:48] [36] global_step=36, grad_norm=0.550143, loss=6.927516
I0402 01:45:55.545262 140281312577344 submission.py:139] 36) loss = 6.928, grad_norm = 0.550
I0402 01:45:55.922209 140228962526976 logging_writer.py:48] [37] global_step=37, grad_norm=0.543283, loss=6.929776
I0402 01:45:55.925725 140281312577344 submission.py:139] 37) loss = 6.930, grad_norm = 0.543
I0402 01:45:56.303049 140228970919680 logging_writer.py:48] [38] global_step=38, grad_norm=0.531854, loss=6.933879
I0402 01:45:56.307104 140281312577344 submission.py:139] 38) loss = 6.934, grad_norm = 0.532
I0402 01:45:56.685049 140228962526976 logging_writer.py:48] [39] global_step=39, grad_norm=0.534563, loss=6.924762
I0402 01:45:56.688930 140281312577344 submission.py:139] 39) loss = 6.925, grad_norm = 0.535
I0402 01:45:57.065450 140228970919680 logging_writer.py:48] [40] global_step=40, grad_norm=0.542540, loss=6.921009
I0402 01:45:57.069256 140281312577344 submission.py:139] 40) loss = 6.921, grad_norm = 0.543
I0402 01:45:57.445611 140228962526976 logging_writer.py:48] [41] global_step=41, grad_norm=0.527874, loss=6.930949
I0402 01:45:57.449287 140281312577344 submission.py:139] 41) loss = 6.931, grad_norm = 0.528
I0402 01:45:57.827962 140228970919680 logging_writer.py:48] [42] global_step=42, grad_norm=0.535648, loss=6.923185
I0402 01:45:57.833034 140281312577344 submission.py:139] 42) loss = 6.923, grad_norm = 0.536
I0402 01:45:58.213012 140228962526976 logging_writer.py:48] [43] global_step=43, grad_norm=0.536814, loss=6.911929
I0402 01:45:58.217836 140281312577344 submission.py:139] 43) loss = 6.912, grad_norm = 0.537
I0402 01:45:58.591766 140228970919680 logging_writer.py:48] [44] global_step=44, grad_norm=0.527362, loss=6.913527
I0402 01:45:58.595554 140281312577344 submission.py:139] 44) loss = 6.914, grad_norm = 0.527
I0402 01:45:58.972053 140228962526976 logging_writer.py:48] [45] global_step=45, grad_norm=0.519556, loss=6.919503
I0402 01:45:58.975598 140281312577344 submission.py:139] 45) loss = 6.920, grad_norm = 0.520
I0402 01:45:59.351051 140228970919680 logging_writer.py:48] [46] global_step=46, grad_norm=0.551550, loss=6.916312
I0402 01:45:59.354691 140281312577344 submission.py:139] 46) loss = 6.916, grad_norm = 0.552
I0402 01:45:59.733234 140228962526976 logging_writer.py:48] [47] global_step=47, grad_norm=0.537398, loss=6.923348
I0402 01:45:59.737135 140281312577344 submission.py:139] 47) loss = 6.923, grad_norm = 0.537
I0402 01:46:00.114897 140228970919680 logging_writer.py:48] [48] global_step=48, grad_norm=0.527338, loss=6.916337
I0402 01:46:00.121614 140281312577344 submission.py:139] 48) loss = 6.916, grad_norm = 0.527
I0402 01:46:00.500985 140228962526976 logging_writer.py:48] [49] global_step=49, grad_norm=0.540105, loss=6.916492
I0402 01:46:00.505061 140281312577344 submission.py:139] 49) loss = 6.916, grad_norm = 0.540
I0402 01:46:00.887489 140228970919680 logging_writer.py:48] [50] global_step=50, grad_norm=0.531146, loss=6.914445
I0402 01:46:00.891637 140281312577344 submission.py:139] 50) loss = 6.914, grad_norm = 0.531
I0402 01:46:01.269596 140228962526976 logging_writer.py:48] [51] global_step=51, grad_norm=0.529405, loss=6.914649
I0402 01:46:01.273400 140281312577344 submission.py:139] 51) loss = 6.915, grad_norm = 0.529
I0402 01:46:01.652799 140228970919680 logging_writer.py:48] [52] global_step=52, grad_norm=0.519992, loss=6.916602
I0402 01:46:01.656863 140281312577344 submission.py:139] 52) loss = 6.917, grad_norm = 0.520
I0402 01:46:02.037490 140228962526976 logging_writer.py:48] [53] global_step=53, grad_norm=0.518214, loss=6.904934
I0402 01:46:02.042546 140281312577344 submission.py:139] 53) loss = 6.905, grad_norm = 0.518
I0402 01:46:02.421142 140228970919680 logging_writer.py:48] [54] global_step=54, grad_norm=0.539941, loss=6.914609
I0402 01:46:02.425905 140281312577344 submission.py:139] 54) loss = 6.915, grad_norm = 0.540
I0402 01:46:02.800667 140228962526976 logging_writer.py:48] [55] global_step=55, grad_norm=0.522424, loss=6.912949
I0402 01:46:02.804750 140281312577344 submission.py:139] 55) loss = 6.913, grad_norm = 0.522
I0402 01:46:03.180890 140228970919680 logging_writer.py:48] [56] global_step=56, grad_norm=0.537402, loss=6.912136
I0402 01:46:03.185221 140281312577344 submission.py:139] 56) loss = 6.912, grad_norm = 0.537
I0402 01:46:03.561078 140228962526976 logging_writer.py:48] [57] global_step=57, grad_norm=0.534619, loss=6.912238
I0402 01:46:03.565803 140281312577344 submission.py:139] 57) loss = 6.912, grad_norm = 0.535
I0402 01:46:04.012750 140228970919680 logging_writer.py:48] [58] global_step=58, grad_norm=0.550309, loss=6.911150
I0402 01:46:04.016917 140281312577344 submission.py:139] 58) loss = 6.911, grad_norm = 0.550
I0402 01:46:04.392243 140228962526976 logging_writer.py:48] [59] global_step=59, grad_norm=0.537362, loss=6.907788
I0402 01:46:04.396067 140281312577344 submission.py:139] 59) loss = 6.908, grad_norm = 0.537
I0402 01:46:04.770595 140228970919680 logging_writer.py:48] [60] global_step=60, grad_norm=0.538415, loss=6.910058
I0402 01:46:04.774611 140281312577344 submission.py:139] 60) loss = 6.910, grad_norm = 0.538
I0402 01:46:05.154058 140228962526976 logging_writer.py:48] [61] global_step=61, grad_norm=0.516052, loss=6.910340
I0402 01:46:05.157519 140281312577344 submission.py:139] 61) loss = 6.910, grad_norm = 0.516
I0402 01:46:05.534191 140228970919680 logging_writer.py:48] [62] global_step=62, grad_norm=0.517833, loss=6.901501
I0402 01:46:05.538906 140281312577344 submission.py:139] 62) loss = 6.902, grad_norm = 0.518
I0402 01:46:05.916151 140228962526976 logging_writer.py:48] [63] global_step=63, grad_norm=0.538203, loss=6.904569
I0402 01:46:05.920236 140281312577344 submission.py:139] 63) loss = 6.905, grad_norm = 0.538
I0402 01:46:06.298816 140228970919680 logging_writer.py:48] [64] global_step=64, grad_norm=0.550176, loss=6.902965
I0402 01:46:06.304353 140281312577344 submission.py:139] 64) loss = 6.903, grad_norm = 0.550
I0402 01:46:06.682538 140228962526976 logging_writer.py:48] [65] global_step=65, grad_norm=0.528562, loss=6.906996
I0402 01:46:06.687803 140281312577344 submission.py:139] 65) loss = 6.907, grad_norm = 0.529
I0402 01:46:07.064039 140228970919680 logging_writer.py:48] [66] global_step=66, grad_norm=0.528908, loss=6.907596
I0402 01:46:07.067972 140281312577344 submission.py:139] 66) loss = 6.908, grad_norm = 0.529
I0402 01:46:07.445210 140228962526976 logging_writer.py:48] [67] global_step=67, grad_norm=0.535339, loss=6.911509
I0402 01:46:07.448805 140281312577344 submission.py:139] 67) loss = 6.912, grad_norm = 0.535
I0402 01:46:07.827925 140228970919680 logging_writer.py:48] [68] global_step=68, grad_norm=0.538561, loss=6.909015
I0402 01:46:07.831392 140281312577344 submission.py:139] 68) loss = 6.909, grad_norm = 0.539
I0402 01:46:08.210990 140228962526976 logging_writer.py:48] [69] global_step=69, grad_norm=0.512223, loss=6.909410
I0402 01:46:08.215163 140281312577344 submission.py:139] 69) loss = 6.909, grad_norm = 0.512
I0402 01:46:08.591732 140228970919680 logging_writer.py:48] [70] global_step=70, grad_norm=0.532524, loss=6.909344
I0402 01:46:08.596749 140281312577344 submission.py:139] 70) loss = 6.909, grad_norm = 0.533
I0402 01:46:08.980265 140228962526976 logging_writer.py:48] [71] global_step=71, grad_norm=0.533092, loss=6.910621
I0402 01:46:08.984142 140281312577344 submission.py:139] 71) loss = 6.911, grad_norm = 0.533
I0402 01:46:09.360886 140228970919680 logging_writer.py:48] [72] global_step=72, grad_norm=0.532410, loss=6.898620
I0402 01:46:09.364955 140281312577344 submission.py:139] 72) loss = 6.899, grad_norm = 0.532
I0402 01:46:09.744669 140228962526976 logging_writer.py:48] [73] global_step=73, grad_norm=0.547941, loss=6.902765
I0402 01:46:09.748904 140281312577344 submission.py:139] 73) loss = 6.903, grad_norm = 0.548
I0402 01:46:10.126338 140228970919680 logging_writer.py:48] [74] global_step=74, grad_norm=0.515597, loss=6.907615
I0402 01:46:10.131026 140281312577344 submission.py:139] 74) loss = 6.908, grad_norm = 0.516
I0402 01:46:10.561603 140228962526976 logging_writer.py:48] [75] global_step=75, grad_norm=0.520701, loss=6.899458
I0402 01:46:10.565102 140281312577344 submission.py:139] 75) loss = 6.899, grad_norm = 0.521
I0402 01:46:10.945641 140228970919680 logging_writer.py:48] [76] global_step=76, grad_norm=0.519684, loss=6.902240
I0402 01:46:10.951347 140281312577344 submission.py:139] 76) loss = 6.902, grad_norm = 0.520
I0402 01:46:11.329339 140228962526976 logging_writer.py:48] [77] global_step=77, grad_norm=0.526710, loss=6.895298
I0402 01:46:11.333211 140281312577344 submission.py:139] 77) loss = 6.895, grad_norm = 0.527
I0402 01:46:11.708981 140228970919680 logging_writer.py:48] [78] global_step=78, grad_norm=0.539699, loss=6.903018
I0402 01:46:11.712636 140281312577344 submission.py:139] 78) loss = 6.903, grad_norm = 0.540
I0402 01:46:12.087275 140228962526976 logging_writer.py:48] [79] global_step=79, grad_norm=0.546861, loss=6.898925
I0402 01:46:12.091157 140281312577344 submission.py:139] 79) loss = 6.899, grad_norm = 0.547
I0402 01:46:12.469259 140228970919680 logging_writer.py:48] [80] global_step=80, grad_norm=0.513510, loss=6.896312
I0402 01:46:12.473434 140281312577344 submission.py:139] 80) loss = 6.896, grad_norm = 0.514
I0402 01:46:12.857319 140228962526976 logging_writer.py:48] [81] global_step=81, grad_norm=0.519868, loss=6.903239
I0402 01:46:12.861078 140281312577344 submission.py:139] 81) loss = 6.903, grad_norm = 0.520
I0402 01:46:13.236888 140228970919680 logging_writer.py:48] [82] global_step=82, grad_norm=0.522831, loss=6.893888
I0402 01:46:13.240593 140281312577344 submission.py:139] 82) loss = 6.894, grad_norm = 0.523
I0402 01:46:13.616024 140228962526976 logging_writer.py:48] [83] global_step=83, grad_norm=0.535108, loss=6.897515
I0402 01:46:13.619853 140281312577344 submission.py:139] 83) loss = 6.898, grad_norm = 0.535
I0402 01:46:14.000875 140228970919680 logging_writer.py:48] [84] global_step=84, grad_norm=0.525703, loss=6.904310
I0402 01:46:14.004479 140281312577344 submission.py:139] 84) loss = 6.904, grad_norm = 0.526
I0402 01:46:14.381769 140228962526976 logging_writer.py:48] [85] global_step=85, grad_norm=0.520208, loss=6.898429
I0402 01:46:14.385951 140281312577344 submission.py:139] 85) loss = 6.898, grad_norm = 0.520
I0402 01:46:14.761000 140228970919680 logging_writer.py:48] [86] global_step=86, grad_norm=0.536147, loss=6.889902
I0402 01:46:14.771806 140281312577344 submission.py:139] 86) loss = 6.890, grad_norm = 0.536
I0402 01:46:15.147787 140228962526976 logging_writer.py:48] [87] global_step=87, grad_norm=0.531189, loss=6.892770
I0402 01:46:15.151488 140281312577344 submission.py:139] 87) loss = 6.893, grad_norm = 0.531
I0402 01:46:15.530672 140228970919680 logging_writer.py:48] [88] global_step=88, grad_norm=0.524656, loss=6.895243
I0402 01:46:15.534961 140281312577344 submission.py:139] 88) loss = 6.895, grad_norm = 0.525
I0402 01:46:15.909620 140228962526976 logging_writer.py:48] [89] global_step=89, grad_norm=0.525825, loss=6.886789
I0402 01:46:15.913872 140281312577344 submission.py:139] 89) loss = 6.887, grad_norm = 0.526
I0402 01:46:16.293421 140228970919680 logging_writer.py:48] [90] global_step=90, grad_norm=0.538262, loss=6.896959
I0402 01:46:16.297459 140281312577344 submission.py:139] 90) loss = 6.897, grad_norm = 0.538
I0402 01:46:16.675933 140228962526976 logging_writer.py:48] [91] global_step=91, grad_norm=0.521069, loss=6.890509
I0402 01:46:16.679742 140281312577344 submission.py:139] 91) loss = 6.891, grad_norm = 0.521
I0402 01:46:17.059863 140228970919680 logging_writer.py:48] [92] global_step=92, grad_norm=0.540503, loss=6.890405
I0402 01:46:17.063560 140281312577344 submission.py:139] 92) loss = 6.890, grad_norm = 0.541
I0402 01:46:17.440143 140228962526976 logging_writer.py:48] [93] global_step=93, grad_norm=0.527613, loss=6.890152
I0402 01:46:17.445498 140281312577344 submission.py:139] 93) loss = 6.890, grad_norm = 0.528
I0402 01:46:17.822777 140228970919680 logging_writer.py:48] [94] global_step=94, grad_norm=0.518999, loss=6.894835
I0402 01:46:17.827144 140281312577344 submission.py:139] 94) loss = 6.895, grad_norm = 0.519
I0402 01:46:18.204924 140228962526976 logging_writer.py:48] [95] global_step=95, grad_norm=0.527877, loss=6.891180
I0402 01:46:18.208787 140281312577344 submission.py:139] 95) loss = 6.891, grad_norm = 0.528
I0402 01:46:18.586000 140228970919680 logging_writer.py:48] [96] global_step=96, grad_norm=0.526803, loss=6.883219
I0402 01:46:18.589990 140281312577344 submission.py:139] 96) loss = 6.883, grad_norm = 0.527
I0402 01:46:18.967402 140228962526976 logging_writer.py:48] [97] global_step=97, grad_norm=0.536815, loss=6.877973
I0402 01:46:18.971388 140281312577344 submission.py:139] 97) loss = 6.878, grad_norm = 0.537
I0402 01:46:19.346355 140228970919680 logging_writer.py:48] [98] global_step=98, grad_norm=0.531151, loss=6.883871
I0402 01:46:19.350808 140281312577344 submission.py:139] 98) loss = 6.884, grad_norm = 0.531
I0402 01:46:19.728871 140228962526976 logging_writer.py:48] [99] global_step=99, grad_norm=0.536215, loss=6.882921
I0402 01:46:19.732761 140281312577344 submission.py:139] 99) loss = 6.883, grad_norm = 0.536
I0402 01:46:20.110090 140228970919680 logging_writer.py:48] [100] global_step=100, grad_norm=0.531251, loss=6.889602
I0402 01:46:20.115910 140281312577344 submission.py:139] 100) loss = 6.890, grad_norm = 0.531
I0402 01:48:48.334326 140228962526976 logging_writer.py:48] [500] global_step=500, grad_norm=0.617554, loss=6.557433
I0402 01:48:48.342181 140281312577344 submission.py:139] 500) loss = 6.557, grad_norm = 0.618
I0402 01:51:53.433706 140228970919680 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.780327, loss=6.265012
I0402 01:51:53.438225 140281312577344 submission.py:139] 1000) loss = 6.265, grad_norm = 0.780
I0402 01:54:12.039391 140281312577344 submission_runner.py:373] Before eval at step 1372: RAM USED (GB) 98.474496
I0402 01:54:12.039633 140281312577344 spec.py:298] Evaluating on the training split.
I0402 01:54:53.998416 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 01:55:38.667384 140281312577344 spec.py:326] Evaluating on the test split.
I0402 01:55:40.041576 140281312577344 submission_runner.py:382] Time since start: 633.59s, 	Step: 1372, 	{'train/accuracy': 0.05893255739795918, 'train/loss': 5.632380271444515, 'validation/accuracy': 0.05514, 'validation/loss': 5.694425, 'validation/num_examples': 50000, 'test/accuracy': 0.0365, 'test/loss': 5.920837109375, 'test/num_examples': 10000}
I0402 01:55:40.041969 140281312577344 submission_runner.py:396] After eval at step 1372: RAM USED (GB) 98.49450496
I0402 01:55:40.050237 140228979312384 logging_writer.py:48] [1372] global_step=1372, preemption_count=0, score=475.060665, test/accuracy=0.036500, test/loss=5.920837, test/num_examples=10000, total_duration=633.588903, train/accuracy=0.058933, train/loss=5.632380, validation/accuracy=0.055140, validation/loss=5.694425, validation/num_examples=50000
I0402 01:55:40.363357 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_1372.
I0402 01:55:40.364144 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 1372: RAM USED (GB) 98.493464576
I0402 01:56:27.980202 140228987705088 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.838164, loss=5.984845
I0402 01:56:27.984103 140281312577344 submission.py:139] 1500) loss = 5.985, grad_norm = 0.838
I0402 01:59:32.734337 140228979312384 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.836368, loss=5.722438
I0402 01:59:32.738271 140281312577344 submission.py:139] 2000) loss = 5.722, grad_norm = 0.836
I0402 02:02:37.759040 140228987705088 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.945332, loss=5.460663
I0402 02:02:37.762794 140281312577344 submission.py:139] 2500) loss = 5.461, grad_norm = 0.945
I0402 02:04:10.641491 140281312577344 submission_runner.py:373] Before eval at step 2749: RAM USED (GB) 99.839946752
I0402 02:04:10.641726 140281312577344 spec.py:298] Evaluating on the training split.
I0402 02:04:55.838935 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 02:05:41.401578 140281312577344 spec.py:326] Evaluating on the test split.
I0402 02:05:42.759112 140281312577344 submission_runner.py:382] Time since start: 1232.19s, 	Step: 2749, 	{'train/accuracy': 0.16169084821428573, 'train/loss': 4.418681864835778, 'validation/accuracy': 0.15074, 'validation/loss': 4.519574375, 'validation/num_examples': 50000, 'test/accuracy': 0.1013, 'test/loss': 4.98422734375, 'test/num_examples': 10000}
I0402 02:05:42.759456 140281312577344 submission_runner.py:396] After eval at step 2749: RAM USED (GB) 99.869421568
I0402 02:05:42.767655 140228979312384 logging_writer.py:48] [2749] global_step=2749, preemption_count=0, score=938.269160, test/accuracy=0.101300, test/loss=4.984227, test/num_examples=10000, total_duration=1232.190924, train/accuracy=0.161691, train/loss=4.418682, validation/accuracy=0.150740, validation/loss=4.519574, validation/num_examples=50000
I0402 02:05:43.078678 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_2749.
I0402 02:05:43.079507 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 2749: RAM USED (GB) 99.866402816
I0402 02:07:16.028959 140228987705088 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.050647, loss=5.187912
I0402 02:07:16.032611 140281312577344 submission.py:139] 3000) loss = 5.188, grad_norm = 1.051
I0402 02:10:20.575593 140228979312384 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.883347, loss=4.861254
I0402 02:10:20.579572 140281312577344 submission.py:139] 3500) loss = 4.861, grad_norm = 0.883
I0402 02:13:26.507546 140228987705088 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.841473, loss=4.778764
I0402 02:13:26.514824 140281312577344 submission.py:139] 4000) loss = 4.779, grad_norm = 0.841
I0402 02:14:13.352672 140281312577344 submission_runner.py:373] Before eval at step 4128: RAM USED (GB) 99.908308992
I0402 02:14:13.352902 140281312577344 spec.py:298] Evaluating on the training split.
I0402 02:14:56.083519 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 02:15:50.670210 140281312577344 spec.py:326] Evaluating on the test split.
I0402 02:15:52.023312 140281312577344 submission_runner.py:382] Time since start: 1834.90s, 	Step: 4128, 	{'train/accuracy': 0.29282924107142855, 'train/loss': 3.5153002057756697, 'validation/accuracy': 0.2685, 'validation/loss': 3.6523490625, 'validation/num_examples': 50000, 'test/accuracy': 0.1893, 'test/loss': 4.2758046875, 'test/num_examples': 10000}
I0402 02:15:52.023713 140281312577344 submission_runner.py:396] After eval at step 4128: RAM USED (GB) 99.88089856
I0402 02:15:52.032403 140228979312384 logging_writer.py:48] [4128] global_step=4128, preemption_count=0, score=1401.473273, test/accuracy=0.189300, test/loss=4.275805, test/num_examples=10000, total_duration=1834.902276, train/accuracy=0.292829, train/loss=3.515300, validation/accuracy=0.268500, validation/loss=3.652349, validation/num_examples=50000
I0402 02:15:52.338095 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_4128.
I0402 02:15:52.338891 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 4128: RAM USED (GB) 99.879440384
I0402 02:18:09.854939 140228987705088 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.773999, loss=4.469988
I0402 02:18:09.859525 140281312577344 submission.py:139] 4500) loss = 4.470, grad_norm = 0.774
I0402 02:21:14.554043 140228979312384 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.780156, loss=4.461134
I0402 02:21:14.558675 140281312577344 submission.py:139] 5000) loss = 4.461, grad_norm = 0.780
I0402 02:24:20.412671 140228987705088 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.745133, loss=4.380843
I0402 02:24:20.416458 140281312577344 submission.py:139] 5500) loss = 4.381, grad_norm = 0.745
I0402 02:24:22.620607 140281312577344 submission_runner.py:373] Before eval at step 5507: RAM USED (GB) 100.1076736
I0402 02:24:22.620833 140281312577344 spec.py:298] Evaluating on the training split.
I0402 02:25:06.758200 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 02:25:52.115170 140281312577344 spec.py:326] Evaluating on the test split.
I0402 02:25:53.466034 140281312577344 submission_runner.py:382] Time since start: 2444.17s, 	Step: 5507, 	{'train/accuracy': 0.4198022959183674, 'train/loss': 2.792347810706314, 'validation/accuracy': 0.3876, 'validation/loss': 2.955465, 'validation/num_examples': 50000, 'test/accuracy': 0.2875, 'test/loss': 3.589047265625, 'test/num_examples': 10000}
I0402 02:25:53.466408 140281312577344 submission_runner.py:396] After eval at step 5507: RAM USED (GB) 100.0095744
I0402 02:25:53.473885 140228979312384 logging_writer.py:48] [5507] global_step=5507, preemption_count=0, score=1864.661355, test/accuracy=0.287500, test/loss=3.589047, test/num_examples=10000, total_duration=2444.169547, train/accuracy=0.419802, train/loss=2.792348, validation/accuracy=0.387600, validation/loss=2.955465, validation/num_examples=50000
I0402 02:25:53.781032 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_5507.
I0402 02:25:53.781757 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 5507: RAM USED (GB) 100.008374272
I0402 02:28:56.043428 140228987705088 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.727590, loss=4.230416
I0402 02:28:56.047733 140281312577344 submission.py:139] 6000) loss = 4.230, grad_norm = 0.728
I0402 02:32:01.984821 140228979312384 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.694356, loss=4.089665
I0402 02:32:01.989502 140281312577344 submission.py:139] 6500) loss = 4.090, grad_norm = 0.694
I0402 02:34:24.164783 140281312577344 submission_runner.py:373] Before eval at step 6887: RAM USED (GB) 100.049657856
I0402 02:34:24.164994 140281312577344 spec.py:298] Evaluating on the training split.
I0402 02:35:07.403733 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 02:36:01.641281 140281312577344 spec.py:326] Evaluating on the test split.
I0402 02:36:02.989357 140281312577344 submission_runner.py:382] Time since start: 3045.71s, 	Step: 6887, 	{'train/accuracy': 0.5086694834183674, 'train/loss': 2.352378378109056, 'validation/accuracy': 0.46592, 'validation/loss': 2.5465859375, 'validation/num_examples': 50000, 'test/accuracy': 0.3435, 'test/loss': 3.23773984375, 'test/num_examples': 10000}
I0402 02:36:02.989727 140281312577344 submission_runner.py:396] After eval at step 6887: RAM USED (GB) 100.02679808
I0402 02:36:02.997400 140228987705088 logging_writer.py:48] [6887] global_step=6887, preemption_count=0, score=2327.899218, test/accuracy=0.343500, test/loss=3.237740, test/num_examples=10000, total_duration=3045.713584, train/accuracy=0.508669, train/loss=2.352378, validation/accuracy=0.465920, validation/loss=2.546586, validation/num_examples=50000
I0402 02:36:03.301194 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_6887.
I0402 02:36:03.301932 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 6887: RAM USED (GB) 100.026605568
I0402 02:36:45.373641 140228979312384 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.686114, loss=4.053514
I0402 02:36:45.377352 140281312577344 submission.py:139] 7000) loss = 4.054, grad_norm = 0.686
I0402 02:39:50.035452 140228987705088 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.606064, loss=3.950128
I0402 02:39:50.040184 140281312577344 submission.py:139] 7500) loss = 3.950, grad_norm = 0.606
I0402 02:42:55.803274 140228979312384 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.657166, loss=3.884233
I0402 02:42:55.807829 140281312577344 submission.py:139] 8000) loss = 3.884, grad_norm = 0.657
I0402 02:44:33.444267 140281312577344 submission_runner.py:373] Before eval at step 8266: RAM USED (GB) 100.169080832
I0402 02:44:33.444481 140281312577344 spec.py:298] Evaluating on the training split.
I0402 02:45:17.192390 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 02:46:02.441148 140281312577344 spec.py:326] Evaluating on the test split.
I0402 02:46:03.793351 140281312577344 submission_runner.py:382] Time since start: 3654.99s, 	Step: 8266, 	{'train/accuracy': 0.5285794005102041, 'train/loss': 2.2830496028978, 'validation/accuracy': 0.48742, 'validation/loss': 2.48391046875, 'validation/num_examples': 50000, 'test/accuracy': 0.3578, 'test/loss': 3.206183984375, 'test/num_examples': 10000}
I0402 02:46:03.793684 140281312577344 submission_runner.py:396] After eval at step 8266: RAM USED (GB) 100.116852736
I0402 02:46:03.801918 140228987705088 logging_writer.py:48] [8266] global_step=8266, preemption_count=0, score=2790.916612, test/accuracy=0.357800, test/loss=3.206184, test/num_examples=10000, total_duration=3654.993938, train/accuracy=0.528579, train/loss=2.283050, validation/accuracy=0.487420, validation/loss=2.483910, validation/num_examples=50000
I0402 02:46:04.119210 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_8266.
I0402 02:46:04.120024 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 8266: RAM USED (GB) 100.115337216
I0402 02:47:30.854562 140228979312384 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.599164, loss=3.861699
I0402 02:47:30.858064 140281312577344 submission.py:139] 8500) loss = 3.862, grad_norm = 0.599
I0402 02:50:37.176225 140228987705088 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.553043, loss=3.831892
I0402 02:50:37.181905 140281312577344 submission.py:139] 9000) loss = 3.832, grad_norm = 0.553
I0402 02:53:41.429287 140228979312384 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.538273, loss=3.790191
I0402 02:53:41.433132 140281312577344 submission.py:139] 9500) loss = 3.790, grad_norm = 0.538
I0402 02:54:34.485641 140281312577344 submission_runner.py:373] Before eval at step 9645: RAM USED (GB) 100.07363584
I0402 02:54:34.485852 140281312577344 spec.py:298] Evaluating on the training split.
I0402 02:55:17.464442 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 02:56:09.916959 140281312577344 spec.py:326] Evaluating on the test split.
I0402 02:56:11.259059 140281312577344 submission_runner.py:382] Time since start: 4256.03s, 	Step: 9645, 	{'train/accuracy': 0.5784438775510204, 'train/loss': 1.9636554328762754, 'validation/accuracy': 0.53272, 'validation/loss': 2.1882903125, 'validation/num_examples': 50000, 'test/accuracy': 0.3952, 'test/loss': 2.9422205078125, 'test/num_examples': 10000}
I0402 02:56:11.259409 140281312577344 submission_runner.py:396] After eval at step 9645: RAM USED (GB) 100.06482944
I0402 02:56:11.267803 140228987705088 logging_writer.py:48] [9645] global_step=9645, preemption_count=0, score=3254.179135, test/accuracy=0.395200, test/loss=2.942221, test/num_examples=10000, total_duration=4256.034840, train/accuracy=0.578444, train/loss=1.963655, validation/accuracy=0.532720, validation/loss=2.188290, validation/num_examples=50000
I0402 02:56:11.576189 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_9645.
I0402 02:56:11.576941 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 9645: RAM USED (GB) 100.064313344
I0402 02:58:23.061829 140228979312384 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.520558, loss=3.604111
I0402 02:58:23.065992 140281312577344 submission.py:139] 10000) loss = 3.604, grad_norm = 0.521
I0402 03:01:28.788124 140228987705088 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.521525, loss=3.669313
I0402 03:01:28.804445 140281312577344 submission.py:139] 10500) loss = 3.669, grad_norm = 0.522
I0402 03:04:33.215483 140228979312384 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.528235, loss=3.577416
I0402 03:04:33.219476 140281312577344 submission.py:139] 11000) loss = 3.577, grad_norm = 0.528
I0402 03:04:41.715831 140281312577344 submission_runner.py:373] Before eval at step 11024: RAM USED (GB) 100.052934656
I0402 03:04:41.716040 140281312577344 spec.py:298] Evaluating on the training split.
I0402 03:05:24.871693 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 03:06:10.116794 140281312577344 spec.py:326] Evaluating on the test split.
I0402 03:06:11.467269 140281312577344 submission_runner.py:382] Time since start: 4863.26s, 	Step: 11024, 	{'train/accuracy': 0.6095942283163265, 'train/loss': 1.9132852359693877, 'validation/accuracy': 0.55432, 'validation/loss': 2.15206921875, 'validation/num_examples': 50000, 'test/accuracy': 0.4166, 'test/loss': 2.9193625, 'test/num_examples': 10000}
I0402 03:06:11.467612 140281312577344 submission_runner.py:396] After eval at step 11024: RAM USED (GB) 100.025745408
I0402 03:06:11.480667 140228987705088 logging_writer.py:48] [11024] global_step=11024, preemption_count=0, score=3717.178575, test/accuracy=0.416600, test/loss=2.919363, test/num_examples=10000, total_duration=4863.260985, train/accuracy=0.609594, train/loss=1.913285, validation/accuracy=0.554320, validation/loss=2.152069, validation/num_examples=50000
I0402 03:06:11.798755 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_11024.
I0402 03:06:11.799538 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 11024: RAM USED (GB) 100.024778752
I0402 03:09:09.234122 140228979312384 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.498913, loss=3.502540
I0402 03:09:09.238974 140281312577344 submission.py:139] 11500) loss = 3.503, grad_norm = 0.499
I0402 03:12:13.501787 140228987705088 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.537083, loss=3.546282
I0402 03:12:13.505895 140281312577344 submission.py:139] 12000) loss = 3.546, grad_norm = 0.537
I0402 03:14:41.842303 140281312577344 submission_runner.py:373] Before eval at step 12403: RAM USED (GB) 100.066275328
I0402 03:14:41.842521 140281312577344 spec.py:298] Evaluating on the training split.
I0402 03:15:24.456557 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 03:16:18.764017 140281312577344 spec.py:326] Evaluating on the test split.
I0402 03:16:20.130779 140281312577344 submission_runner.py:382] Time since start: 5463.39s, 	Step: 12403, 	{'train/accuracy': 0.6385722257653061, 'train/loss': 1.7247949716996174, 'validation/accuracy': 0.58468, 'validation/loss': 1.98398, 'validation/num_examples': 50000, 'test/accuracy': 0.4463, 'test/loss': 2.706627734375, 'test/num_examples': 10000}
I0402 03:16:20.132066 140281312577344 submission_runner.py:396] After eval at step 12403: RAM USED (GB) 100.110143488
I0402 03:16:20.140268 140228979312384 logging_writer.py:48] [12403] global_step=12403, preemption_count=0, score=4180.074799, test/accuracy=0.446300, test/loss=2.706628, test/num_examples=10000, total_duration=5463.392298, train/accuracy=0.638572, train/loss=1.724795, validation/accuracy=0.584680, validation/loss=1.983980, validation/num_examples=50000
I0402 03:16:20.458816 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_12403.
I0402 03:16:20.459538 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 12403: RAM USED (GB) 100.107759616
I0402 03:16:56.664649 140228987705088 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.497347, loss=3.592259
I0402 03:16:56.672408 140281312577344 submission.py:139] 12500) loss = 3.592, grad_norm = 0.497
I0402 03:20:02.522322 140228979312384 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.502673, loss=3.466531
I0402 03:20:02.529106 140281312577344 submission.py:139] 13000) loss = 3.467, grad_norm = 0.503
I0402 03:23:06.905296 140228987705088 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.481607, loss=3.440606
I0402 03:23:06.909790 140281312577344 submission.py:139] 13500) loss = 3.441, grad_norm = 0.482
I0402 03:24:50.859698 140281312577344 submission_runner.py:373] Before eval at step 13778: RAM USED (GB) 99.996131328
I0402 03:24:50.859940 140281312577344 spec.py:298] Evaluating on the training split.
I0402 03:25:33.460024 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 03:26:22.539611 140281312577344 spec.py:326] Evaluating on the test split.
I0402 03:26:23.890776 140281312577344 submission_runner.py:382] Time since start: 6072.41s, 	Step: 13778, 	{'train/accuracy': 0.6738679846938775, 'train/loss': 1.5288059468172035, 'validation/accuracy': 0.61028, 'validation/loss': 1.82220234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4734, 'test/loss': 2.525687890625, 'test/num_examples': 10000}
I0402 03:26:23.891108 140281312577344 submission_runner.py:396] After eval at step 13778: RAM USED (GB) 100.01946624
I0402 03:26:23.899868 140228979312384 logging_writer.py:48] [13778] global_step=13778, preemption_count=0, score=4643.486900, test/accuracy=0.473400, test/loss=2.525688, test/num_examples=10000, total_duration=6072.408801, train/accuracy=0.673868, train/loss=1.528806, validation/accuracy=0.610280, validation/loss=1.822202, validation/num_examples=50000
I0402 03:26:24.215596 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_13778.
I0402 03:26:24.216410 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 13778: RAM USED (GB) 100.018434048
I0402 03:27:49.189024 140281312577344 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.129329152
I0402 03:27:49.189239 140281312577344 spec.py:298] Evaluating on the training split.
I0402 03:28:31.160773 140281312577344 spec.py:310] Evaluating on the validation split.
I0402 03:29:15.910385 140281312577344 spec.py:326] Evaluating on the test split.
I0402 03:29:17.257736 140281312577344 submission_runner.py:382] Time since start: 6250.74s, 	Step: 14000, 	{'train/accuracy': 0.6592992665816326, 'train/loss': 1.6074729452327805, 'validation/accuracy': 0.59628, 'validation/loss': 1.902615625, 'validation/num_examples': 50000, 'test/accuracy': 0.458, 'test/loss': 2.627416015625, 'test/num_examples': 10000}
I0402 03:29:17.258053 140281312577344 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.139962368
I0402 03:29:17.266816 140228987705088 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4720.886478, test/accuracy=0.458000, test/loss=2.627416, test/num_examples=10000, total_duration=6250.738515, train/accuracy=0.659299, train/loss=1.607473, validation/accuracy=0.596280, validation/loss=1.902616, validation/num_examples=50000
I0402 03:29:17.577307 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0402 03:29:17.578107 140281312577344 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.138471424
I0402 03:29:17.586326 140228979312384 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4720.886478
I0402 03:29:18.446714 140281312577344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0402 03:29:18.800936 140281312577344 submission_runner.py:550] Tuning trial 1/1
I0402 03:29:18.801147 140281312577344 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 03:29:18.801752 140281312577344 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008569834183673469, 'train/loss': 6.924480827487245, 'validation/accuracy': 0.00118, 'validation/loss': 6.925161875, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92762734375, 'test/num_examples': 10000, 'score': 8.483572006225586, 'total_duration': 8.485728740692139, 'global_step': 1, 'preemption_count': 0}), (1372, {'train/accuracy': 0.05893255739795918, 'train/loss': 5.632380271444515, 'validation/accuracy': 0.05514, 'validation/loss': 5.694425, 'validation/num_examples': 50000, 'test/accuracy': 0.0365, 'test/loss': 5.920837109375, 'test/num_examples': 10000, 'score': 475.06066489219666, 'total_duration': 633.5889031887054, 'global_step': 1372, 'preemption_count': 0}), (2749, {'train/accuracy': 0.16169084821428573, 'train/loss': 4.418681864835778, 'validation/accuracy': 0.15074, 'validation/loss': 4.519574375, 'validation/num_examples': 50000, 'test/accuracy': 0.1013, 'test/loss': 4.98422734375, 'test/num_examples': 10000, 'score': 938.2691597938538, 'total_duration': 1232.190924167633, 'global_step': 2749, 'preemption_count': 0}), (4128, {'train/accuracy': 0.29282924107142855, 'train/loss': 3.5153002057756697, 'validation/accuracy': 0.2685, 'validation/loss': 3.6523490625, 'validation/num_examples': 50000, 'test/accuracy': 0.1893, 'test/loss': 4.2758046875, 'test/num_examples': 10000, 'score': 1401.473272562027, 'total_duration': 1834.902275800705, 'global_step': 4128, 'preemption_count': 0}), (5507, {'train/accuracy': 0.4198022959183674, 'train/loss': 2.792347810706314, 'validation/accuracy': 0.3876, 'validation/loss': 2.955465, 'validation/num_examples': 50000, 'test/accuracy': 0.2875, 'test/loss': 3.589047265625, 'test/num_examples': 10000, 'score': 1864.6613552570343, 'total_duration': 2444.1695470809937, 'global_step': 5507, 'preemption_count': 0}), (6887, {'train/accuracy': 0.5086694834183674, 'train/loss': 2.352378378109056, 'validation/accuracy': 0.46592, 'validation/loss': 2.5465859375, 'validation/num_examples': 50000, 'test/accuracy': 0.3435, 'test/loss': 3.23773984375, 'test/num_examples': 10000, 'score': 2327.8992178440094, 'total_duration': 3045.713583946228, 'global_step': 6887, 'preemption_count': 0}), (8266, {'train/accuracy': 0.5285794005102041, 'train/loss': 2.2830496028978, 'validation/accuracy': 0.48742, 'validation/loss': 2.48391046875, 'validation/num_examples': 50000, 'test/accuracy': 0.3578, 'test/loss': 3.206183984375, 'test/num_examples': 10000, 'score': 2790.9166119098663, 'total_duration': 3654.993937730789, 'global_step': 8266, 'preemption_count': 0}), (9645, {'train/accuracy': 0.5784438775510204, 'train/loss': 1.9636554328762754, 'validation/accuracy': 0.53272, 'validation/loss': 2.1882903125, 'validation/num_examples': 50000, 'test/accuracy': 0.3952, 'test/loss': 2.9422205078125, 'test/num_examples': 10000, 'score': 3254.179135084152, 'total_duration': 4256.034840106964, 'global_step': 9645, 'preemption_count': 0}), (11024, {'train/accuracy': 0.6095942283163265, 'train/loss': 1.9132852359693877, 'validation/accuracy': 0.55432, 'validation/loss': 2.15206921875, 'validation/num_examples': 50000, 'test/accuracy': 0.4166, 'test/loss': 2.9193625, 'test/num_examples': 10000, 'score': 3717.1785748004913, 'total_duration': 4863.260985136032, 'global_step': 11024, 'preemption_count': 0}), (12403, {'train/accuracy': 0.6385722257653061, 'train/loss': 1.7247949716996174, 'validation/accuracy': 0.58468, 'validation/loss': 1.98398, 'validation/num_examples': 50000, 'test/accuracy': 0.4463, 'test/loss': 2.706627734375, 'test/num_examples': 10000, 'score': 4180.07479929924, 'total_duration': 5463.392298460007, 'global_step': 12403, 'preemption_count': 0}), (13778, {'train/accuracy': 0.6738679846938775, 'train/loss': 1.5288059468172035, 'validation/accuracy': 0.61028, 'validation/loss': 1.82220234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4734, 'test/loss': 2.525687890625, 'test/num_examples': 10000, 'score': 4643.486899614334, 'total_duration': 6072.408801317215, 'global_step': 13778, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6592992665816326, 'train/loss': 1.6074729452327805, 'validation/accuracy': 0.59628, 'validation/loss': 1.902615625, 'validation/num_examples': 50000, 'test/accuracy': 0.458, 'test/loss': 2.627416015625, 'test/num_examples': 10000, 'score': 4720.886478424072, 'total_duration': 6250.738515377045, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0402 03:29:18.801851 140281312577344 submission_runner.py:553] Timing: 4720.886478424072
I0402 03:29:18.801894 140281312577344 submission_runner.py:554] ====================
I0402 03:29:18.801987 140281312577344 submission_runner.py:613] Final imagenet_resnet score: 4720.886478424072
