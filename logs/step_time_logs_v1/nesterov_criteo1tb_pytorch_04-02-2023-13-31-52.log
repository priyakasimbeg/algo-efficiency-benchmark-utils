WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 13:32:14.400140 140060759770944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 13:32:14.400204 139683136710464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 13:32:14.400204 140671575861056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 13:32:14.401104 139667992926016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 13:32:14.401157 140127291205440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 13:32:14.401180 140388011444032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 13:32:14.401208 139712407586624 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 13:32:14.401481 140127291205440 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.401502 140388011444032 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.401531 139712407586624 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.401542 139804701747008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 13:32:14.401934 139804701747008 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.410911 140060759770944 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.410963 139683136710464 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.410967 140671575861056 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.411658 139667992926016 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 13:32:14.418765 140127291205440 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/criteo1tb_pytorch.
W0402 13:32:14.760668 140388011444032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.760668 140671575861056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.760666 139712407586624 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.760669 139683136710464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.761617 140060759770944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.761649 139667992926016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.762395 140127291205440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 13:32:14.762506 139804701747008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 13:32:14.767097 140127291205440 submission_runner.py:511] Using RNG seed 3242008892
I0402 13:32:14.768113 140127291205440 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 13:32:14.768237 140127291205440 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1.
I0402 13:32:14.768438 140127291205440 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/hparams.json.
I0402 13:32:14.769348 140127291205440 submission_runner.py:230] Starting train once: RAM USED (GB) 5.597048832
I0402 13:32:14.769442 140127291205440 submission_runner.py:231] Initializing dataset.
I0402 13:32:14.769609 140127291205440 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.597048832
I0402 13:32:14.769674 140127291205440 submission_runner.py:240] Initializing model.
I0402 13:32:29.628205 140127291205440 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.191519232
I0402 13:32:29.628384 140127291205440 submission_runner.py:252] Initializing optimizer.
I0402 13:32:30.171103 140127291205440 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.195672576
I0402 13:32:30.171276 140127291205440 submission_runner.py:261] Initializing metrics bundle.
I0402 13:32:30.171325 140127291205440 submission_runner.py:276] Initializing checkpoint and logger.
I0402 13:32:30.174802 140127291205440 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 13:32:30.174941 140127291205440 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 13:32:30.775537 140127291205440 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0402 13:32:30.776508 140127291205440 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/flags_0.json.
I0402 13:32:30.814171 140127291205440 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.246790656
I0402 13:32:30.815162 140127291205440 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.246790656
I0402 13:32:30.815272 140127291205440 submission_runner.py:313] Starting training loop.
I0402 13:35:00.566150 140127291205440 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 54.063976448
I0402 13:35:03.901887 140044119570176 logging_writer.py:48] [0] global_step=0, grad_norm=4.680261, loss=0.339268
I0402 13:35:03.907578 140127291205440 submission.py:139] 0) loss = 0.339, grad_norm = 4.680
I0402 13:35:03.908128 140127291205440 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 58.320605184
I0402 13:35:03.908784 140127291205440 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 58.320605184
I0402 13:35:03.908897 140127291205440 spec.py:298] Evaluating on the training split.
I0402 13:44:36.524522 140127291205440 spec.py:310] Evaluating on the validation split.
I0402 13:49:46.877344 140127291205440 spec.py:326] Evaluating on the test split.
I0402 13:54:13.387373 140127291205440 submission_runner.py:382] Time since start: 153.09s, 	Step: 1, 	{'train/loss': 0.339504778078148, 'validation/loss': 0.3439924269662921, 'validation/num_examples': 89000000, 'test/loss': 0.34199724609353493, 'test/num_examples': 89274637}
I0402 13:54:13.387826 140127291205440 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.321877504
I0402 13:54:13.397630 139989769684736 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=153.092044, test/loss=0.341997, test/num_examples=89274637, total_duration=153.093934, train/loss=0.339505, validation/loss=0.343992, validation/num_examples=89000000
I0402 13:54:22.808976 140127291205440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_1.
I0402 13:54:22.809448 140127291205440 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.26570496
I0402 13:54:22.826634 140127291205440 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.224138752
I0402 13:54:22.829883 140127291205440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829868 139667992926016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829874 139804701747008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829881 140388011444032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829873 140060759770944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829886 139683136710464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829886 139712407586624 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:22.829889 140671575861056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 13:54:24.605457 139989761292032 logging_writer.py:48] [1] global_step=1, grad_norm=4.669741, loss=0.339546
I0402 13:54:24.614414 140127291205440 submission.py:139] 1) loss = 0.340, grad_norm = 4.670
I0402 13:54:24.614834 140127291205440 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.356689408
I0402 13:54:26.327040 139989769684736 logging_writer.py:48] [2] global_step=2, grad_norm=3.727310, loss=0.293456
I0402 13:54:26.348710 140127291205440 submission.py:139] 2) loss = 0.293, grad_norm = 3.727
I0402 13:54:28.171597 139989761292032 logging_writer.py:48] [3] global_step=3, grad_norm=2.132491, loss=0.225507
I0402 13:54:28.183393 140127291205440 submission.py:139] 3) loss = 0.226, grad_norm = 2.132
I0402 13:54:29.948738 139989769684736 logging_writer.py:48] [4] global_step=4, grad_norm=0.898362, loss=0.178266
I0402 13:54:29.959266 140127291205440 submission.py:139] 4) loss = 0.178, grad_norm = 0.898
I0402 13:54:31.700173 139989761292032 logging_writer.py:48] [5] global_step=5, grad_norm=0.166076, loss=0.162523
I0402 13:54:31.708172 140127291205440 submission.py:139] 5) loss = 0.163, grad_norm = 0.166
I0402 13:54:33.447703 139989769684736 logging_writer.py:48] [6] global_step=6, grad_norm=0.453230, loss=0.169997
I0402 13:54:33.460964 140127291205440 submission.py:139] 6) loss = 0.170, grad_norm = 0.453
I0402 13:54:35.250170 139989761292032 logging_writer.py:48] [7] global_step=7, grad_norm=0.764840, loss=0.184636
I0402 13:54:35.258180 140127291205440 submission.py:139] 7) loss = 0.185, grad_norm = 0.765
I0402 13:54:37.102518 139989769684736 logging_writer.py:48] [8] global_step=8, grad_norm=0.987255, loss=0.202020
I0402 13:54:37.117779 140127291205440 submission.py:139] 8) loss = 0.202, grad_norm = 0.987
I0402 13:54:38.889418 139989761292032 logging_writer.py:48] [9] global_step=9, grad_norm=1.086798, loss=0.211304
I0402 13:54:38.910268 140127291205440 submission.py:139] 9) loss = 0.211, grad_norm = 1.087
I0402 13:54:40.644596 139989769684736 logging_writer.py:48] [10] global_step=10, grad_norm=1.153429, loss=0.219119
I0402 13:54:40.654043 140127291205440 submission.py:139] 10) loss = 0.219, grad_norm = 1.153
I0402 13:54:42.470491 139989761292032 logging_writer.py:48] [11] global_step=11, grad_norm=1.058576, loss=0.208058
I0402 13:54:42.488015 140127291205440 submission.py:139] 11) loss = 0.208, grad_norm = 1.059
I0402 13:54:44.295089 139989769684736 logging_writer.py:48] [12] global_step=12, grad_norm=0.858395, loss=0.187116
I0402 13:54:44.308115 140127291205440 submission.py:139] 12) loss = 0.187, grad_norm = 0.858
I0402 13:54:46.042659 139989761292032 logging_writer.py:48] [13] global_step=13, grad_norm=0.590043, loss=0.169943
I0402 13:54:46.069616 140127291205440 submission.py:139] 13) loss = 0.170, grad_norm = 0.590
I0402 13:54:47.807223 139989769684736 logging_writer.py:48] [14] global_step=14, grad_norm=0.243815, loss=0.157050
I0402 13:54:47.814087 140127291205440 submission.py:139] 14) loss = 0.157, grad_norm = 0.244
I0402 13:54:49.690499 139989761292032 logging_writer.py:48] [15] global_step=15, grad_norm=0.167024, loss=0.153265
I0402 13:54:49.713990 140127291205440 submission.py:139] 15) loss = 0.153, grad_norm = 0.167
I0402 13:54:51.588350 139989761292032 logging_writer.py:48] [16] global_step=16, grad_norm=0.406429, loss=0.156747
I0402 13:54:51.636260 140127291205440 submission.py:139] 16) loss = 0.157, grad_norm = 0.406
I0402 13:54:53.425506 139989769684736 logging_writer.py:48] [17] global_step=17, grad_norm=0.514335, loss=0.161977
I0402 13:54:53.436664 140127291205440 submission.py:139] 17) loss = 0.162, grad_norm = 0.514
I0402 13:54:55.180553 139989761292032 logging_writer.py:48] [18] global_step=18, grad_norm=0.516147, loss=0.160281
I0402 13:54:55.204011 140127291205440 submission.py:139] 18) loss = 0.160, grad_norm = 0.516
I0402 13:54:56.953038 139989769684736 logging_writer.py:48] [19] global_step=19, grad_norm=0.417418, loss=0.155372
I0402 13:54:56.968201 140127291205440 submission.py:139] 19) loss = 0.155, grad_norm = 0.417
I0402 13:54:58.712291 139989761292032 logging_writer.py:48] [20] global_step=20, grad_norm=0.256172, loss=0.149020
I0402 13:54:58.748539 140127291205440 submission.py:139] 20) loss = 0.149, grad_norm = 0.256
I0402 13:55:00.553753 139989769684736 logging_writer.py:48] [21] global_step=21, grad_norm=0.081462, loss=0.145264
I0402 13:55:00.574982 140127291205440 submission.py:139] 21) loss = 0.145, grad_norm = 0.081
I0402 13:55:02.362504 139989761292032 logging_writer.py:48] [22] global_step=22, grad_norm=0.115364, loss=0.145734
I0402 13:55:02.367412 140127291205440 submission.py:139] 22) loss = 0.146, grad_norm = 0.115
I0402 13:55:04.086962 139989769684736 logging_writer.py:48] [23] global_step=23, grad_norm=0.208840, loss=0.146462
I0402 13:55:04.103473 140127291205440 submission.py:139] 23) loss = 0.146, grad_norm = 0.209
I0402 13:55:05.860023 139989761292032 logging_writer.py:48] [24] global_step=24, grad_norm=0.244454, loss=0.145595
I0402 13:55:05.873201 140127291205440 submission.py:139] 24) loss = 0.146, grad_norm = 0.244
I0402 13:55:07.584547 139989769684736 logging_writer.py:48] [25] global_step=25, grad_norm=0.218666, loss=0.143795
I0402 13:55:07.616839 140127291205440 submission.py:139] 25) loss = 0.144, grad_norm = 0.219
I0402 13:55:09.324609 139989761292032 logging_writer.py:48] [26] global_step=26, grad_norm=0.181947, loss=0.145681
I0402 13:55:09.331137 140127291205440 submission.py:139] 26) loss = 0.146, grad_norm = 0.182
I0402 13:55:11.054206 139989769684736 logging_writer.py:48] [27] global_step=27, grad_norm=0.095437, loss=0.143781
I0402 13:55:11.062754 140127291205440 submission.py:139] 27) loss = 0.144, grad_norm = 0.095
I0402 13:55:12.768701 139989761292032 logging_writer.py:48] [28] global_step=28, grad_norm=0.040785, loss=0.142892
I0402 13:55:12.808229 140127291205440 submission.py:139] 28) loss = 0.143, grad_norm = 0.041
I0402 13:55:14.519912 139989769684736 logging_writer.py:48] [29] global_step=29, grad_norm=0.070719, loss=0.140222
I0402 13:55:14.530471 140127291205440 submission.py:139] 29) loss = 0.140, grad_norm = 0.071
I0402 13:55:16.264997 139989761292032 logging_writer.py:48] [30] global_step=30, grad_norm=0.059789, loss=0.144726
I0402 13:55:16.269694 140127291205440 submission.py:139] 30) loss = 0.145, grad_norm = 0.060
I0402 13:55:18.065414 139989769684736 logging_writer.py:48] [31] global_step=31, grad_norm=0.075823, loss=0.141712
I0402 13:55:18.094370 140127291205440 submission.py:139] 31) loss = 0.142, grad_norm = 0.076
I0402 13:55:19.839331 139989761292032 logging_writer.py:48] [32] global_step=32, grad_norm=0.043101, loss=0.142992
I0402 13:55:19.844798 140127291205440 submission.py:139] 32) loss = 0.143, grad_norm = 0.043
I0402 13:55:21.558047 139989769684736 logging_writer.py:48] [33] global_step=33, grad_norm=0.029105, loss=0.141871
I0402 13:55:21.589002 140127291205440 submission.py:139] 33) loss = 0.142, grad_norm = 0.029
I0402 13:55:23.325009 139989761292032 logging_writer.py:48] [34] global_step=34, grad_norm=0.033925, loss=0.142312
I0402 13:55:23.354650 140127291205440 submission.py:139] 34) loss = 0.142, grad_norm = 0.034
I0402 13:55:25.141378 139989769684736 logging_writer.py:48] [35] global_step=35, grad_norm=0.041191, loss=0.142411
I0402 13:55:25.144585 140127291205440 submission.py:139] 35) loss = 0.142, grad_norm = 0.041
I0402 13:55:26.857366 139989761292032 logging_writer.py:48] [36] global_step=36, grad_norm=0.037330, loss=0.141731
I0402 13:55:26.861047 140127291205440 submission.py:139] 36) loss = 0.142, grad_norm = 0.037
I0402 13:55:28.579382 139989769684736 logging_writer.py:48] [37] global_step=37, grad_norm=0.026293, loss=0.139640
I0402 13:55:28.582525 140127291205440 submission.py:139] 37) loss = 0.140, grad_norm = 0.026
I0402 13:55:30.326404 139989761292032 logging_writer.py:48] [38] global_step=38, grad_norm=0.025967, loss=0.140417
I0402 13:55:30.329509 140127291205440 submission.py:139] 38) loss = 0.140, grad_norm = 0.026
I0402 13:55:32.034710 139989769684736 logging_writer.py:48] [39] global_step=39, grad_norm=0.026026, loss=0.142377
I0402 13:55:32.037955 140127291205440 submission.py:139] 39) loss = 0.142, grad_norm = 0.026
I0402 13:55:33.722050 139989761292032 logging_writer.py:48] [40] global_step=40, grad_norm=0.025666, loss=0.143090
I0402 13:55:33.725073 140127291205440 submission.py:139] 40) loss = 0.143, grad_norm = 0.026
I0402 13:55:35.475062 139989769684736 logging_writer.py:48] [41] global_step=41, grad_norm=0.021935, loss=0.142085
I0402 13:55:35.478013 140127291205440 submission.py:139] 41) loss = 0.142, grad_norm = 0.022
I0402 13:55:37.188825 139989761292032 logging_writer.py:48] [42] global_step=42, grad_norm=0.027822, loss=0.139469
I0402 13:55:37.192395 140127291205440 submission.py:139] 42) loss = 0.139, grad_norm = 0.028
I0402 13:55:38.929656 139989769684736 logging_writer.py:48] [43] global_step=43, grad_norm=0.020878, loss=0.141352
I0402 13:55:38.934691 140127291205440 submission.py:139] 43) loss = 0.141, grad_norm = 0.021
I0402 13:55:40.672791 139989761292032 logging_writer.py:48] [44] global_step=44, grad_norm=0.019809, loss=0.139043
I0402 13:55:40.676227 140127291205440 submission.py:139] 44) loss = 0.139, grad_norm = 0.020
I0402 13:55:42.403470 139989769684736 logging_writer.py:48] [45] global_step=45, grad_norm=0.021081, loss=0.139536
I0402 13:55:42.407254 140127291205440 submission.py:139] 45) loss = 0.140, grad_norm = 0.021
I0402 13:55:44.125249 139989761292032 logging_writer.py:48] [46] global_step=46, grad_norm=0.021892, loss=0.140236
I0402 13:55:44.129078 140127291205440 submission.py:139] 46) loss = 0.140, grad_norm = 0.022
I0402 13:55:45.870783 139989769684736 logging_writer.py:48] [47] global_step=47, grad_norm=0.015900, loss=0.139102
I0402 13:55:45.874522 140127291205440 submission.py:139] 47) loss = 0.139, grad_norm = 0.016
I0402 13:55:47.647029 139989761292032 logging_writer.py:48] [48] global_step=48, grad_norm=0.019840, loss=0.141122
I0402 13:55:47.650512 140127291205440 submission.py:139] 48) loss = 0.141, grad_norm = 0.020
I0402 13:55:49.346893 139989769684736 logging_writer.py:48] [49] global_step=49, grad_norm=0.015614, loss=0.140083
I0402 13:55:49.350194 140127291205440 submission.py:139] 49) loss = 0.140, grad_norm = 0.016
I0402 13:55:51.082199 139989761292032 logging_writer.py:48] [50] global_step=50, grad_norm=0.017517, loss=0.139532
I0402 13:55:51.085928 140127291205440 submission.py:139] 50) loss = 0.140, grad_norm = 0.018
I0402 13:55:52.858132 139989769684736 logging_writer.py:48] [51] global_step=51, grad_norm=0.015664, loss=0.139649
I0402 13:55:52.861110 140127291205440 submission.py:139] 51) loss = 0.140, grad_norm = 0.016
I0402 13:55:54.570801 139989761292032 logging_writer.py:48] [52] global_step=52, grad_norm=0.014501, loss=0.139806
I0402 13:55:54.573822 140127291205440 submission.py:139] 52) loss = 0.140, grad_norm = 0.015
I0402 13:55:56.329038 139989769684736 logging_writer.py:48] [53] global_step=53, grad_norm=0.015076, loss=0.140806
I0402 13:55:56.332091 140127291205440 submission.py:139] 53) loss = 0.141, grad_norm = 0.015
I0402 13:55:58.040466 139989761292032 logging_writer.py:48] [54] global_step=54, grad_norm=0.012869, loss=0.140203
I0402 13:55:58.043522 140127291205440 submission.py:139] 54) loss = 0.140, grad_norm = 0.013
I0402 13:55:59.758238 139989769684736 logging_writer.py:48] [55] global_step=55, grad_norm=0.012472, loss=0.139737
I0402 13:55:59.761682 140127291205440 submission.py:139] 55) loss = 0.140, grad_norm = 0.012
I0402 13:56:01.479125 139989761292032 logging_writer.py:48] [56] global_step=56, grad_norm=0.013071, loss=0.139084
I0402 13:56:01.483003 140127291205440 submission.py:139] 56) loss = 0.139, grad_norm = 0.013
I0402 13:56:03.221230 139989769684736 logging_writer.py:48] [57] global_step=57, grad_norm=0.019150, loss=0.141356
I0402 13:56:03.224229 140127291205440 submission.py:139] 57) loss = 0.141, grad_norm = 0.019
I0402 13:56:04.931856 139989761292032 logging_writer.py:48] [58] global_step=58, grad_norm=0.016249, loss=0.138330
I0402 13:56:04.934778 140127291205440 submission.py:139] 58) loss = 0.138, grad_norm = 0.016
I0402 13:56:06.676306 139989769684736 logging_writer.py:48] [59] global_step=59, grad_norm=0.011500, loss=0.139947
I0402 13:56:06.679286 140127291205440 submission.py:139] 59) loss = 0.140, grad_norm = 0.012
I0402 13:56:08.407587 139989761292032 logging_writer.py:48] [60] global_step=60, grad_norm=0.015481, loss=0.137715
I0402 13:56:08.410557 140127291205440 submission.py:139] 60) loss = 0.138, grad_norm = 0.015
I0402 13:56:10.173394 139989769684736 logging_writer.py:48] [61] global_step=61, grad_norm=0.015145, loss=0.139553
I0402 13:56:10.176298 140127291205440 submission.py:139] 61) loss = 0.140, grad_norm = 0.015
I0402 13:56:11.887874 139989761292032 logging_writer.py:48] [62] global_step=62, grad_norm=0.010838, loss=0.138288
I0402 13:56:11.892210 140127291205440 submission.py:139] 62) loss = 0.138, grad_norm = 0.011
I0402 13:56:13.632802 139989769684736 logging_writer.py:48] [63] global_step=63, grad_norm=0.010635, loss=0.137660
I0402 13:56:13.636099 140127291205440 submission.py:139] 63) loss = 0.138, grad_norm = 0.011
I0402 13:56:15.401916 139989761292032 logging_writer.py:48] [64] global_step=64, grad_norm=0.011280, loss=0.138640
I0402 13:56:15.405360 140127291205440 submission.py:139] 64) loss = 0.139, grad_norm = 0.011
I0402 13:56:17.282300 139989769684736 logging_writer.py:48] [65] global_step=65, grad_norm=0.012728, loss=0.139717
I0402 13:56:17.286187 140127291205440 submission.py:139] 65) loss = 0.140, grad_norm = 0.013
I0402 13:56:19.087862 139989761292032 logging_writer.py:48] [66] global_step=66, grad_norm=0.010262, loss=0.140132
I0402 13:56:19.090893 140127291205440 submission.py:139] 66) loss = 0.140, grad_norm = 0.010
I0402 13:56:20.827545 139989769684736 logging_writer.py:48] [67] global_step=67, grad_norm=0.010692, loss=0.140125
I0402 13:56:20.830934 140127291205440 submission.py:139] 67) loss = 0.140, grad_norm = 0.011
I0402 13:56:22.618535 139989761292032 logging_writer.py:48] [68] global_step=68, grad_norm=0.011275, loss=0.139043
I0402 13:56:22.622027 140127291205440 submission.py:139] 68) loss = 0.139, grad_norm = 0.011
I0402 13:56:24.408672 139989769684736 logging_writer.py:48] [69] global_step=69, grad_norm=0.009742, loss=0.138133
I0402 13:56:24.412364 140127291205440 submission.py:139] 69) loss = 0.138, grad_norm = 0.010
I0402 13:56:26.144334 139989761292032 logging_writer.py:48] [70] global_step=70, grad_norm=0.010617, loss=0.139183
I0402 13:56:26.147872 140127291205440 submission.py:139] 70) loss = 0.139, grad_norm = 0.011
I0402 13:56:27.876451 139989769684736 logging_writer.py:48] [71] global_step=71, grad_norm=0.010034, loss=0.139911
I0402 13:56:27.879920 140127291205440 submission.py:139] 71) loss = 0.140, grad_norm = 0.010
I0402 13:56:29.712068 139989761292032 logging_writer.py:48] [72] global_step=72, grad_norm=0.010147, loss=0.137804
I0402 13:56:29.714927 140127291205440 submission.py:139] 72) loss = 0.138, grad_norm = 0.010
I0402 13:56:31.429284 139989769684736 logging_writer.py:48] [73] global_step=73, grad_norm=0.008312, loss=0.138763
I0402 13:56:31.432981 140127291205440 submission.py:139] 73) loss = 0.139, grad_norm = 0.008
I0402 13:56:33.140592 139989761292032 logging_writer.py:48] [74] global_step=74, grad_norm=0.010848, loss=0.138922
I0402 13:56:33.143785 140127291205440 submission.py:139] 74) loss = 0.139, grad_norm = 0.011
I0402 13:56:34.870330 139989769684736 logging_writer.py:48] [75] global_step=75, grad_norm=0.007796, loss=0.138659
I0402 13:56:34.873521 140127291205440 submission.py:139] 75) loss = 0.139, grad_norm = 0.008
I0402 13:56:36.590744 139989761292032 logging_writer.py:48] [76] global_step=76, grad_norm=0.010268, loss=0.138330
I0402 13:56:36.593835 140127291205440 submission.py:139] 76) loss = 0.138, grad_norm = 0.010
I0402 13:56:38.347703 139989769684736 logging_writer.py:48] [77] global_step=77, grad_norm=0.009540, loss=0.137450
I0402 13:56:38.351136 140127291205440 submission.py:139] 77) loss = 0.137, grad_norm = 0.010
I0402 13:56:40.098821 139989761292032 logging_writer.py:48] [78] global_step=78, grad_norm=0.009493, loss=0.138369
I0402 13:56:40.101952 140127291205440 submission.py:139] 78) loss = 0.138, grad_norm = 0.009
I0402 13:56:41.924116 139989769684736 logging_writer.py:48] [79] global_step=79, grad_norm=0.012802, loss=0.139685
I0402 13:56:41.927764 140127291205440 submission.py:139] 79) loss = 0.140, grad_norm = 0.013
I0402 13:56:43.672413 139989761292032 logging_writer.py:48] [80] global_step=80, grad_norm=0.007880, loss=0.139851
I0402 13:56:43.675883 140127291205440 submission.py:139] 80) loss = 0.140, grad_norm = 0.008
I0402 13:56:45.413638 139989769684736 logging_writer.py:48] [81] global_step=81, grad_norm=0.014248, loss=0.138626
I0402 13:56:45.416801 140127291205440 submission.py:139] 81) loss = 0.139, grad_norm = 0.014
I0402 13:56:47.203739 139989761292032 logging_writer.py:48] [82] global_step=82, grad_norm=0.008935, loss=0.137770
I0402 13:56:47.206988 140127291205440 submission.py:139] 82) loss = 0.138, grad_norm = 0.009
I0402 13:56:48.970539 139989769684736 logging_writer.py:48] [83] global_step=83, grad_norm=0.006365, loss=0.137990
I0402 13:56:48.973771 140127291205440 submission.py:139] 83) loss = 0.138, grad_norm = 0.006
I0402 13:56:50.723715 139989761292032 logging_writer.py:48] [84] global_step=84, grad_norm=0.010244, loss=0.138705
I0402 13:56:50.726872 140127291205440 submission.py:139] 84) loss = 0.139, grad_norm = 0.010
I0402 13:56:52.470992 139989769684736 logging_writer.py:48] [85] global_step=85, grad_norm=0.009311, loss=0.139427
I0402 13:56:52.474134 140127291205440 submission.py:139] 85) loss = 0.139, grad_norm = 0.009
I0402 13:56:54.242932 139989761292032 logging_writer.py:48] [86] global_step=86, grad_norm=0.012564, loss=0.137771
I0402 13:56:54.246196 140127291205440 submission.py:139] 86) loss = 0.138, grad_norm = 0.013
I0402 13:56:55.968474 139989769684736 logging_writer.py:48] [87] global_step=87, grad_norm=0.005588, loss=0.138420
I0402 13:56:55.972036 140127291205440 submission.py:139] 87) loss = 0.138, grad_norm = 0.006
I0402 13:56:57.708892 139989761292032 logging_writer.py:48] [88] global_step=88, grad_norm=0.006131, loss=0.137871
I0402 13:56:57.712749 140127291205440 submission.py:139] 88) loss = 0.138, grad_norm = 0.006
I0402 13:56:59.531183 139989769684736 logging_writer.py:48] [89] global_step=89, grad_norm=0.009571, loss=0.136330
I0402 13:56:59.534245 140127291205440 submission.py:139] 89) loss = 0.136, grad_norm = 0.010
I0402 13:57:01.400726 139989761292032 logging_writer.py:48] [90] global_step=90, grad_norm=0.013983, loss=0.138041
I0402 13:57:01.403832 140127291205440 submission.py:139] 90) loss = 0.138, grad_norm = 0.014
I0402 13:57:03.140696 139989769684736 logging_writer.py:48] [91] global_step=91, grad_norm=0.006041, loss=0.136628
I0402 13:57:03.143899 140127291205440 submission.py:139] 91) loss = 0.137, grad_norm = 0.006
I0402 13:57:04.876959 139989761292032 logging_writer.py:48] [92] global_step=92, grad_norm=0.007133, loss=0.135693
I0402 13:57:04.880175 140127291205440 submission.py:139] 92) loss = 0.136, grad_norm = 0.007
I0402 13:57:06.648967 139989769684736 logging_writer.py:48] [93] global_step=93, grad_norm=0.020920, loss=0.140235
I0402 13:57:06.651816 140127291205440 submission.py:139] 93) loss = 0.140, grad_norm = 0.021
I0402 13:57:08.371268 139989761292032 logging_writer.py:48] [94] global_step=94, grad_norm=0.007810, loss=0.140931
I0402 13:57:08.374622 140127291205440 submission.py:139] 94) loss = 0.141, grad_norm = 0.008
I0402 13:57:10.087419 139989769684736 logging_writer.py:48] [95] global_step=95, grad_norm=0.024037, loss=0.139302
I0402 13:57:10.090451 140127291205440 submission.py:139] 95) loss = 0.139, grad_norm = 0.024
I0402 13:57:11.830061 139989761292032 logging_writer.py:48] [96] global_step=96, grad_norm=0.011618, loss=0.138482
I0402 13:57:11.832951 140127291205440 submission.py:139] 96) loss = 0.138, grad_norm = 0.012
I0402 13:57:13.587813 139989769684736 logging_writer.py:48] [97] global_step=97, grad_norm=0.007543, loss=0.136119
I0402 13:57:13.590792 140127291205440 submission.py:139] 97) loss = 0.136, grad_norm = 0.008
I0402 13:57:15.321309 139989761292032 logging_writer.py:48] [98] global_step=98, grad_norm=0.024845, loss=0.138879
I0402 13:57:15.324373 140127291205440 submission.py:139] 98) loss = 0.139, grad_norm = 0.025
I0402 13:57:17.053607 139989769684736 logging_writer.py:48] [99] global_step=99, grad_norm=0.009932, loss=0.139500
I0402 13:57:17.056537 140127291205440 submission.py:139] 99) loss = 0.140, grad_norm = 0.010
I0402 13:57:18.832437 139989761292032 logging_writer.py:48] [100] global_step=100, grad_norm=0.009569, loss=0.139354
I0402 13:57:18.835493 140127291205440 submission.py:139] 100) loss = 0.139, grad_norm = 0.010
I0402 14:03:24.405251 140127291205440 submission_runner.py:373] Before eval at step 311: RAM USED (GB) 113.739952128
I0402 14:03:24.405487 140127291205440 spec.py:298] Evaluating on the training split.
I0402 14:13:26.929112 140127291205440 spec.py:310] Evaluating on the validation split.
I0402 14:18:42.351638 140127291205440 spec.py:326] Evaluating on the test split.
I0402 14:23:57.585867 140127291205440 submission_runner.py:382] Time since start: 1853.50s, 	Step: 311, 	{'train/loss': 0.13688403670624655, 'validation/loss': 0.13706276404494383, 'validation/num_examples': 89000000, 'test/loss': 0.1404693362124788, 'test/num_examples': 89274637}
I0402 14:23:57.586260 140127291205440 submission_runner.py:396] After eval at step 311: RAM USED (GB) 118.682415104
I0402 14:23:57.594350 139985869076224 logging_writer.py:48] [311] global_step=311, preemption_count=0, score=676.061770, test/loss=0.140469, test/num_examples=89274637, total_duration=1853.504735, train/loss=0.136884, validation/loss=0.137063, validation/num_examples=89000000
I0402 14:24:07.001899 140127291205440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_311.
I0402 14:24:07.002311 140127291205440 submission_runner.py:416] After logging and checkpointing eval at step 311: RAM USED (GB) 118.710300672
I0402 14:29:35.016622 139985860683520 logging_writer.py:48] [500] global_step=500, grad_norm=0.100119, loss=0.140154
I0402 14:29:35.021397 140127291205440 submission.py:139] 500) loss = 0.140, grad_norm = 0.100
I0402 14:33:07.318739 140127291205440 submission_runner.py:373] Before eval at step 625: RAM USED (GB) 121.11331328
I0402 14:33:07.318932 140127291205440 spec.py:298] Evaluating on the training split.
I0402 14:43:13.915134 140127291205440 spec.py:310] Evaluating on the validation split.
I0402 14:48:23.613825 140127291205440 spec.py:326] Evaluating on the test split.
I0402 14:53:32.630868 140127291205440 submission_runner.py:382] Time since start: 3636.42s, 	Step: 625, 	{'train/loss': 0.13491636027826642, 'validation/loss': 0.13588149438202246, 'validation/num_examples': 89000000, 'test/loss': 0.13922712449673696, 'test/num_examples': 89274637}
I0402 14:53:32.631266 140127291205440 submission_runner.py:396] After eval at step 625: RAM USED (GB) 123.299639296
I0402 14:53:32.641053 140024616036096 logging_writer.py:48] [625] global_step=625, preemption_count=0, score=1188.771396, test/loss=0.139227, test/num_examples=89274637, total_duration=3636.417094, train/loss=0.134916, validation/loss=0.135881, validation/num_examples=89000000
I0402 14:53:41.871596 140127291205440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_625.
I0402 14:53:41.872049 140127291205440 submission_runner.py:416] After logging and checkpointing eval at step 625: RAM USED (GB) 123.338788864
I0402 14:58:42.780898 140127291205440 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 124.723740672
I0402 14:58:42.781080 140127291205440 spec.py:298] Evaluating on the training split.
I0402 15:08:30.207390 140127291205440 spec.py:310] Evaluating on the validation split.
I0402 15:13:23.604038 140127291205440 spec.py:326] Evaluating on the test split.
I0402 15:18:13.159632 140127291205440 submission_runner.py:382] Time since start: 5171.88s, 	Step: 800, 	{'train/loss': 0.13497273583373573, 'validation/loss': 0.13476782022471911, 'validation/num_examples': 89000000, 'test/loss': 0.13831209417295082, 'test/num_examples': 89274637}
I0402 15:18:13.160050 140127291205440 submission_runner.py:396] After eval at step 800: RAM USED (GB) 126.570479616
I0402 15:18:13.169436 139983729952512 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1474.262975, test/loss=0.138312, test/num_examples=89274637, total_duration=5171.878782, train/loss=0.134973, validation/loss=0.134768, validation/num_examples=89000000
I0402 15:18:22.658272 140127291205440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_800.
I0402 15:18:22.658699 140127291205440 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 126.576062464
I0402 15:18:22.665886 139983721559808 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1474.262975
I0402 15:18:34.613236 140127291205440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_800.
I0402 15:20:13.328739 140127291205440 submission_runner.py:550] Tuning trial 1/1
I0402 15:20:13.329167 140127291205440 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 15:20:13.334682 140127291205440 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 0.339504778078148, 'validation/loss': 0.3439924269662921, 'validation/num_examples': 89000000, 'test/loss': 0.34199724609353493, 'test/num_examples': 89274637, 'score': 153.09204363822937, 'total_duration': 153.09393405914307, 'global_step': 1, 'preemption_count': 0}), (311, {'train/loss': 0.13688403670624655, 'validation/loss': 0.13706276404494383, 'validation/num_examples': 89000000, 'test/loss': 0.1404693362124788, 'test/num_examples': 89274637, 'score': 676.061770439148, 'total_duration': 1853.504734992981, 'global_step': 311, 'preemption_count': 0}), (625, {'train/loss': 0.13491636027826642, 'validation/loss': 0.13588149438202246, 'validation/num_examples': 89000000, 'test/loss': 0.13922712449673696, 'test/num_examples': 89274637, 'score': 1188.7713961601257, 'total_duration': 3636.417094230652, 'global_step': 625, 'preemption_count': 0}), (800, {'train/loss': 0.13497273583373573, 'validation/loss': 0.13476782022471911, 'validation/num_examples': 89000000, 'test/loss': 0.13831209417295082, 'test/num_examples': 89274637, 'score': 1474.2629754543304, 'total_duration': 5171.878782272339, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0402 15:20:13.335210 140127291205440 submission_runner.py:553] Timing: 1474.2629754543304
I0402 15:20:13.335283 140127291205440 submission_runner.py:554] ====================
I0402 15:20:13.335371 140127291205440 submission_runner.py:613] Final criteo1tb score: 1474.2629754543304
