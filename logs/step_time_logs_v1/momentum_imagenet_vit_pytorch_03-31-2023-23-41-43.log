WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0331 23:42:04.580835 139643111667520 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0331 23:42:04.580865 140110666688320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0331 23:42:04.580889 140698742298432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0331 23:42:04.581770 140499329345344 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0331 23:42:04.581835 140498347837248 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0331 23:42:04.581874 139791380428608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0331 23:42:04.581968 140404322244416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0331 23:42:04.591889 140637436720960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0331 23:42:04.592183 140637436720960 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.592275 140499329345344 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.592460 140498347837248 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.592507 139791380428608 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.592572 140404322244416 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.601717 140110666688320 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.601816 139643111667520 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:04.601841 140698742298432 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:42:06.613311 140637436720960 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/imagenet_vit_pytorch.
W0331 23:42:06.724998 140698742298432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.725005 140499329345344 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.725050 140110666688320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.726086 140498347837248 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.726446 140404322244416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.726687 139791380428608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.728058 139643111667520 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:42:06.728914 140637436720960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0331 23:42:06.732522 140637436720960 submission_runner.py:504] Using RNG seed 3447215154
I0331 23:42:06.733845 140637436720960 submission_runner.py:513] --- Tuning run 1/1 ---
I0331 23:42:06.733963 140637436720960 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1.
I0331 23:42:06.734196 140637436720960 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/hparams.json.
I0331 23:42:06.735108 140637436720960 submission_runner.py:230] Starting train once: RAM USED (GB) 5.783220224
I0331 23:42:06.735204 140637436720960 submission_runner.py:231] Initializing dataset.
I0331 23:42:11.140960 140637436720960 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.944241152
I0331 23:42:11.141165 140637436720960 submission_runner.py:240] Initializing model.
I0331 23:42:15.509431 140637436720960 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.04109824
I0331 23:42:15.509650 140637436720960 submission_runner.py:252] Initializing optimizer.
I0331 23:42:15.992845 140637436720960 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.045018112
I0331 23:42:15.993040 140637436720960 submission_runner.py:261] Initializing metrics bundle.
I0331 23:42:15.993087 140637436720960 submission_runner.py:275] Initializing checkpoint and logger.
I0331 23:42:16.637687 140637436720960 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0331 23:42:16.638667 140637436720960 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/flags_0.json.
I0331 23:42:16.683348 140637436720960 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 18.096156672
I0331 23:42:16.684605 140637436720960 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.096152576
I0331 23:42:16.684744 140637436720960 submission_runner.py:312] Starting training loop.
I0331 23:42:19.128312 140637436720960 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 23.726788608
I0331 23:42:22.923612 140608555439872 logging_writer.py:48] [0] global_step=0, grad_norm=0.298911, loss=6.907754
I0331 23:42:22.947344 140637436720960 submission.py:139] 0) loss = 6.908, grad_norm = 0.299
I0331 23:42:22.948208 140637436720960 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 31.929163776
I0331 23:42:22.948888 140637436720960 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 31.929163776
I0331 23:42:22.949033 140637436720960 spec.py:298] Evaluating on the training split.
I0331 23:43:10.509384 140637436720960 spec.py:310] Evaluating on the validation split.
I0331 23:43:52.770742 140637436720960 spec.py:326] Evaluating on the test split.
I0331 23:43:52.786710 140637436720960 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 23:43:52.793755 140637436720960 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0331 23:43:52.877050 140637436720960 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 23:44:05.780734 140637436720960 submission_runner.py:380] Time since start: 6.26s, 	Step: 1, 	{'train/accuracy': 0.00078125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0331 23:44:05.781190 140637436720960 submission_runner.py:390] After eval at step 1: RAM USED (GB) 93.043990528
I0331 23:44:05.790464 140603505506048 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.262601, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=6.264778, train/accuracy=0.000781, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0331 23:44:06.059600 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0331 23:44:06.060247 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 93.042724864
I0331 23:44:06.063544 140637436720960 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 93.042978816
I0331 23:44:06.071309 140637436720960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.071144 140499329345344 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.071928 139643111667520 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.071969 140110666688320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.072001 140404322244416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.072004 139791380428608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.072283 140498347837248 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.072927 140698742298432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:44:06.664723 140603497113344 logging_writer.py:48] [1] global_step=1, grad_norm=0.300681, loss=6.907755
I0331 23:44:06.667906 140637436720960 submission.py:139] 1) loss = 6.908, grad_norm = 0.301
I0331 23:44:06.668805 140637436720960 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 93.102665728
I0331 23:44:07.066258 140603505506048 logging_writer.py:48] [2] global_step=2, grad_norm=0.304434, loss=6.907755
I0331 23:44:07.070462 140637436720960 submission.py:139] 2) loss = 6.908, grad_norm = 0.304
I0331 23:44:07.460161 140603497113344 logging_writer.py:48] [3] global_step=3, grad_norm=0.300007, loss=6.907754
I0331 23:44:07.463496 140637436720960 submission.py:139] 3) loss = 6.908, grad_norm = 0.300
I0331 23:44:07.857224 140603505506048 logging_writer.py:48] [4] global_step=4, grad_norm=0.301049, loss=6.907754
I0331 23:44:07.861138 140637436720960 submission.py:139] 4) loss = 6.908, grad_norm = 0.301
I0331 23:44:08.261873 140603497113344 logging_writer.py:48] [5] global_step=5, grad_norm=0.299196, loss=6.907753
I0331 23:44:08.266845 140637436720960 submission.py:139] 5) loss = 6.908, grad_norm = 0.299
I0331 23:44:08.658483 140603505506048 logging_writer.py:48] [6] global_step=6, grad_norm=0.300561, loss=6.907751
I0331 23:44:08.662967 140637436720960 submission.py:139] 6) loss = 6.908, grad_norm = 0.301
I0331 23:44:09.055690 140603497113344 logging_writer.py:48] [7] global_step=7, grad_norm=0.304415, loss=6.907748
I0331 23:44:09.060353 140637436720960 submission.py:139] 7) loss = 6.908, grad_norm = 0.304
I0331 23:44:09.455530 140603505506048 logging_writer.py:48] [8] global_step=8, grad_norm=0.307760, loss=6.907745
I0331 23:44:09.465700 140637436720960 submission.py:139] 8) loss = 6.908, grad_norm = 0.308
I0331 23:44:09.854892 140603497113344 logging_writer.py:48] [9] global_step=9, grad_norm=0.304568, loss=6.907748
I0331 23:44:09.858293 140637436720960 submission.py:139] 9) loss = 6.908, grad_norm = 0.305
I0331 23:44:10.251553 140603505506048 logging_writer.py:48] [10] global_step=10, grad_norm=0.304945, loss=6.907734
I0331 23:44:10.256354 140637436720960 submission.py:139] 10) loss = 6.908, grad_norm = 0.305
I0331 23:44:10.648158 140603497113344 logging_writer.py:48] [11] global_step=11, grad_norm=0.305655, loss=6.907738
I0331 23:44:10.652090 140637436720960 submission.py:139] 11) loss = 6.908, grad_norm = 0.306
I0331 23:44:11.042349 140603505506048 logging_writer.py:48] [12] global_step=12, grad_norm=0.304747, loss=6.907740
I0331 23:44:11.045862 140637436720960 submission.py:139] 12) loss = 6.908, grad_norm = 0.305
I0331 23:44:11.438327 140603497113344 logging_writer.py:48] [13] global_step=13, grad_norm=0.301822, loss=6.907720
I0331 23:44:11.441783 140637436720960 submission.py:139] 13) loss = 6.908, grad_norm = 0.302
I0331 23:44:11.832117 140603505506048 logging_writer.py:48] [14] global_step=14, grad_norm=0.312473, loss=6.907744
I0331 23:44:11.839373 140637436720960 submission.py:139] 14) loss = 6.908, grad_norm = 0.312
I0331 23:44:12.237650 140603497113344 logging_writer.py:48] [15] global_step=15, grad_norm=0.302858, loss=6.907710
I0331 23:44:12.241648 140637436720960 submission.py:139] 15) loss = 6.908, grad_norm = 0.303
I0331 23:44:12.634265 140603505506048 logging_writer.py:48] [16] global_step=16, grad_norm=0.300069, loss=6.907689
I0331 23:44:12.638319 140637436720960 submission.py:139] 16) loss = 6.908, grad_norm = 0.300
I0331 23:44:13.029992 140603497113344 logging_writer.py:48] [17] global_step=17, grad_norm=0.304297, loss=6.907674
I0331 23:44:13.034046 140637436720960 submission.py:139] 17) loss = 6.908, grad_norm = 0.304
I0331 23:44:13.427026 140603505506048 logging_writer.py:48] [18] global_step=18, grad_norm=0.307527, loss=6.907692
I0331 23:44:13.430857 140637436720960 submission.py:139] 18) loss = 6.908, grad_norm = 0.308
I0331 23:44:13.823004 140603497113344 logging_writer.py:48] [19] global_step=19, grad_norm=0.305362, loss=6.907700
I0331 23:44:13.827067 140637436720960 submission.py:139] 19) loss = 6.908, grad_norm = 0.305
I0331 23:44:14.220160 140603505506048 logging_writer.py:48] [20] global_step=20, grad_norm=0.302537, loss=6.907613
I0331 23:44:14.224175 140637436720960 submission.py:139] 20) loss = 6.908, grad_norm = 0.303
I0331 23:44:14.615601 140603497113344 logging_writer.py:48] [21] global_step=21, grad_norm=0.299774, loss=6.907604
I0331 23:44:14.620102 140637436720960 submission.py:139] 21) loss = 6.908, grad_norm = 0.300
I0331 23:44:15.017817 140603505506048 logging_writer.py:48] [22] global_step=22, grad_norm=0.307380, loss=6.907619
I0331 23:44:15.021591 140637436720960 submission.py:139] 22) loss = 6.908, grad_norm = 0.307
I0331 23:44:15.417635 140603497113344 logging_writer.py:48] [23] global_step=23, grad_norm=0.292378, loss=6.907659
I0331 23:44:15.421146 140637436720960 submission.py:139] 23) loss = 6.908, grad_norm = 0.292
I0331 23:44:15.811910 140603505506048 logging_writer.py:48] [24] global_step=24, grad_norm=0.305942, loss=6.907527
I0331 23:44:15.815885 140637436720960 submission.py:139] 24) loss = 6.908, grad_norm = 0.306
I0331 23:44:16.205991 140603497113344 logging_writer.py:48] [25] global_step=25, grad_norm=0.298062, loss=6.907573
I0331 23:44:16.209960 140637436720960 submission.py:139] 25) loss = 6.908, grad_norm = 0.298
I0331 23:44:16.600459 140603505506048 logging_writer.py:48] [26] global_step=26, grad_norm=0.302008, loss=6.907577
I0331 23:44:16.610118 140637436720960 submission.py:139] 26) loss = 6.908, grad_norm = 0.302
I0331 23:44:17.001924 140603497113344 logging_writer.py:48] [27] global_step=27, grad_norm=0.302428, loss=6.907493
I0331 23:44:17.006651 140637436720960 submission.py:139] 27) loss = 6.907, grad_norm = 0.302
I0331 23:44:17.395985 140603505506048 logging_writer.py:48] [28] global_step=28, grad_norm=0.306921, loss=6.907550
I0331 23:44:17.399515 140637436720960 submission.py:139] 28) loss = 6.908, grad_norm = 0.307
I0331 23:44:17.791862 140603497113344 logging_writer.py:48] [29] global_step=29, grad_norm=0.305518, loss=6.907583
I0331 23:44:17.796025 140637436720960 submission.py:139] 29) loss = 6.908, grad_norm = 0.306
I0331 23:44:18.187464 140603505506048 logging_writer.py:48] [30] global_step=30, grad_norm=0.304605, loss=6.907458
I0331 23:44:18.191860 140637436720960 submission.py:139] 30) loss = 6.907, grad_norm = 0.305
I0331 23:44:18.582384 140603497113344 logging_writer.py:48] [31] global_step=31, grad_norm=0.300343, loss=6.907491
I0331 23:44:18.587851 140637436720960 submission.py:139] 31) loss = 6.907, grad_norm = 0.300
I0331 23:44:18.980381 140603505506048 logging_writer.py:48] [32] global_step=32, grad_norm=0.296535, loss=6.907401
I0331 23:44:18.984190 140637436720960 submission.py:139] 32) loss = 6.907, grad_norm = 0.297
I0331 23:44:19.376034 140603497113344 logging_writer.py:48] [33] global_step=33, grad_norm=0.304710, loss=6.907328
I0331 23:44:19.380911 140637436720960 submission.py:139] 33) loss = 6.907, grad_norm = 0.305
I0331 23:44:19.772622 140603505506048 logging_writer.py:48] [34] global_step=34, grad_norm=0.302764, loss=6.907440
I0331 23:44:19.777902 140637436720960 submission.py:139] 34) loss = 6.907, grad_norm = 0.303
I0331 23:44:20.175011 140603497113344 logging_writer.py:48] [35] global_step=35, grad_norm=0.298290, loss=6.907356
I0331 23:44:20.178523 140637436720960 submission.py:139] 35) loss = 6.907, grad_norm = 0.298
I0331 23:44:20.580870 140603505506048 logging_writer.py:48] [36] global_step=36, grad_norm=0.308348, loss=6.907302
I0331 23:44:20.585238 140637436720960 submission.py:139] 36) loss = 6.907, grad_norm = 0.308
I0331 23:44:20.977743 140603497113344 logging_writer.py:48] [37] global_step=37, grad_norm=0.300674, loss=6.907362
I0331 23:44:20.983046 140637436720960 submission.py:139] 37) loss = 6.907, grad_norm = 0.301
I0331 23:44:21.377605 140603505506048 logging_writer.py:48] [38] global_step=38, grad_norm=0.295839, loss=6.907358
I0331 23:44:21.386397 140637436720960 submission.py:139] 38) loss = 6.907, grad_norm = 0.296
I0331 23:44:21.778660 140603497113344 logging_writer.py:48] [39] global_step=39, grad_norm=0.300447, loss=6.907259
I0331 23:44:21.783388 140637436720960 submission.py:139] 39) loss = 6.907, grad_norm = 0.300
I0331 23:44:22.175573 140603505506048 logging_writer.py:48] [40] global_step=40, grad_norm=0.303845, loss=6.907127
I0331 23:44:22.179901 140637436720960 submission.py:139] 40) loss = 6.907, grad_norm = 0.304
I0331 23:44:22.576704 140603497113344 logging_writer.py:48] [41] global_step=41, grad_norm=0.303452, loss=6.907173
I0331 23:44:22.580477 140637436720960 submission.py:139] 41) loss = 6.907, grad_norm = 0.303
I0331 23:44:22.976069 140603505506048 logging_writer.py:48] [42] global_step=42, grad_norm=0.299953, loss=6.907453
I0331 23:44:22.981242 140637436720960 submission.py:139] 42) loss = 6.907, grad_norm = 0.300
I0331 23:44:23.372957 140603497113344 logging_writer.py:48] [43] global_step=43, grad_norm=0.305073, loss=6.907114
I0331 23:44:23.376917 140637436720960 submission.py:139] 43) loss = 6.907, grad_norm = 0.305
I0331 23:44:23.767038 140603505506048 logging_writer.py:48] [44] global_step=44, grad_norm=0.300259, loss=6.907132
I0331 23:44:23.771226 140637436720960 submission.py:139] 44) loss = 6.907, grad_norm = 0.300
I0331 23:44:24.161982 140603497113344 logging_writer.py:48] [45] global_step=45, grad_norm=0.294650, loss=6.906944
I0331 23:44:24.166443 140637436720960 submission.py:139] 45) loss = 6.907, grad_norm = 0.295
I0331 23:44:24.565776 140603505506048 logging_writer.py:48] [46] global_step=46, grad_norm=0.309255, loss=6.906927
I0331 23:44:24.572976 140637436720960 submission.py:139] 46) loss = 6.907, grad_norm = 0.309
I0331 23:44:24.962666 140603497113344 logging_writer.py:48] [47] global_step=47, grad_norm=0.298500, loss=6.906925
I0331 23:44:24.966668 140637436720960 submission.py:139] 47) loss = 6.907, grad_norm = 0.298
I0331 23:44:25.358678 140603505506048 logging_writer.py:48] [48] global_step=48, grad_norm=0.303180, loss=6.906817
I0331 23:44:25.363024 140637436720960 submission.py:139] 48) loss = 6.907, grad_norm = 0.303
I0331 23:44:25.756598 140603497113344 logging_writer.py:48] [49] global_step=49, grad_norm=0.309684, loss=6.906756
I0331 23:44:25.760863 140637436720960 submission.py:139] 49) loss = 6.907, grad_norm = 0.310
I0331 23:44:26.150367 140603505506048 logging_writer.py:48] [50] global_step=50, grad_norm=0.288874, loss=6.906886
I0331 23:44:26.154072 140637436720960 submission.py:139] 50) loss = 6.907, grad_norm = 0.289
I0331 23:44:26.545444 140603497113344 logging_writer.py:48] [51] global_step=51, grad_norm=0.300371, loss=6.906929
I0331 23:44:26.550351 140637436720960 submission.py:139] 51) loss = 6.907, grad_norm = 0.300
I0331 23:44:26.941797 140603505506048 logging_writer.py:48] [52] global_step=52, grad_norm=0.292733, loss=6.907031
I0331 23:44:26.946058 140637436720960 submission.py:139] 52) loss = 6.907, grad_norm = 0.293
I0331 23:44:27.344413 140603497113344 logging_writer.py:48] [53] global_step=53, grad_norm=0.297765, loss=6.906158
I0331 23:44:27.348126 140637436720960 submission.py:139] 53) loss = 6.906, grad_norm = 0.298
I0331 23:44:27.741148 140603505506048 logging_writer.py:48] [54] global_step=54, grad_norm=0.304982, loss=6.907089
I0331 23:44:27.745655 140637436720960 submission.py:139] 54) loss = 6.907, grad_norm = 0.305
I0331 23:44:28.136504 140603497113344 logging_writer.py:48] [55] global_step=55, grad_norm=0.302591, loss=6.907009
I0331 23:44:28.140035 140637436720960 submission.py:139] 55) loss = 6.907, grad_norm = 0.303
I0331 23:44:28.530886 140603505506048 logging_writer.py:48] [56] global_step=56, grad_norm=0.302961, loss=6.906433
I0331 23:44:28.535381 140637436720960 submission.py:139] 56) loss = 6.906, grad_norm = 0.303
I0331 23:44:28.929151 140603497113344 logging_writer.py:48] [57] global_step=57, grad_norm=0.304771, loss=6.907229
I0331 23:44:28.933327 140637436720960 submission.py:139] 57) loss = 6.907, grad_norm = 0.305
I0331 23:44:29.324171 140603505506048 logging_writer.py:48] [58] global_step=58, grad_norm=0.308587, loss=6.906365
I0331 23:44:29.327929 140637436720960 submission.py:139] 58) loss = 6.906, grad_norm = 0.309
I0331 23:44:29.719005 140603497113344 logging_writer.py:48] [59] global_step=59, grad_norm=0.304037, loss=6.906753
I0331 23:44:29.726495 140637436720960 submission.py:139] 59) loss = 6.907, grad_norm = 0.304
I0331 23:44:30.120125 140603505506048 logging_writer.py:48] [60] global_step=60, grad_norm=0.302644, loss=6.906128
I0331 23:44:30.125377 140637436720960 submission.py:139] 60) loss = 6.906, grad_norm = 0.303
I0331 23:44:30.520307 140603497113344 logging_writer.py:48] [61] global_step=61, grad_norm=0.300721, loss=6.905873
I0331 23:44:30.524556 140637436720960 submission.py:139] 61) loss = 6.906, grad_norm = 0.301
I0331 23:44:30.921476 140603505506048 logging_writer.py:48] [62] global_step=62, grad_norm=0.299910, loss=6.906585
I0331 23:44:30.925454 140637436720960 submission.py:139] 62) loss = 6.907, grad_norm = 0.300
I0331 23:44:31.319842 140603497113344 logging_writer.py:48] [63] global_step=63, grad_norm=0.306255, loss=6.905785
I0331 23:44:31.323989 140637436720960 submission.py:139] 63) loss = 6.906, grad_norm = 0.306
I0331 23:44:31.716640 140603505506048 logging_writer.py:48] [64] global_step=64, grad_norm=0.305791, loss=6.905879
I0331 23:44:31.721368 140637436720960 submission.py:139] 64) loss = 6.906, grad_norm = 0.306
I0331 23:44:32.113781 140603497113344 logging_writer.py:48] [65] global_step=65, grad_norm=0.299688, loss=6.906082
I0331 23:44:32.117861 140637436720960 submission.py:139] 65) loss = 6.906, grad_norm = 0.300
I0331 23:44:32.510268 140603505506048 logging_writer.py:48] [66] global_step=66, grad_norm=0.304849, loss=6.905986
I0331 23:44:32.515679 140637436720960 submission.py:139] 66) loss = 6.906, grad_norm = 0.305
I0331 23:44:32.911048 140603497113344 logging_writer.py:48] [67] global_step=67, grad_norm=0.304984, loss=6.906917
I0331 23:44:32.914800 140637436720960 submission.py:139] 67) loss = 6.907, grad_norm = 0.305
I0331 23:44:33.308829 140603505506048 logging_writer.py:48] [68] global_step=68, grad_norm=0.300482, loss=6.906067
I0331 23:44:33.312929 140637436720960 submission.py:139] 68) loss = 6.906, grad_norm = 0.300
I0331 23:44:33.704634 140603497113344 logging_writer.py:48] [69] global_step=69, grad_norm=0.300060, loss=6.905776
I0331 23:44:33.709034 140637436720960 submission.py:139] 69) loss = 6.906, grad_norm = 0.300
I0331 23:44:34.104049 140603505506048 logging_writer.py:48] [70] global_step=70, grad_norm=0.303568, loss=6.905300
I0331 23:44:34.107896 140637436720960 submission.py:139] 70) loss = 6.905, grad_norm = 0.304
I0331 23:44:34.500621 140603497113344 logging_writer.py:48] [71] global_step=71, grad_norm=0.305245, loss=6.905970
I0331 23:44:34.504678 140637436720960 submission.py:139] 71) loss = 6.906, grad_norm = 0.305
I0331 23:44:34.896074 140603505506048 logging_writer.py:48] [72] global_step=72, grad_norm=0.305065, loss=6.904573
I0331 23:44:34.900199 140637436720960 submission.py:139] 72) loss = 6.905, grad_norm = 0.305
I0331 23:44:35.294802 140603497113344 logging_writer.py:48] [73] global_step=73, grad_norm=0.300165, loss=6.905869
I0331 23:44:35.298284 140637436720960 submission.py:139] 73) loss = 6.906, grad_norm = 0.300
I0331 23:44:35.692023 140603505506048 logging_writer.py:48] [74] global_step=74, grad_norm=0.295274, loss=6.905302
I0331 23:44:35.697469 140637436720960 submission.py:139] 74) loss = 6.905, grad_norm = 0.295
I0331 23:44:36.091865 140603497113344 logging_writer.py:48] [75] global_step=75, grad_norm=0.291378, loss=6.905592
I0331 23:44:36.095929 140637436720960 submission.py:139] 75) loss = 6.906, grad_norm = 0.291
I0331 23:44:36.488262 140603505506048 logging_writer.py:48] [76] global_step=76, grad_norm=0.297664, loss=6.905651
I0331 23:44:36.491972 140637436720960 submission.py:139] 76) loss = 6.906, grad_norm = 0.298
I0331 23:44:36.884295 140603497113344 logging_writer.py:48] [77] global_step=77, grad_norm=0.303968, loss=6.905182
I0331 23:44:36.887730 140637436720960 submission.py:139] 77) loss = 6.905, grad_norm = 0.304
I0331 23:44:37.282228 140603505506048 logging_writer.py:48] [78] global_step=78, grad_norm=0.298509, loss=6.905263
I0331 23:44:37.287507 140637436720960 submission.py:139] 78) loss = 6.905, grad_norm = 0.299
I0331 23:44:37.686793 140603497113344 logging_writer.py:48] [79] global_step=79, grad_norm=0.305287, loss=6.906116
I0331 23:44:37.693109 140637436720960 submission.py:139] 79) loss = 6.906, grad_norm = 0.305
I0331 23:44:38.087850 140603505506048 logging_writer.py:48] [80] global_step=80, grad_norm=0.299317, loss=6.904599
I0331 23:44:38.091334 140637436720960 submission.py:139] 80) loss = 6.905, grad_norm = 0.299
I0331 23:44:38.493816 140603497113344 logging_writer.py:48] [81] global_step=81, grad_norm=0.298637, loss=6.905509
I0331 23:44:38.498322 140637436720960 submission.py:139] 81) loss = 6.906, grad_norm = 0.299
I0331 23:44:38.890051 140603505506048 logging_writer.py:48] [82] global_step=82, grad_norm=0.306743, loss=6.904504
I0331 23:44:38.894047 140637436720960 submission.py:139] 82) loss = 6.905, grad_norm = 0.307
I0331 23:44:39.290139 140603497113344 logging_writer.py:48] [83] global_step=83, grad_norm=0.299113, loss=6.904447
I0331 23:44:39.293807 140637436720960 submission.py:139] 83) loss = 6.904, grad_norm = 0.299
I0331 23:44:39.685487 140603505506048 logging_writer.py:48] [84] global_step=84, grad_norm=0.299308, loss=6.905375
I0331 23:44:39.690985 140637436720960 submission.py:139] 84) loss = 6.905, grad_norm = 0.299
I0331 23:44:40.083572 140603497113344 logging_writer.py:48] [85] global_step=85, grad_norm=0.297021, loss=6.905070
I0331 23:44:40.087143 140637436720960 submission.py:139] 85) loss = 6.905, grad_norm = 0.297
I0331 23:44:40.481038 140603505506048 logging_writer.py:48] [86] global_step=86, grad_norm=0.307910, loss=6.904459
I0331 23:44:40.485046 140637436720960 submission.py:139] 86) loss = 6.904, grad_norm = 0.308
I0331 23:44:40.879166 140603497113344 logging_writer.py:48] [87] global_step=87, grad_norm=0.302579, loss=6.904092
I0331 23:44:40.882664 140637436720960 submission.py:139] 87) loss = 6.904, grad_norm = 0.303
I0331 23:44:41.279527 140603505506048 logging_writer.py:48] [88] global_step=88, grad_norm=0.300277, loss=6.904667
I0331 23:44:41.283075 140637436720960 submission.py:139] 88) loss = 6.905, grad_norm = 0.300
I0331 23:44:41.674786 140603497113344 logging_writer.py:48] [89] global_step=89, grad_norm=0.306536, loss=6.902697
I0331 23:44:41.678503 140637436720960 submission.py:139] 89) loss = 6.903, grad_norm = 0.307
I0331 23:44:42.071162 140603505506048 logging_writer.py:48] [90] global_step=90, grad_norm=0.302894, loss=6.905364
I0331 23:44:42.074760 140637436720960 submission.py:139] 90) loss = 6.905, grad_norm = 0.303
I0331 23:44:42.467630 140603497113344 logging_writer.py:48] [91] global_step=91, grad_norm=0.296345, loss=6.903296
I0331 23:44:42.471430 140637436720960 submission.py:139] 91) loss = 6.903, grad_norm = 0.296
I0331 23:44:42.865651 140603505506048 logging_writer.py:48] [92] global_step=92, grad_norm=0.304106, loss=6.903461
I0331 23:44:42.871237 140637436720960 submission.py:139] 92) loss = 6.903, grad_norm = 0.304
I0331 23:44:43.271749 140603497113344 logging_writer.py:48] [93] global_step=93, grad_norm=0.301593, loss=6.904248
I0331 23:44:43.276131 140637436720960 submission.py:139] 93) loss = 6.904, grad_norm = 0.302
I0331 23:44:43.669575 140603505506048 logging_writer.py:48] [94] global_step=94, grad_norm=0.295654, loss=6.903812
I0331 23:44:43.673133 140637436720960 submission.py:139] 94) loss = 6.904, grad_norm = 0.296
I0331 23:44:44.067803 140603497113344 logging_writer.py:48] [95] global_step=95, grad_norm=0.297282, loss=6.904309
I0331 23:44:44.071342 140637436720960 submission.py:139] 95) loss = 6.904, grad_norm = 0.297
I0331 23:44:44.463269 140603505506048 logging_writer.py:48] [96] global_step=96, grad_norm=0.301151, loss=6.904391
I0331 23:44:44.467683 140637436720960 submission.py:139] 96) loss = 6.904, grad_norm = 0.301
I0331 23:44:44.860400 140603497113344 logging_writer.py:48] [97] global_step=97, grad_norm=0.299996, loss=6.901744
I0331 23:44:44.864374 140637436720960 submission.py:139] 97) loss = 6.902, grad_norm = 0.300
I0331 23:44:45.258923 140603505506048 logging_writer.py:48] [98] global_step=98, grad_norm=0.301412, loss=6.904442
I0331 23:44:45.262491 140637436720960 submission.py:139] 98) loss = 6.904, grad_norm = 0.301
I0331 23:44:45.660830 140603497113344 logging_writer.py:48] [99] global_step=99, grad_norm=0.302736, loss=6.903283
I0331 23:44:45.665084 140637436720960 submission.py:139] 99) loss = 6.903, grad_norm = 0.303
I0331 23:44:46.058479 140603505506048 logging_writer.py:48] [100] global_step=100, grad_norm=0.302846, loss=6.904555
I0331 23:44:46.062536 140637436720960 submission.py:139] 100) loss = 6.905, grad_norm = 0.303
I0331 23:47:20.859768 140603497113344 logging_writer.py:48] [500] global_step=500, grad_norm=0.777980, loss=6.766001
I0331 23:47:20.864260 140637436720960 submission.py:139] 500) loss = 6.766, grad_norm = 0.778
I0331 23:50:34.336621 140603505506048 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.889020, loss=6.599929
I0331 23:50:34.341565 140637436720960 submission.py:139] 1000) loss = 6.600, grad_norm = 0.889
I0331 23:51:06.446004 140637436720960 submission_runner.py:371] Before eval at step 1084: RAM USED (GB) 99.11834624
I0331 23:51:06.446255 140637436720960 spec.py:298] Evaluating on the training split.
I0331 23:51:49.814821 140637436720960 spec.py:310] Evaluating on the validation split.
I0331 23:52:32.375975 140637436720960 spec.py:326] Evaluating on the test split.
I0331 23:52:33.831591 140637436720960 submission_runner.py:380] Time since start: 529.76s, 	Step: 1084, 	{'train/accuracy': 0.050703125, 'train/loss': 5.894288330078125, 'validation/accuracy': 0.04602, 'validation/loss': 5.9313075, 'validation/num_examples': 50000, 'test/accuracy': 0.0353, 'test/loss': 6.05332265625, 'test/num_examples': 10000}
I0331 23:52:33.831976 140637436720960 submission_runner.py:390] After eval at step 1084: RAM USED (GB) 98.717294592
I0331 23:52:33.839636 140593397229312 logging_writer.py:48] [1084] global_step=1084, preemption_count=0, score=420.535985, test/accuracy=0.035300, test/loss=6.053323, test/num_examples=10000, total_duration=529.758028, train/accuracy=0.050703, train/loss=5.894288, validation/accuracy=0.046020, validation/loss=5.931307, validation/num_examples=50000
I0331 23:52:34.118547 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_1084.
I0331 23:52:34.119188 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 1084: RAM USED (GB) 98.716225536
I0331 23:55:16.973660 140593405622016 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.806592, loss=6.465676
I0331 23:55:16.979586 140637436720960 submission.py:139] 1500) loss = 6.466, grad_norm = 0.807
I0331 23:58:30.630626 140593397229312 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.931872, loss=6.212379
I0331 23:58:30.640290 140637436720960 submission.py:139] 2000) loss = 6.212, grad_norm = 0.932
I0331 23:59:34.460397 140637436720960 submission_runner.py:371] Before eval at step 2166: RAM USED (GB) 100.107956224
I0331 23:59:34.460616 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:00:17.222304 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:01:00.911760 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:01:02.329526 140637436720960 submission_runner.py:380] Time since start: 1037.77s, 	Step: 2166, 	{'train/accuracy': 0.08154296875, 'train/loss': 5.427593994140625, 'validation/accuracy': 0.07382, 'validation/loss': 5.484860625, 'validation/num_examples': 50000, 'test/accuracy': 0.0574, 'test/loss': 5.67441796875, 'test/num_examples': 10000}
I0401 00:01:02.329886 140637436720960 submission_runner.py:390] After eval at step 2166: RAM USED (GB) 100.196999168
I0401 00:01:02.338207 140593405622016 logging_writer.py:48] [2166] global_step=2166, preemption_count=0, score=834.392452, test/accuracy=0.057400, test/loss=5.674418, test/num_examples=10000, total_duration=1037.771685, train/accuracy=0.081543, train/loss=5.427594, validation/accuracy=0.073820, validation/loss=5.484861, validation/num_examples=50000
I0401 00:01:02.618681 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_2166.
I0401 00:01:02.619353 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 2166: RAM USED (GB) 100.19586048
I0401 00:03:12.317416 140593397229312 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.709895, loss=6.317430
I0401 00:03:12.321501 140637436720960 submission.py:139] 2500) loss = 6.317, grad_norm = 0.710
I0401 00:06:27.819647 140593405622016 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.893232, loss=6.127727
I0401 00:06:27.825398 140637436720960 submission.py:139] 3000) loss = 6.128, grad_norm = 0.893
I0401 00:08:02.685919 140637436720960 submission_runner.py:371] Before eval at step 3246: RAM USED (GB) 100.758798336
I0401 00:08:02.686176 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:08:45.516235 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:09:29.367069 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:09:30.782073 140637436720960 submission_runner.py:380] Time since start: 1546.00s, 	Step: 3246, 	{'train/accuracy': 0.11205078125, 'train/loss': 5.056005249023437, 'validation/accuracy': 0.10406, 'validation/loss': 5.1287559375, 'validation/num_examples': 50000, 'test/accuracy': 0.0787, 'test/loss': 5.3931375, 'test/num_examples': 10000}
I0401 00:09:30.782432 140637436720960 submission_runner.py:390] After eval at step 3246: RAM USED (GB) 100.770349056
I0401 00:09:30.791607 140593397229312 logging_writer.py:48] [3246] global_step=3246, preemption_count=0, score=1248.010416, test/accuracy=0.078700, test/loss=5.393137, test/num_examples=10000, total_duration=1545.998449, train/accuracy=0.112051, train/loss=5.056005, validation/accuracy=0.104060, validation/loss=5.128756, validation/num_examples=50000
I0401 00:09:31.067559 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_3246.
I0401 00:09:31.068242 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 3246: RAM USED (GB) 100.771319808
I0401 00:11:09.578087 140593405622016 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.711976, loss=6.152149
I0401 00:11:09.581844 140637436720960 submission.py:139] 3500) loss = 6.152, grad_norm = 0.712
I0401 00:14:25.360966 140593397229312 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.594259, loss=6.390386
I0401 00:14:25.369403 140637436720960 submission.py:139] 4000) loss = 6.390, grad_norm = 0.594
I0401 00:16:31.202634 140637436720960 submission_runner.py:371] Before eval at step 4326: RAM USED (GB) 100.858851328
I0401 00:16:31.202908 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:17:15.349750 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:17:59.922286 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:18:01.335583 140637436720960 submission_runner.py:380] Time since start: 2054.51s, 	Step: 4326, 	{'train/accuracy': 0.144765625, 'train/loss': 4.7445916748046875, 'validation/accuracy': 0.13424, 'validation/loss': 4.829335625, 'validation/num_examples': 50000, 'test/accuracy': 0.1002, 'test/loss': 5.13111015625, 'test/num_examples': 10000}
I0401 00:18:01.335983 140637436720960 submission_runner.py:390] After eval at step 4326: RAM USED (GB) 100.79516672
I0401 00:18:01.344531 140593405622016 logging_writer.py:48] [4326] global_step=4326, preemption_count=0, score=1661.710253, test/accuracy=0.100200, test/loss=5.131110, test/num_examples=10000, total_duration=2054.514221, train/accuracy=0.144766, train/loss=4.744592, validation/accuracy=0.134240, validation/loss=4.829336, validation/num_examples=50000
I0401 00:18:01.630068 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_4326.
I0401 00:18:01.630714 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 4326: RAM USED (GB) 100.818731008
I0401 00:19:09.420958 140593397229312 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.673590, loss=6.043979
I0401 00:19:09.424748 140637436720960 submission.py:139] 4500) loss = 6.044, grad_norm = 0.674
I0401 00:22:23.755808 140593405622016 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.662502, loss=5.934863
I0401 00:22:23.760456 140637436720960 submission.py:139] 5000) loss = 5.935, grad_norm = 0.663
I0401 00:25:01.789232 140637436720960 submission_runner.py:371] Before eval at step 5404: RAM USED (GB) 100.753846272
I0401 00:25:01.789450 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:25:45.084445 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:26:29.887930 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:26:31.304506 140637436720960 submission_runner.py:380] Time since start: 2565.10s, 	Step: 5404, 	{'train/accuracy': 0.1758984375, 'train/loss': 4.447415161132812, 'validation/accuracy': 0.16152, 'validation/loss': 4.543638125, 'validation/num_examples': 50000, 'test/accuracy': 0.1221, 'test/loss': 4.896507421875, 'test/num_examples': 10000}
I0401 00:26:31.304831 140637436720960 submission_runner.py:390] After eval at step 5404: RAM USED (GB) 100.9801216
I0401 00:26:31.313245 140593397229312 logging_writer.py:48] [5404] global_step=5404, preemption_count=0, score=2075.427943, test/accuracy=0.122100, test/loss=4.896507, test/num_examples=10000, total_duration=2565.100597, train/accuracy=0.175898, train/loss=4.447415, validation/accuracy=0.161520, validation/loss=4.543638, validation/num_examples=50000
I0401 00:26:31.591901 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_5404.
I0401 00:26:31.592556 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 5404: RAM USED (GB) 100.978999296
I0401 00:27:09.021184 140593405622016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.583133, loss=5.843340
I0401 00:27:09.026237 140637436720960 submission.py:139] 5500) loss = 5.843, grad_norm = 0.583
I0401 00:30:22.630687 140593397229312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.641710, loss=5.816067
I0401 00:30:22.636017 140637436720960 submission.py:139] 6000) loss = 5.816, grad_norm = 0.642
I0401 00:33:31.722431 140637436720960 submission_runner.py:371] Before eval at step 6482: RAM USED (GB) 100.926189568
I0401 00:33:31.722714 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:34:14.615058 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:34:58.239693 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:34:59.655293 140637436720960 submission_runner.py:380] Time since start: 3075.03s, 	Step: 6482, 	{'train/accuracy': 0.20845703125, 'train/loss': 4.222894592285156, 'validation/accuracy': 0.19212, 'validation/loss': 4.32191375, 'validation/num_examples': 50000, 'test/accuracy': 0.1446, 'test/loss': 4.697025390625, 'test/num_examples': 10000}
I0401 00:34:59.655619 140637436720960 submission_runner.py:390] After eval at step 6482: RAM USED (GB) 100.8617472
I0401 00:34:59.663388 140593405622016 logging_writer.py:48] [6482] global_step=6482, preemption_count=0, score=2489.110119, test/accuracy=0.144600, test/loss=4.697025, test/num_examples=10000, total_duration=3075.032994, train/accuracy=0.208457, train/loss=4.222895, validation/accuracy=0.192120, validation/loss=4.321914, validation/num_examples=50000
I0401 00:34:59.940576 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_6482.
I0401 00:34:59.941205 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 6482: RAM USED (GB) 100.8606208
I0401 00:35:07.269816 140593397229312 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.657484, loss=5.718743
I0401 00:35:07.274969 140637436720960 submission.py:139] 6500) loss = 5.719, grad_norm = 0.657
I0401 00:38:20.780092 140593405622016 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.578178, loss=5.631524
I0401 00:38:20.787401 140637436720960 submission.py:139] 7000) loss = 5.632, grad_norm = 0.578
I0401 00:41:35.319404 140593397229312 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.615946, loss=5.580800
I0401 00:41:35.323369 140637436720960 submission.py:139] 7500) loss = 5.581, grad_norm = 0.616
I0401 00:42:00.138504 140637436720960 submission_runner.py:371] Before eval at step 7560: RAM USED (GB) 100.886421504
I0401 00:42:00.138749 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:42:43.953892 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:43:28.459360 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:43:29.870684 140637436720960 submission_runner.py:380] Time since start: 3583.45s, 	Step: 7560, 	{'train/accuracy': 0.2546875, 'train/loss': 3.902576599121094, 'validation/accuracy': 0.23164, 'validation/loss': 4.022051875, 'validation/num_examples': 50000, 'test/accuracy': 0.1779, 'test/loss': 4.420820703125, 'test/num_examples': 10000}
I0401 00:43:29.871042 140637436720960 submission_runner.py:390] After eval at step 7560: RAM USED (GB) 100.914020352
I0401 00:43:29.882675 140593405622016 logging_writer.py:48] [7560] global_step=7560, preemption_count=0, score=2902.863075, test/accuracy=0.177900, test/loss=4.420821, test/num_examples=10000, total_duration=3583.449628, train/accuracy=0.254688, train/loss=3.902577, validation/accuracy=0.231640, validation/loss=4.022052, validation/num_examples=50000
I0401 00:43:30.160059 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_7560.
I0401 00:43:30.160707 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 7560: RAM USED (GB) 100.913414144
I0401 00:46:20.843335 140593397229312 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.580641, loss=5.430469
I0401 00:46:20.847901 140637436720960 submission.py:139] 8000) loss = 5.430, grad_norm = 0.581
I0401 00:49:34.461919 140593405622016 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.612620, loss=5.512582
I0401 00:49:34.467925 140637436720960 submission.py:139] 8500) loss = 5.513, grad_norm = 0.613
I0401 00:50:30.308414 140637436720960 submission_runner.py:371] Before eval at step 8645: RAM USED (GB) 100.80053248
I0401 00:50:30.308637 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:51:13.590878 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 00:51:59.470141 140637436720960 spec.py:326] Evaluating on the test split.
I0401 00:52:00.884908 140637436720960 submission_runner.py:380] Time since start: 4093.62s, 	Step: 8645, 	{'train/accuracy': 0.277890625, 'train/loss': 3.6796719360351564, 'validation/accuracy': 0.2534, 'validation/loss': 3.80911125, 'validation/num_examples': 50000, 'test/accuracy': 0.1992, 'test/loss': 4.225508984375, 'test/num_examples': 10000}
I0401 00:52:00.885240 140637436720960 submission_runner.py:390] After eval at step 8645: RAM USED (GB) 100.800258048
I0401 00:52:00.894275 140593397229312 logging_writer.py:48] [8645] global_step=8645, preemption_count=0, score=3316.514809, test/accuracy=0.199200, test/loss=4.225509, test/num_examples=10000, total_duration=4093.620328, train/accuracy=0.277891, train/loss=3.679672, validation/accuracy=0.253400, validation/loss=3.809111, validation/num_examples=50000
I0401 00:52:01.203525 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_8645.
I0401 00:52:01.204164 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 8645: RAM USED (GB) 100.790648832
I0401 00:54:21.917693 140593405622016 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.567811, loss=5.524765
I0401 00:54:21.922559 140637436720960 submission.py:139] 9000) loss = 5.525, grad_norm = 0.568
I0401 00:57:35.733402 140593397229312 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.583497, loss=5.447248
I0401 00:57:35.738295 140637436720960 submission.py:139] 9500) loss = 5.447, grad_norm = 0.583
I0401 00:59:01.329975 140637436720960 submission_runner.py:371] Before eval at step 9722: RAM USED (GB) 101.003513856
I0401 00:59:01.330199 140637436720960 spec.py:298] Evaluating on the training split.
I0401 00:59:46.207437 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 01:00:34.797558 140637436720960 spec.py:326] Evaluating on the test split.
I0401 01:00:36.217582 140637436720960 submission_runner.py:380] Time since start: 4604.64s, 	Step: 9722, 	{'train/accuracy': 0.31251953125, 'train/loss': 3.532811279296875, 'validation/accuracy': 0.28368, 'validation/loss': 3.6689871875, 'validation/num_examples': 50000, 'test/accuracy': 0.2218, 'test/loss': 4.10836796875, 'test/num_examples': 10000}
I0401 01:00:36.217903 140637436720960 submission_runner.py:390] After eval at step 9722: RAM USED (GB) 100.87262208
I0401 01:00:36.226784 140593405622016 logging_writer.py:48] [9722] global_step=9722, preemption_count=0, score=3730.177339, test/accuracy=0.221800, test/loss=4.108368, test/num_examples=10000, total_duration=4604.641269, train/accuracy=0.312520, train/loss=3.532811, validation/accuracy=0.283680, validation/loss=3.668987, validation/num_examples=50000
I0401 01:00:36.503607 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_9722.
I0401 01:00:36.504229 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 9722: RAM USED (GB) 100.872015872
I0401 01:02:25.699136 140593397229312 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.589222, loss=5.346728
I0401 01:02:25.703193 140637436720960 submission.py:139] 10000) loss = 5.347, grad_norm = 0.589
I0401 01:05:41.122474 140593405622016 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.542666, loss=5.347159
I0401 01:05:41.126244 140637436720960 submission.py:139] 10500) loss = 5.347, grad_norm = 0.543
I0401 01:07:36.653008 140637436720960 submission_runner.py:371] Before eval at step 10799: RAM USED (GB) 101.061517312
I0401 01:07:36.653210 140637436720960 spec.py:298] Evaluating on the training split.
I0401 01:08:19.369808 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 01:09:05.699157 140637436720960 spec.py:326] Evaluating on the test split.
I0401 01:09:07.119182 140637436720960 submission_runner.py:380] Time since start: 5119.97s, 	Step: 10799, 	{'train/accuracy': 0.34310546875, 'train/loss': 3.3288455200195313, 'validation/accuracy': 0.3128, 'validation/loss': 3.48526375, 'validation/num_examples': 50000, 'test/accuracy': 0.2432, 'test/loss': 3.95111171875, 'test/num_examples': 10000}
I0401 01:09:07.119518 140637436720960 submission_runner.py:390] After eval at step 10799: RAM USED (GB) 101.042556928
I0401 01:09:07.127143 140593397229312 logging_writer.py:48] [10799] global_step=10799, preemption_count=0, score=4143.861928, test/accuracy=0.243200, test/loss=3.951112, test/num_examples=10000, total_duration=5119.965341, train/accuracy=0.343105, train/loss=3.328846, validation/accuracy=0.312800, validation/loss=3.485264, validation/num_examples=50000
I0401 01:09:07.406157 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_10799.
I0401 01:09:07.406766 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 10799: RAM USED (GB) 101.041410048
I0401 01:10:25.344534 140593405622016 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.578698, loss=5.216126
I0401 01:10:25.349362 140637436720960 submission.py:139] 11000) loss = 5.216, grad_norm = 0.579
I0401 01:13:42.599439 140593397229312 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.594223, loss=5.482806
I0401 01:13:42.604177 140637436720960 submission.py:139] 11500) loss = 5.483, grad_norm = 0.594
I0401 01:16:07.631729 140637436720960 submission_runner.py:371] Before eval at step 11875: RAM USED (GB) 100.937056256
I0401 01:16:07.631964 140637436720960 spec.py:298] Evaluating on the training split.
I0401 01:16:51.183469 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 01:17:35.992901 140637436720960 spec.py:326] Evaluating on the test split.
I0401 01:17:37.407591 140637436720960 submission_runner.py:380] Time since start: 5630.94s, 	Step: 11875, 	{'train/accuracy': 0.3745703125, 'train/loss': 3.1386236572265624, 'validation/accuracy': 0.33966, 'validation/loss': 3.2959540625, 'validation/num_examples': 50000, 'test/accuracy': 0.2644, 'test/loss': 3.79481328125, 'test/num_examples': 10000}
I0401 01:17:37.407953 140637436720960 submission_runner.py:390] After eval at step 11875: RAM USED (GB) 101.008039936
I0401 01:17:37.420932 140593405622016 logging_writer.py:48] [11875] global_step=11875, preemption_count=0, score=4557.634842, test/accuracy=0.264400, test/loss=3.794813, test/num_examples=10000, total_duration=5630.942020, train/accuracy=0.374570, train/loss=3.138624, validation/accuracy=0.339660, validation/loss=3.295954, validation/num_examples=50000
I0401 01:17:37.708794 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_11875.
I0401 01:17:37.709473 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 11875: RAM USED (GB) 101.007179776
I0401 01:18:26.343342 140593397229312 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.609860, loss=5.156911
I0401 01:18:26.348031 140637436720960 submission.py:139] 12000) loss = 5.157, grad_norm = 0.610
I0401 01:21:40.082520 140593405622016 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.638520, loss=5.187339
I0401 01:21:40.087517 140637436720960 submission.py:139] 12500) loss = 5.187, grad_norm = 0.639
I0401 01:24:37.946183 140637436720960 submission_runner.py:371] Before eval at step 12955: RAM USED (GB) 100.847661056
I0401 01:24:37.946383 140637436720960 spec.py:298] Evaluating on the training split.
I0401 01:25:22.001522 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 01:26:06.617425 140637436720960 spec.py:326] Evaluating on the test split.
I0401 01:26:08.028680 140637436720960 submission_runner.py:380] Time since start: 6141.26s, 	Step: 12955, 	{'train/accuracy': 0.40498046875, 'train/loss': 2.9626495361328127, 'validation/accuracy': 0.36908, 'validation/loss': 3.1302703125, 'validation/num_examples': 50000, 'test/accuracy': 0.2851, 'test/loss': 3.651301953125, 'test/num_examples': 10000}
I0401 01:26:08.029041 140637436720960 submission_runner.py:390] After eval at step 12955: RAM USED (GB) 100.804661248
I0401 01:26:08.037401 140593397229312 logging_writer.py:48] [12955] global_step=12955, preemption_count=0, score=4971.399239, test/accuracy=0.285100, test/loss=3.651302, test/num_examples=10000, total_duration=6141.257989, train/accuracy=0.404980, train/loss=2.962650, validation/accuracy=0.369080, validation/loss=3.130270, validation/num_examples=50000
I0401 01:26:08.323292 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_12955.
I0401 01:26:08.323911 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 12955: RAM USED (GB) 100.80405504
I0401 01:26:26.227666 140593405622016 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.613878, loss=5.256085
I0401 01:26:26.232756 140637436720960 submission.py:139] 13000) loss = 5.256, grad_norm = 0.614
I0401 01:29:39.861411 140593397229312 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.568151, loss=5.050716
I0401 01:29:39.866726 140637436720960 submission.py:139] 13500) loss = 5.051, grad_norm = 0.568
I0401 01:32:55.500094 140637436720960 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 100.979531776
I0401 01:32:55.500430 140637436720960 spec.py:298] Evaluating on the training split.
I0401 01:33:38.014637 140637436720960 spec.py:310] Evaluating on the validation split.
I0401 01:34:22.491635 140637436720960 spec.py:326] Evaluating on the test split.
I0401 01:34:23.910096 140637436720960 submission_runner.py:380] Time since start: 6638.81s, 	Step: 14000, 	{'train/accuracy': 0.41654296875, 'train/loss': 2.9149078369140624, 'validation/accuracy': 0.38074, 'validation/loss': 3.09303375, 'validation/num_examples': 50000, 'test/accuracy': 0.296, 'test/loss': 3.61245234375, 'test/num_examples': 10000}
I0401 01:34:23.910418 140637436720960 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 100.93828096
I0401 01:34:23.918969 140593405622016 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5372.321829, test/accuracy=0.296000, test/loss=3.612452, test/num_examples=10000, total_duration=6638.811413, train/accuracy=0.416543, train/loss=2.914908, validation/accuracy=0.380740, validation/loss=3.093034, validation/num_examples=50000
I0401 01:34:24.195649 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0401 01:34:24.196365 140637436720960 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.937158656
I0401 01:34:24.203951 140593397229312 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5372.321829
I0401 01:34:25.011936 140637436720960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0401 01:34:25.348161 140637436720960 submission_runner.py:543] Tuning trial 1/1
I0401 01:34:25.348393 140637436720960 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 01:34:25.348920 140637436720960 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00078125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.262601137161255, 'total_duration': 6.26477837562561, 'global_step': 1, 'preemption_count': 0}), (1084, {'train/accuracy': 0.050703125, 'train/loss': 5.894288330078125, 'validation/accuracy': 0.04602, 'validation/loss': 5.9313075, 'validation/num_examples': 50000, 'test/accuracy': 0.0353, 'test/loss': 6.05332265625, 'test/num_examples': 10000, 'score': 420.5359854698181, 'total_duration': 529.7580277919769, 'global_step': 1084, 'preemption_count': 0}), (2166, {'train/accuracy': 0.08154296875, 'train/loss': 5.427593994140625, 'validation/accuracy': 0.07382, 'validation/loss': 5.484860625, 'validation/num_examples': 50000, 'test/accuracy': 0.0574, 'test/loss': 5.67441796875, 'test/num_examples': 10000, 'score': 834.3924517631531, 'total_duration': 1037.7716853618622, 'global_step': 2166, 'preemption_count': 0}), (3246, {'train/accuracy': 0.11205078125, 'train/loss': 5.056005249023437, 'validation/accuracy': 0.10406, 'validation/loss': 5.1287559375, 'validation/num_examples': 50000, 'test/accuracy': 0.0787, 'test/loss': 5.3931375, 'test/num_examples': 10000, 'score': 1248.0104162693024, 'total_duration': 1545.9984488487244, 'global_step': 3246, 'preemption_count': 0}), (4326, {'train/accuracy': 0.144765625, 'train/loss': 4.7445916748046875, 'validation/accuracy': 0.13424, 'validation/loss': 4.829335625, 'validation/num_examples': 50000, 'test/accuracy': 0.1002, 'test/loss': 5.13111015625, 'test/num_examples': 10000, 'score': 1661.7102525234222, 'total_duration': 2054.5142211914062, 'global_step': 4326, 'preemption_count': 0}), (5404, {'train/accuracy': 0.1758984375, 'train/loss': 4.447415161132812, 'validation/accuracy': 0.16152, 'validation/loss': 4.543638125, 'validation/num_examples': 50000, 'test/accuracy': 0.1221, 'test/loss': 4.896507421875, 'test/num_examples': 10000, 'score': 2075.427943468094, 'total_duration': 2565.100597143173, 'global_step': 5404, 'preemption_count': 0}), (6482, {'train/accuracy': 0.20845703125, 'train/loss': 4.222894592285156, 'validation/accuracy': 0.19212, 'validation/loss': 4.32191375, 'validation/num_examples': 50000, 'test/accuracy': 0.1446, 'test/loss': 4.697025390625, 'test/num_examples': 10000, 'score': 2489.110118627548, 'total_duration': 3075.0329937934875, 'global_step': 6482, 'preemption_count': 0}), (7560, {'train/accuracy': 0.2546875, 'train/loss': 3.902576599121094, 'validation/accuracy': 0.23164, 'validation/loss': 4.022051875, 'validation/num_examples': 50000, 'test/accuracy': 0.1779, 'test/loss': 4.420820703125, 'test/num_examples': 10000, 'score': 2902.8630754947662, 'total_duration': 3583.449627637863, 'global_step': 7560, 'preemption_count': 0}), (8645, {'train/accuracy': 0.277890625, 'train/loss': 3.6796719360351564, 'validation/accuracy': 0.2534, 'validation/loss': 3.80911125, 'validation/num_examples': 50000, 'test/accuracy': 0.1992, 'test/loss': 4.225508984375, 'test/num_examples': 10000, 'score': 3316.514808654785, 'total_duration': 4093.6203281879425, 'global_step': 8645, 'preemption_count': 0}), (9722, {'train/accuracy': 0.31251953125, 'train/loss': 3.532811279296875, 'validation/accuracy': 0.28368, 'validation/loss': 3.6689871875, 'validation/num_examples': 50000, 'test/accuracy': 0.2218, 'test/loss': 4.10836796875, 'test/num_examples': 10000, 'score': 3730.177339076996, 'total_duration': 4604.641269445419, 'global_step': 9722, 'preemption_count': 0}), (10799, {'train/accuracy': 0.34310546875, 'train/loss': 3.3288455200195313, 'validation/accuracy': 0.3128, 'validation/loss': 3.48526375, 'validation/num_examples': 50000, 'test/accuracy': 0.2432, 'test/loss': 3.95111171875, 'test/num_examples': 10000, 'score': 4143.861927986145, 'total_duration': 5119.965341091156, 'global_step': 10799, 'preemption_count': 0}), (11875, {'train/accuracy': 0.3745703125, 'train/loss': 3.1386236572265624, 'validation/accuracy': 0.33966, 'validation/loss': 3.2959540625, 'validation/num_examples': 50000, 'test/accuracy': 0.2644, 'test/loss': 3.79481328125, 'test/num_examples': 10000, 'score': 4557.634842157364, 'total_duration': 5630.94202041626, 'global_step': 11875, 'preemption_count': 0}), (12955, {'train/accuracy': 0.40498046875, 'train/loss': 2.9626495361328127, 'validation/accuracy': 0.36908, 'validation/loss': 3.1302703125, 'validation/num_examples': 50000, 'test/accuracy': 0.2851, 'test/loss': 3.651301953125, 'test/num_examples': 10000, 'score': 4971.3992393016815, 'total_duration': 6141.257989406586, 'global_step': 12955, 'preemption_count': 0}), (14000, {'train/accuracy': 0.41654296875, 'train/loss': 2.9149078369140624, 'validation/accuracy': 0.38074, 'validation/loss': 3.09303375, 'validation/num_examples': 50000, 'test/accuracy': 0.296, 'test/loss': 3.61245234375, 'test/num_examples': 10000, 'score': 5372.321829319, 'total_duration': 6638.811412572861, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0401 01:34:25.349011 140637436720960 submission_runner.py:546] Timing: 5372.321829319
I0401 01:34:25.349055 140637436720960 submission_runner.py:547] ====================
I0401 01:34:25.349144 140637436720960 submission_runner.py:606] Final imagenet_vit score: 5372.321829319
