WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 01:57:14.185627 139995583932224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 01:57:15.201568 140523749717824 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 01:57:15.201671 140351906486080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 01:57:15.201768 140152516568896 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 01:57:15.201816 140304743393088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 01:57:15.201919 140408829212480 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 01:57:15.201923 140090707711808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 01:57:15.212787 139639207950144 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 01:57:15.213276 139639207950144 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.221132 139995583932224 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.222555 140523749717824 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.222584 140152516568896 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.222671 140304743393088 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.222708 140351906486080 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.222996 140090707711808 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:15.223016 140408829212480 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:57:16.380776 139639207950144 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/ogbg_pytorch.
W0401 01:57:16.492981 140408829212480 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.493295 140152516568896 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.493615 140351906486080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.493654 140090707711808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.494675 140304743393088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.494706 140523749717824 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.495097 139995583932224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:57:16.496019 139639207950144 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 01:57:16.499744 139639207950144 submission_runner.py:504] Using RNG seed 579874437
I0401 01:57:16.500864 139639207950144 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 01:57:16.500983 139639207950144 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/ogbg_pytorch/trial_1.
I0401 01:57:16.501226 139639207950144 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/hparams.json.
I0401 01:57:16.502285 139639207950144 submission_runner.py:230] Starting train once: RAM USED (GB) 5.760151552
I0401 01:57:16.502386 139639207950144 submission_runner.py:231] Initializing dataset.
I0401 01:57:16.502593 139639207950144 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.760667648
I0401 01:57:16.502659 139639207950144 submission_runner.py:240] Initializing model.
I0401 01:57:20.419967 139639207950144 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.411552256
I0401 01:57:20.420181 139639207950144 submission_runner.py:252] Initializing optimizer.
I0401 01:57:20.526523 139639207950144 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.414677504
I0401 01:57:20.526713 139639207950144 submission_runner.py:261] Initializing metrics bundle.
I0401 01:57:20.526762 139639207950144 submission_runner.py:275] Initializing checkpoint and logger.
I0401 01:57:20.528067 139639207950144 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 01:57:20.528177 139639207950144 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 01:57:21.177847 139639207950144 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/meta_data_0.json.
I0401 01:57:21.178847 139639207950144 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/flags_0.json.
I0401 01:57:21.210928 139639207950144 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 15.467446272
I0401 01:57:21.212095 139639207950144 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.467446272
I0401 01:57:21.212226 139639207950144 submission_runner.py:312] Starting training loop.
I0401 01:57:21.431024 139639207950144 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 01:57:21.436588 139639207950144 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 01:57:21.536926 139639207950144 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0401 01:57:22.998187 139639207950144 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 15.699480576
I0401 01:57:26.577667 139601159120640 logging_writer.py:48] [0] global_step=0, grad_norm=2.366218, loss=0.743106
I0401 01:57:26.583350 139639207950144 submission.py:139] 0) loss = 0.743, grad_norm = 2.366
I0401 01:57:26.583805 139639207950144 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 22.081994752
I0401 01:57:26.584373 139639207950144 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 22.081994752
I0401 01:57:26.584472 139639207950144 spec.py:298] Evaluating on the training split.
I0401 01:57:26.589002 139639207950144 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 01:57:26.592628 139639207950144 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 01:57:26.645746 139639207950144 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0401 01:57:41.477050 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.686380 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.686392 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.686503 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.706991 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.707015 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.707147 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:41.708003 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.182631 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.334510 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.335530 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.342178 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.342295 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.342616 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.342757 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:57:55.342895 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 01:58:20.040138 139639207950144 spec.py:310] Evaluating on the validation split.
I0401 01:58:20.042894 139639207950144 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 01:58:20.046637 139639207950144 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 01:58:20.098438 139639207950144 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0401 01:58:33.443919 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.589039 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.589305 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.595842 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.596237 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.596446 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.597704 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:33.598210 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:38.937396 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.084147 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.084384 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.091015 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.091133 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.091280 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.091696 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:58:39.092240 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 01:59:02.973152 139639207950144 spec.py:326] Evaluating on the test split.
I0401 01:59:02.975709 139639207950144 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 01:59:02.979521 139639207950144 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 01:59:03.032851 139639207950144 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0401 01:59:17.142940 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.285216 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.285514 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.291604 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.291636 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.291724 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.292837 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:17.293104 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.546205 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.703373 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.703816 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.709915 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.710194 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.710211 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.710249 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 01:59:22.711522 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 01:59:47.568198 139639207950144 submission_runner.py:380] Time since start: 5.37s, 	Step: 1, 	{'train/accuracy': 0.5069393926735902, 'train/loss': 0.743148684501648, 'train/mean_average_precision': 0.02259403342851612, 'validation/accuracy': 0.516115845732792, 'validation/loss': 0.7423849105834961, 'validation/mean_average_precision': 0.023596159006050894, 'validation/num_examples': 43793, 'test/accuracy': 0.5170280587263325, 'test/loss': 0.7438092231750488, 'test/mean_average_precision': 0.02613851768256295, 'test/num_examples': 43793}
I0401 01:59:47.568637 139639207950144 submission_runner.py:390] After eval at step 1: RAM USED (GB) 26.006941696
I0401 01:59:47.577985 139587250358016 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.370654, test/accuracy=0.517028, test/loss=0.743809, test/mean_average_precision=0.026139, test/num_examples=43793, total_duration=5.372767, train/accuracy=0.506939, train/loss=0.743149, train/mean_average_precision=0.022594, validation/accuracy=0.516116, validation/loss=0.742385, validation/mean_average_precision=0.023596, validation/num_examples=43793
I0401 01:59:47.864434 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_1.
I0401 01:59:47.864937 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 26.006450176
I0401 01:59:48.093525 139639207950144 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 26.054209536
I0401 01:59:48.095978 139639207950144 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.102599 140351906486080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.102855 140090707711808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.103188 140523749717824 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.103192 139995583932224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.103194 140152516568896 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.103192 140408829212480 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.103259 140304743393088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:59:48.131726 139587258750720 logging_writer.py:48] [1] global_step=1, grad_norm=2.363436, loss=0.743633
I0401 01:59:48.135800 139639207950144 submission.py:139] 1) loss = 0.744, grad_norm = 2.363
I0401 01:59:48.136297 139639207950144 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 26.056298496
I0401 01:59:48.397598 139587250358016 logging_writer.py:48] [2] global_step=2, grad_norm=2.352587, loss=0.742180
I0401 01:59:48.401578 139639207950144 submission.py:139] 2) loss = 0.742, grad_norm = 2.353
I0401 01:59:48.662307 139587258750720 logging_writer.py:48] [3] global_step=3, grad_norm=2.363732, loss=0.737823
I0401 01:59:48.666315 139639207950144 submission.py:139] 3) loss = 0.738, grad_norm = 2.364
I0401 01:59:48.928220 139587250358016 logging_writer.py:48] [4] global_step=4, grad_norm=2.360184, loss=0.731510
I0401 01:59:48.932063 139639207950144 submission.py:139] 4) loss = 0.732, grad_norm = 2.360
I0401 01:59:49.192299 139587258750720 logging_writer.py:48] [5] global_step=5, grad_norm=2.363350, loss=0.721016
I0401 01:59:49.196430 139639207950144 submission.py:139] 5) loss = 0.721, grad_norm = 2.363
I0401 01:59:49.466093 139587250358016 logging_writer.py:48] [6] global_step=6, grad_norm=2.271118, loss=0.706306
I0401 01:59:49.469945 139639207950144 submission.py:139] 6) loss = 0.706, grad_norm = 2.271
I0401 01:59:49.734749 139587258750720 logging_writer.py:48] [7] global_step=7, grad_norm=2.130281, loss=0.688556
I0401 01:59:49.738580 139639207950144 submission.py:139] 7) loss = 0.689, grad_norm = 2.130
I0401 01:59:49.998116 139587250358016 logging_writer.py:48] [8] global_step=8, grad_norm=1.953529, loss=0.669137
I0401 01:59:50.002015 139639207950144 submission.py:139] 8) loss = 0.669, grad_norm = 1.954
I0401 01:59:50.262127 139587258750720 logging_writer.py:48] [9] global_step=9, grad_norm=1.822168, loss=0.648966
I0401 01:59:50.266041 139639207950144 submission.py:139] 9) loss = 0.649, grad_norm = 1.822
I0401 01:59:50.526765 139587250358016 logging_writer.py:48] [10] global_step=10, grad_norm=1.695050, loss=0.627901
I0401 01:59:50.530904 139639207950144 submission.py:139] 10) loss = 0.628, grad_norm = 1.695
I0401 01:59:50.788655 139587258750720 logging_writer.py:48] [11] global_step=11, grad_norm=1.603575, loss=0.609089
I0401 01:59:50.792844 139639207950144 submission.py:139] 11) loss = 0.609, grad_norm = 1.604
I0401 01:59:51.048593 139587250358016 logging_writer.py:48] [12] global_step=12, grad_norm=1.578751, loss=0.592128
I0401 01:59:51.052449 139639207950144 submission.py:139] 12) loss = 0.592, grad_norm = 1.579
I0401 01:59:51.311989 139587258750720 logging_writer.py:48] [13] global_step=13, grad_norm=1.367212, loss=0.574359
I0401 01:59:51.315754 139639207950144 submission.py:139] 13) loss = 0.574, grad_norm = 1.367
I0401 01:59:51.574768 139587250358016 logging_writer.py:48] [14] global_step=14, grad_norm=1.208317, loss=0.557012
I0401 01:59:51.578624 139639207950144 submission.py:139] 14) loss = 0.557, grad_norm = 1.208
I0401 01:59:51.834779 139587258750720 logging_writer.py:48] [15] global_step=15, grad_norm=1.124595, loss=0.543561
I0401 01:59:51.838780 139639207950144 submission.py:139] 15) loss = 0.544, grad_norm = 1.125
I0401 01:59:52.094141 139587250358016 logging_writer.py:48] [16] global_step=16, grad_norm=1.067690, loss=0.532010
I0401 01:59:52.098029 139639207950144 submission.py:139] 16) loss = 0.532, grad_norm = 1.068
I0401 01:59:52.352041 139587258750720 logging_writer.py:48] [17] global_step=17, grad_norm=1.006294, loss=0.520886
I0401 01:59:52.355682 139639207950144 submission.py:139] 17) loss = 0.521, grad_norm = 1.006
I0401 01:59:52.610254 139587250358016 logging_writer.py:48] [18] global_step=18, grad_norm=0.953056, loss=0.509929
I0401 01:59:52.614171 139639207950144 submission.py:139] 18) loss = 0.510, grad_norm = 0.953
I0401 01:59:52.872933 139587258750720 logging_writer.py:48] [19] global_step=19, grad_norm=0.925635, loss=0.499160
I0401 01:59:52.876996 139639207950144 submission.py:139] 19) loss = 0.499, grad_norm = 0.926
I0401 01:59:53.130550 139587250358016 logging_writer.py:48] [20] global_step=20, grad_norm=0.830626, loss=0.487053
I0401 01:59:53.134444 139639207950144 submission.py:139] 20) loss = 0.487, grad_norm = 0.831
I0401 01:59:53.386184 139587258750720 logging_writer.py:48] [21] global_step=21, grad_norm=0.790257, loss=0.475988
I0401 01:59:53.390210 139639207950144 submission.py:139] 21) loss = 0.476, grad_norm = 0.790
I0401 01:59:53.643988 139587250358016 logging_writer.py:48] [22] global_step=22, grad_norm=0.731998, loss=0.468170
I0401 01:59:53.647812 139639207950144 submission.py:139] 22) loss = 0.468, grad_norm = 0.732
I0401 01:59:53.901734 139587258750720 logging_writer.py:48] [23] global_step=23, grad_norm=0.689495, loss=0.459019
I0401 01:59:53.905590 139639207950144 submission.py:139] 23) loss = 0.459, grad_norm = 0.689
I0401 01:59:54.158382 139587250358016 logging_writer.py:48] [24] global_step=24, grad_norm=0.666996, loss=0.448664
I0401 01:59:54.162351 139639207950144 submission.py:139] 24) loss = 0.449, grad_norm = 0.667
I0401 01:59:54.415706 139587258750720 logging_writer.py:48] [25] global_step=25, grad_norm=0.626031, loss=0.436472
I0401 01:59:54.419466 139639207950144 submission.py:139] 25) loss = 0.436, grad_norm = 0.626
I0401 01:59:54.675246 139587250358016 logging_writer.py:48] [26] global_step=26, grad_norm=0.581334, loss=0.428384
I0401 01:59:54.679428 139639207950144 submission.py:139] 26) loss = 0.428, grad_norm = 0.581
I0401 01:59:54.937985 139587258750720 logging_writer.py:48] [27] global_step=27, grad_norm=0.544890, loss=0.419167
I0401 01:59:54.941951 139639207950144 submission.py:139] 27) loss = 0.419, grad_norm = 0.545
I0401 01:59:55.202143 139587250358016 logging_writer.py:48] [28] global_step=28, grad_norm=0.521087, loss=0.410789
I0401 01:59:55.206109 139639207950144 submission.py:139] 28) loss = 0.411, grad_norm = 0.521
I0401 01:59:55.464770 139587258750720 logging_writer.py:48] [29] global_step=29, grad_norm=0.493389, loss=0.402491
I0401 01:59:55.468659 139639207950144 submission.py:139] 29) loss = 0.402, grad_norm = 0.493
I0401 01:59:55.725160 139587250358016 logging_writer.py:48] [30] global_step=30, grad_norm=0.481899, loss=0.392639
I0401 01:59:55.728935 139639207950144 submission.py:139] 30) loss = 0.393, grad_norm = 0.482
I0401 01:59:55.986482 139587258750720 logging_writer.py:48] [31] global_step=31, grad_norm=0.467921, loss=0.384058
I0401 01:59:55.990413 139639207950144 submission.py:139] 31) loss = 0.384, grad_norm = 0.468
I0401 01:59:56.247438 139587250358016 logging_writer.py:48] [32] global_step=32, grad_norm=0.454322, loss=0.374807
I0401 01:59:56.251787 139639207950144 submission.py:139] 32) loss = 0.375, grad_norm = 0.454
I0401 01:59:56.513353 139587258750720 logging_writer.py:48] [33] global_step=33, grad_norm=0.440694, loss=0.366007
I0401 01:59:56.517397 139639207950144 submission.py:139] 33) loss = 0.366, grad_norm = 0.441
I0401 01:59:56.774277 139587250358016 logging_writer.py:48] [34] global_step=34, grad_norm=0.427884, loss=0.359553
I0401 01:59:56.778155 139639207950144 submission.py:139] 34) loss = 0.360, grad_norm = 0.428
I0401 01:59:57.033743 139587258750720 logging_writer.py:48] [35] global_step=35, grad_norm=0.413629, loss=0.349308
I0401 01:59:57.037574 139639207950144 submission.py:139] 35) loss = 0.349, grad_norm = 0.414
I0401 01:59:57.293637 139587250358016 logging_writer.py:48] [36] global_step=36, grad_norm=0.405876, loss=0.344641
I0401 01:59:57.297693 139639207950144 submission.py:139] 36) loss = 0.345, grad_norm = 0.406
I0401 01:59:57.553652 139587258750720 logging_writer.py:48] [37] global_step=37, grad_norm=0.400428, loss=0.336682
I0401 01:59:57.557281 139639207950144 submission.py:139] 37) loss = 0.337, grad_norm = 0.400
I0401 01:59:57.815431 139587250358016 logging_writer.py:48] [38] global_step=38, grad_norm=0.397025, loss=0.326263
I0401 01:59:57.819276 139639207950144 submission.py:139] 38) loss = 0.326, grad_norm = 0.397
I0401 01:59:58.075481 139587258750720 logging_writer.py:48] [39] global_step=39, grad_norm=0.389728, loss=0.319578
I0401 01:59:58.079525 139639207950144 submission.py:139] 39) loss = 0.320, grad_norm = 0.390
I0401 01:59:58.333961 139587250358016 logging_writer.py:48] [40] global_step=40, grad_norm=0.377837, loss=0.312614
I0401 01:59:58.338084 139639207950144 submission.py:139] 40) loss = 0.313, grad_norm = 0.378
I0401 01:59:58.596222 139587258750720 logging_writer.py:48] [41] global_step=41, grad_norm=0.365308, loss=0.305116
I0401 01:59:58.600265 139639207950144 submission.py:139] 41) loss = 0.305, grad_norm = 0.365
I0401 01:59:58.852545 139587250358016 logging_writer.py:48] [42] global_step=42, grad_norm=0.353961, loss=0.297390
I0401 01:59:58.856318 139639207950144 submission.py:139] 42) loss = 0.297, grad_norm = 0.354
I0401 01:59:59.111151 139587258750720 logging_writer.py:48] [43] global_step=43, grad_norm=0.339556, loss=0.290625
I0401 01:59:59.115037 139639207950144 submission.py:139] 43) loss = 0.291, grad_norm = 0.340
I0401 01:59:59.370678 139587250358016 logging_writer.py:48] [44] global_step=44, grad_norm=0.331565, loss=0.283912
I0401 01:59:59.374580 139639207950144 submission.py:139] 44) loss = 0.284, grad_norm = 0.332
I0401 01:59:59.629299 139587258750720 logging_writer.py:48] [45] global_step=45, grad_norm=0.322330, loss=0.272774
I0401 01:59:59.633142 139639207950144 submission.py:139] 45) loss = 0.273, grad_norm = 0.322
I0401 01:59:59.887129 139587250358016 logging_writer.py:48] [46] global_step=46, grad_norm=0.311866, loss=0.268515
I0401 01:59:59.891091 139639207950144 submission.py:139] 46) loss = 0.269, grad_norm = 0.312
I0401 02:00:00.148136 139587258750720 logging_writer.py:48] [47] global_step=47, grad_norm=0.305296, loss=0.261443
I0401 02:00:00.152156 139639207950144 submission.py:139] 47) loss = 0.261, grad_norm = 0.305
I0401 02:00:00.410466 139587250358016 logging_writer.py:48] [48] global_step=48, grad_norm=0.298974, loss=0.250482
I0401 02:00:00.414280 139639207950144 submission.py:139] 48) loss = 0.250, grad_norm = 0.299
I0401 02:00:00.673378 139587258750720 logging_writer.py:48] [49] global_step=49, grad_norm=0.291145, loss=0.248773
I0401 02:00:00.677445 139639207950144 submission.py:139] 49) loss = 0.249, grad_norm = 0.291
I0401 02:00:00.934893 139587250358016 logging_writer.py:48] [50] global_step=50, grad_norm=0.284677, loss=0.242717
I0401 02:00:00.938473 139639207950144 submission.py:139] 50) loss = 0.243, grad_norm = 0.285
I0401 02:00:01.198375 139587258750720 logging_writer.py:48] [51] global_step=51, grad_norm=0.273438, loss=0.234062
I0401 02:00:01.202338 139639207950144 submission.py:139] 51) loss = 0.234, grad_norm = 0.273
I0401 02:00:01.471877 139587250358016 logging_writer.py:48] [52] global_step=52, grad_norm=0.264405, loss=0.228823
I0401 02:00:01.475553 139639207950144 submission.py:139] 52) loss = 0.229, grad_norm = 0.264
I0401 02:00:01.737024 139587258750720 logging_writer.py:48] [53] global_step=53, grad_norm=0.260130, loss=0.224770
I0401 02:00:01.740928 139639207950144 submission.py:139] 53) loss = 0.225, grad_norm = 0.260
I0401 02:00:02.017984 139587250358016 logging_writer.py:48] [54] global_step=54, grad_norm=0.250983, loss=0.214782
I0401 02:00:02.022063 139639207950144 submission.py:139] 54) loss = 0.215, grad_norm = 0.251
I0401 02:00:02.284986 139587258750720 logging_writer.py:48] [55] global_step=55, grad_norm=0.240019, loss=0.211764
I0401 02:00:02.289004 139639207950144 submission.py:139] 55) loss = 0.212, grad_norm = 0.240
I0401 02:00:02.556266 139587250358016 logging_writer.py:48] [56] global_step=56, grad_norm=0.232218, loss=0.209649
I0401 02:00:02.560179 139639207950144 submission.py:139] 56) loss = 0.210, grad_norm = 0.232
I0401 02:00:02.830647 139587258750720 logging_writer.py:48] [57] global_step=57, grad_norm=0.226385, loss=0.201987
I0401 02:00:02.834645 139639207950144 submission.py:139] 57) loss = 0.202, grad_norm = 0.226
I0401 02:00:03.092516 139587250358016 logging_writer.py:48] [58] global_step=58, grad_norm=0.215457, loss=0.196966
I0401 02:00:03.096263 139639207950144 submission.py:139] 58) loss = 0.197, grad_norm = 0.215
I0401 02:00:03.353916 139587258750720 logging_writer.py:48] [59] global_step=59, grad_norm=0.210350, loss=0.189956
I0401 02:00:03.357707 139639207950144 submission.py:139] 59) loss = 0.190, grad_norm = 0.210
I0401 02:00:03.621370 139587250358016 logging_writer.py:48] [60] global_step=60, grad_norm=0.201521, loss=0.185053
I0401 02:00:03.625260 139639207950144 submission.py:139] 60) loss = 0.185, grad_norm = 0.202
I0401 02:00:03.888047 139587258750720 logging_writer.py:48] [61] global_step=61, grad_norm=0.195345, loss=0.179720
I0401 02:00:03.892203 139639207950144 submission.py:139] 61) loss = 0.180, grad_norm = 0.195
I0401 02:00:04.152498 139587250358016 logging_writer.py:48] [62] global_step=62, grad_norm=0.191174, loss=0.179150
I0401 02:00:04.156254 139639207950144 submission.py:139] 62) loss = 0.179, grad_norm = 0.191
I0401 02:00:04.414445 139587258750720 logging_writer.py:48] [63] global_step=63, grad_norm=0.185621, loss=0.174090
I0401 02:00:04.418426 139639207950144 submission.py:139] 63) loss = 0.174, grad_norm = 0.186
I0401 02:00:04.678489 139587250358016 logging_writer.py:48] [64] global_step=64, grad_norm=0.179361, loss=0.166136
I0401 02:00:04.682815 139639207950144 submission.py:139] 64) loss = 0.166, grad_norm = 0.179
I0401 02:00:04.943587 139587258750720 logging_writer.py:48] [65] global_step=65, grad_norm=0.175969, loss=0.162491
I0401 02:00:04.947460 139639207950144 submission.py:139] 65) loss = 0.162, grad_norm = 0.176
I0401 02:00:05.205822 139587250358016 logging_writer.py:48] [66] global_step=66, grad_norm=0.168103, loss=0.161885
I0401 02:00:05.209670 139639207950144 submission.py:139] 66) loss = 0.162, grad_norm = 0.168
I0401 02:00:05.463902 139587258750720 logging_writer.py:48] [67] global_step=67, grad_norm=0.166445, loss=0.159919
I0401 02:00:05.467743 139639207950144 submission.py:139] 67) loss = 0.160, grad_norm = 0.166
I0401 02:00:05.724441 139587250358016 logging_writer.py:48] [68] global_step=68, grad_norm=0.158681, loss=0.151791
I0401 02:00:05.728223 139639207950144 submission.py:139] 68) loss = 0.152, grad_norm = 0.159
I0401 02:00:05.988273 139587258750720 logging_writer.py:48] [69] global_step=69, grad_norm=0.156215, loss=0.150541
I0401 02:00:05.992291 139639207950144 submission.py:139] 69) loss = 0.151, grad_norm = 0.156
I0401 02:00:06.248484 139587250358016 logging_writer.py:48] [70] global_step=70, grad_norm=0.150843, loss=0.144421
I0401 02:00:06.252518 139639207950144 submission.py:139] 70) loss = 0.144, grad_norm = 0.151
I0401 02:00:06.509020 139587258750720 logging_writer.py:48] [71] global_step=71, grad_norm=0.148684, loss=0.140624
I0401 02:00:06.513157 139639207950144 submission.py:139] 71) loss = 0.141, grad_norm = 0.149
I0401 02:00:06.770484 139587250358016 logging_writer.py:48] [72] global_step=72, grad_norm=0.140436, loss=0.139173
I0401 02:00:06.774418 139639207950144 submission.py:139] 72) loss = 0.139, grad_norm = 0.140
I0401 02:00:07.039186 139587258750720 logging_writer.py:48] [73] global_step=73, grad_norm=0.137302, loss=0.137885
I0401 02:00:07.043357 139639207950144 submission.py:139] 73) loss = 0.138, grad_norm = 0.137
I0401 02:00:07.297340 139587250358016 logging_writer.py:48] [74] global_step=74, grad_norm=0.134737, loss=0.133097
I0401 02:00:07.301176 139639207950144 submission.py:139] 74) loss = 0.133, grad_norm = 0.135
I0401 02:00:07.557512 139587258750720 logging_writer.py:48] [75] global_step=75, grad_norm=0.128852, loss=0.129994
I0401 02:00:07.561371 139639207950144 submission.py:139] 75) loss = 0.130, grad_norm = 0.129
I0401 02:00:07.823119 139587250358016 logging_writer.py:48] [76] global_step=76, grad_norm=0.125908, loss=0.132264
I0401 02:00:07.826848 139639207950144 submission.py:139] 76) loss = 0.132, grad_norm = 0.126
I0401 02:00:08.111104 139587258750720 logging_writer.py:48] [77] global_step=77, grad_norm=0.121729, loss=0.124849
I0401 02:00:08.114779 139639207950144 submission.py:139] 77) loss = 0.125, grad_norm = 0.122
I0401 02:00:08.393319 139587250358016 logging_writer.py:48] [78] global_step=78, grad_norm=0.117065, loss=0.123304
I0401 02:00:08.397119 139639207950144 submission.py:139] 78) loss = 0.123, grad_norm = 0.117
I0401 02:00:08.688455 139587258750720 logging_writer.py:48] [79] global_step=79, grad_norm=0.117664, loss=0.118418
I0401 02:00:08.692532 139639207950144 submission.py:139] 79) loss = 0.118, grad_norm = 0.118
I0401 02:00:08.964477 139587250358016 logging_writer.py:48] [80] global_step=80, grad_norm=0.110850, loss=0.117567
I0401 02:00:08.968250 139639207950144 submission.py:139] 80) loss = 0.118, grad_norm = 0.111
I0401 02:00:09.227474 139587258750720 logging_writer.py:48] [81] global_step=81, grad_norm=0.111383, loss=0.114636
I0401 02:00:09.231399 139639207950144 submission.py:139] 81) loss = 0.115, grad_norm = 0.111
I0401 02:00:09.488728 139587250358016 logging_writer.py:48] [82] global_step=82, grad_norm=0.106157, loss=0.112672
I0401 02:00:09.492724 139639207950144 submission.py:139] 82) loss = 0.113, grad_norm = 0.106
I0401 02:00:09.754093 139587258750720 logging_writer.py:48] [83] global_step=83, grad_norm=0.102699, loss=0.110941
I0401 02:00:09.758013 139639207950144 submission.py:139] 83) loss = 0.111, grad_norm = 0.103
I0401 02:00:10.022789 139587250358016 logging_writer.py:48] [84] global_step=84, grad_norm=0.099033, loss=0.111167
I0401 02:00:10.026834 139639207950144 submission.py:139] 84) loss = 0.111, grad_norm = 0.099
I0401 02:00:10.290869 139587258750720 logging_writer.py:48] [85] global_step=85, grad_norm=0.098296, loss=0.109501
I0401 02:00:10.294759 139639207950144 submission.py:139] 85) loss = 0.110, grad_norm = 0.098
I0401 02:00:10.561759 139587250358016 logging_writer.py:48] [86] global_step=86, grad_norm=0.096007, loss=0.109252
I0401 02:00:10.565591 139639207950144 submission.py:139] 86) loss = 0.109, grad_norm = 0.096
I0401 02:00:10.824534 139587258750720 logging_writer.py:48] [87] global_step=87, grad_norm=0.092181, loss=0.101519
I0401 02:00:10.828292 139639207950144 submission.py:139] 87) loss = 0.102, grad_norm = 0.092
I0401 02:00:11.084962 139587250358016 logging_writer.py:48] [88] global_step=88, grad_norm=0.092436, loss=0.106178
I0401 02:00:11.088822 139639207950144 submission.py:139] 88) loss = 0.106, grad_norm = 0.092
I0401 02:00:11.346728 139587258750720 logging_writer.py:48] [89] global_step=89, grad_norm=0.090339, loss=0.100748
I0401 02:00:11.350907 139639207950144 submission.py:139] 89) loss = 0.101, grad_norm = 0.090
I0401 02:00:11.607850 139587250358016 logging_writer.py:48] [90] global_step=90, grad_norm=0.085955, loss=0.098143
I0401 02:00:11.611752 139639207950144 submission.py:139] 90) loss = 0.098, grad_norm = 0.086
I0401 02:00:11.869257 139587258750720 logging_writer.py:48] [91] global_step=91, grad_norm=0.084086, loss=0.099719
I0401 02:00:11.873044 139639207950144 submission.py:139] 91) loss = 0.100, grad_norm = 0.084
I0401 02:00:12.128429 139587250358016 logging_writer.py:48] [92] global_step=92, grad_norm=0.080195, loss=0.101624
I0401 02:00:12.132626 139639207950144 submission.py:139] 92) loss = 0.102, grad_norm = 0.080
I0401 02:00:12.393524 139587258750720 logging_writer.py:48] [93] global_step=93, grad_norm=0.079513, loss=0.095561
I0401 02:00:12.397525 139639207950144 submission.py:139] 93) loss = 0.096, grad_norm = 0.080
I0401 02:00:12.660240 139587250358016 logging_writer.py:48] [94] global_step=94, grad_norm=0.078029, loss=0.096222
I0401 02:00:12.664199 139639207950144 submission.py:139] 94) loss = 0.096, grad_norm = 0.078
I0401 02:00:12.930796 139587258750720 logging_writer.py:48] [95] global_step=95, grad_norm=0.077375, loss=0.094635
I0401 02:00:12.934639 139639207950144 submission.py:139] 95) loss = 0.095, grad_norm = 0.077
I0401 02:00:13.193777 139587250358016 logging_writer.py:48] [96] global_step=96, grad_norm=0.073168, loss=0.094048
I0401 02:00:13.197575 139639207950144 submission.py:139] 96) loss = 0.094, grad_norm = 0.073
I0401 02:00:13.454020 139587258750720 logging_writer.py:48] [97] global_step=97, grad_norm=0.072058, loss=0.089605
I0401 02:00:13.457850 139639207950144 submission.py:139] 97) loss = 0.090, grad_norm = 0.072
I0401 02:00:13.714474 139587250358016 logging_writer.py:48] [98] global_step=98, grad_norm=0.070946, loss=0.088621
I0401 02:00:13.718061 139639207950144 submission.py:139] 98) loss = 0.089, grad_norm = 0.071
I0401 02:00:13.980100 139587258750720 logging_writer.py:48] [99] global_step=99, grad_norm=0.066908, loss=0.095676
I0401 02:00:13.984127 139639207950144 submission.py:139] 99) loss = 0.096, grad_norm = 0.067
I0401 02:00:14.250507 139587250358016 logging_writer.py:48] [100] global_step=100, grad_norm=0.067528, loss=0.089538
I0401 02:00:14.254466 139639207950144 submission.py:139] 100) loss = 0.090, grad_norm = 0.068
I0401 02:01:56.889153 139587258750720 logging_writer.py:48] [500] global_step=500, grad_norm=0.010694, loss=0.057287
I0401 02:01:56.893585 139639207950144 submission.py:139] 500) loss = 0.057, grad_norm = 0.011
I0401 02:03:48.051531 139639207950144 submission_runner.py:371] Before eval at step 934: RAM USED (GB) 26.834354176
I0401 02:03:48.051763 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:04:01.723213 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.889140 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.889140 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.895082 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.895917 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.896447 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.896551 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:01.896662 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.761418 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.901536 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.901577 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.907967 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.908705 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.909135 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.909274 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:15.909857 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:04:40.235550 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:04:40.570344 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.745256 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.745329 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.751864 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.752106 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.752157 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.752348 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.752418 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:40.914283 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.064018 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.065283 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.070333 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.071353 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.072902 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.073256 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:41.073940 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:04:43.488883 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:04:43.828761 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.006114 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.006161 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.012490 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.013104 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.013509 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.013624 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.013789 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.175711 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.323522 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.324196 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.330442 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.330589 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.331250 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.332182 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:44.332359 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:04:46.698073 139639207950144 submission_runner.py:380] Time since start: 386.84s, 	Step: 934, 	{'train/accuracy': 0.9866413841003798, 'train/loss': 0.05522485449910164, 'train/mean_average_precision': 0.03420775636426137, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06499487161636353, 'validation/mean_average_precision': 0.035467385783812595, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06834197789430618, 'test/mean_average_precision': 0.03684746394541861, 'test/num_examples': 43793}
I0401 02:04:46.698460 139639207950144 submission_runner.py:390] After eval at step 934: RAM USED (GB) 27.773898752
I0401 02:04:46.706413 139587250358016 logging_writer.py:48] [934] global_step=934, preemption_count=0, score=244.655745, test/accuracy=0.983142, test/loss=0.068342, test/mean_average_precision=0.036847, test/num_examples=43793, total_duration=386.839799, train/accuracy=0.986641, train/loss=0.055225, train/mean_average_precision=0.034208, validation/accuracy=0.984118, validation/loss=0.064995, validation/mean_average_precision=0.035467, validation/num_examples=43793
I0401 02:04:46.769832 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_934.
I0401 02:04:46.770248 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 934: RAM USED (GB) 27.773333504
I0401 02:05:04.474964 139587258750720 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.017144, loss=0.057161
I0401 02:05:04.479326 139639207950144 submission.py:139] 1000) loss = 0.057, grad_norm = 0.017
I0401 02:07:13.678661 139587250358016 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.020732, loss=0.050603
I0401 02:07:13.683350 139639207950144 submission.py:139] 1500) loss = 0.051, grad_norm = 0.021
I0401 02:08:46.991068 139639207950144 submission_runner.py:371] Before eval at step 1862: RAM USED (GB) 28.120694784
I0401 02:08:46.991269 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:09:00.919303 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.096857 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.098726 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.103355 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.103524 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.104347 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.104361 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:01.104698 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.222277 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.367670 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.369212 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.373376 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.374379 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.375200 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.375623 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:15.375906 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:09:41.214261 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:09:41.549349 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.736166 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.736711 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.741758 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.741968 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.742513 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.742684 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.742767 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:41.893812 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.049970 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.050044 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.054259 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.056592 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.056893 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.057320 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:42.057471 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:09:44.447376 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:09:44.777074 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.971976 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.972178 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.977731 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.977863 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.978080 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.978320 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:44.978443 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.125048 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.282614 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.283241 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.288036 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.288459 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.288690 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.289983 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:45.290009 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:09:47.645626 139639207950144 submission_runner.py:380] Time since start: 685.78s, 	Step: 1862, 	{'train/accuracy': 0.9868083119768818, 'train/loss': 0.05200014263391495, 'train/mean_average_precision': 0.05063854266366845, 'validation/accuracy': 0.9841208176944679, 'validation/loss': 0.06150413677096367, 'validation/mean_average_precision': 0.049301677801379104, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06472355127334595, 'test/mean_average_precision': 0.050484940268355635, 'test/num_examples': 43793}
I0401 02:09:47.646055 139639207950144 submission_runner.py:390] After eval at step 1862: RAM USED (GB) 28.554579968
I0401 02:09:47.654431 139587258750720 logging_writer.py:48] [1862] global_step=1862, preemption_count=0, score=483.906868, test/accuracy=0.983142, test/loss=0.064724, test/mean_average_precision=0.050485, test/num_examples=43793, total_duration=685.779349, train/accuracy=0.986808, train/loss=0.052000, train/mean_average_precision=0.050639, validation/accuracy=0.984121, validation/loss=0.061504, validation/mean_average_precision=0.049302, validation/num_examples=43793
I0401 02:09:47.720295 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_1862.
I0401 02:09:47.720768 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 1862: RAM USED (GB) 28.554022912
I0401 02:10:24.006336 139587250358016 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.048158, loss=0.055615
I0401 02:10:24.011167 139639207950144 submission.py:139] 2000) loss = 0.056, grad_norm = 0.048
I0401 02:12:33.160903 139587258750720 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.054360, loss=0.051689
I0401 02:12:33.165374 139639207950144 submission.py:139] 2500) loss = 0.052, grad_norm = 0.054
I0401 02:13:47.726821 139639207950144 submission_runner.py:371] Before eval at step 2788: RAM USED (GB) 28.73808896
I0401 02:13:47.727023 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:14:01.898839 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.052557 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.056293 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.059091 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.059213 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.059533 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.059924 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:02.060298 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.310121 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.472853 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.473331 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.477954 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.477965 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.479157 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.479324 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:16.480381 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:14:42.569087 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:14:42.891704 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.064896 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.066606 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.070759 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.071169 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.071669 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.071826 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.072097 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.230770 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.378906 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.378957 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.384390 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.384387 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.386559 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.386735 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:43.386878 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:14:45.839171 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:14:46.165479 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.329856 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.331619 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.338079 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.338662 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.339020 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.339149 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.339495 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.507688 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.658012 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.658319 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.662547 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.663314 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.664699 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.664797 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:46.665544 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:14:49.074193 139639207950144 submission_runner.py:380] Time since start: 986.52s, 	Step: 2788, 	{'train/accuracy': 0.9868359129374749, 'train/loss': 0.050345566123723984, 'train/mean_average_precision': 0.0691846753051992, 'validation/accuracy': 0.9841451741363815, 'validation/loss': 0.06031258404254913, 'validation/mean_average_precision': 0.06489549442122848, 'validation/num_examples': 43793, 'test/accuracy': 0.983172008749056, 'test/loss': 0.06365763396024704, 'test/mean_average_precision': 0.06641752277416993, 'test/num_examples': 43793}
I0401 02:14:49.074587 139639207950144 submission_runner.py:390] After eval at step 2788: RAM USED (GB) 29.097725952
I0401 02:14:49.082809 139587250358016 logging_writer.py:48] [2788] global_step=2788, preemption_count=0, score=722.945467, test/accuracy=0.983172, test/loss=0.063658, test/mean_average_precision=0.066418, test/num_examples=43793, total_duration=986.515058, train/accuracy=0.986836, train/loss=0.050346, train/mean_average_precision=0.069185, validation/accuracy=0.984145, validation/loss=0.060313, validation/mean_average_precision=0.064895, validation/num_examples=43793
I0401 02:14:49.148569 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_2788.
I0401 02:14:49.149031 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 2788: RAM USED (GB) 29.096906752
I0401 02:15:44.931074 139587258750720 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.046337, loss=0.052970
I0401 02:15:44.935899 139639207950144 submission.py:139] 3000) loss = 0.053, grad_norm = 0.046
I0401 02:17:54.325175 139587250358016 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.076382, loss=0.049509
I0401 02:17:54.331380 139639207950144 submission.py:139] 3500) loss = 0.050, grad_norm = 0.076
I0401 02:18:49.301699 139639207950144 submission_runner.py:371] Before eval at step 3712: RAM USED (GB) 29.375971328
I0401 02:18:49.301907 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:19:03.468470 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.617502 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.620662 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.624034 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.624066 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.624230 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.624763 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:03.624841 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.602017 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.750447 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.752490 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.756562 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.757605 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.758267 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.758541 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:17.758613 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:19:44.174251 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:19:44.494065 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.666189 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.666441 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.672937 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.673367 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.674240 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.674336 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.674534 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.840864 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.989411 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.989623 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.995622 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.996429 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.996679 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.997439 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:44.997740 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:19:47.420967 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:19:47.743972 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.910235 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.911490 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.918276 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.918602 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.918678 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.919506 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:47.919747 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.078356 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.226146 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.226614 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.232288 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.233675 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.233797 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.235070 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:48.235122 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:19:50.599551 139639207950144 submission_runner.py:380] Time since start: 1288.09s, 	Step: 3712, 	{'train/accuracy': 0.9867895758465811, 'train/loss': 0.04929176717996597, 'train/mean_average_precision': 0.09091147939026324, 'validation/accuracy': 0.9842101246481509, 'validation/loss': 0.058382000774145126, 'validation/mean_average_precision': 0.09221966955377464, 'validation/num_examples': 43793, 'test/accuracy': 0.9832200249431492, 'test/loss': 0.0617007240653038, 'test/mean_average_precision': 0.0944233435150632, 'test/num_examples': 43793}
I0401 02:19:50.599929 139639207950144 submission_runner.py:390] After eval at step 3712: RAM USED (GB) 29.685256192
I0401 02:19:50.607965 139587258750720 logging_writer.py:48] [3712] global_step=3712, preemption_count=0, score=962.183703, test/accuracy=0.983220, test/loss=0.061701, test/mean_average_precision=0.094423, test/num_examples=43793, total_duration=1288.089996, train/accuracy=0.986790, train/loss=0.049292, train/mean_average_precision=0.090911, validation/accuracy=0.984210, validation/loss=0.058382, validation/mean_average_precision=0.092220, validation/num_examples=43793
I0401 02:19:50.672733 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_3712.
I0401 02:19:50.673192 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 3712: RAM USED (GB) 29.684150272
I0401 02:21:06.689599 139587250358016 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.062866, loss=0.044026
I0401 02:21:06.694433 139639207950144 submission.py:139] 4000) loss = 0.044, grad_norm = 0.063
I0401 02:23:15.756178 139587258750720 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.090994, loss=0.047400
I0401 02:23:15.761722 139639207950144 submission.py:139] 4500) loss = 0.047, grad_norm = 0.091
I0401 02:23:50.764896 139639207950144 submission_runner.py:371] Before eval at step 4638: RAM USED (GB) 30.017409024
I0401 02:23:50.765084 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:24:04.599310 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.756435 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.757458 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.762950 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.763029 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.763699 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.763864 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:04.763995 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:18.903810 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.040550 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.041263 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.045581 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.046733 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.048036 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.048600 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:19.048981 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:24:46.072569 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:24:46.406203 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.567467 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.568496 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.575757 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.575782 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.576133 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.576323 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.577225 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.755075 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.905390 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.907170 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.912097 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.912738 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.912978 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.913304 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:46.913860 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:24:49.347842 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:24:49.673514 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.837858 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.838650 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.845342 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.846180 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.846360 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.846698 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:49.847017 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.014261 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.165257 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.166365 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.172023 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.172076 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.173157 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.173326 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:50.173543 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:24:52.553849 139639207950144 submission_runner.py:380] Time since start: 1589.55s, 	Step: 4638, 	{'train/accuracy': 0.9869588216785967, 'train/loss': 0.04732585698366165, 'train/mean_average_precision': 0.12110401271518415, 'validation/accuracy': 0.9843351543833071, 'validation/loss': 0.05675164610147476, 'validation/mean_average_precision': 0.11173895729012102, 'validation/num_examples': 43793, 'test/accuracy': 0.9833543860476733, 'test/loss': 0.06006234884262085, 'test/mean_average_precision': 0.1169843667075113, 'test/num_examples': 43793}
I0401 02:24:52.554282 139639207950144 submission_runner.py:390] After eval at step 4638: RAM USED (GB) 30.367203328
I0401 02:24:52.562602 139587250358016 logging_writer.py:48] [4638] global_step=4638, preemption_count=0, score=1201.326606, test/accuracy=0.983354, test/loss=0.060062, test/mean_average_precision=0.116984, test/num_examples=43793, total_duration=1589.553198, train/accuracy=0.986959, train/loss=0.047326, train/mean_average_precision=0.121104, validation/accuracy=0.984335, validation/loss=0.056752, validation/mean_average_precision=0.111739, validation/num_examples=43793
I0401 02:24:52.627176 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_4638.
I0401 02:24:52.627613 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 4638: RAM USED (GB) 30.36667904
I0401 02:26:26.742378 139587258750720 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.030598, loss=0.044743
I0401 02:26:26.750087 139639207950144 submission.py:139] 5000) loss = 0.045, grad_norm = 0.031
I0401 02:28:35.445116 139587250358016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.084555, loss=0.050726
I0401 02:28:35.450270 139639207950144 submission.py:139] 5500) loss = 0.051, grad_norm = 0.085
I0401 02:28:52.629820 139639207950144 submission_runner.py:371] Before eval at step 5568: RAM USED (GB) 30.503542784
I0401 02:28:52.630034 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:29:06.804259 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.969395 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.969721 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.975667 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.976481 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.976549 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.976680 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:06.976809 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.378797 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.515061 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.516950 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.522858 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.523334 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.523583 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.523888 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:21.524816 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:29:48.489241 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:29:48.810731 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.982393 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.983471 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.989717 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.990963 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.991423 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.991650 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:48.992104 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.154101 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.306239 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.307740 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.313808 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.314126 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.314276 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.314343 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:49.314412 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:29:51.746384 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:29:52.068273 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.242641 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.243387 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.250100 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.250710 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.251161 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.252010 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.252394 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.418257 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.568664 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.569585 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.575531 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.575648 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.575949 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.576910 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.577062 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:29:54.888051 139639207950144 submission_runner.py:380] Time since start: 1891.42s, 	Step: 5568, 	{'train/accuracy': 0.9875977356103284, 'train/loss': 0.0441487580537796, 'train/mean_average_precision': 0.13656302257405117, 'validation/accuracy': 0.9849018475984954, 'validation/loss': 0.05397336557507515, 'validation/mean_average_precision': 0.13130454572503797, 'validation/num_examples': 43793, 'test/accuracy': 0.9838770886517937, 'test/loss': 0.05709874629974365, 'test/mean_average_precision': 0.1318661803538981, 'test/num_examples': 43793}
I0401 02:29:54.888458 139639207950144 submission_runner.py:390] After eval at step 5568: RAM USED (GB) 30.915457024
I0401 02:29:54.899280 139587258750720 logging_writer.py:48] [5568] global_step=5568, preemption_count=0, score=1440.376949, test/accuracy=0.983877, test/loss=0.057099, test/mean_average_precision=0.131866, test/num_examples=43793, total_duration=1891.418009, train/accuracy=0.987598, train/loss=0.044149, train/mean_average_precision=0.136563, validation/accuracy=0.984902, validation/loss=0.053973, validation/mean_average_precision=0.131305, validation/num_examples=43793
I0401 02:29:54.963522 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_5568.
I0401 02:29:54.963934 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 5568: RAM USED (GB) 30.914895872
I0401 02:31:46.737614 139639207950144 submission_runner.py:371] Before eval at step 6000: RAM USED (GB) 30.963912704
I0401 02:31:46.737909 139639207950144 spec.py:298] Evaluating on the training split.
W0401 02:32:00.977575 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.132470 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.134512 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.138161 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.138225 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.138704 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.139441 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:01.139793 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.356748 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.498324 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.498783 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.504683 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.504956 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.505028 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.505210 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:15.505614 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:32:42.897104 139639207950144 spec.py:310] Evaluating on the validation split.
W0401 02:32:43.226269 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.406584 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.407190 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.413477 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.413660 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.413954 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.414516 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.414586 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.576128 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.731885 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.732851 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.738083 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.738153 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.738316 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.738422 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:43.739376 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:32:46.157935 139639207950144 spec.py:326] Evaluating on the test split.
W0401 02:32:46.491058 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.669822 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.670817 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.676233 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.677465 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.677562 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.677764 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.677977 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:46.844926 139639207950144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.000058 140523749717824 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.001697 140351906486080 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.005672 140090707711808 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.006536 139995583932224 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.007135 140304743393088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.007253 140152516568896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:32:47.007900 140408829212480 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:32:49.372642 139639207950144 submission_runner.py:380] Time since start: 2065.53s, 	Step: 6000, 	{'train/accuracy': 0.9874159536473895, 'train/loss': 0.043923649936914444, 'train/mean_average_precision': 0.14710723153238536, 'validation/accuracy': 0.9846862930875606, 'validation/loss': 0.053403474390506744, 'validation/mean_average_precision': 0.1386795443624961, 'validation/num_examples': 43793, 'test/accuracy': 0.9837317764854588, 'test/loss': 0.056378792971372604, 'test/mean_average_precision': 0.13998484228555372, 'test/num_examples': 43793}
I0401 02:32:49.373034 139639207950144 submission_runner.py:390] After eval at step 6000: RAM USED (GB) 31.439171584
I0401 02:32:49.381729 139587250358016 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1551.718166, test/accuracy=0.983732, test/loss=0.056379, test/mean_average_precision=0.139985, test/num_examples=43793, total_duration=2065.525836, train/accuracy=0.987416, train/loss=0.043924, train/mean_average_precision=0.147107, validation/accuracy=0.984686, validation/loss=0.053403, validation/mean_average_precision=0.138680, validation/num_examples=43793
I0401 02:32:49.444833 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_6000.
I0401 02:32:49.445244 139639207950144 submission_runner.py:409] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.438614528
I0401 02:32:49.452893 139587258750720 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1551.718166
I0401 02:32:49.580038 139639207950144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_pytorch/trial_1/checkpoint_6000.
I0401 02:32:49.753236 139639207950144 submission_runner.py:543] Tuning trial 1/1
I0401 02:32:49.753477 139639207950144 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 02:32:49.754582 139639207950144 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5069393926735902, 'train/loss': 0.743148684501648, 'train/mean_average_precision': 0.02259403342851612, 'validation/accuracy': 0.516115845732792, 'validation/loss': 0.7423849105834961, 'validation/mean_average_precision': 0.023596159006050894, 'validation/num_examples': 43793, 'test/accuracy': 0.5170280587263325, 'test/loss': 0.7438092231750488, 'test/mean_average_precision': 0.02613851768256295, 'test/num_examples': 43793, 'score': 5.370654344558716, 'total_duration': 5.372766733169556, 'global_step': 1, 'preemption_count': 0}), (934, {'train/accuracy': 0.9866413841003798, 'train/loss': 0.05522485449910164, 'train/mean_average_precision': 0.03420775636426137, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06499487161636353, 'validation/mean_average_precision': 0.035467385783812595, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06834197789430618, 'test/mean_average_precision': 0.03684746394541861, 'test/num_examples': 43793, 'score': 244.65574526786804, 'total_duration': 386.83979868888855, 'global_step': 934, 'preemption_count': 0}), (1862, {'train/accuracy': 0.9868083119768818, 'train/loss': 0.05200014263391495, 'train/mean_average_precision': 0.05063854266366845, 'validation/accuracy': 0.9841208176944679, 'validation/loss': 0.06150413677096367, 'validation/mean_average_precision': 0.049301677801379104, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06472355127334595, 'test/mean_average_precision': 0.050484940268355635, 'test/num_examples': 43793, 'score': 483.90686798095703, 'total_duration': 685.7793493270874, 'global_step': 1862, 'preemption_count': 0}), (2788, {'train/accuracy': 0.9868359129374749, 'train/loss': 0.050345566123723984, 'train/mean_average_precision': 0.0691846753051992, 'validation/accuracy': 0.9841451741363815, 'validation/loss': 0.06031258404254913, 'validation/mean_average_precision': 0.06489549442122848, 'validation/num_examples': 43793, 'test/accuracy': 0.983172008749056, 'test/loss': 0.06365763396024704, 'test/mean_average_precision': 0.06641752277416993, 'test/num_examples': 43793, 'score': 722.9454674720764, 'total_duration': 986.5150580406189, 'global_step': 2788, 'preemption_count': 0}), (3712, {'train/accuracy': 0.9867895758465811, 'train/loss': 0.04929176717996597, 'train/mean_average_precision': 0.09091147939026324, 'validation/accuracy': 0.9842101246481509, 'validation/loss': 0.058382000774145126, 'validation/mean_average_precision': 0.09221966955377464, 'validation/num_examples': 43793, 'test/accuracy': 0.9832200249431492, 'test/loss': 0.0617007240653038, 'test/mean_average_precision': 0.0944233435150632, 'test/num_examples': 43793, 'score': 962.1837034225464, 'total_duration': 1288.0899963378906, 'global_step': 3712, 'preemption_count': 0}), (4638, {'train/accuracy': 0.9869588216785967, 'train/loss': 0.04732585698366165, 'train/mean_average_precision': 0.12110401271518415, 'validation/accuracy': 0.9843351543833071, 'validation/loss': 0.05675164610147476, 'validation/mean_average_precision': 0.11173895729012102, 'validation/num_examples': 43793, 'test/accuracy': 0.9833543860476733, 'test/loss': 0.06006234884262085, 'test/mean_average_precision': 0.1169843667075113, 'test/num_examples': 43793, 'score': 1201.3266055583954, 'total_duration': 1589.5531980991364, 'global_step': 4638, 'preemption_count': 0}), (5568, {'train/accuracy': 0.9875977356103284, 'train/loss': 0.0441487580537796, 'train/mean_average_precision': 0.13656302257405117, 'validation/accuracy': 0.9849018475984954, 'validation/loss': 0.05397336557507515, 'validation/mean_average_precision': 0.13130454572503797, 'validation/num_examples': 43793, 'test/accuracy': 0.9838770886517937, 'test/loss': 0.05709874629974365, 'test/mean_average_precision': 0.1318661803538981, 'test/num_examples': 43793, 'score': 1440.3769493103027, 'total_duration': 1891.4180085659027, 'global_step': 5568, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9874159536473895, 'train/loss': 0.043923649936914444, 'train/mean_average_precision': 0.14710723153238536, 'validation/accuracy': 0.9846862930875606, 'validation/loss': 0.053403474390506744, 'validation/mean_average_precision': 0.1386795443624961, 'validation/num_examples': 43793, 'test/accuracy': 0.9837317764854588, 'test/loss': 0.056378792971372604, 'test/mean_average_precision': 0.13998484228555372, 'test/num_examples': 43793, 'score': 1551.7181656360626, 'total_duration': 2065.5258355140686, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0401 02:32:49.754694 139639207950144 submission_runner.py:546] Timing: 1551.7181656360626
I0401 02:32:49.754739 139639207950144 submission_runner.py:547] ====================
I0401 02:32:49.754832 139639207950144 submission_runner.py:606] Final ogbg score: 1551.7181656360626
