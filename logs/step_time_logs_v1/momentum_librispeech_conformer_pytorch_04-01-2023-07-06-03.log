WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 07:06:25.328689 139667657197376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 07:06:25.328733 139842364495680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 07:06:25.329425 140250795460416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 07:06:25.329541 140670688008000 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 07:06:25.329624 140557144405824 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 07:06:25.329667 140076935726912 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 07:06:25.329709 140300219410240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 07:06:25.339950 140250795460416 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.339850 140663657273152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 07:06:25.340136 140663657273152 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.340109 140670688008000 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.340137 140557144405824 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.340209 140300219410240 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.340279 140076935726912 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.349594 139842364495680 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.349653 139667657197376 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:25.880695 140663657273152 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/librispeech_conformer_pytorch.
W0401 07:06:25.981011 140250795460416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.981519 140670688008000 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.981537 140076935726912 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.981832 139667657197376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.982183 140663657273152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.982521 139842364495680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.983192 140557144405824 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:25.986043 140300219410240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 07:06:25.986950 140663657273152 submission_runner.py:504] Using RNG seed 1175101327
I0401 07:06:25.987978 140663657273152 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 07:06:25.988097 140663657273152 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1.
I0401 07:06:25.988274 140663657273152 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/hparams.json.
I0401 07:06:25.989254 140663657273152 submission_runner.py:230] Starting train once: RAM USED (GB) 5.72735488
I0401 07:06:25.989350 140663657273152 submission_runner.py:231] Initializing dataset.
I0401 07:06:25.989442 140663657273152 input_pipeline.py:20] Loading split = train-clean-100
I0401 07:06:26.016549 140663657273152 input_pipeline.py:20] Loading split = train-clean-360
I0401 07:06:26.335595 140663657273152 input_pipeline.py:20] Loading split = train-other-500
I0401 07:06:26.759230 140663657273152 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 6.056763392
I0401 07:06:26.759413 140663657273152 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0401 07:06:33.664210 140663657273152 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.204804608
I0401 07:06:33.664437 140663657273152 submission_runner.py:252] Initializing optimizer.
I0401 07:06:34.214291 140663657273152 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.197005824
I0401 07:06:34.214471 140663657273152 submission_runner.py:261] Initializing metrics bundle.
I0401 07:06:34.214520 140663657273152 submission_runner.py:275] Initializing checkpoint and logger.
I0401 07:06:34.215718 140663657273152 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 07:06:34.215839 140663657273152 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 07:06:34.952296 140663657273152 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0401 07:06:34.953277 140663657273152 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0401 07:06:34.958093 140663657273152 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 19.201961984
I0401 07:06:34.959209 140663657273152 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.201961984
I0401 07:06:34.959305 140663657273152 submission_runner.py:312] Starting training loop.
I0401 07:06:36.584200 140663657273152 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 24.891650048
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0401 07:06:41.447040 140637406365440 logging_writer.py:48] [0] global_step=0, grad_norm=27.399818, loss=33.516876
I0401 07:06:41.459708 140663657273152 submission.py:139] 0) loss = 33.517, grad_norm = 27.400
I0401 07:06:41.460488 140663657273152 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 32.944234496
I0401 07:06:41.461100 140663657273152 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 32.944234496
I0401 07:06:41.461253 140663657273152 spec.py:298] Evaluating on the training split.
I0401 07:06:41.462003 140663657273152 input_pipeline.py:20] Loading split = train-clean-100
I0401 07:06:41.487967 140663657273152 input_pipeline.py:20] Loading split = train-clean-360
I0401 07:06:41.901852 140663657273152 input_pipeline.py:20] Loading split = train-other-500
I0401 07:06:55.226795 140663657273152 spec.py:310] Evaluating on the validation split.
I0401 07:06:55.227982 140663657273152 input_pipeline.py:20] Loading split = dev-clean
I0401 07:06:55.231786 140663657273152 input_pipeline.py:20] Loading split = dev-other
I0401 07:07:04.885347 140663657273152 spec.py:326] Evaluating on the test split.
I0401 07:07:04.886493 140663657273152 input_pipeline.py:20] Loading split = test-clean
I0401 07:07:10.273890 140663657273152 submission_runner.py:380] Time since start: 6.50s, 	Step: 1, 	{'train/ctc_loss': 32.69260718816068, 'train/wer': 1.829593397177966, 'validation/ctc_loss': 31.4420992816958, 'validation/wer': 1.7364070873364554, 'validation/num_examples': 5348, 'test/ctc_loss': 31.578443770565844, 'test/wer': 1.772327503909979, 'test/num_examples': 2472}
I0401 07:07:10.274662 140663657273152 submission_runner.py:390] After eval at step 1: RAM USED (GB) 46.709202944
I0401 07:07:10.290367 140636202534656 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.500237, test/ctc_loss=31.578444, test/num_examples=2472, test/wer=1.772328, total_duration=6.502395, train/ctc_loss=32.692607, train/wer=1.829593, validation/ctc_loss=31.442099, validation/num_examples=5348, validation/wer=1.736407
I0401 07:07:10.679424 140663657273152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0401 07:07:10.679986 140663657273152 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 46.735659008
I0401 07:07:10.685420 140663657273152 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 46.747561984
I0401 07:07:10.728161 140663657273152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728241 140670688008000 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728283 140076935726912 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728272 140250795460416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728249 140300219410240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728252 140557144405824 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728348 139842364495680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:10.728771 139667657197376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:11.767458 140635046672128 logging_writer.py:48] [1] global_step=1, grad_norm=26.337961, loss=32.965374
I0401 07:07:11.770838 140663657273152 submission.py:139] 1) loss = 32.965, grad_norm = 26.338
I0401 07:07:11.771555 140663657273152 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 46.985699328
I0401 07:07:12.653366 140636202534656 logging_writer.py:48] [2] global_step=2, grad_norm=28.958426, loss=33.345745
I0401 07:07:12.657168 140663657273152 submission.py:139] 2) loss = 33.346, grad_norm = 28.958
I0401 07:07:13.662863 140635046672128 logging_writer.py:48] [3] global_step=3, grad_norm=33.291328, loss=33.165894
I0401 07:07:13.666865 140663657273152 submission.py:139] 3) loss = 33.166, grad_norm = 33.291
I0401 07:07:14.468616 140636202534656 logging_writer.py:48] [4] global_step=4, grad_norm=48.000828, loss=31.923996
I0401 07:07:14.471971 140663657273152 submission.py:139] 4) loss = 31.924, grad_norm = 48.001
I0401 07:07:15.276052 140635046672128 logging_writer.py:48] [5] global_step=5, grad_norm=80.489861, loss=29.977304
I0401 07:07:15.280088 140663657273152 submission.py:139] 5) loss = 29.977, grad_norm = 80.490
I0401 07:07:16.085343 140636202534656 logging_writer.py:48] [6] global_step=6, grad_norm=128.738876, loss=24.330664
I0401 07:07:16.089289 140663657273152 submission.py:139] 6) loss = 24.331, grad_norm = 128.739
I0401 07:07:16.889152 140635046672128 logging_writer.py:48] [7] global_step=7, grad_norm=118.715042, loss=11.840804
I0401 07:07:16.892429 140663657273152 submission.py:139] 7) loss = 11.841, grad_norm = 118.715
I0401 07:07:17.695278 140636202534656 logging_writer.py:48] [8] global_step=8, grad_norm=20.129910, loss=7.762345
I0401 07:07:17.699490 140663657273152 submission.py:139] 8) loss = 7.762, grad_norm = 20.130
I0401 07:07:18.499330 140635046672128 logging_writer.py:48] [9] global_step=9, grad_norm=27.766817, loss=10.028401
I0401 07:07:18.503423 140663657273152 submission.py:139] 9) loss = 10.028, grad_norm = 27.767
I0401 07:07:19.303164 140636202534656 logging_writer.py:48] [10] global_step=10, grad_norm=28.326908, loss=11.293650
I0401 07:07:19.306677 140663657273152 submission.py:139] 10) loss = 11.294, grad_norm = 28.327
I0401 07:07:20.106225 140635046672128 logging_writer.py:48] [11] global_step=11, grad_norm=29.157373, loss=11.611957
I0401 07:07:20.110168 140663657273152 submission.py:139] 11) loss = 11.612, grad_norm = 29.157
I0401 07:07:20.911599 140636202534656 logging_writer.py:48] [12] global_step=12, grad_norm=30.219997, loss=11.038487
I0401 07:07:20.915485 140663657273152 submission.py:139] 12) loss = 11.038, grad_norm = 30.220
I0401 07:07:21.712451 140635046672128 logging_writer.py:48] [13] global_step=13, grad_norm=30.852016, loss=9.571436
I0401 07:07:21.716048 140663657273152 submission.py:139] 13) loss = 9.571, grad_norm = 30.852
I0401 07:07:22.517725 140636202534656 logging_writer.py:48] [14] global_step=14, grad_norm=20.611032, loss=7.453219
I0401 07:07:22.521142 140663657273152 submission.py:139] 14) loss = 7.453, grad_norm = 20.611
I0401 07:07:23.324924 140635046672128 logging_writer.py:48] [15] global_step=15, grad_norm=89.567596, loss=9.597250
I0401 07:07:23.328697 140663657273152 submission.py:139] 15) loss = 9.597, grad_norm = 89.568
I0401 07:07:24.129815 140636202534656 logging_writer.py:48] [16] global_step=16, grad_norm=97.553314, loss=10.026199
I0401 07:07:24.133670 140663657273152 submission.py:139] 16) loss = 10.026, grad_norm = 97.553
I0401 07:07:24.932502 140635046672128 logging_writer.py:48] [17] global_step=17, grad_norm=20.469572, loss=7.354820
I0401 07:07:24.936149 140663657273152 submission.py:139] 17) loss = 7.355, grad_norm = 20.470
I0401 07:07:25.737624 140636202534656 logging_writer.py:48] [18] global_step=18, grad_norm=30.037979, loss=9.327581
I0401 07:07:25.741016 140663657273152 submission.py:139] 18) loss = 9.328, grad_norm = 30.038
I0401 07:07:26.541028 140635046672128 logging_writer.py:48] [19] global_step=19, grad_norm=29.786787, loss=10.342908
I0401 07:07:26.544524 140663657273152 submission.py:139] 19) loss = 10.343, grad_norm = 29.787
I0401 07:07:27.345640 140636202534656 logging_writer.py:48] [20] global_step=20, grad_norm=29.298100, loss=10.199403
I0401 07:07:27.349155 140663657273152 submission.py:139] 20) loss = 10.199, grad_norm = 29.298
I0401 07:07:28.149919 140635046672128 logging_writer.py:48] [21] global_step=21, grad_norm=28.114964, loss=8.887344
I0401 07:07:28.153978 140663657273152 submission.py:139] 21) loss = 8.887, grad_norm = 28.115
I0401 07:07:28.957334 140636202534656 logging_writer.py:48] [22] global_step=22, grad_norm=14.411420, loss=6.944200
I0401 07:07:28.960814 140663657273152 submission.py:139] 22) loss = 6.944, grad_norm = 14.411
I0401 07:07:29.767883 140635046672128 logging_writer.py:48] [23] global_step=23, grad_norm=76.765694, loss=9.310980
I0401 07:07:29.771240 140663657273152 submission.py:139] 23) loss = 9.311, grad_norm = 76.766
I0401 07:07:30.574665 140636202534656 logging_writer.py:48] [24] global_step=24, grad_norm=53.640038, loss=7.993597
I0401 07:07:30.577889 140663657273152 submission.py:139] 24) loss = 7.994, grad_norm = 53.640
I0401 07:07:31.379228 140635046672128 logging_writer.py:48] [25] global_step=25, grad_norm=20.485146, loss=7.258110
I0401 07:07:31.382925 140663657273152 submission.py:139] 25) loss = 7.258, grad_norm = 20.485
I0401 07:07:32.182836 140636202534656 logging_writer.py:48] [26] global_step=26, grad_norm=26.546640, loss=8.782575
I0401 07:07:32.186301 140663657273152 submission.py:139] 26) loss = 8.783, grad_norm = 26.547
I0401 07:07:32.988790 140635046672128 logging_writer.py:48] [27] global_step=27, grad_norm=26.655579, loss=9.170518
I0401 07:07:32.992248 140663657273152 submission.py:139] 27) loss = 9.171, grad_norm = 26.656
I0401 07:07:33.796523 140636202534656 logging_writer.py:48] [28] global_step=28, grad_norm=25.170559, loss=8.258915
I0401 07:07:33.799929 140663657273152 submission.py:139] 28) loss = 8.259, grad_norm = 25.171
I0401 07:07:34.599393 140635046672128 logging_writer.py:48] [29] global_step=29, grad_norm=10.375242, loss=6.670764
I0401 07:07:34.602601 140663657273152 submission.py:139] 29) loss = 6.671, grad_norm = 10.375
I0401 07:07:35.403875 140636202534656 logging_writer.py:48] [30] global_step=30, grad_norm=68.082756, loss=8.933574
I0401 07:07:35.407008 140663657273152 submission.py:139] 30) loss = 8.934, grad_norm = 68.083
I0401 07:07:36.207772 140635046672128 logging_writer.py:48] [31] global_step=31, grad_norm=26.203653, loss=6.905921
I0401 07:07:36.211047 140663657273152 submission.py:139] 31) loss = 6.906, grad_norm = 26.204
I0401 07:07:37.010589 140636202534656 logging_writer.py:48] [32] global_step=32, grad_norm=20.368532, loss=7.200405
I0401 07:07:37.013839 140663657273152 submission.py:139] 32) loss = 7.200, grad_norm = 20.369
I0401 07:07:37.816530 140635046672128 logging_writer.py:48] [33] global_step=33, grad_norm=24.489168, loss=8.190593
I0401 07:07:37.819960 140663657273152 submission.py:139] 33) loss = 8.191, grad_norm = 24.489
I0401 07:07:38.620740 140636202534656 logging_writer.py:48] [34] global_step=34, grad_norm=23.759369, loss=7.866061
I0401 07:07:38.623716 140663657273152 submission.py:139] 34) loss = 7.866, grad_norm = 23.759
I0401 07:07:39.427981 140635046672128 logging_writer.py:48] [35] global_step=35, grad_norm=13.611271, loss=6.584636
I0401 07:07:39.431279 140663657273152 submission.py:139] 35) loss = 6.585, grad_norm = 13.611
I0401 07:07:40.228483 140636202534656 logging_writer.py:48] [36] global_step=36, grad_norm=48.784397, loss=7.681509
I0401 07:07:40.231930 140663657273152 submission.py:139] 36) loss = 7.682, grad_norm = 48.784
I0401 07:07:41.034176 140635046672128 logging_writer.py:48] [37] global_step=37, grad_norm=22.849119, loss=6.659821
I0401 07:07:41.037527 140663657273152 submission.py:139] 37) loss = 6.660, grad_norm = 22.849
I0401 07:07:41.840885 140636202534656 logging_writer.py:48] [38] global_step=38, grad_norm=17.374681, loss=6.803302
I0401 07:07:41.844168 140663657273152 submission.py:139] 38) loss = 6.803, grad_norm = 17.375
I0401 07:07:42.644594 140635046672128 logging_writer.py:48] [39] global_step=39, grad_norm=22.043570, loss=7.469678
I0401 07:07:42.648308 140663657273152 submission.py:139] 39) loss = 7.470, grad_norm = 22.044
I0401 07:07:43.447887 140636202534656 logging_writer.py:48] [40] global_step=40, grad_norm=18.831911, loss=6.911489
I0401 07:07:43.450979 140663657273152 submission.py:139] 40) loss = 6.911, grad_norm = 18.832
I0401 07:07:44.252242 140635046672128 logging_writer.py:48] [41] global_step=41, grad_norm=9.773069, loss=6.326798
I0401 07:07:44.255516 140663657273152 submission.py:139] 41) loss = 6.327, grad_norm = 9.773
I0401 07:07:45.059013 140636202534656 logging_writer.py:48] [42] global_step=42, grad_norm=46.085258, loss=7.505277
I0401 07:07:45.062261 140663657273152 submission.py:139] 42) loss = 7.505, grad_norm = 46.085
I0401 07:07:45.863809 140635046672128 logging_writer.py:48] [43] global_step=43, grad_norm=11.123581, loss=6.395127
I0401 07:07:45.867465 140663657273152 submission.py:139] 43) loss = 6.395, grad_norm = 11.124
I0401 07:07:46.669682 140636202534656 logging_writer.py:48] [44] global_step=44, grad_norm=20.963737, loss=7.193863
I0401 07:07:46.673059 140663657273152 submission.py:139] 44) loss = 7.194, grad_norm = 20.964
I0401 07:07:47.475235 140635046672128 logging_writer.py:48] [45] global_step=45, grad_norm=19.207108, loss=6.921072
I0401 07:07:47.478492 140663657273152 submission.py:139] 45) loss = 6.921, grad_norm = 19.207
I0401 07:07:48.279284 140636202534656 logging_writer.py:48] [46] global_step=46, grad_norm=3.953632, loss=6.174126
I0401 07:07:48.282493 140663657273152 submission.py:139] 46) loss = 6.174, grad_norm = 3.954
I0401 07:07:49.084539 140635046672128 logging_writer.py:48] [47] global_step=47, grad_norm=46.377075, loss=7.480912
I0401 07:07:49.087925 140663657273152 submission.py:139] 47) loss = 7.481, grad_norm = 46.377
I0401 07:07:49.887913 140636202534656 logging_writer.py:48] [48] global_step=48, grad_norm=11.331545, loss=6.322827
I0401 07:07:49.891366 140663657273152 submission.py:139] 48) loss = 6.323, grad_norm = 11.332
I0401 07:07:50.690423 140635046672128 logging_writer.py:48] [49] global_step=49, grad_norm=20.811243, loss=7.148865
I0401 07:07:50.693536 140663657273152 submission.py:139] 49) loss = 7.149, grad_norm = 20.811
I0401 07:07:51.492993 140636202534656 logging_writer.py:48] [50] global_step=50, grad_norm=18.371889, loss=6.775952
I0401 07:07:51.496189 140663657273152 submission.py:139] 50) loss = 6.776, grad_norm = 18.372
I0401 07:07:52.295826 140635046672128 logging_writer.py:48] [51] global_step=51, grad_norm=11.103289, loss=6.195139
I0401 07:07:52.299043 140663657273152 submission.py:139] 51) loss = 6.195, grad_norm = 11.103
I0401 07:07:53.099967 140636202534656 logging_writer.py:48] [52] global_step=52, grad_norm=39.488323, loss=7.127660
I0401 07:07:53.103574 140663657273152 submission.py:139] 52) loss = 7.128, grad_norm = 39.488
I0401 07:07:53.901797 140635046672128 logging_writer.py:48] [53] global_step=53, grad_norm=14.816712, loss=6.452648
I0401 07:07:53.905688 140663657273152 submission.py:139] 53) loss = 6.453, grad_norm = 14.817
I0401 07:07:54.705311 140636202534656 logging_writer.py:48] [54] global_step=54, grad_norm=20.886936, loss=7.202210
I0401 07:07:54.708961 140663657273152 submission.py:139] 54) loss = 7.202, grad_norm = 20.887
I0401 07:07:55.508113 140635046672128 logging_writer.py:48] [55] global_step=55, grad_norm=16.020386, loss=6.523417
I0401 07:07:55.511238 140663657273152 submission.py:139] 55) loss = 6.523, grad_norm = 16.020
I0401 07:07:56.317525 140636202534656 logging_writer.py:48] [56] global_step=56, grad_norm=27.365551, loss=6.574260
I0401 07:07:56.320634 140663657273152 submission.py:139] 56) loss = 6.574, grad_norm = 27.366
I0401 07:07:57.125729 140635046672128 logging_writer.py:48] [57] global_step=57, grad_norm=16.631678, loss=6.268315
I0401 07:07:57.128973 140663657273152 submission.py:139] 57) loss = 6.268, grad_norm = 16.632
I0401 07:07:57.926558 140636202534656 logging_writer.py:48] [58] global_step=58, grad_norm=16.114002, loss=6.506738
I0401 07:07:57.930537 140663657273152 submission.py:139] 58) loss = 6.507, grad_norm = 16.114
I0401 07:07:58.731421 140635046672128 logging_writer.py:48] [59] global_step=59, grad_norm=17.765085, loss=6.681347
I0401 07:07:58.735090 140663657273152 submission.py:139] 59) loss = 6.681, grad_norm = 17.765
I0401 07:07:59.534251 140636202534656 logging_writer.py:48] [60] global_step=60, grad_norm=0.661983, loss=6.042180
I0401 07:07:59.537534 140663657273152 submission.py:139] 60) loss = 6.042, grad_norm = 0.662
I0401 07:08:00.340996 140635046672128 logging_writer.py:48] [61] global_step=61, grad_norm=37.654018, loss=6.990507
I0401 07:08:00.344487 140663657273152 submission.py:139] 61) loss = 6.991, grad_norm = 37.654
I0401 07:08:01.148746 140636202534656 logging_writer.py:48] [62] global_step=62, grad_norm=13.999743, loss=6.345721
I0401 07:08:01.151867 140663657273152 submission.py:139] 62) loss = 6.346, grad_norm = 14.000
I0401 07:08:01.953234 140635046672128 logging_writer.py:48] [63] global_step=63, grad_norm=19.506321, loss=6.916814
I0401 07:08:01.956441 140663657273152 submission.py:139] 63) loss = 6.917, grad_norm = 19.506
I0401 07:08:02.758061 140636202534656 logging_writer.py:48] [64] global_step=64, grad_norm=10.328506, loss=6.153021
I0401 07:08:02.761432 140663657273152 submission.py:139] 64) loss = 6.153, grad_norm = 10.329
I0401 07:08:03.563466 140635046672128 logging_writer.py:48] [65] global_step=65, grad_norm=42.299686, loss=7.239612
I0401 07:08:03.566681 140663657273152 submission.py:139] 65) loss = 7.240, grad_norm = 42.300
I0401 07:08:04.369428 140636202534656 logging_writer.py:48] [66] global_step=66, grad_norm=12.429802, loss=6.227052
I0401 07:08:04.373287 140663657273152 submission.py:139] 66) loss = 6.227, grad_norm = 12.430
I0401 07:08:05.176945 140635046672128 logging_writer.py:48] [67] global_step=67, grad_norm=19.024706, loss=6.834850
I0401 07:08:05.180814 140663657273152 submission.py:139] 67) loss = 6.835, grad_norm = 19.025
I0401 07:08:05.983851 140636202534656 logging_writer.py:48] [68] global_step=68, grad_norm=9.291956, loss=6.107647
I0401 07:08:05.987871 140663657273152 submission.py:139] 68) loss = 6.108, grad_norm = 9.292
I0401 07:08:06.789025 140635046672128 logging_writer.py:48] [69] global_step=69, grad_norm=44.241924, loss=7.338748
I0401 07:08:06.792470 140663657273152 submission.py:139] 69) loss = 7.339, grad_norm = 44.242
I0401 07:08:07.594584 140636202534656 logging_writer.py:48] [70] global_step=70, grad_norm=16.238325, loss=6.491173
I0401 07:08:07.597951 140663657273152 submission.py:139] 70) loss = 6.491, grad_norm = 16.238
I0401 07:08:08.401319 140635046672128 logging_writer.py:48] [71] global_step=71, grad_norm=20.446960, loss=7.165929
I0401 07:08:08.404506 140663657273152 submission.py:139] 71) loss = 7.166, grad_norm = 20.447
I0401 07:08:09.210127 140636202534656 logging_writer.py:48] [72] global_step=72, grad_norm=10.861658, loss=6.134689
I0401 07:08:09.213636 140663657273152 submission.py:139] 72) loss = 6.135, grad_norm = 10.862
I0401 07:08:10.013755 140635046672128 logging_writer.py:48] [73] global_step=73, grad_norm=53.704979, loss=7.950483
I0401 07:08:10.017009 140663657273152 submission.py:139] 73) loss = 7.950, grad_norm = 53.705
I0401 07:08:10.820420 140636202534656 logging_writer.py:48] [74] global_step=74, grad_norm=18.937534, loss=6.867750
I0401 07:08:10.823557 140663657273152 submission.py:139] 74) loss = 6.868, grad_norm = 18.938
I0401 07:08:11.624854 140635046672128 logging_writer.py:48] [75] global_step=75, grad_norm=22.180517, loss=7.968752
I0401 07:08:11.628371 140663657273152 submission.py:139] 75) loss = 7.969, grad_norm = 22.181
I0401 07:08:12.430658 140636202534656 logging_writer.py:48] [76] global_step=76, grad_norm=18.250891, loss=6.753235
I0401 07:08:12.433887 140663657273152 submission.py:139] 76) loss = 6.753, grad_norm = 18.251
I0401 07:08:13.238077 140635046672128 logging_writer.py:48] [77] global_step=77, grad_norm=49.642864, loss=7.714171
I0401 07:08:13.241367 140663657273152 submission.py:139] 77) loss = 7.714, grad_norm = 49.643
I0401 07:08:14.045632 140636202534656 logging_writer.py:48] [78] global_step=78, grad_norm=12.451952, loss=6.243272
I0401 07:08:14.049579 140663657273152 submission.py:139] 78) loss = 6.243, grad_norm = 12.452
I0401 07:08:14.850573 140635046672128 logging_writer.py:48] [79] global_step=79, grad_norm=19.071884, loss=6.927876
I0401 07:08:14.853982 140663657273152 submission.py:139] 79) loss = 6.928, grad_norm = 19.072
I0401 07:08:15.655485 140636202534656 logging_writer.py:48] [80] global_step=80, grad_norm=7.844329, loss=6.047925
I0401 07:08:15.658835 140663657273152 submission.py:139] 80) loss = 6.048, grad_norm = 7.844
I0401 07:08:16.463694 140635046672128 logging_writer.py:48] [81] global_step=81, grad_norm=48.620934, loss=7.647954
I0401 07:08:16.466941 140663657273152 submission.py:139] 81) loss = 7.648, grad_norm = 48.621
I0401 07:08:17.271982 140636202534656 logging_writer.py:48] [82] global_step=82, grad_norm=19.896358, loss=7.138694
I0401 07:08:17.275529 140663657273152 submission.py:139] 82) loss = 7.139, grad_norm = 19.896
I0401 07:08:18.078014 140635046672128 logging_writer.py:48] [83] global_step=83, grad_norm=22.027622, loss=8.207329
I0401 07:08:18.081145 140663657273152 submission.py:139] 83) loss = 8.207, grad_norm = 22.028
I0401 07:08:18.881077 140636202534656 logging_writer.py:48] [84] global_step=84, grad_norm=17.744141, loss=6.726929
I0401 07:08:18.884341 140663657273152 submission.py:139] 84) loss = 6.727, grad_norm = 17.744
I0401 07:08:19.684906 140635046672128 logging_writer.py:48] [85] global_step=85, grad_norm=61.989967, loss=8.743251
I0401 07:08:19.688210 140663657273152 submission.py:139] 85) loss = 8.743, grad_norm = 61.990
I0401 07:08:20.493592 140636202534656 logging_writer.py:48] [86] global_step=86, grad_norm=19.765755, loss=7.153287
I0401 07:08:20.496670 140663657273152 submission.py:139] 86) loss = 7.153, grad_norm = 19.766
I0401 07:08:21.300948 140635046672128 logging_writer.py:48] [87] global_step=87, grad_norm=22.124252, loss=8.572393
I0401 07:08:21.304268 140663657273152 submission.py:139] 87) loss = 8.572, grad_norm = 22.124
I0401 07:08:22.106808 140636202534656 logging_writer.py:48] [88] global_step=88, grad_norm=19.764917, loss=7.203598
I0401 07:08:22.110416 140663657273152 submission.py:139] 88) loss = 7.204, grad_norm = 19.765
I0401 07:08:22.909652 140635046672128 logging_writer.py:48] [89] global_step=89, grad_norm=47.491348, loss=7.654329
I0401 07:08:22.912840 140663657273152 submission.py:139] 89) loss = 7.654, grad_norm = 47.491
I0401 07:08:23.718328 140636202534656 logging_writer.py:48] [90] global_step=90, grad_norm=11.856826, loss=6.192779
I0401 07:08:23.722639 140663657273152 submission.py:139] 90) loss = 6.193, grad_norm = 11.857
I0401 07:08:24.528161 140635046672128 logging_writer.py:48] [91] global_step=91, grad_norm=17.524561, loss=6.734680
I0401 07:08:24.531410 140663657273152 submission.py:139] 91) loss = 6.735, grad_norm = 17.525
I0401 07:08:25.338482 140636202534656 logging_writer.py:48] [92] global_step=92, grad_norm=1.599226, loss=5.938000
I0401 07:08:25.341812 140663657273152 submission.py:139] 92) loss = 5.938, grad_norm = 1.599
I0401 07:08:26.141983 140635046672128 logging_writer.py:48] [93] global_step=93, grad_norm=42.500725, loss=7.337693
I0401 07:08:26.145292 140663657273152 submission.py:139] 93) loss = 7.338, grad_norm = 42.501
I0401 07:08:26.946949 140636202534656 logging_writer.py:48] [94] global_step=94, grad_norm=20.419973, loss=7.571615
I0401 07:08:26.950464 140663657273152 submission.py:139] 94) loss = 7.572, grad_norm = 20.420
I0401 07:08:27.755050 140635046672128 logging_writer.py:48] [95] global_step=95, grad_norm=21.669064, loss=8.639133
I0401 07:08:27.758574 140663657273152 submission.py:139] 95) loss = 8.639, grad_norm = 21.669
I0401 07:08:28.564906 140636202534656 logging_writer.py:48] [96] global_step=96, grad_norm=17.902042, loss=6.850836
I0401 07:08:28.568070 140663657273152 submission.py:139] 96) loss = 6.851, grad_norm = 17.902
I0401 07:08:29.372221 140635046672128 logging_writer.py:48] [97] global_step=97, grad_norm=66.288361, loss=9.539897
I0401 07:08:29.375366 140663657273152 submission.py:139] 97) loss = 9.540, grad_norm = 66.288
I0401 07:08:30.177690 140636202534656 logging_writer.py:48] [98] global_step=98, grad_norm=20.442095, loss=7.698852
I0401 07:08:30.180882 140663657273152 submission.py:139] 98) loss = 7.699, grad_norm = 20.442
I0401 07:08:30.982078 140635046672128 logging_writer.py:48] [99] global_step=99, grad_norm=21.823389, loss=9.679365
I0401 07:08:30.985215 140663657273152 submission.py:139] 99) loss = 9.679, grad_norm = 21.823
I0401 07:08:31.790962 140636202534656 logging_writer.py:48] [100] global_step=100, grad_norm=21.226786, loss=8.486744
I0401 07:08:31.794356 140663657273152 submission.py:139] 100) loss = 8.487, grad_norm = 21.227
I0401 07:13:50.987450 140635046672128 logging_writer.py:48] [500] global_step=500, grad_norm=0.466720, loss=5.835173
I0401 07:13:50.991998 140663657273152 submission.py:139] 500) loss = 5.835, grad_norm = 0.467
I0401 07:20:25.997238 140636202534656 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0401 07:20:26.002173 140663657273152 submission.py:139] 1000) loss = nan, grad_norm = nan
I0401 07:26:46.384506 140636202534656 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0401 07:26:46.392509 140663657273152 submission.py:139] 1500) loss = nan, grad_norm = nan
I0401 07:33:04.925998 140635046672128 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0401 07:33:04.930578 140663657273152 submission.py:139] 2000) loss = nan, grad_norm = nan
I0401 07:39:25.224081 140635046672128 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0401 07:39:25.260363 140663657273152 submission.py:139] 2500) loss = nan, grad_norm = nan
I0401 07:45:43.847863 140635038279424 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0401 07:45:43.853588 140663657273152 submission.py:139] 3000) loss = nan, grad_norm = nan
I0401 07:47:11.382866 140663657273152 submission_runner.py:371] Before eval at step 3114: RAM USED (GB) 40.987283456
I0401 07:47:11.383276 140663657273152 spec.py:298] Evaluating on the training split.
I0401 07:47:21.172964 140663657273152 spec.py:310] Evaluating on the validation split.
I0401 07:47:30.530972 140663657273152 spec.py:326] Evaluating on the test split.
I0401 07:47:35.617161 140663657273152 submission_runner.py:380] Time since start: 2436.07s, 	Step: 3114, 	{'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 07:47:35.617888 140663657273152 submission_runner.py:390] After eval at step 3114: RAM USED (GB) 39.461818368
I0401 07:47:35.632828 140635046672128 logging_writer.py:48] [3114] global_step=3114, preemption_count=0, score=1301.724640, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2436.065052, train/ctc_loss=nan, train/wer=0.941694, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0401 07:47:36.029381 140663657273152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_3114.
I0401 07:47:36.030054 140663657273152 submission_runner.py:409] After logging and checkpointing eval at step 3114: RAM USED (GB) 39.468621824
I0401 07:52:28.996163 140635038279424 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0401 07:52:29.001194 140663657273152 submission.py:139] 3500) loss = nan, grad_norm = nan
I0401 07:58:47.686669 140635046672128 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0401 07:58:47.692167 140663657273152 submission.py:139] 4000) loss = nan, grad_norm = nan
I0401 08:05:08.093867 140635038279424 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0401 08:05:08.216890 140663657273152 submission.py:139] 4500) loss = nan, grad_norm = nan
I0401 08:11:26.615533 140635029886720 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0401 08:11:26.620604 140663657273152 submission.py:139] 5000) loss = nan, grad_norm = nan
I0401 08:17:46.873534 140635038279424 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0401 08:17:46.880647 140663657273152 submission.py:139] 5500) loss = nan, grad_norm = nan
I0401 08:24:05.411311 140635029886720 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0401 08:24:05.416137 140663657273152 submission.py:139] 6000) loss = nan, grad_norm = nan
I0401 08:27:36.432962 140663657273152 submission_runner.py:371] Before eval at step 6277: RAM USED (GB) 39.65442048
I0401 08:27:36.433397 140663657273152 spec.py:298] Evaluating on the training split.
I0401 08:27:46.031474 140663657273152 spec.py:310] Evaluating on the validation split.
I0401 08:27:55.322216 140663657273152 spec.py:326] Evaluating on the test split.
I0401 08:28:00.586291 140663657273152 submission_runner.py:380] Time since start: 4861.11s, 	Step: 6277, 	{'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 08:28:00.587227 140663657273152 submission_runner.py:390] After eval at step 6277: RAM USED (GB) 39.634731008
I0401 08:28:00.606031 140635038279424 logging_writer.py:48] [6277] global_step=6277, preemption_count=0, score=2562.793048, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4861.112556, train/ctc_loss=nan, train/wer=0.941694, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0401 08:28:01.001979 140663657273152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_6277.
I0401 08:28:01.002694 140663657273152 submission_runner.py:409] After logging and checkpointing eval at step 6277: RAM USED (GB) 39.640809472
I0401 08:30:50.506161 140635029886720 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0401 08:30:50.510372 140663657273152 submission.py:139] 6500) loss = nan, grad_norm = nan
I0401 08:37:09.137766 140635038279424 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0401 08:37:09.141778 140663657273152 submission.py:139] 7000) loss = nan, grad_norm = nan
I0401 08:43:29.375612 140635038279424 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0401 08:43:29.382202 140663657273152 submission.py:139] 7500) loss = nan, grad_norm = nan
I0401 08:49:47.819477 140635029886720 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0401 08:49:47.824002 140663657273152 submission.py:139] 8000) loss = nan, grad_norm = nan
I0401 08:56:08.003258 140635029886720 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0401 08:56:08.010128 140663657273152 submission.py:139] 8500) loss = nan, grad_norm = nan
I0401 09:02:26.410774 140634778236672 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0401 09:02:26.415486 140663657273152 submission.py:139] 9000) loss = nan, grad_norm = nan
I0401 09:08:01.897005 140663657273152 submission_runner.py:371] Before eval at step 9442: RAM USED (GB) 39.881887744
I0401 09:08:01.897461 140663657273152 spec.py:298] Evaluating on the training split.
I0401 09:08:11.648995 140663657273152 spec.py:310] Evaluating on the validation split.
I0401 09:08:21.009250 140663657273152 spec.py:326] Evaluating on the test split.
I0401 09:08:26.321133 140663657273152 submission_runner.py:380] Time since start: 7286.58s, 	Step: 9442, 	{'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 09:08:26.321994 140663657273152 submission_runner.py:390] After eval at step 9442: RAM USED (GB) 39.685341184
I0401 09:08:26.337728 140635029886720 logging_writer.py:48] [9442] global_step=9442, preemption_count=0, score=3823.685249, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7286.575968, train/ctc_loss=nan, train/wer=0.941694, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0401 09:08:26.738784 140663657273152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_9442.
I0401 09:08:26.739396 140663657273152 submission_runner.py:409] After logging and checkpointing eval at step 9442: RAM USED (GB) 39.691956224
I0401 09:09:11.356768 140634778236672 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0401 09:09:11.360814 140663657273152 submission.py:139] 9500) loss = nan, grad_norm = nan
I0401 09:15:29.171713 140663657273152 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 40.075288576
I0401 09:15:29.171922 140663657273152 spec.py:298] Evaluating on the training split.
I0401 09:15:38.456066 140663657273152 spec.py:310] Evaluating on the validation split.
I0401 09:15:47.691501 140663657273152 spec.py:326] Evaluating on the test split.
I0401 09:15:53.419847 140663657273152 submission_runner.py:380] Time since start: 7733.85s, 	Step: 10000, 	{'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 09:15:53.420683 140663657273152 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 40.008126464
I0401 09:15:53.437935 140635029886720 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4045.137978, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7733.852595, train/ctc_loss=nan, train/wer=0.941694, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0401 09:15:53.824534 140663657273152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0401 09:15:53.825170 140663657273152 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 40.01492992
I0401 09:15:53.837641 140634778236672 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4045.137978
I0401 09:15:54.555641 140663657273152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0401 09:15:54.699424 140663657273152 submission_runner.py:543] Tuning trial 1/1
I0401 09:15:54.699646 140663657273152 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 09:15:54.699966 140663657273152 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ctc_loss': 32.69260718816068, 'train/wer': 1.829593397177966, 'validation/ctc_loss': 31.4420992816958, 'validation/wer': 1.7364070873364554, 'validation/num_examples': 5348, 'test/ctc_loss': 31.578443770565844, 'test/wer': 1.772327503909979, 'test/num_examples': 2472, 'score': 6.500236988067627, 'total_duration': 6.502394914627075, 'global_step': 1, 'preemption_count': 0}), (3114, {'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1301.7246398925781, 'total_duration': 2436.0650520324707, 'global_step': 3114, 'preemption_count': 0}), (6277, {'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2562.7930479049683, 'total_duration': 4861.1125564575195, 'global_step': 6277, 'preemption_count': 0}), (9442, {'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 3823.6852493286133, 'total_duration': 7286.575967788696, 'global_step': 9442, 'preemption_count': 0}), (10000, {'train/ctc_loss': nan, 'train/wer': 0.9416938711321757, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4045.1379783153534, 'total_duration': 7733.852595329285, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 09:15:54.700063 140663657273152 submission_runner.py:546] Timing: 4045.1379783153534
I0401 09:15:54.700111 140663657273152 submission_runner.py:547] ====================
I0401 09:15:54.700249 140663657273152 submission_runner.py:606] Final librispeech_conformer score: 4045.1379783153534
