WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 06:06:13.243980 140661947967296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 06:06:13.244031 140652414433088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 06:06:13.244020 139818821744448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 06:06:13.244056 139961263494976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 06:06:14.230872 139661137712960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 06:06:14.230905 140539115226944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 06:06:14.231075 139954977130304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 06:06:14.234400 140657493141312 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 06:06:14.234697 140657493141312 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.241653 139661137712960 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.241677 140539115226944 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.241698 139954977130304 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.243645 140661947967296 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.243911 139818821744448 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.243927 139961263494976 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:14.243906 140652414433088 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:15.413258 140657493141312 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/ogbg_pytorch.
W0402 06:06:15.529185 139818821744448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.529890 139954977130304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.530110 139961263494976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.530554 140539115226944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.530951 140657493141312 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.531115 140661947967296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.531523 139661137712960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:15.532187 140652414433088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 06:06:15.534667 140657493141312 submission_runner.py:511] Using RNG seed 1489774426
I0402 06:06:15.535751 140657493141312 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 06:06:15.535872 140657493141312 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1.
I0402 06:06:15.536093 140657493141312 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/hparams.json.
I0402 06:06:15.537034 140657493141312 submission_runner.py:230] Starting train once: RAM USED (GB) 5.753155584
I0402 06:06:15.537129 140657493141312 submission_runner.py:231] Initializing dataset.
I0402 06:06:15.537286 140657493141312 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.753155584
I0402 06:06:15.537340 140657493141312 submission_runner.py:240] Initializing model.
I0402 06:06:19.356619 140657493141312 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.393902592
I0402 06:06:19.356840 140657493141312 submission_runner.py:252] Initializing optimizer.
I0402 06:06:19.468886 140657493141312 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.395725312
I0402 06:06:19.469060 140657493141312 submission_runner.py:261] Initializing metrics bundle.
I0402 06:06:19.469107 140657493141312 submission_runner.py:276] Initializing checkpoint and logger.
I0402 06:06:19.470345 140657493141312 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 06:06:19.470470 140657493141312 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 06:06:20.124585 140657493141312 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/meta_data_0.json.
I0402 06:06:20.125466 140657493141312 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/flags_0.json.
I0402 06:06:20.159822 140657493141312 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.444893696
I0402 06:06:20.160883 140657493141312 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.444893696
I0402 06:06:20.160999 140657493141312 submission_runner.py:313] Starting training loop.
I0402 06:06:20.386399 140657493141312 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:06:20.392168 140657493141312 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:06:20.491208 140657493141312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:06:21.980000 140657493141312 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 15.67905792
I0402 06:06:25.608774 140618428835584 logging_writer.py:48] [0] global_step=0, grad_norm=2.933774, loss=0.782556
I0402 06:06:25.616210 140657493141312 submission.py:139] 0) loss = 0.783, grad_norm = 2.934
I0402 06:06:25.616768 140657493141312 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 22.068764672
I0402 06:06:25.629017 140657493141312 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 22.068764672
I0402 06:06:25.629122 140657493141312 spec.py:298] Evaluating on the training split.
I0402 06:06:25.633545 140657493141312 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:06:25.636874 140657493141312 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:06:25.687191 140657493141312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0402 06:06:40.627966 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.887087 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.887115 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.887137 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.901310 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.901889 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.901984 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:40.902118 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.410281 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.606867 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.607853 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.613397 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.613509 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.614197 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.614955 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:06:54.615261 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:07:19.222367 140657493141312 spec.py:310] Evaluating on the validation split.
I0402 06:07:19.224980 140657493141312 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:07:19.228694 140657493141312 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:07:19.279851 140657493141312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0402 06:07:32.293585 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.536271 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.536552 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.543015 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.543192 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.543187 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.543626 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:32.543784 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.654504 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.876655 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.876765 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.883381 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.883850 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.883874 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.884188 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:07:37.884258 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:08:02.320971 140657493141312 spec.py:326] Evaluating on the test split.
I0402 06:08:02.323953 140657493141312 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:08:02.328042 140657493141312 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:08:02.386486 140657493141312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0402 06:08:15.370238 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.617084 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.617257 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.623456 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.623497 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.623743 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.623842 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:15.624647 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:20.862159 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.087485 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.088161 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.093753 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.094712 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.094718 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.094822 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:08:21.095463 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:08:46.266881 140657493141312 submission_runner.py:382] Time since start: 5.47s, 	Step: 1, 	{'train/accuracy': 0.47149647026757274, 'train/loss': 0.7833452224731445, 'train/mean_average_precision': 0.023938336418131638, 'validation/accuracy': 0.47010652695811583, 'validation/loss': 0.7843211889266968, 'validation/mean_average_precision': 0.027072326455109562, 'validation/num_examples': 43793, 'test/accuracy': 0.47031946353275356, 'test/loss': 0.7845950722694397, 'test/mean_average_precision': 0.028901715039386926, 'test/num_examples': 43793}
I0402 06:08:46.267323 140657493141312 submission_runner.py:396] After eval at step 1: RAM USED (GB) 26.44819968
I0402 06:08:46.275378 140605535098624 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.466755, test/accuracy=0.470319, test/loss=0.784595, test/mean_average_precision=0.028902, test/num_examples=43793, total_duration=5.468705, train/accuracy=0.471496, train/loss=0.783345, train/mean_average_precision=0.023938, validation/accuracy=0.470107, validation/loss=0.784321, validation/mean_average_precision=0.027072, validation/num_examples=43793
I0402 06:08:46.570298 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_1.
I0402 06:08:46.570773 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 25.9887104
I0402 06:08:46.816774 140657493141312 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 26.029666304
I0402 06:08:46.819497 140657493141312 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.825774 139954977130304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.825974 140539115226944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.826429 140652414433088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.826435 139961263494976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.826448 139818821744448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.826611 140661947967296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.826618 139661137712960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:08:46.854542 140605543491328 logging_writer.py:48] [1] global_step=1, grad_norm=2.937628, loss=0.782684
I0402 06:08:46.859524 140657493141312 submission.py:139] 1) loss = 0.783, grad_norm = 2.938
I0402 06:08:46.860078 140657493141312 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 26.030505984
I0402 06:08:47.109501 140605535098624 logging_writer.py:48] [2] global_step=2, grad_norm=2.932919, loss=0.780640
I0402 06:08:47.113569 140657493141312 submission.py:139] 2) loss = 0.781, grad_norm = 2.933
I0402 06:08:47.367181 140605543491328 logging_writer.py:48] [3] global_step=3, grad_norm=2.903481, loss=0.772007
I0402 06:08:47.370952 140657493141312 submission.py:139] 3) loss = 0.772, grad_norm = 2.903
I0402 06:08:47.627737 140605535098624 logging_writer.py:48] [4] global_step=4, grad_norm=2.815524, loss=0.761695
I0402 06:08:47.631431 140657493141312 submission.py:139] 4) loss = 0.762, grad_norm = 2.816
I0402 06:08:47.889089 140605543491328 logging_writer.py:48] [5] global_step=5, grad_norm=2.656314, loss=0.743903
I0402 06:08:47.892746 140657493141312 submission.py:139] 5) loss = 0.744, grad_norm = 2.656
I0402 06:08:48.143351 140605535098624 logging_writer.py:48] [6] global_step=6, grad_norm=2.447632, loss=0.723938
I0402 06:08:48.146950 140657493141312 submission.py:139] 6) loss = 0.724, grad_norm = 2.448
I0402 06:08:48.396353 140605543491328 logging_writer.py:48] [7] global_step=7, grad_norm=2.018365, loss=0.702045
I0402 06:08:48.400211 140657493141312 submission.py:139] 7) loss = 0.702, grad_norm = 2.018
I0402 06:08:48.650753 140605535098624 logging_writer.py:48] [8] global_step=8, grad_norm=1.921399, loss=0.682275
I0402 06:08:48.654414 140657493141312 submission.py:139] 8) loss = 0.682, grad_norm = 1.921
I0402 06:08:48.906492 140605543491328 logging_writer.py:48] [9] global_step=9, grad_norm=1.739530, loss=0.661542
I0402 06:08:48.910780 140657493141312 submission.py:139] 9) loss = 0.662, grad_norm = 1.740
I0402 06:08:49.162441 140605535098624 logging_writer.py:48] [10] global_step=10, grad_norm=1.692267, loss=0.642205
I0402 06:08:49.166183 140657493141312 submission.py:139] 10) loss = 0.642, grad_norm = 1.692
I0402 06:08:49.422545 140605543491328 logging_writer.py:48] [11] global_step=11, grad_norm=1.560532, loss=0.622102
I0402 06:08:49.426180 140657493141312 submission.py:139] 11) loss = 0.622, grad_norm = 1.561
I0402 06:08:49.684457 140605535098624 logging_writer.py:48] [12] global_step=12, grad_norm=1.432943, loss=0.604274
I0402 06:08:49.688173 140657493141312 submission.py:139] 12) loss = 0.604, grad_norm = 1.433
I0402 06:08:49.947895 140605543491328 logging_writer.py:48] [13] global_step=13, grad_norm=1.321522, loss=0.586348
I0402 06:08:49.951622 140657493141312 submission.py:139] 13) loss = 0.586, grad_norm = 1.322
I0402 06:08:50.207487 140605535098624 logging_writer.py:48] [14] global_step=14, grad_norm=1.280375, loss=0.568016
I0402 06:08:50.211412 140657493141312 submission.py:139] 14) loss = 0.568, grad_norm = 1.280
I0402 06:08:50.467778 140605543491328 logging_writer.py:48] [15] global_step=15, grad_norm=1.199950, loss=0.549405
I0402 06:08:50.471513 140657493141312 submission.py:139] 15) loss = 0.549, grad_norm = 1.200
I0402 06:08:50.724588 140605535098624 logging_writer.py:48] [16] global_step=16, grad_norm=1.107165, loss=0.536718
I0402 06:08:50.728233 140657493141312 submission.py:139] 16) loss = 0.537, grad_norm = 1.107
I0402 06:08:50.982829 140605543491328 logging_writer.py:48] [17] global_step=17, grad_norm=1.024994, loss=0.517873
I0402 06:08:50.986592 140657493141312 submission.py:139] 17) loss = 0.518, grad_norm = 1.025
I0402 06:08:51.242033 140605535098624 logging_writer.py:48] [18] global_step=18, grad_norm=0.966361, loss=0.502571
I0402 06:08:51.246044 140657493141312 submission.py:139] 18) loss = 0.503, grad_norm = 0.966
I0402 06:08:51.504176 140605543491328 logging_writer.py:48] [19] global_step=19, grad_norm=0.882687, loss=0.490692
I0402 06:08:51.507850 140657493141312 submission.py:139] 19) loss = 0.491, grad_norm = 0.883
I0402 06:08:51.759033 140605535098624 logging_writer.py:48] [20] global_step=20, grad_norm=0.806853, loss=0.476401
I0402 06:08:51.762833 140657493141312 submission.py:139] 20) loss = 0.476, grad_norm = 0.807
I0402 06:08:52.018275 140605543491328 logging_writer.py:48] [21] global_step=21, grad_norm=0.775460, loss=0.464222
I0402 06:08:52.022081 140657493141312 submission.py:139] 21) loss = 0.464, grad_norm = 0.775
I0402 06:08:52.277207 140605535098624 logging_writer.py:48] [22] global_step=22, grad_norm=0.707874, loss=0.452483
I0402 06:08:52.280863 140657493141312 submission.py:139] 22) loss = 0.452, grad_norm = 0.708
I0402 06:08:52.536721 140605543491328 logging_writer.py:48] [23] global_step=23, grad_norm=0.670102, loss=0.441247
I0402 06:08:52.540655 140657493141312 submission.py:139] 23) loss = 0.441, grad_norm = 0.670
I0402 06:08:52.795565 140605535098624 logging_writer.py:48] [24] global_step=24, grad_norm=0.646675, loss=0.430636
I0402 06:08:52.799389 140657493141312 submission.py:139] 24) loss = 0.431, grad_norm = 0.647
I0402 06:08:53.052979 140605543491328 logging_writer.py:48] [25] global_step=25, grad_norm=0.629487, loss=0.421034
I0402 06:08:53.056772 140657493141312 submission.py:139] 25) loss = 0.421, grad_norm = 0.629
I0402 06:08:53.309074 140605535098624 logging_writer.py:48] [26] global_step=26, grad_norm=0.613988, loss=0.409706
I0402 06:08:53.312839 140657493141312 submission.py:139] 26) loss = 0.410, grad_norm = 0.614
I0402 06:08:53.569093 140605543491328 logging_writer.py:48] [27] global_step=27, grad_norm=0.576205, loss=0.398354
I0402 06:08:53.572971 140657493141312 submission.py:139] 27) loss = 0.398, grad_norm = 0.576
I0402 06:08:53.829868 140605535098624 logging_writer.py:48] [28] global_step=28, grad_norm=0.522235, loss=0.391346
I0402 06:08:53.834061 140657493141312 submission.py:139] 28) loss = 0.391, grad_norm = 0.522
I0402 06:08:54.088942 140605543491328 logging_writer.py:48] [29] global_step=29, grad_norm=0.498800, loss=0.379989
I0402 06:08:54.092909 140657493141312 submission.py:139] 29) loss = 0.380, grad_norm = 0.499
I0402 06:08:54.347953 140605535098624 logging_writer.py:48] [30] global_step=30, grad_norm=0.481658, loss=0.372355
I0402 06:08:54.351775 140657493141312 submission.py:139] 30) loss = 0.372, grad_norm = 0.482
I0402 06:08:54.610395 140605543491328 logging_writer.py:48] [31] global_step=31, grad_norm=0.464334, loss=0.362542
I0402 06:08:54.614191 140657493141312 submission.py:139] 31) loss = 0.363, grad_norm = 0.464
I0402 06:08:54.890990 140605535098624 logging_writer.py:48] [32] global_step=32, grad_norm=0.445582, loss=0.355744
I0402 06:08:54.895348 140657493141312 submission.py:139] 32) loss = 0.356, grad_norm = 0.446
I0402 06:08:55.167711 140605543491328 logging_writer.py:48] [33] global_step=33, grad_norm=0.431525, loss=0.346645
I0402 06:08:55.171834 140657493141312 submission.py:139] 33) loss = 0.347, grad_norm = 0.432
I0402 06:08:55.447866 140605535098624 logging_writer.py:48] [34] global_step=34, grad_norm=0.417700, loss=0.338231
I0402 06:08:55.452143 140657493141312 submission.py:139] 34) loss = 0.338, grad_norm = 0.418
I0402 06:08:55.729083 140605543491328 logging_writer.py:48] [35] global_step=35, grad_norm=0.403056, loss=0.330790
I0402 06:08:55.733283 140657493141312 submission.py:139] 35) loss = 0.331, grad_norm = 0.403
I0402 06:08:56.011558 140605535098624 logging_writer.py:48] [36] global_step=36, grad_norm=0.388968, loss=0.322564
I0402 06:08:56.015460 140657493141312 submission.py:139] 36) loss = 0.323, grad_norm = 0.389
I0402 06:08:56.311017 140605543491328 logging_writer.py:48] [37] global_step=37, grad_norm=0.377847, loss=0.314877
I0402 06:08:56.314678 140657493141312 submission.py:139] 37) loss = 0.315, grad_norm = 0.378
I0402 06:08:56.594385 140605535098624 logging_writer.py:48] [38] global_step=38, grad_norm=0.366097, loss=0.306442
I0402 06:08:56.597932 140657493141312 submission.py:139] 38) loss = 0.306, grad_norm = 0.366
I0402 06:08:56.874691 140605543491328 logging_writer.py:48] [39] global_step=39, grad_norm=0.356527, loss=0.299292
I0402 06:08:56.878418 140657493141312 submission.py:139] 39) loss = 0.299, grad_norm = 0.357
I0402 06:08:57.156444 140605535098624 logging_writer.py:48] [40] global_step=40, grad_norm=0.343036, loss=0.291860
I0402 06:08:57.160326 140657493141312 submission.py:139] 40) loss = 0.292, grad_norm = 0.343
I0402 06:08:57.439267 140605543491328 logging_writer.py:48] [41] global_step=41, grad_norm=0.334635, loss=0.285353
I0402 06:08:57.442917 140657493141312 submission.py:139] 41) loss = 0.285, grad_norm = 0.335
I0402 06:08:57.716786 140605535098624 logging_writer.py:48] [42] global_step=42, grad_norm=0.324047, loss=0.276655
I0402 06:08:57.721052 140657493141312 submission.py:139] 42) loss = 0.277, grad_norm = 0.324
I0402 06:08:57.997432 140605543491328 logging_writer.py:48] [43] global_step=43, grad_norm=0.315689, loss=0.271462
I0402 06:08:58.001357 140657493141312 submission.py:139] 43) loss = 0.271, grad_norm = 0.316
I0402 06:08:58.280704 140605535098624 logging_writer.py:48] [44] global_step=44, grad_norm=0.307427, loss=0.264423
I0402 06:08:58.285405 140657493141312 submission.py:139] 44) loss = 0.264, grad_norm = 0.307
I0402 06:08:58.561512 140605543491328 logging_writer.py:48] [45] global_step=45, grad_norm=0.299354, loss=0.257827
I0402 06:08:58.565097 140657493141312 submission.py:139] 45) loss = 0.258, grad_norm = 0.299
I0402 06:08:58.839238 140605535098624 logging_writer.py:48] [46] global_step=46, grad_norm=0.292521, loss=0.249601
I0402 06:08:58.843125 140657493141312 submission.py:139] 46) loss = 0.250, grad_norm = 0.293
I0402 06:08:59.115221 140605543491328 logging_writer.py:48] [47] global_step=47, grad_norm=0.282193, loss=0.245554
I0402 06:08:59.118924 140657493141312 submission.py:139] 47) loss = 0.246, grad_norm = 0.282
I0402 06:08:59.393298 140605535098624 logging_writer.py:48] [48] global_step=48, grad_norm=0.275114, loss=0.239954
I0402 06:08:59.396866 140657493141312 submission.py:139] 48) loss = 0.240, grad_norm = 0.275
I0402 06:08:59.679143 140605543491328 logging_writer.py:48] [49] global_step=49, grad_norm=0.267289, loss=0.234375
I0402 06:08:59.682840 140657493141312 submission.py:139] 49) loss = 0.234, grad_norm = 0.267
I0402 06:08:59.958189 140605535098624 logging_writer.py:48] [50] global_step=50, grad_norm=0.262419, loss=0.226350
I0402 06:08:59.962002 140657493141312 submission.py:139] 50) loss = 0.226, grad_norm = 0.262
I0402 06:09:00.239482 140605543491328 logging_writer.py:48] [51] global_step=51, grad_norm=0.255694, loss=0.221639
I0402 06:09:00.243164 140657493141312 submission.py:139] 51) loss = 0.222, grad_norm = 0.256
I0402 06:09:00.519810 140605535098624 logging_writer.py:48] [52] global_step=52, grad_norm=0.247226, loss=0.217629
I0402 06:09:00.523751 140657493141312 submission.py:139] 52) loss = 0.218, grad_norm = 0.247
I0402 06:09:00.801370 140605543491328 logging_writer.py:48] [53] global_step=53, grad_norm=0.240148, loss=0.211194
I0402 06:09:00.805355 140657493141312 submission.py:139] 53) loss = 0.211, grad_norm = 0.240
I0402 06:09:01.080974 140605535098624 logging_writer.py:48] [54] global_step=54, grad_norm=0.232835, loss=0.203597
I0402 06:09:01.084798 140657493141312 submission.py:139] 54) loss = 0.204, grad_norm = 0.233
I0402 06:09:01.363418 140605543491328 logging_writer.py:48] [55] global_step=55, grad_norm=0.228614, loss=0.198556
I0402 06:09:01.367020 140657493141312 submission.py:139] 55) loss = 0.199, grad_norm = 0.229
I0402 06:09:01.639016 140605535098624 logging_writer.py:48] [56] global_step=56, grad_norm=0.220383, loss=0.192715
I0402 06:09:01.643028 140657493141312 submission.py:139] 56) loss = 0.193, grad_norm = 0.220
I0402 06:09:01.927098 140605543491328 logging_writer.py:48] [57] global_step=57, grad_norm=0.210617, loss=0.193162
I0402 06:09:01.931110 140657493141312 submission.py:139] 57) loss = 0.193, grad_norm = 0.211
I0402 06:09:02.220343 140605535098624 logging_writer.py:48] [58] global_step=58, grad_norm=0.207501, loss=0.184476
I0402 06:09:02.224407 140657493141312 submission.py:139] 58) loss = 0.184, grad_norm = 0.208
I0402 06:09:02.501434 140605543491328 logging_writer.py:48] [59] global_step=59, grad_norm=0.201459, loss=0.178585
I0402 06:09:02.504942 140657493141312 submission.py:139] 59) loss = 0.179, grad_norm = 0.201
I0402 06:09:02.782229 140605535098624 logging_writer.py:48] [60] global_step=60, grad_norm=0.193364, loss=0.175850
I0402 06:09:02.786469 140657493141312 submission.py:139] 60) loss = 0.176, grad_norm = 0.193
I0402 06:09:03.062569 140605543491328 logging_writer.py:48] [61] global_step=61, grad_norm=0.186619, loss=0.173814
I0402 06:09:03.066597 140657493141312 submission.py:139] 61) loss = 0.174, grad_norm = 0.187
I0402 06:09:03.346798 140605535098624 logging_writer.py:48] [62] global_step=62, grad_norm=0.182921, loss=0.169140
I0402 06:09:03.351130 140657493141312 submission.py:139] 62) loss = 0.169, grad_norm = 0.183
I0402 06:09:03.624692 140605543491328 logging_writer.py:48] [63] global_step=63, grad_norm=0.176714, loss=0.160629
I0402 06:09:03.628997 140657493141312 submission.py:139] 63) loss = 0.161, grad_norm = 0.177
I0402 06:09:03.908990 140605535098624 logging_writer.py:48] [64] global_step=64, grad_norm=0.172116, loss=0.160557
I0402 06:09:03.913369 140657493141312 submission.py:139] 64) loss = 0.161, grad_norm = 0.172
I0402 06:09:04.198123 140605543491328 logging_writer.py:48] [65] global_step=65, grad_norm=0.165841, loss=0.155150
I0402 06:09:04.202368 140657493141312 submission.py:139] 65) loss = 0.155, grad_norm = 0.166
I0402 06:09:04.476897 140605535098624 logging_writer.py:48] [66] global_step=66, grad_norm=0.158919, loss=0.154688
I0402 06:09:04.480789 140657493141312 submission.py:139] 66) loss = 0.155, grad_norm = 0.159
I0402 06:09:04.754375 140605543491328 logging_writer.py:48] [67] global_step=67, grad_norm=0.157064, loss=0.147077
I0402 06:09:04.758125 140657493141312 submission.py:139] 67) loss = 0.147, grad_norm = 0.157
I0402 06:09:05.028467 140605535098624 logging_writer.py:48] [68] global_step=68, grad_norm=0.151004, loss=0.146888
I0402 06:09:05.032125 140657493141312 submission.py:139] 68) loss = 0.147, grad_norm = 0.151
I0402 06:09:05.313465 140605543491328 logging_writer.py:48] [69] global_step=69, grad_norm=0.148389, loss=0.142800
I0402 06:09:05.316956 140657493141312 submission.py:139] 69) loss = 0.143, grad_norm = 0.148
I0402 06:09:05.594978 140605535098624 logging_writer.py:48] [70] global_step=70, grad_norm=0.144742, loss=0.139596
I0402 06:09:05.598631 140657493141312 submission.py:139] 70) loss = 0.140, grad_norm = 0.145
I0402 06:09:05.876012 140605543491328 logging_writer.py:48] [71] global_step=71, grad_norm=0.139586, loss=0.138710
I0402 06:09:05.879830 140657493141312 submission.py:139] 71) loss = 0.139, grad_norm = 0.140
I0402 06:09:06.157634 140605535098624 logging_writer.py:48] [72] global_step=72, grad_norm=0.135866, loss=0.136645
I0402 06:09:06.161603 140657493141312 submission.py:139] 72) loss = 0.137, grad_norm = 0.136
I0402 06:09:06.440212 140605543491328 logging_writer.py:48] [73] global_step=73, grad_norm=0.130220, loss=0.131733
I0402 06:09:06.444074 140657493141312 submission.py:139] 73) loss = 0.132, grad_norm = 0.130
I0402 06:09:06.720941 140605535098624 logging_writer.py:48] [74] global_step=74, grad_norm=0.124234, loss=0.128258
I0402 06:09:06.724401 140657493141312 submission.py:139] 74) loss = 0.128, grad_norm = 0.124
I0402 06:09:07.002719 140605543491328 logging_writer.py:48] [75] global_step=75, grad_norm=0.124069, loss=0.125746
I0402 06:09:07.006418 140657493141312 submission.py:139] 75) loss = 0.126, grad_norm = 0.124
I0402 06:09:07.283833 140605535098624 logging_writer.py:48] [76] global_step=76, grad_norm=0.121169, loss=0.122809
I0402 06:09:07.287449 140657493141312 submission.py:139] 76) loss = 0.123, grad_norm = 0.121
I0402 06:09:07.566675 140605543491328 logging_writer.py:48] [77] global_step=77, grad_norm=0.118630, loss=0.119232
I0402 06:09:07.570306 140657493141312 submission.py:139] 77) loss = 0.119, grad_norm = 0.119
I0402 06:09:07.849088 140605535098624 logging_writer.py:48] [78] global_step=78, grad_norm=0.114183, loss=0.118495
I0402 06:09:07.853046 140657493141312 submission.py:139] 78) loss = 0.118, grad_norm = 0.114
I0402 06:09:08.126829 140605543491328 logging_writer.py:48] [79] global_step=79, grad_norm=0.111092, loss=0.117870
I0402 06:09:08.130542 140657493141312 submission.py:139] 79) loss = 0.118, grad_norm = 0.111
I0402 06:09:08.396878 140605535098624 logging_writer.py:48] [80] global_step=80, grad_norm=0.110980, loss=0.113978
I0402 06:09:08.400604 140657493141312 submission.py:139] 80) loss = 0.114, grad_norm = 0.111
I0402 06:09:08.673114 140605543491328 logging_writer.py:48] [81] global_step=81, grad_norm=0.102877, loss=0.111812
I0402 06:09:08.676626 140657493141312 submission.py:139] 81) loss = 0.112, grad_norm = 0.103
I0402 06:09:08.946639 140605535098624 logging_writer.py:48] [82] global_step=82, grad_norm=0.103163, loss=0.108594
I0402 06:09:08.950243 140657493141312 submission.py:139] 82) loss = 0.109, grad_norm = 0.103
I0402 06:09:09.221004 140605543491328 logging_writer.py:48] [83] global_step=83, grad_norm=0.101769, loss=0.110660
I0402 06:09:09.224706 140657493141312 submission.py:139] 83) loss = 0.111, grad_norm = 0.102
I0402 06:09:09.516927 140605535098624 logging_writer.py:48] [84] global_step=84, grad_norm=0.097282, loss=0.110032
I0402 06:09:09.520768 140657493141312 submission.py:139] 84) loss = 0.110, grad_norm = 0.097
I0402 06:09:09.804327 140605543491328 logging_writer.py:48] [85] global_step=85, grad_norm=0.094735, loss=0.105940
I0402 06:09:09.807970 140657493141312 submission.py:139] 85) loss = 0.106, grad_norm = 0.095
I0402 06:09:10.090516 140605535098624 logging_writer.py:48] [86] global_step=86, grad_norm=0.093074, loss=0.102436
I0402 06:09:10.094506 140657493141312 submission.py:139] 86) loss = 0.102, grad_norm = 0.093
I0402 06:09:10.367732 140605543491328 logging_writer.py:48] [87] global_step=87, grad_norm=0.088780, loss=0.102052
I0402 06:09:10.371454 140657493141312 submission.py:139] 87) loss = 0.102, grad_norm = 0.089
I0402 06:09:10.649110 140605535098624 logging_writer.py:48] [88] global_step=88, grad_norm=0.086865, loss=0.100208
I0402 06:09:10.652649 140657493141312 submission.py:139] 88) loss = 0.100, grad_norm = 0.087
I0402 06:09:10.930088 140605543491328 logging_writer.py:48] [89] global_step=89, grad_norm=0.085390, loss=0.103022
I0402 06:09:10.933871 140657493141312 submission.py:139] 89) loss = 0.103, grad_norm = 0.085
I0402 06:09:11.210606 140605535098624 logging_writer.py:48] [90] global_step=90, grad_norm=0.083304, loss=0.102434
I0402 06:09:11.214339 140657493141312 submission.py:139] 90) loss = 0.102, grad_norm = 0.083
I0402 06:09:11.488926 140605543491328 logging_writer.py:48] [91] global_step=91, grad_norm=0.082589, loss=0.095341
I0402 06:09:11.492589 140657493141312 submission.py:139] 91) loss = 0.095, grad_norm = 0.083
I0402 06:09:11.764875 140605535098624 logging_writer.py:48] [92] global_step=92, grad_norm=0.078792, loss=0.097538
I0402 06:09:11.768484 140657493141312 submission.py:139] 92) loss = 0.098, grad_norm = 0.079
I0402 06:09:12.042015 140605543491328 logging_writer.py:48] [93] global_step=93, grad_norm=0.076411, loss=0.099398
I0402 06:09:12.046197 140657493141312 submission.py:139] 93) loss = 0.099, grad_norm = 0.076
I0402 06:09:12.325547 140605535098624 logging_writer.py:48] [94] global_step=94, grad_norm=0.077032, loss=0.093859
I0402 06:09:12.329613 140657493141312 submission.py:139] 94) loss = 0.094, grad_norm = 0.077
I0402 06:09:12.601399 140605543491328 logging_writer.py:48] [95] global_step=95, grad_norm=0.073353, loss=0.095363
I0402 06:09:12.605676 140657493141312 submission.py:139] 95) loss = 0.095, grad_norm = 0.073
I0402 06:09:12.881679 140605535098624 logging_writer.py:48] [96] global_step=96, grad_norm=0.071883, loss=0.089579
I0402 06:09:12.885363 140657493141312 submission.py:139] 96) loss = 0.090, grad_norm = 0.072
I0402 06:09:13.166496 140605543491328 logging_writer.py:48] [97] global_step=97, grad_norm=0.069972, loss=0.088257
I0402 06:09:13.170108 140657493141312 submission.py:139] 97) loss = 0.088, grad_norm = 0.070
I0402 06:09:13.456798 140605535098624 logging_writer.py:48] [98] global_step=98, grad_norm=0.069030, loss=0.093911
I0402 06:09:13.460498 140657493141312 submission.py:139] 98) loss = 0.094, grad_norm = 0.069
I0402 06:09:13.737375 140605543491328 logging_writer.py:48] [99] global_step=99, grad_norm=0.066728, loss=0.087766
I0402 06:09:13.741337 140657493141312 submission.py:139] 99) loss = 0.088, grad_norm = 0.067
I0402 06:09:14.019964 140605535098624 logging_writer.py:48] [100] global_step=100, grad_norm=0.067427, loss=0.088711
I0402 06:09:14.023598 140657493141312 submission.py:139] 100) loss = 0.089, grad_norm = 0.067
I0402 06:10:57.258254 140605543491328 logging_writer.py:48] [500] global_step=500, grad_norm=0.009603, loss=0.050917
I0402 06:10:57.262344 140657493141312 submission.py:139] 500) loss = 0.051, grad_norm = 0.010
I0402 06:12:46.703864 140657493141312 submission_runner.py:373] Before eval at step 929: RAM USED (GB) 26.760179712
I0402 06:12:46.704099 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:13:01.669274 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.950268 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.950459 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.956781 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.957370 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.957458 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.957511 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:01.957757 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.039171 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.284831 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.291791 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.292456 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.292678 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.292733 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.292814 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:16.299767 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:13:41.449563 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:13:42.340301 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.644294 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.647620 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.650688 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.652399 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.655483 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.663158 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.665024 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:42.809265 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.059402 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.063809 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.065359 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.065371 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.065635 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.067892 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:43.076132 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:13:45.572925 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:13:46.687899 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.968526 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.969125 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.974709 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.975334 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.975768 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.975896 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:46.976067 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.148700 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.402177 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.408133 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.408897 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.409058 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.412207 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.416013 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:13:47.416189 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:13:49.844434 140657493141312 submission_runner.py:382] Time since start: 386.54s, 	Step: 929, 	{'train/accuracy': 0.9866167197539738, 'train/loss': 0.055285390466451645, 'train/mean_average_precision': 0.032963932154322126, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.0648745745420456, 'validation/mean_average_precision': 0.03566493514024928, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06818205118179321, 'test/mean_average_precision': 0.03699774701189092, 'test/num_examples': 43793}
I0402 06:13:49.844899 140657493141312 submission_runner.py:396] After eval at step 929: RAM USED (GB) 28.097642496
I0402 06:13:49.853220 140605535098624 logging_writer.py:48] [929] global_step=929, preemption_count=0, score=244.643700, test/accuracy=0.983142, test/loss=0.068182, test/mean_average_precision=0.036998, test/num_examples=43793, total_duration=386.543267, train/accuracy=0.986617, train/loss=0.055285, train/mean_average_precision=0.032964, validation/accuracy=0.984118, validation/loss=0.064875, validation/mean_average_precision=0.035665, validation/num_examples=43793
I0402 06:13:49.918684 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_929.
I0402 06:13:49.919250 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 929: RAM USED (GB) 28.064858112
I0402 06:14:08.675368 140605543491328 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.026245, loss=0.054663
I0402 06:14:08.679642 140657493141312 submission.py:139] 1000) loss = 0.055, grad_norm = 0.026
I0402 06:16:17.183478 140605535098624 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.023313, loss=0.052894
I0402 06:16:17.188046 140657493141312 submission.py:139] 1500) loss = 0.053, grad_norm = 0.023
I0402 06:17:50.254328 140657493141312 submission_runner.py:373] Before eval at step 1864: RAM USED (GB) 27.952242688
I0402 06:17:50.254580 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:18:04.465713 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.751409 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.752442 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.758032 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.758280 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.758921 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.759183 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:04.759193 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:18.824485 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.059045 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.065272 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.065466 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.065630 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.065631 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.066427 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:19.071749 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:18:45.448839 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:18:46.391045 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.680659 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.680861 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.682737 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.682760 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.684405 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.687377 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.695646 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:46.845860 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.092075 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.092108 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.097829 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.098273 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.098919 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.098933 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:47.106774 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:18:49.632864 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:18:50.092518 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.379278 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.379379 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.385155 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.385191 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.385305 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.386479 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.387332 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.545893 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.799731 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.800096 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.805986 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.806365 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.806843 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.812278 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:18:50.812493 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:18:53.289260 140657493141312 submission_runner.py:382] Time since start: 690.09s, 	Step: 1864, 	{'train/accuracy': 0.9868194384499402, 'train/loss': 0.05228053778409958, 'train/mean_average_precision': 0.04772143479279816, 'validation/accuracy': 0.9841183820502766, 'validation/loss': 0.06199125200510025, 'validation/mean_average_precision': 0.04804741118757696, 'validation/num_examples': 43793, 'test/accuracy': 0.983143367510474, 'test/loss': 0.0653071403503418, 'test/mean_average_precision': 0.04961075015149086, 'test/num_examples': 43793}
I0402 06:18:53.289703 140657493141312 submission_runner.py:396] After eval at step 1864: RAM USED (GB) 28.83704832
I0402 06:18:53.297983 140605543491328 logging_writer.py:48] [1864] global_step=1864, preemption_count=0, score=483.937582, test/accuracy=0.983143, test/loss=0.065307, test/mean_average_precision=0.049611, test/num_examples=43793, total_duration=690.093840, train/accuracy=0.986819, train/loss=0.052281, train/mean_average_precision=0.047721, validation/accuracy=0.984118, validation/loss=0.061991, validation/mean_average_precision=0.048047, validation/num_examples=43793
I0402 06:18:53.362196 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_1864.
I0402 06:18:53.362722 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 1864: RAM USED (GB) 28.73810944
I0402 06:19:28.650785 140605535098624 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.036484, loss=0.052286
I0402 06:19:28.655310 140657493141312 submission.py:139] 2000) loss = 0.052, grad_norm = 0.036
I0402 06:21:37.304359 140605543491328 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.060619, loss=0.057366
I0402 06:21:37.309675 140657493141312 submission.py:139] 2500) loss = 0.057, grad_norm = 0.061
I0402 06:22:53.458975 140657493141312 submission_runner.py:373] Before eval at step 2797: RAM USED (GB) 28.776198144
I0402 06:22:53.459194 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:23:07.904763 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.189861 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.190849 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.195859 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.197720 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.197883 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.198487 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:08.204389 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.442415 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.707663 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.713835 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.714060 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.715312 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.715439 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.722316 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:22.723456 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:23:48.865008 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:23:50.485301 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.769670 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.774289 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.775255 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.777377 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.778753 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.780274 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.787225 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:50.968085 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.216366 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.216502 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.222496 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.223440 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.223530 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.228817 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:51.231927 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:23:53.755813 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:23:54.224853 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.507111 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.507620 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.514424 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.514578 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.514737 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.514830 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.514906 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.684956 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.936707 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.936957 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.942162 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.942902 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.943359 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.943982 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:23:54.951767 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:23:57.456914 140657493141312 submission_runner.py:382] Time since start: 993.30s, 	Step: 2797, 	{'train/accuracy': 0.9868393183067273, 'train/loss': 0.05008167773485184, 'train/mean_average_precision': 0.07732153730412732, 'validation/accuracy': 0.984179679095759, 'validation/loss': 0.05977568402886391, 'validation/mean_average_precision': 0.07427573534064093, 'validation/num_examples': 43793, 'test/accuracy': 0.9832153918016139, 'test/loss': 0.06302538514137268, 'test/mean_average_precision': 0.07441813605093704, 'test/num_examples': 43793}
I0402 06:23:57.457414 140657493141312 submission_runner.py:396] After eval at step 2797: RAM USED (GB) 29.664837632
I0402 06:23:57.467057 140605535098624 logging_writer.py:48] [2797] global_step=2797, preemption_count=0, score=723.034854, test/accuracy=0.983215, test/loss=0.063025, test/mean_average_precision=0.074418, test/num_examples=43793, total_duration=993.298400, train/accuracy=0.986839, train/loss=0.050082, train/mean_average_precision=0.077322, validation/accuracy=0.984180, validation/loss=0.059776, validation/mean_average_precision=0.074276, validation/num_examples=43793
I0402 06:23:57.535356 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_2797.
I0402 06:23:57.535909 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 2797: RAM USED (GB) 29.435650048
I0402 06:24:50.415505 140605543491328 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.046376, loss=0.049612
I0402 06:24:50.420031 140657493141312 submission.py:139] 3000) loss = 0.050, grad_norm = 0.046
I0402 06:26:59.520084 140605535098624 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.074757, loss=0.047831
I0402 06:26:59.524638 140657493141312 submission.py:139] 3500) loss = 0.048, grad_norm = 0.075
I0402 06:27:57.637407 140657493141312 submission_runner.py:373] Before eval at step 3728: RAM USED (GB) 29.445996544
I0402 06:27:57.637602 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:28:11.966803 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.234894 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.235944 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.242666 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.242795 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.242987 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.243172 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:12.243462 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.110890 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.338305 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.345185 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.345282 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.345475 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.345926 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.345965 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:27.350710 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:28:54.012975 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:28:55.486666 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.763706 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.769490 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.769788 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.770062 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.770067 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.773119 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.776813 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:55.963156 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.219960 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.224531 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.225092 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.225771 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.225770 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.232994 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:56.233271 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:28:58.760592 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:28:59.226457 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.494616 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.494747 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.500900 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.501057 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.501180 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.501603 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.501690 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.695762 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.940327 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.946690 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.947175 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.947315 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.947575 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.953496 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:59.954147 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:29:02.451024 140657493141312 submission_runner.py:382] Time since start: 1297.48s, 	Step: 3728, 	{'train/accuracy': 0.987220323052417, 'train/loss': 0.04794537276029587, 'train/mean_average_precision': 0.10012658864001289, 'validation/accuracy': 0.9844451643126165, 'validation/loss': 0.057496074587106705, 'validation/mean_average_precision': 0.10173768616404698, 'validation/num_examples': 43793, 'test/accuracy': 0.983482429231922, 'test/loss': 0.06059655174612999, 'test/mean_average_precision': 0.10274536157982073, 'test/num_examples': 43793}
I0402 06:29:02.451490 140657493141312 submission_runner.py:396] After eval at step 3728: RAM USED (GB) 30.286077952
I0402 06:29:02.459941 140605543491328 logging_writer.py:48] [3728] global_step=3728, preemption_count=0, score=962.119179, test/accuracy=0.983482, test/loss=0.060597, test/mean_average_precision=0.102745, test/num_examples=43793, total_duration=1297.476846, train/accuracy=0.987220, train/loss=0.047945, train/mean_average_precision=0.100127, validation/accuracy=0.984445, validation/loss=0.057496, validation/mean_average_precision=0.101738, validation/num_examples=43793
I0402 06:29:02.527708 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_3728.
I0402 06:29:02.528210 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 3728: RAM USED (GB) 30.121029632
I0402 06:30:12.940152 140605535098624 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.069132, loss=0.052866
I0402 06:30:12.948575 140657493141312 submission.py:139] 4000) loss = 0.053, grad_norm = 0.069
I0402 06:32:21.371583 140605543491328 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.064511, loss=0.053381
I0402 06:32:21.377241 140657493141312 submission.py:139] 4500) loss = 0.053, grad_norm = 0.065
I0402 06:33:02.606290 140657493141312 submission_runner.py:373] Before eval at step 4662: RAM USED (GB) 30.148755456
I0402 06:33:02.606485 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:33:17.050064 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.327655 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.328068 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.333986 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.334550 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.334976 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.335031 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:17.335059 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.574708 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.810736 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.816583 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.817366 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.817437 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.817662 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.823017 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:31.823116 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:33:58.997858 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:34:00.155810 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.446238 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.453217 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.453300 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.453700 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.453797 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.454056 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.461170 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.652011 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.901424 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.905885 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.907835 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.908643 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.908797 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.914990 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.915664 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:34:03.509409 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:34:03.969618 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.248724 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.248912 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.254959 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.255248 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.255290 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.255737 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.256050 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.425148 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.677571 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.677566 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.683823 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.684258 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.684505 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.684528 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:04.690425 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:34:07.215038 140657493141312 submission_runner.py:382] Time since start: 1602.45s, 	Step: 4662, 	{'train/accuracy': 0.9871841013202819, 'train/loss': 0.04545004665851593, 'train/mean_average_precision': 0.13792217066200224, 'validation/accuracy': 0.9845064613580989, 'validation/loss': 0.05531194433569908, 'validation/mean_average_precision': 0.12517679358592795, 'validation/num_examples': 43793, 'test/accuracy': 0.9835148612226692, 'test/loss': 0.05845732241868973, 'test/mean_average_precision': 0.12972123708821992, 'test/num_examples': 43793}
I0402 06:34:07.215481 140657493141312 submission_runner.py:396] After eval at step 4662: RAM USED (GB) 31.028944896
I0402 06:34:07.223948 140605535098624 logging_writer.py:48] [4662] global_step=4662, preemption_count=0, score=1201.202825, test/accuracy=0.983515, test/loss=0.058457, test/mean_average_precision=0.129721, test/num_examples=43793, total_duration=1602.445799, train/accuracy=0.987184, train/loss=0.045450, train/mean_average_precision=0.137922, validation/accuracy=0.984506, validation/loss=0.055312, validation/mean_average_precision=0.125177, validation/num_examples=43793
I0402 06:34:07.293465 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_4662.
I0402 06:34:07.293960 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 4662: RAM USED (GB) 30.782242816
I0402 06:35:35.218303 140605543491328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.069400, loss=0.047554
I0402 06:35:35.223499 140657493141312 submission.py:139] 5000) loss = 0.048, grad_norm = 0.069
I0402 06:37:43.797430 140605535098624 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.040520, loss=0.042747
I0402 06:37:43.802195 140657493141312 submission.py:139] 5500) loss = 0.043, grad_norm = 0.041
I0402 06:38:07.488102 140657493141312 submission_runner.py:373] Before eval at step 5593: RAM USED (GB) 30.705483776
I0402 06:38:07.488302 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:38:22.250952 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.530112 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.530763 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.537017 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.537270 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.537590 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.537753 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:22.537949 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.254921 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.499230 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.499273 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.504746 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.505322 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.505828 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.512413 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:37.513025 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:39:04.667934 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:39:05.946686 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.234440 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.239476 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.240472 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.240842 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.240885 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.248189 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.252765 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.423064 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.669114 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.669166 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.674834 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.675132 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.675955 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.678361 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:06.683743 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:39:09.277722 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:39:09.728241 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.007231 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.007285 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.013696 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.013866 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.013873 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.014195 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.014787 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.195567 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.446454 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.446686 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.452802 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.452884 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.453006 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.453061 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:10.453561 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:39:14.558327 140657493141312 submission_runner.py:382] Time since start: 1907.33s, 	Step: 5593, 	{'train/accuracy': 0.9872362495000095, 'train/loss': 0.044589001685380936, 'train/mean_average_precision': 0.15013656261612424, 'validation/accuracy': 0.9846862930875606, 'validation/loss': 0.05416582524776459, 'validation/mean_average_precision': 0.13758170789335317, 'validation/num_examples': 43793, 'test/accuracy': 0.9836993444947116, 'test/loss': 0.05733022838830948, 'test/mean_average_precision': 0.14008508045851148, 'test/num_examples': 43793}
I0402 06:39:14.558753 140657493141312 submission_runner.py:396] After eval at step 5593: RAM USED (GB) 31.232483328
I0402 06:39:14.567510 140605543491328 logging_writer.py:48] [5593] global_step=5593, preemption_count=0, score=1440.423369, test/accuracy=0.983699, test/loss=0.057330, test/mean_average_precision=0.140085, test/num_examples=43793, total_duration=1907.327480, train/accuracy=0.987236, train/loss=0.044589, train/mean_average_precision=0.150137, validation/accuracy=0.984686, validation/loss=0.054166, validation/mean_average_precision=0.137582, validation/num_examples=43793
I0402 06:39:14.632204 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_5593.
I0402 06:39:14.632625 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 5593: RAM USED (GB) 31.231471616
I0402 06:40:58.351533 140657493141312 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 31.287361536
I0402 06:40:58.351755 140657493141312 spec.py:298] Evaluating on the training split.
W0402 06:41:12.801540 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.045782 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.046733 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.051768 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.052188 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.052265 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.052282 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:13.052805 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.003349 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.241197 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.242464 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.247010 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.247841 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.247964 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.248324 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:27.248412 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:41:54.744854 140657493141312 spec.py:310] Evaluating on the validation split.
W0402 06:41:56.121591 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.385932 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.389032 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.392088 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.392289 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.394855 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.405564 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.405567 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.589528 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.839414 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.841001 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.846002 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.846093 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.846117 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.846404 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:41:56.848762 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:41:59.413395 140657493141312 spec.py:326] Evaluating on the test split.
W0402 06:41:59.865816 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.118000 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.118066 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.124359 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.124801 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.124978 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.125423 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.125628 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.340592 140657493141312 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.596345 139954977130304 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.602412 140661947967296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.602474 139818821744448 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.602630 140539115226944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.602654 139661137712960 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.603477 139961263494976 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:42:00.603859 140652414433088 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:42:03.132540 140657493141312 submission_runner.py:382] Time since start: 2078.19s, 	Step: 6000, 	{'train/accuracy': 0.9876143069378986, 'train/loss': 0.04296990483999252, 'train/mean_average_precision': 0.16835523466953928, 'validation/accuracy': 0.9848916990810315, 'validation/loss': 0.053108248859643936, 'validation/mean_average_precision': 0.14832337260653056, 'validation/num_examples': 43793, 'test/accuracy': 0.9839739634293503, 'test/loss': 0.05615011602640152, 'test/mean_average_precision': 0.1548099198300333, 'test/num_examples': 43793}
I0402 06:42:03.133019 140657493141312 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 32.037572608
I0402 06:42:03.141867 140605535098624 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1543.717460, test/accuracy=0.983974, test/loss=0.056150, test/mean_average_precision=0.154810, test/num_examples=43793, total_duration=2078.190959, train/accuracy=0.987614, train/loss=0.042970, train/mean_average_precision=0.168355, validation/accuracy=0.984892, validation/loss=0.053108, validation/mean_average_precision=0.148323, validation/num_examples=43793
I0402 06:42:03.205351 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_6000.
I0402 06:42:03.205864 140657493141312 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.734099968
I0402 06:42:03.222314 140605543491328 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1543.717460
I0402 06:42:03.325195 140657493141312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_6000.
I0402 06:42:03.488777 140657493141312 submission_runner.py:550] Tuning trial 1/1
I0402 06:42:03.489032 140657493141312 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 06:42:03.490026 140657493141312 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.47149647026757274, 'train/loss': 0.7833452224731445, 'train/mean_average_precision': 0.023938336418131638, 'validation/accuracy': 0.47010652695811583, 'validation/loss': 0.7843211889266968, 'validation/mean_average_precision': 0.027072326455109562, 'validation/num_examples': 43793, 'test/accuracy': 0.47031946353275356, 'test/loss': 0.7845950722694397, 'test/mean_average_precision': 0.028901715039386926, 'test/num_examples': 43793, 'score': 5.466754913330078, 'total_duration': 5.468705177307129, 'global_step': 1, 'preemption_count': 0}), (929, {'train/accuracy': 0.9866167197539738, 'train/loss': 0.055285390466451645, 'train/mean_average_precision': 0.032963932154322126, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.0648745745420456, 'validation/mean_average_precision': 0.03566493514024928, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06818205118179321, 'test/mean_average_precision': 0.03699774701189092, 'test/num_examples': 43793, 'score': 244.64369988441467, 'total_duration': 386.54326701164246, 'global_step': 929, 'preemption_count': 0}), (1864, {'train/accuracy': 0.9868194384499402, 'train/loss': 0.05228053778409958, 'train/mean_average_precision': 0.04772143479279816, 'validation/accuracy': 0.9841183820502766, 'validation/loss': 0.06199125200510025, 'validation/mean_average_precision': 0.04804741118757696, 'validation/num_examples': 43793, 'test/accuracy': 0.983143367510474, 'test/loss': 0.0653071403503418, 'test/mean_average_precision': 0.04961075015149086, 'test/num_examples': 43793, 'score': 483.93758177757263, 'total_duration': 690.0938396453857, 'global_step': 1864, 'preemption_count': 0}), (2797, {'train/accuracy': 0.9868393183067273, 'train/loss': 0.05008167773485184, 'train/mean_average_precision': 0.07732153730412732, 'validation/accuracy': 0.984179679095759, 'validation/loss': 0.05977568402886391, 'validation/mean_average_precision': 0.07427573534064093, 'validation/num_examples': 43793, 'test/accuracy': 0.9832153918016139, 'test/loss': 0.06302538514137268, 'test/mean_average_precision': 0.07441813605093704, 'test/num_examples': 43793, 'score': 723.0348539352417, 'total_duration': 993.2984001636505, 'global_step': 2797, 'preemption_count': 0}), (3728, {'train/accuracy': 0.987220323052417, 'train/loss': 0.04794537276029587, 'train/mean_average_precision': 0.10012658864001289, 'validation/accuracy': 0.9844451643126165, 'validation/loss': 0.057496074587106705, 'validation/mean_average_precision': 0.10173768616404698, 'validation/num_examples': 43793, 'test/accuracy': 0.983482429231922, 'test/loss': 0.06059655174612999, 'test/mean_average_precision': 0.10274536157982073, 'test/num_examples': 43793, 'score': 962.1191787719727, 'total_duration': 1297.476845741272, 'global_step': 3728, 'preemption_count': 0}), (4662, {'train/accuracy': 0.9871841013202819, 'train/loss': 0.04545004665851593, 'train/mean_average_precision': 0.13792217066200224, 'validation/accuracy': 0.9845064613580989, 'validation/loss': 0.05531194433569908, 'validation/mean_average_precision': 0.12517679358592795, 'validation/num_examples': 43793, 'test/accuracy': 0.9835148612226692, 'test/loss': 0.05845732241868973, 'test/mean_average_precision': 0.12972123708821992, 'test/num_examples': 43793, 'score': 1201.2028245925903, 'total_duration': 1602.4457993507385, 'global_step': 4662, 'preemption_count': 0}), (5593, {'train/accuracy': 0.9872362495000095, 'train/loss': 0.044589001685380936, 'train/mean_average_precision': 0.15013656261612424, 'validation/accuracy': 0.9846862930875606, 'validation/loss': 0.05416582524776459, 'validation/mean_average_precision': 0.13758170789335317, 'validation/num_examples': 43793, 'test/accuracy': 0.9836993444947116, 'test/loss': 0.05733022838830948, 'test/mean_average_precision': 0.14008508045851148, 'test/num_examples': 43793, 'score': 1440.4233694076538, 'total_duration': 1907.327479839325, 'global_step': 5593, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9876143069378986, 'train/loss': 0.04296990483999252, 'train/mean_average_precision': 0.16835523466953928, 'validation/accuracy': 0.9848916990810315, 'validation/loss': 0.053108248859643936, 'validation/mean_average_precision': 0.14832337260653056, 'validation/num_examples': 43793, 'test/accuracy': 0.9839739634293503, 'test/loss': 0.05615011602640152, 'test/mean_average_precision': 0.1548099198300333, 'test/num_examples': 43793, 'score': 1543.7174599170685, 'total_duration': 2078.1909589767456, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0402 06:42:03.490127 140657493141312 submission_runner.py:553] Timing: 1543.7174599170685
I0402 06:42:03.490168 140657493141312 submission_runner.py:554] ====================
I0402 06:42:03.490250 140657493141312 submission_runner.py:613] Final ogbg score: 1543.7174599170685
