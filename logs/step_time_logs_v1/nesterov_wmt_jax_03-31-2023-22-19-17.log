I0331 22:19:31.811354 140184187594560 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/wmt_jax.
I0331 22:19:31.859721 140184187594560 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 22:19:32.720039 140184187594560 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0331 22:19:32.721395 140184187594560 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 22:19:32.725075 140184187594560 submission_runner.py:511] Using RNG seed 1470462638
I0331 22:19:33.992008 140184187594560 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 22:19:33.992219 140184187594560 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/wmt_jax/trial_1.
I0331 22:19:33.992409 140184187594560 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/wmt_jax/trial_1/hparams.json.
I0331 22:19:34.115762 140184187594560 submission_runner.py:230] Starting train once: RAM USED (GB) 4.128514048
I0331 22:19:34.115959 140184187594560 submission_runner.py:231] Initializing dataset.
I0331 22:19:34.124371 140184187594560 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:19:34.127583 140184187594560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:19:34.127700 140184187594560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:19:34.201421 140184187594560 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:19:36.038472 140184187594560 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.225245184
I0331 22:19:36.038673 140184187594560 submission_runner.py:240] Initializing model.
I0331 22:19:47.854701 140184187594560 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.307204096
I0331 22:19:47.854923 140184187594560 submission_runner.py:252] Initializing optimizer.
I0331 22:19:48.472531 140184187594560 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.307195904
I0331 22:19:48.472713 140184187594560 submission_runner.py:261] Initializing metrics bundle.
I0331 22:19:48.472760 140184187594560 submission_runner.py:276] Initializing checkpoint and logger.
I0331 22:19:48.473684 140184187594560 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov/wmt_jax/trial_1 with prefix checkpoint_
I0331 22:19:48.474015 140184187594560 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0331 22:19:48.474090 140184187594560 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0331 22:19:49.460452 140184187594560 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/wmt_jax/trial_1/meta_data_0.json.
I0331 22:19:49.461444 140184187594560 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/wmt_jax/trial_1/flags_0.json.
I0331 22:19:49.464393 140184187594560 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.299495424
I0331 22:19:49.464613 140184187594560 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.299495424
I0331 22:19:49.464683 140184187594560 submission_runner.py:313] Starting training loop.
I0331 22:19:50.213088 140184187594560 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.440266752
I0331 22:20:19.414563 140007771727616 logging_writer.py:48] [0] global_step=0, grad_norm=4.9658098220825195, loss=11.048286437988281
I0331 22:20:19.424844 140184187594560 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 11.149889536
I0331 22:20:19.425145 140184187594560 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 11.149889536
I0331 22:20:19.425237 140184187594560 spec.py:298] Evaluating on the training split.
I0331 22:20:19.427608 140184187594560 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:20:19.429961 140184187594560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:20:19.430062 140184187594560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:20:19.459866 140184187594560 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:20:27.607913 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 22:25:32.093993 140184187594560 spec.py:310] Evaluating on the validation split.
I0331 22:25:32.097676 140184187594560 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:25:32.100975 140184187594560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:25:32.101078 140184187594560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:25:32.131408 140184187594560 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:25:39.762230 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 22:30:37.524572 140184187594560 spec.py:326] Evaluating on the test split.
I0331 22:30:37.526908 140184187594560 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:30:37.529567 140184187594560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:30:37.529689 140184187594560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:30:37.560690 140184187594560 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:30:44.405519 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 22:35:36.116139 140184187594560 submission_runner.py:382] Time since start: 29.96s, 	Step: 1, 	{'train/accuracy': 0.0006353600765578449, 'train/loss': 11.039224624633789, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.060032844543457, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.044007301330566, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0331 22:35:36.116727 140184187594560 submission_runner.py:396] After eval at step 1: RAM USED (GB) 11.627884544
I0331 22:35:36.125264 139996982986496 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=29.749681, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.044007, test/num_examples=3003, total_duration=29.960455, train/accuracy=0.000635, train/bleu=0.000000, train/loss=11.039225, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.060033, validation/num_examples=3000
I0331 22:35:36.830494 140184187594560 checkpoints.py:356] Saving checkpoint at step: 1
I0331 22:35:39.370645 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_1
I0331 22:35:39.374413 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_1.
I0331 22:35:39.386642 140184187594560 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.121935872
I0331 22:35:39.389278 140184187594560 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.121935872
I0331 22:35:39.463275 140184187594560 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.121567232
I0331 22:36:15.200649 139996974593792 logging_writer.py:48] [100] global_step=100, grad_norm=0.1695464551448822, loss=9.097899436950684
I0331 22:36:50.993293 139996941022976 logging_writer.py:48] [200] global_step=200, grad_norm=0.13655982911586761, loss=8.932866096496582
I0331 22:37:26.816963 139996974593792 logging_writer.py:48] [300] global_step=300, grad_norm=0.15298907458782196, loss=8.775118827819824
I0331 22:38:02.644741 139996941022976 logging_writer.py:48] [400] global_step=400, grad_norm=0.4668109714984894, loss=8.636055946350098
I0331 22:38:38.462142 139996974593792 logging_writer.py:48] [500] global_step=500, grad_norm=0.9531703591346741, loss=8.404635429382324
I0331 22:39:14.335988 139996941022976 logging_writer.py:48] [600] global_step=600, grad_norm=0.4814334511756897, loss=8.25901985168457
I0331 22:39:50.185656 139996974593792 logging_writer.py:48] [700] global_step=700, grad_norm=0.6364917755126953, loss=8.139087677001953
I0331 22:40:26.025629 139996941022976 logging_writer.py:48] [800] global_step=800, grad_norm=0.538494884967804, loss=7.987228870391846
I0331 22:41:01.882133 139996974593792 logging_writer.py:48] [900] global_step=900, grad_norm=0.6248366236686707, loss=7.873143196105957
I0331 22:41:37.789610 139996941022976 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6850458979606628, loss=7.80604362487793
I0331 22:42:13.652586 139996974593792 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7967416644096375, loss=7.738572597503662
I0331 22:42:49.525660 139996941022976 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8253676891326904, loss=7.616671562194824
I0331 22:43:25.377742 139996974593792 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.743410050868988, loss=7.582461833953857
I0331 22:44:01.246633 139996941022976 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6597979664802551, loss=7.502431392669678
I0331 22:44:37.113536 139996974593792 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5866937041282654, loss=7.454629898071289
I0331 22:45:12.989337 139996941022976 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5664345026016235, loss=7.35681676864624
I0331 22:45:48.910470 139996974593792 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8339335918426514, loss=7.291830062866211
I0331 22:46:24.760375 139996941022976 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8387242555618286, loss=7.178496360778809
I0331 22:47:00.654318 139996974593792 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6788241863250732, loss=7.071660041809082
I0331 22:47:36.539530 139996941022976 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7775028944015503, loss=7.1046037673950195
I0331 22:48:12.424232 139996974593792 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6471676826477051, loss=6.918726921081543
I0331 22:48:48.305476 139996941022976 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7174973487854004, loss=6.954520225524902
I0331 22:49:24.179571 139996974593792 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5918570756912231, loss=6.791404724121094
I0331 22:49:39.662989 140184187594560 submission_runner.py:373] Before eval at step 2345: RAM USED (GB) 12.002254848
I0331 22:49:39.663206 140184187594560 spec.py:298] Evaluating on the training split.
I0331 22:49:42.712909 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 22:54:04.490911 140184187594560 spec.py:310] Evaluating on the validation split.
I0331 22:54:07.196896 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 22:58:12.909530 140184187594560 spec.py:326] Evaluating on the test split.
I0331 22:58:15.657501 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:02:30.427149 140184187594560 submission_runner.py:382] Time since start: 1790.20s, 	Step: 2345, 	{'train/accuracy': 0.30772462487220764, 'train/loss': 5.52118444442749, 'train/bleu': 6.842426138439567, 'validation/accuracy': 0.28377825021743774, 'validation/loss': 5.792331218719482, 'validation/bleu': 3.685785097415426, 'validation/num_examples': 3000, 'test/accuracy': 0.26108884811401367, 'test/loss': 6.0868682861328125, 'test/bleu': 2.5667535518782065, 'test/num_examples': 3003}
I0331 23:02:30.427726 140184187594560 submission_runner.py:396] After eval at step 2345: RAM USED (GB) 12.207300608
I0331 23:02:30.435589 139996941022976 logging_writer.py:48] [2345] global_step=2345, preemption_count=0, score=865.478132, test/accuracy=0.261089, test/bleu=2.566754, test/loss=6.086868, test/num_examples=3003, total_duration=1790.197098, train/accuracy=0.307725, train/bleu=6.842426, train/loss=5.521184, validation/accuracy=0.283778, validation/bleu=3.685785, validation/loss=5.792331, validation/num_examples=3000
I0331 23:02:31.158934 140184187594560 checkpoints.py:356] Saving checkpoint at step: 2345
I0331 23:02:33.661722 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_2345
I0331 23:02:33.665020 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_2345.
I0331 23:02:33.669182 140184187594560 submission_runner.py:416] After logging and checkpointing eval at step 2345: RAM USED (GB) 12.959936512
I0331 23:02:53.770112 139996974593792 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8523715138435364, loss=6.859302520751953
I0331 23:03:29.637779 139996636907264 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8131023645401001, loss=6.7717509269714355
I0331 23:04:05.501943 139996974593792 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7134311199188232, loss=6.592066287994385
I0331 23:04:41.389757 139996636907264 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5703308582305908, loss=6.5740647315979
I0331 23:05:17.279901 139996974593792 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7758574485778809, loss=6.567660808563232
I0331 23:05:53.144932 139996636907264 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5728015303611755, loss=6.573941230773926
I0331 23:06:29.045858 139996974593792 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6660841107368469, loss=6.452969551086426
I0331 23:07:04.948894 139996636907264 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8140574097633362, loss=6.443698883056641
I0331 23:07:40.865073 139996974593792 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.639790415763855, loss=6.325368881225586
I0331 23:08:16.745189 139996636907264 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8446177840232849, loss=6.308855056762695
I0331 23:08:52.626586 139996974593792 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8199995756149292, loss=6.244274616241455
I0331 23:09:28.543419 139996636907264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6049384474754333, loss=6.184671878814697
I0331 23:10:04.397825 139996974593792 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8078538775444031, loss=6.0684380531311035
I0331 23:10:40.310096 139996636907264 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5780186653137207, loss=5.992722511291504
I0331 23:11:16.190984 139996974593792 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7631681561470032, loss=6.069259166717529
I0331 23:11:52.053547 139996636907264 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7018139958381653, loss=5.945621490478516
I0331 23:12:27.902455 139996974593792 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6008718609809875, loss=5.939294815063477
I0331 23:13:03.787245 139996636907264 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5566135048866272, loss=5.876648426055908
I0331 23:13:39.663496 139996974593792 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6337819695472717, loss=5.902026653289795
I0331 23:14:15.510126 139996636907264 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6512981057167053, loss=5.746580600738525
I0331 23:14:51.408179 139996974593792 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7281501889228821, loss=5.73043155670166
I0331 23:15:27.280292 139996636907264 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7082293033599854, loss=5.558419227600098
I0331 23:16:03.160620 139996974593792 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5904818773269653, loss=5.562994003295898
I0331 23:16:33.741569 140184187594560 submission_runner.py:373] Before eval at step 4687: RAM USED (GB) 12.30432256
I0331 23:16:33.741768 140184187594560 spec.py:298] Evaluating on the training split.
I0331 23:16:36.764883 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:20:20.843885 140184187594560 spec.py:310] Evaluating on the validation split.
I0331 23:20:23.527696 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:23:25.825547 140184187594560 spec.py:326] Evaluating on the test split.
I0331 23:23:28.566962 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:26:40.379871 140184187594560 submission_runner.py:382] Time since start: 3404.28s, 	Step: 4687, 	{'train/accuracy': 0.46274638175964355, 'train/loss': 3.8859801292419434, 'train/bleu': 17.634782824072662, 'validation/accuracy': 0.4519100785255432, 'validation/loss': 3.9586377143859863, 'validation/bleu': 13.40766113161758, 'validation/num_examples': 3000, 'test/accuracy': 0.43995121121406555, 'test/loss': 4.112425327301025, 'test/bleu': 11.29719085929977, 'test/num_examples': 3003}
I0331 23:26:40.380361 140184187594560 submission_runner.py:396] After eval at step 4687: RAM USED (GB) 12.48393216
I0331 23:26:40.388821 139996636907264 logging_writer.py:48] [4687] global_step=4687, preemption_count=0, score=1702.236536, test/accuracy=0.439951, test/bleu=11.297191, test/loss=4.112425, test/num_examples=3003, total_duration=3404.276435, train/accuracy=0.462746, train/bleu=17.634783, train/loss=3.885980, validation/accuracy=0.451910, validation/bleu=13.407661, validation/loss=3.958638, validation/num_examples=3000
I0331 23:26:41.082768 140184187594560 checkpoints.py:356] Saving checkpoint at step: 4687
I0331 23:26:43.531288 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_4687
I0331 23:26:43.534427 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_4687.
I0331 23:26:43.539182 140184187594560 submission_runner.py:416] After logging and checkpointing eval at step 4687: RAM USED (GB) 13.216620544
I0331 23:26:48.570463 139996974593792 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.568084716796875, loss=5.457967758178711
I0331 23:27:24.472801 139996628514560 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6766660213470459, loss=5.515691757202148
I0331 23:28:00.318146 139996974593792 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5056653618812561, loss=5.4212470054626465
I0331 23:28:36.190224 139996628514560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.4984637200832367, loss=5.399205684661865
I0331 23:29:12.089374 139996974593792 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6246498823165894, loss=5.429988384246826
I0331 23:29:47.956205 139996628514560 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.49792492389678955, loss=5.432806491851807
I0331 23:30:23.848195 139996974593792 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6335001587867737, loss=5.373738765716553
I0331 23:30:59.744331 139996628514560 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5439994931221008, loss=5.326850891113281
I0331 23:31:35.622671 139996974593792 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5257956385612488, loss=5.333171844482422
I0331 23:32:11.496044 139996628514560 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5846412181854248, loss=5.242610931396484
I0331 23:32:47.350196 139996974593792 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5326314568519592, loss=5.3512349128723145
I0331 23:33:23.221769 139996628514560 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.491710364818573, loss=5.253302574157715
I0331 23:33:59.101439 139996974593792 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5648661255836487, loss=5.232779502868652
I0331 23:34:34.968965 139996628514560 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5036476254463196, loss=5.28275728225708
I0331 23:35:10.854640 139996974593792 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4968717396259308, loss=5.163961410522461
I0331 23:35:46.747433 139996628514560 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5107506513595581, loss=5.211636543273926
I0331 23:36:22.676549 139996974593792 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4846338629722595, loss=5.145624160766602
I0331 23:36:58.573691 139996628514560 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.44985994696617126, loss=5.164008140563965
I0331 23:37:34.445673 139996974593792 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4620285630226135, loss=5.04388427734375
I0331 23:38:10.330298 139996628514560 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.43450984358787537, loss=5.118366241455078
I0331 23:38:46.183216 139996974593792 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4864996671676636, loss=5.028681755065918
I0331 23:39:22.032534 139996628514560 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.42709243297576904, loss=5.0600199699401855
I0331 23:39:57.923923 139996974593792 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.447561115026474, loss=5.07580041885376
I0331 23:40:33.785362 139996628514560 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.48825767636299133, loss=4.982316493988037
I0331 23:40:43.887281 140184187594560 submission_runner.py:373] Before eval at step 7030: RAM USED (GB) 12.798173184
I0331 23:40:43.887480 140184187594560 spec.py:298] Evaluating on the training split.
I0331 23:40:46.912009 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:43:32.730057 140184187594560 spec.py:310] Evaluating on the validation split.
I0331 23:43:35.411029 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:46:12.596163 140184187594560 spec.py:326] Evaluating on the test split.
I0331 23:46:15.330180 140184187594560 workload.py:179] Translating evaluation dataset.
I0331 23:48:46.915566 140184187594560 submission_runner.py:382] Time since start: 4854.42s, 	Step: 7030, 	{'train/accuracy': 0.5397709012031555, 'train/loss': 3.1431641578674316, 'train/bleu': 23.42405137828031, 'validation/accuracy': 0.5314255356788635, 'validation/loss': 3.1641266345977783, 'validation/bleu': 19.46384892681455, 'validation/num_examples': 3000, 'test/accuracy': 0.5301260948181152, 'test/loss': 3.2204949855804443, 'test/bleu': 17.931415691719625, 'test/num_examples': 3003}
I0331 23:48:46.916036 140184187594560 submission_runner.py:396] After eval at step 7030: RAM USED (GB) 12.920188928
I0331 23:48:46.923766 139996974593792 logging_writer.py:48] [7030] global_step=7030, preemption_count=0, score=2539.227795, test/accuracy=0.530126, test/bleu=17.931416, test/loss=3.220495, test/num_examples=3003, total_duration=4854.422010, train/accuracy=0.539771, train/bleu=23.424051, train/loss=3.143164, validation/accuracy=0.531426, validation/bleu=19.463849, validation/loss=3.164127, validation/num_examples=3000
I0331 23:48:47.619291 140184187594560 checkpoints.py:356] Saving checkpoint at step: 7030
I0331 23:48:50.122683 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_7030
I0331 23:48:50.125899 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_7030.
I0331 23:48:50.130670 140184187594560 submission_runner.py:416] After logging and checkpointing eval at step 7030: RAM USED (GB) 13.65200896
I0331 23:49:15.584054 139996628514560 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4486013352870941, loss=5.029752731323242
I0331 23:49:51.432392 139996620121856 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4853236675262451, loss=4.966831684112549
I0331 23:50:27.300298 139996628514560 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4225599765777588, loss=4.971794128417969
I0331 23:51:03.186771 139996620121856 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4127386808395386, loss=4.888355255126953
I0331 23:51:39.056649 139996628514560 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.48985210061073303, loss=4.8161396980285645
I0331 23:52:14.902857 139996620121856 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.47131282091140747, loss=5.018375873565674
I0331 23:52:50.743089 139996628514560 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.40912583470344543, loss=4.9416913986206055
I0331 23:53:26.607493 139996620121856 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.40415850281715393, loss=4.865856647491455
I0331 23:54:02.505254 139996628514560 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.45082560181617737, loss=4.910589218139648
I0331 23:54:38.313158 139996620121856 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4107610285282135, loss=4.918379306793213
I0331 23:55:14.147954 139996628514560 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.45677971839904785, loss=4.959160327911377
I0331 23:55:50.029328 139996620121856 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.433901846408844, loss=4.9186177253723145
I0331 23:56:25.886053 139996628514560 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.39242371916770935, loss=4.854686737060547
I0331 23:57:01.694467 139996620121856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.40686920285224915, loss=4.816909313201904
I0331 23:57:37.520282 139996628514560 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3959349989891052, loss=4.808713912963867
I0331 23:58:13.385095 139996620121856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.39117783308029175, loss=4.850781440734863
I0331 23:58:49.289701 139996628514560 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3818376064300537, loss=4.8400750160217285
I0331 23:59:25.111231 139996620121856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3915015459060669, loss=4.834960460662842
I0401 00:00:00.959656 139996628514560 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.38205617666244507, loss=4.776663780212402
I0401 00:00:36.807894 139996620121856 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.37711456418037415, loss=4.800692081451416
I0401 00:01:12.659088 139996628514560 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3854171633720398, loss=4.82425594329834
I0401 00:01:48.517779 139996620121856 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.37015753984451294, loss=4.818582534790039
I0401 00:02:24.346983 139996628514560 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.37184518575668335, loss=4.726775169372559
I0401 00:02:50.225069 140184187594560 submission_runner.py:373] Before eval at step 9374: RAM USED (GB) 13.158670336
I0401 00:02:50.225273 140184187594560 spec.py:298] Evaluating on the training split.
I0401 00:02:53.238476 140184187594560 workload.py:179] Translating evaluation dataset.
I0401 00:05:37.274749 140184187594560 spec.py:310] Evaluating on the validation split.
I0401 00:05:39.964762 140184187594560 workload.py:179] Translating evaluation dataset.
I0401 00:08:08.817018 140184187594560 spec.py:326] Evaluating on the test split.
I0401 00:08:11.544263 140184187594560 workload.py:179] Translating evaluation dataset.
I0401 00:10:38.223983 140184187594560 submission_runner.py:382] Time since start: 6180.76s, 	Step: 9374, 	{'train/accuracy': 0.5657699704170227, 'train/loss': 2.8572750091552734, 'train/bleu': 26.265276362898646, 'validation/accuracy': 0.5674697160720825, 'validation/loss': 2.7884793281555176, 'validation/bleu': 21.822314174052558, 'validation/num_examples': 3000, 'test/accuracy': 0.5705653429031372, 'test/loss': 2.8034660816192627, 'test/bleu': 20.871254743524837, 'test/num_examples': 3003}
I0401 00:10:38.224452 140184187594560 submission_runner.py:396] After eval at step 9374: RAM USED (GB) 13.24265472
I0401 00:10:38.232263 139996620121856 logging_writer.py:48] [9374] global_step=9374, preemption_count=0, score=3376.008925, test/accuracy=0.570565, test/bleu=20.871255, test/loss=2.803466, test/num_examples=3003, total_duration=6180.759857, train/accuracy=0.565770, train/bleu=26.265276, train/loss=2.857275, validation/accuracy=0.567470, validation/bleu=21.822314, validation/loss=2.788479, validation/num_examples=3000
I0401 00:10:38.926500 140184187594560 checkpoints.py:356] Saving checkpoint at step: 9374
I0401 00:10:41.459399 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_9374
I0401 00:10:41.462623 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_9374.
I0401 00:10:41.466581 140184187594560 submission_runner.py:416] After logging and checkpointing eval at step 9374: RAM USED (GB) 13.995659264
I0401 00:10:51.138865 139996628514560 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3751954734325409, loss=4.845667839050293
I0401 00:11:26.983705 139996611729152 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3696559965610504, loss=4.748950004577637
I0401 00:12:02.806070 139996628514560 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3787900507450104, loss=4.781672954559326
I0401 00:12:38.626853 139996611729152 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3767944276332855, loss=4.695379734039307
I0401 00:13:14.467739 139996628514560 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.42182937264442444, loss=4.778872013092041
I0401 00:13:50.329865 139996611729152 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3754422068595886, loss=4.680140495300293
I0401 00:14:25.523814 140184187594560 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 13.249703936
I0401 00:14:25.524001 140184187594560 spec.py:298] Evaluating on the training split.
I0401 00:14:28.534469 140184187594560 workload.py:179] Translating evaluation dataset.
I0401 00:17:08.125661 140184187594560 spec.py:310] Evaluating on the validation split.
I0401 00:17:10.806072 140184187594560 workload.py:179] Translating evaluation dataset.
I0401 00:19:43.586298 140184187594560 spec.py:326] Evaluating on the test split.
I0401 00:19:46.320377 140184187594560 workload.py:179] Translating evaluation dataset.
I0401 00:22:12.175964 140184187594560 submission_runner.py:382] Time since start: 6876.06s, 	Step: 10000, 	{'train/accuracy': 0.5689003467559814, 'train/loss': 2.833481550216675, 'train/bleu': 26.52372392942127, 'validation/accuracy': 0.5735824704170227, 'validation/loss': 2.757929801940918, 'validation/bleu': 22.246032156778472, 'validation/num_examples': 3000, 'test/accuracy': 0.5756783485412598, 'test/loss': 2.7706234455108643, 'test/bleu': 21.140724594809207, 'test/num_examples': 3003}
I0401 00:22:12.176429 140184187594560 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 13.2924416
I0401 00:22:12.184293 139996628514560 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3599.216737, test/accuracy=0.575678, test/bleu=21.140725, test/loss=2.770623, test/num_examples=3003, total_duration=6876.058615, train/accuracy=0.568900, train/bleu=26.523724, train/loss=2.833482, validation/accuracy=0.573582, validation/bleu=22.246032, validation/loss=2.757930, validation/num_examples=3000
I0401 00:22:12.877697 140184187594560 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 00:22:15.387489 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_10000
I0401 00:22:15.390754 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_10000.
I0401 00:22:15.395411 140184187594560 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.0237824
I0401 00:22:15.402438 139996611729152 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3599.216737
I0401 00:22:15.769649 140184187594560 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 00:22:19.749040 140184187594560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_10000
I0401 00:22:19.752313 140184187594560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_jax/trial_1/checkpoint_10000.
I0401 00:22:19.813336 140184187594560 submission_runner.py:550] Tuning trial 1/1
I0401 00:22:19.813506 140184187594560 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 00:22:19.814629 140184187594560 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006353600765578449, 'train/loss': 11.039224624633789, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.060032844543457, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.044007301330566, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 29.749680519104004, 'total_duration': 29.9604549407959, 'global_step': 1, 'preemption_count': 0}), (2345, {'train/accuracy': 0.30772462487220764, 'train/loss': 5.52118444442749, 'train/bleu': 6.842426138439567, 'validation/accuracy': 0.28377825021743774, 'validation/loss': 5.792331218719482, 'validation/bleu': 3.685785097415426, 'validation/num_examples': 3000, 'test/accuracy': 0.26108884811401367, 'test/loss': 6.0868682861328125, 'test/bleu': 2.5667535518782065, 'test/num_examples': 3003, 'score': 865.4781317710876, 'total_duration': 1790.197098493576, 'global_step': 2345, 'preemption_count': 0}), (4687, {'train/accuracy': 0.46274638175964355, 'train/loss': 3.8859801292419434, 'train/bleu': 17.634782824072662, 'validation/accuracy': 0.4519100785255432, 'validation/loss': 3.9586377143859863, 'validation/bleu': 13.40766113161758, 'validation/num_examples': 3000, 'test/accuracy': 0.43995121121406555, 'test/loss': 4.112425327301025, 'test/bleu': 11.29719085929977, 'test/num_examples': 3003, 'score': 1702.2365357875824, 'total_duration': 3404.2764348983765, 'global_step': 4687, 'preemption_count': 0}), (7030, {'train/accuracy': 0.5397709012031555, 'train/loss': 3.1431641578674316, 'train/bleu': 23.42405137828031, 'validation/accuracy': 0.5314255356788635, 'validation/loss': 3.1641266345977783, 'validation/bleu': 19.46384892681455, 'validation/num_examples': 3000, 'test/accuracy': 0.5301260948181152, 'test/loss': 3.2204949855804443, 'test/bleu': 17.931415691719625, 'test/num_examples': 3003, 'score': 2539.2277948856354, 'total_duration': 4854.422010421753, 'global_step': 7030, 'preemption_count': 0}), (9374, {'train/accuracy': 0.5657699704170227, 'train/loss': 2.8572750091552734, 'train/bleu': 26.265276362898646, 'validation/accuracy': 0.5674697160720825, 'validation/loss': 2.7884793281555176, 'validation/bleu': 21.822314174052558, 'validation/num_examples': 3000, 'test/accuracy': 0.5705653429031372, 'test/loss': 2.8034660816192627, 'test/bleu': 20.871254743524837, 'test/num_examples': 3003, 'score': 3376.0089251995087, 'total_duration': 6180.759856939316, 'global_step': 9374, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5689003467559814, 'train/loss': 2.833481550216675, 'train/bleu': 26.52372392942127, 'validation/accuracy': 0.5735824704170227, 'validation/loss': 2.757929801940918, 'validation/bleu': 22.246032156778472, 'validation/num_examples': 3000, 'test/accuracy': 0.5756783485412598, 'test/loss': 2.7706234455108643, 'test/bleu': 21.140724594809207, 'test/num_examples': 3003, 'score': 3599.2167370319366, 'total_duration': 6876.058615207672, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 00:22:19.814743 140184187594560 submission_runner.py:553] Timing: 3599.2167370319366
I0401 00:22:19.814784 140184187594560 submission_runner.py:554] ====================
I0401 00:22:19.814924 140184187594560 submission_runner.py:613] Final wmt score: 3599.2167370319366
