WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0331 23:41:28.429743 139671410534208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0331 23:41:29.442142 140282704250688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0331 23:41:29.442299 140637852878656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0331 23:41:29.442429 140019994814272 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0331 23:41:29.442519 140039658387264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0331 23:41:29.442569 139865512077120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0331 23:41:29.443399 140588926613312 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0331 23:41:29.452448 140349087110976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0331 23:41:29.452752 140282704250688 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.452817 140349087110976 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.452967 140637852878656 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.453118 140039658387264 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.453168 140019994814272 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.453340 139865512077120 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.454026 140588926613312 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 23:41:29.461773 139671410534208 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
W0331 23:41:31.545842 140039658387264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:41:31.546021 140019994814272 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:41:31.546181 140637852878656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:41:31.546187 139865512077120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:41:31.546303 140282704250688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0331 23:41:31.558982 140349087110976 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/imagenet_vit_pytorch.
W0331 23:41:31.586924 140588926613312 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:41:31.590049 140349087110976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 23:41:31.591901 139671410534208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0331 23:41:31.593542 140349087110976 submission_runner.py:504] Using RNG seed 2556189187
I0331 23:41:31.594621 140349087110976 submission_runner.py:513] --- Tuning run 1/1 ---
I0331 23:41:31.594734 140349087110976 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1.
I0331 23:41:31.594955 140349087110976 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0331 23:41:31.595854 140349087110976 submission_runner.py:230] Starting train once: RAM USED (GB) 5.758758912
I0331 23:41:31.595947 140349087110976 submission_runner.py:231] Initializing dataset.
I0331 23:41:35.843170 140349087110976 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.77056256
I0331 23:41:35.843343 140349087110976 submission_runner.py:240] Initializing model.
I0331 23:41:40.242960 140349087110976 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.000187392
I0331 23:41:40.243139 140349087110976 submission_runner.py:252] Initializing optimizer.
I0331 23:41:40.244352 140349087110976 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.000187392
I0331 23:41:40.244443 140349087110976 submission_runner.py:261] Initializing metrics bundle.
I0331 23:41:40.244487 140349087110976 submission_runner.py:275] Initializing checkpoint and logger.
I0331 23:41:40.896529 140349087110976 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0331 23:41:40.898118 140349087110976 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0331 23:41:40.938626 140349087110976 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 18.051190784
I0331 23:41:40.939980 140349087110976 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.051190784
I0331 23:41:40.940098 140349087110976 submission_runner.py:312] Starting training loop.
I0331 23:41:43.401005 140349087110976 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 23.554736128
I0331 23:41:47.263228 140320230352640 logging_writer.py:48] [0] global_step=0, grad_norm=0.332036, loss=6.907756
I0331 23:41:47.275425 140349087110976 submission.py:119] 0) loss = 6.908, grad_norm = 0.332
I0331 23:41:47.276118 140349087110976 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 31.924842496
I0331 23:41:47.276715 140349087110976 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 31.925358592
I0331 23:41:47.276847 140349087110976 spec.py:298] Evaluating on the training split.
I0331 23:42:35.577536 140349087110976 spec.py:310] Evaluating on the validation split.
I0331 23:43:17.811949 140349087110976 spec.py:326] Evaluating on the test split.
I0331 23:43:17.827747 140349087110976 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 23:43:17.834358 140349087110976 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0331 23:43:17.916946 140349087110976 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 23:43:31.060807 140349087110976 submission_runner.py:380] Time since start: 6.34s, 	Step: 1, 	{'train/accuracy': 0.00224609375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00186, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0023, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0331 23:43:31.061230 140349087110976 submission_runner.py:390] After eval at step 1: RAM USED (GB) 93.091094528
I0331 23:43:31.070734 140315272935168 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.335194, test/accuracy=0.002300, test/loss=6.907755, test/num_examples=10000, total_duration=6.337453, train/accuracy=0.002246, train/loss=6.907756, validation/accuracy=0.001860, validation/loss=6.907756, validation/num_examples=50000
I0331 23:43:31.482973 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0331 23:43:31.483575 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 93.095010304
I0331 23:43:31.487077 140349087110976 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 93.095333888
I0331 23:43:31.493881 140349087110976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.493975 139865512077120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.494026 139671410534208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.494058 140039658387264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.494068 140282704250688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.494057 140588926613312 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.494041 140637852878656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:31.494582 140019994814272 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 23:43:32.099139 140315264542464 logging_writer.py:48] [1] global_step=1, grad_norm=0.341863, loss=6.907756
I0331 23:43:32.102534 140349087110976 submission.py:119] 1) loss = 6.908, grad_norm = 0.342
I0331 23:43:32.103382 140349087110976 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 93.120303104
I0331 23:43:32.514073 140315272935168 logging_writer.py:48] [2] global_step=2, grad_norm=0.345362, loss=6.907755
I0331 23:43:32.517370 140349087110976 submission.py:119] 2) loss = 6.908, grad_norm = 0.345
I0331 23:43:32.918171 140315264542464 logging_writer.py:48] [3] global_step=3, grad_norm=0.341095, loss=6.907752
I0331 23:43:32.921837 140349087110976 submission.py:119] 3) loss = 6.908, grad_norm = 0.341
I0331 23:43:33.323035 140315272935168 logging_writer.py:48] [4] global_step=4, grad_norm=0.339253, loss=6.907753
I0331 23:43:33.327043 140349087110976 submission.py:119] 4) loss = 6.908, grad_norm = 0.339
I0331 23:43:33.728647 140315264542464 logging_writer.py:48] [5] global_step=5, grad_norm=0.333374, loss=6.907754
I0331 23:43:33.732318 140349087110976 submission.py:119] 5) loss = 6.908, grad_norm = 0.333
I0331 23:43:34.135008 140315272935168 logging_writer.py:48] [6] global_step=6, grad_norm=0.351369, loss=6.907738
I0331 23:43:34.138623 140349087110976 submission.py:119] 6) loss = 6.908, grad_norm = 0.351
I0331 23:43:34.542865 140315264542464 logging_writer.py:48] [7] global_step=7, grad_norm=0.349641, loss=6.907743
I0331 23:43:34.546438 140349087110976 submission.py:119] 7) loss = 6.908, grad_norm = 0.350
I0331 23:43:34.949387 140315272935168 logging_writer.py:48] [8] global_step=8, grad_norm=0.341214, loss=6.907735
I0331 23:43:34.956281 140349087110976 submission.py:119] 8) loss = 6.908, grad_norm = 0.341
I0331 23:43:35.362874 140315264542464 logging_writer.py:48] [9] global_step=9, grad_norm=0.345659, loss=6.907737
I0331 23:43:35.368735 140349087110976 submission.py:119] 9) loss = 6.908, grad_norm = 0.346
I0331 23:43:35.772017 140315272935168 logging_writer.py:48] [10] global_step=10, grad_norm=0.341062, loss=6.907727
I0331 23:43:35.775638 140349087110976 submission.py:119] 10) loss = 6.908, grad_norm = 0.341
I0331 23:43:36.181884 140315264542464 logging_writer.py:48] [11] global_step=11, grad_norm=0.338579, loss=6.907719
I0331 23:43:36.185455 140349087110976 submission.py:119] 11) loss = 6.908, grad_norm = 0.339
I0331 23:43:36.592824 140315272935168 logging_writer.py:48] [12] global_step=12, grad_norm=0.345954, loss=6.907718
I0331 23:43:36.600536 140349087110976 submission.py:119] 12) loss = 6.908, grad_norm = 0.346
I0331 23:43:37.020220 140315264542464 logging_writer.py:48] [13] global_step=13, grad_norm=0.333359, loss=6.907709
I0331 23:43:37.024021 140349087110976 submission.py:119] 13) loss = 6.908, grad_norm = 0.333
I0331 23:43:37.427053 140315272935168 logging_writer.py:48] [14] global_step=14, grad_norm=0.346866, loss=6.907716
I0331 23:43:37.430451 140349087110976 submission.py:119] 14) loss = 6.908, grad_norm = 0.347
I0331 23:43:37.851807 140315264542464 logging_writer.py:48] [15] global_step=15, grad_norm=0.336465, loss=6.907703
I0331 23:43:37.855567 140349087110976 submission.py:119] 15) loss = 6.908, grad_norm = 0.336
I0331 23:43:38.264336 140315272935168 logging_writer.py:48] [16] global_step=16, grad_norm=0.334502, loss=6.907655
I0331 23:43:38.267971 140349087110976 submission.py:119] 16) loss = 6.908, grad_norm = 0.335
I0331 23:43:38.694976 140315264542464 logging_writer.py:48] [17] global_step=17, grad_norm=0.336700, loss=6.907630
I0331 23:43:38.699086 140349087110976 submission.py:119] 17) loss = 6.908, grad_norm = 0.337
I0331 23:43:39.111012 140315272935168 logging_writer.py:48] [18] global_step=18, grad_norm=0.344120, loss=6.907688
I0331 23:43:39.121581 140349087110976 submission.py:119] 18) loss = 6.908, grad_norm = 0.344
I0331 23:43:39.524650 140315264542464 logging_writer.py:48] [19] global_step=19, grad_norm=0.337783, loss=6.907642
I0331 23:43:39.528162 140349087110976 submission.py:119] 19) loss = 6.908, grad_norm = 0.338
I0331 23:43:39.932475 140315272935168 logging_writer.py:48] [20] global_step=20, grad_norm=0.335931, loss=6.907593
I0331 23:43:39.936255 140349087110976 submission.py:119] 20) loss = 6.908, grad_norm = 0.336
I0331 23:43:40.343944 140315264542464 logging_writer.py:48] [21] global_step=21, grad_norm=0.339410, loss=6.907564
I0331 23:43:40.347857 140349087110976 submission.py:119] 21) loss = 6.908, grad_norm = 0.339
I0331 23:43:40.750962 140315272935168 logging_writer.py:48] [22] global_step=22, grad_norm=0.339923, loss=6.907581
I0331 23:43:40.754586 140349087110976 submission.py:119] 22) loss = 6.908, grad_norm = 0.340
I0331 23:43:41.179167 140315264542464 logging_writer.py:48] [23] global_step=23, grad_norm=0.336651, loss=6.907591
I0331 23:43:41.183587 140349087110976 submission.py:119] 23) loss = 6.908, grad_norm = 0.337
I0331 23:43:41.584822 140315272935168 logging_writer.py:48] [24] global_step=24, grad_norm=0.340062, loss=6.907581
I0331 23:43:41.588171 140349087110976 submission.py:119] 24) loss = 6.908, grad_norm = 0.340
I0331 23:43:42.020931 140315264542464 logging_writer.py:48] [25] global_step=25, grad_norm=0.341676, loss=6.907408
I0331 23:43:42.025926 140349087110976 submission.py:119] 25) loss = 6.907, grad_norm = 0.342
I0331 23:43:42.429680 140315272935168 logging_writer.py:48] [26] global_step=26, grad_norm=0.345880, loss=6.907346
I0331 23:43:42.433573 140349087110976 submission.py:119] 26) loss = 6.907, grad_norm = 0.346
I0331 23:43:42.839071 140315264542464 logging_writer.py:48] [27] global_step=27, grad_norm=0.342756, loss=6.907383
I0331 23:43:42.843023 140349087110976 submission.py:119] 27) loss = 6.907, grad_norm = 0.343
I0331 23:43:43.246335 140315272935168 logging_writer.py:48] [28] global_step=28, grad_norm=0.364409, loss=6.907272
I0331 23:43:43.250487 140349087110976 submission.py:119] 28) loss = 6.907, grad_norm = 0.364
I0331 23:43:43.652589 140315264542464 logging_writer.py:48] [29] global_step=29, grad_norm=0.346624, loss=6.907364
I0331 23:43:43.656449 140349087110976 submission.py:119] 29) loss = 6.907, grad_norm = 0.347
I0331 23:43:44.063951 140315272935168 logging_writer.py:48] [30] global_step=30, grad_norm=0.358507, loss=6.907175
I0331 23:43:44.067599 140349087110976 submission.py:119] 30) loss = 6.907, grad_norm = 0.359
I0331 23:43:44.471901 140315264542464 logging_writer.py:48] [31] global_step=31, grad_norm=0.346279, loss=6.907244
I0331 23:43:44.476074 140349087110976 submission.py:119] 31) loss = 6.907, grad_norm = 0.346
I0331 23:43:44.878969 140315272935168 logging_writer.py:48] [32] global_step=32, grad_norm=0.343099, loss=6.907275
I0331 23:43:44.882797 140349087110976 submission.py:119] 32) loss = 6.907, grad_norm = 0.343
I0331 23:43:45.289649 140315264542464 logging_writer.py:48] [33] global_step=33, grad_norm=0.361376, loss=6.906919
I0331 23:43:45.293618 140349087110976 submission.py:119] 33) loss = 6.907, grad_norm = 0.361
I0331 23:43:45.697348 140315272935168 logging_writer.py:48] [34] global_step=34, grad_norm=0.354002, loss=6.906840
I0331 23:43:45.701770 140349087110976 submission.py:119] 34) loss = 6.907, grad_norm = 0.354
I0331 23:43:46.106964 140315264542464 logging_writer.py:48] [35] global_step=35, grad_norm=0.363812, loss=6.906969
I0331 23:43:46.110902 140349087110976 submission.py:119] 35) loss = 6.907, grad_norm = 0.364
I0331 23:43:46.518299 140315272935168 logging_writer.py:48] [36] global_step=36, grad_norm=0.363063, loss=6.906763
I0331 23:43:46.521982 140349087110976 submission.py:119] 36) loss = 6.907, grad_norm = 0.363
I0331 23:43:46.929628 140315264542464 logging_writer.py:48] [37] global_step=37, grad_norm=0.361557, loss=6.906773
I0331 23:43:46.938514 140349087110976 submission.py:119] 37) loss = 6.907, grad_norm = 0.362
I0331 23:43:47.344110 140315272935168 logging_writer.py:48] [38] global_step=38, grad_norm=0.355640, loss=6.906602
I0331 23:43:47.347712 140349087110976 submission.py:119] 38) loss = 6.907, grad_norm = 0.356
I0331 23:43:47.753309 140315264542464 logging_writer.py:48] [39] global_step=39, grad_norm=0.379509, loss=6.906646
I0331 23:43:47.757495 140349087110976 submission.py:119] 39) loss = 6.907, grad_norm = 0.380
I0331 23:43:48.161546 140315272935168 logging_writer.py:48] [40] global_step=40, grad_norm=0.374904, loss=6.906435
I0331 23:43:48.165081 140349087110976 submission.py:119] 40) loss = 6.906, grad_norm = 0.375
I0331 23:43:48.568509 140315264542464 logging_writer.py:48] [41] global_step=41, grad_norm=0.359421, loss=6.906368
I0331 23:43:48.572434 140349087110976 submission.py:119] 41) loss = 6.906, grad_norm = 0.359
I0331 23:43:48.979597 140315272935168 logging_writer.py:48] [42] global_step=42, grad_norm=0.375716, loss=6.906199
I0331 23:43:48.984016 140349087110976 submission.py:119] 42) loss = 6.906, grad_norm = 0.376
I0331 23:43:49.388771 140315264542464 logging_writer.py:48] [43] global_step=43, grad_norm=0.386208, loss=6.906077
I0331 23:43:49.392803 140349087110976 submission.py:119] 43) loss = 6.906, grad_norm = 0.386
I0331 23:43:49.797713 140315272935168 logging_writer.py:48] [44] global_step=44, grad_norm=0.380236, loss=6.905500
I0331 23:43:49.806712 140349087110976 submission.py:119] 44) loss = 6.906, grad_norm = 0.380
I0331 23:43:50.225829 140315264542464 logging_writer.py:48] [45] global_step=45, grad_norm=0.389060, loss=6.905489
I0331 23:43:50.229269 140349087110976 submission.py:119] 45) loss = 6.905, grad_norm = 0.389
I0331 23:43:50.631918 140315272935168 logging_writer.py:48] [46] global_step=46, grad_norm=0.395306, loss=6.905544
I0331 23:43:50.635719 140349087110976 submission.py:119] 46) loss = 6.906, grad_norm = 0.395
I0331 23:43:51.039846 140315264542464 logging_writer.py:48] [47] global_step=47, grad_norm=0.388528, loss=6.905668
I0331 23:43:51.044190 140349087110976 submission.py:119] 47) loss = 6.906, grad_norm = 0.389
I0331 23:43:51.448251 140315272935168 logging_writer.py:48] [48] global_step=48, grad_norm=0.399325, loss=6.905056
I0331 23:43:51.452167 140349087110976 submission.py:119] 48) loss = 6.905, grad_norm = 0.399
I0331 23:43:51.872172 140315264542464 logging_writer.py:48] [49] global_step=49, grad_norm=0.389029, loss=6.905149
I0331 23:43:51.875714 140349087110976 submission.py:119] 49) loss = 6.905, grad_norm = 0.389
I0331 23:43:52.279388 140315272935168 logging_writer.py:48] [50] global_step=50, grad_norm=0.386267, loss=6.905649
I0331 23:43:52.283437 140349087110976 submission.py:119] 50) loss = 6.906, grad_norm = 0.386
I0331 23:43:52.687147 140315264542464 logging_writer.py:48] [51] global_step=51, grad_norm=0.410825, loss=6.904984
I0331 23:43:52.690808 140349087110976 submission.py:119] 51) loss = 6.905, grad_norm = 0.411
I0331 23:43:53.095469 140315272935168 logging_writer.py:48] [52] global_step=52, grad_norm=0.409568, loss=6.904562
I0331 23:43:53.099442 140349087110976 submission.py:119] 52) loss = 6.905, grad_norm = 0.410
I0331 23:43:53.505945 140315264542464 logging_writer.py:48] [53] global_step=53, grad_norm=0.393857, loss=6.904712
I0331 23:43:53.510643 140349087110976 submission.py:119] 53) loss = 6.905, grad_norm = 0.394
I0331 23:43:53.933125 140315272935168 logging_writer.py:48] [54] global_step=54, grad_norm=0.411826, loss=6.904459
I0331 23:43:53.939373 140349087110976 submission.py:119] 54) loss = 6.904, grad_norm = 0.412
I0331 23:43:54.370267 140315264542464 logging_writer.py:48] [55] global_step=55, grad_norm=0.402870, loss=6.904325
I0331 23:43:54.374195 140349087110976 submission.py:119] 55) loss = 6.904, grad_norm = 0.403
I0331 23:43:54.778192 140315272935168 logging_writer.py:48] [56] global_step=56, grad_norm=0.395642, loss=6.903801
I0331 23:43:54.781853 140349087110976 submission.py:119] 56) loss = 6.904, grad_norm = 0.396
I0331 23:43:55.190566 140315264542464 logging_writer.py:48] [57] global_step=57, grad_norm=0.409744, loss=6.904137
I0331 23:43:55.194640 140349087110976 submission.py:119] 57) loss = 6.904, grad_norm = 0.410
I0331 23:43:55.611860 140315272935168 logging_writer.py:48] [58] global_step=58, grad_norm=0.415581, loss=6.903452
I0331 23:43:55.615532 140349087110976 submission.py:119] 58) loss = 6.903, grad_norm = 0.416
I0331 23:43:56.034042 140315264542464 logging_writer.py:48] [59] global_step=59, grad_norm=0.414808, loss=6.903158
I0331 23:43:56.038102 140349087110976 submission.py:119] 59) loss = 6.903, grad_norm = 0.415
I0331 23:43:56.443860 140315272935168 logging_writer.py:48] [60] global_step=60, grad_norm=0.418508, loss=6.902858
I0331 23:43:56.447781 140349087110976 submission.py:119] 60) loss = 6.903, grad_norm = 0.419
I0331 23:43:56.852618 140315264542464 logging_writer.py:48] [61] global_step=61, grad_norm=0.409638, loss=6.902765
I0331 23:43:56.857410 140349087110976 submission.py:119] 61) loss = 6.903, grad_norm = 0.410
I0331 23:43:57.276776 140315272935168 logging_writer.py:48] [62] global_step=62, grad_norm=0.421423, loss=6.902715
I0331 23:43:57.284429 140349087110976 submission.py:119] 62) loss = 6.903, grad_norm = 0.421
I0331 23:43:57.690891 140315264542464 logging_writer.py:48] [63] global_step=63, grad_norm=0.426024, loss=6.901658
I0331 23:43:57.694531 140349087110976 submission.py:119] 63) loss = 6.902, grad_norm = 0.426
I0331 23:43:58.107859 140315272935168 logging_writer.py:48] [64] global_step=64, grad_norm=0.414734, loss=6.902835
I0331 23:43:58.112222 140349087110976 submission.py:119] 64) loss = 6.903, grad_norm = 0.415
I0331 23:43:58.519871 140315264542464 logging_writer.py:48] [65] global_step=65, grad_norm=0.435463, loss=6.902616
I0331 23:43:58.523627 140349087110976 submission.py:119] 65) loss = 6.903, grad_norm = 0.435
I0331 23:43:58.928588 140315272935168 logging_writer.py:48] [66] global_step=66, grad_norm=0.438341, loss=6.900910
I0331 23:43:58.932225 140349087110976 submission.py:119] 66) loss = 6.901, grad_norm = 0.438
I0331 23:43:59.335149 140315264542464 logging_writer.py:48] [67] global_step=67, grad_norm=0.426695, loss=6.901844
I0331 23:43:59.338990 140349087110976 submission.py:119] 67) loss = 6.902, grad_norm = 0.427
I0331 23:43:59.756964 140315272935168 logging_writer.py:48] [68] global_step=68, grad_norm=0.441628, loss=6.899376
I0331 23:43:59.762219 140349087110976 submission.py:119] 68) loss = 6.899, grad_norm = 0.442
I0331 23:44:00.175156 140315264542464 logging_writer.py:48] [69] global_step=69, grad_norm=0.416540, loss=6.902012
I0331 23:44:00.179213 140349087110976 submission.py:119] 69) loss = 6.902, grad_norm = 0.417
I0331 23:44:00.584804 140315272935168 logging_writer.py:48] [70] global_step=70, grad_norm=0.416732, loss=6.901910
I0331 23:44:00.588189 140349087110976 submission.py:119] 70) loss = 6.902, grad_norm = 0.417
I0331 23:44:01.007302 140315264542464 logging_writer.py:48] [71] global_step=71, grad_norm=0.437080, loss=6.901173
I0331 23:44:01.011671 140349087110976 submission.py:119] 71) loss = 6.901, grad_norm = 0.437
I0331 23:44:01.447633 140315272935168 logging_writer.py:48] [72] global_step=72, grad_norm=0.412693, loss=6.900061
I0331 23:44:01.453024 140349087110976 submission.py:119] 72) loss = 6.900, grad_norm = 0.413
I0331 23:44:01.864162 140315264542464 logging_writer.py:48] [73] global_step=73, grad_norm=0.449445, loss=6.898629
I0331 23:44:01.868144 140349087110976 submission.py:119] 73) loss = 6.899, grad_norm = 0.449
I0331 23:44:02.286647 140315272935168 logging_writer.py:48] [74] global_step=74, grad_norm=0.444719, loss=6.901106
I0331 23:44:02.290737 140349087110976 submission.py:119] 74) loss = 6.901, grad_norm = 0.445
I0331 23:44:02.695564 140315264542464 logging_writer.py:48] [75] global_step=75, grad_norm=0.432537, loss=6.897970
I0331 23:44:02.699688 140349087110976 submission.py:119] 75) loss = 6.898, grad_norm = 0.433
I0331 23:44:03.101521 140315272935168 logging_writer.py:48] [76] global_step=76, grad_norm=0.461724, loss=6.898813
I0331 23:44:03.106081 140349087110976 submission.py:119] 76) loss = 6.899, grad_norm = 0.462
I0331 23:44:03.523128 140315264542464 logging_writer.py:48] [77] global_step=77, grad_norm=0.443338, loss=6.898233
I0331 23:44:03.526651 140349087110976 submission.py:119] 77) loss = 6.898, grad_norm = 0.443
I0331 23:44:03.928112 140315272935168 logging_writer.py:48] [78] global_step=78, grad_norm=0.476056, loss=6.896888
I0331 23:44:03.932038 140349087110976 submission.py:119] 78) loss = 6.897, grad_norm = 0.476
I0331 23:44:04.336425 140315264542464 logging_writer.py:48] [79] global_step=79, grad_norm=0.438504, loss=6.897539
I0331 23:44:04.340176 140349087110976 submission.py:119] 79) loss = 6.898, grad_norm = 0.439
I0331 23:44:04.744571 140315272935168 logging_writer.py:48] [80] global_step=80, grad_norm=0.450792, loss=6.898175
I0331 23:44:04.748702 140349087110976 submission.py:119] 80) loss = 6.898, grad_norm = 0.451
I0331 23:44:05.166164 140315264542464 logging_writer.py:48] [81] global_step=81, grad_norm=0.444156, loss=6.896915
I0331 23:44:05.170017 140349087110976 submission.py:119] 81) loss = 6.897, grad_norm = 0.444
I0331 23:44:05.586059 140315272935168 logging_writer.py:48] [82] global_step=82, grad_norm=0.441972, loss=6.896184
I0331 23:44:05.589885 140349087110976 submission.py:119] 82) loss = 6.896, grad_norm = 0.442
I0331 23:44:06.001120 140315264542464 logging_writer.py:48] [83] global_step=83, grad_norm=0.447877, loss=6.897733
I0331 23:44:06.005057 140349087110976 submission.py:119] 83) loss = 6.898, grad_norm = 0.448
I0331 23:44:06.408702 140315272935168 logging_writer.py:48] [84] global_step=84, grad_norm=0.455422, loss=6.897018
I0331 23:44:06.412629 140349087110976 submission.py:119] 84) loss = 6.897, grad_norm = 0.455
I0331 23:44:06.825931 140315264542464 logging_writer.py:48] [85] global_step=85, grad_norm=0.469069, loss=6.894936
I0331 23:44:06.830025 140349087110976 submission.py:119] 85) loss = 6.895, grad_norm = 0.469
I0331 23:44:07.236211 140315272935168 logging_writer.py:48] [86] global_step=86, grad_norm=0.450870, loss=6.896441
I0331 23:44:07.240129 140349087110976 submission.py:119] 86) loss = 6.896, grad_norm = 0.451
I0331 23:44:07.650780 140315264542464 logging_writer.py:48] [87] global_step=87, grad_norm=0.463128, loss=6.894853
I0331 23:44:07.654428 140349087110976 submission.py:119] 87) loss = 6.895, grad_norm = 0.463
I0331 23:44:08.059489 140315272935168 logging_writer.py:48] [88] global_step=88, grad_norm=0.444276, loss=6.898332
I0331 23:44:08.063180 140349087110976 submission.py:119] 88) loss = 6.898, grad_norm = 0.444
I0331 23:44:08.484546 140315264542464 logging_writer.py:48] [89] global_step=89, grad_norm=0.442528, loss=6.893567
I0331 23:44:08.489349 140349087110976 submission.py:119] 89) loss = 6.894, grad_norm = 0.443
I0331 23:44:08.901789 140315272935168 logging_writer.py:48] [90] global_step=90, grad_norm=0.477082, loss=6.894240
I0331 23:44:08.905311 140349087110976 submission.py:119] 90) loss = 6.894, grad_norm = 0.477
I0331 23:44:09.311732 140315264542464 logging_writer.py:48] [91] global_step=91, grad_norm=0.468796, loss=6.893754
I0331 23:44:09.315254 140349087110976 submission.py:119] 91) loss = 6.894, grad_norm = 0.469
I0331 23:44:09.734642 140315272935168 logging_writer.py:48] [92] global_step=92, grad_norm=0.466324, loss=6.894200
I0331 23:44:09.738620 140349087110976 submission.py:119] 92) loss = 6.894, grad_norm = 0.466
I0331 23:44:10.146524 140315264542464 logging_writer.py:48] [93] global_step=93, grad_norm=0.468166, loss=6.891438
I0331 23:44:10.150495 140349087110976 submission.py:119] 93) loss = 6.891, grad_norm = 0.468
I0331 23:44:10.556622 140315272935168 logging_writer.py:48] [94] global_step=94, grad_norm=0.455592, loss=6.892721
I0331 23:44:10.560316 140349087110976 submission.py:119] 94) loss = 6.893, grad_norm = 0.456
I0331 23:44:10.964289 140315264542464 logging_writer.py:48] [95] global_step=95, grad_norm=0.434521, loss=6.896287
I0331 23:44:10.968901 140349087110976 submission.py:119] 95) loss = 6.896, grad_norm = 0.435
I0331 23:44:11.375169 140315272935168 logging_writer.py:48] [96] global_step=96, grad_norm=0.447404, loss=6.893139
I0331 23:44:11.379023 140349087110976 submission.py:119] 96) loss = 6.893, grad_norm = 0.447
I0331 23:44:11.806447 140315264542464 logging_writer.py:48] [97] global_step=97, grad_norm=0.470927, loss=6.892885
I0331 23:44:11.810008 140349087110976 submission.py:119] 97) loss = 6.893, grad_norm = 0.471
I0331 23:44:12.216382 140315272935168 logging_writer.py:48] [98] global_step=98, grad_norm=0.434665, loss=6.891815
I0331 23:44:12.220069 140349087110976 submission.py:119] 98) loss = 6.892, grad_norm = 0.435
I0331 23:44:12.623792 140315264542464 logging_writer.py:48] [99] global_step=99, grad_norm=0.445350, loss=6.889606
I0331 23:44:12.627260 140349087110976 submission.py:119] 99) loss = 6.890, grad_norm = 0.445
I0331 23:44:13.031367 140315272935168 logging_writer.py:48] [100] global_step=100, grad_norm=0.484923, loss=6.892511
I0331 23:44:13.035267 140349087110976 submission.py:119] 100) loss = 6.893, grad_norm = 0.485
I0331 23:46:53.421639 140315264542464 logging_writer.py:48] [500] global_step=500, grad_norm=0.718094, loss=6.686487
I0331 23:46:53.427240 140349087110976 submission.py:119] 500) loss = 6.686, grad_norm = 0.718
I0331 23:50:13.838654 140315272935168 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.780548, loss=6.442385
I0331 23:50:13.843705 140349087110976 submission.py:119] 1000) loss = 6.442, grad_norm = 0.781
I0331 23:50:31.788067 140349087110976 submission_runner.py:371] Before eval at step 1046: RAM USED (GB) 99.212775424
I0331 23:50:31.788290 140349087110976 spec.py:298] Evaluating on the training split.
I0331 23:51:15.085862 140349087110976 spec.py:310] Evaluating on the validation split.
I0331 23:52:01.272723 140349087110976 spec.py:326] Evaluating on the test split.
I0331 23:52:02.742033 140349087110976 submission_runner.py:380] Time since start: 530.85s, 	Step: 1046, 	{'train/accuracy': 0.03189453125, 'train/loss': 5.960023803710937, 'validation/accuracy': 0.02988, 'validation/loss': 5.982645, 'validation/num_examples': 50000, 'test/accuracy': 0.021, 'test/loss': 6.1151890625, 'test/num_examples': 10000}
I0331 23:52:02.742417 140349087110976 submission_runner.py:390] After eval at step 1046: RAM USED (GB) 98.846482432
I0331 23:52:02.751013 140306070611712 logging_writer.py:48] [1046] global_step=1046, preemption_count=0, score=424.270649, test/accuracy=0.021000, test/loss=6.115189, test/num_examples=10000, total_duration=530.848353, train/accuracy=0.031895, train/loss=5.960024, validation/accuracy=0.029880, validation/loss=5.982645, validation/num_examples=50000
I0331 23:52:03.155709 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_1046.
I0331 23:52:03.156494 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 1046: RAM USED (GB) 98.845536256
I0331 23:55:07.122498 140306079004416 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.973210, loss=6.257354
I0331 23:55:07.126681 140349087110976 submission.py:119] 1500) loss = 6.257, grad_norm = 0.973
I0331 23:58:27.805460 140306070611712 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.985640, loss=6.035466
I0331 23:58:27.810205 140349087110976 submission.py:119] 2000) loss = 6.035, grad_norm = 0.986
I0331 23:59:03.550061 140349087110976 submission_runner.py:371] Before eval at step 2090: RAM USED (GB) 100.191305728
I0331 23:59:03.550279 140349087110976 spec.py:298] Evaluating on the training split.
I0331 23:59:46.735231 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:00:33.260462 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:00:34.675753 140349087110976 submission_runner.py:380] Time since start: 1042.61s, 	Step: 2090, 	{'train/accuracy': 0.0784375, 'train/loss': 5.261727294921875, 'validation/accuracy': 0.07458, 'validation/loss': 5.304615, 'validation/num_examples': 50000, 'test/accuracy': 0.0549, 'test/loss': 5.534515625, 'test/num_examples': 10000}
I0401 00:00:34.676083 140349087110976 submission_runner.py:390] After eval at step 2090: RAM USED (GB) 100.331524096
I0401 00:00:34.683735 140306079004416 logging_writer.py:48] [2090] global_step=2090, preemption_count=0, score=842.206819, test/accuracy=0.054900, test/loss=5.534516, test/num_examples=10000, total_duration=1042.610053, train/accuracy=0.078437, train/loss=5.261727, validation/accuracy=0.074580, validation/loss=5.304615, validation/num_examples=50000
I0401 00:00:35.106482 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_2090.
I0401 00:00:35.107164 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 2090: RAM USED (GB) 100.330606592
I0401 00:03:19.640701 140306070611712 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.932105, loss=5.850089
I0401 00:03:19.643920 140349087110976 submission.py:119] 2500) loss = 5.850, grad_norm = 0.932
I0401 00:06:42.840985 140306079004416 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.241298, loss=5.830763
I0401 00:06:42.845237 140349087110976 submission.py:119] 3000) loss = 5.831, grad_norm = 1.241
I0401 00:07:35.376178 140349087110976 submission_runner.py:371] Before eval at step 3132: RAM USED (GB) 100.693356544
I0401 00:07:35.376409 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:08:18.639590 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:09:03.285148 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:09:04.701807 140349087110976 submission_runner.py:380] Time since start: 1554.44s, 	Step: 3132, 	{'train/accuracy': 0.13818359375, 'train/loss': 4.729986572265625, 'validation/accuracy': 0.12916, 'validation/loss': 4.785410625, 'validation/num_examples': 50000, 'test/accuracy': 0.0959, 'test/loss': 5.08208671875, 'test/num_examples': 10000}
I0401 00:09:04.702128 140349087110976 submission_runner.py:390] After eval at step 3132: RAM USED (GB) 100.80401408
I0401 00:09:04.709945 140306070611712 logging_writer.py:48] [3132] global_step=3132, preemption_count=0, score=1260.011850, test/accuracy=0.095900, test/loss=5.082087, test/num_examples=10000, total_duration=1554.436196, train/accuracy=0.138184, train/loss=4.729987, validation/accuracy=0.129160, validation/loss=4.785411, validation/num_examples=50000
I0401 00:09:05.120392 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_3132.
I0401 00:09:05.121171 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 3132: RAM USED (GB) 100.803084288
I0401 00:11:32.882617 140306079004416 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.058949, loss=5.668328
I0401 00:11:32.887930 140349087110976 submission.py:119] 3500) loss = 5.668, grad_norm = 1.059
I0401 00:14:55.258417 140306070611712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.961323, loss=5.693782
I0401 00:14:55.263597 140349087110976 submission.py:119] 4000) loss = 5.694, grad_norm = 0.961
I0401 00:16:05.153877 140349087110976 submission_runner.py:371] Before eval at step 4175: RAM USED (GB) 101.20230912
I0401 00:16:05.154103 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:16:48.412929 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:17:32.192310 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:17:33.608221 140349087110976 submission_runner.py:380] Time since start: 2064.21s, 	Step: 4175, 	{'train/accuracy': 0.19943359375, 'train/loss': 4.221939392089844, 'validation/accuracy': 0.18408, 'validation/loss': 4.312911875, 'validation/num_examples': 50000, 'test/accuracy': 0.1444, 'test/loss': 4.645348828125, 'test/num_examples': 10000}
I0401 00:17:33.608580 140349087110976 submission_runner.py:390] After eval at step 4175: RAM USED (GB) 101.01489664
I0401 00:17:33.617010 140306079004416 logging_writer.py:48] [4175] global_step=4175, preemption_count=0, score=1677.635936, test/accuracy=0.144400, test/loss=4.645349, test/num_examples=10000, total_duration=2064.213850, train/accuracy=0.199434, train/loss=4.221939, validation/accuracy=0.184080, validation/loss=4.312912, validation/num_examples=50000
I0401 00:17:34.053489 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_4175.
I0401 00:17:34.054310 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 4175: RAM USED (GB) 101.01501952
I0401 00:19:45.212477 140306070611712 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.884374, loss=5.262960
I0401 00:19:45.217951 140349087110976 submission.py:119] 4500) loss = 5.263, grad_norm = 0.884
I0401 00:23:05.258336 140306079004416 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.830044, loss=5.333301
I0401 00:23:05.262689 140349087110976 submission.py:119] 5000) loss = 5.333, grad_norm = 0.830
I0401 00:24:34.097540 140349087110976 submission_runner.py:371] Before eval at step 5216: RAM USED (GB) 100.812017664
I0401 00:24:34.097761 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:25:17.347810 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:26:01.605238 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:26:03.034070 140349087110976 submission_runner.py:380] Time since start: 2573.16s, 	Step: 5216, 	{'train/accuracy': 0.24181640625, 'train/loss': 3.854183349609375, 'validation/accuracy': 0.2243, 'validation/loss': 3.9674240625, 'validation/num_examples': 50000, 'test/accuracy': 0.1663, 'test/loss': 4.4006328125, 'test/num_examples': 10000}
I0401 00:26:03.034409 140349087110976 submission_runner.py:390] After eval at step 5216: RAM USED (GB) 100.995866624
I0401 00:26:03.042471 140306070611712 logging_writer.py:48] [5216] global_step=5216, preemption_count=0, score=2095.312510, test/accuracy=0.166300, test/loss=4.400633, test/num_examples=10000, total_duration=2573.157615, train/accuracy=0.241816, train/loss=3.854183, validation/accuracy=0.224300, validation/loss=3.967424, validation/num_examples=50000
I0401 00:26:03.491398 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_5216.
I0401 00:26:03.492097 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 5216: RAM USED (GB) 100.995182592
I0401 00:27:57.950387 140306079004416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.840787, loss=5.246032
I0401 00:27:57.954450 140349087110976 submission.py:119] 5500) loss = 5.246, grad_norm = 0.841
I0401 00:31:18.389717 140306070611712 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.880829, loss=5.229648
I0401 00:31:18.394279 140349087110976 submission.py:119] 6000) loss = 5.230, grad_norm = 0.881
I0401 00:33:03.867704 140349087110976 submission_runner.py:371] Before eval at step 6260: RAM USED (GB) 100.992516096
I0401 00:33:03.867984 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:33:48.267439 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:34:34.987522 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:34:36.405351 140349087110976 submission_runner.py:380] Time since start: 3082.93s, 	Step: 6260, 	{'train/accuracy': 0.28767578125, 'train/loss': 3.542598876953125, 'validation/accuracy': 0.26468, 'validation/loss': 3.673434375, 'validation/num_examples': 50000, 'test/accuracy': 0.2007, 'test/loss': 4.138240625, 'test/num_examples': 10000}
I0401 00:34:36.405667 140349087110976 submission_runner.py:390] After eval at step 6260: RAM USED (GB) 101.0291712
I0401 00:34:36.414268 140306079004416 logging_writer.py:48] [6260] global_step=6260, preemption_count=0, score=2513.310442, test/accuracy=0.200700, test/loss=4.138241, test/num_examples=10000, total_duration=3082.927575, train/accuracy=0.287676, train/loss=3.542599, validation/accuracy=0.264680, validation/loss=3.673434, validation/num_examples=50000
I0401 00:34:36.832158 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_6260.
I0401 00:34:36.832836 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 6260: RAM USED (GB) 101.028261888
I0401 00:36:13.466874 140306070611712 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.789119, loss=5.399584
I0401 00:36:13.470852 140349087110976 submission.py:119] 6500) loss = 5.400, grad_norm = 0.789
I0401 00:39:34.362975 140306079004416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.843816, loss=4.736368
I0401 00:39:34.367631 140349087110976 submission.py:119] 7000) loss = 4.736, grad_norm = 0.844
I0401 00:41:37.069836 140349087110976 submission_runner.py:371] Before eval at step 7307: RAM USED (GB) 101.124923392
I0401 00:41:37.070057 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:42:20.265676 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:43:04.121735 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:43:05.537030 140349087110976 submission_runner.py:380] Time since start: 3596.13s, 	Step: 7307, 	{'train/accuracy': 0.3244921875, 'train/loss': 3.2925213623046874, 'validation/accuracy': 0.30102, 'validation/loss': 3.4244684375, 'validation/num_examples': 50000, 'test/accuracy': 0.2318, 'test/loss': 3.918516796875, 'test/num_examples': 10000}
I0401 00:43:05.537405 140349087110976 submission_runner.py:390] After eval at step 7307: RAM USED (GB) 101.255159808
I0401 00:43:05.544926 140306070611712 logging_writer.py:48] [7307] global_step=7307, preemption_count=0, score=2931.127057, test/accuracy=0.231800, test/loss=3.918517, test/num_examples=10000, total_duration=3596.129922, train/accuracy=0.324492, train/loss=3.292521, validation/accuracy=0.301020, validation/loss=3.424468, validation/num_examples=50000
I0401 00:43:05.960113 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_7307.
I0401 00:43:05.960838 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 7307: RAM USED (GB) 101.254242304
I0401 00:44:23.527755 140306079004416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.772046, loss=5.403991
I0401 00:44:23.531714 140349087110976 submission.py:119] 7500) loss = 5.404, grad_norm = 0.772
I0401 00:47:46.475520 140306070611712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.782967, loss=5.121727
I0401 00:47:46.480882 140349087110976 submission.py:119] 8000) loss = 5.122, grad_norm = 0.783
I0401 00:50:06.262238 140349087110976 submission_runner.py:371] Before eval at step 8349: RAM USED (GB) 101.138526208
I0401 00:50:06.262465 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:50:49.965539 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 00:51:33.785073 140349087110976 spec.py:326] Evaluating on the test split.
I0401 00:51:35.204631 140349087110976 submission_runner.py:380] Time since start: 4105.32s, 	Step: 8349, 	{'train/accuracy': 0.35958984375, 'train/loss': 3.0853927612304686, 'validation/accuracy': 0.33092, 'validation/loss': 3.2445625, 'validation/num_examples': 50000, 'test/accuracy': 0.2587, 'test/loss': 3.773323828125, 'test/num_examples': 10000}
I0401 00:51:35.204946 140349087110976 submission_runner.py:390] After eval at step 8349: RAM USED (GB) 101.083541504
I0401 00:51:35.212801 140306079004416 logging_writer.py:48] [8349] global_step=8349, preemption_count=0, score=3349.008539, test/accuracy=0.258700, test/loss=3.773324, test/num_examples=10000, total_duration=4105.322270, train/accuracy=0.359590, train/loss=3.085393, validation/accuracy=0.330920, validation/loss=3.244562, validation/num_examples=50000
I0401 00:51:35.628776 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_8349.
I0401 00:51:35.629560 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 8349: RAM USED (GB) 101.083144192
I0401 00:52:36.417190 140306070611712 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.664158, loss=4.928689
I0401 00:52:36.422807 140349087110976 submission.py:119] 8500) loss = 4.929, grad_norm = 0.664
I0401 00:55:59.287284 140306079004416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.614475, loss=4.833158
I0401 00:55:59.291955 140349087110976 submission.py:119] 9000) loss = 4.833, grad_norm = 0.614
I0401 00:58:35.673402 140349087110976 submission_runner.py:371] Before eval at step 9390: RAM USED (GB) 100.95613952
I0401 00:58:35.673638 140349087110976 spec.py:298] Evaluating on the training split.
I0401 00:59:18.616138 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 01:00:02.692909 140349087110976 spec.py:326] Evaluating on the test split.
I0401 01:00:04.112235 140349087110976 submission_runner.py:380] Time since start: 4614.73s, 	Step: 9390, 	{'train/accuracy': 0.39736328125, 'train/loss': 2.8511282348632814, 'validation/accuracy': 0.36344, 'validation/loss': 3.0291284375, 'validation/num_examples': 50000, 'test/accuracy': 0.282, 'test/loss': 3.563748828125, 'test/num_examples': 10000}
I0401 01:00:04.112581 140349087110976 submission_runner.py:390] After eval at step 9390: RAM USED (GB) 101.189763072
I0401 01:00:04.120268 140306070611712 logging_writer.py:48] [9390] global_step=9390, preemption_count=0, score=3766.664099, test/accuracy=0.282000, test/loss=3.563749, test/num_examples=10000, total_duration=4614.733295, train/accuracy=0.397363, train/loss=2.851128, validation/accuracy=0.363440, validation/loss=3.029128, validation/num_examples=50000
I0401 01:00:04.531965 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_9390.
I0401 01:00:04.532637 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 9390: RAM USED (GB) 101.188837376
I0401 01:00:48.893750 140306079004416 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.683535, loss=4.730424
I0401 01:00:48.899276 140349087110976 submission.py:119] 9500) loss = 4.730, grad_norm = 0.684
I0401 01:04:09.166013 140306070611712 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.616946, loss=4.817332
I0401 01:04:09.170837 140349087110976 submission.py:119] 10000) loss = 4.817, grad_norm = 0.617
I0401 01:07:04.846053 140349087110976 submission_runner.py:371] Before eval at step 10434: RAM USED (GB) 101.013659648
I0401 01:07:04.846332 140349087110976 spec.py:298] Evaluating on the training split.
I0401 01:07:47.518746 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 01:08:32.153905 140349087110976 spec.py:326] Evaluating on the test split.
I0401 01:08:33.571022 140349087110976 submission_runner.py:380] Time since start: 5123.91s, 	Step: 10434, 	{'train/accuracy': 0.431171875, 'train/loss': 2.68815673828125, 'validation/accuracy': 0.39512, 'validation/loss': 2.8714853125, 'validation/num_examples': 50000, 'test/accuracy': 0.306, 'test/loss': 3.41387265625, 'test/num_examples': 10000}
I0401 01:08:33.571363 140349087110976 submission_runner.py:390] After eval at step 10434: RAM USED (GB) 100.995309568
I0401 01:08:33.580014 140306079004416 logging_writer.py:48] [10434] global_step=10434, preemption_count=0, score=4184.580236, test/accuracy=0.306000, test/loss=3.413873, test/num_examples=10000, total_duration=5123.905997, train/accuracy=0.431172, train/loss=2.688157, validation/accuracy=0.395120, validation/loss=2.871485, validation/num_examples=50000
I0401 01:08:33.995138 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_10434.
I0401 01:08:33.995811 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 10434: RAM USED (GB) 100.994916352
I0401 01:09:00.869495 140306070611712 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.641079, loss=4.656505
I0401 01:09:00.875649 140349087110976 submission.py:119] 10500) loss = 4.657, grad_norm = 0.641
I0401 01:12:21.490056 140306079004416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.806150, loss=4.472095
I0401 01:12:21.494419 140349087110976 submission.py:119] 11000) loss = 4.472, grad_norm = 0.806
I0401 01:15:34.183231 140349087110976 submission_runner.py:371] Before eval at step 11477: RAM USED (GB) 101.053943808
I0401 01:15:34.183480 140349087110976 spec.py:298] Evaluating on the training split.
I0401 01:16:18.026329 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 01:17:05.282098 140349087110976 spec.py:326] Evaluating on the test split.
I0401 01:17:06.696800 140349087110976 submission_runner.py:380] Time since start: 5633.24s, 	Step: 11477, 	{'train/accuracy': 0.46994140625, 'train/loss': 2.4739622497558593, 'validation/accuracy': 0.4326, 'validation/loss': 2.6748953125, 'validation/num_examples': 50000, 'test/accuracy': 0.3315, 'test/loss': 3.2608013671875, 'test/num_examples': 10000}
I0401 01:17:06.697140 140349087110976 submission_runner.py:390] After eval at step 11477: RAM USED (GB) 101.126004736
I0401 01:17:06.705072 140306070611712 logging_writer.py:48] [11477] global_step=11477, preemption_count=0, score=4602.385396, test/accuracy=0.331500, test/loss=3.260801, test/num_examples=10000, total_duration=5633.243368, train/accuracy=0.469941, train/loss=2.473962, validation/accuracy=0.432600, validation/loss=2.674895, validation/num_examples=50000
I0401 01:17:07.132785 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_11477.
I0401 01:17:07.133498 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 11477: RAM USED (GB) 101.125083136
I0401 01:17:16.733430 140306079004416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.659784, loss=4.243658
I0401 01:17:16.736674 140349087110976 submission.py:119] 11500) loss = 4.244, grad_norm = 0.660
I0401 01:20:37.757523 140306070611712 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.654063, loss=4.467399
I0401 01:20:37.762124 140349087110976 submission.py:119] 12000) loss = 4.467, grad_norm = 0.654
I0401 01:23:57.758922 140306079004416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.662822, loss=4.402062
I0401 01:23:57.763606 140349087110976 submission.py:119] 12500) loss = 4.402, grad_norm = 0.663
I0401 01:24:07.521221 140349087110976 submission_runner.py:371] Before eval at step 12521: RAM USED (GB) 101.165686784
I0401 01:24:07.521425 140349087110976 spec.py:298] Evaluating on the training split.
I0401 01:24:51.953425 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 01:25:38.117681 140349087110976 spec.py:326] Evaluating on the test split.
I0401 01:25:39.533699 140349087110976 submission_runner.py:380] Time since start: 6146.58s, 	Step: 12521, 	{'train/accuracy': 0.4896875, 'train/loss': 2.410396728515625, 'validation/accuracy': 0.4517, 'validation/loss': 2.6056484375, 'validation/num_examples': 50000, 'test/accuracy': 0.3579, 'test/loss': 3.1715791015625, 'test/num_examples': 10000}
I0401 01:25:39.534144 140349087110976 submission_runner.py:390] After eval at step 12521: RAM USED (GB) 101.236174848
I0401 01:25:39.543541 140306070611712 logging_writer.py:48] [12521] global_step=12521, preemption_count=0, score=5020.405760, test/accuracy=0.357900, test/loss=3.171579, test/num_examples=10000, total_duration=6146.581406, train/accuracy=0.489687, train/loss=2.410397, validation/accuracy=0.451700, validation/loss=2.605648, validation/num_examples=50000
I0401 01:25:39.979509 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_12521.
I0401 01:25:39.980380 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 12521: RAM USED (GB) 101.235781632
I0401 01:28:53.134376 140306079004416 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.669402, loss=4.452141
I0401 01:28:53.138301 140349087110976 submission.py:119] 13000) loss = 4.452, grad_norm = 0.669
I0401 01:32:13.617669 140306070611712 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.582452, loss=4.310282
I0401 01:32:13.623018 140349087110976 submission.py:119] 13500) loss = 4.310, grad_norm = 0.582
I0401 01:32:40.072494 140349087110976 submission_runner.py:371] Before eval at step 13567: RAM USED (GB) 100.965343232
I0401 01:32:40.072709 140349087110976 spec.py:298] Evaluating on the training split.
I0401 01:33:23.439280 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 01:34:08.107661 140349087110976 spec.py:326] Evaluating on the test split.
I0401 01:34:09.525207 140349087110976 submission_runner.py:380] Time since start: 6659.13s, 	Step: 13567, 	{'train/accuracy': 0.5203125, 'train/loss': 2.2009754943847657, 'validation/accuracy': 0.47888, 'validation/loss': 2.42013515625, 'validation/num_examples': 50000, 'test/accuracy': 0.3779, 'test/loss': 3.0206734375, 'test/num_examples': 10000}
I0401 01:34:09.525638 140349087110976 submission_runner.py:390] After eval at step 13567: RAM USED (GB) 100.877144064
I0401 01:34:09.534648 140306079004416 logging_writer.py:48] [13567] global_step=13567, preemption_count=0, score=5438.079477, test/accuracy=0.377900, test/loss=3.020673, test/num_examples=10000, total_duration=6659.132614, train/accuracy=0.520312, train/loss=2.200975, validation/accuracy=0.478880, validation/loss=2.420135, validation/num_examples=50000
I0401 01:34:09.967207 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_13567.
I0401 01:34:09.968001 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 13567: RAM USED (GB) 100.871909376
I0401 01:37:05.297665 140349087110976 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 101.02331392
I0401 01:37:05.297956 140349087110976 spec.py:298] Evaluating on the training split.
I0401 01:37:48.391668 140349087110976 spec.py:310] Evaluating on the validation split.
I0401 01:38:32.583992 140349087110976 spec.py:326] Evaluating on the test split.
I0401 01:38:34.000860 140349087110976 submission_runner.py:380] Time since start: 6924.36s, 	Step: 14000, 	{'train/accuracy': 0.537265625, 'train/loss': 2.124048156738281, 'validation/accuracy': 0.491, 'validation/loss': 2.34310609375, 'validation/num_examples': 50000, 'test/accuracy': 0.3855, 'test/loss': 2.9598669921875, 'test/num_examples': 10000}
I0401 01:38:34.001197 140349087110976 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 101.062643712
I0401 01:38:34.010497 140306070611712 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5612.417754, test/accuracy=0.385500, test/loss=2.959867, test/num_examples=10000, total_duration=6924.357624, train/accuracy=0.537266, train/loss=2.124048, validation/accuracy=0.491000, validation/loss=2.343106, validation/num_examples=50000
I0401 01:38:34.430357 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0401 01:38:34.431040 140349087110976 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 101.0617344
I0401 01:38:34.439326 140306079004416 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5612.417754
I0401 01:38:35.505003 140349087110976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0401 01:38:35.866469 140349087110976 submission_runner.py:543] Tuning trial 1/1
I0401 01:38:35.866665 140349087110976 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 01:38:35.867185 140349087110976 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00224609375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00186, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0023, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.33519434928894, 'total_duration': 6.33745265007019, 'global_step': 1, 'preemption_count': 0}), (1046, {'train/accuracy': 0.03189453125, 'train/loss': 5.960023803710937, 'validation/accuracy': 0.02988, 'validation/loss': 5.982645, 'validation/num_examples': 50000, 'test/accuracy': 0.021, 'test/loss': 6.1151890625, 'test/num_examples': 10000, 'score': 424.27064871788025, 'total_duration': 530.8483531475067, 'global_step': 1046, 'preemption_count': 0}), (2090, {'train/accuracy': 0.0784375, 'train/loss': 5.261727294921875, 'validation/accuracy': 0.07458, 'validation/loss': 5.304615, 'validation/num_examples': 50000, 'test/accuracy': 0.0549, 'test/loss': 5.534515625, 'test/num_examples': 10000, 'score': 842.2068192958832, 'total_duration': 1042.6100528240204, 'global_step': 2090, 'preemption_count': 0}), (3132, {'train/accuracy': 0.13818359375, 'train/loss': 4.729986572265625, 'validation/accuracy': 0.12916, 'validation/loss': 4.785410625, 'validation/num_examples': 50000, 'test/accuracy': 0.0959, 'test/loss': 5.08208671875, 'test/num_examples': 10000, 'score': 1260.0118498802185, 'total_duration': 1554.4361956119537, 'global_step': 3132, 'preemption_count': 0}), (4175, {'train/accuracy': 0.19943359375, 'train/loss': 4.221939392089844, 'validation/accuracy': 0.18408, 'validation/loss': 4.312911875, 'validation/num_examples': 50000, 'test/accuracy': 0.1444, 'test/loss': 4.645348828125, 'test/num_examples': 10000, 'score': 1677.6359357833862, 'total_duration': 2064.2138504981995, 'global_step': 4175, 'preemption_count': 0}), (5216, {'train/accuracy': 0.24181640625, 'train/loss': 3.854183349609375, 'validation/accuracy': 0.2243, 'validation/loss': 3.9674240625, 'validation/num_examples': 50000, 'test/accuracy': 0.1663, 'test/loss': 4.4006328125, 'test/num_examples': 10000, 'score': 2095.3125097751617, 'total_duration': 2573.1576149463654, 'global_step': 5216, 'preemption_count': 0}), (6260, {'train/accuracy': 0.28767578125, 'train/loss': 3.542598876953125, 'validation/accuracy': 0.26468, 'validation/loss': 3.673434375, 'validation/num_examples': 50000, 'test/accuracy': 0.2007, 'test/loss': 4.138240625, 'test/num_examples': 10000, 'score': 2513.310442209244, 'total_duration': 3082.927575111389, 'global_step': 6260, 'preemption_count': 0}), (7307, {'train/accuracy': 0.3244921875, 'train/loss': 3.2925213623046874, 'validation/accuracy': 0.30102, 'validation/loss': 3.4244684375, 'validation/num_examples': 50000, 'test/accuracy': 0.2318, 'test/loss': 3.918516796875, 'test/num_examples': 10000, 'score': 2931.1270565986633, 'total_duration': 3596.129921913147, 'global_step': 7307, 'preemption_count': 0}), (8349, {'train/accuracy': 0.35958984375, 'train/loss': 3.0853927612304686, 'validation/accuracy': 0.33092, 'validation/loss': 3.2445625, 'validation/num_examples': 50000, 'test/accuracy': 0.2587, 'test/loss': 3.773323828125, 'test/num_examples': 10000, 'score': 3349.008538722992, 'total_duration': 4105.322269916534, 'global_step': 8349, 'preemption_count': 0}), (9390, {'train/accuracy': 0.39736328125, 'train/loss': 2.8511282348632814, 'validation/accuracy': 0.36344, 'validation/loss': 3.0291284375, 'validation/num_examples': 50000, 'test/accuracy': 0.282, 'test/loss': 3.563748828125, 'test/num_examples': 10000, 'score': 3766.6640989780426, 'total_duration': 4614.733295440674, 'global_step': 9390, 'preemption_count': 0}), (10434, {'train/accuracy': 0.431171875, 'train/loss': 2.68815673828125, 'validation/accuracy': 0.39512, 'validation/loss': 2.8714853125, 'validation/num_examples': 50000, 'test/accuracy': 0.306, 'test/loss': 3.41387265625, 'test/num_examples': 10000, 'score': 4184.5802364349365, 'total_duration': 5123.90599656105, 'global_step': 10434, 'preemption_count': 0}), (11477, {'train/accuracy': 0.46994140625, 'train/loss': 2.4739622497558593, 'validation/accuracy': 0.4326, 'validation/loss': 2.6748953125, 'validation/num_examples': 50000, 'test/accuracy': 0.3315, 'test/loss': 3.2608013671875, 'test/num_examples': 10000, 'score': 4602.385396242142, 'total_duration': 5633.243368148804, 'global_step': 11477, 'preemption_count': 0}), (12521, {'train/accuracy': 0.4896875, 'train/loss': 2.410396728515625, 'validation/accuracy': 0.4517, 'validation/loss': 2.6056484375, 'validation/num_examples': 50000, 'test/accuracy': 0.3579, 'test/loss': 3.1715791015625, 'test/num_examples': 10000, 'score': 5020.4057602882385, 'total_duration': 6146.581405639648, 'global_step': 12521, 'preemption_count': 0}), (13567, {'train/accuracy': 0.5203125, 'train/loss': 2.2009754943847657, 'validation/accuracy': 0.47888, 'validation/loss': 2.42013515625, 'validation/num_examples': 50000, 'test/accuracy': 0.3779, 'test/loss': 3.0206734375, 'test/num_examples': 10000, 'score': 5438.079476594925, 'total_duration': 6659.132613897324, 'global_step': 13567, 'preemption_count': 0}), (14000, {'train/accuracy': 0.537265625, 'train/loss': 2.124048156738281, 'validation/accuracy': 0.491, 'validation/loss': 2.34310609375, 'validation/num_examples': 50000, 'test/accuracy': 0.3855, 'test/loss': 2.9598669921875, 'test/num_examples': 10000, 'score': 5612.41775393486, 'total_duration': 6924.357623577118, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0401 01:38:35.867280 140349087110976 submission_runner.py:546] Timing: 5612.41775393486
I0401 01:38:35.867326 140349087110976 submission_runner.py:547] ====================
I0401 01:38:35.867424 140349087110976 submission_runner.py:606] Final imagenet_vit score: 5612.41775393486
