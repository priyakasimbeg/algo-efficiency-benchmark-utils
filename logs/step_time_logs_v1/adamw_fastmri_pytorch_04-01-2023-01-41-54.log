WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 01:42:16.385426 139983199561536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 01:42:16.385462 140011638187840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 01:42:16.385481 139703855310656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 01:42:16.386196 140419922835264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 01:42:16.386648 140537743664960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 01:42:16.386643 139903747729216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 01:42:16.387220 140439411939136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 01:42:16.396625 139890160002880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 01:42:16.396780 140419922835264 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.396925 139890160002880 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.397512 139903747729216 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.397537 140537743664960 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.397814 140439411939136 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.406380 139983199561536 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.406419 140011638187840 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.406469 139703855310656 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:42:16.934972 139890160002880 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/fastmri_pytorch.
W0401 01:42:16.971464 140011638187840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.971497 139903747729216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.971891 139890160002880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.972883 139703855310656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.972862 140439411939136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.973016 140537743664960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.973026 140419922835264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:42:16.973482 139983199561536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 01:42:16.978502 139890160002880 submission_runner.py:504] Using RNG seed 470864562
I0401 01:42:16.979442 139890160002880 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 01:42:16.979552 139890160002880 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/fastmri_pytorch/trial_1.
I0401 01:42:16.979727 139890160002880 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/hparams.json.
I0401 01:42:16.980580 139890160002880 submission_runner.py:230] Starting train once: RAM USED (GB) 5.620678656
I0401 01:42:16.980677 139890160002880 submission_runner.py:231] Initializing dataset.
I0401 01:42:16.980855 139890160002880 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.621235712
I0401 01:42:16.980919 139890160002880 submission_runner.py:240] Initializing model.
I0401 01:42:21.304412 139890160002880 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.276740608
I0401 01:42:21.304640 139890160002880 submission_runner.py:252] Initializing optimizer.
I0401 01:42:21.305476 139890160002880 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.276765184
I0401 01:42:21.305595 139890160002880 submission_runner.py:261] Initializing metrics bundle.
I0401 01:42:21.305653 139890160002880 submission_runner.py:275] Initializing checkpoint and logger.
I0401 01:42:21.308921 139890160002880 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 01:42:21.309031 139890160002880 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 01:42:21.916064 139890160002880 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0401 01:42:21.917060 139890160002880 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/flags_0.json.
I0401 01:42:21.960272 139890160002880 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 15.327309824
I0401 01:42:21.961405 139890160002880 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.327309824
I0401 01:42:21.961536 139890160002880 submission_runner.py:312] Starting training loop.
I0401 01:43:04.688934 139890160002880 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 24.086913024
I0401 01:43:08.743100 139847641585408 logging_writer.py:48] [0] global_step=0, grad_norm=3.418002, loss=0.763961
I0401 01:43:08.750150 139890160002880 submission.py:119] 0) loss = 0.764, grad_norm = 3.418
I0401 01:43:08.750708 139890160002880 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 32.88008704
I0401 01:43:08.751358 139890160002880 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 32.880082944
I0401 01:43:08.751481 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:44:46.932043 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:45:49.125878 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:46:50.636145 139890160002880 submission_runner.py:380] Time since start: 46.79s, 	Step: 1, 	{'train/ssim': 0.22869530745915004, 'train/loss': 0.7880049433026995, 'validation/ssim': 0.2242310909857291, 'validation/loss': 0.7957238845095315, 'validation/num_examples': 3554, 'test/ssim': 0.24630869396969246, 'test/loss': 0.7966822645778064, 'test/num_examples': 3581}
I0401 01:46:50.636528 139890160002880 submission_runner.py:390] After eval at step 1: RAM USED (GB) 69.158088704
I0401 01:46:50.645739 139823616595712 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=46.788358, test/loss=0.796682, test/num_examples=3581, test/ssim=0.246309, total_duration=46.790373, train/loss=0.788005, train/ssim=0.228695, validation/loss=0.795724, validation/num_examples=3554, validation/ssim=0.224231
I0401 01:46:50.786285 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1.
I0401 01:46:50.786757 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 69.16093952
I0401 01:46:50.798168 139890160002880 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 69.16993024
I0401 01:46:50.802206 139890160002880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802186 140011638187840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802179 139703855310656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802192 139903747729216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802194 140537743664960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802181 139983199561536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802193 140419922835264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.802222 140439411939136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:46:50.873878 139823532734208 logging_writer.py:48] [1] global_step=1, grad_norm=3.995063, loss=0.757176
I0401 01:46:50.879690 139890160002880 submission.py:119] 1) loss = 0.757, grad_norm = 3.995
I0401 01:46:50.881175 139890160002880 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 69.195472896
I0401 01:46:50.958320 139823616595712 logging_writer.py:48] [2] global_step=2, grad_norm=4.283126, loss=0.782962
I0401 01:46:50.963398 139890160002880 submission.py:119] 2) loss = 0.783, grad_norm = 4.283
I0401 01:46:51.035140 139823532734208 logging_writer.py:48] [3] global_step=3, grad_norm=3.769316, loss=0.751144
I0401 01:46:51.039021 139890160002880 submission.py:119] 3) loss = 0.751, grad_norm = 3.769
I0401 01:46:51.108441 139823616595712 logging_writer.py:48] [4] global_step=4, grad_norm=3.514368, loss=0.791215
I0401 01:46:51.114832 139890160002880 submission.py:119] 4) loss = 0.791, grad_norm = 3.514
I0401 01:46:51.191189 139823532734208 logging_writer.py:48] [5] global_step=5, grad_norm=3.439478, loss=0.820987
I0401 01:46:51.197756 139890160002880 submission.py:119] 5) loss = 0.821, grad_norm = 3.439
I0401 01:46:51.269291 139823616595712 logging_writer.py:48] [6] global_step=6, grad_norm=3.620261, loss=0.782660
I0401 01:46:51.275573 139890160002880 submission.py:119] 6) loss = 0.783, grad_norm = 3.620
I0401 01:46:51.350054 139823532734208 logging_writer.py:48] [7] global_step=7, grad_norm=3.162704, loss=0.866420
I0401 01:46:51.353337 139890160002880 submission.py:119] 7) loss = 0.866, grad_norm = 3.163
I0401 01:46:51.426205 139823616595712 logging_writer.py:48] [8] global_step=8, grad_norm=3.524658, loss=0.766517
I0401 01:46:51.431882 139890160002880 submission.py:119] 8) loss = 0.767, grad_norm = 3.525
I0401 01:46:51.503590 139823532734208 logging_writer.py:48] [9] global_step=9, grad_norm=3.645274, loss=0.778576
I0401 01:46:51.509493 139890160002880 submission.py:119] 9) loss = 0.779, grad_norm = 3.645
I0401 01:46:51.577906 139823616595712 logging_writer.py:48] [10] global_step=10, grad_norm=3.339445, loss=0.754319
I0401 01:46:51.581470 139890160002880 submission.py:119] 10) loss = 0.754, grad_norm = 3.339
I0401 01:46:51.646935 139823532734208 logging_writer.py:48] [11] global_step=11, grad_norm=3.657004, loss=0.710536
I0401 01:46:51.651652 139890160002880 submission.py:119] 11) loss = 0.711, grad_norm = 3.657
I0401 01:46:51.724133 139823616595712 logging_writer.py:48] [12] global_step=12, grad_norm=3.572708, loss=0.710633
I0401 01:46:51.728758 139890160002880 submission.py:119] 12) loss = 0.711, grad_norm = 3.573
I0401 01:46:51.808376 139823532734208 logging_writer.py:48] [13] global_step=13, grad_norm=3.301276, loss=0.644852
I0401 01:46:51.814435 139890160002880 submission.py:119] 13) loss = 0.645, grad_norm = 3.301
I0401 01:46:51.881769 139823616595712 logging_writer.py:48] [14] global_step=14, grad_norm=2.853665, loss=0.750461
I0401 01:46:51.885757 139890160002880 submission.py:119] 14) loss = 0.750, grad_norm = 2.854
I0401 01:46:52.130086 139823532734208 logging_writer.py:48] [15] global_step=15, grad_norm=3.008169, loss=0.691651
I0401 01:46:52.135227 139890160002880 submission.py:119] 15) loss = 0.692, grad_norm = 3.008
I0401 01:46:52.394907 139823616595712 logging_writer.py:48] [16] global_step=16, grad_norm=3.002448, loss=0.648939
I0401 01:46:52.398607 139890160002880 submission.py:119] 16) loss = 0.649, grad_norm = 3.002
I0401 01:46:52.676958 139823532734208 logging_writer.py:48] [17] global_step=17, grad_norm=2.917343, loss=0.622439
I0401 01:46:52.682859 139890160002880 submission.py:119] 17) loss = 0.622, grad_norm = 2.917
I0401 01:46:52.908681 139823616595712 logging_writer.py:48] [18] global_step=18, grad_norm=2.426785, loss=0.667385
I0401 01:46:52.914067 139890160002880 submission.py:119] 18) loss = 0.667, grad_norm = 2.427
I0401 01:46:53.167112 139823532734208 logging_writer.py:48] [19] global_step=19, grad_norm=2.729985, loss=0.668529
I0401 01:46:53.172543 139890160002880 submission.py:119] 19) loss = 0.669, grad_norm = 2.730
I0401 01:46:53.389334 139823616595712 logging_writer.py:48] [20] global_step=20, grad_norm=2.702744, loss=0.590543
I0401 01:46:53.395178 139890160002880 submission.py:119] 20) loss = 0.591, grad_norm = 2.703
I0401 01:46:53.664751 139823532734208 logging_writer.py:48] [21] global_step=21, grad_norm=2.513942, loss=0.553863
I0401 01:46:53.670241 139890160002880 submission.py:119] 21) loss = 0.554, grad_norm = 2.514
I0401 01:46:53.946086 139823616595712 logging_writer.py:48] [22] global_step=22, grad_norm=2.176716, loss=0.534553
I0401 01:46:53.951532 139890160002880 submission.py:119] 22) loss = 0.535, grad_norm = 2.177
I0401 01:46:54.202336 139823532734208 logging_writer.py:48] [23] global_step=23, grad_norm=1.882197, loss=0.580772
I0401 01:46:54.208451 139890160002880 submission.py:119] 23) loss = 0.581, grad_norm = 1.882
I0401 01:46:54.476423 139823616595712 logging_writer.py:48] [24] global_step=24, grad_norm=1.659496, loss=0.667298
I0401 01:46:54.482413 139890160002880 submission.py:119] 24) loss = 0.667, grad_norm = 1.659
I0401 01:46:54.671883 139823532734208 logging_writer.py:48] [25] global_step=25, grad_norm=1.594960, loss=0.543500
I0401 01:46:54.675492 139890160002880 submission.py:119] 25) loss = 0.543, grad_norm = 1.595
I0401 01:46:54.955192 139823616595712 logging_writer.py:48] [26] global_step=26, grad_norm=1.819029, loss=0.545601
I0401 01:46:54.961769 139890160002880 submission.py:119] 26) loss = 0.546, grad_norm = 1.819
I0401 01:46:55.170540 139823532734208 logging_writer.py:48] [27] global_step=27, grad_norm=1.475419, loss=0.498469
I0401 01:46:55.173952 139890160002880 submission.py:119] 27) loss = 0.498, grad_norm = 1.475
I0401 01:46:55.437367 139823616595712 logging_writer.py:48] [28] global_step=28, grad_norm=1.445572, loss=0.452192
I0401 01:46:55.440831 139890160002880 submission.py:119] 28) loss = 0.452, grad_norm = 1.446
I0401 01:46:55.708145 139823532734208 logging_writer.py:48] [29] global_step=29, grad_norm=1.073991, loss=0.619190
I0401 01:46:55.711615 139890160002880 submission.py:119] 29) loss = 0.619, grad_norm = 1.074
I0401 01:46:55.932453 139823616595712 logging_writer.py:48] [30] global_step=30, grad_norm=1.055863, loss=0.515312
I0401 01:46:55.935886 139890160002880 submission.py:119] 30) loss = 0.515, grad_norm = 1.056
I0401 01:46:56.241583 139823532734208 logging_writer.py:48] [31] global_step=31, grad_norm=0.989884, loss=0.499955
I0401 01:46:56.245035 139890160002880 submission.py:119] 31) loss = 0.500, grad_norm = 0.990
I0401 01:46:56.519338 139823616595712 logging_writer.py:48] [32] global_step=32, grad_norm=1.051842, loss=0.435689
I0401 01:46:56.522845 139890160002880 submission.py:119] 32) loss = 0.436, grad_norm = 1.052
I0401 01:46:56.802206 139823532734208 logging_writer.py:48] [33] global_step=33, grad_norm=0.901317, loss=0.509452
I0401 01:46:56.806064 139890160002880 submission.py:119] 33) loss = 0.509, grad_norm = 0.901
I0401 01:46:57.014791 139823616595712 logging_writer.py:48] [34] global_step=34, grad_norm=0.787957, loss=0.604572
I0401 01:46:57.022219 139890160002880 submission.py:119] 34) loss = 0.605, grad_norm = 0.788
I0401 01:46:57.287676 139823532734208 logging_writer.py:48] [35] global_step=35, grad_norm=0.859596, loss=0.503243
I0401 01:46:57.293216 139890160002880 submission.py:119] 35) loss = 0.503, grad_norm = 0.860
I0401 01:46:57.521055 139823616595712 logging_writer.py:48] [36] global_step=36, grad_norm=0.777413, loss=0.505088
I0401 01:46:57.527233 139890160002880 submission.py:119] 36) loss = 0.505, grad_norm = 0.777
I0401 01:46:57.769318 139823532734208 logging_writer.py:48] [37] global_step=37, grad_norm=0.825515, loss=0.408864
I0401 01:46:57.773980 139890160002880 submission.py:119] 37) loss = 0.409, grad_norm = 0.826
I0401 01:46:58.046924 139823616595712 logging_writer.py:48] [38] global_step=38, grad_norm=0.695743, loss=0.420780
I0401 01:46:58.053039 139890160002880 submission.py:119] 38) loss = 0.421, grad_norm = 0.696
I0401 01:46:58.317810 139823532734208 logging_writer.py:48] [39] global_step=39, grad_norm=0.672415, loss=0.486152
I0401 01:46:58.323538 139890160002880 submission.py:119] 39) loss = 0.486, grad_norm = 0.672
I0401 01:46:58.573975 139823616595712 logging_writer.py:48] [40] global_step=40, grad_norm=0.700283, loss=0.453802
I0401 01:46:58.580276 139890160002880 submission.py:119] 40) loss = 0.454, grad_norm = 0.700
I0401 01:46:58.856782 139823532734208 logging_writer.py:48] [41] global_step=41, grad_norm=0.600250, loss=0.492562
I0401 01:46:58.863568 139890160002880 submission.py:119] 41) loss = 0.493, grad_norm = 0.600
I0401 01:46:59.074318 139823616595712 logging_writer.py:48] [42] global_step=42, grad_norm=0.600581, loss=0.433337
I0401 01:46:59.079365 139890160002880 submission.py:119] 42) loss = 0.433, grad_norm = 0.601
I0401 01:46:59.349025 139823532734208 logging_writer.py:48] [43] global_step=43, grad_norm=0.589129, loss=0.461417
I0401 01:46:59.354795 139890160002880 submission.py:119] 43) loss = 0.461, grad_norm = 0.589
I0401 01:46:59.636448 139823616595712 logging_writer.py:48] [44] global_step=44, grad_norm=0.543357, loss=0.459665
I0401 01:46:59.639944 139890160002880 submission.py:119] 44) loss = 0.460, grad_norm = 0.543
I0401 01:46:59.871356 139823532734208 logging_writer.py:48] [45] global_step=45, grad_norm=0.534987, loss=0.440495
I0401 01:46:59.875364 139890160002880 submission.py:119] 45) loss = 0.440, grad_norm = 0.535
I0401 01:47:00.131614 139823616595712 logging_writer.py:48] [46] global_step=46, grad_norm=0.517995, loss=0.461316
I0401 01:47:00.135277 139890160002880 submission.py:119] 46) loss = 0.461, grad_norm = 0.518
I0401 01:47:00.388742 139823532734208 logging_writer.py:48] [47] global_step=47, grad_norm=0.564569, loss=0.417131
I0401 01:47:00.392295 139890160002880 submission.py:119] 47) loss = 0.417, grad_norm = 0.565
I0401 01:47:00.658096 139823616595712 logging_writer.py:48] [48] global_step=48, grad_norm=0.540275, loss=0.401638
I0401 01:47:00.661853 139890160002880 submission.py:119] 48) loss = 0.402, grad_norm = 0.540
I0401 01:47:00.907991 139823532734208 logging_writer.py:48] [49] global_step=49, grad_norm=0.451660, loss=0.489919
I0401 01:47:00.912932 139890160002880 submission.py:119] 49) loss = 0.490, grad_norm = 0.452
I0401 01:47:01.153699 139823616595712 logging_writer.py:48] [50] global_step=50, grad_norm=0.473860, loss=0.523680
I0401 01:47:01.159236 139890160002880 submission.py:119] 50) loss = 0.524, grad_norm = 0.474
I0401 01:47:01.516376 139823532734208 logging_writer.py:48] [51] global_step=51, grad_norm=0.525724, loss=0.452774
I0401 01:47:01.519449 139890160002880 submission.py:119] 51) loss = 0.453, grad_norm = 0.526
I0401 01:47:01.727028 139823616595712 logging_writer.py:48] [52] global_step=52, grad_norm=0.445270, loss=0.433775
I0401 01:47:01.732319 139890160002880 submission.py:119] 52) loss = 0.434, grad_norm = 0.445
I0401 01:47:02.020274 139823532734208 logging_writer.py:48] [53] global_step=53, grad_norm=0.425382, loss=0.409663
I0401 01:47:02.025658 139890160002880 submission.py:119] 53) loss = 0.410, grad_norm = 0.425
I0401 01:47:02.240813 139823616595712 logging_writer.py:48] [54] global_step=54, grad_norm=0.636488, loss=0.344868
I0401 01:47:02.244762 139890160002880 submission.py:119] 54) loss = 0.345, grad_norm = 0.636
I0401 01:47:02.536626 139823532734208 logging_writer.py:48] [55] global_step=55, grad_norm=0.386557, loss=0.394791
I0401 01:47:02.541666 139890160002880 submission.py:119] 55) loss = 0.395, grad_norm = 0.387
I0401 01:47:02.765803 139823616595712 logging_writer.py:48] [56] global_step=56, grad_norm=0.359797, loss=0.419649
I0401 01:47:02.771264 139890160002880 submission.py:119] 56) loss = 0.420, grad_norm = 0.360
I0401 01:47:03.012868 139823532734208 logging_writer.py:48] [57] global_step=57, grad_norm=0.341735, loss=0.457068
I0401 01:47:03.017750 139890160002880 submission.py:119] 57) loss = 0.457, grad_norm = 0.342
I0401 01:47:03.263375 139823616595712 logging_writer.py:48] [58] global_step=58, grad_norm=0.458689, loss=0.366470
I0401 01:47:03.269129 139890160002880 submission.py:119] 58) loss = 0.366, grad_norm = 0.459
I0401 01:47:03.530718 139823532734208 logging_writer.py:48] [59] global_step=59, grad_norm=0.398663, loss=0.422520
I0401 01:47:03.537029 139890160002880 submission.py:119] 59) loss = 0.423, grad_norm = 0.399
I0401 01:47:03.824573 139823616595712 logging_writer.py:48] [60] global_step=60, grad_norm=0.331057, loss=0.430660
I0401 01:47:03.830268 139890160002880 submission.py:119] 60) loss = 0.431, grad_norm = 0.331
I0401 01:47:04.062058 139823532734208 logging_writer.py:48] [61] global_step=61, grad_norm=0.542188, loss=0.317044
I0401 01:47:04.069587 139890160002880 submission.py:119] 61) loss = 0.317, grad_norm = 0.542
I0401 01:47:04.297449 139823616595712 logging_writer.py:48] [62] global_step=62, grad_norm=0.320937, loss=0.379461
I0401 01:47:04.300876 139890160002880 submission.py:119] 62) loss = 0.379, grad_norm = 0.321
I0401 01:47:04.587402 139823532734208 logging_writer.py:48] [63] global_step=63, grad_norm=0.287445, loss=0.473307
I0401 01:47:04.591089 139890160002880 submission.py:119] 63) loss = 0.473, grad_norm = 0.287
I0401 01:47:04.853071 139823616595712 logging_writer.py:48] [64] global_step=64, grad_norm=0.310107, loss=0.365541
I0401 01:47:04.856813 139890160002880 submission.py:119] 64) loss = 0.366, grad_norm = 0.310
I0401 01:47:05.136361 139823532734208 logging_writer.py:48] [65] global_step=65, grad_norm=0.374016, loss=0.355353
I0401 01:47:05.141529 139890160002880 submission.py:119] 65) loss = 0.355, grad_norm = 0.374
I0401 01:47:05.454336 139823616595712 logging_writer.py:48] [66] global_step=66, grad_norm=0.341089, loss=0.393557
I0401 01:47:05.458111 139890160002880 submission.py:119] 66) loss = 0.394, grad_norm = 0.341
I0401 01:47:05.744335 139823532734208 logging_writer.py:48] [67] global_step=67, grad_norm=0.380022, loss=0.366902
I0401 01:47:05.747742 139890160002880 submission.py:119] 67) loss = 0.367, grad_norm = 0.380
I0401 01:47:05.972285 139823616595712 logging_writer.py:48] [68] global_step=68, grad_norm=0.411456, loss=0.382459
I0401 01:47:05.976962 139890160002880 submission.py:119] 68) loss = 0.382, grad_norm = 0.411
I0401 01:47:06.227029 139823532734208 logging_writer.py:48] [69] global_step=69, grad_norm=0.259376, loss=0.311959
I0401 01:47:06.231023 139890160002880 submission.py:119] 69) loss = 0.312, grad_norm = 0.259
I0401 01:47:06.519144 139823616595712 logging_writer.py:48] [70] global_step=70, grad_norm=0.316689, loss=0.357459
I0401 01:47:06.524202 139890160002880 submission.py:119] 70) loss = 0.357, grad_norm = 0.317
I0401 01:47:06.732754 139823532734208 logging_writer.py:48] [71] global_step=71, grad_norm=0.289272, loss=0.337308
I0401 01:47:06.736329 139890160002880 submission.py:119] 71) loss = 0.337, grad_norm = 0.289
I0401 01:47:07.031942 139823616595712 logging_writer.py:48] [72] global_step=72, grad_norm=0.240059, loss=0.449597
I0401 01:47:07.035679 139890160002880 submission.py:119] 72) loss = 0.450, grad_norm = 0.240
I0401 01:47:07.254889 139823532734208 logging_writer.py:48] [73] global_step=73, grad_norm=0.233726, loss=0.404677
I0401 01:47:07.260334 139890160002880 submission.py:119] 73) loss = 0.405, grad_norm = 0.234
I0401 01:47:07.531041 139823616595712 logging_writer.py:48] [74] global_step=74, grad_norm=0.239836, loss=0.412539
I0401 01:47:07.535927 139890160002880 submission.py:119] 74) loss = 0.413, grad_norm = 0.240
I0401 01:47:07.784185 139823532734208 logging_writer.py:48] [75] global_step=75, grad_norm=0.234423, loss=0.331118
I0401 01:47:07.789846 139890160002880 submission.py:119] 75) loss = 0.331, grad_norm = 0.234
I0401 01:47:08.023085 139823616595712 logging_writer.py:48] [76] global_step=76, grad_norm=0.374730, loss=0.381872
I0401 01:47:08.028209 139890160002880 submission.py:119] 76) loss = 0.382, grad_norm = 0.375
I0401 01:47:08.288419 139823532734208 logging_writer.py:48] [77] global_step=77, grad_norm=0.317495, loss=0.243794
I0401 01:47:08.294289 139890160002880 submission.py:119] 77) loss = 0.244, grad_norm = 0.317
I0401 01:47:08.539801 139823616595712 logging_writer.py:48] [78] global_step=78, grad_norm=0.215324, loss=0.288863
I0401 01:47:08.545270 139890160002880 submission.py:119] 78) loss = 0.289, grad_norm = 0.215
I0401 01:47:08.776034 139823532734208 logging_writer.py:48] [79] global_step=79, grad_norm=0.218908, loss=0.345863
I0401 01:47:08.779145 139890160002880 submission.py:119] 79) loss = 0.346, grad_norm = 0.219
I0401 01:47:09.038648 139823616595712 logging_writer.py:48] [80] global_step=80, grad_norm=0.301945, loss=0.307554
I0401 01:47:09.041819 139890160002880 submission.py:119] 80) loss = 0.308, grad_norm = 0.302
I0401 01:47:09.285007 139823532734208 logging_writer.py:48] [81] global_step=81, grad_norm=0.203957, loss=0.292917
I0401 01:47:09.288386 139890160002880 submission.py:119] 81) loss = 0.293, grad_norm = 0.204
I0401 01:47:09.556757 139823616595712 logging_writer.py:48] [82] global_step=82, grad_norm=0.281487, loss=0.365189
I0401 01:47:09.559949 139890160002880 submission.py:119] 82) loss = 0.365, grad_norm = 0.281
I0401 01:47:09.836545 139823532734208 logging_writer.py:48] [83] global_step=83, grad_norm=0.216345, loss=0.380424
I0401 01:47:09.839760 139890160002880 submission.py:119] 83) loss = 0.380, grad_norm = 0.216
I0401 01:47:10.113293 139823616595712 logging_writer.py:48] [84] global_step=84, grad_norm=0.232739, loss=0.319724
I0401 01:47:10.119000 139890160002880 submission.py:119] 84) loss = 0.320, grad_norm = 0.233
I0401 01:47:10.339395 139823532734208 logging_writer.py:48] [85] global_step=85, grad_norm=0.182368, loss=0.301615
I0401 01:47:10.344442 139890160002880 submission.py:119] 85) loss = 0.302, grad_norm = 0.182
I0401 01:47:10.623585 139823616595712 logging_writer.py:48] [86] global_step=86, grad_norm=0.198407, loss=0.313296
I0401 01:47:10.626862 139890160002880 submission.py:119] 86) loss = 0.313, grad_norm = 0.198
I0401 01:47:10.854415 139823532734208 logging_writer.py:48] [87] global_step=87, grad_norm=0.184563, loss=0.290011
I0401 01:47:10.857654 139890160002880 submission.py:119] 87) loss = 0.290, grad_norm = 0.185
I0401 01:47:11.151541 139823616595712 logging_writer.py:48] [88] global_step=88, grad_norm=0.223915, loss=0.322861
I0401 01:47:11.154832 139890160002880 submission.py:119] 88) loss = 0.323, grad_norm = 0.224
I0401 01:47:11.404717 139823532734208 logging_writer.py:48] [89] global_step=89, grad_norm=0.464561, loss=0.264910
I0401 01:47:11.408071 139890160002880 submission.py:119] 89) loss = 0.265, grad_norm = 0.465
I0401 01:47:11.708698 139823616595712 logging_writer.py:48] [90] global_step=90, grad_norm=0.219870, loss=0.285459
I0401 01:47:11.714359 139890160002880 submission.py:119] 90) loss = 0.285, grad_norm = 0.220
I0401 01:47:12.011979 139823532734208 logging_writer.py:48] [91] global_step=91, grad_norm=0.270711, loss=0.392479
I0401 01:47:12.018757 139890160002880 submission.py:119] 91) loss = 0.392, grad_norm = 0.271
I0401 01:47:12.280802 139823616595712 logging_writer.py:48] [92] global_step=92, grad_norm=0.315922, loss=0.303621
I0401 01:47:12.286463 139890160002880 submission.py:119] 92) loss = 0.304, grad_norm = 0.316
I0401 01:47:12.508671 139823532734208 logging_writer.py:48] [93] global_step=93, grad_norm=0.185326, loss=0.310026
I0401 01:47:12.511959 139890160002880 submission.py:119] 93) loss = 0.310, grad_norm = 0.185
I0401 01:47:12.791000 139823616595712 logging_writer.py:48] [94] global_step=94, grad_norm=0.159860, loss=0.370909
I0401 01:47:12.794386 139890160002880 submission.py:119] 94) loss = 0.371, grad_norm = 0.160
I0401 01:47:13.072812 139823532734208 logging_writer.py:48] [95] global_step=95, grad_norm=0.193354, loss=0.265072
I0401 01:47:13.076788 139890160002880 submission.py:119] 95) loss = 0.265, grad_norm = 0.193
I0401 01:47:13.300948 139823616595712 logging_writer.py:48] [96] global_step=96, grad_norm=0.284145, loss=0.395063
I0401 01:47:13.304270 139890160002880 submission.py:119] 96) loss = 0.395, grad_norm = 0.284
I0401 01:47:13.586920 139823532734208 logging_writer.py:48] [97] global_step=97, grad_norm=0.294191, loss=0.279461
I0401 01:47:13.593309 139890160002880 submission.py:119] 97) loss = 0.279, grad_norm = 0.294
I0401 01:47:13.872827 139823616595712 logging_writer.py:48] [98] global_step=98, grad_norm=0.280685, loss=0.293574
I0401 01:47:13.876502 139890160002880 submission.py:119] 98) loss = 0.294, grad_norm = 0.281
I0401 01:47:14.102998 139823532734208 logging_writer.py:48] [99] global_step=99, grad_norm=0.216281, loss=0.358036
I0401 01:47:14.107251 139890160002880 submission.py:119] 99) loss = 0.358, grad_norm = 0.216
I0401 01:47:14.369531 139823616595712 logging_writer.py:48] [100] global_step=100, grad_norm=0.167474, loss=0.265233
I0401 01:47:14.375090 139890160002880 submission.py:119] 100) loss = 0.265, grad_norm = 0.167
I0401 01:48:10.894790 139890160002880 submission_runner.py:371] Before eval at step 315: RAM USED (GB) 87.14774528
I0401 01:48:10.894995 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:48:12.926012 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:48:15.710983 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:48:17.969238 139890160002880 submission_runner.py:380] Time since start: 348.91s, 	Step: 315, 	{'train/ssim': 0.7057980128696987, 'train/loss': 0.3025545392717634, 'validation/ssim': 0.6836743974658835, 'validation/loss': 0.32410018311233824, 'validation/num_examples': 3554, 'test/ssim': 0.7015653906904495, 'test/loss': 0.32601162575703363, 'test/num_examples': 3581}
I0401 01:48:17.969775 139890160002880 submission_runner.py:390] After eval at step 315: RAM USED (GB) 88.174174208
I0401 01:48:17.979333 139823532734208 logging_writer.py:48] [315] global_step=315, preemption_count=0, score=123.137992, test/loss=0.326012, test/num_examples=3581, test/ssim=0.701565, total_duration=348.914618, train/loss=0.302555, train/ssim=0.705798, validation/loss=0.324100, validation/num_examples=3554, validation/ssim=0.683674
I0401 01:48:18.128051 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_315.
I0401 01:48:18.128638 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 315: RAM USED (GB) 88.181362688
I0401 01:49:25.203234 139823616595712 logging_writer.py:48] [500] global_step=500, grad_norm=0.516226, loss=0.318671
I0401 01:49:25.209818 139890160002880 submission.py:119] 500) loss = 0.319, grad_norm = 0.516
I0401 01:49:38.226413 139890160002880 submission_runner.py:371] Before eval at step 535: RAM USED (GB) 105.2077056
I0401 01:49:38.226628 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:49:40.253140 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:49:42.943795 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:49:45.057072 139890160002880 submission_runner.py:380] Time since start: 436.25s, 	Step: 535, 	{'train/ssim': 0.7146897315979004, 'train/loss': 0.29390651839120047, 'validation/ssim': 0.693775046052863, 'validation/loss': 0.31483462048703925, 'validation/num_examples': 3554, 'test/ssim': 0.7110748759730173, 'test/loss': 0.31694072093819814, 'test/num_examples': 3581}
I0401 01:49:45.057461 139890160002880 submission_runner.py:390] After eval at step 535: RAM USED (GB) 106.32026112
I0401 01:49:45.067192 139823532734208 logging_writer.py:48] [535] global_step=535, preemption_count=0, score=198.947790, test/loss=0.316941, test/num_examples=3581, test/ssim=0.711075, total_duration=436.252311, train/loss=0.293907, train/ssim=0.714690, validation/loss=0.314835, validation/num_examples=3554, validation/ssim=0.693775
I0401 01:49:45.227200 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_535.
I0401 01:49:45.227949 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 535: RAM USED (GB) 106.335326208
I0401 01:51:05.545348 139890160002880 submission_runner.py:371] Before eval at step 768: RAM USED (GB) 124.167606272
I0401 01:51:05.545607 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:51:07.584262 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:51:10.643824 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:51:12.767461 139890160002880 submission_runner.py:380] Time since start: 523.57s, 	Step: 768, 	{'train/ssim': 0.7096787861415318, 'train/loss': 0.29718572752816336, 'validation/ssim': 0.6905906389587436, 'validation/loss': 0.3170765719523776, 'validation/num_examples': 3554, 'test/ssim': 0.7071177661835032, 'test/loss': 0.319563068047944, 'test/num_examples': 3581}
I0401 01:51:12.767896 139890160002880 submission_runner.py:390] After eval at step 768: RAM USED (GB) 125.322174464
I0401 01:51:12.780626 139823616595712 logging_writer.py:48] [768] global_step=768, preemption_count=0, score=274.796140, test/loss=0.319563, test/num_examples=3581, test/ssim=0.707118, total_duration=523.568695, train/loss=0.297186, train/ssim=0.709679, validation/loss=0.317077, validation/num_examples=3554, validation/ssim=0.690591
I0401 01:51:12.934144 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_768.
I0401 01:51:12.935362 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 768: RAM USED (GB) 125.336928256
I0401 01:52:30.267477 139823532734208 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.242731, loss=0.239601
I0401 01:52:30.274761 139890160002880 submission.py:119] 1000) loss = 0.240, grad_norm = 0.243
I0401 01:52:33.193082 139890160002880 submission_runner.py:371] Before eval at step 1012: RAM USED (GB) 143.161556992
I0401 01:52:33.193349 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:52:35.091964 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:52:38.054989 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:52:40.220002 139890160002880 submission_runner.py:380] Time since start: 611.21s, 	Step: 1012, 	{'train/ssim': 0.7272084099905831, 'train/loss': 0.281517710004534, 'validation/ssim': 0.7045299420547271, 'validation/loss': 0.3028182167650007, 'validation/num_examples': 3554, 'test/ssim': 0.7216485989423346, 'test/loss': 0.3048014913781067, 'test/num_examples': 3581}
I0401 01:52:40.220364 139890160002880 submission_runner.py:390] After eval at step 1012: RAM USED (GB) 143.245672448
I0401 01:52:40.228367 139823616595712 logging_writer.py:48] [1012] global_step=1012, preemption_count=0, score=350.591635, test/loss=0.304801, test/num_examples=3581, test/ssim=0.721649, total_duration=611.208444, train/loss=0.281518, train/ssim=0.727208, validation/loss=0.302818, validation/num_examples=3554, validation/ssim=0.704530
I0401 01:52:40.371934 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1012.
I0401 01:52:40.372537 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 1012: RAM USED (GB) 143.254503424
I0401 01:54:00.610142 139890160002880 submission_runner.py:371] Before eval at step 1329: RAM USED (GB) 143.653076992
I0401 01:54:00.610486 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:54:02.519930 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:54:04.527862 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:54:06.533378 139890160002880 submission_runner.py:380] Time since start: 698.63s, 	Step: 1329, 	{'train/ssim': 0.727893761226109, 'train/loss': 0.2826306479317801, 'validation/ssim': 0.7053033059141108, 'validation/loss': 0.3036398385841657, 'validation/num_examples': 3554, 'test/ssim': 0.7223500004363307, 'test/loss': 0.305477122094038, 'test/num_examples': 3581}
I0401 01:54:06.533757 139890160002880 submission_runner.py:390] After eval at step 1329: RAM USED (GB) 143.66050304
I0401 01:54:06.541780 139823532734208 logging_writer.py:48] [1329] global_step=1329, preemption_count=0, score=423.489417, test/loss=0.305477, test/num_examples=3581, test/ssim=0.722350, total_duration=698.626995, train/loss=0.282631, train/ssim=0.727894, validation/loss=0.303640, validation/num_examples=3554, validation/ssim=0.705303
I0401 01:54:06.681413 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1329.
I0401 01:54:06.681946 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 1329: RAM USED (GB) 143.659384832
I0401 01:54:48.676449 139823616595712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.215301, loss=0.323254
I0401 01:54:48.680032 139890160002880 submission.py:119] 1500) loss = 0.323, grad_norm = 0.215
I0401 01:55:26.754951 139890160002880 submission_runner.py:371] Before eval at step 1646: RAM USED (GB) 143.719337984
I0401 01:55:26.755205 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:55:28.674623 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:55:30.702002 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:55:32.716267 139890160002880 submission_runner.py:380] Time since start: 784.77s, 	Step: 1646, 	{'train/ssim': 0.7304712704249791, 'train/loss': 0.2777766500200544, 'validation/ssim': 0.7071676774848762, 'validation/loss': 0.29958895209403136, 'validation/num_examples': 3554, 'test/ssim': 0.7245627421635018, 'test/loss': 0.3012138309175161, 'test/num_examples': 3581}
I0401 01:55:32.716629 139890160002880 submission_runner.py:390] After eval at step 1646: RAM USED (GB) 143.720947712
I0401 01:55:32.724905 139823532734208 logging_writer.py:48] [1646] global_step=1646, preemption_count=0, score=496.215945, test/loss=0.301214, test/num_examples=3581, test/ssim=0.724563, total_duration=784.770954, train/loss=0.277777, train/ssim=0.730471, validation/loss=0.299589, validation/num_examples=3554, validation/ssim=0.707168
I0401 01:55:32.863985 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1646.
I0401 01:55:32.864581 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 1646: RAM USED (GB) 143.721385984
I0401 01:56:53.002852 139890160002880 submission_runner.py:371] Before eval at step 1963: RAM USED (GB) 143.779274752
I0401 01:56:53.003066 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:56:54.917167 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:56:57.000146 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:56:59.061288 139890160002880 submission_runner.py:380] Time since start: 871.02s, 	Step: 1963, 	{'train/ssim': 0.7358740397862026, 'train/loss': 0.27352614062173025, 'validation/ssim': 0.7132105357739519, 'validation/loss': 0.29478479491637943, 'validation/num_examples': 3554, 'test/ssim': 0.7303666213610025, 'test/loss': 0.29645257740505443, 'test/num_examples': 3581}
I0401 01:56:59.061741 139890160002880 submission_runner.py:390] After eval at step 1963: RAM USED (GB) 143.793328128
I0401 01:56:59.070374 139823616595712 logging_writer.py:48] [1963] global_step=1963, preemption_count=0, score=568.927930, test/loss=0.296453, test/num_examples=3581, test/ssim=0.730367, total_duration=871.018354, train/loss=0.273526, train/ssim=0.735874, validation/loss=0.294785, validation/num_examples=3554, validation/ssim=0.713211
I0401 01:56:59.223489 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1963.
I0401 01:56:59.224112 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 1963: RAM USED (GB) 143.793758208
I0401 01:57:06.739542 139823532734208 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.238478, loss=0.296783
I0401 01:57:06.743759 139890160002880 submission.py:119] 2000) loss = 0.297, grad_norm = 0.238
I0401 01:58:19.339327 139890160002880 submission_runner.py:371] Before eval at step 2279: RAM USED (GB) 143.894523904
I0401 01:58:19.339589 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:58:21.258212 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:58:23.386924 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:58:25.502285 139890160002880 submission_runner.py:380] Time since start: 957.36s, 	Step: 2279, 	{'train/ssim': 0.7068871770586286, 'train/loss': 0.3027754511151995, 'validation/ssim': 0.6865736532560144, 'validation/loss': 0.3231149650921497, 'validation/num_examples': 3554, 'test/ssim': 0.7038369006998744, 'test/loss': 0.32486520110478917, 'test/num_examples': 3581}
I0401 01:58:25.502683 139890160002880 submission_runner.py:390] After eval at step 2279: RAM USED (GB) 143.904002048
I0401 01:58:25.511689 139823616595712 logging_writer.py:48] [2279] global_step=2279, preemption_count=0, score=641.686091, test/loss=0.324865, test/num_examples=3581, test/ssim=0.703837, total_duration=957.356254, train/loss=0.302775, train/ssim=0.706887, validation/loss=0.323115, validation/num_examples=3554, validation/ssim=0.686574
I0401 01:58:25.659000 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2279.
I0401 01:58:25.659587 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 2279: RAM USED (GB) 143.90237184
I0401 01:59:21.216984 139823532734208 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.150578, loss=0.312661
I0401 01:59:21.221511 139890160002880 submission.py:119] 2500) loss = 0.313, grad_norm = 0.151
I0401 01:59:45.879485 139890160002880 submission_runner.py:371] Before eval at step 2596: RAM USED (GB) 143.915962368
I0401 01:59:45.879738 139890160002880 spec.py:298] Evaluating on the training split.
I0401 01:59:47.852083 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 01:59:49.908401 139890160002880 spec.py:326] Evaluating on the test split.
I0401 01:59:51.979875 139890160002880 submission_runner.py:380] Time since start: 1043.90s, 	Step: 2596, 	{'train/ssim': 0.7362116404942104, 'train/loss': 0.27342922346932547, 'validation/ssim': 0.7132649419008511, 'validation/loss': 0.2948249125655072, 'validation/num_examples': 3554, 'test/ssim': 0.7304297529495951, 'test/loss': 0.2965494564411128, 'test/num_examples': 3581}
I0401 01:59:51.980302 139890160002880 submission_runner.py:390] After eval at step 2596: RAM USED (GB) 143.9147008
I0401 01:59:51.989341 139823616595712 logging_writer.py:48] [2596] global_step=2596, preemption_count=0, score=714.548306, test/loss=0.296549, test/num_examples=3581, test/ssim=0.730430, total_duration=1043.895577, train/loss=0.273429, train/ssim=0.736212, validation/loss=0.294825, validation/num_examples=3554, validation/ssim=0.713265
I0401 01:59:52.141000 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2596.
I0401 01:59:52.141682 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 2596: RAM USED (GB) 143.91310336
I0401 02:00:20.294758 139890160002880 submission_runner.py:371] Before eval at step 2714: RAM USED (GB) 143.907602432
I0401 02:00:20.294981 139890160002880 spec.py:298] Evaluating on the training split.
I0401 02:00:22.242214 139890160002880 spec.py:310] Evaluating on the validation split.
I0401 02:00:24.271888 139890160002880 spec.py:326] Evaluating on the test split.
I0401 02:00:26.311847 139890160002880 submission_runner.py:380] Time since start: 1078.31s, 	Step: 2714, 	{'train/ssim': 0.7369739668709892, 'train/loss': 0.2725751059395926, 'validation/ssim': 0.7138281002699424, 'validation/loss': 0.2940728440335713, 'validation/num_examples': 3554, 'test/ssim': 0.7309262153989807, 'test/loss': 0.2957664474832449, 'test/num_examples': 3581}
I0401 02:00:26.312295 139890160002880 submission_runner.py:390] After eval at step 2714: RAM USED (GB) 143.908139008
I0401 02:00:26.320815 139823532734208 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=740.024261, test/loss=0.295766, test/num_examples=3581, test/ssim=0.730926, total_duration=1078.309920, train/loss=0.272575, train/ssim=0.736974, validation/loss=0.294073, validation/num_examples=3554, validation/ssim=0.713828
I0401 02:00:26.469567 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0401 02:00:26.470222 139890160002880 submission_runner.py:409] After logging and checkpointing eval at step 2714: RAM USED (GB) 143.906516992
I0401 02:00:26.478392 139823616595712 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=740.024261
I0401 02:00:26.738875 139890160002880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0401 02:00:28.476345 139890160002880 submission_runner.py:543] Tuning trial 1/1
I0401 02:00:28.476645 139890160002880 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 02:00:28.481844 139890160002880 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ssim': 0.22869530745915004, 'train/loss': 0.7880049433026995, 'validation/ssim': 0.2242310909857291, 'validation/loss': 0.7957238845095315, 'validation/num_examples': 3554, 'test/ssim': 0.24630869396969246, 'test/loss': 0.7966822645778064, 'test/num_examples': 3581, 'score': 46.788357973098755, 'total_duration': 46.79037261009216, 'global_step': 1, 'preemption_count': 0}), (315, {'train/ssim': 0.7057980128696987, 'train/loss': 0.3025545392717634, 'validation/ssim': 0.6836743974658835, 'validation/loss': 0.32410018311233824, 'validation/num_examples': 3554, 'test/ssim': 0.7015653906904495, 'test/loss': 0.32601162575703363, 'test/num_examples': 3581, 'score': 123.1379919052124, 'total_duration': 348.9146177768707, 'global_step': 315, 'preemption_count': 0}), (535, {'train/ssim': 0.7146897315979004, 'train/loss': 0.29390651839120047, 'validation/ssim': 0.693775046052863, 'validation/loss': 0.31483462048703925, 'validation/num_examples': 3554, 'test/ssim': 0.7110748759730173, 'test/loss': 0.31694072093819814, 'test/num_examples': 3581, 'score': 198.94778966903687, 'total_duration': 436.25231099128723, 'global_step': 535, 'preemption_count': 0}), (768, {'train/ssim': 0.7096787861415318, 'train/loss': 0.29718572752816336, 'validation/ssim': 0.6905906389587436, 'validation/loss': 0.3170765719523776, 'validation/num_examples': 3554, 'test/ssim': 0.7071177661835032, 'test/loss': 0.319563068047944, 'test/num_examples': 3581, 'score': 274.7961401939392, 'total_duration': 523.568695306778, 'global_step': 768, 'preemption_count': 0}), (1012, {'train/ssim': 0.7272084099905831, 'train/loss': 0.281517710004534, 'validation/ssim': 0.7045299420547271, 'validation/loss': 0.3028182167650007, 'validation/num_examples': 3554, 'test/ssim': 0.7216485989423346, 'test/loss': 0.3048014913781067, 'test/num_examples': 3581, 'score': 350.59163546562195, 'total_duration': 611.2084436416626, 'global_step': 1012, 'preemption_count': 0}), (1329, {'train/ssim': 0.727893761226109, 'train/loss': 0.2826306479317801, 'validation/ssim': 0.7053033059141108, 'validation/loss': 0.3036398385841657, 'validation/num_examples': 3554, 'test/ssim': 0.7223500004363307, 'test/loss': 0.305477122094038, 'test/num_examples': 3581, 'score': 423.48941683769226, 'total_duration': 698.6269946098328, 'global_step': 1329, 'preemption_count': 0}), (1646, {'train/ssim': 0.7304712704249791, 'train/loss': 0.2777766500200544, 'validation/ssim': 0.7071676774848762, 'validation/loss': 0.29958895209403136, 'validation/num_examples': 3554, 'test/ssim': 0.7245627421635018, 'test/loss': 0.3012138309175161, 'test/num_examples': 3581, 'score': 496.2159445285797, 'total_duration': 784.7709541320801, 'global_step': 1646, 'preemption_count': 0}), (1963, {'train/ssim': 0.7358740397862026, 'train/loss': 0.27352614062173025, 'validation/ssim': 0.7132105357739519, 'validation/loss': 0.29478479491637943, 'validation/num_examples': 3554, 'test/ssim': 0.7303666213610025, 'test/loss': 0.29645257740505443, 'test/num_examples': 3581, 'score': 568.9279301166534, 'total_duration': 871.018354177475, 'global_step': 1963, 'preemption_count': 0}), (2279, {'train/ssim': 0.7068871770586286, 'train/loss': 0.3027754511151995, 'validation/ssim': 0.6865736532560144, 'validation/loss': 0.3231149650921497, 'validation/num_examples': 3554, 'test/ssim': 0.7038369006998744, 'test/loss': 0.32486520110478917, 'test/num_examples': 3581, 'score': 641.6860914230347, 'total_duration': 957.3562541007996, 'global_step': 2279, 'preemption_count': 0}), (2596, {'train/ssim': 0.7362116404942104, 'train/loss': 0.27342922346932547, 'validation/ssim': 0.7132649419008511, 'validation/loss': 0.2948249125655072, 'validation/num_examples': 3554, 'test/ssim': 0.7304297529495951, 'test/loss': 0.2965494564411128, 'test/num_examples': 3581, 'score': 714.5483064651489, 'total_duration': 1043.895576953888, 'global_step': 2596, 'preemption_count': 0}), (2714, {'train/ssim': 0.7369739668709892, 'train/loss': 0.2725751059395926, 'validation/ssim': 0.7138281002699424, 'validation/loss': 0.2940728440335713, 'validation/num_examples': 3554, 'test/ssim': 0.7309262153989807, 'test/loss': 0.2957664474832449, 'test/num_examples': 3581, 'score': 740.0242614746094, 'total_duration': 1078.3099195957184, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0401 02:00:28.481987 139890160002880 submission_runner.py:546] Timing: 740.0242614746094
I0401 02:00:28.482038 139890160002880 submission_runner.py:547] ====================
I0401 02:00:28.482188 139890160002880 submission_runner.py:606] Final fastmri score: 740.0242614746094
