I0331 22:19:33.158773 139949517576000 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/wmt_jax.
I0331 22:19:33.204994 139949517576000 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 22:19:34.041002 139949517576000 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0331 22:19:34.041734 139949517576000 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 22:19:34.045175 139949517576000 submission_runner.py:511] Using RNG seed 1685028930
I0331 22:19:35.312459 139949517576000 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 22:19:35.312652 139949517576000 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/wmt_jax/trial_1.
I0331 22:19:35.312818 139949517576000 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/wmt_jax/trial_1/hparams.json.
I0331 22:19:35.438811 139949517576000 submission_runner.py:230] Starting train once: RAM USED (GB) 4.530348032
I0331 22:19:35.438975 139949517576000 submission_runner.py:231] Initializing dataset.
I0331 22:19:35.446038 139949517576000 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:19:35.449065 139949517576000 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:19:35.449179 139949517576000 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:19:35.519035 139949517576000 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:19:37.309635 139949517576000 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.603047936
I0331 22:19:37.309871 139949517576000 submission_runner.py:240] Initializing model.
I0331 22:19:49.284725 139949517576000 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.681529344
I0331 22:19:49.284918 139949517576000 submission_runner.py:252] Initializing optimizer.
I0331 22:19:50.295075 139949517576000 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.681791488
I0331 22:19:50.295263 139949517576000 submission_runner.py:261] Initializing metrics bundle.
I0331 22:19:50.295312 139949517576000 submission_runner.py:276] Initializing checkpoint and logger.
I0331 22:19:50.296164 139949517576000 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw/wmt_jax/trial_1 with prefix checkpoint_
I0331 22:19:50.296436 139949517576000 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0331 22:19:50.296510 139949517576000 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0331 22:19:51.242646 139949517576000 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/wmt_jax/trial_1/meta_data_0.json.
I0331 22:19:51.243738 139949517576000 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/wmt_jax/trial_1/flags_0.json.
I0331 22:19:51.246820 139949517576000 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.6780928
I0331 22:19:51.246998 139949517576000 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.6780928
I0331 22:19:51.247056 139949517576000 submission_runner.py:313] Starting training loop.
I0331 22:19:51.941359 139949517576000 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.812343296
I0331 22:20:24.919365 139773301749504 logging_writer.py:48] [0] global_step=0, grad_norm=5.541873455047607, loss=11.12378978729248
I0331 22:20:24.931833 139949517576000 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 11.477438464
I0331 22:20:24.932064 139949517576000 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 11.477438464
I0331 22:20:24.932182 139949517576000 spec.py:298] Evaluating on the training split.
I0331 22:20:24.935203 139949517576000 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:20:24.937648 139949517576000 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:20:24.937752 139949517576000 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:20:24.968669 139949517576000 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0331 22:20:33.051253 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 22:25:37.971120 139949517576000 spec.py:310] Evaluating on the validation split.
I0331 22:25:37.974218 139949517576000 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:25:37.977200 139949517576000 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:25:37.977310 139949517576000 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:25:38.011120 139949517576000 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:25:45.409099 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 22:30:42.734610 139949517576000 spec.py:326] Evaluating on the test split.
I0331 22:30:42.736857 139949517576000 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:30:42.739413 139949517576000 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 22:30:42.739514 139949517576000 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 22:30:42.770085 139949517576000 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0331 22:30:49.641490 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 22:35:41.162918 139949517576000 submission_runner.py:382] Time since start: 33.69s, 	Step: 1, 	{'train/accuracy': 0.0006561981863342226, 'train/loss': 11.124899864196777, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.121123313903809, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.125858306884766, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0331 22:35:41.164111 139949517576000 submission_runner.py:396] After eval at step 1: RAM USED (GB) 11.906871296
I0331 22:35:41.172375 139762236114688 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=33.480204, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.125858, test/num_examples=3003, total_duration=33.685037, train/accuracy=0.000656, train/bleu=0.000000, train/loss=11.124900, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.121123, validation/num_examples=3000
I0331 22:35:42.222114 139949517576000 checkpoints.py:356] Saving checkpoint at step: 1
I0331 22:35:46.053360 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_1
I0331 22:35:46.057787 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_1.
I0331 22:35:46.063165 139949517576000 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 13.001936896
I0331 22:35:46.065756 139949517576000 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 13.001936896
I0331 22:35:46.141024 139949517576000 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 13.001547776
I0331 22:36:22.004749 139762244507392 logging_writer.py:48] [100] global_step=100, grad_norm=0.18383491039276123, loss=8.695673942565918
I0331 22:36:57.947333 139762336827136 logging_writer.py:48] [200] global_step=200, grad_norm=0.2799566388130188, loss=8.226905822753906
I0331 22:37:33.860944 139762244507392 logging_writer.py:48] [300] global_step=300, grad_norm=0.6913319826126099, loss=7.636650562286377
I0331 22:38:09.782986 139762336827136 logging_writer.py:48] [400] global_step=400, grad_norm=0.8386206030845642, loss=7.287294864654541
I0331 22:38:45.697973 139762244507392 logging_writer.py:48] [500] global_step=500, grad_norm=0.787510335445404, loss=6.892872333526611
I0331 22:39:21.674380 139762336827136 logging_writer.py:48] [600] global_step=600, grad_norm=0.808761715888977, loss=6.627610206604004
I0331 22:39:57.633312 139762244507392 logging_writer.py:48] [700] global_step=700, grad_norm=0.5834181904792786, loss=6.378326892852783
I0331 22:40:33.620948 139762336827136 logging_writer.py:48] [800] global_step=800, grad_norm=0.712110698223114, loss=6.096434116363525
I0331 22:41:09.605028 139762244507392 logging_writer.py:48] [900] global_step=900, grad_norm=0.5228091478347778, loss=5.913821697235107
I0331 22:41:45.549649 139762336827136 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9803068041801453, loss=5.684709548950195
I0331 22:42:21.572413 139762244507392 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7497628331184387, loss=5.531747341156006
I0331 22:42:57.542214 139762336827136 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9815905094146729, loss=5.312158584594727
I0331 22:43:33.526476 139762244507392 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9202900528907776, loss=5.209404468536377
I0331 22:44:09.531131 139762336827136 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7729814648628235, loss=5.143515110015869
I0331 22:44:45.477471 139762244507392 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6371808052062988, loss=4.826693058013916
I0331 22:45:21.432407 139762336827136 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5564081072807312, loss=4.740494728088379
I0331 22:45:57.384903 139762244507392 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6893168091773987, loss=4.664135456085205
I0331 22:46:33.392435 139762336827136 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6161919236183167, loss=4.466587543487549
I0331 22:47:09.359657 139762244507392 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7966392040252686, loss=4.45714807510376
I0331 22:47:45.334301 139762336827136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6735150814056396, loss=4.476137161254883
I0331 22:48:21.290161 139762244507392 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5551652312278748, loss=4.223155498504639
I0331 22:48:57.247430 139762336827136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6515259742736816, loss=4.260091781616211
I0331 22:49:33.234111 139762244507392 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6240385174751282, loss=4.227274417877197
I0331 22:49:46.244350 139949517576000 submission_runner.py:373] Before eval at step 2338: RAM USED (GB) 12.2406912
I0331 22:49:46.244537 139949517576000 spec.py:298] Evaluating on the training split.
I0331 22:49:49.238817 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 22:52:29.191206 139949517576000 spec.py:310] Evaluating on the validation split.
I0331 22:52:31.835717 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 22:55:01.018590 139949517576000 spec.py:326] Evaluating on the test split.
I0331 22:55:03.746916 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 22:57:35.543145 139949517576000 submission_runner.py:382] Time since start: 1795.00s, 	Step: 2338, 	{'train/accuracy': 0.49461987614631653, 'train/loss': 3.1559081077575684, 'train/bleu': 20.673531988074718, 'validation/accuracy': 0.49190959334373474, 'validation/loss': 3.176603078842163, 'validation/bleu': 16.345482354165828, 'validation/num_examples': 3000, 'test/accuracy': 0.48797863721847534, 'test/loss': 3.259354829788208, 'test/bleu': 14.975878449648697, 'test/num_examples': 3003}
I0331 22:57:35.543611 139949517576000 submission_runner.py:396] After eval at step 2338: RAM USED (GB) 12.512116736
I0331 22:57:35.551171 139762336827136 logging_writer.py:48] [2338] global_step=2338, preemption_count=0, score=869.726404, test/accuracy=0.487979, test/bleu=14.975878, test/loss=3.259355, test/num_examples=3003, total_duration=1794.996544, train/accuracy=0.494620, train/bleu=20.673532, train/loss=3.155908, validation/accuracy=0.491910, validation/bleu=16.345482, validation/loss=3.176603, validation/num_examples=3000
I0331 22:57:36.627096 139949517576000 checkpoints.py:356] Saving checkpoint at step: 2338
I0331 22:57:40.356937 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_2338
I0331 22:57:40.361429 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_2338.
I0331 22:57:40.366174 139949517576000 submission_runner.py:416] After logging and checkpointing eval at step 2338: RAM USED (GB) 13.664874496
I0331 22:58:02.993446 139762244507392 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.596102237701416, loss=4.116840362548828
I0331 22:58:38.939210 139762311649024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5991281867027283, loss=4.117264747619629
I0331 22:59:14.885468 139762244507392 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4546816647052765, loss=4.024383068084717
I0331 22:59:50.826527 139762311649024 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5108156800270081, loss=3.930915594100952
I0331 23:00:26.770965 139762244507392 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6210729479789734, loss=3.919646739959717
I0331 23:01:02.688321 139762311649024 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5625370144844055, loss=4.057705879211426
I0331 23:01:38.661949 139762244507392 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.4841662645339966, loss=3.896761178970337
I0331 23:02:14.620286 139762311649024 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5292659401893616, loss=3.7872159481048584
I0331 23:02:50.550799 139762244507392 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.517477810382843, loss=3.805290460586548
I0331 23:03:26.441544 139762311649024 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.4849979281425476, loss=3.816011428833008
I0331 23:04:02.373817 139762244507392 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.4432220458984375, loss=3.754709243774414
I0331 23:04:38.309872 139762311649024 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.42566022276878357, loss=3.6956164836883545
I0331 23:05:14.262391 139762244507392 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.39861518144607544, loss=3.784151792526245
I0331 23:05:50.216034 139762311649024 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4539337456226349, loss=3.740326166152954
I0331 23:06:26.171713 139762244507392 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.38971462845802307, loss=3.6994681358337402
I0331 23:07:02.153284 139762311649024 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.36295515298843384, loss=3.5912654399871826
I0331 23:07:38.084302 139762244507392 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3431105613708496, loss=3.7177491188049316
I0331 23:08:14.022595 139762311649024 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.3561577796936035, loss=3.6141507625579834
I0331 23:08:49.929665 139762244507392 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3407818675041199, loss=3.64764666557312
I0331 23:09:25.847231 139762311649024 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3258167505264282, loss=3.6326279640197754
I0331 23:10:01.753994 139762244507392 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.33759480714797974, loss=3.5793750286102295
I0331 23:10:37.690601 139762311649024 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.30368927121162415, loss=3.485236883163452
I0331 23:11:13.656855 139762244507392 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.35904714465141296, loss=3.56669282913208
I0331 23:11:40.685503 139949517576000 submission_runner.py:373] Before eval at step 4677: RAM USED (GB) 12.646100992
I0331 23:11:40.685737 139949517576000 spec.py:298] Evaluating on the training split.
I0331 23:11:43.683226 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:14:23.870217 139949517576000 spec.py:310] Evaluating on the validation split.
I0331 23:14:26.524388 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:16:50.813484 139949517576000 spec.py:326] Evaluating on the test split.
I0331 23:16:53.512709 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:19:19.810137 139949517576000 submission_runner.py:382] Time since start: 3109.44s, 	Step: 4677, 	{'train/accuracy': 0.5661449432373047, 'train/loss': 2.4700348377227783, 'train/bleu': 26.642990562374948, 'validation/accuracy': 0.5757647156715393, 'validation/loss': 2.3900439739227295, 'validation/bleu': 22.66955669151662, 'validation/num_examples': 3000, 'test/accuracy': 0.5771076679229736, 'test/loss': 2.395925521850586, 'test/bleu': 21.279533098307095, 'test/num_examples': 3003}
I0331 23:19:19.810661 139949517576000 submission_runner.py:396] After eval at step 4677: RAM USED (GB) 12.815310848
I0331 23:19:19.818238 139762311649024 logging_writer.py:48] [4677] global_step=4677, preemption_count=0, score=1706.027066, test/accuracy=0.577108, test/bleu=21.279533, test/loss=2.395926, test/num_examples=3003, total_duration=3109.437527, train/accuracy=0.566145, train/bleu=26.642991, train/loss=2.470035, validation/accuracy=0.575765, validation/bleu=22.669557, validation/loss=2.390044, validation/num_examples=3000
I0331 23:19:20.875858 139949517576000 checkpoints.py:356] Saving checkpoint at step: 4677
I0331 23:19:24.588928 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_4677
I0331 23:19:24.593486 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_4677.
I0331 23:19:24.598076 139949517576000 submission_runner.py:416] After logging and checkpointing eval at step 4677: RAM USED (GB) 13.966544896
I0331 23:19:33.206074 139762244507392 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.28730297088623047, loss=3.5502514839172363
I0331 23:20:09.146575 139762294863616 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.2955362796783447, loss=3.5662028789520264
I0331 23:20:45.063022 139762244507392 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3031998872756958, loss=3.549827814102173
I0331 23:21:21.000148 139762294863616 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.2731977701187134, loss=3.4841926097869873
I0331 23:21:56.902133 139762244507392 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.2518262565135956, loss=3.555575370788574
I0331 23:22:32.835089 139762294863616 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.2667500972747803, loss=3.472404956817627
I0331 23:23:08.811660 139762244507392 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.28538429737091064, loss=3.484866142272949
I0331 23:23:44.750294 139762294863616 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.2675758898258209, loss=3.4300806522369385
I0331 23:24:20.699122 139762244507392 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.26933327317237854, loss=3.4348270893096924
I0331 23:24:56.612525 139762294863616 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.23495614528656006, loss=3.415154218673706
I0331 23:25:32.540727 139762244507392 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.21569503843784332, loss=3.5245375633239746
I0331 23:26:08.447709 139762294863616 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2269560694694519, loss=3.367144823074341
I0331 23:26:44.378824 139762244507392 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.2544240951538086, loss=3.458716630935669
I0331 23:27:20.335471 139762294863616 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.2258319854736328, loss=3.35809588432312
I0331 23:27:56.314164 139762244507392 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.25656449794769287, loss=3.3399622440338135
I0331 23:28:32.223068 139762294863616 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.21437455713748932, loss=3.377735137939453
I0331 23:29:08.161823 139762244507392 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.2246064841747284, loss=3.3929190635681152
I0331 23:29:44.117091 139762294863616 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.24480204284191132, loss=3.3987104892730713
I0331 23:30:20.052727 139762244507392 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.2064189314842224, loss=3.338468551635742
I0331 23:30:56.002329 139762294863616 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2030370831489563, loss=3.3233768939971924
I0331 23:31:31.927597 139762244507392 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1884271204471588, loss=3.33703351020813
I0331 23:32:07.871317 139762294863616 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.24991951882839203, loss=3.3757333755493164
I0331 23:32:43.826245 139762244507392 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.21660368144512177, loss=3.3264544010162354
I0331 23:33:19.761343 139762294863616 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.19370795786380768, loss=3.3419992923736572
I0331 23:33:24.860691 139949517576000 submission_runner.py:373] Before eval at step 7016: RAM USED (GB) 13.067870208
I0331 23:33:24.860922 139949517576000 spec.py:298] Evaluating on the training split.
I0331 23:33:27.859760 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:36:13.034757 139949517576000 spec.py:310] Evaluating on the validation split.
I0331 23:36:15.681125 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:38:38.343092 139949517576000 spec.py:326] Evaluating on the test split.
I0331 23:38:41.065841 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:40:59.428231 139949517576000 submission_runner.py:382] Time since start: 4413.61s, 	Step: 7016, 	{'train/accuracy': 0.5962191820144653, 'train/loss': 2.19571590423584, 'train/bleu': 28.38407561252888, 'validation/accuracy': 0.6060061454772949, 'validation/loss': 2.1231424808502197, 'validation/bleu': 24.79763113570809, 'validation/num_examples': 3000, 'test/accuracy': 0.6099936366081238, 'test/loss': 2.105609178543091, 'test/bleu': 23.27768829240837, 'test/num_examples': 3003}
I0331 23:40:59.428717 139949517576000 submission_runner.py:396] After eval at step 7016: RAM USED (GB) 13.189828608
I0331 23:40:59.436747 139762244507392 logging_writer.py:48] [7016] global_step=7016, preemption_count=0, score=2542.479002, test/accuracy=0.609994, test/bleu=23.277688, test/loss=2.105609, test/num_examples=3003, total_duration=4413.612576, train/accuracy=0.596219, train/bleu=28.384076, train/loss=2.195716, validation/accuracy=0.606006, validation/bleu=24.797631, validation/loss=2.123142, validation/num_examples=3000
I0331 23:41:00.489811 139949517576000 checkpoints.py:356] Saving checkpoint at step: 7016
I0331 23:41:04.211726 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_7016
I0331 23:41:04.216114 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_7016.
I0331 23:41:04.220877 139949517576000 submission_runner.py:416] After logging and checkpointing eval at step 7016: RAM USED (GB) 14.344613888
I0331 23:41:34.786685 139762294863616 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.18837976455688477, loss=3.4155914783477783
I0331 23:42:10.701917 139762286470912 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.17325064539909363, loss=3.403698444366455
I0331 23:42:46.611685 139762294863616 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1855192929506302, loss=3.3063626289367676
I0331 23:43:22.516351 139762286470912 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.1744489073753357, loss=3.2622716426849365
I0331 23:43:58.427426 139762294863616 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.15997812151908875, loss=3.286665916442871
I0331 23:44:34.323357 139762286470912 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1821567267179489, loss=3.2865653038024902
I0331 23:45:10.265272 139762294863616 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.1946575939655304, loss=3.3271408081054688
I0331 23:45:46.165319 139762286470912 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.15870538353919983, loss=3.2389986515045166
I0331 23:46:22.081783 139762294863616 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.18175266683101654, loss=3.290344476699829
I0331 23:46:58.046463 139762286470912 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.1743030548095703, loss=3.2675912380218506
I0331 23:47:33.988093 139762294863616 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.14744476974010468, loss=3.2700130939483643
I0331 23:48:09.877799 139762286470912 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1653745472431183, loss=3.3122098445892334
I0331 23:48:45.809987 139762294863616 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.2155667543411255, loss=3.23201584815979
I0331 23:49:21.731300 139762286470912 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.17556889355182648, loss=3.2667064666748047
I0331 23:49:57.662591 139762294863616 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1564663201570511, loss=3.2222635746002197
I0331 23:50:33.586941 139762286470912 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.19446291029453278, loss=3.2718985080718994
I0331 23:51:09.519613 139762294863616 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.16219401359558105, loss=3.207703113555908
I0331 23:51:45.421235 139762286470912 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.15947599709033966, loss=3.2427191734313965
I0331 23:52:21.312100 139762294863616 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.1671937108039856, loss=3.2714807987213135
I0331 23:52:57.230287 139762286470912 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.16831296682357788, loss=3.3172876834869385
I0331 23:53:33.169604 139762294863616 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.15705664455890656, loss=3.2522425651550293
I0331 23:54:09.086335 139762286470912 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.16306814551353455, loss=3.294081211090088
I0331 23:54:45.046120 139762294863616 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.1506400853395462, loss=3.175762414932251
I0331 23:55:04.489545 139949517576000 submission_runner.py:373] Before eval at step 9356: RAM USED (GB) 13.452546048
I0331 23:55:04.489745 139949517576000 spec.py:298] Evaluating on the training split.
I0331 23:55:07.492064 139949517576000 workload.py:179] Translating evaluation dataset.
I0331 23:57:50.351056 139949517576000 spec.py:310] Evaluating on the validation split.
I0331 23:57:52.997163 139949517576000 workload.py:179] Translating evaluation dataset.
I0401 00:00:16.739511 139949517576000 spec.py:326] Evaluating on the test split.
I0401 00:00:19.437741 139949517576000 workload.py:179] Translating evaluation dataset.
I0401 00:02:28.316368 139949517576000 submission_runner.py:382] Time since start: 5713.24s, 	Step: 9356, 	{'train/accuracy': 0.6121424436569214, 'train/loss': 2.0680320262908936, 'train/bleu': 29.414064928727665, 'validation/accuracy': 0.626477062702179, 'validation/loss': 1.9646204710006714, 'validation/bleu': 25.92495479186465, 'validation/num_examples': 3000, 'test/accuracy': 0.6308175325393677, 'test/loss': 1.9283477067947388, 'test/bleu': 24.784486919343873, 'test/num_examples': 3003}
I0401 00:02:28.316832 139949517576000 submission_runner.py:396] After eval at step 9356: RAM USED (GB) 13.5462912
I0401 00:02:28.324692 139762286470912 logging_writer.py:48] [9356] global_step=9356, preemption_count=0, score=3379.055480, test/accuracy=0.630818, test/bleu=24.784487, test/loss=1.928348, test/num_examples=3003, total_duration=5713.241758, train/accuracy=0.612142, train/bleu=29.414065, train/loss=2.068032, validation/accuracy=0.626477, validation/bleu=25.924955, validation/loss=1.964620, validation/num_examples=3000
I0401 00:02:29.380917 139949517576000 checkpoints.py:356] Saving checkpoint at step: 9356
I0401 00:02:33.089432 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_9356
I0401 00:02:33.093802 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_9356.
I0401 00:02:33.098499 139949517576000 submission_runner.py:416] After logging and checkpointing eval at step 9356: RAM USED (GB) 14.698778624
I0401 00:02:49.259124 139762294863616 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.16971364617347717, loss=3.107191562652588
I0401 00:03:25.189728 139761876457216 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.16004839539527893, loss=3.2312352657318115
I0401 00:04:01.164718 139762294863616 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.20605126023292542, loss=3.1534712314605713
I0401 00:04:37.044481 139761876457216 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15859416127204895, loss=3.1816024780273438
I0401 00:05:12.969684 139762294863616 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.13856343924999237, loss=3.1821305751800537
I0401 00:05:48.881751 139761876457216 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16734518110752106, loss=3.1243436336517334
I0401 00:06:24.162402 139949517576000 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 13.571641344
I0401 00:06:24.162663 139949517576000 spec.py:298] Evaluating on the training split.
I0401 00:06:27.156861 139949517576000 workload.py:179] Translating evaluation dataset.
I0401 00:09:13.782619 139949517576000 spec.py:310] Evaluating on the validation split.
I0401 00:09:16.427483 139949517576000 workload.py:179] Translating evaluation dataset.
I0401 00:11:36.066962 139949517576000 spec.py:326] Evaluating on the test split.
I0401 00:11:38.773988 139949517576000 workload.py:179] Translating evaluation dataset.
I0401 00:13:48.704813 139949517576000 submission_runner.py:382] Time since start: 6392.91s, 	Step: 10000, 	{'train/accuracy': 0.6106635928153992, 'train/loss': 2.070141553878784, 'train/bleu': 28.646759522457057, 'validation/accuracy': 0.6298620104789734, 'validation/loss': 1.9383872747421265, 'validation/bleu': 26.42119760943259, 'validation/num_examples': 3000, 'test/accuracy': 0.6348033547401428, 'test/loss': 1.8949586153030396, 'test/bleu': 25.100154633974164, 'test/num_examples': 3003}
I0401 00:13:48.705290 139949517576000 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 13.605163008
I0401 00:13:48.713405 139762294863616 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3609.021402, test/accuracy=0.634803, test/bleu=25.100155, test/loss=1.894959, test/num_examples=3003, total_duration=6392.914329, train/accuracy=0.610664, train/bleu=28.646760, train/loss=2.070142, validation/accuracy=0.629862, validation/bleu=26.421198, validation/loss=1.938387, validation/num_examples=3000
I0401 00:13:49.764349 139949517576000 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 00:13:53.453658 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_10000
I0401 00:13:53.457956 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_10000.
I0401 00:13:53.462686 139949517576000 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.7597312
I0401 00:13:53.468835 139761876457216 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3609.021402
I0401 00:13:54.029808 139949517576000 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 00:13:59.578185 139949517576000 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_10000
I0401 00:13:59.582668 139949517576000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_jax/trial_1/checkpoint_10000.
I0401 00:13:59.666111 139949517576000 submission_runner.py:550] Tuning trial 1/1
I0401 00:13:59.666314 139949517576000 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 00:13:59.667486 139949517576000 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006561981863342226, 'train/loss': 11.124899864196777, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.121123313903809, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.125858306884766, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 33.480204343795776, 'total_duration': 33.68503737449646, 'global_step': 1, 'preemption_count': 0}), (2338, {'train/accuracy': 0.49461987614631653, 'train/loss': 3.1559081077575684, 'train/bleu': 20.673531988074718, 'validation/accuracy': 0.49190959334373474, 'validation/loss': 3.176603078842163, 'validation/bleu': 16.345482354165828, 'validation/num_examples': 3000, 'test/accuracy': 0.48797863721847534, 'test/loss': 3.259354829788208, 'test/bleu': 14.975878449648697, 'test/num_examples': 3003, 'score': 869.7264041900635, 'total_duration': 1794.9965443611145, 'global_step': 2338, 'preemption_count': 0}), (4677, {'train/accuracy': 0.5661449432373047, 'train/loss': 2.4700348377227783, 'train/bleu': 26.642990562374948, 'validation/accuracy': 0.5757647156715393, 'validation/loss': 2.3900439739227295, 'validation/bleu': 22.66955669151662, 'validation/num_examples': 3000, 'test/accuracy': 0.5771076679229736, 'test/loss': 2.395925521850586, 'test/bleu': 21.279533098307095, 'test/num_examples': 3003, 'score': 1706.0270655155182, 'total_duration': 3109.437527179718, 'global_step': 4677, 'preemption_count': 0}), (7016, {'train/accuracy': 0.5962191820144653, 'train/loss': 2.19571590423584, 'train/bleu': 28.38407561252888, 'validation/accuracy': 0.6060061454772949, 'validation/loss': 2.1231424808502197, 'validation/bleu': 24.79763113570809, 'validation/num_examples': 3000, 'test/accuracy': 0.6099936366081238, 'test/loss': 2.105609178543091, 'test/bleu': 23.27768829240837, 'test/num_examples': 3003, 'score': 2542.4790024757385, 'total_duration': 4413.612575531006, 'global_step': 7016, 'preemption_count': 0}), (9356, {'train/accuracy': 0.6121424436569214, 'train/loss': 2.0680320262908936, 'train/bleu': 29.414064928727665, 'validation/accuracy': 0.626477062702179, 'validation/loss': 1.9646204710006714, 'validation/bleu': 25.92495479186465, 'validation/num_examples': 3000, 'test/accuracy': 0.6308175325393677, 'test/loss': 1.9283477067947388, 'test/bleu': 24.784486919343873, 'test/num_examples': 3003, 'score': 3379.055480003357, 'total_duration': 5713.241757631302, 'global_step': 9356, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6106635928153992, 'train/loss': 2.070141553878784, 'train/bleu': 28.646759522457057, 'validation/accuracy': 0.6298620104789734, 'validation/loss': 1.9383872747421265, 'validation/bleu': 26.42119760943259, 'validation/num_examples': 3000, 'test/accuracy': 0.6348033547401428, 'test/loss': 1.8949586153030396, 'test/bleu': 25.100154633974164, 'test/num_examples': 3003, 'score': 3609.021401643753, 'total_duration': 6392.914328813553, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 00:13:59.667601 139949517576000 submission_runner.py:553] Timing: 3609.021401643753
I0401 00:13:59.667644 139949517576000 submission_runner.py:554] ====================
I0401 00:13:59.667755 139949517576000 submission_runner.py:613] Final wmt score: 3609.021401643753
