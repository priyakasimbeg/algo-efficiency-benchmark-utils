WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 02:02:25.737441 140694017992512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 02:02:25.737426 140177751729984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 02:02:25.738353 140558380787520 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 02:02:25.738385 139876441626432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 02:02:25.738578 139990385076032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 02:02:26.721737 140385046226752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 02:02:26.728322 140637496436544 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 02:02:26.729859 140525275027264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 02:02:26.730213 140525275027264 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.732620 140385046226752 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.738931 140637496436544 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.739057 140177751729984 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.739086 140694017992512 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.739239 140558380787520 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.739264 139876441626432 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:26.739225 139990385076032 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:02:27.855864 140525275027264 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/ogbg_pytorch.
W0401 02:02:27.892023 140637496436544 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.893579 140694017992512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.893643 139990385076032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.893727 140558380787520 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.893906 140177751729984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.894501 140525275027264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.894950 139876441626432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:02:27.896516 140385046226752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 02:02:27.898092 140525275027264 submission_runner.py:504] Using RNG seed 622911210
I0401 02:02:27.899097 140525275027264 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 02:02:27.899206 140525275027264 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/ogbg_pytorch/trial_1.
I0401 02:02:27.899401 140525275027264 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/hparams.json.
I0401 02:02:27.900285 140525275027264 submission_runner.py:230] Starting train once: RAM USED (GB) 5.763371008
I0401 02:02:27.900376 140525275027264 submission_runner.py:231] Initializing dataset.
I0401 02:02:27.900526 140525275027264 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.763371008
I0401 02:02:27.900582 140525275027264 submission_runner.py:240] Initializing model.
I0401 02:02:31.790768 140525275027264 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.429599232
I0401 02:02:31.790987 140525275027264 submission_runner.py:252] Initializing optimizer.
I0401 02:02:31.791844 140525275027264 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.429599232
I0401 02:02:31.791941 140525275027264 submission_runner.py:261] Initializing metrics bundle.
I0401 02:02:31.791988 140525275027264 submission_runner.py:275] Initializing checkpoint and logger.
I0401 02:02:31.793151 140525275027264 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 02:02:31.793245 140525275027264 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 02:02:32.410279 140525275027264 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0401 02:02:32.411216 140525275027264 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/flags_0.json.
I0401 02:02:32.444420 140525275027264 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 15.478718464
I0401 02:02:32.445527 140525275027264 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.478718464
I0401 02:02:32.445644 140525275027264 submission_runner.py:312] Starting training loop.
I0401 02:02:32.698181 140525275027264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 02:02:32.703533 140525275027264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 02:02:32.802623 140525275027264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0401 02:02:34.218443 140525275027264 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 15.716438016
I0401 02:02:37.885078 140486920607488 logging_writer.py:48] [0] global_step=0, grad_norm=2.577610, loss=0.728018
I0401 02:02:37.891031 140525275027264 submission.py:119] 0) loss = 0.728, grad_norm = 2.578
I0401 02:02:37.891450 140525275027264 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 22.086627328
I0401 02:02:37.899662 140525275027264 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 22.086627328
I0401 02:02:37.899773 140525275027264 spec.py:298] Evaluating on the training split.
I0401 02:02:37.904487 140525275027264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 02:02:37.907781 140525275027264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 02:02:37.957015 140525275027264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0401 02:02:52.668943 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.899952 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.901142 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.901387 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.901568 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.912398 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.912547 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:02:52.912648 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.163291 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.340256 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.342359 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.346266 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.347314 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.347519 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.347758 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:06.347963 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:03:30.595186 140525275027264 spec.py:310] Evaluating on the validation split.
I0401 02:03:30.598237 140525275027264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 02:03:30.602400 140525275027264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 02:03:30.655513 140525275027264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0401 02:03:43.798264 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.971677 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.974061 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.977156 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.978344 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.978718 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.978734 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:43.978739 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.150754 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.312861 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.314966 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.319432 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.319579 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.319928 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.319990 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:03:49.320266 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:04:13.284342 140525275027264 spec.py:326] Evaluating on the test split.
I0401 02:04:13.286937 140525275027264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0401 02:04:13.290762 140525275027264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0401 02:04:13.343617 140525275027264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0401 02:04:26.388128 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.547685 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.549056 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.554916 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.555017 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.555228 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.556030 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:26.556287 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:31.947752 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.104500 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.106992 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.110562 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.110871 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.111461 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.112406 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:04:32.112861 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:04:56.595777 140525275027264 submission_runner.py:380] Time since start: 5.45s, 	Step: 1, 	{'train/accuracy': 0.5816566196704617, 'train/loss': 0.7269815802574158, 'train/mean_average_precision': 0.0234192588720906, 'validation/accuracy': 0.5732028802304444, 'validation/loss': 0.7356288433074951, 'validation/mean_average_precision': 0.027120670980945116, 'validation/num_examples': 43793, 'test/accuracy': 0.5694598473000789, 'test/loss': 0.7382012605667114, 'test/mean_average_precision': 0.02818801717248282, 'test/num_examples': 43793}
I0401 02:04:56.596164 140525275027264 submission_runner.py:390] After eval at step 1: RAM USED (GB) 25.980735488
I0401 02:04:56.603672 140473313851136 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.452687, test/accuracy=0.569460, test/loss=0.738201, test/mean_average_precision=0.028188, test/num_examples=43793, total_duration=5.454640, train/accuracy=0.581657, train/loss=0.726982, train/mean_average_precision=0.023419, validation/accuracy=0.573203, validation/loss=0.735629, validation/mean_average_precision=0.027121, validation/num_examples=43793
I0401 02:04:56.915176 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_1.
I0401 02:04:56.915765 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 25.980284928
I0401 02:04:57.142482 140525275027264 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 26.029408256
I0401 02:04:57.145188 140525275027264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152724 139876441626432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152724 140558380787520 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152728 140177751729984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152734 140694017992512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152739 140385046226752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152772 139990385076032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.152841 140637496436544 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:04:57.184844 140473322243840 logging_writer.py:48] [1] global_step=1, grad_norm=2.586065, loss=0.726632
I0401 02:04:57.188581 140525275027264 submission.py:119] 1) loss = 0.727, grad_norm = 2.586
I0401 02:04:57.189020 140525275027264 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 26.029113344
I0401 02:04:57.449416 140473313851136 logging_writer.py:48] [2] global_step=2, grad_norm=2.562922, loss=0.724239
I0401 02:04:57.453357 140525275027264 submission.py:119] 2) loss = 0.724, grad_norm = 2.563
I0401 02:04:57.711873 140473322243840 logging_writer.py:48] [3] global_step=3, grad_norm=2.584736, loss=0.725497
I0401 02:04:57.715966 140525275027264 submission.py:119] 3) loss = 0.725, grad_norm = 2.585
I0401 02:04:57.984439 140473313851136 logging_writer.py:48] [4] global_step=4, grad_norm=2.617457, loss=0.723866
I0401 02:04:57.988589 140525275027264 submission.py:119] 4) loss = 0.724, grad_norm = 2.617
I0401 02:04:58.251623 140473322243840 logging_writer.py:48] [5] global_step=5, grad_norm=2.590742, loss=0.718244
I0401 02:04:58.255814 140525275027264 submission.py:119] 5) loss = 0.718, grad_norm = 2.591
I0401 02:04:58.515693 140473313851136 logging_writer.py:48] [6] global_step=6, grad_norm=2.539655, loss=0.716210
I0401 02:04:58.519496 140525275027264 submission.py:119] 6) loss = 0.716, grad_norm = 2.540
I0401 02:04:58.777173 140473322243840 logging_writer.py:48] [7] global_step=7, grad_norm=2.526600, loss=0.713742
I0401 02:04:58.781070 140525275027264 submission.py:119] 7) loss = 0.714, grad_norm = 2.527
I0401 02:04:59.042342 140473313851136 logging_writer.py:48] [8] global_step=8, grad_norm=2.421500, loss=0.706446
I0401 02:04:59.046423 140525275027264 submission.py:119] 8) loss = 0.706, grad_norm = 2.422
I0401 02:04:59.304428 140473322243840 logging_writer.py:48] [9] global_step=9, grad_norm=2.338436, loss=0.699620
I0401 02:04:59.308331 140525275027264 submission.py:119] 9) loss = 0.700, grad_norm = 2.338
I0401 02:04:59.569969 140473313851136 logging_writer.py:48] [10] global_step=10, grad_norm=2.263704, loss=0.693228
I0401 02:04:59.573889 140525275027264 submission.py:119] 10) loss = 0.693, grad_norm = 2.264
I0401 02:04:59.838263 140473322243840 logging_writer.py:48] [11] global_step=11, grad_norm=2.178879, loss=0.687635
I0401 02:04:59.842075 140525275027264 submission.py:119] 11) loss = 0.688, grad_norm = 2.179
I0401 02:05:00.106276 140473313851136 logging_writer.py:48] [12] global_step=12, grad_norm=2.133816, loss=0.681304
I0401 02:05:00.110301 140525275027264 submission.py:119] 12) loss = 0.681, grad_norm = 2.134
I0401 02:05:00.372581 140473322243840 logging_writer.py:48] [13] global_step=13, grad_norm=2.130035, loss=0.675656
I0401 02:05:00.377255 140525275027264 submission.py:119] 13) loss = 0.676, grad_norm = 2.130
I0401 02:05:00.642824 140473313851136 logging_writer.py:48] [14] global_step=14, grad_norm=2.080270, loss=0.665794
I0401 02:05:00.646874 140525275027264 submission.py:119] 14) loss = 0.666, grad_norm = 2.080
I0401 02:05:00.912341 140473322243840 logging_writer.py:48] [15] global_step=15, grad_norm=2.012651, loss=0.658871
I0401 02:05:00.916365 140525275027264 submission.py:119] 15) loss = 0.659, grad_norm = 2.013
I0401 02:05:01.188271 140473313851136 logging_writer.py:48] [16] global_step=16, grad_norm=1.986044, loss=0.651366
I0401 02:05:01.192089 140525275027264 submission.py:119] 16) loss = 0.651, grad_norm = 1.986
I0401 02:05:01.456935 140473322243840 logging_writer.py:48] [17] global_step=17, grad_norm=1.899381, loss=0.641511
I0401 02:05:01.461004 140525275027264 submission.py:119] 17) loss = 0.642, grad_norm = 1.899
I0401 02:05:01.725929 140473313851136 logging_writer.py:48] [18] global_step=18, grad_norm=1.830043, loss=0.632918
I0401 02:05:01.729881 140525275027264 submission.py:119] 18) loss = 0.633, grad_norm = 1.830
I0401 02:05:01.999631 140473322243840 logging_writer.py:48] [19] global_step=19, grad_norm=1.829138, loss=0.623392
I0401 02:05:02.003316 140525275027264 submission.py:119] 19) loss = 0.623, grad_norm = 1.829
I0401 02:05:02.277935 140473313851136 logging_writer.py:48] [20] global_step=20, grad_norm=1.763927, loss=0.613827
I0401 02:05:02.281968 140525275027264 submission.py:119] 20) loss = 0.614, grad_norm = 1.764
I0401 02:05:02.544230 140473322243840 logging_writer.py:48] [21] global_step=21, grad_norm=1.670202, loss=0.607840
I0401 02:05:02.548257 140525275027264 submission.py:119] 21) loss = 0.608, grad_norm = 1.670
I0401 02:05:02.809849 140473313851136 logging_writer.py:48] [22] global_step=22, grad_norm=1.639359, loss=0.598082
I0401 02:05:02.813805 140525275027264 submission.py:119] 22) loss = 0.598, grad_norm = 1.639
I0401 02:05:03.075723 140473322243840 logging_writer.py:48] [23] global_step=23, grad_norm=1.618053, loss=0.586084
I0401 02:05:03.079812 140525275027264 submission.py:119] 23) loss = 0.586, grad_norm = 1.618
I0401 02:05:03.341815 140473313851136 logging_writer.py:48] [24] global_step=24, grad_norm=1.511669, loss=0.579120
I0401 02:05:03.345958 140525275027264 submission.py:119] 24) loss = 0.579, grad_norm = 1.512
I0401 02:05:03.608838 140473322243840 logging_writer.py:48] [25] global_step=25, grad_norm=1.408402, loss=0.569104
I0401 02:05:03.612645 140525275027264 submission.py:119] 25) loss = 0.569, grad_norm = 1.408
I0401 02:05:03.872752 140473313851136 logging_writer.py:48] [26] global_step=26, grad_norm=1.373051, loss=0.563642
I0401 02:05:03.876783 140525275027264 submission.py:119] 26) loss = 0.564, grad_norm = 1.373
I0401 02:05:04.143214 140473322243840 logging_writer.py:48] [27] global_step=27, grad_norm=1.288668, loss=0.554122
I0401 02:05:04.147207 140525275027264 submission.py:119] 27) loss = 0.554, grad_norm = 1.289
I0401 02:05:04.407559 140473313851136 logging_writer.py:48] [28] global_step=28, grad_norm=1.222771, loss=0.547932
I0401 02:05:04.411525 140525275027264 submission.py:119] 28) loss = 0.548, grad_norm = 1.223
I0401 02:05:04.668538 140473322243840 logging_writer.py:48] [29] global_step=29, grad_norm=1.190789, loss=0.538863
I0401 02:05:04.672428 140525275027264 submission.py:119] 29) loss = 0.539, grad_norm = 1.191
I0401 02:05:04.933234 140473313851136 logging_writer.py:48] [30] global_step=30, grad_norm=1.130470, loss=0.531360
I0401 02:05:04.937183 140525275027264 submission.py:119] 30) loss = 0.531, grad_norm = 1.130
I0401 02:05:05.197467 140473322243840 logging_writer.py:48] [31] global_step=31, grad_norm=1.063331, loss=0.524963
I0401 02:05:05.201139 140525275027264 submission.py:119] 31) loss = 0.525, grad_norm = 1.063
I0401 02:05:05.462067 140473313851136 logging_writer.py:48] [32] global_step=32, grad_norm=1.014326, loss=0.517899
I0401 02:05:05.466053 140525275027264 submission.py:119] 32) loss = 0.518, grad_norm = 1.014
I0401 02:05:05.725930 140473322243840 logging_writer.py:48] [33] global_step=33, grad_norm=0.949622, loss=0.512778
I0401 02:05:05.729697 140525275027264 submission.py:119] 33) loss = 0.513, grad_norm = 0.950
I0401 02:05:05.999688 140473313851136 logging_writer.py:48] [34] global_step=34, grad_norm=0.917875, loss=0.506222
I0401 02:05:06.003720 140525275027264 submission.py:119] 34) loss = 0.506, grad_norm = 0.918
I0401 02:05:06.269581 140473322243840 logging_writer.py:48] [35] global_step=35, grad_norm=0.883030, loss=0.501750
I0401 02:05:06.273575 140525275027264 submission.py:119] 35) loss = 0.502, grad_norm = 0.883
I0401 02:05:06.534801 140473313851136 logging_writer.py:48] [36] global_step=36, grad_norm=0.841642, loss=0.497033
I0401 02:05:06.538524 140525275027264 submission.py:119] 36) loss = 0.497, grad_norm = 0.842
I0401 02:05:06.797854 140473322243840 logging_writer.py:48] [37] global_step=37, grad_norm=0.834958, loss=0.490991
I0401 02:05:06.801653 140525275027264 submission.py:119] 37) loss = 0.491, grad_norm = 0.835
I0401 02:05:07.072959 140473313851136 logging_writer.py:48] [38] global_step=38, grad_norm=0.827847, loss=0.484603
I0401 02:05:07.076960 140525275027264 submission.py:119] 38) loss = 0.485, grad_norm = 0.828
I0401 02:05:07.339898 140473322243840 logging_writer.py:48] [39] global_step=39, grad_norm=0.828806, loss=0.480650
I0401 02:05:07.343915 140525275027264 submission.py:119] 39) loss = 0.481, grad_norm = 0.829
I0401 02:05:07.601919 140473313851136 logging_writer.py:48] [40] global_step=40, grad_norm=0.814556, loss=0.475575
I0401 02:05:07.605938 140525275027264 submission.py:119] 40) loss = 0.476, grad_norm = 0.815
I0401 02:05:07.863452 140473322243840 logging_writer.py:48] [41] global_step=41, grad_norm=0.775118, loss=0.469546
I0401 02:05:07.867427 140525275027264 submission.py:119] 41) loss = 0.470, grad_norm = 0.775
I0401 02:05:08.129027 140473313851136 logging_writer.py:48] [42] global_step=42, grad_norm=0.749499, loss=0.463266
I0401 02:05:08.132883 140525275027264 submission.py:119] 42) loss = 0.463, grad_norm = 0.749
I0401 02:05:08.390300 140473322243840 logging_writer.py:48] [43] global_step=43, grad_norm=0.708965, loss=0.457669
I0401 02:05:08.394101 140525275027264 submission.py:119] 43) loss = 0.458, grad_norm = 0.709
I0401 02:05:08.653569 140473313851136 logging_writer.py:48] [44] global_step=44, grad_norm=0.683427, loss=0.454422
I0401 02:05:08.657439 140525275027264 submission.py:119] 44) loss = 0.454, grad_norm = 0.683
I0401 02:05:08.915925 140473322243840 logging_writer.py:48] [45] global_step=45, grad_norm=0.695440, loss=0.448617
I0401 02:05:08.919820 140525275027264 submission.py:119] 45) loss = 0.449, grad_norm = 0.695
I0401 02:05:09.182368 140473313851136 logging_writer.py:48] [46] global_step=46, grad_norm=0.670987, loss=0.444096
I0401 02:05:09.186263 140525275027264 submission.py:119] 46) loss = 0.444, grad_norm = 0.671
I0401 02:05:09.444416 140473322243840 logging_writer.py:48] [47] global_step=47, grad_norm=0.656215, loss=0.439563
I0401 02:05:09.448391 140525275027264 submission.py:119] 47) loss = 0.440, grad_norm = 0.656
I0401 02:05:09.707229 140473313851136 logging_writer.py:48] [48] global_step=48, grad_norm=0.656775, loss=0.437114
I0401 02:05:09.710938 140525275027264 submission.py:119] 48) loss = 0.437, grad_norm = 0.657
I0401 02:05:09.971476 140473322243840 logging_writer.py:48] [49] global_step=49, grad_norm=0.654856, loss=0.432466
I0401 02:05:09.975326 140525275027264 submission.py:119] 49) loss = 0.432, grad_norm = 0.655
I0401 02:05:10.236478 140473313851136 logging_writer.py:48] [50] global_step=50, grad_norm=0.604811, loss=0.429264
I0401 02:05:10.240338 140525275027264 submission.py:119] 50) loss = 0.429, grad_norm = 0.605
I0401 02:05:10.508895 140473322243840 logging_writer.py:48] [51] global_step=51, grad_norm=0.581370, loss=0.423808
I0401 02:05:10.512688 140525275027264 submission.py:119] 51) loss = 0.424, grad_norm = 0.581
I0401 02:05:10.770028 140473313851136 logging_writer.py:48] [52] global_step=52, grad_norm=0.574161, loss=0.420660
I0401 02:05:10.773679 140525275027264 submission.py:119] 52) loss = 0.421, grad_norm = 0.574
I0401 02:05:11.032157 140473322243840 logging_writer.py:48] [53] global_step=53, grad_norm=0.572893, loss=0.414074
I0401 02:05:11.036136 140525275027264 submission.py:119] 53) loss = 0.414, grad_norm = 0.573
I0401 02:05:11.299525 140473313851136 logging_writer.py:48] [54] global_step=54, grad_norm=0.554014, loss=0.411085
I0401 02:05:11.303634 140525275027264 submission.py:119] 54) loss = 0.411, grad_norm = 0.554
I0401 02:05:11.559860 140473322243840 logging_writer.py:48] [55] global_step=55, grad_norm=0.531541, loss=0.406974
I0401 02:05:11.563936 140525275027264 submission.py:119] 55) loss = 0.407, grad_norm = 0.532
I0401 02:05:11.819571 140473313851136 logging_writer.py:48] [56] global_step=56, grad_norm=0.512357, loss=0.405069
I0401 02:05:11.823688 140525275027264 submission.py:119] 56) loss = 0.405, grad_norm = 0.512
I0401 02:05:12.080289 140473322243840 logging_writer.py:48] [57] global_step=57, grad_norm=0.498763, loss=0.403366
I0401 02:05:12.084321 140525275027264 submission.py:119] 57) loss = 0.403, grad_norm = 0.499
I0401 02:05:12.340468 140473313851136 logging_writer.py:48] [58] global_step=58, grad_norm=0.492684, loss=0.398930
I0401 02:05:12.344403 140525275027264 submission.py:119] 58) loss = 0.399, grad_norm = 0.493
I0401 02:05:12.600175 140473322243840 logging_writer.py:48] [59] global_step=59, grad_norm=0.479061, loss=0.395222
I0401 02:05:12.603963 140525275027264 submission.py:119] 59) loss = 0.395, grad_norm = 0.479
I0401 02:05:12.860510 140473313851136 logging_writer.py:48] [60] global_step=60, grad_norm=0.469498, loss=0.395766
I0401 02:05:12.864641 140525275027264 submission.py:119] 60) loss = 0.396, grad_norm = 0.469
I0401 02:05:13.119046 140473322243840 logging_writer.py:48] [61] global_step=61, grad_norm=0.475762, loss=0.389641
I0401 02:05:13.123492 140525275027264 submission.py:119] 61) loss = 0.390, grad_norm = 0.476
I0401 02:05:13.380283 140473313851136 logging_writer.py:48] [62] global_step=62, grad_norm=0.455169, loss=0.388004
I0401 02:05:13.384057 140525275027264 submission.py:119] 62) loss = 0.388, grad_norm = 0.455
I0401 02:05:13.642647 140473322243840 logging_writer.py:48] [63] global_step=63, grad_norm=0.451966, loss=0.385109
I0401 02:05:13.646737 140525275027264 submission.py:119] 63) loss = 0.385, grad_norm = 0.452
I0401 02:05:13.905262 140473313851136 logging_writer.py:48] [64] global_step=64, grad_norm=0.442812, loss=0.383537
I0401 02:05:13.909129 140525275027264 submission.py:119] 64) loss = 0.384, grad_norm = 0.443
I0401 02:05:14.164729 140473322243840 logging_writer.py:48] [65] global_step=65, grad_norm=0.439778, loss=0.381109
I0401 02:05:14.168709 140525275027264 submission.py:119] 65) loss = 0.381, grad_norm = 0.440
I0401 02:05:14.425605 140473313851136 logging_writer.py:48] [66] global_step=66, grad_norm=0.437755, loss=0.379700
I0401 02:05:14.430021 140525275027264 submission.py:119] 66) loss = 0.380, grad_norm = 0.438
I0401 02:05:14.695346 140473322243840 logging_writer.py:48] [67] global_step=67, grad_norm=0.435985, loss=0.374352
I0401 02:05:14.699329 140525275027264 submission.py:119] 67) loss = 0.374, grad_norm = 0.436
I0401 02:05:14.963050 140473313851136 logging_writer.py:48] [68] global_step=68, grad_norm=0.426832, loss=0.374233
I0401 02:05:14.967041 140525275027264 submission.py:119] 68) loss = 0.374, grad_norm = 0.427
I0401 02:05:15.237434 140473322243840 logging_writer.py:48] [69] global_step=69, grad_norm=0.424030, loss=0.371090
I0401 02:05:15.241418 140525275027264 submission.py:119] 69) loss = 0.371, grad_norm = 0.424
I0401 02:05:15.505666 140473313851136 logging_writer.py:48] [70] global_step=70, grad_norm=0.419118, loss=0.371193
I0401 02:05:15.509683 140525275027264 submission.py:119] 70) loss = 0.371, grad_norm = 0.419
I0401 02:05:15.776076 140473322243840 logging_writer.py:48] [71] global_step=71, grad_norm=0.417432, loss=0.369060
I0401 02:05:15.780113 140525275027264 submission.py:119] 71) loss = 0.369, grad_norm = 0.417
I0401 02:05:16.056820 140473313851136 logging_writer.py:48] [72] global_step=72, grad_norm=0.409012, loss=0.365487
I0401 02:05:16.061061 140525275027264 submission.py:119] 72) loss = 0.365, grad_norm = 0.409
I0401 02:05:16.333720 140473322243840 logging_writer.py:48] [73] global_step=73, grad_norm=0.412386, loss=0.361811
I0401 02:05:16.337844 140525275027264 submission.py:119] 73) loss = 0.362, grad_norm = 0.412
I0401 02:05:16.605454 140473313851136 logging_writer.py:48] [74] global_step=74, grad_norm=0.407802, loss=0.361070
I0401 02:05:16.609476 140525275027264 submission.py:119] 74) loss = 0.361, grad_norm = 0.408
I0401 02:05:16.884955 140473322243840 logging_writer.py:48] [75] global_step=75, grad_norm=0.404068, loss=0.358878
I0401 02:05:16.888871 140525275027264 submission.py:119] 75) loss = 0.359, grad_norm = 0.404
I0401 02:05:17.162304 140473313851136 logging_writer.py:48] [76] global_step=76, grad_norm=0.402519, loss=0.357794
I0401 02:05:17.166323 140525275027264 submission.py:119] 76) loss = 0.358, grad_norm = 0.403
I0401 02:05:17.428857 140473322243840 logging_writer.py:48] [77] global_step=77, grad_norm=0.396071, loss=0.356590
I0401 02:05:17.432841 140525275027264 submission.py:119] 77) loss = 0.357, grad_norm = 0.396
I0401 02:05:17.697940 140473313851136 logging_writer.py:48] [78] global_step=78, grad_norm=0.394669, loss=0.354766
I0401 02:05:17.702076 140525275027264 submission.py:119] 78) loss = 0.355, grad_norm = 0.395
I0401 02:05:17.969288 140473322243840 logging_writer.py:48] [79] global_step=79, grad_norm=0.395658, loss=0.350472
I0401 02:05:17.973184 140525275027264 submission.py:119] 79) loss = 0.350, grad_norm = 0.396
I0401 02:05:18.244759 140473313851136 logging_writer.py:48] [80] global_step=80, grad_norm=0.390935, loss=0.349989
I0401 02:05:18.248649 140525275027264 submission.py:119] 80) loss = 0.350, grad_norm = 0.391
I0401 02:05:18.523805 140473322243840 logging_writer.py:48] [81] global_step=81, grad_norm=0.387058, loss=0.348583
I0401 02:05:18.527840 140525275027264 submission.py:119] 81) loss = 0.349, grad_norm = 0.387
I0401 02:05:18.796513 140473313851136 logging_writer.py:48] [82] global_step=82, grad_norm=0.386332, loss=0.346922
I0401 02:05:18.800477 140525275027264 submission.py:119] 82) loss = 0.347, grad_norm = 0.386
I0401 02:05:19.064759 140473322243840 logging_writer.py:48] [83] global_step=83, grad_norm=0.384463, loss=0.344787
I0401 02:05:19.068716 140525275027264 submission.py:119] 83) loss = 0.345, grad_norm = 0.384
I0401 02:05:19.333161 140473313851136 logging_writer.py:48] [84] global_step=84, grad_norm=0.383252, loss=0.343845
I0401 02:05:19.337215 140525275027264 submission.py:119] 84) loss = 0.344, grad_norm = 0.383
I0401 02:05:19.602102 140473322243840 logging_writer.py:48] [85] global_step=85, grad_norm=0.383933, loss=0.341369
I0401 02:05:19.606044 140525275027264 submission.py:119] 85) loss = 0.341, grad_norm = 0.384
I0401 02:05:19.869602 140473313851136 logging_writer.py:48] [86] global_step=86, grad_norm=0.379723, loss=0.342656
I0401 02:05:19.873417 140525275027264 submission.py:119] 86) loss = 0.343, grad_norm = 0.380
I0401 02:05:20.139493 140473322243840 logging_writer.py:48] [87] global_step=87, grad_norm=0.381547, loss=0.340187
I0401 02:05:20.143547 140525275027264 submission.py:119] 87) loss = 0.340, grad_norm = 0.382
I0401 02:05:20.413309 140473313851136 logging_writer.py:48] [88] global_step=88, grad_norm=0.378596, loss=0.337891
I0401 02:05:20.417362 140525275027264 submission.py:119] 88) loss = 0.338, grad_norm = 0.379
I0401 02:05:20.687380 140473322243840 logging_writer.py:48] [89] global_step=89, grad_norm=0.380989, loss=0.335495
I0401 02:05:20.691457 140525275027264 submission.py:119] 89) loss = 0.335, grad_norm = 0.381
I0401 02:05:20.962482 140473313851136 logging_writer.py:48] [90] global_step=90, grad_norm=0.375170, loss=0.335121
I0401 02:05:20.966520 140525275027264 submission.py:119] 90) loss = 0.335, grad_norm = 0.375
I0401 02:05:21.233068 140473322243840 logging_writer.py:48] [91] global_step=91, grad_norm=0.377098, loss=0.333651
I0401 02:05:21.237163 140525275027264 submission.py:119] 91) loss = 0.334, grad_norm = 0.377
I0401 02:05:21.516986 140473313851136 logging_writer.py:48] [92] global_step=92, grad_norm=0.377022, loss=0.330360
I0401 02:05:21.520925 140525275027264 submission.py:119] 92) loss = 0.330, grad_norm = 0.377
I0401 02:05:21.788724 140473322243840 logging_writer.py:48] [93] global_step=93, grad_norm=0.375378, loss=0.329894
I0401 02:05:21.792650 140525275027264 submission.py:119] 93) loss = 0.330, grad_norm = 0.375
I0401 02:05:22.073048 140473313851136 logging_writer.py:48] [94] global_step=94, grad_norm=0.370351, loss=0.328623
I0401 02:05:22.077025 140525275027264 submission.py:119] 94) loss = 0.329, grad_norm = 0.370
I0401 02:05:22.337041 140473322243840 logging_writer.py:48] [95] global_step=95, grad_norm=0.364748, loss=0.326392
I0401 02:05:22.340958 140525275027264 submission.py:119] 95) loss = 0.326, grad_norm = 0.365
I0401 02:05:22.602356 140473313851136 logging_writer.py:48] [96] global_step=96, grad_norm=0.369899, loss=0.326041
I0401 02:05:22.606183 140525275027264 submission.py:119] 96) loss = 0.326, grad_norm = 0.370
I0401 02:05:22.868757 140473322243840 logging_writer.py:48] [97] global_step=97, grad_norm=0.365761, loss=0.323770
I0401 02:05:22.872613 140525275027264 submission.py:119] 97) loss = 0.324, grad_norm = 0.366
I0401 02:05:23.134507 140473313851136 logging_writer.py:48] [98] global_step=98, grad_norm=0.370030, loss=0.322183
I0401 02:05:23.138701 140525275027264 submission.py:119] 98) loss = 0.322, grad_norm = 0.370
I0401 02:05:23.407165 140473322243840 logging_writer.py:48] [99] global_step=99, grad_norm=0.362520, loss=0.321169
I0401 02:05:23.411386 140525275027264 submission.py:119] 99) loss = 0.321, grad_norm = 0.363
I0401 02:05:23.678002 140473313851136 logging_writer.py:48] [100] global_step=100, grad_norm=0.363573, loss=0.319998
I0401 02:05:23.682120 140525275027264 submission.py:119] 100) loss = 0.320, grad_norm = 0.364
I0401 02:07:07.386165 140473322243840 logging_writer.py:48] [500] global_step=500, grad_norm=0.053099, loss=0.061945
I0401 02:07:07.390686 140525275027264 submission.py:119] 500) loss = 0.062, grad_norm = 0.053
I0401 02:08:56.919580 140525275027264 submission_runner.py:371] Before eval at step 927: RAM USED (GB) 26.796965888
I0401 02:08:56.919833 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:09:10.696517 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.867771 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.868700 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.873988 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.874169 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.874288 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.875237 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:10.875813 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.546903 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.690773 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.691030 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.696037 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.696124 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.697352 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.697472 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:24.697542 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:09:49.344748 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:09:49.672475 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.846878 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.848516 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.852705 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.852932 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.853187 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.853522 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:49.854487 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.011170 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.168703 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.170162 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.175780 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.175798 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.175942 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.176105 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:50.176497 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:09:52.580486 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:09:52.899858 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.060427 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.060806 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.068930 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.068926 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.069309 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.069477 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.069808 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.235524 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.391628 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.393480 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.397529 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.397726 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.397761 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.398134 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:09:53.398251 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:09:55.690527 140525275027264 submission_runner.py:380] Time since start: 384.47s, 	Step: 927, 	{'train/accuracy': 0.9864393528250182, 'train/loss': 0.0525132454931736, 'train/mean_average_precision': 0.05636010676782553, 'validation/accuracy': 0.9836101442956807, 'validation/loss': 0.061678990721702576, 'validation/mean_average_precision': 0.05674771535115736, 'validation/num_examples': 43793, 'test/accuracy': 0.9825343199959229, 'test/loss': 0.06492893397808075, 'test/mean_average_precision': 0.05546132895541424, 'test/num_examples': 43793}
I0401 02:09:55.690917 140525275027264 submission_runner.py:390] After eval at step 927: RAM USED (GB) 28.040773632
I0401 02:09:55.698812 140473313851136 logging_writer.py:48] [927] global_step=927, preemption_count=0, score=244.479134, test/accuracy=0.982534, test/loss=0.064929, test/mean_average_precision=0.055461, test/num_examples=43793, total_duration=384.474254, train/accuracy=0.986439, train/loss=0.052513, train/mean_average_precision=0.056360, validation/accuracy=0.983610, validation/loss=0.061679, validation/mean_average_precision=0.056748, validation/num_examples=43793
I0401 02:09:55.787479 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_927.
I0401 02:09:55.788003 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 927: RAM USED (GB) 28.04035584
I0401 02:10:14.897384 140473322243840 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.070779, loss=0.054463
I0401 02:10:14.901970 140525275027264 submission.py:119] 1000) loss = 0.054, grad_norm = 0.071
I0401 02:12:24.333068 140473313851136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.050861, loss=0.054339
I0401 02:12:24.337977 140525275027264 submission.py:119] 1500) loss = 0.054, grad_norm = 0.051
I0401 02:13:55.898277 140525275027264 submission_runner.py:371] Before eval at step 1856: RAM USED (GB) 28.372217856
I0401 02:13:55.898568 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:14:09.693609 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.836647 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.838510 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.843920 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.844108 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.844260 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.844310 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:09.844980 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:23.869956 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.015864 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.016338 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.020307 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.021763 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.022132 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.022156 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:24.022567 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:14:49.743031 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:14:50.061652 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.219921 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.220906 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.227201 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.227903 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.228133 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.228623 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.228914 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.387830 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.541043 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.541582 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.546280 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.546395 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.546843 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.546901 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:50.547097 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:14:52.904047 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:14:53.226583 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.382838 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.383448 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.389004 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.389486 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.389889 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.389892 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.390012 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.568749 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.728086 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.729113 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.733469 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.734079 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.734631 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.734711 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:14:53.735212 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:14:56.014715 140525275027264 submission_runner.py:380] Time since start: 683.45s, 	Step: 1856, 	{'train/accuracy': 0.9871421229806356, 'train/loss': 0.04841460660099983, 'train/mean_average_precision': 0.09060277106775994, 'validation/accuracy': 0.9844593722370661, 'validation/loss': 0.058164551854133606, 'validation/mean_average_precision': 0.08906351552868715, 'validation/num_examples': 43793, 'test/accuracy': 0.983482850426607, 'test/loss': 0.06137758493423462, 'test/mean_average_precision': 0.08937046016132462, 'test/num_examples': 43793}
I0401 02:14:56.015142 140525275027264 submission_runner.py:390] After eval at step 1856: RAM USED (GB) 28.837818368
I0401 02:14:56.024196 140473322243840 logging_writer.py:48] [1856] global_step=1856, preemption_count=0, score=483.604909, test/accuracy=0.983483, test/loss=0.061378, test/mean_average_precision=0.089370, test/num_examples=43793, total_duration=683.452884, train/accuracy=0.987142, train/loss=0.048415, train/mean_average_precision=0.090603, validation/accuracy=0.984459, validation/loss=0.058165, validation/mean_average_precision=0.089064, validation/num_examples=43793
I0401 02:14:56.120996 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_1856.
I0401 02:14:56.121577 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 1856: RAM USED (GB) 28.837855232
I0401 02:15:34.199411 140473313851136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.020734, loss=0.047189
I0401 02:15:34.204192 140525275027264 submission.py:119] 2000) loss = 0.047, grad_norm = 0.021
I0401 02:17:44.262148 140473322243840 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.035356, loss=0.044310
I0401 02:17:44.267076 140525275027264 submission.py:119] 2500) loss = 0.044, grad_norm = 0.035
I0401 02:18:56.339321 140525275027264 submission_runner.py:371] Before eval at step 2779: RAM USED (GB) 29.111820288
I0401 02:18:56.339528 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:19:10.573986 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.722768 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.723000 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.728033 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.728215 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.728323 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.728946 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:10.729188 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.735607 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.870429 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.871954 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.877483 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.877506 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.877652 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.877717 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:24.877924 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:19:50.976609 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:19:51.292071 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.443549 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.445271 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.451857 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.451988 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.452401 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.452784 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.453494 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.626267 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.774307 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.774707 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.780166 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.780287 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.780529 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.780548 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:51.781000 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:19:54.210961 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:19:54.533723 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.697363 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.698545 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.703907 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.704244 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.705539 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.705899 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.706009 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:54.872547 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.023317 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.023790 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.028984 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.029416 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.029536 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.029837 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:19:55.030596 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:19:57.358253 140525275027264 submission_runner.py:380] Time since start: 983.89s, 	Step: 2779, 	{'train/accuracy': 0.9879097508566804, 'train/loss': 0.043474845588207245, 'train/mean_average_precision': 0.1518303489940671, 'validation/accuracy': 0.9849818179161115, 'validation/loss': 0.052732788026332855, 'validation/mean_average_precision': 0.1368441009503704, 'validation/num_examples': 43793, 'test/accuracy': 0.9839946019689166, 'test/loss': 0.05561356246471405, 'test/mean_average_precision': 0.13791653531525844, 'test/num_examples': 43793}
I0401 02:19:57.358672 140525275027264 submission_runner.py:390] After eval at step 2779: RAM USED (GB) 29.518168064
I0401 02:19:57.366761 140473313851136 logging_writer.py:48] [2779] global_step=2779, preemption_count=0, score=722.847010, test/accuracy=0.983995, test/loss=0.055614, test/mean_average_precision=0.137917, test/num_examples=43793, total_duration=983.894034, train/accuracy=0.987910, train/loss=0.043475, train/mean_average_precision=0.151830, validation/accuracy=0.984982, validation/loss=0.052733, validation/mean_average_precision=0.136844, validation/num_examples=43793
I0401 02:19:57.457016 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_2779.
I0401 02:19:57.457519 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 2779: RAM USED (GB) 29.517996032
I0401 02:20:58.042915 140473322243840 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.016819, loss=0.043952
I0401 02:20:58.047980 140525275027264 submission.py:119] 3000) loss = 0.044, grad_norm = 0.017
I0401 02:23:08.302081 140473313851136 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.015800, loss=0.040892
I0401 02:23:08.307327 140525275027264 submission.py:119] 3500) loss = 0.041, grad_norm = 0.016
I0401 02:23:57.511368 140525275027264 submission_runner.py:371] Before eval at step 3691: RAM USED (GB) 29.723267072
I0401 02:23:57.511589 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:24:11.189177 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.332878 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.333985 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.339433 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.339700 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.339823 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.340332 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:11.340851 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.273113 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.416419 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.417956 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.423911 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.423925 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.424373 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.425319 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:25.425315 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:24:51.669835 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:24:51.993278 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.150777 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.152247 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.158191 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.158213 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.158984 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.159681 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.159986 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.344858 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.492750 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.494345 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.498956 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.499750 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.499867 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.500039 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:52.500459 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:24:54.887506 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:24:55.223545 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.380715 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.382150 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.388630 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.388670 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.389268 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.389361 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.389837 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.576426 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.732785 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.734532 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.738791 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.739087 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.739365 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.739431 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:24:55.739844 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:24:58.024730 140525275027264 submission_runner.py:380] Time since start: 1285.07s, 	Step: 3691, 	{'train/accuracy': 0.9876461804121097, 'train/loss': 0.04381176829338074, 'train/mean_average_precision': 0.1548892628449574, 'validation/accuracy': 0.9849939961370683, 'validation/loss': 0.052919477224349976, 'validation/mean_average_precision': 0.14453855536861987, 'validation/num_examples': 43793, 'test/accuracy': 0.9840771561271823, 'test/loss': 0.05580686405301094, 'test/mean_average_precision': 0.14703941893768646, 'test/num_examples': 43793}
I0401 02:24:58.025164 140525275027264 submission_runner.py:390] After eval at step 3691: RAM USED (GB) 30.10048
I0401 02:24:58.033349 140473322243840 logging_writer.py:48] [3691] global_step=3691, preemption_count=0, score=961.910362, test/accuracy=0.984077, test/loss=0.055807, test/mean_average_precision=0.147039, test/num_examples=43793, total_duration=1285.066101, train/accuracy=0.987646, train/loss=0.043812, train/mean_average_precision=0.154889, validation/accuracy=0.984994, validation/loss=0.052919, validation/mean_average_precision=0.144539, validation/num_examples=43793
I0401 02:24:58.122809 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_3691.
I0401 02:24:58.123289 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 3691: RAM USED (GB) 30.10002944
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0401 02:26:21.338434 140473313851136 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.030665, loss=0.050044
I0401 02:26:21.344465 140525275027264 submission.py:119] 4000) loss = 0.050, grad_norm = 0.031
I0401 02:28:30.697551 140473322243840 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016604, loss=0.042176
I0401 02:28:30.703851 140525275027264 submission.py:119] 4500) loss = 0.042, grad_norm = 0.017
I0401 02:28:58.330925 140525275027264 submission_runner.py:371] Before eval at step 4608: RAM USED (GB) 30.404653056
I0401 02:28:58.331109 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:29:12.246678 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.394414 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.396138 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.402081 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.402320 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.402487 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.402870 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:12.403377 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.100380 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.247308 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.249812 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.253987 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.254914 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.254949 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.255882 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:26.255920 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:29:51.587468 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:29:51.915349 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.088625 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.091404 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.096536 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.096891 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.097153 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.098309 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.098981 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.256489 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.417343 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.419045 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.422451 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.422771 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.424636 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.424977 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:52.425945 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:29:54.679266 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:29:55.008174 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.174413 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.175851 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.182106 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.183433 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.183530 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.183608 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.184461 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.340599 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.500192 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.502611 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.505923 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.506507 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.507130 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.508332 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:29:55.508309 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:29:57.702625 140525275027264 submission_runner.py:380] Time since start: 1585.89s, 	Step: 4608, 	{'train/accuracy': 0.9884154645251283, 'train/loss': 0.040413450449705124, 'train/mean_average_precision': 0.20896140744931646, 'validation/accuracy': 0.985356095240183, 'validation/loss': 0.04995374754071236, 'validation/mean_average_precision': 0.1799355765996078, 'validation/num_examples': 43793, 'test/accuracy': 0.984471815547054, 'test/loss': 0.052807070314884186, 'test/mean_average_precision': 0.1802063248325041, 'test/num_examples': 43793}
I0401 02:29:57.702992 140525275027264 submission_runner.py:390] After eval at step 4608: RAM USED (GB) 31.177158656
I0401 02:29:57.710907 140473313851136 logging_writer.py:48] [4608] global_step=4608, preemption_count=0, score=1201.135493, test/accuracy=0.984472, test/loss=0.052807, test/mean_average_precision=0.180206, test/num_examples=43793, total_duration=1585.885854, train/accuracy=0.988415, train/loss=0.040413, train/mean_average_precision=0.208961, validation/accuracy=0.985356, validation/loss=0.049954, validation/mean_average_precision=0.179936, validation/num_examples=43793
I0401 02:29:57.802794 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_4608.
I0401 02:29:57.803273 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 4608: RAM USED (GB) 31.077433344
I0401 02:31:39.314777 140473322243840 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014689, loss=0.039338
I0401 02:31:39.320908 140525275027264 submission.py:119] 5000) loss = 0.039, grad_norm = 0.015
I0401 02:33:48.062799 140473313851136 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.008979, loss=0.037014
I0401 02:33:48.069952 140525275027264 submission.py:119] 5500) loss = 0.037, grad_norm = 0.009
I0401 02:33:58.004085 140525275027264 submission_runner.py:371] Before eval at step 5539: RAM USED (GB) 31.030509568
I0401 02:33:58.004301 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:34:12.134204 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.278729 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.279669 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.285356 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.285470 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.286195 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.286304 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:12.286623 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.491884 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.643627 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.645171 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.650115 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.650470 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.650558 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.650995 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:26.652141 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:34:53.134597 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:34:53.467331 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.630717 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.633104 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.638529 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.638694 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.640777 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.640912 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.641068 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.808054 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.966402 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.967985 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.972398 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.973444 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.974004 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.974415 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:53.974534 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:34:56.300586 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:34:56.628407 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.790894 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.792186 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.798506 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.799259 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.799705 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.799979 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.799990 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:56.966901 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.126607 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.128485 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.132050 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.133220 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.133887 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.134042 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:34:57.134253 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:34:59.381251 140525275027264 submission_runner.py:380] Time since start: 1885.56s, 	Step: 5539, 	{'train/accuracy': 0.9887237548587513, 'train/loss': 0.03869689628481865, 'train/mean_average_precision': 0.22823472952540813, 'validation/accuracy': 0.9858050656527891, 'validation/loss': 0.04821711778640747, 'validation/mean_average_precision': 0.19602886851268175, 'validation/num_examples': 43793, 'test/accuracy': 0.9849785127531433, 'test/loss': 0.05084436386823654, 'test/mean_average_precision': 0.19671039147341404, 'test/num_examples': 43793}
I0401 02:34:59.381622 140525275027264 submission_runner.py:390] After eval at step 5539: RAM USED (GB) 31.380975616
I0401 02:34:59.389907 140473322243840 logging_writer.py:48] [5539] global_step=5539, preemption_count=0, score=1440.278960, test/accuracy=0.984979, test/loss=0.050844, test/mean_average_precision=0.196710, test/num_examples=43793, total_duration=1885.558744, train/accuracy=0.988724, train/loss=0.038697, train/mean_average_precision=0.228235, validation/accuracy=0.985805, validation/loss=0.048217, validation/mean_average_precision=0.196029, validation/num_examples=43793
I0401 02:34:59.483103 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_5539.
I0401 02:34:59.483677 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 5539: RAM USED (GB) 31.381041152
I0401 02:36:58.714644 140525275027264 submission_runner.py:371] Before eval at step 6000: RAM USED (GB) 31.387070464
I0401 02:36:58.714899 140525275027264 spec.py:298] Evaluating on the training split.
W0401 02:37:12.845817 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.989893 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.990952 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.993774 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.995888 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.996403 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.996786 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:12.997524 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.150306 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.303225 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.304439 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.308546 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.309426 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.309465 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.309487 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:27.309952 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:37:53.824872 140525275027264 spec.py:310] Evaluating on the validation split.
W0401 02:37:54.165687 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.316928 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.318385 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.323863 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.325076 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.325316 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.325702 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.325957 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.509231 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.669953 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.671047 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.674324 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.675162 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.675992 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.676153 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:54.677225 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:37:57.026922 140525275027264 spec.py:326] Evaluating on the test split.
W0401 02:37:57.357147 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.510621 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.511648 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.518710 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.519141 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.519463 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.519819 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.520221 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.695917 140525275027264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.844484 140385046226752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.845863 139990385076032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.850557 140177751729984 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.850588 140637496436544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.851587 140694017992512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.851599 139876441626432 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0401 02:37:57.851693 140558380787520 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0401 02:38:00.154419 140525275027264 submission_runner.py:380] Time since start: 2066.27s, 	Step: 6000, 	{'train/accuracy': 0.9888416747462792, 'train/loss': 0.03780388459563255, 'train/mean_average_precision': 0.2629860474695337, 'validation/accuracy': 0.9859305013286439, 'validation/loss': 0.047705333679914474, 'validation/mean_average_precision': 0.20695320061331077, 'validation/num_examples': 43793, 'test/accuracy': 0.9850711755838495, 'test/loss': 0.05040833353996277, 'test/mean_average_precision': 0.20605791217575467, 'test/num_examples': 43793}
I0401 02:38:00.154905 140525275027264 submission_runner.py:390] After eval at step 6000: RAM USED (GB) 31.864639488
I0401 02:38:00.163366 140473313851136 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1559.008449, test/accuracy=0.985071, test/loss=0.050408, test/mean_average_precision=0.206058, test/num_examples=43793, total_duration=2066.269329, train/accuracy=0.988842, train/loss=0.037804, train/mean_average_precision=0.262986, validation/accuracy=0.985931, validation/loss=0.047705, validation/mean_average_precision=0.206953, validation/num_examples=43793
I0401 02:38:00.255052 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0401 02:38:00.255554 140525275027264 submission_runner.py:409] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.864705024
I0401 02:38:00.263049 140473322243840 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1559.008449
I0401 02:38:00.423263 140525275027264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0401 02:38:00.599112 140525275027264 submission_runner.py:543] Tuning trial 1/1
I0401 02:38:00.599336 140525275027264 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 02:38:00.600506 140525275027264 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5816566196704617, 'train/loss': 0.7269815802574158, 'train/mean_average_precision': 0.0234192588720906, 'validation/accuracy': 0.5732028802304444, 'validation/loss': 0.7356288433074951, 'validation/mean_average_precision': 0.027120670980945116, 'validation/num_examples': 43793, 'test/accuracy': 0.5694598473000789, 'test/loss': 0.7382012605667114, 'test/mean_average_precision': 0.02818801717248282, 'test/num_examples': 43793, 'score': 5.45268702507019, 'total_duration': 5.4546403884887695, 'global_step': 1, 'preemption_count': 0}), (927, {'train/accuracy': 0.9864393528250182, 'train/loss': 0.0525132454931736, 'train/mean_average_precision': 0.05636010676782553, 'validation/accuracy': 0.9836101442956807, 'validation/loss': 0.061678990721702576, 'validation/mean_average_precision': 0.05674771535115736, 'validation/num_examples': 43793, 'test/accuracy': 0.9825343199959229, 'test/loss': 0.06492893397808075, 'test/mean_average_precision': 0.05546132895541424, 'test/num_examples': 43793, 'score': 244.47913432121277, 'total_duration': 384.47425413131714, 'global_step': 927, 'preemption_count': 0}), (1856, {'train/accuracy': 0.9871421229806356, 'train/loss': 0.04841460660099983, 'train/mean_average_precision': 0.09060277106775994, 'validation/accuracy': 0.9844593722370661, 'validation/loss': 0.058164551854133606, 'validation/mean_average_precision': 0.08906351552868715, 'validation/num_examples': 43793, 'test/accuracy': 0.983482850426607, 'test/loss': 0.06137758493423462, 'test/mean_average_precision': 0.08937046016132462, 'test/num_examples': 43793, 'score': 483.60490918159485, 'total_duration': 683.4528841972351, 'global_step': 1856, 'preemption_count': 0}), (2779, {'train/accuracy': 0.9879097508566804, 'train/loss': 0.043474845588207245, 'train/mean_average_precision': 0.1518303489940671, 'validation/accuracy': 0.9849818179161115, 'validation/loss': 0.052732788026332855, 'validation/mean_average_precision': 0.1368441009503704, 'validation/num_examples': 43793, 'test/accuracy': 0.9839946019689166, 'test/loss': 0.05561356246471405, 'test/mean_average_precision': 0.13791653531525844, 'test/num_examples': 43793, 'score': 722.847009897232, 'total_duration': 983.8940343856812, 'global_step': 2779, 'preemption_count': 0}), (3691, {'train/accuracy': 0.9876461804121097, 'train/loss': 0.04381176829338074, 'train/mean_average_precision': 0.1548892628449574, 'validation/accuracy': 0.9849939961370683, 'validation/loss': 0.052919477224349976, 'validation/mean_average_precision': 0.14453855536861987, 'validation/num_examples': 43793, 'test/accuracy': 0.9840771561271823, 'test/loss': 0.05580686405301094, 'test/mean_average_precision': 0.14703941893768646, 'test/num_examples': 43793, 'score': 961.9103617668152, 'total_duration': 1285.0661013126373, 'global_step': 3691, 'preemption_count': 0}), (4608, {'train/accuracy': 0.9884154645251283, 'train/loss': 0.040413450449705124, 'train/mean_average_precision': 0.20896140744931646, 'validation/accuracy': 0.985356095240183, 'validation/loss': 0.04995374754071236, 'validation/mean_average_precision': 0.1799355765996078, 'validation/num_examples': 43793, 'test/accuracy': 0.984471815547054, 'test/loss': 0.052807070314884186, 'test/mean_average_precision': 0.1802063248325041, 'test/num_examples': 43793, 'score': 1201.1354932785034, 'total_duration': 1585.8858544826508, 'global_step': 4608, 'preemption_count': 0}), (5539, {'train/accuracy': 0.9887237548587513, 'train/loss': 0.03869689628481865, 'train/mean_average_precision': 0.22823472952540813, 'validation/accuracy': 0.9858050656527891, 'validation/loss': 0.04821711778640747, 'validation/mean_average_precision': 0.19602886851268175, 'validation/num_examples': 43793, 'test/accuracy': 0.9849785127531433, 'test/loss': 0.05084436386823654, 'test/mean_average_precision': 0.19671039147341404, 'test/num_examples': 43793, 'score': 1440.2789597511292, 'total_duration': 1885.5587437152863, 'global_step': 5539, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9888416747462792, 'train/loss': 0.03780388459563255, 'train/mean_average_precision': 0.2629860474695337, 'validation/accuracy': 0.9859305013286439, 'validation/loss': 0.047705333679914474, 'validation/mean_average_precision': 0.20695320061331077, 'validation/num_examples': 43793, 'test/accuracy': 0.9850711755838495, 'test/loss': 0.05040833353996277, 'test/mean_average_precision': 0.20605791217575467, 'test/num_examples': 43793, 'score': 1559.0084490776062, 'total_duration': 2066.2693293094635, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0401 02:38:00.600629 140525275027264 submission_runner.py:546] Timing: 1559.0084490776062
I0401 02:38:00.600675 140525275027264 submission_runner.py:547] ====================
I0401 02:38:00.600766 140525275027264 submission_runner.py:606] Final ogbg score: 1559.0084490776062
