I0401 00:32:48.197712 139914191025984 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax.
I0401 00:32:48.247217 139914191025984 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0401 00:32:49.146231 139914191025984 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0401 00:32:49.147539 139914191025984 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0401 00:32:49.151004 139914191025984 submission_runner.py:511] Using RNG seed 1099810609
I0401 00:32:50.479109 139914191025984 submission_runner.py:520] --- Tuning run 1/1 ---
I0401 00:32:50.479293 139914191025984 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1.
I0401 00:32:50.479467 139914191025984 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/hparams.json.
I0401 00:32:50.605032 139914191025984 submission_runner.py:230] Starting train once: RAM USED (GB) 4.39422976
I0401 00:32:50.605191 139914191025984 submission_runner.py:231] Initializing dataset.
I0401 00:32:50.605349 139914191025984 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.39422976
I0401 00:32:50.605406 139914191025984 submission_runner.py:240] Initializing model.
I0401 00:33:06.064251 139914191025984 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.988123136
I0401 00:33:06.064448 139914191025984 submission_runner.py:252] Initializing optimizer.
I0401 00:33:06.613330 139914191025984 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.989253632
I0401 00:33:06.613512 139914191025984 submission_runner.py:261] Initializing metrics bundle.
I0401 00:33:06.613567 139914191025984 submission_runner.py:276] Initializing checkpoint and logger.
I0401 00:33:06.614435 139914191025984 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0401 00:33:06.614692 139914191025984 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 00:33:06.614753 139914191025984 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 00:33:07.674627 139914191025984 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0401 00:33:07.676776 139914191025984 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0401 00:33:07.680277 139914191025984 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.987041792
I0401 00:33:07.680469 139914191025984 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.987041792
I0401 00:33:07.680528 139914191025984 submission_runner.py:313] Starting training loop.
I0401 00:33:07.884133 139914191025984 input_pipeline.py:20] Loading split = train-clean-100
I0401 00:33:07.916776 139914191025984 input_pipeline.py:20] Loading split = train-clean-360
I0401 00:33:08.307274 139914191025984 input_pipeline.py:20] Loading split = train-other-500
I0401 00:33:12.817002 139914191025984 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 10.959990784
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0401 00:34:01.400270 139737532724992 logging_writer.py:48] [0] global_step=0, grad_norm=19.402374267578125, loss=33.69783020019531
I0401 00:34:01.412655 139914191025984 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 14.171918336
I0401 00:34:01.412919 139914191025984 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 14.171918336
I0401 00:34:01.413007 139914191025984 spec.py:298] Evaluating on the training split.
I0401 00:34:01.545475 139914191025984 input_pipeline.py:20] Loading split = train-clean-100
I0401 00:34:01.571336 139914191025984 input_pipeline.py:20] Loading split = train-clean-360
I0401 00:34:01.858799 139914191025984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0401 00:35:32.310100 139914191025984 spec.py:310] Evaluating on the validation split.
I0401 00:35:32.409900 139914191025984 input_pipeline.py:20] Loading split = dev-clean
I0401 00:35:32.414984 139914191025984 input_pipeline.py:20] Loading split = dev-other
I0401 00:36:26.908712 139914191025984 spec.py:326] Evaluating on the test split.
I0401 00:36:27.010979 139914191025984 input_pipeline.py:20] Loading split = test-clean
I0401 00:37:01.720882 139914191025984 submission_runner.py:382] Time since start: 53.73s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.513237, dtype=float32), 'train/wer': 4.527289007350644, 'validation/ctc_loss': DeviceArray(30.168247, dtype=float32), 'validation/wer': 4.0306708217155975, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.454542, dtype=float32), 'test/wer': 4.369650437714541, 'test/num_examples': 2472}
I0401 00:37:01.722194 139914191025984 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.306228736
I0401 00:37:01.734918 139735502673664 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=53.528765, test/ctc_loss=30.45454216003418, test/num_examples=2472, test/wer=4.369650, total_duration=53.732424, train/ctc_loss=31.51323699951172, train/wer=4.527289, validation/ctc_loss=30.16824722290039, validation/num_examples=5348, validation/wer=4.030671
I0401 00:37:01.822165 139914191025984 checkpoints.py:356] Saving checkpoint at step: 1
I0401 00:37:02.104585 139914191025984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0401 00:37:02.105358 139914191025984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0401 00:37:02.109258 139914191025984 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.264994304
I0401 00:37:02.157385 139914191025984 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.266415616
I0401 00:37:18.853001 139914191025984 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.710299136
I0401 00:39:12.693803 139739656177408 logging_writer.py:48] [100] global_step=100, grad_norm=5.237105846405029, loss=6.166214942932129
I0401 00:41:07.636224 139739664570112 logging_writer.py:48] [200] global_step=200, grad_norm=1.5796558856964111, loss=5.947171688079834
I0401 00:43:03.270060 139739656177408 logging_writer.py:48] [300] global_step=300, grad_norm=0.47427359223365784, loss=5.874634265899658
I0401 00:44:59.155587 139739664570112 logging_writer.py:48] [400] global_step=400, grad_norm=1.6223849058151245, loss=5.822531700134277
I0401 00:46:54.361619 139739656177408 logging_writer.py:48] [500] global_step=500, grad_norm=2.618593692779541, loss=5.743049621582031
I0401 00:48:49.811755 139739664570112 logging_writer.py:48] [600] global_step=600, grad_norm=6.731450080871582, loss=6.131474494934082
I0401 00:50:45.459589 139739656177408 logging_writer.py:48] [700] global_step=700, grad_norm=2.0547518730163574, loss=5.149786472320557
I0401 00:52:40.742083 139739664570112 logging_writer.py:48] [800] global_step=800, grad_norm=1.991042971611023, loss=4.753731727600098
I0401 00:54:35.878852 139739656177408 logging_writer.py:48] [900] global_step=900, grad_norm=2.1444244384765625, loss=4.509334564208984
I0401 00:56:30.803036 139739664570112 logging_writer.py:48] [1000] global_step=1000, grad_norm=37.787391662597656, loss=215.351806640625
I0401 00:58:28.621113 139739798853376 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1853.4403076171875
I0401 01:00:23.094830 139739790460672 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1854.91748046875
I0401 01:02:17.883008 139739798853376 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1789.814453125
I0401 01:04:12.317173 139739790460672 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1792.195068359375
I0401 01:06:06.949136 139739798853376 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1814.296142578125
I0401 01:08:01.733465 139739790460672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1814.5531005859375
I0401 01:09:55.900728 139739798853376 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1808.6558837890625
I0401 01:11:50.531300 139739790460672 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1851.0279541015625
I0401 01:13:44.585900 139739798853376 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1748.136962890625
I0401 01:15:38.766526 139739790460672 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1770.50390625
I0401 01:17:02.441277 139914191025984 submission_runner.py:373] Before eval at step 2071: RAM USED (GB) 23.79823104
I0401 01:17:02.441713 139914191025984 spec.py:298] Evaluating on the training split.
I0401 01:17:30.701922 139914191025984 spec.py:310] Evaluating on the validation split.
I0401 01:18:05.374685 139914191025984 spec.py:326] Evaluating on the test split.
I0401 01:18:24.112342 139914191025984 submission_runner.py:382] Time since start: 2634.76s, 	Step: 2071, 	{'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 01:18:24.113790 139914191025984 submission_runner.py:396] After eval at step 2071: RAM USED (GB) 22.17957376
I0401 01:18:24.138231 139740024133376 logging_writer.py:48] [2071] global_step=2071, preemption_count=0, score=2450.273003, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2634.759623, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0401 01:18:24.244783 139914191025984 checkpoints.py:356] Saving checkpoint at step: 2071
I0401 01:18:24.901947 139914191025984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_2071
I0401 01:18:24.912316 139914191025984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_2071.
I0401 01:18:24.915914 139914191025984 submission_runner.py:416] After logging and checkpointing eval at step 2071: RAM USED (GB) 22.151528448
I0401 01:18:59.440830 139740015740672 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1851.0279541015625
I0401 01:20:54.362456 139739973777152 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1830.774169921875
I0401 01:22:49.171364 139740015740672 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1829.3350830078125
I0401 01:24:43.946013 139739973777152 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1860.85009765625
I0401 01:26:39.032095 139740015740672 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1874.7503662109375
I0401 01:28:34.129247 139739973777152 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1876.5367431640625
I0401 01:30:29.599462 139740015740672 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1889.1385498046875
I0401 01:32:24.842068 139739973777152 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1800.2613525390625
I0401 01:34:19.824148 139740015740672 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1752.56494140625
I0401 01:36:14.174586 139739973777152 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1802.2891845703125
I0401 01:38:11.799398 139739368773376 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1834.71044921875
I0401 01:40:06.025370 139739360380672 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1820.747802734375
I0401 01:42:00.276205 139739368773376 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1827.5069580078125
I0401 01:43:54.792718 139739360380672 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1869.2747802734375
I0401 01:45:49.439818 139739368773376 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1860.985595703125
I0401 01:47:44.043660 139739360380672 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1879.9820556640625
I0401 01:49:38.687658 139739368773376 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1852.099365234375
I0401 01:51:33.780134 139739360380672 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1808.4002685546875
I0401 01:53:28.841642 139739368773376 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1812.2410888671875
I0401 01:55:24.416367 139739360380672 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1833.2652587890625
I0401 01:57:18.945835 139739368773376 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1843.1649169921875
I0401 01:58:25.064535 139914191025984 submission_runner.py:373] Before eval at step 4156: RAM USED (GB) 23.589552128
I0401 01:58:25.065042 139914191025984 spec.py:298] Evaluating on the training split.
I0401 01:58:53.515763 139914191025984 spec.py:310] Evaluating on the validation split.
I0401 01:59:28.281397 139914191025984 spec.py:326] Evaluating on the test split.
I0401 01:59:46.094609 139914191025984 submission_runner.py:382] Time since start: 5117.38s, 	Step: 4156, 	{'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 01:59:46.096008 139914191025984 submission_runner.py:396] After eval at step 4156: RAM USED (GB) 20.455374848
I0401 01:59:46.118403 139739368773376 logging_writer.py:48] [4156] global_step=4156, preemption_count=0, score=4846.712707, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5117.381645, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0401 01:59:46.219448 139914191025984 checkpoints.py:356] Saving checkpoint at step: 4156
I0401 01:59:46.624138 139914191025984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_4156
I0401 01:59:46.634625 139914191025984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_4156.
I0401 01:59:46.638098 139914191025984 submission_runner.py:416] After logging and checkpointing eval at step 4156: RAM USED (GB) 20.426878976
I0401 02:00:38.178812 139739360380672 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1826.9852294921875
I0401 02:02:32.359889 139739310024448 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1778.4990234375
I0401 02:04:26.827866 139739360380672 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1809.0394287109375
I0401 02:06:20.991310 139739310024448 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1802.7967529296875
I0401 02:08:15.504651 139739360380672 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1837.34423828125
I0401 02:10:09.667717 139739310024448 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1853.8428955078125
I0401 02:12:03.981444 139739360380672 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1824.6412353515625
I0401 02:13:57.913255 139739310024448 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1822.822265625
I0401 02:15:52.059039 139739360380672 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1779.9830322265625
I0401 02:17:46.632390 139739310024448 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1831.298095703125
I0401 02:19:45.070887 139740024133376 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1850.89404296875
I0401 02:21:39.415856 139740015740672 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1877.637939453125
I0401 02:23:33.542939 139740024133376 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1857.7442626953125
I0401 02:25:28.154659 139740015740672 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1748.614501953125
I0401 02:27:23.173373 139740024133376 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1800.2613525390625
I0401 02:29:17.595801 139740015740672 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1794.330810546875
I0401 02:31:13.043450 139740024133376 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1891.3717041015625
I0401 02:33:07.619523 139740015740672 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1834.5789794921875
I0401 02:35:02.103768 139740024133376 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1765.2542724609375
I0401 02:36:56.724695 139740015740672 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1777.6346435546875
I0401 02:38:55.032441 139739368773376 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1839.45654296875
I0401 02:39:46.714733 139914191025984 submission_runner.py:373] Before eval at step 6246: RAM USED (GB) 21.533609984
I0401 02:39:46.714966 139914191025984 spec.py:298] Evaluating on the training split.
I0401 02:40:15.326396 139914191025984 spec.py:310] Evaluating on the validation split.
I0401 02:40:50.692896 139914191025984 spec.py:326] Evaluating on the test split.
I0401 02:41:08.404395 139914191025984 submission_runner.py:382] Time since start: 7599.03s, 	Step: 6246, 	{'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 02:41:08.405847 139914191025984 submission_runner.py:396] After eval at step 6246: RAM USED (GB) 20.535197696
I0401 02:41:08.424751 139739368773376 logging_writer.py:48] [6246] global_step=6246, preemption_count=0, score=7243.157049, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7599.032035, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0401 02:41:08.526712 139914191025984 checkpoints.py:356] Saving checkpoint at step: 6246
I0401 02:41:08.938535 139914191025984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_6246
I0401 02:41:08.948961 139914191025984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_6246.
I0401 02:41:08.952577 139914191025984 submission_runner.py:416] After logging and checkpointing eval at step 6246: RAM USED (GB) 20.507049984
I0401 02:42:11.942509 139739360380672 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1836.0263671875
I0401 02:44:06.187468 139739301631744 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1796.2191162109375
I0401 02:46:01.128635 139739360380672 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1932.92919921875
I0401 02:47:56.105886 139739301631744 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1857.8790283203125
I0401 02:49:50.725095 139739360380672 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1856.935791015625
I0401 02:51:45.744246 139739301631744 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1763.9161376953125
I0401 02:53:39.959763 139739360380672 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1909.8565673828125
I0401 02:55:34.514425 139739301631744 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1821.0069580078125
I0401 02:57:28.916331 139739360380672 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1864.102294921875
I0401 02:59:23.302101 139739301631744 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1759.3096923828125
I0401 03:01:21.506974 139739368773376 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1790.5655517578125
I0401 03:03:15.828571 139739360380672 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1748.8533935546875
I0401 03:05:09.727320 139739368773376 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1802.669921875
I0401 03:07:04.210524 139739360380672 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1827.6373291015625
I0401 03:08:58.911748 139739368773376 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1827.5069580078125
I0401 03:10:53.726751 139739360380672 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1734.9859619140625
I0401 03:12:48.341078 139739368773376 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1814.681884765625
I0401 03:14:41.186606 139914191025984 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 21.550518272
I0401 03:14:41.186837 139914191025984 spec.py:298] Evaluating on the training split.
I0401 03:15:10.259035 139914191025984 spec.py:310] Evaluating on the validation split.
I0401 03:15:44.675341 139914191025984 spec.py:326] Evaluating on the test split.
I0401 03:16:03.318380 139914191025984 submission_runner.py:382] Time since start: 9693.50s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 03:16:03.319749 139914191025984 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 20.557504512
I0401 03:16:03.337769 139740454213376 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9252.336047, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=9693.501724, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0401 03:16:03.441345 139914191025984 checkpoints.py:356] Saving checkpoint at step: 8000
I0401 03:16:03.850226 139914191025984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0401 03:16:03.860802 139914191025984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0401 03:16:03.864452 139914191025984 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 20.52765696
I0401 03:16:03.871258 139740445820672 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9252.336047
I0401 03:16:03.940541 139914191025984 checkpoints.py:356] Saving checkpoint at step: 8000
I0401 03:16:04.454591 139914191025984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0401 03:16:04.465018 139914191025984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0401 03:16:05.726593 139914191025984 submission_runner.py:550] Tuning trial 1/1
I0401 03:16:05.726847 139914191025984 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 03:16:05.730715 139914191025984 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.513237, dtype=float32), 'train/wer': 4.527289007350644, 'validation/ctc_loss': DeviceArray(30.168247, dtype=float32), 'validation/wer': 4.0306708217155975, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.454542, dtype=float32), 'test/wer': 4.369650437714541, 'test/num_examples': 2472, 'score': 53.528764963150024, 'total_duration': 53.73242449760437, 'global_step': 1, 'preemption_count': 0}), (2071, {'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2450.2730028629303, 'total_duration': 2634.7596225738525, 'global_step': 2071, 'preemption_count': 0}), (4156, {'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4846.712707281113, 'total_duration': 5117.3816447258, 'global_step': 4156, 'preemption_count': 0}), (6246, {'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7243.157049179077, 'total_duration': 7599.032034635544, 'global_step': 6246, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9252.33604669571, 'total_duration': 9693.501724481583, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0401 03:16:05.730908 139914191025984 submission_runner.py:553] Timing: 9252.33604669571
I0401 03:16:05.730960 139914191025984 submission_runner.py:554] ====================
I0401 03:16:05.731388 139914191025984 submission_runner.py:613] Final librispeech_deepspeech score: 9252.33604669571
