WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 11:15:13.856092 140297230411584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 11:15:13.856106 140494285207360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 11:15:13.856126 140674426746688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 11:15:13.856144 139938146621248 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 11:15:13.856828 139744263759680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 11:15:13.856863 139778711578432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 11:15:13.857155 139744263759680 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.857007 140689295501120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 11:15:13.857186 139778711578432 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.857112 140001494234944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 11:15:13.857344 140689295501120 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.857460 140001494234944 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.866772 140297230411584 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.866806 140674426746688 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.866797 140494285207360 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:13.866830 139938146621248 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:15:14.391631 140001494234944 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/librispeech_conformer_pytorch.
W0402 11:15:14.490979 139744263759680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.490979 140674426746688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.491014 139778711578432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.491662 140001494234944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.491917 140689295501120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.492113 140297230411584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.492236 139938146621248 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:15:14.492631 140494285207360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 11:15:14.497080 140001494234944 submission_runner.py:511] Using RNG seed 3659889383
I0402 11:15:14.498118 140001494234944 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 11:15:14.498238 140001494234944 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1.
I0402 11:15:14.498470 140001494234944 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/hparams.json.
I0402 11:15:14.499443 140001494234944 submission_runner.py:230] Starting train once: RAM USED (GB) 5.73956096
I0402 11:15:14.499566 140001494234944 submission_runner.py:231] Initializing dataset.
I0402 11:15:14.499646 140001494234944 input_pipeline.py:20] Loading split = train-clean-100
I0402 11:15:14.527736 140001494234944 input_pipeline.py:20] Loading split = train-clean-360
I0402 11:15:14.858363 140001494234944 input_pipeline.py:20] Loading split = train-other-500
I0402 11:15:15.286332 140001494234944 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 6.078808064
I0402 11:15:15.286504 140001494234944 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0402 11:15:22.252074 140001494234944 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.507392512
I0402 11:15:22.252265 140001494234944 submission_runner.py:252] Initializing optimizer.
I0402 11:15:22.818328 140001494234944 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.244412928
I0402 11:15:22.818577 140001494234944 submission_runner.py:261] Initializing metrics bundle.
I0402 11:15:22.818627 140001494234944 submission_runner.py:276] Initializing checkpoint and logger.
I0402 11:15:22.819854 140001494234944 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 11:15:22.820012 140001494234944 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 11:15:23.583541 140001494234944 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0402 11:15:23.584599 140001494234944 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0402 11:15:23.588831 140001494234944 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.244552192
I0402 11:15:23.590008 140001494234944 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.244544
I0402 11:15:23.590104 140001494234944 submission_runner.py:313] Starting training loop.
I0402 11:15:25.334155 140001494234944 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 24.833703936
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0402 11:15:30.002507 139975148427008 logging_writer.py:48] [0] global_step=0, grad_norm=42.268002, loss=32.424763
I0402 11:15:30.016247 140001494234944 submission.py:139] 0) loss = 32.425, grad_norm = 42.268
I0402 11:15:30.016976 140001494234944 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.770699264
I0402 11:15:30.017569 140001494234944 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.770699264
I0402 11:15:30.017698 140001494234944 spec.py:298] Evaluating on the training split.
I0402 11:15:30.018518 140001494234944 input_pipeline.py:20] Loading split = train-clean-100
I0402 11:15:30.045526 140001494234944 input_pipeline.py:20] Loading split = train-clean-360
I0402 11:15:30.462035 140001494234944 input_pipeline.py:20] Loading split = train-other-500
I0402 11:15:42.814118 140001494234944 spec.py:310] Evaluating on the validation split.
I0402 11:15:42.815330 140001494234944 input_pipeline.py:20] Loading split = dev-clean
I0402 11:15:42.818933 140001494234944 input_pipeline.py:20] Loading split = dev-other
I0402 11:15:52.837891 140001494234944 spec.py:326] Evaluating on the test split.
I0402 11:15:52.842957 140001494234944 input_pipeline.py:20] Loading split = test-clean
I0402 11:15:58.180445 140001494234944 submission_runner.py:382] Time since start: 6.43s, 	Step: 1, 	{'train/ctc_loss': 31.432636781585863, 'train/wer': 1.3650862092197245, 'validation/ctc_loss': 30.214147578862768, 'validation/wer': 1.726210592381596, 'validation/num_examples': 5348, 'test/ctc_loss': 30.22735660365185, 'test/wer': 1.7166737757195376, 'test/num_examples': 2472}
I0402 11:15:58.181185 140001494234944 submission_runner.py:396] After eval at step 1: RAM USED (GB) 45.90053376
I0402 11:15:58.196884 139972615075584 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.426043, test/ctc_loss=30.227357, test/num_examples=2472, test/wer=1.716674, total_duration=6.428129, train/ctc_loss=31.432637, train/wer=1.365086, validation/ctc_loss=30.214148, validation/num_examples=5348, validation/wer=1.726211
I0402 11:15:58.600780 140001494234944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0402 11:15:58.601360 140001494234944 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.945946112
I0402 11:15:58.605290 140001494234944 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.950365696
I0402 11:15:58.650198 140001494234944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650295 139744263759680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650281 140674426746688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650277 140297230411584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650315 139938146621248 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650319 140494285207360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650277 139778711578432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:58.650315 140689295501120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:15:59.791034 139972606682880 logging_writer.py:48] [1] global_step=1, grad_norm=43.406475, loss=31.832228
I0402 11:15:59.794083 140001494234944 submission.py:139] 1) loss = 31.832, grad_norm = 43.406
I0402 11:15:59.794865 140001494234944 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 46.148919296
I0402 11:16:00.670236 139972615075584 logging_writer.py:48] [2] global_step=2, grad_norm=50.185223, loss=31.902502
I0402 11:16:00.673325 140001494234944 submission.py:139] 2) loss = 31.903, grad_norm = 50.185
I0402 11:16:01.687246 139972606682880 logging_writer.py:48] [3] global_step=3, grad_norm=57.850697, loss=30.885086
I0402 11:16:01.690264 140001494234944 submission.py:139] 3) loss = 30.885, grad_norm = 57.851
I0402 11:16:02.497710 139972615075584 logging_writer.py:48] [4] global_step=4, grad_norm=87.023918, loss=27.904400
I0402 11:16:02.500838 140001494234944 submission.py:139] 4) loss = 27.904, grad_norm = 87.024
I0402 11:16:03.307707 139972606682880 logging_writer.py:48] [5] global_step=5, grad_norm=149.452621, loss=19.403734
I0402 11:16:03.310802 140001494234944 submission.py:139] 5) loss = 19.404, grad_norm = 149.453
I0402 11:16:04.118194 139972615075584 logging_writer.py:48] [6] global_step=6, grad_norm=7.023153, loss=7.242607
I0402 11:16:04.121297 140001494234944 submission.py:139] 6) loss = 7.243, grad_norm = 7.023
I0402 11:16:04.926702 139972606682880 logging_writer.py:48] [7] global_step=7, grad_norm=26.506527, loss=8.484110
I0402 11:16:04.929734 140001494234944 submission.py:139] 7) loss = 8.484, grad_norm = 26.507
I0402 11:16:05.736548 139972615075584 logging_writer.py:48] [8] global_step=8, grad_norm=28.830067, loss=9.449808
I0402 11:16:05.739700 140001494234944 submission.py:139] 8) loss = 9.450, grad_norm = 28.830
I0402 11:16:06.540267 139972606682880 logging_writer.py:48] [9] global_step=9, grad_norm=29.633768, loss=9.579370
I0402 11:16:06.543800 140001494234944 submission.py:139] 9) loss = 9.579, grad_norm = 29.634
I0402 11:16:07.344252 139972615075584 logging_writer.py:48] [10] global_step=10, grad_norm=29.452940, loss=8.867127
I0402 11:16:07.347152 140001494234944 submission.py:139] 10) loss = 8.867, grad_norm = 29.453
I0402 11:16:08.147330 139972606682880 logging_writer.py:48] [11] global_step=11, grad_norm=21.283897, loss=7.532623
I0402 11:16:08.150789 140001494234944 submission.py:139] 11) loss = 7.533, grad_norm = 21.284
I0402 11:16:08.952027 139972615075584 logging_writer.py:48] [12] global_step=12, grad_norm=34.272018, loss=7.527534
I0402 11:16:08.955562 140001494234944 submission.py:139] 12) loss = 7.528, grad_norm = 34.272
I0402 11:16:09.759363 139972606682880 logging_writer.py:48] [13] global_step=13, grad_norm=28.197176, loss=7.337043
I0402 11:16:09.762655 140001494234944 submission.py:139] 13) loss = 7.337, grad_norm = 28.197
I0402 11:16:10.564063 139972615075584 logging_writer.py:48] [14] global_step=14, grad_norm=2.605062, loss=6.991628
I0402 11:16:10.567882 140001494234944 submission.py:139] 14) loss = 6.992, grad_norm = 2.605
I0402 11:16:11.369493 139972606682880 logging_writer.py:48] [15] global_step=15, grad_norm=2.600255, loss=6.937578
I0402 11:16:11.372578 140001494234944 submission.py:139] 15) loss = 6.938, grad_norm = 2.600
I0402 11:16:12.175268 139972615075584 logging_writer.py:48] [16] global_step=16, grad_norm=2.651490, loss=6.917329
I0402 11:16:12.178501 140001494234944 submission.py:139] 16) loss = 6.917, grad_norm = 2.651
I0402 11:16:12.980535 139972606682880 logging_writer.py:48] [17] global_step=17, grad_norm=2.604307, loss=6.883774
I0402 11:16:12.983959 140001494234944 submission.py:139] 17) loss = 6.884, grad_norm = 2.604
I0402 11:16:13.783082 139972615075584 logging_writer.py:48] [18] global_step=18, grad_norm=2.136980, loss=6.814305
I0402 11:16:13.786314 140001494234944 submission.py:139] 18) loss = 6.814, grad_norm = 2.137
I0402 11:16:14.583674 139972606682880 logging_writer.py:48] [19] global_step=19, grad_norm=2.130499, loss=6.773943
I0402 11:16:14.587056 140001494234944 submission.py:139] 19) loss = 6.774, grad_norm = 2.130
I0402 11:16:15.388353 139972615075584 logging_writer.py:48] [20] global_step=20, grad_norm=1.877759, loss=6.705493
I0402 11:16:15.391999 140001494234944 submission.py:139] 20) loss = 6.705, grad_norm = 1.878
I0402 11:16:16.192596 139972606682880 logging_writer.py:48] [21] global_step=21, grad_norm=1.799868, loss=6.671538
I0402 11:16:16.195969 140001494234944 submission.py:139] 21) loss = 6.672, grad_norm = 1.800
I0402 11:16:16.998601 139972615075584 logging_writer.py:48] [22] global_step=22, grad_norm=1.641596, loss=6.640093
I0402 11:16:17.001628 140001494234944 submission.py:139] 22) loss = 6.640, grad_norm = 1.642
I0402 11:16:17.802474 139972606682880 logging_writer.py:48] [23] global_step=23, grad_norm=2.129755, loss=6.595206
I0402 11:16:17.806187 140001494234944 submission.py:139] 23) loss = 6.595, grad_norm = 2.130
I0402 11:16:18.606037 139972615075584 logging_writer.py:48] [24] global_step=24, grad_norm=4.121600, loss=6.546822
I0402 11:16:18.609590 140001494234944 submission.py:139] 24) loss = 6.547, grad_norm = 4.122
I0402 11:16:19.409350 139972606682880 logging_writer.py:48] [25] global_step=25, grad_norm=7.892958, loss=6.564103
I0402 11:16:19.412714 140001494234944 submission.py:139] 25) loss = 6.564, grad_norm = 7.893
I0402 11:16:20.215195 139972615075584 logging_writer.py:48] [26] global_step=26, grad_norm=20.389524, loss=6.620289
I0402 11:16:20.218199 140001494234944 submission.py:139] 26) loss = 6.620, grad_norm = 20.390
I0402 11:16:21.017133 139972606682880 logging_writer.py:48] [27] global_step=27, grad_norm=30.239769, loss=7.344198
I0402 11:16:21.020283 140001494234944 submission.py:139] 27) loss = 7.344, grad_norm = 30.240
I0402 11:16:21.819211 139972615075584 logging_writer.py:48] [28] global_step=28, grad_norm=61.688511, loss=7.345409
I0402 11:16:21.822245 140001494234944 submission.py:139] 28) loss = 7.345, grad_norm = 61.689
I0402 11:16:22.622639 139972606682880 logging_writer.py:48] [29] global_step=29, grad_norm=33.686073, loss=11.061101
I0402 11:16:22.625812 140001494234944 submission.py:139] 29) loss = 11.061, grad_norm = 33.686
I0402 11:16:23.427855 139972615075584 logging_writer.py:48] [30] global_step=30, grad_norm=33.362762, loss=9.396036
I0402 11:16:23.430957 140001494234944 submission.py:139] 30) loss = 9.396, grad_norm = 33.363
I0402 11:16:24.236343 139972606682880 logging_writer.py:48] [31] global_step=31, grad_norm=6.238276, loss=6.344701
I0402 11:16:24.239507 140001494234944 submission.py:139] 31) loss = 6.345, grad_norm = 6.238
I0402 11:16:25.040539 139972615075584 logging_writer.py:48] [32] global_step=32, grad_norm=92.559074, loss=8.907665
I0402 11:16:25.043439 140001494234944 submission.py:139] 32) loss = 8.908, grad_norm = 92.559
I0402 11:16:25.842196 139972606682880 logging_writer.py:48] [33] global_step=33, grad_norm=29.599384, loss=13.229738
I0402 11:16:25.845326 140001494234944 submission.py:139] 33) loss = 13.230, grad_norm = 29.599
I0402 11:16:26.646398 139972615075584 logging_writer.py:48] [34] global_step=34, grad_norm=28.734180, loss=12.687232
I0402 11:16:26.649364 140001494234944 submission.py:139] 34) loss = 12.687, grad_norm = 28.734
I0402 11:16:27.451505 139972606682880 logging_writer.py:48] [35] global_step=35, grad_norm=28.128605, loss=10.351755
I0402 11:16:27.455185 140001494234944 submission.py:139] 35) loss = 10.352, grad_norm = 28.129
I0402 11:16:28.251804 139972615075584 logging_writer.py:48] [36] global_step=36, grad_norm=18.224422, loss=6.729365
I0402 11:16:28.254973 140001494234944 submission.py:139] 36) loss = 6.729, grad_norm = 18.224
I0402 11:16:29.053357 139972606682880 logging_writer.py:48] [37] global_step=37, grad_norm=99.205139, loss=12.026531
I0402 11:16:29.056373 140001494234944 submission.py:139] 37) loss = 12.027, grad_norm = 99.205
I0402 11:16:29.853197 139972615075584 logging_writer.py:48] [38] global_step=38, grad_norm=26.409910, loss=11.621090
I0402 11:16:29.856301 140001494234944 submission.py:139] 38) loss = 11.621, grad_norm = 26.410
I0402 11:16:30.658198 139972606682880 logging_writer.py:48] [39] global_step=39, grad_norm=25.937397, loss=10.932995
I0402 11:16:30.661259 140001494234944 submission.py:139] 39) loss = 10.933, grad_norm = 25.937
I0402 11:16:31.465633 139972615075584 logging_writer.py:48] [40] global_step=40, grad_norm=24.676258, loss=8.578899
I0402 11:16:31.468796 140001494234944 submission.py:139] 40) loss = 8.579, grad_norm = 24.676
I0402 11:16:32.270379 139972606682880 logging_writer.py:48] [41] global_step=41, grad_norm=9.559560, loss=6.306767
I0402 11:16:32.273802 140001494234944 submission.py:139] 41) loss = 6.307, grad_norm = 9.560
I0402 11:16:33.073786 139972615075584 logging_writer.py:48] [42] global_step=42, grad_norm=32.676674, loss=6.922062
I0402 11:16:33.076880 140001494234944 submission.py:139] 42) loss = 6.922, grad_norm = 32.677
I0402 11:16:33.875567 139972606682880 logging_writer.py:48] [43] global_step=43, grad_norm=20.342928, loss=7.156184
I0402 11:16:33.879206 140001494234944 submission.py:139] 43) loss = 7.156, grad_norm = 20.343
I0402 11:16:34.680765 139972615075584 logging_writer.py:48] [44] global_step=44, grad_norm=5.022313, loss=6.206570
I0402 11:16:34.683988 140001494234944 submission.py:139] 44) loss = 6.207, grad_norm = 5.022
I0402 11:16:35.482930 139972606682880 logging_writer.py:48] [45] global_step=45, grad_norm=3.261992, loss=6.181261
I0402 11:16:35.486079 140001494234944 submission.py:139] 45) loss = 6.181, grad_norm = 3.262
I0402 11:16:36.286542 139972615075584 logging_writer.py:48] [46] global_step=46, grad_norm=0.926636, loss=6.149334
I0402 11:16:36.289741 140001494234944 submission.py:139] 46) loss = 6.149, grad_norm = 0.927
I0402 11:16:37.091517 139972606682880 logging_writer.py:48] [47] global_step=47, grad_norm=2.786218, loss=6.144542
I0402 11:16:37.094621 140001494234944 submission.py:139] 47) loss = 6.145, grad_norm = 2.786
I0402 11:16:37.895641 139972615075584 logging_writer.py:48] [48] global_step=48, grad_norm=2.559094, loss=6.159065
I0402 11:16:37.898675 140001494234944 submission.py:139] 48) loss = 6.159, grad_norm = 2.559
I0402 11:16:38.702262 139972606682880 logging_writer.py:48] [49] global_step=49, grad_norm=3.358509, loss=6.119841
I0402 11:16:38.705379 140001494234944 submission.py:139] 49) loss = 6.120, grad_norm = 3.359
I0402 11:16:39.507251 139972615075584 logging_writer.py:48] [50] global_step=50, grad_norm=4.758048, loss=6.140819
I0402 11:16:39.510317 140001494234944 submission.py:139] 50) loss = 6.141, grad_norm = 4.758
I0402 11:16:40.311140 139972606682880 logging_writer.py:48] [51] global_step=51, grad_norm=6.959823, loss=6.132957
I0402 11:16:40.314322 140001494234944 submission.py:139] 51) loss = 6.133, grad_norm = 6.960
I0402 11:16:41.115140 139972615075584 logging_writer.py:48] [52] global_step=52, grad_norm=9.134079, loss=6.192754
I0402 11:16:41.118914 140001494234944 submission.py:139] 52) loss = 6.193, grad_norm = 9.134
I0402 11:16:41.920047 139972606682880 logging_writer.py:48] [53] global_step=53, grad_norm=14.081534, loss=6.250466
I0402 11:16:41.923480 140001494234944 submission.py:139] 53) loss = 6.250, grad_norm = 14.082
I0402 11:16:42.721586 139972615075584 logging_writer.py:48] [54] global_step=54, grad_norm=16.040571, loss=6.569623
I0402 11:16:42.724853 140001494234944 submission.py:139] 54) loss = 6.570, grad_norm = 16.041
I0402 11:16:43.523470 139972606682880 logging_writer.py:48] [55] global_step=55, grad_norm=17.222557, loss=6.311739
I0402 11:16:43.527133 140001494234944 submission.py:139] 55) loss = 6.312, grad_norm = 17.223
I0402 11:16:44.327928 139972615075584 logging_writer.py:48] [56] global_step=56, grad_norm=18.205765, loss=6.767690
I0402 11:16:44.330826 140001494234944 submission.py:139] 56) loss = 6.768, grad_norm = 18.206
I0402 11:16:45.132179 139972606682880 logging_writer.py:48] [57] global_step=57, grad_norm=17.358313, loss=6.297445
I0402 11:16:45.136009 140001494234944 submission.py:139] 57) loss = 6.297, grad_norm = 17.358
I0402 11:16:45.940724 139972615075584 logging_writer.py:48] [58] global_step=58, grad_norm=18.156210, loss=6.790053
I0402 11:16:45.943986 140001494234944 submission.py:139] 58) loss = 6.790, grad_norm = 18.156
I0402 11:16:46.746031 139972606682880 logging_writer.py:48] [59] global_step=59, grad_norm=21.302010, loss=6.413643
I0402 11:16:46.749086 140001494234944 submission.py:139] 59) loss = 6.414, grad_norm = 21.302
I0402 11:16:47.553472 139972615075584 logging_writer.py:48] [60] global_step=60, grad_norm=20.384745, loss=7.235619
I0402 11:16:47.556534 140001494234944 submission.py:139] 60) loss = 7.236, grad_norm = 20.385
I0402 11:16:48.355863 139972606682880 logging_writer.py:48] [61] global_step=61, grad_norm=11.373610, loss=6.137643
I0402 11:16:48.359258 140001494234944 submission.py:139] 61) loss = 6.138, grad_norm = 11.374
I0402 11:16:49.159931 139972615075584 logging_writer.py:48] [62] global_step=62, grad_norm=12.095206, loss=6.267505
I0402 11:16:49.162946 140001494234944 submission.py:139] 62) loss = 6.268, grad_norm = 12.095
I0402 11:16:49.958703 139972606682880 logging_writer.py:48] [63] global_step=63, grad_norm=30.577089, loss=6.712257
I0402 11:16:49.962317 140001494234944 submission.py:139] 63) loss = 6.712, grad_norm = 30.577
I0402 11:16:50.762925 139972615075584 logging_writer.py:48] [64] global_step=64, grad_norm=22.851690, loss=8.747652
I0402 11:16:50.766536 140001494234944 submission.py:139] 64) loss = 8.748, grad_norm = 22.852
I0402 11:16:51.569350 139972606682880 logging_writer.py:48] [65] global_step=65, grad_norm=16.288355, loss=6.577814
I0402 11:16:51.572693 140001494234944 submission.py:139] 65) loss = 6.578, grad_norm = 16.288
I0402 11:16:52.373071 139972615075584 logging_writer.py:48] [66] global_step=66, grad_norm=56.947105, loss=8.275458
I0402 11:16:52.376910 140001494234944 submission.py:139] 66) loss = 8.275, grad_norm = 56.947
I0402 11:16:53.178638 139972606682880 logging_writer.py:48] [67] global_step=67, grad_norm=23.299908, loss=12.396194
I0402 11:16:53.181892 140001494234944 submission.py:139] 67) loss = 12.396, grad_norm = 23.300
I0402 11:16:53.983265 139972615075584 logging_writer.py:48] [68] global_step=68, grad_norm=23.185360, loss=11.297828
I0402 11:16:53.986459 140001494234944 submission.py:139] 68) loss = 11.298, grad_norm = 23.185
I0402 11:16:54.785906 139972606682880 logging_writer.py:48] [69] global_step=69, grad_norm=21.665442, loss=7.892016
I0402 11:16:54.789665 140001494234944 submission.py:139] 69) loss = 7.892, grad_norm = 21.665
I0402 11:16:55.591254 139972615075584 logging_writer.py:48] [70] global_step=70, grad_norm=67.367256, loss=9.349788
I0402 11:16:55.594356 140001494234944 submission.py:139] 70) loss = 9.350, grad_norm = 67.367
I0402 11:16:56.393926 139972606682880 logging_writer.py:48] [71] global_step=71, grad_norm=23.017164, loss=13.359078
I0402 11:16:56.397866 140001494234944 submission.py:139] 71) loss = 13.359, grad_norm = 23.017
I0402 11:16:57.196126 139972615075584 logging_writer.py:48] [72] global_step=72, grad_norm=22.913933, loss=12.085560
I0402 11:16:57.199810 140001494234944 submission.py:139] 72) loss = 12.086, grad_norm = 22.914
I0402 11:16:58.000530 139972606682880 logging_writer.py:48] [73] global_step=73, grad_norm=22.019367, loss=8.448686
I0402 11:16:58.003921 140001494234944 submission.py:139] 73) loss = 8.449, grad_norm = 22.019
I0402 11:16:58.806532 139972615075584 logging_writer.py:48] [74] global_step=74, grad_norm=59.984074, loss=8.689628
I0402 11:16:58.809604 140001494234944 submission.py:139] 74) loss = 8.690, grad_norm = 59.984
I0402 11:16:59.611117 139972606682880 logging_writer.py:48] [75] global_step=75, grad_norm=22.610947, loss=12.321892
I0402 11:16:59.614338 140001494234944 submission.py:139] 75) loss = 12.322, grad_norm = 22.611
I0402 11:17:00.418365 139972615075584 logging_writer.py:48] [76] global_step=76, grad_norm=22.396364, loss=10.283495
I0402 11:17:00.422379 140001494234944 submission.py:139] 76) loss = 10.283, grad_norm = 22.396
I0402 11:17:01.222520 139972606682880 logging_writer.py:48] [77] global_step=77, grad_norm=13.094603, loss=6.296910
I0402 11:17:01.226299 140001494234944 submission.py:139] 77) loss = 6.297, grad_norm = 13.095
I0402 11:17:02.026940 139972615075584 logging_writer.py:48] [78] global_step=78, grad_norm=86.101051, loss=13.536777
I0402 11:17:02.030671 140001494234944 submission.py:139] 78) loss = 13.537, grad_norm = 86.101
I0402 11:17:02.831271 139972606682880 logging_writer.py:48] [79] global_step=79, grad_norm=22.280193, loss=17.285990
I0402 11:17:02.834392 140001494234944 submission.py:139] 79) loss = 17.286, grad_norm = 22.280
I0402 11:17:03.635395 139972615075584 logging_writer.py:48] [80] global_step=80, grad_norm=22.175800, loss=18.048796
I0402 11:17:03.638657 140001494234944 submission.py:139] 80) loss = 18.049, grad_norm = 22.176
I0402 11:17:04.438056 139972606682880 logging_writer.py:48] [81] global_step=81, grad_norm=22.048172, loss=16.198765
I0402 11:17:04.441443 140001494234944 submission.py:139] 81) loss = 16.199, grad_norm = 22.048
I0402 11:17:05.241458 139972615075584 logging_writer.py:48] [82] global_step=82, grad_norm=21.868525, loss=11.896203
I0402 11:17:05.244712 140001494234944 submission.py:139] 82) loss = 11.896, grad_norm = 21.869
I0402 11:17:06.045413 139972606682880 logging_writer.py:48] [83] global_step=83, grad_norm=7.162455, loss=6.049801
I0402 11:17:06.049111 140001494234944 submission.py:139] 83) loss = 6.050, grad_norm = 7.162
I0402 11:17:06.850383 139972615075584 logging_writer.py:48] [84] global_step=84, grad_norm=77.694733, loss=18.300922
I0402 11:17:06.853662 140001494234944 submission.py:139] 84) loss = 18.301, grad_norm = 77.695
I0402 11:17:07.657743 139972606682880 logging_writer.py:48] [85] global_step=85, grad_norm=21.644623, loss=13.590206
I0402 11:17:07.661495 140001494234944 submission.py:139] 85) loss = 13.590, grad_norm = 21.645
I0402 11:17:08.463402 139972615075584 logging_writer.py:48] [86] global_step=86, grad_norm=21.511070, loss=12.623919
I0402 11:17:08.466652 140001494234944 submission.py:139] 86) loss = 12.624, grad_norm = 21.511
I0402 11:17:09.265925 139972606682880 logging_writer.py:48] [87] global_step=87, grad_norm=20.864498, loss=9.137438
I0402 11:17:09.268973 140001494234944 submission.py:139] 87) loss = 9.137, grad_norm = 20.864
I0402 11:17:10.070403 139972615075584 logging_writer.py:48] [88] global_step=88, grad_norm=60.575909, loss=8.952556
I0402 11:17:10.073549 140001494234944 submission.py:139] 88) loss = 8.953, grad_norm = 60.576
I0402 11:17:10.873679 139972606682880 logging_writer.py:48] [89] global_step=89, grad_norm=21.186596, loss=13.127733
I0402 11:17:10.876634 140001494234944 submission.py:139] 89) loss = 13.128, grad_norm = 21.187
I0402 11:17:11.675315 139972615075584 logging_writer.py:48] [90] global_step=90, grad_norm=20.988663, loss=10.990676
I0402 11:17:11.679084 140001494234944 submission.py:139] 90) loss = 10.991, grad_norm = 20.989
I0402 11:17:12.478277 139972606682880 logging_writer.py:48] [91] global_step=91, grad_norm=13.955770, loss=6.715959
I0402 11:17:12.482150 140001494234944 submission.py:139] 91) loss = 6.716, grad_norm = 13.956
I0402 11:17:13.282793 139972615075584 logging_writer.py:48] [92] global_step=92, grad_norm=84.380112, loss=15.583261
I0402 11:17:13.286207 140001494234944 submission.py:139] 92) loss = 15.583, grad_norm = 84.380
I0402 11:17:14.088847 139972606682880 logging_writer.py:48] [93] global_step=93, grad_norm=20.836077, loss=18.361908
I0402 11:17:14.092576 140001494234944 submission.py:139] 93) loss = 18.362, grad_norm = 20.836
I0402 11:17:14.897156 139972615075584 logging_writer.py:48] [94] global_step=94, grad_norm=20.717403, loss=19.407270
I0402 11:17:14.900349 140001494234944 submission.py:139] 94) loss = 19.407, grad_norm = 20.717
I0402 11:17:15.703437 139972606682880 logging_writer.py:48] [95] global_step=95, grad_norm=20.541649, loss=17.701654
I0402 11:17:15.706936 140001494234944 submission.py:139] 95) loss = 17.702, grad_norm = 20.542
I0402 11:17:16.507604 139972615075584 logging_writer.py:48] [96] global_step=96, grad_norm=20.306501, loss=13.471502
I0402 11:17:16.510984 140001494234944 submission.py:139] 96) loss = 13.472, grad_norm = 20.307
I0402 11:17:17.313032 139972606682880 logging_writer.py:48] [97] global_step=97, grad_norm=15.541637, loss=7.317302
I0402 11:17:17.316142 140001494234944 submission.py:139] 97) loss = 7.317, grad_norm = 15.542
I0402 11:17:18.117162 139972615075584 logging_writer.py:48] [98] global_step=98, grad_norm=52.563400, loss=22.465792
I0402 11:17:18.121134 140001494234944 submission.py:139] 98) loss = 22.466, grad_norm = 52.563
I0402 11:17:18.918976 139972606682880 logging_writer.py:48] [99] global_step=99, grad_norm=14.063947, loss=7.532929
I0402 11:17:18.922373 140001494234944 submission.py:139] 99) loss = 7.533, grad_norm = 14.064
I0402 11:17:19.722099 139972615075584 logging_writer.py:48] [100] global_step=100, grad_norm=47.145290, loss=8.668749
I0402 11:17:19.725779 140001494234944 submission.py:139] 100) loss = 8.669, grad_norm = 47.145
I0402 11:22:37.592906 139972606682880 logging_writer.py:48] [500] global_step=500, grad_norm=2.327673, loss=5.880986
I0402 11:22:37.596972 140001494234944 submission.py:139] 500) loss = 5.881, grad_norm = 2.328
I0402 11:29:14.589804 139972615075584 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.021367, loss=5.832024
I0402 11:29:14.594557 140001494234944 submission.py:139] 1000) loss = 5.832, grad_norm = 1.021
I0402 11:35:53.767477 139972615075584 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.938141, loss=5.812937
I0402 11:35:53.774045 140001494234944 submission.py:139] 1500) loss = 5.813, grad_norm = 0.938
I0402 11:42:31.127698 139972606682880 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.630481, loss=5.807215
I0402 11:42:31.131904 140001494234944 submission.py:139] 2000) loss = 5.807, grad_norm = 0.630
I0402 11:49:10.192440 139972606682880 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.613263, loss=5.814291
I0402 11:49:10.206755 140001494234944 submission.py:139] 2500) loss = 5.814, grad_norm = 0.613
I0402 11:55:47.485186 139972598290176 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.358265, loss=5.808833
I0402 11:55:47.489056 140001494234944 submission.py:139] 3000) loss = 5.809, grad_norm = 0.358
I0402 11:55:59.401420 140001494234944 submission_runner.py:373] Before eval at step 3016: RAM USED (GB) 40.820912128
I0402 11:55:59.401635 140001494234944 spec.py:298] Evaluating on the training split.
I0402 11:56:09.415395 140001494234944 spec.py:310] Evaluating on the validation split.
I0402 11:56:18.864728 140001494234944 spec.py:326] Evaluating on the test split.
I0402 11:56:24.048478 140001494234944 submission_runner.py:382] Time since start: 2435.46s, 	Step: 3016, 	{'train/ctc_loss': 5.970658730239277, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': 6.035627951074945, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': 6.020725933931222, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0402 11:56:24.049260 140001494234944 submission_runner.py:396] After eval at step 3016: RAM USED (GB) 39.69581056
I0402 11:56:24.063407 139972606682880 logging_writer.py:48] [3016] global_step=3016, preemption_count=0, score=1391.828300, test/ctc_loss=6.020726, test/num_examples=2472, test/wer=0.899580, total_duration=2435.462164, train/ctc_loss=5.970659, train/wer=0.942049, validation/ctc_loss=6.035628, validation/num_examples=5348, validation/wer=0.896722
I0402 11:56:24.459725 140001494234944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_3016.
I0402 11:56:24.460347 140001494234944 submission_runner.py:416] After logging and checkpointing eval at step 3016: RAM USED (GB) 39.701282816
I0402 12:02:48.668104 139972598290176 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0402 12:02:48.674766 140001494234944 submission.py:139] 3500) loss = nan, grad_norm = nan
I0402 12:09:06.559284 139972589897472 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0402 12:09:06.563520 140001494234944 submission.py:139] 4000) loss = nan, grad_norm = nan
I0402 12:15:26.927093 139972598290176 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0402 12:15:26.938751 140001494234944 submission.py:139] 4500) loss = nan, grad_norm = nan
I0402 12:21:44.927011 139972589897472 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0402 12:21:44.931353 140001494234944 submission.py:139] 5000) loss = nan, grad_norm = nan
I0402 12:28:04.536965 139972589897472 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0402 12:28:04.543742 140001494234944 submission.py:139] 5500) loss = nan, grad_norm = nan
I0402 12:34:22.418213 139972581504768 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0402 12:34:22.423318 140001494234944 submission.py:139] 6000) loss = nan, grad_norm = nan
I0402 12:36:24.877864 140001494234944 submission_runner.py:373] Before eval at step 6163: RAM USED (GB) 39.857963008
I0402 12:36:24.878081 140001494234944 spec.py:298] Evaluating on the training split.
I0402 12:36:34.617583 140001494234944 spec.py:310] Evaluating on the validation split.
I0402 12:36:44.242902 140001494234944 spec.py:326] Evaluating on the test split.
I0402 12:36:49.360913 140001494234944 submission_runner.py:382] Time since start: 4860.96s, 	Step: 6163, 	{'train/ctc_loss': nan, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0402 12:36:49.361642 140001494234944 submission_runner.py:396] After eval at step 6163: RAM USED (GB) 39.8010368
I0402 12:36:49.378787 139972589897472 logging_writer.py:48] [6163] global_step=6163, preemption_count=0, score=2762.027902, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4860.961483, train/ctc_loss=nan, train/wer=0.942049, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0402 12:36:49.785963 140001494234944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_6163.
I0402 12:36:49.786599 140001494234944 submission_runner.py:416] After logging and checkpointing eval at step 6163: RAM USED (GB) 39.806783488
I0402 12:41:07.133586 139972589897472 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0402 12:41:07.139333 140001494234944 submission.py:139] 6500) loss = nan, grad_norm = nan
I0402 12:47:25.148181 139972581504768 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0402 12:47:25.152312 140001494234944 submission.py:139] 7000) loss = nan, grad_norm = nan
I0402 12:53:44.991852 139972589897472 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0402 12:53:44.998350 140001494234944 submission.py:139] 7500) loss = nan, grad_norm = nan
I0402 13:00:02.931361 139972581504768 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0402 13:00:02.935737 140001494234944 submission.py:139] 8000) loss = nan, grad_norm = nan
I0402 13:06:22.666438 139972589897472 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0402 13:06:22.672908 140001494234944 submission.py:139] 8500) loss = nan, grad_norm = nan
I0402 13:12:40.448694 139972581504768 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0402 13:12:40.454096 140001494234944 submission.py:139] 9000) loss = nan, grad_norm = nan
I0402 13:16:50.192570 140001494234944 submission_runner.py:373] Before eval at step 9329: RAM USED (GB) 39.387242496
I0402 13:16:50.193035 140001494234944 spec.py:298] Evaluating on the training split.
I0402 13:16:59.941574 140001494234944 spec.py:310] Evaluating on the validation split.
I0402 13:17:09.310493 140001494234944 spec.py:326] Evaluating on the test split.
I0402 13:17:14.535115 140001494234944 submission_runner.py:382] Time since start: 7286.27s, 	Step: 9329, 	{'train/ctc_loss': nan, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0402 13:17:14.535969 140001494234944 submission_runner.py:396] After eval at step 9329: RAM USED (GB) 39.294025728
I0402 13:17:14.552700 139972589897472 logging_writer.py:48] [9329] global_step=9329, preemption_count=0, score=4136.327155, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7286.274610, train/ctc_loss=nan, train/wer=0.942049, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0402 13:17:14.960372 140001494234944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_9329.
I0402 13:17:14.961074 140001494234944 submission_runner.py:416] After logging and checkpointing eval at step 9329: RAM USED (GB) 39.299653632
I0402 13:19:24.964954 139972581504768 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0402 13:19:24.969090 140001494234944 submission.py:139] 9500) loss = nan, grad_norm = nan
I0402 13:25:42.095941 140001494234944 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 39.745667072
I0402 13:25:42.096150 140001494234944 spec.py:298] Evaluating on the training split.
I0402 13:25:51.516566 140001494234944 spec.py:310] Evaluating on the validation split.
I0402 13:26:00.810189 140001494234944 spec.py:326] Evaluating on the test split.
I0402 13:26:05.959170 140001494234944 submission_runner.py:382] Time since start: 7818.18s, 	Step: 10000, 	{'train/ctc_loss': nan, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0402 13:26:05.959934 140001494234944 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 39.707942912
I0402 13:26:05.974977 139972589897472 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4425.790712, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7818.182539, train/ctc_loss=nan, train/wer=0.942049, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0402 13:26:06.358040 140001494234944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0402 13:26:06.358677 140001494234944 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 39.71385344
I0402 13:26:06.369715 139972581504768 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4425.790712
I0402 13:26:07.117533 140001494234944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0402 13:26:07.265380 140001494234944 submission_runner.py:550] Tuning trial 1/1
I0402 13:26:07.265634 140001494234944 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 13:26:07.265981 140001494234944 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.432636781585863, 'train/wer': 1.3650862092197245, 'validation/ctc_loss': 30.214147578862768, 'validation/wer': 1.726210592381596, 'validation/num_examples': 5348, 'test/ctc_loss': 30.22735660365185, 'test/wer': 1.7166737757195376, 'test/num_examples': 2472, 'score': 6.426043272018433, 'total_duration': 6.428128957748413, 'global_step': 1, 'preemption_count': 0}), (3016, {'train/ctc_loss': 5.970658730239277, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': 6.035627951074945, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': 6.020725933931222, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1391.8282997608185, 'total_duration': 2435.462164402008, 'global_step': 3016, 'preemption_count': 0}), (6163, {'train/ctc_loss': nan, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2762.027901649475, 'total_duration': 4860.961483001709, 'global_step': 6163, 'preemption_count': 0}), (9329, {'train/ctc_loss': nan, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4136.32715511322, 'total_duration': 7286.274610280991, 'global_step': 9329, 'preemption_count': 0}), (10000, {'train/ctc_loss': nan, 'train/wer': 0.9420492947498342, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4425.79071187973, 'total_duration': 7818.182538986206, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0402 13:26:07.266058 140001494234944 submission_runner.py:553] Timing: 4425.79071187973
I0402 13:26:07.266107 140001494234944 submission_runner.py:554] ====================
I0402 13:26:07.266275 140001494234944 submission_runner.py:613] Final librispeech_conformer score: 4425.79071187973
