WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 06:26:31.326619 139807450314560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 06:26:31.326698 140140975179584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 06:26:31.327510 140567748110144 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 06:26:31.327538 139748737615680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 06:26:31.327593 139956317890368 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 06:26:31.327623 140526076802880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 06:26:31.328107 139947090044736 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 06:26:31.337522 139998340331328 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 06:26:31.337806 139998340331328 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.338159 140567748110144 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.338198 139748737615680 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.338239 139956317890368 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.338255 140526076802880 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.338632 139947090044736 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.347525 140140975179584 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:31.347593 139807450314560 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:26:32.512143 139998340331328 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/ogbg_pytorch.
W0402 06:26:32.552381 140140975179584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.552571 139956317890368 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.553011 139807450314560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.553192 140567748110144 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.553393 139947090044736 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.553763 139998340331328 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.553776 139748737615680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:26:32.554693 140526076802880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 06:26:32.557230 139998340331328 submission_runner.py:511] Using RNG seed 2271237798
I0402 06:26:32.558215 139998340331328 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 06:26:32.558337 139998340331328 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1.
I0402 06:26:32.558550 139998340331328 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/hparams.json.
I0402 06:26:32.559455 139998340331328 submission_runner.py:230] Starting train once: RAM USED (GB) 5.853003776
I0402 06:26:32.559550 139998340331328 submission_runner.py:231] Initializing dataset.
I0402 06:26:32.559727 139998340331328 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.853003776
I0402 06:26:32.559790 139998340331328 submission_runner.py:240] Initializing model.
I0402 06:26:36.338949 139998340331328 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.495036928
I0402 06:26:36.339136 139998340331328 submission_runner.py:252] Initializing optimizer.
I0402 06:26:36.339938 139998340331328 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.495028736
I0402 06:26:36.340036 139998340331328 submission_runner.py:261] Initializing metrics bundle.
I0402 06:26:36.340093 139998340331328 submission_runner.py:276] Initializing checkpoint and logger.
I0402 06:26:36.341164 139998340331328 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 06:26:36.341268 139998340331328 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 06:26:36.932971 139998340331328 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0402 06:26:36.933814 139998340331328 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/flags_0.json.
I0402 06:26:36.966020 139998340331328 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.544307712
I0402 06:26:36.967381 139998340331328 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.544307712
I0402 06:26:36.967506 139998340331328 submission_runner.py:313] Starting training loop.
I0402 06:26:37.236258 139998340331328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:26:37.241580 139998340331328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:26:37.337173 139998340331328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:26:38.780566 139998340331328 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 15.781195776
I0402 06:26:42.520613 139960459962112 logging_writer.py:48] [0] global_step=0, grad_norm=2.677716, loss=0.819819
I0402 06:26:42.525439 139998340331328 submission.py:296] 0) loss = 0.820, grad_norm = 2.678
I0402 06:26:42.525858 139998340331328 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 22.150807552
I0402 06:26:42.534957 139998340331328 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 22.150807552
I0402 06:26:42.535056 139998340331328 spec.py:298] Evaluating on the training split.
I0402 06:26:42.539648 139998340331328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:26:42.543886 139998340331328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:26:42.592890 139998340331328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0402 06:26:57.479927 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.673467 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.673850 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.674365 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.694227 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.694292 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.694334 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:26:57.694488 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.262744 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.412097 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.412850 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.417431 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.417704 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.418511 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.418689 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:11.419348 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:27:35.832073 139998340331328 spec.py:310] Evaluating on the validation split.
I0402 06:27:35.834702 139998340331328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:27:35.838779 139998340331328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:27:35.890549 139998340331328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0402 06:27:48.694283 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.893089 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.893083 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.898817 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.898875 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.899380 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.899761 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:48.900110 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.057858 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.229981 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.230091 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.236220 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.236366 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.236973 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.237146 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:27:54.237341 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:28:18.365580 139998340331328 spec.py:326] Evaluating on the test split.
I0402 06:28:18.368085 139998340331328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0402 06:28:18.372011 139998340331328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0402 06:28:18.423753 139998340331328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0402 06:28:31.170392 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.379601 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.380909 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.386846 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.386914 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.387018 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.387212 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:31.387242 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.592026 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.798702 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.798874 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.805291 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.805386 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.805469 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.805519 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:28:36.805525 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:29:01.843511 139998340331328 submission_runner.py:382] Time since start: 5.57s, 	Step: 1, 	{'train/accuracy': 0.4312884355723088, 'train/loss': 0.8213552236557007, 'train/mean_average_precision': 0.021203012728293308, 'validation/accuracy': 0.4418780196913714, 'validation/loss': 0.8105841279029846, 'validation/mean_average_precision': 0.0256216969824728, 'validation/num_examples': 43793, 'test/accuracy': 0.4454173807671556, 'test/loss': 0.8078605532646179, 'test/mean_average_precision': 0.02708983913236362, 'test/num_examples': 43793}
I0402 06:29:01.843930 139998340331328 submission_runner.py:396] After eval at step 1: RAM USED (GB) 26.077458432
I0402 06:29:01.853309 139946400229120 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.566149, test/accuracy=0.445417, test/loss=0.807861, test/mean_average_precision=0.027090, test/num_examples=43793, total_duration=5.568256, train/accuracy=0.431288, train/loss=0.821355, train/mean_average_precision=0.021203, validation/accuracy=0.441878, validation/loss=0.810584, validation/mean_average_precision=0.025622, validation/num_examples=43793
I0402 06:29:02.163310 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_1.
I0402 06:29:02.163900 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 26.0765696
I0402 06:29:02.420170 139998340331328 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 26.12555776
I0402 06:29:02.422627 139998340331328 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429141 139947090044736 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429365 139807450314560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429921 140567748110144 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429928 140526076802880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429929 139956317890368 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429937 139748737615680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.429974 140140975179584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:29:02.460153 139946408621824 logging_writer.py:48] [1] global_step=1, grad_norm=2.648698, loss=0.821673
I0402 06:29:02.464500 139998340331328 submission.py:296] 1) loss = 0.822, grad_norm = 2.649
I0402 06:29:02.464976 139998340331328 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 26.124951552
I0402 06:29:02.728044 139946400229120 logging_writer.py:48] [2] global_step=2, grad_norm=2.671047, loss=0.819155
I0402 06:29:02.731967 139998340331328 submission.py:296] 2) loss = 0.819, grad_norm = 2.671
I0402 06:29:03.001193 139946408621824 logging_writer.py:48] [3] global_step=3, grad_norm=2.654038, loss=0.817579
I0402 06:29:03.005276 139998340331328 submission.py:296] 3) loss = 0.818, grad_norm = 2.654
I0402 06:29:03.266421 139946400229120 logging_writer.py:48] [4] global_step=4, grad_norm=2.664160, loss=0.813031
I0402 06:29:03.270230 139998340331328 submission.py:296] 4) loss = 0.813, grad_norm = 2.664
I0402 06:29:03.532927 139946408621824 logging_writer.py:48] [5] global_step=5, grad_norm=2.646702, loss=0.811129
I0402 06:29:03.536919 139998340331328 submission.py:296] 5) loss = 0.811, grad_norm = 2.647
I0402 06:29:03.796636 139946400229120 logging_writer.py:48] [6] global_step=6, grad_norm=2.613734, loss=0.808405
I0402 06:29:03.800538 139998340331328 submission.py:296] 6) loss = 0.808, grad_norm = 2.614
I0402 06:29:04.062576 139946408621824 logging_writer.py:48] [7] global_step=7, grad_norm=2.570907, loss=0.803035
I0402 06:29:04.066371 139998340331328 submission.py:296] 7) loss = 0.803, grad_norm = 2.571
I0402 06:29:04.330669 139946400229120 logging_writer.py:48] [8] global_step=8, grad_norm=2.556192, loss=0.796840
I0402 06:29:04.334487 139998340331328 submission.py:296] 8) loss = 0.797, grad_norm = 2.556
I0402 06:29:04.594910 139946408621824 logging_writer.py:48] [9] global_step=9, grad_norm=2.505709, loss=0.791710
I0402 06:29:04.598610 139998340331328 submission.py:296] 9) loss = 0.792, grad_norm = 2.506
I0402 06:29:04.862130 139946400229120 logging_writer.py:48] [10] global_step=10, grad_norm=2.478408, loss=0.786950
I0402 06:29:04.866028 139998340331328 submission.py:296] 10) loss = 0.787, grad_norm = 2.478
I0402 06:29:05.124678 139946408621824 logging_writer.py:48] [11] global_step=11, grad_norm=2.450460, loss=0.774874
I0402 06:29:05.128336 139998340331328 submission.py:296] 11) loss = 0.775, grad_norm = 2.450
I0402 06:29:05.396549 139946400229120 logging_writer.py:48] [12] global_step=12, grad_norm=2.396432, loss=0.768661
I0402 06:29:05.400917 139998340331328 submission.py:296] 12) loss = 0.769, grad_norm = 2.396
I0402 06:29:05.668089 139946408621824 logging_writer.py:48] [13] global_step=13, grad_norm=2.304708, loss=0.760274
I0402 06:29:05.671847 139998340331328 submission.py:296] 13) loss = 0.760, grad_norm = 2.305
I0402 06:29:05.930207 139946400229120 logging_writer.py:48] [14] global_step=14, grad_norm=2.262535, loss=0.751602
I0402 06:29:05.934122 139998340331328 submission.py:296] 14) loss = 0.752, grad_norm = 2.263
I0402 06:29:06.194772 139946408621824 logging_writer.py:48] [15] global_step=15, grad_norm=2.201243, loss=0.744166
I0402 06:29:06.198350 139998340331328 submission.py:296] 15) loss = 0.744, grad_norm = 2.201
I0402 06:29:06.460694 139946400229120 logging_writer.py:48] [16] global_step=16, grad_norm=2.130142, loss=0.734235
I0402 06:29:06.464799 139998340331328 submission.py:296] 16) loss = 0.734, grad_norm = 2.130
I0402 06:29:06.723673 139946408621824 logging_writer.py:48] [17] global_step=17, grad_norm=2.066878, loss=0.727442
I0402 06:29:06.727505 139998340331328 submission.py:296] 17) loss = 0.727, grad_norm = 2.067
I0402 06:29:06.998821 139946400229120 logging_writer.py:48] [18] global_step=18, grad_norm=2.041432, loss=0.718056
I0402 06:29:07.002669 139998340331328 submission.py:296] 18) loss = 0.718, grad_norm = 2.041
I0402 06:29:07.272697 139946408621824 logging_writer.py:48] [19] global_step=19, grad_norm=1.976741, loss=0.709255
I0402 06:29:07.276591 139998340331328 submission.py:296] 19) loss = 0.709, grad_norm = 1.977
I0402 06:29:07.543753 139946400229120 logging_writer.py:48] [20] global_step=20, grad_norm=1.896402, loss=0.701854
I0402 06:29:07.547509 139998340331328 submission.py:296] 20) loss = 0.702, grad_norm = 1.896
I0402 06:29:07.816196 139946408621824 logging_writer.py:48] [21] global_step=21, grad_norm=1.830700, loss=0.692786
I0402 06:29:07.820539 139998340331328 submission.py:296] 21) loss = 0.693, grad_norm = 1.831
I0402 06:29:08.084504 139946400229120 logging_writer.py:48] [22] global_step=22, grad_norm=1.776340, loss=0.684787
I0402 06:29:08.088630 139998340331328 submission.py:296] 22) loss = 0.685, grad_norm = 1.776
I0402 06:29:08.358050 139946408621824 logging_writer.py:48] [23] global_step=23, grad_norm=1.686541, loss=0.678273
I0402 06:29:08.362622 139998340331328 submission.py:296] 23) loss = 0.678, grad_norm = 1.687
I0402 06:29:08.620892 139946400229120 logging_writer.py:48] [24] global_step=24, grad_norm=1.638439, loss=0.669807
I0402 06:29:08.624687 139998340331328 submission.py:296] 24) loss = 0.670, grad_norm = 1.638
I0402 06:29:08.884053 139946408621824 logging_writer.py:48] [25] global_step=25, grad_norm=1.575217, loss=0.664087
I0402 06:29:08.888173 139998340331328 submission.py:296] 25) loss = 0.664, grad_norm = 1.575
I0402 06:29:09.160010 139946400229120 logging_writer.py:48] [26] global_step=26, grad_norm=1.555217, loss=0.657573
I0402 06:29:09.164203 139998340331328 submission.py:296] 26) loss = 0.658, grad_norm = 1.555
I0402 06:29:09.438465 139946408621824 logging_writer.py:48] [27] global_step=27, grad_norm=1.537194, loss=0.648054
I0402 06:29:09.442774 139998340331328 submission.py:296] 27) loss = 0.648, grad_norm = 1.537
I0402 06:29:09.709420 139946400229120 logging_writer.py:48] [28] global_step=28, grad_norm=1.574866, loss=0.641193
I0402 06:29:09.713119 139998340331328 submission.py:296] 28) loss = 0.641, grad_norm = 1.575
I0402 06:29:09.976626 139946408621824 logging_writer.py:48] [29] global_step=29, grad_norm=1.588865, loss=0.636507
I0402 06:29:09.980639 139998340331328 submission.py:296] 29) loss = 0.637, grad_norm = 1.589
I0402 06:29:10.255059 139946400229120 logging_writer.py:48] [30] global_step=30, grad_norm=1.542657, loss=0.629220
I0402 06:29:10.258905 139998340331328 submission.py:296] 30) loss = 0.629, grad_norm = 1.543
I0402 06:29:10.532396 139946408621824 logging_writer.py:48] [31] global_step=31, grad_norm=1.556906, loss=0.622042
I0402 06:29:10.536542 139998340331328 submission.py:296] 31) loss = 0.622, grad_norm = 1.557
I0402 06:29:10.805498 139946400229120 logging_writer.py:48] [32] global_step=32, grad_norm=1.540922, loss=0.614171
I0402 06:29:10.809377 139998340331328 submission.py:296] 32) loss = 0.614, grad_norm = 1.541
I0402 06:29:11.079503 139946408621824 logging_writer.py:48] [33] global_step=33, grad_norm=1.511317, loss=0.607973
I0402 06:29:11.083299 139998340331328 submission.py:296] 33) loss = 0.608, grad_norm = 1.511
I0402 06:29:11.353103 139946400229120 logging_writer.py:48] [34] global_step=34, grad_norm=1.483640, loss=0.599794
I0402 06:29:11.356824 139998340331328 submission.py:296] 34) loss = 0.600, grad_norm = 1.484
I0402 06:29:11.625036 139946408621824 logging_writer.py:48] [35] global_step=35, grad_norm=1.429275, loss=0.592154
I0402 06:29:11.628924 139998340331328 submission.py:296] 35) loss = 0.592, grad_norm = 1.429
I0402 06:29:11.908668 139946400229120 logging_writer.py:48] [36] global_step=36, grad_norm=1.318368, loss=0.581952
I0402 06:29:11.912652 139998340331328 submission.py:296] 36) loss = 0.582, grad_norm = 1.318
I0402 06:29:12.181498 139946408621824 logging_writer.py:48] [37] global_step=37, grad_norm=1.265608, loss=0.576316
I0402 06:29:12.185306 139998340331328 submission.py:296] 37) loss = 0.576, grad_norm = 1.266
I0402 06:29:12.449939 139946400229120 logging_writer.py:48] [38] global_step=38, grad_norm=1.170662, loss=0.569469
I0402 06:29:12.453612 139998340331328 submission.py:296] 38) loss = 0.569, grad_norm = 1.171
I0402 06:29:12.724639 139946408621824 logging_writer.py:48] [39] global_step=39, grad_norm=1.135129, loss=0.564606
I0402 06:29:12.728484 139998340331328 submission.py:296] 39) loss = 0.565, grad_norm = 1.135
I0402 06:29:13.000048 139946400229120 logging_writer.py:48] [40] global_step=40, grad_norm=1.140124, loss=0.558400
I0402 06:29:13.004365 139998340331328 submission.py:296] 40) loss = 0.558, grad_norm = 1.140
I0402 06:29:13.268064 139946408621824 logging_writer.py:48] [41] global_step=41, grad_norm=1.140197, loss=0.551506
I0402 06:29:13.272437 139998340331328 submission.py:296] 41) loss = 0.552, grad_norm = 1.140
I0402 06:29:13.533353 139946400229120 logging_writer.py:48] [42] global_step=42, grad_norm=1.111929, loss=0.545326
I0402 06:29:13.537289 139998340331328 submission.py:296] 42) loss = 0.545, grad_norm = 1.112
I0402 06:29:13.802335 139946408621824 logging_writer.py:48] [43] global_step=43, grad_norm=1.044663, loss=0.540138
I0402 06:29:13.806320 139998340331328 submission.py:296] 43) loss = 0.540, grad_norm = 1.045
I0402 06:29:14.071633 139946400229120 logging_writer.py:48] [44] global_step=44, grad_norm=0.971524, loss=0.535476
I0402 06:29:14.075246 139998340331328 submission.py:296] 44) loss = 0.535, grad_norm = 0.972
I0402 06:29:14.350051 139946408621824 logging_writer.py:48] [45] global_step=45, grad_norm=0.893897, loss=0.529223
I0402 06:29:14.353743 139998340331328 submission.py:296] 45) loss = 0.529, grad_norm = 0.894
I0402 06:29:14.620135 139946400229120 logging_writer.py:48] [46] global_step=46, grad_norm=0.884041, loss=0.524990
I0402 06:29:14.623959 139998340331328 submission.py:296] 46) loss = 0.525, grad_norm = 0.884
I0402 06:29:14.897719 139946408621824 logging_writer.py:48] [47] global_step=47, grad_norm=0.882716, loss=0.523171
I0402 06:29:14.901517 139998340331328 submission.py:296] 47) loss = 0.523, grad_norm = 0.883
I0402 06:29:15.169321 139946400229120 logging_writer.py:48] [48] global_step=48, grad_norm=0.872308, loss=0.518913
I0402 06:29:15.173383 139998340331328 submission.py:296] 48) loss = 0.519, grad_norm = 0.872
I0402 06:29:15.437451 139946408621824 logging_writer.py:48] [49] global_step=49, grad_norm=0.851571, loss=0.515706
I0402 06:29:15.441519 139998340331328 submission.py:296] 49) loss = 0.516, grad_norm = 0.852
I0402 06:29:15.703157 139946400229120 logging_writer.py:48] [50] global_step=50, grad_norm=0.839088, loss=0.511026
I0402 06:29:15.706899 139998340331328 submission.py:296] 50) loss = 0.511, grad_norm = 0.839
I0402 06:29:15.971777 139946408621824 logging_writer.py:48] [51] global_step=51, grad_norm=0.813184, loss=0.506312
I0402 06:29:15.975573 139998340331328 submission.py:296] 51) loss = 0.506, grad_norm = 0.813
I0402 06:29:16.244588 139946400229120 logging_writer.py:48] [52] global_step=52, grad_norm=0.785762, loss=0.502146
I0402 06:29:16.248394 139998340331328 submission.py:296] 52) loss = 0.502, grad_norm = 0.786
I0402 06:29:16.519720 139946408621824 logging_writer.py:48] [53] global_step=53, grad_norm=0.760680, loss=0.498415
I0402 06:29:16.523581 139998340331328 submission.py:296] 53) loss = 0.498, grad_norm = 0.761
I0402 06:29:16.795703 139946400229120 logging_writer.py:48] [54] global_step=54, grad_norm=0.744022, loss=0.493587
I0402 06:29:16.799420 139998340331328 submission.py:296] 54) loss = 0.494, grad_norm = 0.744
I0402 06:29:17.069320 139946408621824 logging_writer.py:48] [55] global_step=55, grad_norm=0.712131, loss=0.490488
I0402 06:29:17.073319 139998340331328 submission.py:296] 55) loss = 0.490, grad_norm = 0.712
I0402 06:29:17.339388 139946400229120 logging_writer.py:48] [56] global_step=56, grad_norm=0.685311, loss=0.486822
I0402 06:29:17.343623 139998340331328 submission.py:296] 56) loss = 0.487, grad_norm = 0.685
I0402 06:29:17.634583 139946408621824 logging_writer.py:48] [57] global_step=57, grad_norm=0.672204, loss=0.481888
I0402 06:29:17.638718 139998340331328 submission.py:296] 57) loss = 0.482, grad_norm = 0.672
I0402 06:29:17.909223 139946400229120 logging_writer.py:48] [58] global_step=58, grad_norm=0.652083, loss=0.477833
I0402 06:29:17.913052 139998340331328 submission.py:296] 58) loss = 0.478, grad_norm = 0.652
I0402 06:29:18.183040 139946408621824 logging_writer.py:48] [59] global_step=59, grad_norm=0.639000, loss=0.477303
I0402 06:29:18.187174 139998340331328 submission.py:296] 59) loss = 0.477, grad_norm = 0.639
I0402 06:29:18.467417 139946400229120 logging_writer.py:48] [60] global_step=60, grad_norm=0.617589, loss=0.474383
I0402 06:29:18.471376 139998340331328 submission.py:296] 60) loss = 0.474, grad_norm = 0.618
I0402 06:29:18.743057 139946408621824 logging_writer.py:48] [61] global_step=61, grad_norm=0.613642, loss=0.468289
I0402 06:29:18.746995 139998340331328 submission.py:296] 61) loss = 0.468, grad_norm = 0.614
I0402 06:29:19.025238 139946400229120 logging_writer.py:48] [62] global_step=62, grad_norm=0.599578, loss=0.465194
I0402 06:29:19.029223 139998340331328 submission.py:296] 62) loss = 0.465, grad_norm = 0.600
I0402 06:29:19.313565 139946408621824 logging_writer.py:48] [63] global_step=63, grad_norm=0.588650, loss=0.464146
I0402 06:29:19.317494 139998340331328 submission.py:296] 63) loss = 0.464, grad_norm = 0.589
I0402 06:29:19.585097 139946400229120 logging_writer.py:48] [64] global_step=64, grad_norm=0.587134, loss=0.459585
I0402 06:29:19.588949 139998340331328 submission.py:296] 64) loss = 0.460, grad_norm = 0.587
I0402 06:29:19.857170 139946408621824 logging_writer.py:48] [65] global_step=65, grad_norm=0.570676, loss=0.457683
I0402 06:29:19.860863 139998340331328 submission.py:296] 65) loss = 0.458, grad_norm = 0.571
I0402 06:29:20.133501 139946400229120 logging_writer.py:48] [66] global_step=66, grad_norm=0.569377, loss=0.451252
I0402 06:29:20.137601 139998340331328 submission.py:296] 66) loss = 0.451, grad_norm = 0.569
I0402 06:29:20.404545 139946408621824 logging_writer.py:48] [67] global_step=67, grad_norm=0.562475, loss=0.448350
I0402 06:29:20.408498 139998340331328 submission.py:296] 67) loss = 0.448, grad_norm = 0.562
I0402 06:29:20.676230 139946400229120 logging_writer.py:48] [68] global_step=68, grad_norm=0.552339, loss=0.447305
I0402 06:29:20.680189 139998340331328 submission.py:296] 68) loss = 0.447, grad_norm = 0.552
I0402 06:29:20.942276 139946408621824 logging_writer.py:48] [69] global_step=69, grad_norm=0.540535, loss=0.443860
I0402 06:29:20.946243 139998340331328 submission.py:296] 69) loss = 0.444, grad_norm = 0.541
I0402 06:29:21.212245 139946400229120 logging_writer.py:48] [70] global_step=70, grad_norm=0.542415, loss=0.439926
I0402 06:29:21.216321 139998340331328 submission.py:296] 70) loss = 0.440, grad_norm = 0.542
I0402 06:29:21.479780 139946408621824 logging_writer.py:48] [71] global_step=71, grad_norm=0.549763, loss=0.436521
I0402 06:29:21.483820 139998340331328 submission.py:296] 71) loss = 0.437, grad_norm = 0.550
I0402 06:29:21.748120 139946400229120 logging_writer.py:48] [72] global_step=72, grad_norm=0.534102, loss=0.434190
I0402 06:29:21.752004 139998340331328 submission.py:296] 72) loss = 0.434, grad_norm = 0.534
I0402 06:29:22.029146 139946408621824 logging_writer.py:48] [73] global_step=73, grad_norm=0.529862, loss=0.430052
I0402 06:29:22.032685 139998340331328 submission.py:296] 73) loss = 0.430, grad_norm = 0.530
I0402 06:29:22.297014 139946400229120 logging_writer.py:48] [74] global_step=74, grad_norm=0.514038, loss=0.427546
I0402 06:29:22.300895 139998340331328 submission.py:296] 74) loss = 0.428, grad_norm = 0.514
I0402 06:29:22.570047 139946408621824 logging_writer.py:48] [75] global_step=75, grad_norm=0.497706, loss=0.425359
I0402 06:29:22.573676 139998340331328 submission.py:296] 75) loss = 0.425, grad_norm = 0.498
I0402 06:29:22.850590 139946400229120 logging_writer.py:48] [76] global_step=76, grad_norm=0.493167, loss=0.420923
I0402 06:29:22.854283 139998340331328 submission.py:296] 76) loss = 0.421, grad_norm = 0.493
I0402 06:29:23.122759 139946408621824 logging_writer.py:48] [77] global_step=77, grad_norm=0.484963, loss=0.418068
I0402 06:29:23.126635 139998340331328 submission.py:296] 77) loss = 0.418, grad_norm = 0.485
I0402 06:29:23.398603 139946400229120 logging_writer.py:48] [78] global_step=78, grad_norm=0.478119, loss=0.414611
I0402 06:29:23.402367 139998340331328 submission.py:296] 78) loss = 0.415, grad_norm = 0.478
I0402 06:29:23.678845 139946408621824 logging_writer.py:48] [79] global_step=79, grad_norm=0.468198, loss=0.414846
I0402 06:29:23.682895 139998340331328 submission.py:296] 79) loss = 0.415, grad_norm = 0.468
I0402 06:29:23.945443 139946400229120 logging_writer.py:48] [80] global_step=80, grad_norm=0.466489, loss=0.410366
I0402 06:29:23.949285 139998340331328 submission.py:296] 80) loss = 0.410, grad_norm = 0.466
I0402 06:29:24.218800 139946408621824 logging_writer.py:48] [81] global_step=81, grad_norm=0.464322, loss=0.408400
I0402 06:29:24.223070 139998340331328 submission.py:296] 81) loss = 0.408, grad_norm = 0.464
I0402 06:29:24.497698 139946400229120 logging_writer.py:48] [82] global_step=82, grad_norm=0.462608, loss=0.405731
I0402 06:29:24.501826 139998340331328 submission.py:296] 82) loss = 0.406, grad_norm = 0.463
I0402 06:29:24.769353 139946408621824 logging_writer.py:48] [83] global_step=83, grad_norm=0.456204, loss=0.405087
I0402 06:29:24.773172 139998340331328 submission.py:296] 83) loss = 0.405, grad_norm = 0.456
I0402 06:29:25.039495 139946400229120 logging_writer.py:48] [84] global_step=84, grad_norm=0.461936, loss=0.402089
I0402 06:29:25.043409 139998340331328 submission.py:296] 84) loss = 0.402, grad_norm = 0.462
I0402 06:29:25.307001 139946408621824 logging_writer.py:48] [85] global_step=85, grad_norm=0.450948, loss=0.401068
I0402 06:29:25.310638 139998340331328 submission.py:296] 85) loss = 0.401, grad_norm = 0.451
I0402 06:29:25.581377 139946400229120 logging_writer.py:48] [86] global_step=86, grad_norm=0.448782, loss=0.397589
I0402 06:29:25.585229 139998340331328 submission.py:296] 86) loss = 0.398, grad_norm = 0.449
I0402 06:29:25.859695 139946408621824 logging_writer.py:48] [87] global_step=87, grad_norm=0.452269, loss=0.394004
I0402 06:29:25.863479 139998340331328 submission.py:296] 87) loss = 0.394, grad_norm = 0.452
I0402 06:29:26.136199 139946400229120 logging_writer.py:48] [88] global_step=88, grad_norm=0.444431, loss=0.393568
I0402 06:29:26.139831 139998340331328 submission.py:296] 88) loss = 0.394, grad_norm = 0.444
I0402 06:29:26.412937 139946408621824 logging_writer.py:48] [89] global_step=89, grad_norm=0.439711, loss=0.392062
I0402 06:29:26.417012 139998340331328 submission.py:296] 89) loss = 0.392, grad_norm = 0.440
I0402 06:29:26.689955 139946400229120 logging_writer.py:48] [90] global_step=90, grad_norm=0.437570, loss=0.389330
I0402 06:29:26.693744 139998340331328 submission.py:296] 90) loss = 0.389, grad_norm = 0.438
I0402 06:29:26.965215 139946408621824 logging_writer.py:48] [91] global_step=91, grad_norm=0.438442, loss=0.384762
I0402 06:29:26.969322 139998340331328 submission.py:296] 91) loss = 0.385, grad_norm = 0.438
I0402 06:29:27.237531 139946400229120 logging_writer.py:48] [92] global_step=92, grad_norm=0.431589, loss=0.384972
I0402 06:29:27.241277 139998340331328 submission.py:296] 92) loss = 0.385, grad_norm = 0.432
I0402 06:29:27.515879 139946408621824 logging_writer.py:48] [93] global_step=93, grad_norm=0.428422, loss=0.382662
I0402 06:29:27.519608 139998340331328 submission.py:296] 93) loss = 0.383, grad_norm = 0.428
I0402 06:29:27.794389 139946400229120 logging_writer.py:48] [94] global_step=94, grad_norm=0.423708, loss=0.381363
I0402 06:29:27.798221 139998340331328 submission.py:296] 94) loss = 0.381, grad_norm = 0.424
I0402 06:29:28.067738 139946408621824 logging_writer.py:48] [95] global_step=95, grad_norm=0.422345, loss=0.379777
I0402 06:29:28.071419 139998340331328 submission.py:296] 95) loss = 0.380, grad_norm = 0.422
I0402 06:29:28.342192 139946400229120 logging_writer.py:48] [96] global_step=96, grad_norm=0.421773, loss=0.376688
I0402 06:29:28.346406 139998340331328 submission.py:296] 96) loss = 0.377, grad_norm = 0.422
I0402 06:29:28.613207 139946408621824 logging_writer.py:48] [97] global_step=97, grad_norm=0.413447, loss=0.376097
I0402 06:29:28.617371 139998340331328 submission.py:296] 97) loss = 0.376, grad_norm = 0.413
I0402 06:29:28.885135 139946400229120 logging_writer.py:48] [98] global_step=98, grad_norm=0.413286, loss=0.374013
I0402 06:29:28.889596 139998340331328 submission.py:296] 98) loss = 0.374, grad_norm = 0.413
I0402 06:29:29.157841 139946408621824 logging_writer.py:48] [99] global_step=99, grad_norm=0.412500, loss=0.372587
I0402 06:29:29.161821 139998340331328 submission.py:296] 99) loss = 0.373, grad_norm = 0.412
I0402 06:29:29.430604 139946400229120 logging_writer.py:48] [100] global_step=100, grad_norm=0.413668, loss=0.366981
I0402 06:29:29.434228 139998340331328 submission.py:296] 100) loss = 0.367, grad_norm = 0.414
I0402 06:31:14.814349 139946408621824 logging_writer.py:48] [500] global_step=500, grad_norm=0.030272, loss=0.056373
I0402 06:31:14.818499 139998340331328 submission.py:296] 500) loss = 0.056, grad_norm = 0.030
I0402 06:33:02.397382 139998340331328 submission_runner.py:373] Before eval at step 912: RAM USED (GB) 26.90875392
I0402 06:33:02.397606 139998340331328 spec.py:298] Evaluating on the training split.
W0402 06:33:16.449611 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.750766 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.750931 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.756074 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.756597 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.756912 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.757766 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:16.758027 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.505326 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.747050 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.747580 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.751986 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.752499 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.753234 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.753497 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:30.753882 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:33:56.191120 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 06:33:56.658334 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.944431 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.944630 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.950677 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.951011 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.951517 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.951658 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:56.952098 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.141733 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.400448 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.400588 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.405894 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.406281 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.407085 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.407397 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:33:57.413272 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:34:00.002855 139998340331328 spec.py:326] Evaluating on the test split.
W0402 06:34:00.473462 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.746447 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.747295 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.752337 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.752594 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.753267 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.753373 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.753735 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:00.957363 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.214025 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.214318 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.219722 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.220213 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.220594 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.220895 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:34:01.221392 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:34:03.775178 139998340331328 submission_runner.py:382] Time since start: 385.43s, 	Step: 912, 	{'train/accuracy': 0.9871286842162773, 'train/loss': 0.049570582807064056, 'train/mean_average_precision': 0.06578812754522145, 'validation/accuracy': 0.9844528771858891, 'validation/loss': 0.0591106116771698, 'validation/mean_average_precision': 0.06456644582363333, 'validation/num_examples': 43793, 'test/accuracy': 0.9834512608252299, 'test/loss': 0.06227148324251175, 'test/mean_average_precision': 0.06742294098725984, 'test/num_examples': 43793}
I0402 06:34:03.775595 139998340331328 submission_runner.py:396] After eval at step 912: RAM USED (GB) 28.087959552
I0402 06:34:03.783819 139946400229120 logging_writer.py:48] [912] global_step=912, preemption_count=0, score=244.806805, test/accuracy=0.983451, test/loss=0.062271, test/mean_average_precision=0.067423, test/num_examples=43793, total_duration=385.430212, train/accuracy=0.987129, train/loss=0.049571, train/mean_average_precision=0.065788, validation/accuracy=0.984453, validation/loss=0.059111, validation/mean_average_precision=0.064566, validation/num_examples=43793
I0402 06:34:03.872556 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_912.
I0402 06:34:03.873103 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 912: RAM USED (GB) 28.087246848
I0402 06:34:27.037419 139946408621824 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.060496, loss=0.047229
I0402 06:34:27.041923 139998340331328 submission.py:296] 1000) loss = 0.047, grad_norm = 0.060
I0402 06:36:38.862795 139946400229120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.031822, loss=0.047532
I0402 06:36:38.867978 139998340331328 submission.py:296] 1500) loss = 0.048, grad_norm = 0.032
I0402 06:38:03.961249 139998340331328 submission_runner.py:373] Before eval at step 1819: RAM USED (GB) 28.345540608
I0402 06:38:03.961467 139998340331328 spec.py:298] Evaluating on the training split.
W0402 06:38:17.886870 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.144780 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.144849 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.148712 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.149903 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.150440 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.150770 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:18.151492 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.105708 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.350607 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.351794 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.356085 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.358030 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.358175 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.358168 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:32.358176 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:38:58.874124 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 06:38:59.324804 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.591381 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.591792 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.597257 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.597883 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.597852 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.598692 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.599763 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:38:59.815222 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.080588 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.081276 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.086399 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.087111 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.087588 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.087683 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:00.087779 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:39:02.691337 139998340331328 spec.py:326] Evaluating on the test split.
W0402 06:39:03.159044 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.422855 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.423570 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.428373 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.429164 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.429230 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.429912 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.430403 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.647780 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.903372 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.903451 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.908701 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.908883 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.909697 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.909793 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:39:03.915669 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:39:06.492399 139998340331328 submission_runner.py:382] Time since start: 686.99s, 	Step: 1819, 	{'train/accuracy': 0.9876425047462227, 'train/loss': 0.04444033280014992, 'train/mean_average_precision': 0.1394078148382998, 'validation/accuracy': 0.9847427188446602, 'validation/loss': 0.05460172891616821, 'validation/mean_average_precision': 0.13038959961400035, 'validation/num_examples': 43793, 'test/accuracy': 0.9837793714848672, 'test/loss': 0.05764413997530937, 'test/mean_average_precision': 0.12404389671383546, 'test/num_examples': 43793}
I0402 06:39:06.492861 139998340331328 submission_runner.py:396] After eval at step 1819: RAM USED (GB) 28.952932352
I0402 06:39:06.501977 139946408621824 logging_writer.py:48] [1819] global_step=1819, preemption_count=0, score=483.939406, test/accuracy=0.983779, test/loss=0.057644, test/mean_average_precision=0.124044, test/num_examples=43793, total_duration=686.994164, train/accuracy=0.987643, train/loss=0.044440, train/mean_average_precision=0.139408, validation/accuracy=0.984743, validation/loss=0.054602, validation/mean_average_precision=0.130390, validation/num_examples=43793
I0402 06:39:06.595788 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_1819.
I0402 06:39:06.596402 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 1819: RAM USED (GB) 28.952481792
I0402 06:39:56.168900 139946400229120 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.036633, loss=0.047816
I0402 06:39:56.174360 139998340331328 submission.py:296] 2000) loss = 0.048, grad_norm = 0.037
I0402 06:42:08.502029 139946408621824 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.035399, loss=0.043016
I0402 06:42:08.506746 139998340331328 submission.py:296] 2500) loss = 0.043, grad_norm = 0.035
I0402 06:43:06.833184 139998340331328 submission_runner.py:373] Before eval at step 2721: RAM USED (GB) 29.340696576
I0402 06:43:06.833395 139998340331328 spec.py:298] Evaluating on the training split.
W0402 06:43:21.083139 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.343761 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.344113 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.350477 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.350513 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.351010 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.351037 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:21.351119 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.271455 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.511382 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.511550 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.517084 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.517277 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.517631 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.517973 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:43:35.518814 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:44:01.636471 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 06:44:02.108314 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.377320 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.377336 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.382847 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.382935 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.383982 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.384125 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.384193 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.597165 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.854552 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.855429 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.861298 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.861500 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.861733 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.861793 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:02.862263 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:44:05.473313 139998340331328 spec.py:326] Evaluating on the test split.
W0402 06:44:05.938906 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.201516 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.201778 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.206036 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.207977 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.207994 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.208466 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.208867 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.421119 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.681211 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.681318 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.687139 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.687184 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.687807 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.687987 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:44:06.688180 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:44:09.239631 139998340331328 submission_runner.py:382] Time since start: 989.87s, 	Step: 2721, 	{'train/accuracy': 0.9882857731586709, 'train/loss': 0.0414397157728672, 'train/mean_average_precision': 0.18300367197954498, 'validation/accuracy': 0.9854088675309955, 'validation/loss': 0.050607096403837204, 'validation/mean_average_precision': 0.16030979056043396, 'validation/num_examples': 43793, 'test/accuracy': 0.9845295192189029, 'test/loss': 0.05325239524245262, 'test/mean_average_precision': 0.1618501547451667, 'test/num_examples': 43793}
I0402 06:44:09.240052 139998340331328 submission_runner.py:396] After eval at step 2721: RAM USED (GB) 29.770825728
I0402 06:44:09.248257 139946400229120 logging_writer.py:48] [2721] global_step=2721, preemption_count=0, score=723.190703, test/accuracy=0.984530, test/loss=0.053252, test/mean_average_precision=0.161850, test/num_examples=43793, total_duration=989.866096, train/accuracy=0.988286, train/loss=0.041440, train/mean_average_precision=0.183004, validation/accuracy=0.985409, validation/loss=0.050607, validation/mean_average_precision=0.160310, validation/num_examples=43793
I0402 06:44:09.338249 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_2721.
I0402 06:44:09.338745 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 2721: RAM USED (GB) 29.770145792
I0402 06:45:25.388600 139946408621824 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.045271, loss=0.047705
I0402 06:45:25.394087 139998340331328 submission.py:296] 3000) loss = 0.048, grad_norm = 0.045
I0402 06:47:38.884083 139946400229120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.021024, loss=0.041503
I0402 06:47:38.889663 139998340331328 submission.py:296] 3500) loss = 0.042, grad_norm = 0.021
I0402 06:48:09.578142 139998340331328 submission_runner.py:373] Before eval at step 3617: RAM USED (GB) 30.06711808
I0402 06:48:09.578346 139998340331328 spec.py:298] Evaluating on the training split.
W0402 06:48:23.618763 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.896889 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.897355 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.902054 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.902149 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.902692 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.902948 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:23.903577 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:37.990222 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.237833 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.238290 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.243025 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.243386 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.243855 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.245000 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:48:38.245671 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:49:05.466136 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 06:49:05.941963 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.211225 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.211430 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.217121 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.217463 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.217920 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.217936 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.218242 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.428087 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.685702 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.686316 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.691336 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.691613 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.692117 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.692290 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:06.693457 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:49:09.287533 139998340331328 spec.py:326] Evaluating on the test split.
W0402 06:49:09.751634 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.011293 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.011554 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.015358 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.017281 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.017435 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.017677 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.018703 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.233608 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.493997 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.494649 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.499242 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.499509 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.500383 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.500808 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:49:10.501039 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:49:13.067731 139998340331328 submission_runner.py:382] Time since start: 1292.61s, 	Step: 3617, 	{'train/accuracy': 0.9883858788749408, 'train/loss': 0.040221668779850006, 'train/mean_average_precision': 0.2042542137447393, 'validation/accuracy': 0.9855192834010037, 'validation/loss': 0.04959772527217865, 'validation/mean_average_precision': 0.17541747919631373, 'validation/num_examples': 43793, 'test/accuracy': 0.984575008244886, 'test/loss': 0.052396588027477264, 'test/mean_average_precision': 0.1723116208051774, 'test/num_examples': 43793}
I0402 06:49:13.068113 139998340331328 submission_runner.py:396] After eval at step 3617: RAM USED (GB) 30.698582016
I0402 06:49:13.076942 139946408621824 logging_writer.py:48] [3617] global_step=3617, preemption_count=0, score=962.473727, test/accuracy=0.984575, test/loss=0.052397, test/mean_average_precision=0.172312, test/num_examples=43793, total_duration=1292.611090, train/accuracy=0.988386, train/loss=0.040222, train/mean_average_precision=0.204254, validation/accuracy=0.985519, validation/loss=0.049598, validation/mean_average_precision=0.175417, validation/num_examples=43793
I0402 06:49:13.166628 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_3617.
I0402 06:49:13.167109 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 3617: RAM USED (GB) 30.698151936
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 06:50:55.096668 139946400229120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.035246, loss=0.043931
I0402 06:50:55.104080 139998340331328 submission.py:296] 4000) loss = 0.044, grad_norm = 0.035
I0402 06:53:07.515146 139946408621824 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016722, loss=0.042046
I0402 06:53:07.519778 139998340331328 submission.py:296] 4500) loss = 0.042, grad_norm = 0.017
I0402 06:53:13.429062 139998340331328 submission_runner.py:373] Before eval at step 4523: RAM USED (GB) 30.862647296
I0402 06:53:13.429255 139998340331328 spec.py:298] Evaluating on the training split.
W0402 06:53:27.767129 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.016557 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.017408 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.022858 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.023291 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.023778 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.023938 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:28.027272 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.085276 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.331760 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.331915 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.337937 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.338530 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.338518 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.338804 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:53:42.339107 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:54:09.768016 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 06:54:10.236819 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.495386 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.496657 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.501670 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.501740 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.502684 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.503244 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.503376 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.697264 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.951894 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.952196 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.958207 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.958260 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.958531 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.958906 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:10.966333 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:54:13.584306 139998340331328 spec.py:326] Evaluating on the test split.
W0402 06:54:14.027767 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.290892 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.290970 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.295762 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.296732 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.297077 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.297129 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.297368 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.514373 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.777532 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.777596 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.783527 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.783799 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.784097 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.784653 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:54:14.784827 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:54:17.333813 139998340331328 submission_runner.py:382] Time since start: 1596.46s, 	Step: 4523, 	{'train/accuracy': 0.9887212710832881, 'train/loss': 0.038899507373571396, 'train/mean_average_precision': 0.23914718772494253, 'validation/accuracy': 0.9857104814700249, 'validation/loss': 0.048744603991508484, 'validation/mean_average_precision': 0.19713358825488772, 'validation/num_examples': 43793, 'test/accuracy': 0.9847615974903536, 'test/loss': 0.05170080065727234, 'test/mean_average_precision': 0.19351582563842917, 'test/num_examples': 43793}
I0402 06:54:17.334187 139998340331328 submission_runner.py:396] After eval at step 4523: RAM USED (GB) 31.264636928
I0402 06:54:17.342450 139946400229120 logging_writer.py:48] [4523] global_step=4523, preemption_count=0, score=1201.741250, test/accuracy=0.984762, test/loss=0.051701, test/mean_average_precision=0.193516, test/num_examples=43793, total_duration=1596.462022, train/accuracy=0.988721, train/loss=0.038900, train/mean_average_precision=0.239147, validation/accuracy=0.985710, validation/loss=0.048745, validation/mean_average_precision=0.197134, validation/num_examples=43793
I0402 06:54:17.432133 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_4523.
I0402 06:54:17.432684 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 4523: RAM USED (GB) 31.264186368
I0402 06:56:25.400085 139946408621824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.013078, loss=0.038754
I0402 06:56:25.406665 139998340331328 submission.py:296] 5000) loss = 0.039, grad_norm = 0.013
I0402 06:58:17.515235 139998340331328 submission_runner.py:373] Before eval at step 5422: RAM USED (GB) 31.38056192
I0402 06:58:17.515466 139998340331328 spec.py:298] Evaluating on the training split.
W0402 06:58:32.788977 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.035672 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.035719 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.039951 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.041322 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.041609 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.042151 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:33.042776 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.706607 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.939531 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.940556 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.945537 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.946320 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.946618 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.947343 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:58:47.947401 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:59:15.634544 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 06:59:16.094473 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.356732 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.356777 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.362466 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.363240 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.363359 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.363571 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.364391 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.551291 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.811485 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.811499 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.816322 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.818393 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.818573 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.818609 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:16.819097 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:59:19.399910 139998340331328 spec.py:326] Evaluating on the test split.
W0402 06:59:19.837598 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.099114 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.099495 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.104290 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.105724 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.105984 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.106253 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.106630 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.301076 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.558284 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.558452 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.563324 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.565029 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.565067 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.565205 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 06:59:20.565257 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 06:59:23.072518 139998340331328 submission_runner.py:382] Time since start: 1900.55s, 	Step: 5422, 	{'train/accuracy': 0.988943037532004, 'train/loss': 0.03796296939253807, 'train/mean_average_precision': 0.245261632520202, 'validation/accuracy': 0.9860047884764802, 'validation/loss': 0.047486886382102966, 'validation/mean_average_precision': 0.2117968370380153, 'validation/num_examples': 43793, 'test/accuracy': 0.9851326700078638, 'test/loss': 0.05022045224905014, 'test/mean_average_precision': 0.20405574647718477, 'test/num_examples': 43793}
I0402 06:59:23.072898 139998340331328 submission_runner.py:396] After eval at step 5422: RAM USED (GB) 31.731716096
I0402 06:59:23.081556 139946400229120 logging_writer.py:48] [5422] global_step=5422, preemption_count=0, score=1440.823815, test/accuracy=0.985133, test/loss=0.050220, test/mean_average_precision=0.204056, test/num_examples=43793, total_duration=1900.548127, train/accuracy=0.988943, train/loss=0.037963, train/mean_average_precision=0.245262, validation/accuracy=0.986005, validation/loss=0.047487, validation/mean_average_precision=0.211797, validation/num_examples=43793
I0402 06:59:23.172866 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_5422.
I0402 06:59:23.173383 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 5422: RAM USED (GB) 31.699816448
I0402 06:59:44.418302 139946408621824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013473, loss=0.039735
I0402 06:59:44.424325 139998340331328 submission.py:296] 5500) loss = 0.040, grad_norm = 0.013
I0402 07:01:56.798714 139998340331328 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 31.797571584
I0402 07:01:56.799022 139998340331328 spec.py:298] Evaluating on the training split.
W0402 07:02:11.611922 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.861010 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.861238 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.866571 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.866607 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.867810 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.867882 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:11.868396 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.481436 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.729506 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.729863 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.734887 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.735940 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.736730 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.736749 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:26.737104 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 07:02:53.726281 139998340331328 spec.py:310] Evaluating on the validation split.
W0402 07:02:54.185984 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.444645 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.444856 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.450324 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.450835 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.451254 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.451325 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.451418 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.656955 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.922243 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.922320 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.927810 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.928442 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.928829 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.928923 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:54.929567 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 07:02:57.535022 139998340331328 spec.py:326] Evaluating on the test split.
W0402 07:02:57.988791 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.247704 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.247858 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.252549 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.253740 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.253771 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.253865 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.253959 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.471601 139998340331328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.728380 139947090044736 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.728879 140140975179584 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.734404 139807450314560 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.734802 139956317890368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.735530 140526076802880 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.735955 140567748110144 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0402 07:02:58.736068 139748737615680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0402 07:03:01.285937 139998340331328 submission_runner.py:382] Time since start: 2119.83s, 	Step: 6000, 	{'train/accuracy': 0.9894013731237582, 'train/loss': 0.035978514701128006, 'train/mean_average_precision': 0.27725905029807607, 'validation/accuracy': 0.9862248083350992, 'validation/loss': 0.046435900032520294, 'validation/mean_average_precision': 0.22304817330676552, 'validation/num_examples': 43793, 'test/accuracy': 0.985314204917111, 'test/loss': 0.04924708604812622, 'test/mean_average_precision': 0.21688310993718976, 'test/num_examples': 43793}
I0402 07:03:01.286311 139998340331328 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 32.145563648
I0402 07:03:01.295638 139946400229120 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1593.818165, test/accuracy=0.985314, test/loss=0.049247, test/mean_average_precision=0.216883, test/num_examples=43793, total_duration=2119.831394, train/accuracy=0.989401, train/loss=0.035979, train/mean_average_precision=0.277259, validation/accuracy=0.986225, validation/loss=0.046436, validation/mean_average_precision=0.223048, validation/num_examples=43793
I0402 07:03:01.384083 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0402 07:03:01.384589 139998340331328 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 32.145108992
I0402 07:03:01.391851 139946408621824 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1593.818165
I0402 07:03:01.546403 139998340331328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0402 07:03:01.718395 139998340331328 submission_runner.py:550] Tuning trial 1/1
I0402 07:03:01.718667 139998340331328 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 07:03:01.719924 139998340331328 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4312884355723088, 'train/loss': 0.8213552236557007, 'train/mean_average_precision': 0.021203012728293308, 'validation/accuracy': 0.4418780196913714, 'validation/loss': 0.8105841279029846, 'validation/mean_average_precision': 0.0256216969824728, 'validation/num_examples': 43793, 'test/accuracy': 0.4454173807671556, 'test/loss': 0.8078605532646179, 'test/mean_average_precision': 0.02708983913236362, 'test/num_examples': 43793, 'score': 5.566148519515991, 'total_duration': 5.568255662918091, 'global_step': 1, 'preemption_count': 0}), (912, {'train/accuracy': 0.9871286842162773, 'train/loss': 0.049570582807064056, 'train/mean_average_precision': 0.06578812754522145, 'validation/accuracy': 0.9844528771858891, 'validation/loss': 0.0591106116771698, 'validation/mean_average_precision': 0.06456644582363333, 'validation/num_examples': 43793, 'test/accuracy': 0.9834512608252299, 'test/loss': 0.06227148324251175, 'test/mean_average_precision': 0.06742294098725984, 'test/num_examples': 43793, 'score': 244.806804895401, 'total_duration': 385.43021178245544, 'global_step': 912, 'preemption_count': 0}), (1819, {'train/accuracy': 0.9876425047462227, 'train/loss': 0.04444033280014992, 'train/mean_average_precision': 0.1394078148382998, 'validation/accuracy': 0.9847427188446602, 'validation/loss': 0.05460172891616821, 'validation/mean_average_precision': 0.13038959961400035, 'validation/num_examples': 43793, 'test/accuracy': 0.9837793714848672, 'test/loss': 0.05764413997530937, 'test/mean_average_precision': 0.12404389671383546, 'test/num_examples': 43793, 'score': 483.93940591812134, 'total_duration': 686.9941639900208, 'global_step': 1819, 'preemption_count': 0}), (2721, {'train/accuracy': 0.9882857731586709, 'train/loss': 0.0414397157728672, 'train/mean_average_precision': 0.18300367197954498, 'validation/accuracy': 0.9854088675309955, 'validation/loss': 0.050607096403837204, 'validation/mean_average_precision': 0.16030979056043396, 'validation/num_examples': 43793, 'test/accuracy': 0.9845295192189029, 'test/loss': 0.05325239524245262, 'test/mean_average_precision': 0.1618501547451667, 'test/num_examples': 43793, 'score': 723.1907026767731, 'total_duration': 989.8660957813263, 'global_step': 2721, 'preemption_count': 0}), (3617, {'train/accuracy': 0.9883858788749408, 'train/loss': 0.040221668779850006, 'train/mean_average_precision': 0.2042542137447393, 'validation/accuracy': 0.9855192834010037, 'validation/loss': 0.04959772527217865, 'validation/mean_average_precision': 0.17541747919631373, 'validation/num_examples': 43793, 'test/accuracy': 0.984575008244886, 'test/loss': 0.052396588027477264, 'test/mean_average_precision': 0.1723116208051774, 'test/num_examples': 43793, 'score': 962.4737272262573, 'total_duration': 1292.611089706421, 'global_step': 3617, 'preemption_count': 0}), (4523, {'train/accuracy': 0.9887212710832881, 'train/loss': 0.038899507373571396, 'train/mean_average_precision': 0.23914718772494253, 'validation/accuracy': 0.9857104814700249, 'validation/loss': 0.048744603991508484, 'validation/mean_average_precision': 0.19713358825488772, 'validation/num_examples': 43793, 'test/accuracy': 0.9847615974903536, 'test/loss': 0.05170080065727234, 'test/mean_average_precision': 0.19351582563842917, 'test/num_examples': 43793, 'score': 1201.7412502765656, 'total_duration': 1596.462022304535, 'global_step': 4523, 'preemption_count': 0}), (5422, {'train/accuracy': 0.988943037532004, 'train/loss': 0.03796296939253807, 'train/mean_average_precision': 0.245261632520202, 'validation/accuracy': 0.9860047884764802, 'validation/loss': 0.047486886382102966, 'validation/mean_average_precision': 0.2117968370380153, 'validation/num_examples': 43793, 'test/accuracy': 0.9851326700078638, 'test/loss': 0.05022045224905014, 'test/mean_average_precision': 0.20405574647718477, 'test/num_examples': 43793, 'score': 1440.8238146305084, 'total_duration': 1900.548127412796, 'global_step': 5422, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9894013731237582, 'train/loss': 0.035978514701128006, 'train/mean_average_precision': 0.27725905029807607, 'validation/accuracy': 0.9862248083350992, 'validation/loss': 0.046435900032520294, 'validation/mean_average_precision': 0.22304817330676552, 'validation/num_examples': 43793, 'test/accuracy': 0.985314204917111, 'test/loss': 0.04924708604812622, 'test/mean_average_precision': 0.21688310993718976, 'test/num_examples': 43793, 'score': 1593.8181648254395, 'total_duration': 2119.8313937187195, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0402 07:03:01.720050 139998340331328 submission_runner.py:553] Timing: 1593.8181648254395
I0402 07:03:01.720100 139998340331328 submission_runner.py:554] ====================
I0402 07:03:01.720200 139998340331328 submission_runner.py:613] Final ogbg score: 1593.8181648254395
