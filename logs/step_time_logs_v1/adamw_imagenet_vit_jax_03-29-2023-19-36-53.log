I0329 19:37:13.642860 139798077798208 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/imagenet_vit_jax.
I0329 19:37:13.690049 139798077798208 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 19:37:14.549063 139798077798208 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0329 19:37:14.549718 139798077798208 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 19:37:14.556041 139798077798208 submission_runner.py:504] Using RNG seed 4251038821
I0329 19:37:16.901959 139798077798208 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 19:37:16.902178 139798077798208 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1.
I0329 19:37:16.902345 139798077798208 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/hparams.json.
I0329 19:37:17.042852 139798077798208 submission_runner.py:230] Starting train once: RAM USED (GB) 4.418072576
I0329 19:37:17.043019 139798077798208 submission_runner.py:231] Initializing dataset.
I0329 19:37:17.057981 139798077798208 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:37:17.067174 139798077798208 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 19:37:17.067284 139798077798208 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 19:37:17.336037 139798077798208 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:37:25.780899 139798077798208 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.581879808
I0329 19:37:25.781115 139798077798208 submission_runner.py:240] Initializing model.
I0329 19:37:37.743114 139798077798208 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.585633792
I0329 19:37:37.743348 139798077798208 submission_runner.py:252] Initializing optimizer.
I0329 19:37:38.456592 139798077798208 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.586141696
I0329 19:37:38.456765 139798077798208 submission_runner.py:261] Initializing metrics bundle.
I0329 19:37:38.456816 139798077798208 submission_runner.py:275] Initializing checkpoint and logger.
I0329 19:37:38.457708 139798077798208 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0329 19:37:39.349035 139798077798208 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0329 19:37:39.350000 139798077798208 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/flags_0.json.
I0329 19:37:39.354144 139798077798208 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.584413184
I0329 19:37:39.354388 139798077798208 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.584413184
I0329 19:37:39.354452 139798077798208 submission_runner.py:312] Starting training loop.
I0329 19:37:42.580054 139798077798208 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 14.405718016
I0329 19:38:30.994379 139622097086208 logging_writer.py:48] [0] global_step=0, grad_norm=0.3201664388179779, loss=6.9077534675598145
I0329 19:38:31.008227 139798077798208 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 46.718926848
I0329 19:38:31.008518 139798077798208 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 46.718926848
I0329 19:38:31.008612 139798077798208 spec.py:298] Evaluating on the training split.
I0329 19:38:31.016372 139798077798208 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:38:31.025216 139798077798208 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 19:38:31.025328 139798077798208 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 19:38:31.101953 139798077798208 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:38:51.912808 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 19:38:51.922184 139798077798208 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:38:51.937249 139798077798208 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 19:38:51.937555 139798077798208 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 19:38:52.009042 139798077798208 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:39:12.123730 139798077798208 spec.py:326] Evaluating on the test split.
I0329 19:39:12.130126 139798077798208 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 19:39:12.135182 139798077798208 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0329 19:39:12.168166 139798077798208 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 19:39:23.603019 139798077798208 submission_runner.py:380] Time since start: 51.65s, 	Step: 1, 	{'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0329 19:39:23.603439 139798077798208 submission_runner.py:390] After eval at step 1: RAM USED (GB) 106.11726336
I0329 19:39:23.612003 139562949015296 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=51.566557, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=51.654087, train/accuracy=0.000996, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0329 19:39:23.809503 139798077798208 checkpoints.py:356] Saving checkpoint at step: 1
I0329 19:39:24.580804 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1
I0329 19:39:24.589597 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1.
I0329 19:39:24.591903 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 106.26832384
I0329 19:39:24.596203 139798077798208 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 106.26832384
I0329 19:39:43.621764 139798077798208 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 105.682812928
I0329 19:40:23.006606 139616543790848 logging_writer.py:48] [100] global_step=100, grad_norm=0.45036327838897705, loss=6.897760391235352
I0329 19:41:05.332564 139616552183552 logging_writer.py:48] [200] global_step=200, grad_norm=0.5115023851394653, loss=6.840587615966797
I0329 19:41:50.776556 139616543790848 logging_writer.py:48] [300] global_step=300, grad_norm=0.6331462860107422, loss=6.805296897888184
I0329 19:42:36.371276 139616552183552 logging_writer.py:48] [400] global_step=400, grad_norm=0.8544537425041199, loss=6.728946208953857
I0329 19:43:22.035650 139616543790848 logging_writer.py:48] [500] global_step=500, grad_norm=0.680221438407898, loss=6.680365562438965
I0329 19:44:07.518895 139616552183552 logging_writer.py:48] [600] global_step=600, grad_norm=0.9606902599334717, loss=6.705958843231201
I0329 19:44:53.181209 139616543790848 logging_writer.py:48] [700] global_step=700, grad_norm=0.8202581405639648, loss=6.492341041564941
I0329 19:45:38.498923 139616552183552 logging_writer.py:48] [800] global_step=800, grad_norm=0.9370760917663574, loss=6.468149185180664
I0329 19:46:24.137244 139616543790848 logging_writer.py:48] [900] global_step=900, grad_norm=1.0400323867797852, loss=6.415043830871582
I0329 19:46:24.700092 139798077798208 submission_runner.py:371] Before eval at step 903: RAM USED (GB) 79.818481664
I0329 19:46:24.700494 139798077798208 spec.py:298] Evaluating on the training split.
I0329 19:46:36.625873 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 19:46:45.057104 139798077798208 spec.py:326] Evaluating on the test split.
I0329 19:46:46.926677 139798077798208 submission_runner.py:380] Time since start: 525.34s, 	Step: 903, 	{'train/accuracy': 0.02285156212747097, 'train/loss': 6.152990818023682, 'validation/accuracy': 0.022760000079870224, 'validation/loss': 6.166991710662842, 'validation/num_examples': 50000, 'test/accuracy': 0.017000000923871994, 'test/loss': 6.2465291023254395, 'test/num_examples': 10000}
I0329 19:46:46.927146 139798077798208 submission_runner.py:390] After eval at step 903: RAM USED (GB) 83.269799936
I0329 19:46:46.941264 139563209058048 logging_writer.py:48] [903] global_step=903, preemption_count=0, score=465.248224, test/accuracy=0.017000, test/loss=6.246529, test/num_examples=10000, total_duration=525.341235, train/accuracy=0.022852, train/loss=6.152991, validation/accuracy=0.022760, validation/loss=6.166992, validation/num_examples=50000
I0329 19:46:47.337692 139798077798208 checkpoints.py:356] Saving checkpoint at step: 903
I0329 19:46:48.680017 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_903
I0329 19:46:48.699116 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_903.
I0329 19:46:48.702489 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 903: RAM USED (GB) 90.543857664
I0329 19:47:28.626160 139563217450752 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.5968005657196045, loss=6.401980400085449
I0329 19:48:14.132644 139622466152192 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0806405544281006, loss=6.4957475662231445
I0329 19:48:59.268681 139563217450752 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8477105498313904, loss=6.291611671447754
I0329 19:49:45.306533 139622466152192 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0348334312438965, loss=6.257883548736572
I0329 19:50:30.959791 139563217450752 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8644177913665771, loss=6.736940383911133
I0329 19:51:16.546121 139622466152192 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9337952136993408, loss=6.252498149871826
I0329 19:52:02.207346 139563217450752 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1830965280532837, loss=6.226095676422119
I0329 19:52:47.664903 139622466152192 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.5170997381210327, loss=6.285784721374512
I0329 19:53:33.042923 139563217450752 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0000536441802979, loss=6.084324836730957
I0329 19:53:48.828048 139798077798208 submission_runner.py:371] Before eval at step 1836: RAM USED (GB) 86.08043008
I0329 19:53:48.828367 139798077798208 spec.py:298] Evaluating on the training split.
I0329 19:54:01.367665 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 19:54:10.061957 139798077798208 spec.py:326] Evaluating on the test split.
I0329 19:54:11.776478 139798077798208 submission_runner.py:380] Time since start: 969.47s, 	Step: 1836, 	{'train/accuracy': 0.05386718735098839, 'train/loss': 5.592177391052246, 'validation/accuracy': 0.05069999769330025, 'validation/loss': 5.6237711906433105, 'validation/num_examples': 50000, 'test/accuracy': 0.04270000383257866, 'test/loss': 5.782311916351318, 'test/num_examples': 10000}
I0329 19:54:11.776980 139798077798208 submission_runner.py:390] After eval at step 1836: RAM USED (GB) 90.017865728
I0329 19:54:11.789469 139622466152192 logging_writer.py:48] [1836] global_step=1836, preemption_count=0, score=878.625364, test/accuracy=0.042700, test/loss=5.782312, test/num_examples=10000, total_duration=969.470169, train/accuracy=0.053867, train/loss=5.592177, validation/accuracy=0.050700, validation/loss=5.623771, validation/num_examples=50000
I0329 19:54:12.090601 139798077798208 checkpoints.py:356] Saving checkpoint at step: 1836
I0329 19:54:13.367323 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1836
I0329 19:54:13.389842 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1836.
I0329 19:54:13.393091 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 1836: RAM USED (GB) 96.745271296
I0329 19:54:39.596017 139563217450752 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8440736532211304, loss=6.2994608879089355
I0329 19:55:24.359182 139622432581376 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.047607660293579, loss=6.0095367431640625
I0329 19:56:10.098723 139563217450752 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9448375701904297, loss=6.564377784729004
I0329 19:56:56.037321 139622432581376 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9245306253433228, loss=6.0271735191345215
I0329 19:57:42.204534 139563217450752 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0133942365646362, loss=6.074599742889404
I0329 19:58:27.995218 139622432581376 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8859133720397949, loss=5.854684829711914
I0329 19:59:14.277329 139563217450752 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7529391646385193, loss=6.570796012878418
I0329 20:00:00.455398 139622432581376 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8537126779556274, loss=6.235134124755859
I0329 20:00:46.614080 139563217450752 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8946308493614197, loss=5.818584442138672
I0329 20:01:13.576391 139798077798208 submission_runner.py:371] Before eval at step 2760: RAM USED (GB) 92.613902336
I0329 20:01:13.576695 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:01:27.687958 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:01:37.264105 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:01:38.954009 139798077798208 submission_runner.py:380] Time since start: 1414.22s, 	Step: 2760, 	{'train/accuracy': 0.09880859404802322, 'train/loss': 5.071300506591797, 'validation/accuracy': 0.08816000074148178, 'validation/loss': 5.156027317047119, 'validation/num_examples': 50000, 'test/accuracy': 0.06550000607967377, 'test/loss': 5.386715888977051, 'test/num_examples': 10000}
I0329 20:01:38.954596 139798077798208 submission_runner.py:390] After eval at step 2760: RAM USED (GB) 97.966993408
I0329 20:01:38.966171 139622432581376 logging_writer.py:48] [2760] global_step=2760, preemption_count=0, score=1288.506217, test/accuracy=0.065500, test/loss=5.386716, test/num_examples=10000, total_duration=1414.220554, train/accuracy=0.098809, train/loss=5.071301, validation/accuracy=0.088160, validation/loss=5.156027, validation/num_examples=50000
I0329 20:01:39.168320 139798077798208 checkpoints.py:356] Saving checkpoint at step: 2760
I0329 20:01:40.154013 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_2760
I0329 20:01:40.168562 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_2760.
I0329 20:01:40.180087 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 2760: RAM USED (GB) 102.974410752
I0329 20:01:56.702672 139563217450752 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.1823692321777344, loss=5.8803253173828125
I0329 20:02:40.423425 139622424188672 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.899527907371521, loss=5.97401762008667
I0329 20:03:26.020155 139563217450752 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8153399229049683, loss=6.573940277099609
I0329 20:04:11.377817 139622424188672 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.2722489833831787, loss=5.7962517738342285
I0329 20:04:56.765370 139563217450752 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8254298567771912, loss=5.63665246963501
I0329 20:05:42.239565 139622424188672 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9697288870811462, loss=6.546954154968262
I0329 20:06:27.734062 139563217450752 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.794831395149231, loss=5.743334770202637
I0329 20:07:13.521101 139622424188672 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7513352632522583, loss=6.609301567077637
I0329 20:07:59.192421 139563217450752 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6460220813751221, loss=6.532449722290039
I0329 20:08:40.479933 139798077798208 submission_runner.py:371] Before eval at step 3693: RAM USED (GB) 98.302849024
I0329 20:08:40.480260 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:08:54.949012 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:09:04.883456 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:09:06.588848 139798077798208 submission_runner.py:380] Time since start: 1861.12s, 	Step: 3693, 	{'train/accuracy': 0.14119140803813934, 'train/loss': 4.677453994750977, 'validation/accuracy': 0.13161998987197876, 'validation/loss': 4.7457275390625, 'validation/num_examples': 50000, 'test/accuracy': 0.10220000147819519, 'test/loss': 5.037045955657959, 'test/num_examples': 10000}
I0329 20:09:06.589450 139798077798208 submission_runner.py:390] After eval at step 3693: RAM USED (GB) 104.36251648
I0329 20:09:06.602888 139622424188672 logging_writer.py:48] [3693] global_step=3693, preemption_count=0, score=1701.111942, test/accuracy=0.102200, test/loss=5.037046, test/num_examples=10000, total_duration=1861.121987, train/accuracy=0.141191, train/loss=4.677454, validation/accuracy=0.131620, validation/loss=4.745728, validation/num_examples=50000
I0329 20:09:06.744295 139798077798208 checkpoints.py:356] Saving checkpoint at step: 3693
I0329 20:09:08.006348 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_3693
I0329 20:09:08.020902 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_3693.
I0329 20:09:08.030422 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 3693: RAM USED (GB) 110.448623616
I0329 20:09:11.282341 139563217450752 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6902450919151306, loss=6.4253644943237305
I0329 20:09:53.750290 139622214518528 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0713061094284058, loss=6.640578746795654
I0329 20:10:39.487839 139563217450752 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.900793731212616, loss=6.359081745147705
I0329 20:11:25.122725 139622214518528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6398481130599976, loss=6.531044960021973
I0329 20:12:10.691706 139563217450752 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.696344256401062, loss=6.540947437286377
I0329 20:12:56.432796 139622214518528 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8276063203811646, loss=5.6621785163879395
I0329 20:13:41.923070 139563217450752 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9671986699104309, loss=5.369907379150391
I0329 20:14:27.522210 139622214518528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7145335078239441, loss=6.081658363342285
I0329 20:15:13.247272 139563217450752 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0145260095596313, loss=5.480731964111328
I0329 20:15:58.927001 139622214518528 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8727405667304993, loss=5.264227390289307
I0329 20:16:08.214043 139798077798208 submission_runner.py:371] Before eval at step 4622: RAM USED (GB) 104.238501888
I0329 20:16:08.214329 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:16:22.826845 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:16:33.077678 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:16:34.761988 139798077798208 submission_runner.py:380] Time since start: 2308.86s, 	Step: 4622, 	{'train/accuracy': 0.17863281071186066, 'train/loss': 4.371862888336182, 'validation/accuracy': 0.167619988322258, 'validation/loss': 4.456496238708496, 'validation/num_examples': 50000, 'test/accuracy': 0.12490000575780869, 'test/loss': 4.812922477722168, 'test/num_examples': 10000}
I0329 20:16:34.762565 139798077798208 submission_runner.py:390] After eval at step 4622: RAM USED (GB) 111.753969664
I0329 20:16:34.776070 139563217450752 logging_writer.py:48] [4622] global_step=4622, preemption_count=0, score=2113.871746, test/accuracy=0.124900, test/loss=4.812922, test/num_examples=10000, total_duration=2308.856508, train/accuracy=0.178633, train/loss=4.371863, validation/accuracy=0.167620, validation/loss=4.456496, validation/num_examples=50000
I0329 20:16:34.989397 139798077798208 checkpoints.py:356] Saving checkpoint at step: 4622
I0329 20:16:36.061721 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_4622
I0329 20:16:36.078624 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_4622.
I0329 20:16:36.081383 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 4622: RAM USED (GB) 117.323563008
I0329 20:17:08.326704 139622214518528 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.156950831413269, loss=5.3188629150390625
I0329 20:17:54.132562 139622206125824 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7994105815887451, loss=5.289494514465332
I0329 20:18:39.516958 139622214518528 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7664972543716431, loss=5.9481201171875
I0329 20:19:25.369858 139622206125824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6692725419998169, loss=6.5152387619018555
I0329 20:20:11.112766 139622214518528 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8315335512161255, loss=6.207032203674316
I0329 20:20:57.116565 139622206125824 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.897705614566803, loss=5.215512752532959
I0329 20:21:43.072060 139622214518528 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7280741930007935, loss=6.013388633728027
I0329 20:22:29.164445 139622206125824 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8354388475418091, loss=5.287297248840332
I0329 20:23:15.235609 139622214518528 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7739211320877075, loss=5.246801853179932
I0329 20:23:36.451407 139798077798208 submission_runner.py:371] Before eval at step 5548: RAM USED (GB) 112.048074752
I0329 20:23:36.451728 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:23:50.436938 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:24:01.002309 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:24:02.692858 139798077798208 submission_runner.py:380] Time since start: 2757.10s, 	Step: 5548, 	{'train/accuracy': 0.20970702171325684, 'train/loss': 4.079505920410156, 'validation/accuracy': 0.18835999071598053, 'validation/loss': 4.21451997756958, 'validation/num_examples': 50000, 'test/accuracy': 0.14570000767707825, 'test/loss': 4.621811389923096, 'test/num_examples': 10000}
I0329 20:24:02.693421 139798077798208 submission_runner.py:390] After eval at step 5548: RAM USED (GB) 116.348383232
I0329 20:24:02.704910 139622206125824 logging_writer.py:48] [5548] global_step=5548, preemption_count=0, score=2525.411401, test/accuracy=0.145700, test/loss=4.621811, test/num_examples=10000, total_duration=2757.095457, train/accuracy=0.209707, train/loss=4.079506, validation/accuracy=0.188360, validation/loss=4.214520, validation/num_examples=50000
I0329 20:24:02.926517 139798077798208 checkpoints.py:356] Saving checkpoint at step: 5548
I0329 20:24:04.060897 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_5548
I0329 20:24:04.077182 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_5548.
I0329 20:24:04.088824 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 5548: RAM USED (GB) 122.19629568
I0329 20:24:25.416435 139622214518528 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8531210422515869, loss=5.139582633972168
I0329 20:25:10.611502 139622189340416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6939437389373779, loss=5.916783809661865
I0329 20:25:56.804301 139622214518528 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6181104779243469, loss=6.14664888381958
I0329 20:26:42.682605 139622189340416 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8079365491867065, loss=5.24762487411499
I0329 20:27:28.569947 139622214518528 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8828932642936707, loss=5.120721340179443
I0329 20:28:14.272433 139622189340416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.699325442314148, loss=5.462763786315918
I0329 20:28:59.842719 139622214518528 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8118857145309448, loss=5.180566310882568
I0329 20:29:46.158612 139622189340416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.748817503452301, loss=5.322914123535156
I0329 20:30:32.027215 139622214518528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6763270497322083, loss=6.08437442779541
I0329 20:31:04.209993 139798077798208 submission_runner.py:371] Before eval at step 6471: RAM USED (GB) 118.022549504
I0329 20:31:04.210310 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:31:18.775556 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:31:27.485104 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:31:29.183530 139798077798208 submission_runner.py:380] Time since start: 3204.85s, 	Step: 6471, 	{'train/accuracy': 0.25605466961860657, 'train/loss': 3.7834465503692627, 'validation/accuracy': 0.24129998683929443, 'validation/loss': 3.8848376274108887, 'validation/num_examples': 50000, 'test/accuracy': 0.1811000108718872, 'test/loss': 4.329239845275879, 'test/num_examples': 10000}
I0329 20:31:29.184277 139798077798208 submission_runner.py:390] After eval at step 6471: RAM USED (GB) 120.378417152
I0329 20:31:29.199912 139622189340416 logging_writer.py:48] [6471] global_step=6471, preemption_count=0, score=2936.105125, test/accuracy=0.181100, test/loss=4.329240, test/num_examples=10000, total_duration=3204.853378, train/accuracy=0.256055, train/loss=3.783447, validation/accuracy=0.241300, validation/loss=3.884838, validation/num_examples=50000
I0329 20:31:29.895722 139798077798208 checkpoints.py:356] Saving checkpoint at step: 6471
I0329 20:31:31.078316 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_6471
I0329 20:31:31.095453 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_6471.
I0329 20:31:31.098882 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 6471: RAM USED (GB) 128.163160064
I0329 20:31:43.247402 139622214518528 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7964484691619873, loss=5.0808424949646
I0329 20:32:27.226554 139622180947712 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7790750861167908, loss=5.023583889007568
I0329 20:33:12.649942 139622214518528 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8041006922721863, loss=4.7988786697387695
I0329 20:33:58.278015 139622180947712 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8462778329849243, loss=5.080259799957275
I0329 20:34:43.496412 139622214518528 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9003297090530396, loss=4.98931360244751
I0329 20:35:29.590535 139622180947712 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5964484810829163, loss=5.748401641845703
I0329 20:36:15.655387 139622214518528 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7039041519165039, loss=4.901993751525879
I0329 20:37:01.610248 139622180947712 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9792237877845764, loss=5.154304504394531
I0329 20:37:47.310817 139622214518528 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5827520489692688, loss=6.321969509124756
I0329 20:38:31.229123 139798077798208 submission_runner.py:371] Before eval at step 7397: RAM USED (GB) 123.907985408
I0329 20:38:31.229422 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:38:45.220475 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:38:55.518529 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:38:57.203051 139798077798208 submission_runner.py:380] Time since start: 3651.87s, 	Step: 7397, 	{'train/accuracy': 0.2892773449420929, 'train/loss': 3.532548666000366, 'validation/accuracy': 0.2705399990081787, 'validation/loss': 3.6441681385040283, 'validation/num_examples': 50000, 'test/accuracy': 0.20920000970363617, 'test/loss': 4.116002559661865, 'test/num_examples': 10000}
I0329 20:38:57.203666 139798077798208 submission_runner.py:390] After eval at step 7397: RAM USED (GB) 127.780155392
I0329 20:38:57.219868 139622180947712 logging_writer.py:48] [7397] global_step=7397, preemption_count=0, score=3347.580792, test/accuracy=0.209200, test/loss=4.116003, test/num_examples=10000, total_duration=3651.872151, train/accuracy=0.289277, train/loss=3.532549, validation/accuracy=0.270540, validation/loss=3.644168, validation/num_examples=50000
I0329 20:38:57.540440 139798077798208 checkpoints.py:356] Saving checkpoint at step: 7397
I0329 20:38:58.610139 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_7397
I0329 20:38:58.630458 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_7397.
I0329 20:38:58.642147 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 7397: RAM USED (GB) 133.584449536
I0329 20:39:00.300069 139622214518528 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7038189768791199, loss=4.731246471405029
I0329 20:39:42.588792 139622172555008 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7510175108909607, loss=4.7431535720825195
I0329 20:40:28.691077 139622214518528 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7156922817230225, loss=4.775669097900391
I0329 20:41:14.484738 139622172555008 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8153277635574341, loss=4.7672247886657715
I0329 20:42:00.403646 139622214518528 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8857480883598328, loss=4.899782657623291
I0329 20:42:46.328611 139622172555008 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7568906545639038, loss=5.3136725425720215
I0329 20:43:32.064517 139622214518528 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8251672983169556, loss=4.73006534576416
I0329 20:44:17.995758 139622172555008 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6783632040023804, loss=4.70476770401001
I0329 20:45:03.938808 139622214518528 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5572258234024048, loss=5.963991641998291
I0329 20:45:49.939548 139622172555008 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.656997561454773, loss=4.894180774688721
I0329 20:45:58.776215 139798077798208 submission_runner.py:371] Before eval at step 8321: RAM USED (GB) 130.294272
I0329 20:45:58.776603 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:46:12.210004 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:46:23.442946 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:46:25.132282 139798077798208 submission_runner.py:380] Time since start: 4099.42s, 	Step: 8321, 	{'train/accuracy': 0.322265625, 'train/loss': 3.3023979663848877, 'validation/accuracy': 0.29420000314712524, 'validation/loss': 3.4662792682647705, 'validation/num_examples': 50000, 'test/accuracy': 0.2249000072479248, 'test/loss': 3.9831655025482178, 'test/num_examples': 10000}
I0329 20:46:25.132886 139798077798208 submission_runner.py:390] After eval at step 8321: RAM USED (GB) 135.790952448
I0329 20:46:25.143435 139622214518528 logging_writer.py:48] [8321] global_step=8321, preemption_count=0, score=3758.508564, test/accuracy=0.224900, test/loss=3.983166, test/num_examples=10000, total_duration=4099.419147, train/accuracy=0.322266, train/loss=3.302398, validation/accuracy=0.294200, validation/loss=3.466279, validation/num_examples=50000
I0329 20:46:25.367053 139798077798208 checkpoints.py:356] Saving checkpoint at step: 8321
I0329 20:46:26.576596 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_8321
I0329 20:46:26.594742 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_8321.
I0329 20:46:26.598085 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 8321: RAM USED (GB) 141.965574144
I0329 20:46:59.389755 139622172555008 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6826327443122864, loss=4.71369743347168
I0329 20:47:44.950854 139622164162304 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9853460788726807, loss=4.566137313842773
I0329 20:48:30.682374 139622172555008 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6706485152244568, loss=5.86215353012085
I0329 20:49:16.237067 139622164162304 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6258963346481323, loss=4.542630672454834
I0329 20:50:02.837118 139622172555008 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6520584225654602, loss=4.544650077819824
I0329 20:50:49.296789 139622164162304 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6183103919029236, loss=4.589404106140137
I0329 20:51:35.300482 139622172555008 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5182210206985474, loss=6.251585006713867
I0329 20:52:21.219968 139622164162304 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6676937937736511, loss=4.901736259460449
I0329 20:53:07.124769 139622172555008 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.45383936166763306, loss=6.2015910148620605
I0329 20:53:26.989518 139798077798208 submission_runner.py:371] Before eval at step 9245: RAM USED (GB) 135.999131648
I0329 20:53:26.989768 139798077798208 spec.py:298] Evaluating on the training split.
I0329 20:53:40.648202 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 20:53:52.159466 139798077798208 spec.py:326] Evaluating on the test split.
I0329 20:53:53.840862 139798077798208 submission_runner.py:380] Time since start: 4547.63s, 	Step: 9245, 	{'train/accuracy': 0.3550976514816284, 'train/loss': 3.102385997772217, 'validation/accuracy': 0.3262999951839447, 'validation/loss': 3.257344961166382, 'validation/num_examples': 50000, 'test/accuracy': 0.2519000172615051, 'test/loss': 3.7855732440948486, 'test/num_examples': 10000}
I0329 20:53:53.841546 139798077798208 submission_runner.py:390] After eval at step 9245: RAM USED (GB) 142.316273664
I0329 20:53:53.856288 139622164162304 logging_writer.py:48] [9245] global_step=9245, preemption_count=0, score=4169.711529, test/accuracy=0.251900, test/loss=3.785573, test/num_examples=10000, total_duration=4547.631982, train/accuracy=0.355098, train/loss=3.102386, validation/accuracy=0.326300, validation/loss=3.257345, validation/num_examples=50000
I0329 20:53:54.053988 139798077798208 checkpoints.py:356] Saving checkpoint at step: 9245
I0329 20:53:55.287294 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_9245
I0329 20:53:55.305974 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_9245.
I0329 20:53:55.309159 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 9245: RAM USED (GB) 148.525658112
I0329 20:54:17.754691 139622172555008 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7114765644073486, loss=4.470853805541992
I0329 20:55:02.553795 139622155769600 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7983125448226929, loss=4.675899505615234
I0329 20:55:48.143231 139622172555008 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6590684056282043, loss=4.620784759521484
I0329 20:56:34.148144 139622155769600 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6909266710281372, loss=5.074742317199707
I0329 20:57:20.063488 139622172555008 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5426302552223206, loss=5.1022515296936035
I0329 20:58:06.317077 139622155769600 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6010408401489258, loss=4.399794578552246
I0329 20:58:52.511068 139622172555008 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5941091179847717, loss=5.989520072937012
I0329 20:59:38.796338 139622155769600 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5319745540618896, loss=5.913597106933594
I0329 21:00:25.037114 139622172555008 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7654849290847778, loss=4.437845706939697
I0329 21:00:55.313472 139798077798208 submission_runner.py:371] Before eval at step 10167: RAM USED (GB) 142.02415104
I0329 21:00:55.313751 139798077798208 spec.py:298] Evaluating on the training split.
I0329 21:01:09.465741 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 21:01:21.154408 139798077798208 spec.py:326] Evaluating on the test split.
I0329 21:01:22.844688 139798077798208 submission_runner.py:380] Time since start: 4995.96s, 	Step: 10167, 	{'train/accuracy': 0.3899804651737213, 'train/loss': 2.9321653842926025, 'validation/accuracy': 0.36142000555992126, 'validation/loss': 3.0764076709747314, 'validation/num_examples': 50000, 'test/accuracy': 0.27800002694129944, 'test/loss': 3.642348289489746, 'test/num_examples': 10000}
I0329 21:01:22.845346 139798077798208 submission_runner.py:390] After eval at step 10167: RAM USED (GB) 146.857885696
I0329 21:01:22.863810 139622155769600 logging_writer.py:48] [10167] global_step=10167, preemption_count=0, score=4579.522311, test/accuracy=0.278000, test/loss=3.642348, test/num_examples=10000, total_duration=4995.956042, train/accuracy=0.389980, train/loss=2.932165, validation/accuracy=0.361420, validation/loss=3.076408, validation/num_examples=50000
I0329 21:01:23.105365 139798077798208 checkpoints.py:356] Saving checkpoint at step: 10167
I0329 21:01:24.225827 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_10167
I0329 21:01:24.249498 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_10167.
I0329 21:01:24.253924 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 10167: RAM USED (GB) 152.74584064
I0329 21:01:37.939800 139622172555008 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6454244256019592, loss=4.333802700042725
I0329 21:02:22.520612 139622080300800 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.671851634979248, loss=4.311929702758789
I0329 21:03:08.009403 139622172555008 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5776680707931519, loss=4.587935924530029
I0329 21:03:53.780928 139622080300800 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5564275979995728, loss=6.09025764465332
I0329 21:04:39.453022 139622172555008 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7422789931297302, loss=4.313689231872559
I0329 21:05:25.399064 139622080300800 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6656046509742737, loss=4.212247848510742
I0329 21:06:11.301043 139622172555008 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5834075808525085, loss=6.191585063934326
I0329 21:06:57.058744 139622080300800 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6528509855270386, loss=4.481682777404785
I0329 21:07:42.678906 139622172555008 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6447676420211792, loss=4.21455717086792
I0329 21:08:24.447161 139798077798208 submission_runner.py:371] Before eval at step 11092: RAM USED (GB) 148.02599936
I0329 21:08:24.447531 139798077798208 spec.py:298] Evaluating on the training split.
I0329 21:08:38.752644 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 21:08:50.856210 139798077798208 spec.py:326] Evaluating on the test split.
I0329 21:08:52.548508 139798077798208 submission_runner.py:380] Time since start: 5445.09s, 	Step: 11092, 	{'train/accuracy': 0.4138281047344208, 'train/loss': 2.881742477416992, 'validation/accuracy': 0.38312000036239624, 'validation/loss': 3.0399296283721924, 'validation/num_examples': 50000, 'test/accuracy': 0.2907000184059143, 'test/loss': 3.5877678394317627, 'test/num_examples': 10000}
I0329 21:08:52.549081 139798077798208 submission_runner.py:390] After eval at step 11092: RAM USED (GB) 153.120395264
I0329 21:08:52.562356 139622080300800 logging_writer.py:48] [11092] global_step=11092, preemption_count=0, score=4989.927176, test/accuracy=0.290700, test/loss=3.587768, test/num_examples=10000, total_duration=5445.088646, train/accuracy=0.413828, train/loss=2.881742, validation/accuracy=0.383120, validation/loss=3.039930, validation/num_examples=50000
I0329 21:08:52.798160 139798077798208 checkpoints.py:356] Saving checkpoint at step: 11092
I0329 21:08:53.960435 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_11092
I0329 21:08:53.984709 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_11092.
I0329 21:08:53.997902 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 11092: RAM USED (GB) 158.963245056
I0329 21:08:57.734295 139622172555008 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5595570206642151, loss=6.073991298675537
I0329 21:09:40.744708 139622071908096 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5645793676376343, loss=4.8615827560424805
I0329 21:10:26.654540 139622172555008 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.49918293952941895, loss=5.860986232757568
I0329 21:11:12.683214 139622071908096 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6232306361198425, loss=4.507485866546631
I0329 21:11:58.613548 139622172555008 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.606902003288269, loss=4.321177959442139
I0329 21:12:44.365074 139622071908096 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5981823801994324, loss=4.153989791870117
I0329 21:13:30.116906 139622172555008 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.48030886054039, loss=5.204470634460449
I0329 21:14:15.603979 139622071908096 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5660368204116821, loss=4.747870922088623
I0329 21:15:01.280292 139622172555008 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5254784226417542, loss=5.907623767852783
I0329 21:15:47.358611 139622071908096 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6500003337860107, loss=4.12205171585083
I0329 21:15:54.387756 139798077798208 submission_runner.py:371] Before eval at step 12017: RAM USED (GB) 154.08644096
I0329 21:15:54.388001 139798077798208 spec.py:298] Evaluating on the training split.
I0329 21:16:08.620553 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 21:16:20.526830 139798077798208 spec.py:326] Evaluating on the test split.
I0329 21:16:22.223933 139798077798208 submission_runner.py:380] Time since start: 5895.03s, 	Step: 12017, 	{'train/accuracy': 0.46970701217651367, 'train/loss': 2.4815051555633545, 'validation/accuracy': 0.41137999296188354, 'validation/loss': 2.7642385959625244, 'validation/num_examples': 50000, 'test/accuracy': 0.3099000155925751, 'test/loss': 3.4077556133270264, 'test/num_examples': 10000}
I0329 21:16:22.224404 139798077798208 submission_runner.py:390] After eval at step 12017: RAM USED (GB) 159.244091392
I0329 21:16:22.240697 139622172555008 logging_writer.py:48] [12017] global_step=12017, preemption_count=0, score=5400.645683, test/accuracy=0.309900, test/loss=3.407756, test/num_examples=10000, total_duration=5895.030160, train/accuracy=0.469707, train/loss=2.481505, validation/accuracy=0.411380, validation/loss=2.764239, validation/num_examples=50000
I0329 21:16:22.480096 139798077798208 checkpoints.py:356] Saving checkpoint at step: 12017
I0329 21:16:23.715428 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12017
I0329 21:16:23.734559 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12017.
I0329 21:16:23.746990 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 12017: RAM USED (GB) 165.52617984
I0329 21:16:59.058249 139622071908096 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6600069403648376, loss=4.064966201782227
I0329 21:17:44.752802 139622063515392 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5036123991012573, loss=6.00026273727417
I0329 21:18:30.490256 139622071908096 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7159175872802734, loss=4.04547119140625
I0329 21:19:16.032953 139622063515392 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6644343733787537, loss=4.006933212280273
I0329 21:20:02.330175 139622071908096 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6037468314170837, loss=5.716560363769531
I0329 21:20:48.121668 139622063515392 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6171327233314514, loss=4.017848968505859
I0329 21:21:34.040830 139622071908096 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5423359870910645, loss=5.086115837097168
I0329 21:22:19.991963 139622063515392 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6864053010940552, loss=4.137302398681641
I0329 21:23:05.735999 139622071908096 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6068557500839233, loss=5.178950786590576
I0329 21:23:24.116755 139798077798208 submission_runner.py:371] Before eval at step 12942: RAM USED (GB) 159.721484288
I0329 21:23:24.117046 139798077798208 spec.py:298] Evaluating on the training split.
I0329 21:23:38.869438 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 21:23:50.847853 139798077798208 spec.py:326] Evaluating on the test split.
I0329 21:23:52.533370 139798077798208 submission_runner.py:380] Time since start: 6344.76s, 	Step: 12942, 	{'train/accuracy': 0.4708203077316284, 'train/loss': 2.50431752204895, 'validation/accuracy': 0.4364599883556366, 'validation/loss': 2.671790361404419, 'validation/num_examples': 50000, 'test/accuracy': 0.3392000198364258, 'test/loss': 3.2687740325927734, 'test/num_examples': 10000}
I0329 21:23:52.533856 139798077798208 submission_runner.py:390] After eval at step 12942: RAM USED (GB) 165.846245376
I0329 21:23:52.547873 139622063515392 logging_writer.py:48] [12942] global_step=12942, preemption_count=0, score=5811.173052, test/accuracy=0.339200, test/loss=3.268774, test/num_examples=10000, total_duration=6344.757768, train/accuracy=0.470820, train/loss=2.504318, validation/accuracy=0.436460, validation/loss=2.671790, validation/num_examples=50000
I0329 21:23:52.875172 139798077798208 checkpoints.py:356] Saving checkpoint at step: 12942
I0329 21:23:54.295455 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12942
I0329 21:23:54.318343 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12942.
I0329 21:23:54.321867 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 12942: RAM USED (GB) 173.394182144
I0329 21:24:18.093974 139622071908096 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6019418239593506, loss=5.691098213195801
I0329 21:25:03.769642 139622055122688 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6571602821350098, loss=4.216693878173828
I0329 21:25:49.464749 139622071908096 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6736117601394653, loss=3.9662370681762695
I0329 21:26:35.000387 139622055122688 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6154980659484863, loss=5.960557460784912
I0329 21:27:21.115308 139622071908096 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6468425393104553, loss=3.967013120651245
I0329 21:28:07.222521 139622055122688 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6245651841163635, loss=3.969597101211548
I0329 21:28:53.219938 139622071908096 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5218824148178101, loss=5.815888404846191
I0329 21:29:39.263391 139622055122688 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7069506049156189, loss=3.872464656829834
I0329 21:30:25.804422 139622071908096 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6375115513801575, loss=3.8375988006591797
I0329 21:30:54.602985 139798077798208 submission_runner.py:371] Before eval at step 13864: RAM USED (GB) 165.567655936
I0329 21:30:54.603234 139798077798208 spec.py:298] Evaluating on the training split.
I0329 21:31:09.528436 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 21:31:21.691676 139798077798208 spec.py:326] Evaluating on the test split.
I0329 21:31:23.371003 139798077798208 submission_runner.py:380] Time since start: 6795.25s, 	Step: 13864, 	{'train/accuracy': 0.4861718714237213, 'train/loss': 2.4391865730285645, 'validation/accuracy': 0.45069998502731323, 'validation/loss': 2.616271495819092, 'validation/num_examples': 50000, 'test/accuracy': 0.3491000235080719, 'test/loss': 3.2341256141662598, 'test/num_examples': 10000}
I0329 21:31:23.371725 139798077798208 submission_runner.py:390] After eval at step 13864: RAM USED (GB) 172.678303744
I0329 21:31:23.388467 139622055122688 logging_writer.py:48] [13864] global_step=13864, preemption_count=0, score=6221.113374, test/accuracy=0.349100, test/loss=3.234126, test/num_examples=10000, total_duration=6795.247286, train/accuracy=0.486172, train/loss=2.439187, validation/accuracy=0.450700, validation/loss=2.616271, validation/num_examples=50000
I0329 21:31:23.751011 139798077798208 checkpoints.py:356] Saving checkpoint at step: 13864
I0329 21:31:25.032701 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_13864
I0329 21:31:25.055320 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_13864.
I0329 21:31:25.058395 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 13864: RAM USED (GB) 179.973132288
I0329 21:31:39.845046 139622071908096 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7124993801116943, loss=3.8457629680633545
I0329 21:32:23.697942 139798077798208 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 171.846705152
I0329 21:32:23.698225 139798077798208 spec.py:298] Evaluating on the training split.
I0329 21:32:38.250807 139798077798208 spec.py:310] Evaluating on the validation split.
I0329 21:32:50.207853 139798077798208 spec.py:326] Evaluating on the test split.
I0329 21:32:51.883761 139798077798208 submission_runner.py:380] Time since start: 6884.34s, 	Step: 14000, 	{'train/accuracy': 0.4939062297344208, 'train/loss': 2.314652681350708, 'validation/accuracy': 0.459119975566864, 'validation/loss': 2.5059165954589844, 'validation/num_examples': 50000, 'test/accuracy': 0.3514000177383423, 'test/loss': 3.1464438438415527, 'test/num_examples': 10000}
I0329 21:32:51.884348 139798077798208 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 177.18054912
I0329 21:32:51.900023 139622046729984 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=6277.238643, test/accuracy=0.351400, test/loss=3.146444, test/num_examples=10000, total_duration=6884.340812, train/accuracy=0.493906, train/loss=2.314653, validation/accuracy=0.459120, validation/loss=2.505917, validation/num_examples=50000
I0329 21:32:52.217910 139798077798208 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 21:32:53.489737 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14000
I0329 21:32:53.511542 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14000.
I0329 21:32:53.514521 139798077798208 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 184.098725888
I0329 21:32:53.543934 139622071908096 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=6277.238643
I0329 21:32:53.901745 139798077798208 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 21:32:55.118694 139798077798208 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14000
I0329 21:32:55.133077 139798077798208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14000.
I0329 21:32:55.907271 139798077798208 submission_runner.py:543] Tuning trial 1/1
I0329 21:32:55.908256 139798077798208 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0329 21:32:55.913428 139798077798208 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 51.56655716896057, 'total_duration': 51.65408682823181, 'global_step': 1, 'preemption_count': 0}), (903, {'train/accuracy': 0.02285156212747097, 'train/loss': 6.152990818023682, 'validation/accuracy': 0.022760000079870224, 'validation/loss': 6.166991710662842, 'validation/num_examples': 50000, 'test/accuracy': 0.017000000923871994, 'test/loss': 6.2465291023254395, 'test/num_examples': 10000, 'score': 465.2482240200043, 'total_duration': 525.3412351608276, 'global_step': 903, 'preemption_count': 0}), (1836, {'train/accuracy': 0.05386718735098839, 'train/loss': 5.592177391052246, 'validation/accuracy': 0.05069999769330025, 'validation/loss': 5.6237711906433105, 'validation/num_examples': 50000, 'test/accuracy': 0.04270000383257866, 'test/loss': 5.782311916351318, 'test/num_examples': 10000, 'score': 878.6253643035889, 'total_duration': 969.4701688289642, 'global_step': 1836, 'preemption_count': 0}), (2760, {'train/accuracy': 0.09880859404802322, 'train/loss': 5.071300506591797, 'validation/accuracy': 0.08816000074148178, 'validation/loss': 5.156027317047119, 'validation/num_examples': 50000, 'test/accuracy': 0.06550000607967377, 'test/loss': 5.386715888977051, 'test/num_examples': 10000, 'score': 1288.5062170028687, 'total_duration': 1414.2205538749695, 'global_step': 2760, 'preemption_count': 0}), (3693, {'train/accuracy': 0.14119140803813934, 'train/loss': 4.677453994750977, 'validation/accuracy': 0.13161998987197876, 'validation/loss': 4.7457275390625, 'validation/num_examples': 50000, 'test/accuracy': 0.10220000147819519, 'test/loss': 5.037045955657959, 'test/num_examples': 10000, 'score': 1701.111941576004, 'total_duration': 1861.121987104416, 'global_step': 3693, 'preemption_count': 0}), (4622, {'train/accuracy': 0.17863281071186066, 'train/loss': 4.371862888336182, 'validation/accuracy': 0.167619988322258, 'validation/loss': 4.456496238708496, 'validation/num_examples': 50000, 'test/accuracy': 0.12490000575780869, 'test/loss': 4.812922477722168, 'test/num_examples': 10000, 'score': 2113.871745824814, 'total_duration': 2308.8565077781677, 'global_step': 4622, 'preemption_count': 0}), (5548, {'train/accuracy': 0.20970702171325684, 'train/loss': 4.079505920410156, 'validation/accuracy': 0.18835999071598053, 'validation/loss': 4.21451997756958, 'validation/num_examples': 50000, 'test/accuracy': 0.14570000767707825, 'test/loss': 4.621811389923096, 'test/num_examples': 10000, 'score': 2525.4114005565643, 'total_duration': 2757.095456600189, 'global_step': 5548, 'preemption_count': 0}), (6471, {'train/accuracy': 0.25605466961860657, 'train/loss': 3.7834465503692627, 'validation/accuracy': 0.24129998683929443, 'validation/loss': 3.8848376274108887, 'validation/num_examples': 50000, 'test/accuracy': 0.1811000108718872, 'test/loss': 4.329239845275879, 'test/num_examples': 10000, 'score': 2936.105125427246, 'total_duration': 3204.8533775806427, 'global_step': 6471, 'preemption_count': 0}), (7397, {'train/accuracy': 0.2892773449420929, 'train/loss': 3.532548666000366, 'validation/accuracy': 0.2705399990081787, 'validation/loss': 3.6441681385040283, 'validation/num_examples': 50000, 'test/accuracy': 0.20920000970363617, 'test/loss': 4.116002559661865, 'test/num_examples': 10000, 'score': 3347.5807921886444, 'total_duration': 3651.8721508979797, 'global_step': 7397, 'preemption_count': 0}), (8321, {'train/accuracy': 0.322265625, 'train/loss': 3.3023979663848877, 'validation/accuracy': 0.29420000314712524, 'validation/loss': 3.4662792682647705, 'validation/num_examples': 50000, 'test/accuracy': 0.2249000072479248, 'test/loss': 3.9831655025482178, 'test/num_examples': 10000, 'score': 3758.50856423378, 'total_duration': 4099.419147014618, 'global_step': 8321, 'preemption_count': 0}), (9245, {'train/accuracy': 0.3550976514816284, 'train/loss': 3.102385997772217, 'validation/accuracy': 0.3262999951839447, 'validation/loss': 3.257344961166382, 'validation/num_examples': 50000, 'test/accuracy': 0.2519000172615051, 'test/loss': 3.7855732440948486, 'test/num_examples': 10000, 'score': 4169.711528778076, 'total_duration': 4547.631982088089, 'global_step': 9245, 'preemption_count': 0}), (10167, {'train/accuracy': 0.3899804651737213, 'train/loss': 2.9321653842926025, 'validation/accuracy': 0.36142000555992126, 'validation/loss': 3.0764076709747314, 'validation/num_examples': 50000, 'test/accuracy': 0.27800002694129944, 'test/loss': 3.642348289489746, 'test/num_examples': 10000, 'score': 4579.522310733795, 'total_duration': 4995.956042051315, 'global_step': 10167, 'preemption_count': 0}), (11092, {'train/accuracy': 0.4138281047344208, 'train/loss': 2.881742477416992, 'validation/accuracy': 0.38312000036239624, 'validation/loss': 3.0399296283721924, 'validation/num_examples': 50000, 'test/accuracy': 0.2907000184059143, 'test/loss': 3.5877678394317627, 'test/num_examples': 10000, 'score': 4989.927175521851, 'total_duration': 5445.088645935059, 'global_step': 11092, 'preemption_count': 0}), (12017, {'train/accuracy': 0.46970701217651367, 'train/loss': 2.4815051555633545, 'validation/accuracy': 0.41137999296188354, 'validation/loss': 2.7642385959625244, 'validation/num_examples': 50000, 'test/accuracy': 0.3099000155925751, 'test/loss': 3.4077556133270264, 'test/num_examples': 10000, 'score': 5400.645682811737, 'total_duration': 5895.030160188675, 'global_step': 12017, 'preemption_count': 0}), (12942, {'train/accuracy': 0.4708203077316284, 'train/loss': 2.50431752204895, 'validation/accuracy': 0.4364599883556366, 'validation/loss': 2.671790361404419, 'validation/num_examples': 50000, 'test/accuracy': 0.3392000198364258, 'test/loss': 3.2687740325927734, 'test/num_examples': 10000, 'score': 5811.173052072525, 'total_duration': 6344.757768392563, 'global_step': 12942, 'preemption_count': 0}), (13864, {'train/accuracy': 0.4861718714237213, 'train/loss': 2.4391865730285645, 'validation/accuracy': 0.45069998502731323, 'validation/loss': 2.616271495819092, 'validation/num_examples': 50000, 'test/accuracy': 0.3491000235080719, 'test/loss': 3.2341256141662598, 'test/num_examples': 10000, 'score': 6221.113374233246, 'total_duration': 6795.247286081314, 'global_step': 13864, 'preemption_count': 0}), (14000, {'train/accuracy': 0.4939062297344208, 'train/loss': 2.314652681350708, 'validation/accuracy': 0.459119975566864, 'validation/loss': 2.5059165954589844, 'validation/num_examples': 50000, 'test/accuracy': 0.3514000177383423, 'test/loss': 3.1464438438415527, 'test/num_examples': 10000, 'score': 6277.238643407822, 'total_duration': 6884.34081196785, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0329 21:32:55.913538 139798077798208 submission_runner.py:546] Timing: 6277.238643407822
I0329 21:32:55.913585 139798077798208 submission_runner.py:547] ====================
I0329 21:32:55.913690 139798077798208 submission_runner.py:606] Final imagenet_vit score: 6277.238643407822
