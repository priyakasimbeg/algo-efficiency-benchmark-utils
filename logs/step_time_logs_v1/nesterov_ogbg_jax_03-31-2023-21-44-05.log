I0331 21:44:18.817117 140498423023424 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/ogbg_jax.
I0331 21:44:18.868278 140498423023424 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 21:44:19.701674 140498423023424 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0331 21:44:19.702396 140498423023424 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 21:44:19.706103 140498423023424 submission_runner.py:511] Using RNG seed 1040351228
I0331 21:44:20.953189 140498423023424 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 21:44:20.953376 140498423023424 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/ogbg_jax/trial_1.
I0331 21:44:20.953563 140498423023424 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/hparams.json.
I0331 21:44:21.076627 140498423023424 submission_runner.py:230] Starting train once: RAM USED (GB) 4.265357312
I0331 21:44:21.076805 140498423023424 submission_runner.py:231] Initializing dataset.
I0331 21:44:21.304455 140498423023424 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:44:21.310173 140498423023424 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:44:21.471085 140498423023424 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:44:21.503553 140498423023424 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.339830784
I0331 21:44:21.503731 140498423023424 submission_runner.py:240] Initializing model.
I0331 21:44:28.634631 140498423023424 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.17053696
I0331 21:44:28.634849 140498423023424 submission_runner.py:252] Initializing optimizer.
I0331 21:44:28.992785 140498423023424 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.170545152
I0331 21:44:28.992945 140498423023424 submission_runner.py:261] Initializing metrics bundle.
I0331 21:44:28.992994 140498423023424 submission_runner.py:276] Initializing checkpoint and logger.
I0331 21:44:28.993859 140498423023424 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov/ogbg_jax/trial_1 with prefix checkpoint_
I0331 21:44:28.994097 140498423023424 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0331 21:44:28.994160 140498423023424 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0331 21:44:29.903853 140498423023424 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/meta_data_0.json.
I0331 21:44:29.904818 140498423023424 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/flags_0.json.
I0331 21:44:29.907809 140498423023424 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.167714816
I0331 21:44:29.908012 140498423023424 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.167714816
I0331 21:44:29.908072 140498423023424 submission_runner.py:313] Starting training loop.
I0331 21:44:31.481436 140498423023424 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.3202048
I0331 21:44:48.780325 140322512299776 logging_writer.py:48] [0] global_step=0, grad_norm=2.93355655670166, loss=0.7943315505981445
I0331 21:44:48.789171 140498423023424 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 10.905894912
I0331 21:44:48.789578 140498423023424 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 10.905894912
I0331 21:44:48.789721 140498423023424 spec.py:298] Evaluating on the training split.
I0331 21:44:48.798354 140498423023424 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:44:48.803210 140498423023424 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:44:48.861447 140498423023424 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0331 21:45:04.295997 140498423023424 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0331 21:46:16.983666 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 21:46:16.986559 140498423023424 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:46:16.990197 140498423023424 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:46:17.040842 140498423023424 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:47:19.423166 140498423023424 spec.py:326] Evaluating on the test split.
I0331 21:47:19.425847 140498423023424 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:47:19.429672 140498423023424 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:47:19.480798 140498423023424 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:48:22.330147 140498423023424 submission_runner.py:382] Time since start: 18.88s, 	Step: 1, 	{'train/accuracy': 0.471046507358551, 'train/loss': 0.792041003704071, 'train/mean_average_precision': 0.021880416120185913, 'validation/accuracy': 0.47215548157691956, 'validation/loss': 0.7918241024017334, 'validation/mean_average_precision': 0.02517176784507629, 'validation/num_examples': 43793, 'test/accuracy': 0.4740508198738098, 'test/loss': 0.7922861576080322, 'test/mean_average_precision': 0.02726235985769046, 'test/num_examples': 43793}
I0331 21:48:22.330639 140498423023424 submission_runner.py:396] After eval at step 1: RAM USED (GB) 12.259950592
I0331 21:48:22.337813 140312504243968 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=18.806040, test/accuracy=0.474051, test/loss=0.792286, test/mean_average_precision=0.027262, test/num_examples=43793, total_duration=18.881437, train/accuracy=0.471047, train/loss=0.792041, train/mean_average_precision=0.021880, validation/accuracy=0.472155, validation/loss=0.791824, validation/mean_average_precision=0.025172, validation/num_examples=43793
I0331 21:48:22.364053 140498423023424 checkpoints.py:356] Saving checkpoint at step: 1
I0331 21:48:22.432233 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_1
I0331 21:48:22.432438 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_1.
I0331 21:48:22.432981 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.270374912
I0331 21:48:22.648511 140498423023424 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.318171136
I0331 21:48:22.660283 140498423023424 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.318670848
I0331 21:48:44.685753 140312512636672 logging_writer.py:48] [100] global_step=100, grad_norm=0.06680934131145477, loss=0.0892670676112175
I0331 21:49:06.857291 140313712400128 logging_writer.py:48] [200] global_step=200, grad_norm=0.017845114693045616, loss=0.05469508841633797
I0331 21:49:28.847892 140312512636672 logging_writer.py:48] [300] global_step=300, grad_norm=0.012139004655182362, loss=0.053900428116321564
I0331 21:49:50.977079 140313712400128 logging_writer.py:48] [400] global_step=400, grad_norm=0.012353070080280304, loss=0.05217275023460388
I0331 21:50:13.283420 140312512636672 logging_writer.py:48] [500] global_step=500, grad_norm=0.014024701900780201, loss=0.05742856115102768
I0331 21:50:35.319115 140313712400128 logging_writer.py:48] [600] global_step=600, grad_norm=0.009310577064752579, loss=0.0555339977145195
I0331 21:50:57.605821 140312512636672 logging_writer.py:48] [700] global_step=700, grad_norm=0.008382491767406464, loss=0.05679962411522865
I0331 21:51:20.020029 140313712400128 logging_writer.py:48] [800] global_step=800, grad_norm=0.04315097630023956, loss=0.05696185678243637
I0331 21:51:42.446460 140312512636672 logging_writer.py:48] [900] global_step=900, grad_norm=0.060039740055799484, loss=0.05466528981924057
I0331 21:52:04.912529 140313712400128 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.018772730603814125, loss=0.04874870553612709
I0331 21:52:22.633049 140498423023424 submission_runner.py:373] Before eval at step 1080: RAM USED (GB) 13.09999104
I0331 21:52:22.633233 140498423023424 spec.py:298] Evaluating on the training split.
I0331 21:53:34.367733 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 21:53:36.960138 140498423023424 spec.py:326] Evaluating on the test split.
I0331 21:53:39.497675 140498423023424 submission_runner.py:382] Time since start: 472.72s, 	Step: 1080, 	{'train/accuracy': 0.9866893887519836, 'train/loss': 0.054411716759204865, 'train/mean_average_precision': 0.036484458393807956, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06446503102779388, 'validation/mean_average_precision': 0.04095978489655818, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06778433173894882, 'test/mean_average_precision': 0.04122653068891787, 'test/num_examples': 43793}
I0331 21:53:39.498114 140498423023424 submission_runner.py:396] After eval at step 1080: RAM USED (GB) 13.572489216
I0331 21:53:39.505805 140312512636672 logging_writer.py:48] [1080] global_step=1080, preemption_count=0, score=258.065439, test/accuracy=0.983142, test/loss=0.067784, test/mean_average_precision=0.041227, test/num_examples=43793, total_duration=472.724687, train/accuracy=0.986689, train/loss=0.054412, train/mean_average_precision=0.036484, validation/accuracy=0.984118, validation/loss=0.064465, validation/mean_average_precision=0.040960, validation/num_examples=43793
I0331 21:53:39.530109 140498423023424 checkpoints.py:356] Saving checkpoint at step: 1080
I0331 21:53:39.591746 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_1080
I0331 21:53:39.591926 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_1080.
I0331 21:53:39.592486 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 1080: RAM USED (GB) 13.585031168
I0331 21:53:44.238134 140313712400128 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.12283743917942047, loss=0.05697985738515854
I0331 21:54:06.603461 140313611982592 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.06104226037859917, loss=0.05447359010577202
I0331 21:54:29.005969 140313712400128 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.028854668140411377, loss=0.05063292756676674
I0331 21:54:51.730726 140313611982592 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.15466703474521637, loss=0.056832604110240936
I0331 21:55:14.438191 140313712400128 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02268017828464508, loss=0.05215761438012123
I0331 21:55:36.960170 140313611982592 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.10593859106302261, loss=0.05364440008997917
I0331 21:55:59.221519 140313712400128 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.023047450929880142, loss=0.05550402030348778
I0331 21:56:21.624827 140313611982592 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.05048346519470215, loss=0.049191664904356
I0331 21:56:44.192205 140313712400128 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.07019711285829544, loss=0.048928651958703995
I0331 21:57:06.645090 140313611982592 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.045193642377853394, loss=0.05021509528160095
I0331 21:57:29.327348 140313712400128 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0427478551864624, loss=0.05264014005661011
I0331 21:57:39.718184 140498423023424 submission_runner.py:373] Before eval at step 2147: RAM USED (GB) 14.018392064
I0331 21:57:39.718385 140498423023424 spec.py:298] Evaluating on the training split.
I0331 21:58:53.355764 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 21:58:55.863087 140498423023424 spec.py:326] Evaluating on the test split.
I0331 21:58:58.330861 140498423023424 submission_runner.py:382] Time since start: 789.81s, 	Step: 2147, 	{'train/accuracy': 0.9867061972618103, 'train/loss': 0.05306342989206314, 'train/mean_average_precision': 0.06271785545540466, 'validation/accuracy': 0.9841369986534119, 'validation/loss': 0.06245221942663193, 'validation/mean_average_precision': 0.06040839165093806, 'validation/num_examples': 43793, 'test/accuracy': 0.983150064945221, 'test/loss': 0.06594528257846832, 'test/mean_average_precision': 0.06127212577922302, 'test/num_examples': 43793}
I0331 21:58:58.331305 140498423023424 submission_runner.py:396] After eval at step 2147: RAM USED (GB) 14.345977856
I0331 21:58:58.338240 140313611982592 logging_writer.py:48] [2147] global_step=2147, preemption_count=0, score=497.169930, test/accuracy=0.983150, test/loss=0.065945, test/mean_average_precision=0.061272, test/num_examples=43793, total_duration=789.809754, train/accuracy=0.986706, train/loss=0.053063, train/mean_average_precision=0.062718, validation/accuracy=0.984137, validation/loss=0.062452, validation/mean_average_precision=0.060408, validation/num_examples=43793
I0331 21:58:58.362434 140498423023424 checkpoints.py:356] Saving checkpoint at step: 2147
I0331 21:58:58.415782 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_2147
I0331 21:58:58.415936 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_2147.
I0331 21:58:58.416433 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 2147: RAM USED (GB) 14.343053312
I0331 21:59:10.370903 140313712400128 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0698629841208458, loss=0.05211644247174263
I0331 21:59:33.306664 140313586804480 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.2349434196949005, loss=0.0523810088634491
I0331 21:59:56.150047 140313712400128 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.09502202272415161, loss=0.05093544349074364
I0331 22:00:19.155956 140313586804480 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.03572845086455345, loss=0.05286521837115288
I0331 22:00:42.141360 140313712400128 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.07030859589576721, loss=0.05691700428724289
I0331 22:01:04.880282 140313586804480 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0719740018248558, loss=0.05446368828415871
I0331 22:01:27.516547 140313712400128 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.07141225785017014, loss=0.05356110632419586
I0331 22:01:49.996505 140313586804480 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.03642747923731804, loss=0.05579754337668419
I0331 22:02:12.569821 140313712400128 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.05486440658569336, loss=0.046930983662605286
I0331 22:02:35.310647 140313586804480 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.08735781908035278, loss=0.049334339797496796
I0331 22:02:57.792068 140313712400128 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05695408210158348, loss=0.04676623269915581
I0331 22:02:58.479227 140498423023424 submission_runner.py:373] Before eval at step 3204: RAM USED (GB) 14.573039616
I0331 22:02:58.479448 140498423023424 spec.py:298] Evaluating on the training split.
I0331 22:04:12.141727 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 22:04:14.830101 140498423023424 spec.py:326] Evaluating on the test split.
I0331 22:04:17.433659 140498423023424 submission_runner.py:382] Time since start: 1108.57s, 	Step: 3204, 	{'train/accuracy': 0.9871838688850403, 'train/loss': 0.04762767255306244, 'train/mean_average_precision': 0.0997801834492632, 'validation/accuracy': 0.9844585061073303, 'validation/loss': 0.056491173803806305, 'validation/mean_average_precision': 0.0996042900560821, 'validation/num_examples': 43793, 'test/accuracy': 0.9834364652633667, 'test/loss': 0.05955233797430992, 'test/mean_average_precision': 0.10004800908670076, 'test/num_examples': 43793}
I0331 22:04:17.434145 140498423023424 submission_runner.py:396] After eval at step 3204: RAM USED (GB) 14.865604608
I0331 22:04:17.441902 140313586804480 logging_writer.py:48] [3204] global_step=3204, preemption_count=0, score=736.172131, test/accuracy=0.983436, test/loss=0.059552, test/mean_average_precision=0.100048, test/num_examples=43793, total_duration=1108.570836, train/accuracy=0.987184, train/loss=0.047628, train/mean_average_precision=0.099780, validation/accuracy=0.984459, validation/loss=0.056491, validation/mean_average_precision=0.099604, validation/num_examples=43793
I0331 22:04:17.464053 140498423023424 checkpoints.py:356] Saving checkpoint at step: 3204
I0331 22:04:17.520686 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_3204
I0331 22:04:17.520897 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_3204.
I0331 22:04:17.521509 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 3204: RAM USED (GB) 14.864433152
I0331 22:04:39.205124 140313712400128 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.059267807751894, loss=0.047583892941474915
I0331 22:05:01.659706 140313578411776 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.05699953809380531, loss=0.05060724541544914
I0331 22:05:24.574529 140313712400128 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.08780519664287567, loss=0.056306589394807816
I0331 22:05:47.766170 140313578411776 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.051024045795202255, loss=0.05217742547392845
I0331 22:06:10.656724 140313712400128 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.14781762659549713, loss=0.057848136872053146
I0331 22:06:33.840804 140313578411776 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0650489553809166, loss=0.04730592295527458
I0331 22:06:56.727331 140313712400128 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.08544156700372696, loss=0.05198334529995918
I0331 22:07:19.796818 140313578411776 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.05428721010684967, loss=0.04522626847028732
I0331 22:07:42.822082 140313712400128 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.05315691977739334, loss=0.04470837116241455
I0331 22:08:06.173858 140313578411776 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0941469743847847, loss=0.050294533371925354
I0331 22:08:17.567974 140498423023424 submission_runner.py:373] Before eval at step 4252: RAM USED (GB) 15.209570304
I0331 22:08:17.568149 140498423023424 spec.py:298] Evaluating on the training split.
I0331 22:09:30.408792 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 22:09:32.929677 140498423023424 spec.py:326] Evaluating on the test split.
I0331 22:09:35.374542 140498423023424 submission_runner.py:382] Time since start: 1427.66s, 	Step: 4252, 	{'train/accuracy': 0.9869582653045654, 'train/loss': 0.047357313334941864, 'train/mean_average_precision': 0.12337504106967627, 'validation/accuracy': 0.9843533635139465, 'validation/loss': 0.05710291489958763, 'validation/mean_average_precision': 0.1130646178491294, 'validation/num_examples': 43793, 'test/accuracy': 0.9833703637123108, 'test/loss': 0.06043167784810066, 'test/mean_average_precision': 0.11724550850521388, 'test/num_examples': 43793}
I0331 22:09:35.375000 140498423023424 submission_runner.py:396] After eval at step 4252: RAM USED (GB) 15.571542016
I0331 22:09:35.382417 140313712400128 logging_writer.py:48] [4252] global_step=4252, preemption_count=0, score=975.240571, test/accuracy=0.983370, test/loss=0.060432, test/mean_average_precision=0.117246, test/num_examples=43793, total_duration=1427.659592, train/accuracy=0.986958, train/loss=0.047357, train/mean_average_precision=0.123375, validation/accuracy=0.984353, validation/loss=0.057103, validation/mean_average_precision=0.113065, validation/num_examples=43793
I0331 22:09:35.406720 140498423023424 checkpoints.py:356] Saving checkpoint at step: 4252
I0331 22:09:35.463286 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_4252
I0331 22:09:35.463483 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_4252.
I0331 22:09:35.464109 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 4252: RAM USED (GB) 15.570710528
I0331 22:09:46.455651 140313578411776 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.06000147759914398, loss=0.04681304842233658
I0331 22:10:08.847253 140313570019072 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04305203631520271, loss=0.049402546137571335
I0331 22:10:31.315498 140313578411776 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.11804632097482681, loss=0.04936283454298973
I0331 22:10:53.553388 140313570019072 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.04200797155499458, loss=0.04884479567408562
I0331 22:11:15.978026 140313578411776 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.06376487761735916, loss=0.050399214029312134
I0331 22:11:38.614144 140313570019072 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.043088149279356, loss=0.04349106177687645
I0331 22:12:01.073045 140313578411776 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.06608198583126068, loss=0.04628241807222366
I0331 22:12:23.266669 140313570019072 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.03992938995361328, loss=0.04319155961275101
I0331 22:12:45.442381 140313578411776 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.04097995534539223, loss=0.04660150036215782
I0331 22:13:07.868041 140313570019072 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.038648854941129684, loss=0.04783563315868378
I0331 22:13:30.286947 140313578411776 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.09178445488214493, loss=0.0489913746714592
I0331 22:13:35.471474 140498423023424 submission_runner.py:373] Before eval at step 5324: RAM USED (GB) 15.776800768
I0331 22:13:35.471659 140498423023424 spec.py:298] Evaluating on the training split.
I0331 22:14:48.866306 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 22:14:51.363365 140498423023424 spec.py:326] Evaluating on the test split.
I0331 22:14:53.812928 140498423023424 submission_runner.py:382] Time since start: 1745.56s, 	Step: 5324, 	{'train/accuracy': 0.9873886108398438, 'train/loss': 0.04442542418837547, 'train/mean_average_precision': 0.15539050557275802, 'validation/accuracy': 0.9846371412277222, 'validation/loss': 0.05520220845937729, 'validation/mean_average_precision': 0.1400423739003458, 'validation/num_examples': 43793, 'test/accuracy': 0.9836866855621338, 'test/loss': 0.058391835540533066, 'test/mean_average_precision': 0.1453043872178772, 'test/num_examples': 43793}
I0331 22:14:53.813391 140498423023424 submission_runner.py:396] After eval at step 5324: RAM USED (GB) 16.163323904
I0331 22:14:53.821030 140313570019072 logging_writer.py:48] [5324] global_step=5324, preemption_count=0, score=1214.259568, test/accuracy=0.983687, test/loss=0.058392, test/mean_average_precision=0.145304, test/num_examples=43793, total_duration=1745.563125, train/accuracy=0.987389, train/loss=0.044425, train/mean_average_precision=0.155391, validation/accuracy=0.984637, validation/loss=0.055202, validation/mean_average_precision=0.140042, validation/num_examples=43793
I0331 22:14:53.844990 140498423023424 checkpoints.py:356] Saving checkpoint at step: 5324
I0331 22:14:53.900907 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_5324
I0331 22:14:53.901074 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_5324.
I0331 22:14:53.901639 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 5324: RAM USED (GB) 16.162504704
I0331 22:15:11.077318 140313578411776 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.07297676801681519, loss=0.05001877620816231
I0331 22:15:33.556075 140313561626368 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05475912243127823, loss=0.04397721216082573
I0331 22:15:55.596130 140313578411776 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.08342543989419937, loss=0.043383680284023285
I0331 22:16:17.570692 140313561626368 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.041364848613739014, loss=0.04787058010697365
I0331 22:16:39.686563 140313578411776 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04776221513748169, loss=0.04413171485066414
I0331 22:17:01.987069 140313561626368 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.11369901895523071, loss=0.04514137655496597
I0331 22:17:23.797309 140498423023424 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 16.28358656
I0331 22:17:23.797540 140498423023424 spec.py:298] Evaluating on the training split.
I0331 22:18:34.797219 140498423023424 spec.py:310] Evaluating on the validation split.
I0331 22:18:37.565153 140498423023424 spec.py:326] Evaluating on the test split.
I0331 22:18:40.165007 140498423023424 submission_runner.py:382] Time since start: 1973.89s, 	Step: 6000, 	{'train/accuracy': 0.9876568913459778, 'train/loss': 0.04234648495912552, 'train/mean_average_precision': 0.18171297633388117, 'validation/accuracy': 0.9849956035614014, 'validation/loss': 0.05211673304438591, 'validation/mean_average_precision': 0.15653918930608554, 'validation/num_examples': 43793, 'test/accuracy': 0.9840012788772583, 'test/loss': 0.05502105504274368, 'test/mean_average_precision': 0.1586160018360357, 'test/num_examples': 43793}
I0331 22:18:40.165496 140498423023424 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 16.641159168
I0331 22:18:40.173282 140313578411776 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1363.542879, test/accuracy=0.984001, test/loss=0.055021, test/mean_average_precision=0.158616, test/num_examples=43793, total_duration=1973.888911, train/accuracy=0.987657, train/loss=0.042346, train/mean_average_precision=0.181713, validation/accuracy=0.984996, validation/loss=0.052117, validation/mean_average_precision=0.156539, validation/num_examples=43793
I0331 22:18:40.196134 140498423023424 checkpoints.py:356] Saving checkpoint at step: 6000
I0331 22:18:40.258002 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_6000
I0331 22:18:40.258229 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_6000.
I0331 22:18:40.258866 140498423023424 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.640581632
I0331 22:18:40.265325 140313561626368 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1363.542879
I0331 22:18:40.282728 140498423023424 checkpoints.py:356] Saving checkpoint at step: 6000
I0331 22:18:40.382392 140498423023424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_6000
I0331 22:18:40.382667 140498423023424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/ogbg_jax/trial_1/checkpoint_6000.
I0331 22:18:40.546597 140498423023424 submission_runner.py:550] Tuning trial 1/1
I0331 22:18:40.546852 140498423023424 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0331 22:18:40.547960 140498423023424 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.471046507358551, 'train/loss': 0.792041003704071, 'train/mean_average_precision': 0.021880416120185913, 'validation/accuracy': 0.47215548157691956, 'validation/loss': 0.7918241024017334, 'validation/mean_average_precision': 0.02517176784507629, 'validation/num_examples': 43793, 'test/accuracy': 0.4740508198738098, 'test/loss': 0.7922861576080322, 'test/mean_average_precision': 0.02726235985769046, 'test/num_examples': 43793, 'score': 18.806040048599243, 'total_duration': 18.881436824798584, 'global_step': 1, 'preemption_count': 0}), (1080, {'train/accuracy': 0.9866893887519836, 'train/loss': 0.054411716759204865, 'train/mean_average_precision': 0.036484458393807956, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06446503102779388, 'validation/mean_average_precision': 0.04095978489655818, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06778433173894882, 'test/mean_average_precision': 0.04122653068891787, 'test/num_examples': 43793, 'score': 258.065438747406, 'total_duration': 472.7246868610382, 'global_step': 1080, 'preemption_count': 0}), (2147, {'train/accuracy': 0.9867061972618103, 'train/loss': 0.05306342989206314, 'train/mean_average_precision': 0.06271785545540466, 'validation/accuracy': 0.9841369986534119, 'validation/loss': 0.06245221942663193, 'validation/mean_average_precision': 0.06040839165093806, 'validation/num_examples': 43793, 'test/accuracy': 0.983150064945221, 'test/loss': 0.06594528257846832, 'test/mean_average_precision': 0.06127212577922302, 'test/num_examples': 43793, 'score': 497.16993045806885, 'total_duration': 789.8097543716431, 'global_step': 2147, 'preemption_count': 0}), (3204, {'train/accuracy': 0.9871838688850403, 'train/loss': 0.04762767255306244, 'train/mean_average_precision': 0.0997801834492632, 'validation/accuracy': 0.9844585061073303, 'validation/loss': 0.056491173803806305, 'validation/mean_average_precision': 0.0996042900560821, 'validation/num_examples': 43793, 'test/accuracy': 0.9834364652633667, 'test/loss': 0.05955233797430992, 'test/mean_average_precision': 0.10004800908670076, 'test/num_examples': 43793, 'score': 736.172131061554, 'total_duration': 1108.5708360671997, 'global_step': 3204, 'preemption_count': 0}), (4252, {'train/accuracy': 0.9869582653045654, 'train/loss': 0.047357313334941864, 'train/mean_average_precision': 0.12337504106967627, 'validation/accuracy': 0.9843533635139465, 'validation/loss': 0.05710291489958763, 'validation/mean_average_precision': 0.1130646178491294, 'validation/num_examples': 43793, 'test/accuracy': 0.9833703637123108, 'test/loss': 0.06043167784810066, 'test/mean_average_precision': 0.11724550850521388, 'test/num_examples': 43793, 'score': 975.2405714988708, 'total_duration': 1427.6595916748047, 'global_step': 4252, 'preemption_count': 0}), (5324, {'train/accuracy': 0.9873886108398438, 'train/loss': 0.04442542418837547, 'train/mean_average_precision': 0.15539050557275802, 'validation/accuracy': 0.9846371412277222, 'validation/loss': 0.05520220845937729, 'validation/mean_average_precision': 0.1400423739003458, 'validation/num_examples': 43793, 'test/accuracy': 0.9836866855621338, 'test/loss': 0.058391835540533066, 'test/mean_average_precision': 0.1453043872178772, 'test/num_examples': 43793, 'score': 1214.259568452835, 'total_duration': 1745.563125371933, 'global_step': 5324, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9876568913459778, 'train/loss': 0.04234648495912552, 'train/mean_average_precision': 0.18171297633388117, 'validation/accuracy': 0.9849956035614014, 'validation/loss': 0.05211673304438591, 'validation/mean_average_precision': 0.15653918930608554, 'validation/num_examples': 43793, 'test/accuracy': 0.9840012788772583, 'test/loss': 0.05502105504274368, 'test/mean_average_precision': 0.1586160018360357, 'test/num_examples': 43793, 'score': 1363.5428793430328, 'total_duration': 1973.888911485672, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0331 22:18:40.548082 140498423023424 submission_runner.py:553] Timing: 1363.5428793430328
I0331 22:18:40.548125 140498423023424 submission_runner.py:554] ====================
I0331 22:18:40.548233 140498423023424 submission_runner.py:613] Final ogbg score: 1363.5428793430328
