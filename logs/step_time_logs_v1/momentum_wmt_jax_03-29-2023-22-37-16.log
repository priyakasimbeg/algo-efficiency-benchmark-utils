I0329 22:37:31.081934 139855925724992 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/wmt_jax.
I0329 22:37:31.128424 139855925724992 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 22:37:31.974384 139855925724992 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0329 22:37:31.975448 139855925724992 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 22:37:31.980850 139855925724992 submission_runner.py:504] Using RNG seed 1176553548
I0329 22:37:33.268987 139855925724992 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 22:37:33.269193 139855925724992 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/wmt_jax/trial_1.
I0329 22:37:33.269402 139855925724992 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/wmt_jax/trial_1/hparams.json.
I0329 22:37:33.407680 139855925724992 submission_runner.py:230] Starting train once: RAM USED (GB) 4.572962816
I0329 22:37:33.407856 139855925724992 submission_runner.py:231] Initializing dataset.
I0329 22:37:33.417760 139855925724992 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:37:33.421050 139855925724992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:37:33.421161 139855925724992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:37:33.504794 139855925724992 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:37:35.810737 139855925724992 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.669898752
I0329 22:37:35.810984 139855925724992 submission_runner.py:240] Initializing model.
I0329 22:37:49.198064 139855925724992 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.747741184
I0329 22:37:49.198296 139855925724992 submission_runner.py:252] Initializing optimizer.
I0329 22:37:49.866375 139855925724992 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.747741184
I0329 22:37:49.866589 139855925724992 submission_runner.py:261] Initializing metrics bundle.
I0329 22:37:49.866643 139855925724992 submission_runner.py:275] Initializing checkpoint and logger.
I0329 22:37:49.867647 139855925724992 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum/wmt_jax/trial_1 with prefix checkpoint_
I0329 22:37:49.867975 139855925724992 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0329 22:37:49.868065 139855925724992 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0329 22:37:50.866777 139855925724992 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/wmt_jax/trial_1/meta_data_0.json.
I0329 22:37:50.867805 139855925724992 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/wmt_jax/trial_1/flags_0.json.
I0329 22:37:50.871482 139855925724992 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.743501824
I0329 22:37:50.871689 139855925724992 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.743501824
I0329 22:37:50.871765 139855925724992 submission_runner.py:312] Starting training loop.
I0329 22:37:51.626303 139855925724992 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 8.883564544
I0329 22:38:21.875241 139679542273792 logging_writer.py:48] [0] global_step=0, grad_norm=4.96008825302124, loss=11.076262474060059
I0329 22:38:21.886506 139855925724992 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 11.4999296
I0329 22:38:21.886737 139855925724992 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 11.4999296
I0329 22:38:21.886816 139855925724992 spec.py:298] Evaluating on the training split.
I0329 22:38:21.889594 139855925724992 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:38:21.892210 139855925724992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:38:21.892315 139855925724992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:38:21.929621 139855925724992 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:38:30.603754 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 22:43:37.649923 139855925724992 spec.py:310] Evaluating on the validation split.
I0329 22:43:37.653528 139855925724992 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:43:37.656586 139855925724992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:43:37.656692 139855925724992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:43:37.691329 139855925724992 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:43:45.712252 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 22:48:45.009257 139855925724992 spec.py:326] Evaluating on the test split.
I0329 22:48:45.012738 139855925724992 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:48:45.015561 139855925724992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:48:45.015664 139855925724992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:48:45.046404 139855925724992 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:48:52.586566 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 22:53:44.953608 139855925724992 submission_runner.py:380] Time since start: 31.02s, 	Step: 1, 	{'train/accuracy': 0.0006080073653720319, 'train/loss': 11.094049453735352, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.078652381896973, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.107203483581543, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0329 22:53:44.954369 139855925724992 submission_runner.py:390] After eval at step 1: RAM USED (GB) 11.903614976
I0329 22:53:44.963082 139668568565504 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=30.797326, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.107203, test/num_examples=3003, total_duration=31.015022, train/accuracy=0.000608, train/bleu=0.000000, train/loss=11.094049, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.078652, validation/num_examples=3000
I0329 22:53:45.659569 139855925724992 checkpoints.py:356] Saving checkpoint at step: 1
I0329 22:53:48.174135 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_1
I0329 22:53:48.176891 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_1.
I0329 22:53:48.196011 139855925724992 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 12.202602496
I0329 22:53:48.198425 139855925724992 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 12.202602496
I0329 22:53:48.269518 139855925724992 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 12.20220928
I0329 22:54:24.024728 139668576958208 logging_writer.py:48] [100] global_step=100, grad_norm=0.16828395426273346, loss=9.009958267211914
I0329 22:54:59.858918 139668702848768 logging_writer.py:48] [200] global_step=200, grad_norm=0.13157036900520325, loss=8.906054496765137
I0329 22:55:35.675742 139668576958208 logging_writer.py:48] [300] global_step=300, grad_norm=0.13913045823574066, loss=8.791769981384277
I0329 22:56:11.525957 139668702848768 logging_writer.py:48] [400] global_step=400, grad_norm=0.19939294457435608, loss=8.649576187133789
I0329 22:56:47.396136 139668576958208 logging_writer.py:48] [500] global_step=500, grad_norm=0.3662590980529785, loss=8.450431823730469
I0329 22:57:23.299900 139668702848768 logging_writer.py:48] [600] global_step=600, grad_norm=0.8352652192115784, loss=8.243598937988281
I0329 22:57:59.179453 139668576958208 logging_writer.py:48] [700] global_step=700, grad_norm=0.6322688460350037, loss=8.172313690185547
I0329 22:58:35.055533 139668702848768 logging_writer.py:48] [800] global_step=800, grad_norm=0.5517100691795349, loss=8.00261402130127
I0329 22:59:10.952595 139668576958208 logging_writer.py:48] [900] global_step=900, grad_norm=0.847693145275116, loss=7.9405951499938965
I0329 22:59:46.889064 139668702848768 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8616344332695007, loss=7.833594799041748
I0329 23:00:22.813101 139668576958208 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6843066215515137, loss=7.694936275482178
I0329 23:00:58.715311 139668702848768 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7000908255577087, loss=7.600711345672607
I0329 23:01:34.625650 139668576958208 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5443459153175354, loss=7.662661552429199
I0329 23:02:10.485227 139668702848768 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5975876450538635, loss=7.523907661437988
I0329 23:02:46.378961 139668576958208 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6885025501251221, loss=7.431773662567139
I0329 23:03:22.291973 139668702848768 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.662064254283905, loss=7.345218181610107
I0329 23:03:58.263640 139668576958208 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.58564692735672, loss=7.262772560119629
I0329 23:04:34.172620 139668702848768 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8942619562149048, loss=7.176278591156006
I0329 23:05:10.108803 139668576958208 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6871597766876221, loss=7.157546043395996
I0329 23:05:46.009460 139668702848768 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9276594519615173, loss=7.037556171417236
I0329 23:06:21.948680 139668576958208 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9600722789764404, loss=6.983191013336182
I0329 23:06:57.873886 139668702848768 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6438552141189575, loss=6.936714172363281
I0329 23:07:33.803114 139668576958208 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0023144483566284, loss=6.893226623535156
I0329 23:07:48.215766 139855925724992 submission_runner.py:371] Before eval at step 2342: RAM USED (GB) 12.287791104
I0329 23:07:48.215962 139855925724992 spec.py:298] Evaluating on the training split.
I0329 23:07:51.250341 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 23:12:42.234572 139855925724992 spec.py:310] Evaluating on the validation split.
I0329 23:12:44.932156 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 23:17:24.117767 139855925724992 spec.py:326] Evaluating on the test split.
I0329 23:17:26.880343 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 23:22:18.154041 139855925724992 submission_runner.py:380] Time since start: 1797.34s, 	Step: 2342, 	{'train/accuracy': 0.3093027174472809, 'train/loss': 5.556392192840576, 'train/bleu': 6.962651271800588, 'validation/accuracy': 0.2829227149486542, 'validation/loss': 5.837581157684326, 'validation/bleu': 3.7891007639789547, 'validation/num_examples': 3000, 'test/accuracy': 0.2634826600551605, 'test/loss': 6.122750759124756, 'test/bleu': 2.568551275313094, 'test/num_examples': 3003}
I0329 23:22:18.154541 139855925724992 submission_runner.py:390] After eval at step 2342: RAM USED (GB) 12.45200384
I0329 23:22:18.162678 139668702848768 logging_writer.py:48] [2342] global_step=2342, preemption_count=0, score=866.770350, test/accuracy=0.263483, test/bleu=2.568551, test/loss=6.122751, test/num_examples=3003, total_duration=1797.343239, train/accuracy=0.309303, train/bleu=6.962651, train/loss=5.556392, validation/accuracy=0.282923, validation/bleu=3.789101, validation/loss=5.837581, validation/num_examples=3000
I0329 23:22:18.856430 139855925724992 checkpoints.py:356] Saving checkpoint at step: 2342
I0329 23:22:21.337016 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_2342
I0329 23:22:21.339723 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_2342.
I0329 23:22:21.344084 139855925724992 submission_runner.py:409] After logging and checkpointing eval at step 2342: RAM USED (GB) 13.179441152
I0329 23:22:42.577701 139668576958208 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9019784927368164, loss=6.832818031311035
I0329 23:23:18.496919 139668677670656 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6251136064529419, loss=6.741774559020996
I0329 23:23:54.457503 139668576958208 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6959565877914429, loss=6.714325428009033
I0329 23:24:30.389124 139668677670656 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7389280796051025, loss=6.612800598144531
I0329 23:25:06.352393 139668576958208 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6734251976013184, loss=6.59994649887085
I0329 23:25:42.320924 139668677670656 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6283387541770935, loss=6.5302324295043945
I0329 23:26:18.251073 139668576958208 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7069909572601318, loss=6.431954383850098
I0329 23:26:54.186666 139668677670656 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6514133810997009, loss=6.385103225708008
I0329 23:27:30.128201 139668576958208 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5753801465034485, loss=6.371423721313477
I0329 23:28:06.047742 139668677670656 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7161251902580261, loss=6.323712348937988
I0329 23:28:42.024796 139668576958208 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7490260601043701, loss=6.292443752288818
I0329 23:29:17.942842 139668677670656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6569410562515259, loss=6.161409854888916
I0329 23:29:53.862610 139668576958208 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6942598223686218, loss=6.127318382263184
I0329 23:30:29.821494 139668677670656 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6111212968826294, loss=6.101665019989014
I0329 23:31:05.754396 139668576958208 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6441202163696289, loss=5.94816255569458
I0329 23:31:41.684692 139668677670656 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7832292914390564, loss=5.958111763000488
I0329 23:32:17.583511 139668576958208 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6418082118034363, loss=5.915804386138916
I0329 23:32:53.494629 139668677670656 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.605427086353302, loss=5.858120918273926
I0329 23:33:29.413938 139668576958208 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6943778991699219, loss=5.836151599884033
I0329 23:34:05.325586 139668677670656 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5922485589981079, loss=5.676977634429932
I0329 23:34:41.265191 139668576958208 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6798027157783508, loss=5.697019577026367
I0329 23:35:17.203010 139668677670656 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6090516448020935, loss=5.648478984832764
I0329 23:35:53.080412 139668576958208 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.638009786605835, loss=5.618592739105225
I0329 23:36:21.560697 139855925724992 submission_runner.py:371] Before eval at step 4681: RAM USED (GB) 12.557258752
I0329 23:36:21.560918 139855925724992 spec.py:298] Evaluating on the training split.
I0329 23:36:24.595340 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 23:40:05.320846 139855925724992 spec.py:310] Evaluating on the validation split.
I0329 23:40:08.032119 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 23:43:19.610136 139855925724992 spec.py:326] Evaluating on the test split.
I0329 23:43:22.373499 139855925724992 workload.py:179] Translating evaluation dataset.
I0329 23:46:28.824928 139855925724992 submission_runner.py:380] Time since start: 3510.69s, 	Step: 4681, 	{'train/accuracy': 0.4585556983947754, 'train/loss': 3.934053897857666, 'train/bleu': 17.49753779884196, 'validation/accuracy': 0.4453633427619934, 'validation/loss': 4.01553201675415, 'validation/bleu': 13.020880727470228, 'validation/num_examples': 3000, 'test/accuracy': 0.43564000725746155, 'test/loss': 4.163936138153076, 'test/bleu': 11.29609062612925, 'test/num_examples': 3003}
I0329 23:46:28.825441 139855925724992 submission_runner.py:390] After eval at step 4681: RAM USED (GB) 12.745945088
I0329 23:46:28.834069 139668677670656 logging_writer.py:48] [4681] global_step=4681, preemption_count=0, score=1703.470408, test/accuracy=0.435640, test/bleu=11.296091, test/loss=4.163936, test/num_examples=3003, total_duration=3510.688345, train/accuracy=0.458556, train/bleu=17.497538, train/loss=3.934054, validation/accuracy=0.445363, validation/bleu=13.020881, validation/loss=4.015532, validation/num_examples=3000
I0329 23:46:29.518810 139855925724992 checkpoints.py:356] Saving checkpoint at step: 4681
I0329 23:46:32.006062 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_4681
I0329 23:46:32.008723 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_4681.
I0329 23:46:32.012902 139855925724992 submission_runner.py:409] After logging and checkpointing eval at step 4681: RAM USED (GB) 13.4734848
I0329 23:46:39.194073 139668576958208 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.553706705570221, loss=5.543146133422852
I0329 23:47:15.096412 139668660885248 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5934727787971497, loss=5.517196178436279
I0329 23:47:51.009614 139668576958208 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5996794104576111, loss=5.545684814453125
I0329 23:48:26.940498 139668660885248 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5382941961288452, loss=5.473785877227783
I0329 23:49:02.822061 139668576958208 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5132972598075867, loss=5.429483413696289
I0329 23:49:38.745579 139668660885248 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5732069611549377, loss=5.4486799240112305
I0329 23:50:14.644317 139668576958208 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5055526494979858, loss=5.367143154144287
I0329 23:50:50.576695 139668660885248 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.535076379776001, loss=5.366926193237305
I0329 23:51:26.509643 139668576958208 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5121931433677673, loss=5.410550117492676
I0329 23:52:02.447591 139668660885248 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5225614905357361, loss=5.32741641998291
I0329 23:52:38.375202 139668576958208 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.49921944737434387, loss=5.252974033355713
I0329 23:53:14.308963 139668660885248 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5657705068588257, loss=5.299353122711182
I0329 23:53:50.230709 139668576958208 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4769420623779297, loss=5.201982021331787
I0329 23:54:26.137053 139668660885248 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.46544888615608215, loss=5.156033992767334
I0329 23:55:02.008686 139668576958208 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5374065637588501, loss=5.310437202453613
I0329 23:55:37.976851 139668660885248 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5162540674209595, loss=5.151751518249512
I0329 23:56:13.912332 139668576958208 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.49401962757110596, loss=5.063840389251709
I0329 23:56:49.811147 139668660885248 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4349888563156128, loss=5.231326103210449
I0329 23:57:25.735581 139668576958208 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.44473880529403687, loss=5.144151210784912
I0329 23:58:01.635467 139668660885248 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.42301300168037415, loss=5.0648112297058105
I0329 23:58:37.509567 139668576958208 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4865930378437042, loss=5.077499866485596
I0329 23:59:13.419568 139668660885248 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4441896677017212, loss=5.04701566696167
I0329 23:59:49.277625 139668576958208 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4545200765132904, loss=5.044545650482178
I0330 00:00:25.193046 139668660885248 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.42822322249412537, loss=5.063159465789795
I0330 00:00:32.101411 139855925724992 submission_runner.py:371] Before eval at step 7021: RAM USED (GB) 12.98284544
I0330 00:00:32.101658 139855925724992 spec.py:298] Evaluating on the training split.
I0330 00:00:35.146867 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:03:20.433609 139855925724992 spec.py:310] Evaluating on the validation split.
I0330 00:03:23.142403 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:05:57.153371 139855925724992 spec.py:326] Evaluating on the test split.
I0330 00:05:59.910453 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:08:27.042042 139855925724992 submission_runner.py:380] Time since start: 4961.23s, 	Step: 7021, 	{'train/accuracy': 0.5382667183876038, 'train/loss': 3.122663974761963, 'train/bleu': 24.48784203798811, 'validation/accuracy': 0.5293176770210266, 'validation/loss': 3.1689305305480957, 'validation/bleu': 19.42471223023081, 'validation/num_examples': 3000, 'test/accuracy': 0.5296729207038879, 'test/loss': 3.2260031700134277, 'test/bleu': 18.08772363858833, 'test/num_examples': 3003}
I0330 00:08:27.042551 139855925724992 submission_runner.py:390] After eval at step 7021: RAM USED (GB) 13.097422848
I0330 00:08:27.052876 139668576958208 logging_writer.py:48] [7021] global_step=7021, preemption_count=0, score=2540.122937, test/accuracy=0.529673, test/bleu=18.087724, test/loss=3.226003, test/num_examples=3003, total_duration=4961.228873, train/accuracy=0.538267, train/bleu=24.487842, train/loss=3.122664, validation/accuracy=0.529318, validation/bleu=19.424712, validation/loss=3.168931, validation/num_examples=3000
I0330 00:08:27.774485 139855925724992 checkpoints.py:356] Saving checkpoint at step: 7021
I0330 00:08:30.179508 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_7021
I0330 00:08:30.182323 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_7021.
I0330 00:08:30.186570 139855925724992 submission_runner.py:409] After logging and checkpointing eval at step 7021: RAM USED (GB) 13.824806912
I0330 00:08:58.895374 139668660885248 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4199078381061554, loss=4.995048522949219
I0330 00:09:34.772017 139668652492544 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4161050319671631, loss=4.980556488037109
I0330 00:10:10.660434 139668660885248 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.41824254393577576, loss=4.932126522064209
I0330 00:10:46.577203 139668652492544 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.40929144620895386, loss=4.959018230438232
I0330 00:11:22.481957 139668660885248 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4034399688243866, loss=4.90555477142334
I0330 00:11:58.372119 139668652492544 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4310899078845978, loss=5.0279221534729
I0330 00:12:34.243276 139668660885248 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.3971724510192871, loss=4.911468982696533
I0330 00:13:10.143746 139668652492544 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4038006365299225, loss=4.9013519287109375
I0330 00:13:46.012046 139668660885248 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4065350890159607, loss=4.941980361938477
I0330 00:14:21.879368 139668652492544 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.39003318548202515, loss=4.920500755310059
I0330 00:14:57.743319 139668660885248 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3687019348144531, loss=4.903015613555908
I0330 00:15:33.616345 139668652492544 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.41553235054016113, loss=4.956799030303955
I0330 00:16:09.491697 139668660885248 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4080144166946411, loss=4.882744789123535
I0330 00:16:45.366881 139668652492544 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3924885392189026, loss=4.910787105560303
I0330 00:17:21.263264 139668660885248 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.37292346358299255, loss=4.841315269470215
I0330 00:17:57.144945 139668652492544 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.37551289796829224, loss=4.811440467834473
I0330 00:18:33.048692 139668660885248 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.37529468536376953, loss=4.798173427581787
I0330 00:19:08.956005 139668652492544 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3685683608055115, loss=4.834272861480713
I0330 00:19:44.843195 139668660885248 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3957156240940094, loss=4.836359977722168
I0330 00:20:20.740075 139668652492544 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.35436224937438965, loss=4.802091598510742
I0330 00:20:56.595941 139668660885248 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3858984708786011, loss=4.807777404785156
I0330 00:21:32.468493 139668652492544 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.37594982981681824, loss=4.879456996917725
I0330 00:22:08.365794 139668660885248 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.37361422181129456, loss=4.806484222412109
I0330 00:22:30.309825 139855925724992 submission_runner.py:371] Before eval at step 9363: RAM USED (GB) 13.38775552
I0330 00:22:30.310020 139855925724992 spec.py:298] Evaluating on the training split.
I0330 00:22:33.349692 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:25:11.342623 139855925724992 spec.py:310] Evaluating on the validation split.
I0330 00:25:14.026164 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:27:37.644032 139855925724992 spec.py:326] Evaluating on the test split.
I0330 00:27:40.387848 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:30:01.655336 139855925724992 submission_runner.py:380] Time since start: 6279.44s, 	Step: 9363, 	{'train/accuracy': 0.5664786696434021, 'train/loss': 2.8653457164764404, 'train/bleu': 25.62643694794151, 'validation/accuracy': 0.5674945116043091, 'validation/loss': 2.816063642501831, 'validation/bleu': 21.945195814440016, 'validation/num_examples': 3000, 'test/accuracy': 0.5671373009681702, 'test/loss': 2.8438193798065186, 'test/bleu': 20.057926562562113, 'test/num_examples': 3003}
I0330 00:30:01.655872 139855925724992 submission_runner.py:390] After eval at step 9363: RAM USED (GB) 13.491531776
I0330 00:30:01.665862 139668652492544 logging_writer.py:48] [9363] global_step=9363, preemption_count=0, score=3377.184757, test/accuracy=0.567137, test/bleu=20.057927, test/loss=2.843819, test/num_examples=3003, total_duration=6279.437582, train/accuracy=0.566479, train/bleu=25.626437, train/loss=2.865346, validation/accuracy=0.567495, validation/bleu=21.945196, validation/loss=2.816064, validation/num_examples=3000
I0330 00:30:02.534886 139855925724992 checkpoints.py:356] Saving checkpoint at step: 9363
I0330 00:30:05.540612 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_9363
I0330 00:30:05.543903 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_9363.
I0330 00:30:05.549000 139855925724992 submission_runner.py:409] After logging and checkpointing eval at step 9363: RAM USED (GB) 14.210797568
I0330 00:30:19.176589 139668660885248 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3804808259010315, loss=4.810916900634766
I0330 00:30:55.075851 139668373554944 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.36873626708984375, loss=4.759827613830566
I0330 00:31:30.967229 139668660885248 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.38631802797317505, loss=4.756370544433594
I0330 00:32:06.876022 139668373554944 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3534405529499054, loss=4.752368927001953
I0330 00:32:42.791989 139668660885248 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3449002802371979, loss=4.6765570640563965
I0330 00:33:18.715378 139668373554944 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3622892200946808, loss=4.745972633361816
I0330 00:33:53.945427 139855925724992 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 13.48661248
I0330 00:33:53.945625 139855925724992 spec.py:298] Evaluating on the training split.
I0330 00:33:56.973606 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:36:40.899720 139855925724992 spec.py:310] Evaluating on the validation split.
I0330 00:36:43.600999 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:39:11.299435 139855925724992 spec.py:326] Evaluating on the test split.
I0330 00:39:14.078012 139855925724992 workload.py:179] Translating evaluation dataset.
I0330 00:41:38.539864 139855925724992 submission_runner.py:380] Time since start: 6963.07s, 	Step: 10000, 	{'train/accuracy': 0.5714614987373352, 'train/loss': 2.8178937435150146, 'train/bleu': 26.39931917079628, 'validation/accuracy': 0.5733717083930969, 'validation/loss': 2.745426893234253, 'validation/bleu': 22.341497049804968, 'validation/num_examples': 3000, 'test/accuracy': 0.5753994584083557, 'test/loss': 2.771693229675293, 'test/bleu': 20.933697932335615, 'test/num_examples': 3003}
I0330 00:41:38.540515 139855925724992 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 13.53807872
I0330 00:41:38.550641 139668660885248 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3604.732667, test/accuracy=0.575399, test/bleu=20.933698, test/loss=2.771693, test/num_examples=3003, total_duration=6963.073157, train/accuracy=0.571461, train/bleu=26.399319, train/loss=2.817894, validation/accuracy=0.573372, validation/bleu=22.341497, validation/loss=2.745427, validation/num_examples=3000
I0330 00:41:39.385103 139855925724992 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 00:41:42.372464 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_10000
I0330 00:41:42.375574 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_10000.
I0330 00:41:42.380480 139855925724992 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.266396672
I0330 00:41:42.388459 139668373554944 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3604.732667
I0330 00:41:42.766717 139855925724992 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 00:41:47.038871 139855925724992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_10000
I0330 00:41:47.042157 139855925724992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_jax/trial_1/checkpoint_10000.
I0330 00:41:47.076465 139855925724992 submission_runner.py:543] Tuning trial 1/1
I0330 00:41:47.076694 139855925724992 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0330 00:41:47.077630 139855925724992 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006080073653720319, 'train/loss': 11.094049453735352, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.078652381896973, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.107203483581543, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.79732632637024, 'total_duration': 31.015021562576294, 'global_step': 1, 'preemption_count': 0}), (2342, {'train/accuracy': 0.3093027174472809, 'train/loss': 5.556392192840576, 'train/bleu': 6.962651271800588, 'validation/accuracy': 0.2829227149486542, 'validation/loss': 5.837581157684326, 'validation/bleu': 3.7891007639789547, 'validation/num_examples': 3000, 'test/accuracy': 0.2634826600551605, 'test/loss': 6.122750759124756, 'test/bleu': 2.568551275313094, 'test/num_examples': 3003, 'score': 866.770349740982, 'total_duration': 1797.3432393074036, 'global_step': 2342, 'preemption_count': 0}), (4681, {'train/accuracy': 0.4585556983947754, 'train/loss': 3.934053897857666, 'train/bleu': 17.49753779884196, 'validation/accuracy': 0.4453633427619934, 'validation/loss': 4.01553201675415, 'validation/bleu': 13.020880727470228, 'validation/num_examples': 3000, 'test/accuracy': 0.43564000725746155, 'test/loss': 4.163936138153076, 'test/bleu': 11.29609062612925, 'test/num_examples': 3003, 'score': 1703.4704084396362, 'total_duration': 3510.6883454322815, 'global_step': 4681, 'preemption_count': 0}), (7021, {'train/accuracy': 0.5382667183876038, 'train/loss': 3.122663974761963, 'train/bleu': 24.48784203798811, 'validation/accuracy': 0.5293176770210266, 'validation/loss': 3.1689305305480957, 'validation/bleu': 19.42471223023081, 'validation/num_examples': 3000, 'test/accuracy': 0.5296729207038879, 'test/loss': 3.2260031700134277, 'test/bleu': 18.08772363858833, 'test/num_examples': 3003, 'score': 2540.122937440872, 'total_duration': 4961.228873491287, 'global_step': 7021, 'preemption_count': 0}), (9363, {'train/accuracy': 0.5664786696434021, 'train/loss': 2.8653457164764404, 'train/bleu': 25.62643694794151, 'validation/accuracy': 0.5674945116043091, 'validation/loss': 2.816063642501831, 'validation/bleu': 21.945195814440016, 'validation/num_examples': 3000, 'test/accuracy': 0.5671373009681702, 'test/loss': 2.8438193798065186, 'test/bleu': 20.057926562562113, 'test/num_examples': 3003, 'score': 3377.1847574710846, 'total_duration': 6279.43758225441, 'global_step': 9363, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5714614987373352, 'train/loss': 2.8178937435150146, 'train/bleu': 26.39931917079628, 'validation/accuracy': 0.5733717083930969, 'validation/loss': 2.745426893234253, 'validation/bleu': 22.341497049804968, 'validation/num_examples': 3000, 'test/accuracy': 0.5753994584083557, 'test/loss': 2.771693229675293, 'test/bleu': 20.933697932335615, 'test/num_examples': 3003, 'score': 3604.7326667308807, 'total_duration': 6963.073157072067, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0330 00:41:47.077728 139855925724992 submission_runner.py:546] Timing: 3604.7326667308807
I0330 00:41:47.077777 139855925724992 submission_runner.py:547] ====================
I0330 00:41:47.077867 139855925724992 submission_runner.py:606] Final wmt score: 3604.7326667308807
