WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 02:37:23.204740 139843768854336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 02:37:23.204771 139717582325568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 02:37:23.205588 140401491769152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 02:37:23.205603 139711084889920 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 02:37:23.205681 140717354313536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 02:37:23.206061 140291716740928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 02:37:23.206107 139881462568768 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 02:37:23.215981 140203476997952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 02:37:23.216114 139711084889920 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.216212 140401491769152 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.216281 140203476997952 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.216272 140717354313536 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.216609 140291716740928 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.216635 139881462568768 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.225820 139717582325568 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:23.225843 139843768854336 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:37:27.071682 140203476997952 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/wmt_pytorch.
W0401 02:37:27.137431 139881462568768 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.138138 140291716740928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.139163 139843768854336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.139365 139711084889920 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.141334 140401491769152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.141720 140203476997952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.142195 139717582325568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:37:27.143881 140717354313536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 02:37:27.145619 140203476997952 submission_runner.py:504] Using RNG seed 2969899065
I0401 02:37:27.146627 140203476997952 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 02:37:27.146750 140203476997952 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/wmt_pytorch/trial_1.
I0401 02:37:27.146950 140203476997952 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/hparams.json.
I0401 02:37:27.148066 140203476997952 submission_runner.py:230] Starting train once: RAM USED (GB) 15.253409792
I0401 02:37:27.148175 140203476997952 submission_runner.py:231] Initializing dataset.
I0401 02:37:27.148354 140203476997952 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.253409792
I0401 02:37:27.148439 140203476997952 submission_runner.py:240] Initializing model.
I0401 02:37:30.747280 140203476997952 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.660734464
I0401 02:37:30.747462 140203476997952 submission_runner.py:252] Initializing optimizer.
I0401 02:37:30.851689 140203476997952 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.6642816
I0401 02:37:30.851867 140203476997952 submission_runner.py:261] Initializing metrics bundle.
I0401 02:37:30.851916 140203476997952 submission_runner.py:275] Initializing checkpoint and logger.
I0401 02:37:30.853122 140203476997952 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 02:37:30.853267 140203476997952 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 02:37:31.497184 140203476997952 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/meta_data_0.json.
I0401 02:37:31.498073 140203476997952 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/flags_0.json.
I0401 02:37:31.529525 140203476997952 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 19.719323648
I0401 02:37:31.530651 140203476997952 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.719323648
I0401 02:37:31.530785 140203476997952 submission_runner.py:312] Starting training loop.
I0401 02:37:31.540694 140203476997952 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:37:31.544528 140203476997952 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:37:31.544642 140203476997952 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:37:31.596194 140203476997952 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:37:33.713671 140203476997952 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 20.011290624
I0401 02:37:35.209211 140155771934464 logging_writer.py:48] [0] global_step=0, grad_norm=5.115481, loss=11.012745
I0401 02:37:35.214513 140203476997952 submission.py:139] 0) loss = 11.013, grad_norm = 5.115
I0401 02:37:35.216311 140203476997952 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 20.11375616
I0401 02:37:35.217154 140203476997952 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 20.11375616
I0401 02:37:35.217310 140203476997952 spec.py:298] Evaluating on the training split.
I0401 02:37:35.220517 140203476997952 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:37:35.223660 140203476997952 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:37:35.223783 140203476997952 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:37:35.254729 140203476997952 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:37:39.397602 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 02:42:13.752997 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 02:42:13.756963 140203476997952 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:42:13.760690 140203476997952 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:42:13.760819 140203476997952 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:42:13.790654 140203476997952 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:42:17.632660 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 02:46:47.007616 140203476997952 spec.py:326] Evaluating on the test split.
I0401 02:46:47.010249 140203476997952 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:46:47.013131 140203476997952 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:46:47.013333 140203476997952 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:46:47.040898 140203476997952 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:46:50.947172 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 02:51:25.733270 140203476997952 submission_runner.py:380] Time since start: 3.69s, 	Step: 1, 	{'train/accuracy': 0.000673539048140918, 'train/loss': 10.97117623891229, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 10.961435692055895, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 10.967822903956773, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0401 02:51:25.733723 140203476997952 submission_runner.py:390] After eval at step 1: RAM USED (GB) 20.45155328
I0401 02:51:25.741683 140145763108608 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.684832, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.967823, test/num_examples=3003, total_duration=3.686823, train/accuracy=0.000674, train/bleu=0.000000, train/loss=10.971176, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.961436, validation/num_examples=3000
I0401 02:51:27.268106 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_1.
I0401 02:51:27.268770 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 20.451577856
I0401 02:51:27.273253 140203476997952 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 20.451577856
I0401 02:51:27.276955 140203476997952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.276990 140717354313536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.277002 139711084889920 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.277029 139843768854336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.277078 139717582325568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.277097 140291716740928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.277107 140401491769152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.277343 139881462568768 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:51:27.701300 140145754715904 logging_writer.py:48] [1] global_step=1, grad_norm=5.159334, loss=11.006272
I0401 02:51:27.704443 140203476997952 submission.py:139] 1) loss = 11.006, grad_norm = 5.159
I0401 02:51:27.705219 140203476997952 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 20.451454976
I0401 02:51:28.138381 140145763108608 logging_writer.py:48] [2] global_step=2, grad_norm=5.105200, loss=11.005116
I0401 02:51:28.141986 140203476997952 submission.py:139] 2) loss = 11.005, grad_norm = 5.105
I0401 02:51:28.573935 140145754715904 logging_writer.py:48] [3] global_step=3, grad_norm=5.064508, loss=10.999413
I0401 02:51:28.577110 140203476997952 submission.py:139] 3) loss = 10.999, grad_norm = 5.065
I0401 02:51:29.012742 140145763108608 logging_writer.py:48] [4] global_step=4, grad_norm=4.964488, loss=10.985058
I0401 02:51:29.016159 140203476997952 submission.py:139] 4) loss = 10.985, grad_norm = 4.964
I0401 02:51:29.451196 140145754715904 logging_writer.py:48] [5] global_step=5, grad_norm=4.893972, loss=10.938320
I0401 02:51:29.454469 140203476997952 submission.py:139] 5) loss = 10.938, grad_norm = 4.894
I0401 02:51:29.886142 140145763108608 logging_writer.py:48] [6] global_step=6, grad_norm=4.804052, loss=10.912004
I0401 02:51:29.889463 140203476997952 submission.py:139] 6) loss = 10.912, grad_norm = 4.804
I0401 02:51:30.319616 140145754715904 logging_writer.py:48] [7] global_step=7, grad_norm=4.555546, loss=10.858364
I0401 02:51:30.323076 140203476997952 submission.py:139] 7) loss = 10.858, grad_norm = 4.556
I0401 02:51:30.757776 140145763108608 logging_writer.py:48] [8] global_step=8, grad_norm=4.375022, loss=10.797246
I0401 02:51:30.760997 140203476997952 submission.py:139] 8) loss = 10.797, grad_norm = 4.375
I0401 02:51:31.197251 140145754715904 logging_writer.py:48] [9] global_step=9, grad_norm=4.091750, loss=10.716357
I0401 02:51:31.200160 140203476997952 submission.py:139] 9) loss = 10.716, grad_norm = 4.092
I0401 02:51:31.632023 140145763108608 logging_writer.py:48] [10] global_step=10, grad_norm=3.746564, loss=10.650149
I0401 02:51:31.635033 140203476997952 submission.py:139] 10) loss = 10.650, grad_norm = 3.747
I0401 02:51:32.071420 140145754715904 logging_writer.py:48] [11] global_step=11, grad_norm=3.417711, loss=10.572163
I0401 02:51:32.074537 140203476997952 submission.py:139] 11) loss = 10.572, grad_norm = 3.418
I0401 02:51:32.507004 140145763108608 logging_writer.py:48] [12] global_step=12, grad_norm=3.154562, loss=10.471589
I0401 02:51:32.510057 140203476997952 submission.py:139] 12) loss = 10.472, grad_norm = 3.155
I0401 02:51:32.948656 140145754715904 logging_writer.py:48] [13] global_step=13, grad_norm=2.861543, loss=10.401999
I0401 02:51:32.952003 140203476997952 submission.py:139] 13) loss = 10.402, grad_norm = 2.862
I0401 02:51:33.386124 140145763108608 logging_writer.py:48] [14] global_step=14, grad_norm=2.590025, loss=10.312949
I0401 02:51:33.389272 140203476997952 submission.py:139] 14) loss = 10.313, grad_norm = 2.590
I0401 02:51:33.821801 140145754715904 logging_writer.py:48] [15] global_step=15, grad_norm=2.331372, loss=10.242641
I0401 02:51:33.825115 140203476997952 submission.py:139] 15) loss = 10.243, grad_norm = 2.331
I0401 02:51:34.256795 140145763108608 logging_writer.py:48] [16] global_step=16, grad_norm=2.146639, loss=10.174688
I0401 02:51:34.259972 140203476997952 submission.py:139] 16) loss = 10.175, grad_norm = 2.147
I0401 02:51:34.694088 140145754715904 logging_writer.py:48] [17] global_step=17, grad_norm=1.928685, loss=10.125139
I0401 02:51:34.697126 140203476997952 submission.py:139] 17) loss = 10.125, grad_norm = 1.929
I0401 02:51:35.133211 140145763108608 logging_writer.py:48] [18] global_step=18, grad_norm=1.773748, loss=10.068452
I0401 02:51:35.136614 140203476997952 submission.py:139] 18) loss = 10.068, grad_norm = 1.774
I0401 02:51:35.569441 140145754715904 logging_writer.py:48] [19] global_step=19, grad_norm=1.682672, loss=10.002444
I0401 02:51:35.572694 140203476997952 submission.py:139] 19) loss = 10.002, grad_norm = 1.683
I0401 02:51:36.006011 140145763108608 logging_writer.py:48] [20] global_step=20, grad_norm=1.589557, loss=9.958930
I0401 02:51:36.009196 140203476997952 submission.py:139] 20) loss = 9.959, grad_norm = 1.590
I0401 02:51:36.443848 140145754715904 logging_writer.py:48] [21] global_step=21, grad_norm=1.497646, loss=9.930311
I0401 02:51:36.446837 140203476997952 submission.py:139] 21) loss = 9.930, grad_norm = 1.498
I0401 02:51:36.882826 140145763108608 logging_writer.py:48] [22] global_step=22, grad_norm=1.473243, loss=9.870620
I0401 02:51:36.885885 140203476997952 submission.py:139] 22) loss = 9.871, grad_norm = 1.473
I0401 02:51:37.317251 140145754715904 logging_writer.py:48] [23] global_step=23, grad_norm=1.400395, loss=9.836304
I0401 02:51:37.320853 140203476997952 submission.py:139] 23) loss = 9.836, grad_norm = 1.400
I0401 02:51:37.754136 140145763108608 logging_writer.py:48] [24] global_step=24, grad_norm=1.359988, loss=9.791714
I0401 02:51:37.757344 140203476997952 submission.py:139] 24) loss = 9.792, grad_norm = 1.360
I0401 02:51:38.188052 140145754715904 logging_writer.py:48] [25] global_step=25, grad_norm=1.332243, loss=9.766588
I0401 02:51:38.191230 140203476997952 submission.py:139] 25) loss = 9.767, grad_norm = 1.332
I0401 02:51:38.624415 140145763108608 logging_writer.py:48] [26] global_step=26, grad_norm=1.266574, loss=9.734334
I0401 02:51:38.627504 140203476997952 submission.py:139] 26) loss = 9.734, grad_norm = 1.267
I0401 02:51:39.063903 140145754715904 logging_writer.py:48] [27] global_step=27, grad_norm=1.171988, loss=9.707579
I0401 02:51:39.067086 140203476997952 submission.py:139] 27) loss = 9.708, grad_norm = 1.172
I0401 02:51:39.499385 140145763108608 logging_writer.py:48] [28] global_step=28, grad_norm=1.092363, loss=9.686898
I0401 02:51:39.502490 140203476997952 submission.py:139] 28) loss = 9.687, grad_norm = 1.092
I0401 02:51:39.934990 140145754715904 logging_writer.py:48] [29] global_step=29, grad_norm=1.012295, loss=9.646419
I0401 02:51:39.938449 140203476997952 submission.py:139] 29) loss = 9.646, grad_norm = 1.012
I0401 02:51:40.369044 140145763108608 logging_writer.py:48] [30] global_step=30, grad_norm=0.963056, loss=9.600635
I0401 02:51:40.372473 140203476997952 submission.py:139] 30) loss = 9.601, grad_norm = 0.963
I0401 02:51:40.804213 140145754715904 logging_writer.py:48] [31] global_step=31, grad_norm=0.891980, loss=9.585786
I0401 02:51:40.807452 140203476997952 submission.py:139] 31) loss = 9.586, grad_norm = 0.892
I0401 02:51:41.242254 140145763108608 logging_writer.py:48] [32] global_step=32, grad_norm=0.835164, loss=9.550941
I0401 02:51:41.245780 140203476997952 submission.py:139] 32) loss = 9.551, grad_norm = 0.835
I0401 02:51:41.681098 140145754715904 logging_writer.py:48] [33] global_step=33, grad_norm=0.804121, loss=9.548722
I0401 02:51:41.684918 140203476997952 submission.py:139] 33) loss = 9.549, grad_norm = 0.804
I0401 02:51:42.119663 140145763108608 logging_writer.py:48] [34] global_step=34, grad_norm=0.795870, loss=9.505811
I0401 02:51:42.123282 140203476997952 submission.py:139] 34) loss = 9.506, grad_norm = 0.796
I0401 02:51:42.554235 140145754715904 logging_writer.py:48] [35] global_step=35, grad_norm=0.753222, loss=9.532575
I0401 02:51:42.557765 140203476997952 submission.py:139] 35) loss = 9.533, grad_norm = 0.753
I0401 02:51:42.993118 140145763108608 logging_writer.py:48] [36] global_step=36, grad_norm=0.743370, loss=9.490361
I0401 02:51:42.996348 140203476997952 submission.py:139] 36) loss = 9.490, grad_norm = 0.743
I0401 02:51:43.428334 140145754715904 logging_writer.py:48] [37] global_step=37, grad_norm=0.725339, loss=9.459226
I0401 02:51:43.431779 140203476997952 submission.py:139] 37) loss = 9.459, grad_norm = 0.725
I0401 02:51:43.867522 140145763108608 logging_writer.py:48] [38] global_step=38, grad_norm=0.724658, loss=9.470295
I0401 02:51:43.871488 140203476997952 submission.py:139] 38) loss = 9.470, grad_norm = 0.725
I0401 02:51:44.310827 140145754715904 logging_writer.py:48] [39] global_step=39, grad_norm=0.681046, loss=9.468736
I0401 02:51:44.314172 140203476997952 submission.py:139] 39) loss = 9.469, grad_norm = 0.681
I0401 02:51:44.745595 140145763108608 logging_writer.py:48] [40] global_step=40, grad_norm=0.681782, loss=9.420786
I0401 02:51:44.749541 140203476997952 submission.py:139] 40) loss = 9.421, grad_norm = 0.682
I0401 02:51:45.187518 140145754715904 logging_writer.py:48] [41] global_step=41, grad_norm=0.641899, loss=9.434175
I0401 02:51:45.191880 140203476997952 submission.py:139] 41) loss = 9.434, grad_norm = 0.642
I0401 02:51:45.625273 140145763108608 logging_writer.py:48] [42] global_step=42, grad_norm=0.618791, loss=9.403442
I0401 02:51:45.628887 140203476997952 submission.py:139] 42) loss = 9.403, grad_norm = 0.619
I0401 02:51:46.063904 140145754715904 logging_writer.py:48] [43] global_step=43, grad_norm=0.623597, loss=9.374984
I0401 02:51:46.067577 140203476997952 submission.py:139] 43) loss = 9.375, grad_norm = 0.624
I0401 02:51:46.501731 140145763108608 logging_writer.py:48] [44] global_step=44, grad_norm=0.594389, loss=9.365143
I0401 02:51:46.505074 140203476997952 submission.py:139] 44) loss = 9.365, grad_norm = 0.594
I0401 02:51:46.938264 140145754715904 logging_writer.py:48] [45] global_step=45, grad_norm=0.559041, loss=9.336830
I0401 02:51:46.941797 140203476997952 submission.py:139] 45) loss = 9.337, grad_norm = 0.559
I0401 02:51:47.374573 140145763108608 logging_writer.py:48] [46] global_step=46, grad_norm=0.544063, loss=9.350225
I0401 02:51:47.378112 140203476997952 submission.py:139] 46) loss = 9.350, grad_norm = 0.544
I0401 02:51:47.814507 140145754715904 logging_writer.py:48] [47] global_step=47, grad_norm=0.521867, loss=9.363628
I0401 02:51:47.818195 140203476997952 submission.py:139] 47) loss = 9.364, grad_norm = 0.522
I0401 02:51:48.252550 140145763108608 logging_writer.py:48] [48] global_step=48, grad_norm=0.501112, loss=9.302005
I0401 02:51:48.255892 140203476997952 submission.py:139] 48) loss = 9.302, grad_norm = 0.501
I0401 02:51:48.692690 140145754715904 logging_writer.py:48] [49] global_step=49, grad_norm=0.465761, loss=9.300773
I0401 02:51:48.696218 140203476997952 submission.py:139] 49) loss = 9.301, grad_norm = 0.466
I0401 02:51:49.129583 140145763108608 logging_writer.py:48] [50] global_step=50, grad_norm=0.468625, loss=9.302354
I0401 02:51:49.133437 140203476997952 submission.py:139] 50) loss = 9.302, grad_norm = 0.469
I0401 02:51:49.566722 140145754715904 logging_writer.py:48] [51] global_step=51, grad_norm=0.451189, loss=9.281957
I0401 02:51:49.570389 140203476997952 submission.py:139] 51) loss = 9.282, grad_norm = 0.451
I0401 02:51:50.003134 140145763108608 logging_writer.py:48] [52] global_step=52, grad_norm=0.450810, loss=9.294004
I0401 02:51:50.006602 140203476997952 submission.py:139] 52) loss = 9.294, grad_norm = 0.451
I0401 02:51:50.443452 140145754715904 logging_writer.py:48] [53] global_step=53, grad_norm=0.446958, loss=9.270664
I0401 02:51:50.447005 140203476997952 submission.py:139] 53) loss = 9.271, grad_norm = 0.447
I0401 02:51:50.880416 140145763108608 logging_writer.py:48] [54] global_step=54, grad_norm=0.430748, loss=9.297216
I0401 02:51:50.883979 140203476997952 submission.py:139] 54) loss = 9.297, grad_norm = 0.431
I0401 02:51:51.317988 140145754715904 logging_writer.py:48] [55] global_step=55, grad_norm=0.435584, loss=9.257599
I0401 02:51:51.321404 140203476997952 submission.py:139] 55) loss = 9.258, grad_norm = 0.436
I0401 02:51:51.754306 140145763108608 logging_writer.py:48] [56] global_step=56, grad_norm=0.420604, loss=9.275294
I0401 02:51:51.758088 140203476997952 submission.py:139] 56) loss = 9.275, grad_norm = 0.421
I0401 02:51:52.190394 140145754715904 logging_writer.py:48] [57] global_step=57, grad_norm=0.419947, loss=9.231970
I0401 02:51:52.194157 140203476997952 submission.py:139] 57) loss = 9.232, grad_norm = 0.420
I0401 02:51:52.628890 140145763108608 logging_writer.py:48] [58] global_step=58, grad_norm=0.397181, loss=9.237751
I0401 02:51:52.632593 140203476997952 submission.py:139] 58) loss = 9.238, grad_norm = 0.397
I0401 02:51:53.064698 140145754715904 logging_writer.py:48] [59] global_step=59, grad_norm=0.387992, loss=9.231463
I0401 02:51:53.068318 140203476997952 submission.py:139] 59) loss = 9.231, grad_norm = 0.388
I0401 02:51:53.499512 140145763108608 logging_writer.py:48] [60] global_step=60, grad_norm=0.376531, loss=9.246352
I0401 02:51:53.503117 140203476997952 submission.py:139] 60) loss = 9.246, grad_norm = 0.377
I0401 02:51:53.939012 140145754715904 logging_writer.py:48] [61] global_step=61, grad_norm=0.358467, loss=9.204585
I0401 02:51:53.942886 140203476997952 submission.py:139] 61) loss = 9.205, grad_norm = 0.358
I0401 02:51:54.377135 140145763108608 logging_writer.py:48] [62] global_step=62, grad_norm=0.344044, loss=9.229759
I0401 02:51:54.380733 140203476997952 submission.py:139] 62) loss = 9.230, grad_norm = 0.344
I0401 02:51:54.816809 140145754715904 logging_writer.py:48] [63] global_step=63, grad_norm=0.342410, loss=9.204607
I0401 02:51:54.820435 140203476997952 submission.py:139] 63) loss = 9.205, grad_norm = 0.342
I0401 02:51:55.256163 140145763108608 logging_writer.py:48] [64] global_step=64, grad_norm=0.324391, loss=9.213505
I0401 02:51:55.259621 140203476997952 submission.py:139] 64) loss = 9.214, grad_norm = 0.324
I0401 02:51:55.690464 140145754715904 logging_writer.py:48] [65] global_step=65, grad_norm=0.321973, loss=9.185851
I0401 02:51:55.693964 140203476997952 submission.py:139] 65) loss = 9.186, grad_norm = 0.322
I0401 02:51:56.126731 140145763108608 logging_writer.py:48] [66] global_step=66, grad_norm=0.319179, loss=9.167500
I0401 02:51:56.130580 140203476997952 submission.py:139] 66) loss = 9.167, grad_norm = 0.319
I0401 02:51:56.564229 140145754715904 logging_writer.py:48] [67] global_step=67, grad_norm=0.304226, loss=9.197514
I0401 02:51:56.567720 140203476997952 submission.py:139] 67) loss = 9.198, grad_norm = 0.304
I0401 02:51:57.003399 140145763108608 logging_writer.py:48] [68] global_step=68, grad_norm=0.315036, loss=9.129920
I0401 02:51:57.006762 140203476997952 submission.py:139] 68) loss = 9.130, grad_norm = 0.315
I0401 02:51:57.442987 140145754715904 logging_writer.py:48] [69] global_step=69, grad_norm=0.307023, loss=9.148300
I0401 02:51:57.446304 140203476997952 submission.py:139] 69) loss = 9.148, grad_norm = 0.307
I0401 02:51:57.887371 140145763108608 logging_writer.py:48] [70] global_step=70, grad_norm=0.307616, loss=9.168536
I0401 02:51:57.891064 140203476997952 submission.py:139] 70) loss = 9.169, grad_norm = 0.308
I0401 02:51:58.349182 140145754715904 logging_writer.py:48] [71] global_step=71, grad_norm=0.293410, loss=9.150911
I0401 02:51:58.352901 140203476997952 submission.py:139] 71) loss = 9.151, grad_norm = 0.293
I0401 02:51:58.788768 140145763108608 logging_writer.py:48] [72] global_step=72, grad_norm=0.280923, loss=9.170367
I0401 02:51:58.792253 140203476997952 submission.py:139] 72) loss = 9.170, grad_norm = 0.281
I0401 02:51:59.224656 140145754715904 logging_writer.py:48] [73] global_step=73, grad_norm=0.282279, loss=9.115935
I0401 02:51:59.227919 140203476997952 submission.py:139] 73) loss = 9.116, grad_norm = 0.282
I0401 02:51:59.664659 140145763108608 logging_writer.py:48] [74] global_step=74, grad_norm=0.278722, loss=9.115485
I0401 02:51:59.668021 140203476997952 submission.py:139] 74) loss = 9.115, grad_norm = 0.279
I0401 02:52:00.097710 140145754715904 logging_writer.py:48] [75] global_step=75, grad_norm=0.277345, loss=9.152892
I0401 02:52:00.100981 140203476997952 submission.py:139] 75) loss = 9.153, grad_norm = 0.277
I0401 02:52:00.533411 140145763108608 logging_writer.py:48] [76] global_step=76, grad_norm=0.261427, loss=9.117624
I0401 02:52:00.536711 140203476997952 submission.py:139] 76) loss = 9.118, grad_norm = 0.261
I0401 02:52:00.971804 140145754715904 logging_writer.py:48] [77] global_step=77, grad_norm=0.250587, loss=9.122994
I0401 02:52:00.975206 140203476997952 submission.py:139] 77) loss = 9.123, grad_norm = 0.251
I0401 02:52:01.405316 140145763108608 logging_writer.py:48] [78] global_step=78, grad_norm=0.255251, loss=9.114462
I0401 02:52:01.408640 140203476997952 submission.py:139] 78) loss = 9.114, grad_norm = 0.255
I0401 02:52:01.839886 140145754715904 logging_writer.py:48] [79] global_step=79, grad_norm=0.244012, loss=9.132374
I0401 02:52:01.843590 140203476997952 submission.py:139] 79) loss = 9.132, grad_norm = 0.244
I0401 02:52:02.274991 140145763108608 logging_writer.py:48] [80] global_step=80, grad_norm=0.231502, loss=9.101427
I0401 02:52:02.278963 140203476997952 submission.py:139] 80) loss = 9.101, grad_norm = 0.232
I0401 02:52:02.717969 140145754715904 logging_writer.py:48] [81] global_step=81, grad_norm=0.236839, loss=9.095364
I0401 02:52:02.721810 140203476997952 submission.py:139] 81) loss = 9.095, grad_norm = 0.237
I0401 02:52:03.156580 140145763108608 logging_writer.py:48] [82] global_step=82, grad_norm=0.227390, loss=9.096732
I0401 02:52:03.160174 140203476997952 submission.py:139] 82) loss = 9.097, grad_norm = 0.227
I0401 02:52:03.594508 140145754715904 logging_writer.py:48] [83] global_step=83, grad_norm=0.215999, loss=9.086234
I0401 02:52:03.597956 140203476997952 submission.py:139] 83) loss = 9.086, grad_norm = 0.216
I0401 02:52:04.032866 140145763108608 logging_writer.py:48] [84] global_step=84, grad_norm=0.217936, loss=9.088368
I0401 02:52:04.036292 140203476997952 submission.py:139] 84) loss = 9.088, grad_norm = 0.218
I0401 02:52:04.466941 140145754715904 logging_writer.py:48] [85] global_step=85, grad_norm=0.215535, loss=9.111123
I0401 02:52:04.470645 140203476997952 submission.py:139] 85) loss = 9.111, grad_norm = 0.216
I0401 02:52:04.904865 140145763108608 logging_writer.py:48] [86] global_step=86, grad_norm=0.213692, loss=9.072455
I0401 02:52:04.908186 140203476997952 submission.py:139] 86) loss = 9.072, grad_norm = 0.214
I0401 02:52:05.341660 140145754715904 logging_writer.py:48] [87] global_step=87, grad_norm=0.216346, loss=9.081520
I0401 02:52:05.345348 140203476997952 submission.py:139] 87) loss = 9.082, grad_norm = 0.216
I0401 02:52:05.780613 140145763108608 logging_writer.py:48] [88] global_step=88, grad_norm=0.204191, loss=9.053718
I0401 02:52:05.784271 140203476997952 submission.py:139] 88) loss = 9.054, grad_norm = 0.204
I0401 02:52:06.219708 140145754715904 logging_writer.py:48] [89] global_step=89, grad_norm=0.202449, loss=9.063746
I0401 02:52:06.223129 140203476997952 submission.py:139] 89) loss = 9.064, grad_norm = 0.202
I0401 02:52:06.654916 140145763108608 logging_writer.py:48] [90] global_step=90, grad_norm=0.199268, loss=9.098840
I0401 02:52:06.658223 140203476997952 submission.py:139] 90) loss = 9.099, grad_norm = 0.199
I0401 02:52:07.090582 140145754715904 logging_writer.py:48] [91] global_step=91, grad_norm=0.195118, loss=9.069890
I0401 02:52:07.094044 140203476997952 submission.py:139] 91) loss = 9.070, grad_norm = 0.195
I0401 02:52:07.528429 140145763108608 logging_writer.py:48] [92] global_step=92, grad_norm=0.190041, loss=9.064719
I0401 02:52:07.531849 140203476997952 submission.py:139] 92) loss = 9.065, grad_norm = 0.190
I0401 02:52:07.966068 140145754715904 logging_writer.py:48] [93] global_step=93, grad_norm=0.194983, loss=9.053438
I0401 02:52:07.969533 140203476997952 submission.py:139] 93) loss = 9.053, grad_norm = 0.195
I0401 02:52:08.404036 140145763108608 logging_writer.py:48] [94] global_step=94, grad_norm=0.182384, loss=9.088846
I0401 02:52:08.407711 140203476997952 submission.py:139] 94) loss = 9.089, grad_norm = 0.182
I0401 02:52:08.841317 140145754715904 logging_writer.py:48] [95] global_step=95, grad_norm=0.187286, loss=9.054450
I0401 02:52:08.844745 140203476997952 submission.py:139] 95) loss = 9.054, grad_norm = 0.187
I0401 02:52:09.283660 140145763108608 logging_writer.py:48] [96] global_step=96, grad_norm=0.184282, loss=9.058951
I0401 02:52:09.287086 140203476997952 submission.py:139] 96) loss = 9.059, grad_norm = 0.184
I0401 02:52:09.724033 140145754715904 logging_writer.py:48] [97] global_step=97, grad_norm=0.179206, loss=9.049629
I0401 02:52:09.727407 140203476997952 submission.py:139] 97) loss = 9.050, grad_norm = 0.179
I0401 02:52:10.160990 140145763108608 logging_writer.py:48] [98] global_step=98, grad_norm=0.172485, loss=9.058988
I0401 02:52:10.164765 140203476997952 submission.py:139] 98) loss = 9.059, grad_norm = 0.172
I0401 02:52:10.598822 140145754715904 logging_writer.py:48] [99] global_step=99, grad_norm=0.175454, loss=9.062613
I0401 02:52:10.602325 140203476997952 submission.py:139] 99) loss = 9.063, grad_norm = 0.175
I0401 02:52:11.035589 140145763108608 logging_writer.py:48] [100] global_step=100, grad_norm=0.170977, loss=9.040778
I0401 02:52:11.039382 140203476997952 submission.py:139] 100) loss = 9.041, grad_norm = 0.171
I0401 02:55:01.606229 140145754715904 logging_writer.py:48] [500] global_step=500, grad_norm=0.402142, loss=8.445865
I0401 02:55:01.610015 140203476997952 submission.py:139] 500) loss = 8.446, grad_norm = 0.402
I0401 02:58:35.129733 140145763108608 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.565243, loss=7.855175
I0401 02:58:35.133438 140203476997952 submission.py:139] 1000) loss = 7.855, grad_norm = 0.565
I0401 03:02:08.835552 140145754715904 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.602768, loss=7.441674
I0401 03:02:08.839735 140203476997952 submission.py:139] 1500) loss = 7.442, grad_norm = 0.603
I0401 03:05:27.669679 140203476997952 submission_runner.py:371] Before eval at step 1966: RAM USED (GB) 20.902375424
I0401 03:05:27.669890 140203476997952 spec.py:298] Evaluating on the training split.
I0401 03:05:31.583901 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 03:10:02.186533 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 03:10:05.946815 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 03:14:31.536192 140203476997952 spec.py:326] Evaluating on the test split.
I0401 03:14:35.359428 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 03:19:06.988478 140203476997952 submission_runner.py:380] Time since start: 1676.14s, 	Step: 1966, 	{'train/accuracy': 0.2827604327313022, 'train/loss': 5.882470412511281, 'train/bleu': 5.415484075466146, 'validation/accuracy': 0.26080271788322523, 'validation/loss': 6.159445868619112, 'validation/bleu': 2.7741480730873445, 'validation/num_examples': 3000, 'test/accuracy': 0.24075300679797804, 'test/loss': 6.453020597292429, 'test/bleu': 1.9560797916266284, 'test/num_examples': 3003}
I0401 03:19:06.988880 140203476997952 submission_runner.py:390] After eval at step 1966: RAM USED (GB) 21.137076224
I0401 03:19:06.997069 140145763108608 logging_writer.py:48] [1966] global_step=1966, preemption_count=0, score=838.204733, test/accuracy=0.240753, test/bleu=1.956080, test/loss=6.453021, test/num_examples=3003, total_duration=1676.136429, train/accuracy=0.282760, train/bleu=5.415484, train/loss=5.882470, validation/accuracy=0.260803, validation/bleu=2.774148, validation/loss=6.159446, validation/num_examples=3000
I0401 03:19:08.496053 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_1966.
I0401 03:19:08.496676 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 1966: RAM USED (GB) 21.136838656
I0401 03:19:23.384501 140145754715904 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.840681, loss=7.028693
I0401 03:19:23.388312 140203476997952 submission.py:139] 2000) loss = 7.029, grad_norm = 0.841
I0401 03:22:56.606704 140145763108608 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.954180, loss=6.781340
I0401 03:22:56.610674 140203476997952 submission.py:139] 2500) loss = 6.781, grad_norm = 0.954
I0401 03:26:30.144860 140145754715904 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.702702, loss=6.540178
I0401 03:26:30.147787 140203476997952 submission.py:139] 3000) loss = 6.540, grad_norm = 0.703
I0401 03:30:03.912087 140145763108608 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.696659, loss=6.114793
I0401 03:30:03.915437 140203476997952 submission.py:139] 3500) loss = 6.115, grad_norm = 0.697
I0401 03:33:08.529108 140203476997952 submission_runner.py:371] Before eval at step 3933: RAM USED (GB) 21.189963776
I0401 03:33:08.529328 140203476997952 spec.py:298] Evaluating on the training split.
I0401 03:33:12.457383 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 03:36:51.826441 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 03:36:55.587136 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 03:40:00.721948 140203476997952 spec.py:326] Evaluating on the test split.
I0401 03:40:04.565720 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 03:43:34.188160 140203476997952 submission_runner.py:380] Time since start: 3337.00s, 	Step: 3933, 	{'train/accuracy': 0.41252179498944663, 'train/loss': 4.333516392126273, 'train/bleu': 13.10568504661621, 'validation/accuracy': 0.39561815724541544, 'validation/loss': 4.484816524283642, 'validation/bleu': 9.120581132769281, 'validation/num_examples': 3000, 'test/accuracy': 0.38137237813026553, 'test/loss': 4.691729126721283, 'test/bleu': 7.36322188158621, 'test/num_examples': 3003}
I0401 03:43:34.188678 140203476997952 submission_runner.py:390] After eval at step 3933: RAM USED (GB) 21.32973568
I0401 03:43:34.198084 140145754715904 logging_writer.py:48] [3933] global_step=3933, preemption_count=0, score=1671.997239, test/accuracy=0.381372, test/bleu=7.363222, test/loss=4.691729, test/num_examples=3003, total_duration=3336.995519, train/accuracy=0.412522, train/bleu=13.105685, train/loss=4.333516, validation/accuracy=0.395618, validation/bleu=9.120581, validation/loss=4.484817, validation/num_examples=3000
I0401 03:43:35.698600 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_3933.
I0401 03:43:35.699322 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 3933: RAM USED (GB) 21.32895744
I0401 03:44:04.693763 140145763108608 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.642804, loss=5.954766
I0401 03:44:04.697442 140203476997952 submission.py:139] 4000) loss = 5.955, grad_norm = 0.643
I0401 03:47:38.122071 140145754715904 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.634332, loss=5.564224
I0401 03:47:38.126192 140203476997952 submission.py:139] 4500) loss = 5.564, grad_norm = 0.634
I0401 03:51:11.612189 140145763108608 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.549100, loss=5.458855
I0401 03:51:11.615553 140203476997952 submission.py:139] 5000) loss = 5.459, grad_norm = 0.549
I0401 03:54:45.374971 140145754715904 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.590711, loss=5.441828
I0401 03:54:45.379212 140203476997952 submission.py:139] 5500) loss = 5.442, grad_norm = 0.591
I0401 03:57:35.832426 140203476997952 submission_runner.py:371] Before eval at step 5900: RAM USED (GB) 21.361922048
I0401 03:57:35.832639 140203476997952 spec.py:298] Evaluating on the training split.
I0401 03:57:39.742537 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:00:28.561161 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 04:00:32.316394 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:02:52.182209 140203476997952 spec.py:326] Evaluating on the test split.
I0401 04:02:56.001888 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:05:14.269301 140203476997952 submission_runner.py:380] Time since start: 4804.30s, 	Step: 5900, 	{'train/accuracy': 0.5024791874219886, 'train/loss': 3.5003245702932655, 'train/bleu': 21.391797981133998, 'validation/accuracy': 0.5018660648969014, 'validation/loss': 3.4764335687096253, 'validation/bleu': 17.530896061995033, 'validation/num_examples': 3000, 'test/accuracy': 0.49424205450002906, 'test/loss': 3.5876350880251002, 'test/bleu': 15.493059362032394, 'test/num_examples': 3003}
I0401 04:05:14.269727 140203476997952 submission_runner.py:390] After eval at step 5900: RAM USED (GB) 21.451923456
I0401 04:05:14.279426 140145763108608 logging_writer.py:48] [5900] global_step=5900, preemption_count=0, score=2506.029031, test/accuracy=0.494242, test/bleu=15.493059, test/loss=3.587635, test/num_examples=3003, total_duration=4804.300891, train/accuracy=0.502479, train/bleu=21.391798, train/loss=3.500325, validation/accuracy=0.501866, validation/bleu=17.530896, validation/loss=3.476434, validation/num_examples=3000
I0401 04:05:15.756735 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_5900.
I0401 04:05:15.757330 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 5900: RAM USED (GB) 21.451239424
I0401 04:05:58.872437 140145754715904 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.432057, loss=5.221160
I0401 04:05:58.875564 140203476997952 submission.py:139] 6000) loss = 5.221, grad_norm = 0.432
I0401 04:09:32.321548 140145763108608 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.488302, loss=5.134004
I0401 04:09:32.324995 140203476997952 submission.py:139] 6500) loss = 5.134, grad_norm = 0.488
I0401 04:13:05.910916 140145754715904 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.420771, loss=5.094390
I0401 04:13:05.914155 140203476997952 submission.py:139] 7000) loss = 5.094, grad_norm = 0.421
I0401 04:16:39.276937 140145763108608 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.406957, loss=4.879609
I0401 04:16:39.280601 140203476997952 submission.py:139] 7500) loss = 4.880, grad_norm = 0.407
I0401 04:19:16.094401 140203476997952 submission_runner.py:371] Before eval at step 7868: RAM USED (GB) 21.785051136
I0401 04:19:16.094599 140203476997952 spec.py:298] Evaluating on the training split.
I0401 04:19:20.000818 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:21:58.215630 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 04:22:01.974917 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:24:21.892495 140203476997952 spec.py:326] Evaluating on the test split.
I0401 04:24:25.730644 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:26:39.570999 140203476997952 submission_runner.py:380] Time since start: 6104.56s, 	Step: 7868, 	{'train/accuracy': 0.5494227622741679, 'train/loss': 3.038962116271353, 'train/bleu': 24.40707348950827, 'validation/accuracy': 0.5474947613792761, 'validation/loss': 3.0058812429480106, 'validation/bleu': 20.57233585426239, 'validation/num_examples': 3000, 'test/accuracy': 0.5481378188367905, 'test/loss': 3.0495231973737726, 'test/bleu': 19.23186556160642, 'test/num_examples': 3003}
I0401 04:26:39.571393 140203476997952 submission_runner.py:390] After eval at step 7868: RAM USED (GB) 21.859827712
I0401 04:26:39.579581 140145754715904 logging_writer.py:48] [7868] global_step=7868, preemption_count=0, score=3339.962649, test/accuracy=0.548138, test/bleu=19.231866, test/loss=3.049523, test/num_examples=3003, total_duration=6104.564088, train/accuracy=0.549423, train/bleu=24.407073, train/loss=3.038962, validation/accuracy=0.547495, validation/bleu=20.572336, validation/loss=3.005881, validation/num_examples=3000
I0401 04:26:41.081110 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_7868.
I0401 04:26:41.081699 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 7868: RAM USED (GB) 21.859033088
I0401 04:27:37.729187 140145763108608 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.396169, loss=4.984839
I0401 04:27:37.732479 140203476997952 submission.py:139] 8000) loss = 4.985, grad_norm = 0.396
I0401 04:31:11.259306 140145754715904 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.378624, loss=4.868661
I0401 04:31:11.262396 140203476997952 submission.py:139] 8500) loss = 4.869, grad_norm = 0.379
I0401 04:34:44.695349 140145763108608 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.374443, loss=4.861580
I0401 04:34:44.698931 140203476997952 submission.py:139] 9000) loss = 4.862, grad_norm = 0.374
I0401 04:38:18.173398 140145754715904 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.362828, loss=4.740111
I0401 04:38:18.177158 140203476997952 submission.py:139] 9500) loss = 4.740, grad_norm = 0.363
I0401 04:40:41.254096 140203476997952 submission_runner.py:371] Before eval at step 9836: RAM USED (GB) 22.03793408
I0401 04:40:41.254303 140203476997952 spec.py:298] Evaluating on the training split.
I0401 04:40:45.165807 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:43:01.703021 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 04:43:05.454019 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:45:14.947504 140203476997952 spec.py:326] Evaluating on the test split.
I0401 04:45:18.791689 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:47:23.119867 140203476997952 submission_runner.py:380] Time since start: 7389.72s, 	Step: 9836, 	{'train/accuracy': 0.5744121127181243, 'train/loss': 2.780969087812611, 'train/bleu': 26.431463993296006, 'validation/accuracy': 0.5756035263046956, 'validation/loss': 2.738139096229433, 'validation/bleu': 22.392573813501222, 'validation/num_examples': 3000, 'test/accuracy': 0.5748881529254547, 'test/loss': 2.765191774156063, 'test/bleu': 20.899570782095342, 'test/num_examples': 3003}
I0401 04:47:23.120254 140203476997952 submission_runner.py:390] After eval at step 9836: RAM USED (GB) 22.063755264
I0401 04:47:23.128301 140145763108608 logging_writer.py:48] [9836] global_step=9836, preemption_count=0, score=4173.832595, test/accuracy=0.574888, test/bleu=20.899571, test/loss=2.765192, test/num_examples=3003, total_duration=7389.721778, train/accuracy=0.574412, train/bleu=26.431464, train/loss=2.780969, validation/accuracy=0.575604, validation/bleu=22.392574, validation/loss=2.738139, validation/num_examples=3000
I0401 04:47:24.596364 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_9836.
I0401 04:47:24.596933 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 9836: RAM USED (GB) 22.062718976
I0401 04:48:34.570411 140203476997952 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 22.06181376
I0401 04:48:34.570602 140203476997952 spec.py:298] Evaluating on the training split.
I0401 04:48:38.482681 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:51:03.032467 140203476997952 spec.py:310] Evaluating on the validation split.
I0401 04:51:06.789751 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:53:23.245487 140203476997952 spec.py:326] Evaluating on the test split.
I0401 04:53:27.054276 140203476997952 workload.py:130] Translating evaluation dataset.
I0401 04:55:35.926163 140203476997952 submission_runner.py:380] Time since start: 7863.04s, 	Step: 10000, 	{'train/accuracy': 0.5738644192348321, 'train/loss': 2.7962239284320027, 'train/bleu': 26.446417400506295, 'validation/accuracy': 0.5767070464098399, 'validation/loss': 2.730467732886139, 'validation/bleu': 22.73505299801229, 'validation/num_examples': 3000, 'test/accuracy': 0.5789320783220033, 'test/loss': 2.7491228356864794, 'test/bleu': 21.434485904330632, 'test/num_examples': 3003}
I0401 04:55:35.926545 140203476997952 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 22.099464192
I0401 04:55:35.934755 140145754715904 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4243.324926, test/accuracy=0.578932, test/bleu=21.434486, test/loss=2.749123, test/num_examples=3003, total_duration=7863.037631, train/accuracy=0.573864, train/bleu=26.446417, train/loss=2.796224, validation/accuracy=0.576707, validation/bleu=22.735053, validation/loss=2.730468, validation/num_examples=3000
I0401 04:55:37.443454 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_10000.
I0401 04:55:37.444127 140203476997952 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.098911232
I0401 04:55:37.451841 140145763108608 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4243.324926
I0401 04:55:40.286140 140203476997952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/wmt_pytorch/trial_1/checkpoint_10000.
I0401 04:55:40.308888 140203476997952 submission_runner.py:543] Tuning trial 1/1
I0401 04:55:40.309116 140203476997952 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 04:55:40.309944 140203476997952 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000673539048140918, 'train/loss': 10.97117623891229, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 10.961435692055895, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 10.967822903956773, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.6848316192626953, 'total_duration': 3.6868226528167725, 'global_step': 1, 'preemption_count': 0}), (1966, {'train/accuracy': 0.2827604327313022, 'train/loss': 5.882470412511281, 'train/bleu': 5.415484075466146, 'validation/accuracy': 0.26080271788322523, 'validation/loss': 6.159445868619112, 'validation/bleu': 2.7741480730873445, 'validation/num_examples': 3000, 'test/accuracy': 0.24075300679797804, 'test/loss': 6.453020597292429, 'test/bleu': 1.9560797916266284, 'test/num_examples': 3003, 'score': 838.2047326564789, 'total_duration': 1676.1364290714264, 'global_step': 1966, 'preemption_count': 0}), (3933, {'train/accuracy': 0.41252179498944663, 'train/loss': 4.333516392126273, 'train/bleu': 13.10568504661621, 'validation/accuracy': 0.39561815724541544, 'validation/loss': 4.484816524283642, 'validation/bleu': 9.120581132769281, 'validation/num_examples': 3000, 'test/accuracy': 0.38137237813026553, 'test/loss': 4.691729126721283, 'test/bleu': 7.36322188158621, 'test/num_examples': 3003, 'score': 1671.997239112854, 'total_duration': 3336.995519399643, 'global_step': 3933, 'preemption_count': 0}), (5900, {'train/accuracy': 0.5024791874219886, 'train/loss': 3.5003245702932655, 'train/bleu': 21.391797981133998, 'validation/accuracy': 0.5018660648969014, 'validation/loss': 3.4764335687096253, 'validation/bleu': 17.530896061995033, 'validation/num_examples': 3000, 'test/accuracy': 0.49424205450002906, 'test/loss': 3.5876350880251002, 'test/bleu': 15.493059362032394, 'test/num_examples': 3003, 'score': 2506.0290307998657, 'total_duration': 4804.300890684128, 'global_step': 5900, 'preemption_count': 0}), (7868, {'train/accuracy': 0.5494227622741679, 'train/loss': 3.038962116271353, 'train/bleu': 24.40707348950827, 'validation/accuracy': 0.5474947613792761, 'validation/loss': 3.0058812429480106, 'validation/bleu': 20.57233585426239, 'validation/num_examples': 3000, 'test/accuracy': 0.5481378188367905, 'test/loss': 3.0495231973737726, 'test/bleu': 19.23186556160642, 'test/num_examples': 3003, 'score': 3339.9626491069794, 'total_duration': 6104.564087629318, 'global_step': 7868, 'preemption_count': 0}), (9836, {'train/accuracy': 0.5744121127181243, 'train/loss': 2.780969087812611, 'train/bleu': 26.431463993296006, 'validation/accuracy': 0.5756035263046956, 'validation/loss': 2.738139096229433, 'validation/bleu': 22.392573813501222, 'validation/num_examples': 3000, 'test/accuracy': 0.5748881529254547, 'test/loss': 2.765191774156063, 'test/bleu': 20.899570782095342, 'test/num_examples': 3003, 'score': 4173.832594633102, 'total_duration': 7389.721778154373, 'global_step': 9836, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5738644192348321, 'train/loss': 2.7962239284320027, 'train/bleu': 26.446417400506295, 'validation/accuracy': 0.5767070464098399, 'validation/loss': 2.730467732886139, 'validation/bleu': 22.73505299801229, 'validation/num_examples': 3000, 'test/accuracy': 0.5789320783220033, 'test/loss': 2.7491228356864794, 'test/bleu': 21.434485904330632, 'test/num_examples': 3003, 'score': 4243.324925661087, 'total_duration': 7863.0376307964325, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 04:55:40.310042 140203476997952 submission_runner.py:546] Timing: 4243.324925661087
I0401 04:55:40.310085 140203476997952 submission_runner.py:547] ====================
I0401 04:55:40.310155 140203476997952 submission_runner.py:606] Final wmt score: 4243.324925661087
