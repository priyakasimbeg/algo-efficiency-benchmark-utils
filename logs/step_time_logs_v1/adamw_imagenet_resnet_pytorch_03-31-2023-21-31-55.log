WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0331 21:32:18.085610 140292557645632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0331 21:32:18.085731 140493713823552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0331 21:32:18.085866 140203790776128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0331 21:32:18.086343 140409488176960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0331 21:32:18.086585 139691425253184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0331 21:32:18.087315 140560328779584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0331 21:32:19.066356 140049953105728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0331 21:32:19.070906 140688727717696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0331 21:32:19.071223 140688727717696 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.077090 140049953105728 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.079539 140292557645632 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.080035 140493713823552 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.080093 140560328779584 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.080108 140203790776128 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.080276 140409488176960 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:32:19.080286 139691425253184 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
W0331 21:32:21.153901 140409488176960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:32:21.154634 140560328779584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:32:21.155100 139691425253184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:32:21.155109 140203790776128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:32:21.155431 140493713823552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0331 21:32:21.156176 140688727717696 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/imagenet_resnet_pytorch.
W0331 21:32:21.183482 140049953105728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:32:21.183564 140292557645632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:32:21.189266 140688727717696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0331 21:32:21.192519 140688727717696 submission_runner.py:504] Using RNG seed 4069122978
I0331 21:32:21.193564 140688727717696 submission_runner.py:513] --- Tuning run 1/1 ---
I0331 21:32:21.193679 140688727717696 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1.
I0331 21:32:21.193860 140688727717696 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0331 21:32:21.194774 140688727717696 submission_runner.py:230] Starting train once: RAM USED (GB) 6.40110592
I0331 21:32:21.194862 140688727717696 submission_runner.py:231] Initializing dataset.
I0331 21:32:25.489268 140688727717696 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 8.508022784
I0331 21:32:25.489441 140688727717696 submission_runner.py:240] Initializing model.
I0331 21:32:29.802189 140688727717696 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.2346752
I0331 21:32:29.802373 140688727717696 submission_runner.py:252] Initializing optimizer.
I0331 21:32:29.803449 140688727717696 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.2346752
I0331 21:32:29.803548 140688727717696 submission_runner.py:261] Initializing metrics bundle.
I0331 21:32:29.803602 140688727717696 submission_runner.py:275] Initializing checkpoint and logger.
I0331 21:32:30.235217 140688727717696 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0331 21:32:30.236092 140688727717696 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0331 21:32:30.286711 140688727717696 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 18.282672128
I0331 21:32:30.287679 140688727717696 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.282672128
I0331 21:32:30.287791 140688727717696 submission_runner.py:312] Starting training loop.
I0331 21:32:32.403190 140688727717696 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 23.395659776
I0331 21:32:38.104413 140661403670272 logging_writer.py:48] [0] global_step=0, grad_norm=0.597974, loss=6.934440
I0331 21:32:38.126137 140688727717696 submission.py:119] 0) loss = 6.934, grad_norm = 0.598
I0331 21:32:38.126688 140688727717696 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 32.3991552
I0331 21:32:38.127312 140688727717696 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 32.3991552
I0331 21:32:38.127463 140688727717696 spec.py:298] Evaluating on the training split.
I0331 21:33:26.409727 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 21:34:10.394944 140688727717696 spec.py:326] Evaluating on the test split.
I0331 21:34:10.409800 140688727717696 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 21:34:10.415360 140688727717696 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0331 21:34:10.489987 140688727717696 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 21:34:23.157908 140688727717696 submission_runner.py:380] Time since start: 7.84s, 	Step: 1, 	{'train/accuracy': 0.0008569834183673469, 'train/loss': 6.923496791294643, 'validation/accuracy': 0.00108, 'validation/loss': 6.923540625, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92712265625, 'test/num_examples': 10000}
I0331 21:34:23.158268 140688727717696 submission_runner.py:390] After eval at step 1: RAM USED (GB) 92.013170688
I0331 21:34:23.173492 140636195899136 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=7.838140, test/accuracy=0.000600, test/loss=6.927123, test/num_examples=10000, total_duration=7.839957, train/accuracy=0.000857, train/loss=6.923497, validation/accuracy=0.001080, validation/loss=6.923541, validation/num_examples=50000
I0331 21:34:23.608305 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0331 21:34:23.608977 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 92.013776896
I0331 21:34:23.615049 140688727717696 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 92.01401856
I0331 21:34:23.621010 140688727717696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.621939 139691425253184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.621921 140049953105728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.621945 140493713823552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.621948 140409488176960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.621958 140560328779584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.621983 140292557645632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:23.622102 140203790776128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:34:24.021103 140636187506432 logging_writer.py:48] [1] global_step=1, grad_norm=0.620955, loss=6.915338
I0331 21:34:24.024422 140688727717696 submission.py:119] 1) loss = 6.915, grad_norm = 0.621
I0331 21:34:24.024966 140688727717696 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 92.03218432
I0331 21:34:24.410594 140636195899136 logging_writer.py:48] [2] global_step=2, grad_norm=0.618715, loss=6.931440
I0331 21:34:24.415758 140688727717696 submission.py:119] 2) loss = 6.931, grad_norm = 0.619
I0331 21:34:24.798764 140636187506432 logging_writer.py:48] [3] global_step=3, grad_norm=0.605266, loss=6.913083
I0331 21:34:24.802268 140688727717696 submission.py:119] 3) loss = 6.913, grad_norm = 0.605
I0331 21:34:25.184207 140636195899136 logging_writer.py:48] [4] global_step=4, grad_norm=0.615581, loss=6.926934
I0331 21:34:25.188149 140688727717696 submission.py:119] 4) loss = 6.927, grad_norm = 0.616
I0331 21:34:25.568699 140636187506432 logging_writer.py:48] [5] global_step=5, grad_norm=0.612591, loss=6.925528
I0331 21:34:25.574048 140688727717696 submission.py:119] 5) loss = 6.926, grad_norm = 0.613
I0331 21:34:25.955276 140636195899136 logging_writer.py:48] [6] global_step=6, grad_norm=0.617870, loss=6.916950
I0331 21:34:25.959133 140688727717696 submission.py:119] 6) loss = 6.917, grad_norm = 0.618
I0331 21:34:26.337785 140636187506432 logging_writer.py:48] [7] global_step=7, grad_norm=0.620297, loss=6.918986
I0331 21:34:26.341384 140688727717696 submission.py:119] 7) loss = 6.919, grad_norm = 0.620
I0331 21:34:26.723792 140636195899136 logging_writer.py:48] [8] global_step=8, grad_norm=0.626623, loss=6.928746
I0331 21:34:26.727382 140688727717696 submission.py:119] 8) loss = 6.929, grad_norm = 0.627
I0331 21:34:27.108641 140636187506432 logging_writer.py:48] [9] global_step=9, grad_norm=0.630378, loss=6.917725
I0331 21:34:27.112150 140688727717696 submission.py:119] 9) loss = 6.918, grad_norm = 0.630
I0331 21:34:27.493242 140636195899136 logging_writer.py:48] [10] global_step=10, grad_norm=0.620577, loss=6.931102
I0331 21:34:27.497149 140688727717696 submission.py:119] 10) loss = 6.931, grad_norm = 0.621
I0331 21:34:27.875867 140636187506432 logging_writer.py:48] [11] global_step=11, grad_norm=0.616775, loss=6.941224
I0331 21:34:27.879334 140688727717696 submission.py:119] 11) loss = 6.941, grad_norm = 0.617
I0331 21:34:28.260321 140636195899136 logging_writer.py:48] [12] global_step=12, grad_norm=0.602302, loss=6.921649
I0331 21:34:28.263909 140688727717696 submission.py:119] 12) loss = 6.922, grad_norm = 0.602
I0331 21:34:28.643004 140636187506432 logging_writer.py:48] [13] global_step=13, grad_norm=0.613541, loss=6.916091
I0331 21:34:28.646696 140688727717696 submission.py:119] 13) loss = 6.916, grad_norm = 0.614
I0331 21:34:29.027476 140636195899136 logging_writer.py:48] [14] global_step=14, grad_norm=0.627360, loss=6.922292
I0331 21:34:29.031276 140688727717696 submission.py:119] 14) loss = 6.922, grad_norm = 0.627
I0331 21:34:29.413200 140636187506432 logging_writer.py:48] [15] global_step=15, grad_norm=0.596751, loss=6.918434
I0331 21:34:29.416793 140688727717696 submission.py:119] 15) loss = 6.918, grad_norm = 0.597
I0331 21:34:29.802737 140636195899136 logging_writer.py:48] [16] global_step=16, grad_norm=0.596921, loss=6.930024
I0331 21:34:29.806364 140688727717696 submission.py:119] 16) loss = 6.930, grad_norm = 0.597
I0331 21:34:30.188677 140636187506432 logging_writer.py:48] [17] global_step=17, grad_norm=0.615559, loss=6.926024
I0331 21:34:30.192148 140688727717696 submission.py:119] 17) loss = 6.926, grad_norm = 0.616
I0331 21:34:30.573836 140636195899136 logging_writer.py:48] [18] global_step=18, grad_norm=0.619384, loss=6.922707
I0331 21:34:30.577550 140688727717696 submission.py:119] 18) loss = 6.923, grad_norm = 0.619
I0331 21:34:30.959676 140636187506432 logging_writer.py:48] [19] global_step=19, grad_norm=0.610276, loss=6.933897
I0331 21:34:30.963224 140688727717696 submission.py:119] 19) loss = 6.934, grad_norm = 0.610
I0331 21:34:31.346043 140636195899136 logging_writer.py:48] [20] global_step=20, grad_norm=0.613190, loss=6.913581
I0331 21:34:31.349946 140688727717696 submission.py:119] 20) loss = 6.914, grad_norm = 0.613
I0331 21:34:31.732328 140636187506432 logging_writer.py:48] [21] global_step=21, grad_norm=0.605249, loss=6.913249
I0331 21:34:31.736163 140688727717696 submission.py:119] 21) loss = 6.913, grad_norm = 0.605
I0331 21:34:32.165015 140636195899136 logging_writer.py:48] [22] global_step=22, grad_norm=0.610432, loss=6.927108
I0331 21:34:32.168733 140688727717696 submission.py:119] 22) loss = 6.927, grad_norm = 0.610
I0331 21:34:32.549740 140636187506432 logging_writer.py:48] [23] global_step=23, grad_norm=0.606803, loss=6.919476
I0331 21:34:32.553215 140688727717696 submission.py:119] 23) loss = 6.919, grad_norm = 0.607
I0331 21:34:32.939041 140636195899136 logging_writer.py:48] [24] global_step=24, grad_norm=0.612449, loss=6.915875
I0331 21:34:32.942896 140688727717696 submission.py:119] 24) loss = 6.916, grad_norm = 0.612
I0331 21:34:33.323067 140636187506432 logging_writer.py:48] [25] global_step=25, grad_norm=0.610468, loss=6.927271
I0331 21:34:33.327331 140688727717696 submission.py:119] 25) loss = 6.927, grad_norm = 0.610
I0331 21:34:33.708725 140636195899136 logging_writer.py:48] [26] global_step=26, grad_norm=0.621494, loss=6.932849
I0331 21:34:33.712279 140688727717696 submission.py:119] 26) loss = 6.933, grad_norm = 0.621
I0331 21:34:34.092911 140636187506432 logging_writer.py:48] [27] global_step=27, grad_norm=0.600188, loss=6.925261
I0331 21:34:34.096296 140688727717696 submission.py:119] 27) loss = 6.925, grad_norm = 0.600
I0331 21:34:34.475699 140636195899136 logging_writer.py:48] [28] global_step=28, grad_norm=0.619023, loss=6.916557
I0331 21:34:34.479060 140688727717696 submission.py:119] 28) loss = 6.917, grad_norm = 0.619
I0331 21:34:34.865367 140636187506432 logging_writer.py:48] [29] global_step=29, grad_norm=0.616097, loss=6.931967
I0331 21:34:34.869305 140688727717696 submission.py:119] 29) loss = 6.932, grad_norm = 0.616
I0331 21:34:35.254885 140636195899136 logging_writer.py:48] [30] global_step=30, grad_norm=0.619754, loss=6.927563
I0331 21:34:35.258495 140688727717696 submission.py:119] 30) loss = 6.928, grad_norm = 0.620
I0331 21:34:35.638294 140636187506432 logging_writer.py:48] [31] global_step=31, grad_norm=0.597474, loss=6.920815
I0331 21:34:35.642410 140688727717696 submission.py:119] 31) loss = 6.921, grad_norm = 0.597
I0331 21:34:36.024347 140636195899136 logging_writer.py:48] [32] global_step=32, grad_norm=0.592506, loss=6.926497
I0331 21:34:36.028145 140688727717696 submission.py:119] 32) loss = 6.926, grad_norm = 0.593
I0331 21:34:36.412479 140636187506432 logging_writer.py:48] [33] global_step=33, grad_norm=0.624667, loss=6.914525
I0331 21:34:36.417667 140688727717696 submission.py:119] 33) loss = 6.915, grad_norm = 0.625
I0331 21:34:36.803341 140636195899136 logging_writer.py:48] [34] global_step=34, grad_norm=0.602144, loss=6.916985
I0331 21:34:36.807255 140688727717696 submission.py:119] 34) loss = 6.917, grad_norm = 0.602
I0331 21:34:37.190432 140636187506432 logging_writer.py:48] [35] global_step=35, grad_norm=0.601643, loss=6.926503
I0331 21:34:37.194485 140688727717696 submission.py:119] 35) loss = 6.927, grad_norm = 0.602
I0331 21:34:37.573934 140636195899136 logging_writer.py:48] [36] global_step=36, grad_norm=0.621542, loss=6.918046
I0331 21:34:37.577829 140688727717696 submission.py:119] 36) loss = 6.918, grad_norm = 0.622
I0331 21:34:37.967916 140636187506432 logging_writer.py:48] [37] global_step=37, grad_norm=0.608528, loss=6.920946
I0331 21:34:37.972222 140688727717696 submission.py:119] 37) loss = 6.921, grad_norm = 0.609
I0331 21:34:38.365494 140636195899136 logging_writer.py:48] [38] global_step=38, grad_norm=0.601511, loss=6.928948
I0331 21:34:38.369224 140688727717696 submission.py:119] 38) loss = 6.929, grad_norm = 0.602
I0331 21:34:38.785335 140636187506432 logging_writer.py:48] [39] global_step=39, grad_norm=0.605512, loss=6.916520
I0331 21:34:38.789401 140688727717696 submission.py:119] 39) loss = 6.917, grad_norm = 0.606
I0331 21:34:39.173774 140636195899136 logging_writer.py:48] [40] global_step=40, grad_norm=0.616210, loss=6.916383
I0331 21:34:39.177248 140688727717696 submission.py:119] 40) loss = 6.916, grad_norm = 0.616
I0331 21:34:39.558222 140636187506432 logging_writer.py:48] [41] global_step=41, grad_norm=0.593976, loss=6.917032
I0331 21:34:39.561633 140688727717696 submission.py:119] 41) loss = 6.917, grad_norm = 0.594
I0331 21:34:39.942983 140636195899136 logging_writer.py:48] [42] global_step=42, grad_norm=0.605141, loss=6.923671
I0331 21:34:39.947367 140688727717696 submission.py:119] 42) loss = 6.924, grad_norm = 0.605
I0331 21:34:40.331395 140636187506432 logging_writer.py:48] [43] global_step=43, grad_norm=0.622042, loss=6.929555
I0331 21:34:40.335225 140688727717696 submission.py:119] 43) loss = 6.930, grad_norm = 0.622
I0331 21:34:40.727271 140636195899136 logging_writer.py:48] [44] global_step=44, grad_norm=0.608542, loss=6.916447
I0331 21:34:40.731483 140688727717696 submission.py:119] 44) loss = 6.916, grad_norm = 0.609
I0331 21:34:41.119991 140636187506432 logging_writer.py:48] [45] global_step=45, grad_norm=0.599009, loss=6.917641
I0331 21:34:41.123580 140688727717696 submission.py:119] 45) loss = 6.918, grad_norm = 0.599
I0331 21:34:41.504927 140636195899136 logging_writer.py:48] [46] global_step=46, grad_norm=0.623450, loss=6.915563
I0331 21:34:41.508604 140688727717696 submission.py:119] 46) loss = 6.916, grad_norm = 0.623
I0331 21:34:41.892647 140636187506432 logging_writer.py:48] [47] global_step=47, grad_norm=0.613289, loss=6.917812
I0331 21:34:41.896229 140688727717696 submission.py:119] 47) loss = 6.918, grad_norm = 0.613
I0331 21:34:42.280978 140636195899136 logging_writer.py:48] [48] global_step=48, grad_norm=0.599891, loss=6.915602
I0331 21:34:42.284821 140688727717696 submission.py:119] 48) loss = 6.916, grad_norm = 0.600
I0331 21:34:42.670925 140636187506432 logging_writer.py:48] [49] global_step=49, grad_norm=0.614827, loss=6.910829
I0331 21:34:42.674390 140688727717696 submission.py:119] 49) loss = 6.911, grad_norm = 0.615
I0331 21:34:43.063390 140636195899136 logging_writer.py:48] [50] global_step=50, grad_norm=0.606492, loss=6.917328
I0331 21:34:43.067193 140688727717696 submission.py:119] 50) loss = 6.917, grad_norm = 0.606
I0331 21:34:43.456907 140636187506432 logging_writer.py:48] [51] global_step=51, grad_norm=0.602908, loss=6.915145
I0331 21:34:43.460543 140688727717696 submission.py:119] 51) loss = 6.915, grad_norm = 0.603
I0331 21:34:43.843550 140636195899136 logging_writer.py:48] [52] global_step=52, grad_norm=0.600305, loss=6.910698
I0331 21:34:43.847317 140688727717696 submission.py:119] 52) loss = 6.911, grad_norm = 0.600
I0331 21:34:44.229731 140636187506432 logging_writer.py:48] [53] global_step=53, grad_norm=0.599365, loss=6.914431
I0331 21:34:44.233574 140688727717696 submission.py:119] 53) loss = 6.914, grad_norm = 0.599
I0331 21:34:44.616334 140636195899136 logging_writer.py:48] [54] global_step=54, grad_norm=0.614045, loss=6.901351
I0331 21:34:44.620045 140688727717696 submission.py:119] 54) loss = 6.901, grad_norm = 0.614
I0331 21:34:45.008161 140636187506432 logging_writer.py:48] [55] global_step=55, grad_norm=0.599975, loss=6.910089
I0331 21:34:45.011759 140688727717696 submission.py:119] 55) loss = 6.910, grad_norm = 0.600
I0331 21:34:45.397331 140636195899136 logging_writer.py:48] [56] global_step=56, grad_norm=0.610563, loss=6.905416
I0331 21:34:45.401017 140688727717696 submission.py:119] 56) loss = 6.905, grad_norm = 0.611
I0331 21:34:45.784703 140636187506432 logging_writer.py:48] [57] global_step=57, grad_norm=0.609858, loss=6.901887
I0331 21:34:45.788448 140688727717696 submission.py:119] 57) loss = 6.902, grad_norm = 0.610
I0331 21:34:46.169653 140636195899136 logging_writer.py:48] [58] global_step=58, grad_norm=0.619943, loss=6.903905
I0331 21:34:46.173620 140688727717696 submission.py:119] 58) loss = 6.904, grad_norm = 0.620
I0331 21:34:46.561247 140636187506432 logging_writer.py:48] [59] global_step=59, grad_norm=0.615753, loss=6.907283
I0331 21:34:46.564701 140688727717696 submission.py:119] 59) loss = 6.907, grad_norm = 0.616
I0331 21:34:46.955664 140636195899136 logging_writer.py:48] [60] global_step=60, grad_norm=0.600361, loss=6.898576
I0331 21:34:46.959484 140688727717696 submission.py:119] 60) loss = 6.899, grad_norm = 0.600
I0331 21:34:47.346416 140636187506432 logging_writer.py:48] [61] global_step=61, grad_norm=0.585490, loss=6.900778
I0331 21:34:47.350282 140688727717696 submission.py:119] 61) loss = 6.901, grad_norm = 0.585
I0331 21:34:47.733369 140636195899136 logging_writer.py:48] [62] global_step=62, grad_norm=0.597871, loss=6.920588
I0331 21:34:47.737041 140688727717696 submission.py:119] 62) loss = 6.921, grad_norm = 0.598
I0331 21:34:48.130017 140636187506432 logging_writer.py:48] [63] global_step=63, grad_norm=0.613918, loss=6.903297
I0331 21:34:48.135818 140688727717696 submission.py:119] 63) loss = 6.903, grad_norm = 0.614
I0331 21:34:48.525882 140636195899136 logging_writer.py:48] [64] global_step=64, grad_norm=0.631533, loss=6.901965
I0331 21:34:48.530081 140688727717696 submission.py:119] 64) loss = 6.902, grad_norm = 0.632
I0331 21:34:48.913465 140636187506432 logging_writer.py:48] [65] global_step=65, grad_norm=0.604577, loss=6.905011
I0331 21:34:48.917044 140688727717696 submission.py:119] 65) loss = 6.905, grad_norm = 0.605
I0331 21:34:49.297957 140636195899136 logging_writer.py:48] [66] global_step=66, grad_norm=0.602898, loss=6.899836
I0331 21:34:49.301888 140688727717696 submission.py:119] 66) loss = 6.900, grad_norm = 0.603
I0331 21:34:49.693060 140636187506432 logging_writer.py:48] [67] global_step=67, grad_norm=0.608316, loss=6.901080
I0331 21:34:49.700486 140688727717696 submission.py:119] 67) loss = 6.901, grad_norm = 0.608
I0331 21:34:50.084738 140636195899136 logging_writer.py:48] [68] global_step=68, grad_norm=0.611108, loss=6.897230
I0331 21:34:50.088603 140688727717696 submission.py:119] 68) loss = 6.897, grad_norm = 0.611
I0331 21:34:50.472304 140636187506432 logging_writer.py:48] [69] global_step=69, grad_norm=0.584294, loss=6.896471
I0331 21:34:50.476119 140688727717696 submission.py:119] 69) loss = 6.896, grad_norm = 0.584
I0331 21:34:50.857369 140636195899136 logging_writer.py:48] [70] global_step=70, grad_norm=0.606826, loss=6.901941
I0331 21:34:50.860993 140688727717696 submission.py:119] 70) loss = 6.902, grad_norm = 0.607
I0331 21:34:51.244957 140636187506432 logging_writer.py:48] [71] global_step=71, grad_norm=0.609016, loss=6.904245
I0331 21:34:51.250670 140688727717696 submission.py:119] 71) loss = 6.904, grad_norm = 0.609
I0331 21:34:51.634923 140636195899136 logging_writer.py:48] [72] global_step=72, grad_norm=0.607149, loss=6.894305
I0331 21:34:51.638519 140688727717696 submission.py:119] 72) loss = 6.894, grad_norm = 0.607
I0331 21:34:52.021305 140636187506432 logging_writer.py:48] [73] global_step=73, grad_norm=0.629534, loss=6.900479
I0331 21:34:52.024862 140688727717696 submission.py:119] 73) loss = 6.900, grad_norm = 0.630
I0331 21:34:52.406699 140636195899136 logging_writer.py:48] [74] global_step=74, grad_norm=0.585514, loss=6.900268
I0331 21:34:52.410520 140688727717696 submission.py:119] 74) loss = 6.900, grad_norm = 0.586
I0331 21:34:52.795430 140636187506432 logging_writer.py:48] [75] global_step=75, grad_norm=0.590952, loss=6.896572
I0331 21:34:52.799062 140688727717696 submission.py:119] 75) loss = 6.897, grad_norm = 0.591
I0331 21:34:53.180717 140636195899136 logging_writer.py:48] [76] global_step=76, grad_norm=0.601083, loss=6.898665
I0331 21:34:53.184475 140688727717696 submission.py:119] 76) loss = 6.899, grad_norm = 0.601
I0331 21:34:53.568119 140636187506432 logging_writer.py:48] [77] global_step=77, grad_norm=0.606367, loss=6.888709
I0331 21:34:53.571943 140688727717696 submission.py:119] 77) loss = 6.889, grad_norm = 0.606
I0331 21:34:53.957882 140636195899136 logging_writer.py:48] [78] global_step=78, grad_norm=0.612459, loss=6.895309
I0331 21:34:53.961424 140688727717696 submission.py:119] 78) loss = 6.895, grad_norm = 0.612
I0331 21:34:54.344141 140636187506432 logging_writer.py:48] [79] global_step=79, grad_norm=0.619728, loss=6.900435
I0331 21:34:54.347828 140688727717696 submission.py:119] 79) loss = 6.900, grad_norm = 0.620
I0331 21:34:54.731484 140636195899136 logging_writer.py:48] [80] global_step=80, grad_norm=0.590376, loss=6.888547
I0331 21:34:54.735010 140688727717696 submission.py:119] 80) loss = 6.889, grad_norm = 0.590
I0331 21:34:55.117565 140636187506432 logging_writer.py:48] [81] global_step=81, grad_norm=0.593003, loss=6.897009
I0331 21:34:55.121287 140688727717696 submission.py:119] 81) loss = 6.897, grad_norm = 0.593
I0331 21:34:55.505190 140636195899136 logging_writer.py:48] [82] global_step=82, grad_norm=0.596775, loss=6.886503
I0331 21:34:55.509035 140688727717696 submission.py:119] 82) loss = 6.887, grad_norm = 0.597
I0331 21:34:55.890728 140636187506432 logging_writer.py:48] [83] global_step=83, grad_norm=0.606205, loss=6.890126
I0331 21:34:55.894158 140688727717696 submission.py:119] 83) loss = 6.890, grad_norm = 0.606
I0331 21:34:56.285910 140636195899136 logging_writer.py:48] [84] global_step=84, grad_norm=0.600973, loss=6.893507
I0331 21:34:56.289476 140688727717696 submission.py:119] 84) loss = 6.894, grad_norm = 0.601
I0331 21:34:56.671079 140636187506432 logging_writer.py:48] [85] global_step=85, grad_norm=0.595636, loss=6.890861
I0331 21:34:56.674656 140688727717696 submission.py:119] 85) loss = 6.891, grad_norm = 0.596
I0331 21:34:57.058506 140636195899136 logging_writer.py:48] [86] global_step=86, grad_norm=0.615875, loss=6.884463
I0331 21:34:57.062898 140688727717696 submission.py:119] 86) loss = 6.884, grad_norm = 0.616
I0331 21:34:57.444506 140636187506432 logging_writer.py:48] [87] global_step=87, grad_norm=0.603728, loss=6.883305
I0331 21:34:57.448002 140688727717696 submission.py:119] 87) loss = 6.883, grad_norm = 0.604
I0331 21:34:57.830757 140636195899136 logging_writer.py:48] [88] global_step=88, grad_norm=0.603555, loss=6.880702
I0331 21:34:57.834869 140688727717696 submission.py:119] 88) loss = 6.881, grad_norm = 0.604
I0331 21:34:58.217390 140636187506432 logging_writer.py:48] [89] global_step=89, grad_norm=0.600679, loss=6.884149
I0331 21:34:58.220891 140688727717696 submission.py:119] 89) loss = 6.884, grad_norm = 0.601
I0331 21:34:58.611452 140636195899136 logging_writer.py:48] [90] global_step=90, grad_norm=0.615164, loss=6.889947
I0331 21:34:58.615212 140688727717696 submission.py:119] 90) loss = 6.890, grad_norm = 0.615
I0331 21:34:58.998098 140636187506432 logging_writer.py:48] [91] global_step=91, grad_norm=0.590771, loss=6.883740
I0331 21:34:59.001748 140688727717696 submission.py:119] 91) loss = 6.884, grad_norm = 0.591
I0331 21:34:59.384139 140636195899136 logging_writer.py:48] [92] global_step=92, grad_norm=0.618509, loss=6.879077
I0331 21:34:59.387774 140688727717696 submission.py:119] 92) loss = 6.879, grad_norm = 0.619
I0331 21:34:59.776066 140636187506432 logging_writer.py:48] [93] global_step=93, grad_norm=0.599451, loss=6.882792
I0331 21:34:59.780245 140688727717696 submission.py:119] 93) loss = 6.883, grad_norm = 0.599
I0331 21:35:00.170424 140636195899136 logging_writer.py:48] [94] global_step=94, grad_norm=0.595889, loss=6.886676
I0331 21:35:00.174440 140688727717696 submission.py:119] 94) loss = 6.887, grad_norm = 0.596
I0331 21:35:00.560030 140636187506432 logging_writer.py:48] [95] global_step=95, grad_norm=0.607832, loss=6.881193
I0331 21:35:00.563697 140688727717696 submission.py:119] 95) loss = 6.881, grad_norm = 0.608
I0331 21:35:00.945567 140636195899136 logging_writer.py:48] [96] global_step=96, grad_norm=0.608151, loss=6.878950
I0331 21:35:00.949079 140688727717696 submission.py:119] 96) loss = 6.879, grad_norm = 0.608
I0331 21:35:01.332366 140636187506432 logging_writer.py:48] [97] global_step=97, grad_norm=0.606580, loss=6.867265
I0331 21:35:01.335958 140688727717696 submission.py:119] 97) loss = 6.867, grad_norm = 0.607
I0331 21:35:01.718308 140636195899136 logging_writer.py:48] [98] global_step=98, grad_norm=0.617077, loss=6.870948
I0331 21:35:01.721773 140688727717696 submission.py:119] 98) loss = 6.871, grad_norm = 0.617
I0331 21:35:02.106739 140636187506432 logging_writer.py:48] [99] global_step=99, grad_norm=0.610436, loss=6.871908
I0331 21:35:02.110502 140688727717696 submission.py:119] 99) loss = 6.872, grad_norm = 0.610
I0331 21:35:02.494756 140636195899136 logging_writer.py:48] [100] global_step=100, grad_norm=0.607598, loss=6.876419
I0331 21:35:02.498263 140688727717696 submission.py:119] 100) loss = 6.876, grad_norm = 0.608
I0331 21:37:32.961840 140636187506432 logging_writer.py:48] [500] global_step=500, grad_norm=1.350969, loss=6.287914
I0331 21:37:32.966302 140688727717696 submission.py:119] 500) loss = 6.288, grad_norm = 1.351
I0331 21:40:41.339349 140636195899136 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.401603, loss=5.700572
I0331 21:40:41.345767 140688727717696 submission.py:119] 1000) loss = 5.701, grad_norm = 3.402
I0331 21:42:53.958984 140688727717696 submission_runner.py:371] Before eval at step 1350: RAM USED (GB) 98.508152832
I0331 21:42:53.959201 140688727717696 spec.py:298] Evaluating on the training split.
I0331 21:43:35.799500 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 21:44:19.347281 140688727717696 spec.py:326] Evaluating on the test split.
I0331 21:44:20.736232 140688727717696 submission_runner.py:380] Time since start: 623.67s, 	Step: 1350, 	{'train/accuracy': 0.11220503826530612, 'train/loss': 4.836207798549107, 'validation/accuracy': 0.10164, 'validation/loss': 4.911870625, 'validation/num_examples': 50000, 'test/accuracy': 0.0734, 'test/loss': 5.284830078125, 'test/num_examples': 10000}
I0331 21:44:20.736563 140688727717696 submission_runner.py:390] After eval at step 1350: RAM USED (GB) 98.213019648
I0331 21:44:20.745059 140636204291840 logging_writer.py:48] [1350] global_step=1350, preemption_count=0, score=515.891943, test/accuracy=0.073400, test/loss=5.284830, test/num_examples=10000, total_duration=623.671383, train/accuracy=0.112205, train/loss=4.836208, validation/accuracy=0.101640, validation/loss=4.911871, validation/num_examples=50000
I0331 21:44:21.194026 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_1350.
I0331 21:44:21.194755 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 1350: RAM USED (GB) 98.212634624
I0331 21:45:17.599716 140636212684544 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.655269, loss=5.332658
I0331 21:45:17.603793 140688727717696 submission.py:119] 1500) loss = 5.333, grad_norm = 4.655
I0331 21:48:24.655015 140636204291840 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.936391, loss=4.960173
I0331 21:48:24.658910 140688727717696 submission.py:119] 2000) loss = 4.960, grad_norm = 3.936
I0331 21:51:32.862134 140636212684544 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.263071, loss=4.683734
I0331 21:51:32.866263 140688727717696 submission.py:119] 2500) loss = 4.684, grad_norm = 4.263
I0331 21:52:51.222780 140688727717696 submission_runner.py:371] Before eval at step 2707: RAM USED (GB) 99.601629184
I0331 21:52:51.223000 140688727717696 spec.py:298] Evaluating on the training split.
I0331 21:53:33.557431 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 21:54:27.220474 140688727717696 spec.py:326] Evaluating on the test split.
I0331 21:54:28.617999 140688727717696 submission_runner.py:380] Time since start: 1220.94s, 	Step: 2707, 	{'train/accuracy': 0.24001514668367346, 'train/loss': 3.72224083725287, 'validation/accuracy': 0.21892, 'validation/loss': 3.864601875, 'validation/num_examples': 50000, 'test/accuracy': 0.1469, 'test/loss': 4.5024109375, 'test/num_examples': 10000}
I0331 21:54:28.618328 140688727717696 submission_runner.py:390] After eval at step 2707: RAM USED (GB) 99.628015616
I0331 21:54:28.626597 140636204291840 logging_writer.py:48] [2707] global_step=2707, preemption_count=0, score=1023.662576, test/accuracy=0.146900, test/loss=4.502411, test/num_examples=10000, total_duration=1220.935008, train/accuracy=0.240015, train/loss=3.722241, validation/accuracy=0.218920, validation/loss=3.864602, validation/num_examples=50000
I0331 21:54:29.058551 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_2707.
I0331 21:54:29.059334 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 2707: RAM USED (GB) 99.626491904
I0331 21:56:18.883896 140636212684544 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.643702, loss=4.336322
I0331 21:56:18.887641 140688727717696 submission.py:119] 3000) loss = 4.336, grad_norm = 4.644
I0331 21:59:25.952207 140636204291840 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.021785, loss=4.019859
I0331 21:59:25.956286 140688727717696 submission.py:119] 3500) loss = 4.020, grad_norm = 3.022
I0331 22:02:34.795055 140636212684544 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.396607, loss=3.940984
I0331 22:02:34.799846 140688727717696 submission.py:119] 4000) loss = 3.941, grad_norm = 2.397
I0331 22:02:59.127267 140688727717696 submission_runner.py:371] Before eval at step 4066: RAM USED (GB) 99.75789568
I0331 22:02:59.127472 140688727717696 spec.py:298] Evaluating on the training split.
I0331 22:03:41.195142 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 22:04:36.723134 140688727717696 spec.py:326] Evaluating on the test split.
I0331 22:04:38.082380 140688727717696 submission_runner.py:380] Time since start: 1828.84s, 	Step: 4066, 	{'train/accuracy': 0.36324139030612246, 'train/loss': 2.9780475850007972, 'validation/accuracy': 0.3385, 'validation/loss': 3.1435728125, 'validation/num_examples': 50000, 'test/accuracy': 0.2514, 'test/loss': 3.802466796875, 'test/num_examples': 10000}
I0331 22:04:38.082719 140688727717696 submission_runner.py:390] After eval at step 4066: RAM USED (GB) 99.818819584
I0331 22:04:38.090831 140636204291840 logging_writer.py:48] [4066] global_step=4066, preemption_count=0, score=1531.536463, test/accuracy=0.251400, test/loss=3.802467, test/num_examples=10000, total_duration=1828.839645, train/accuracy=0.363241, train/loss=2.978048, validation/accuracy=0.338500, validation/loss=3.143573, validation/num_examples=50000
I0331 22:04:38.534991 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_4066.
I0331 22:04:38.537106 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 4066: RAM USED (GB) 99.817558016
I0331 22:07:21.146787 140636212684544 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.203023, loss=3.607490
I0331 22:07:21.152242 140688727717696 submission.py:119] 4500) loss = 3.607, grad_norm = 2.203
I0331 22:10:29.070303 140636204291840 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.626412, loss=3.690237
I0331 22:10:29.074847 140688727717696 submission.py:119] 5000) loss = 3.690, grad_norm = 2.626
I0331 22:13:08.732953 140688727717696 submission_runner.py:371] Before eval at step 5424: RAM USED (GB) 99.967012864
I0331 22:13:08.733206 140688727717696 spec.py:298] Evaluating on the training split.
I0331 22:13:53.711573 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 22:14:41.284536 140688727717696 spec.py:326] Evaluating on the test split.
I0331 22:14:42.643802 140688727717696 submission_runner.py:380] Time since start: 2438.45s, 	Step: 5424, 	{'train/accuracy': 0.4326969068877551, 'train/loss': 2.5791337927993463, 'validation/accuracy': 0.393, 'validation/loss': 2.7994503125, 'validation/num_examples': 50000, 'test/accuracy': 0.2899, 'test/loss': 3.551389453125, 'test/num_examples': 10000}
I0331 22:14:42.644145 140688727717696 submission_runner.py:390] After eval at step 5424: RAM USED (GB) 100.008267776
I0331 22:14:42.653362 140636212684544 logging_writer.py:48] [5424] global_step=5424, preemption_count=0, score=2039.524770, test/accuracy=0.289900, test/loss=3.551389, test/num_examples=10000, total_duration=2438.445244, train/accuracy=0.432697, train/loss=2.579134, validation/accuracy=0.393000, validation/loss=2.799450, validation/num_examples=50000
I0331 22:14:43.096344 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_5424.
I0331 22:14:43.097146 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 5424: RAM USED (GB) 100.008095744
I0331 22:15:11.842748 140636204291840 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.532304, loss=3.583169
I0331 22:15:11.846420 140688727717696 submission.py:119] 5500) loss = 3.583, grad_norm = 1.532
I0331 22:18:18.919222 140636212684544 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.221447, loss=3.455337
I0331 22:18:18.923379 140688727717696 submission.py:119] 6000) loss = 3.455, grad_norm = 2.221
I0331 22:21:27.802124 140636204291840 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.446594, loss=3.289221
I0331 22:21:27.808058 140688727717696 submission.py:119] 6500) loss = 3.289, grad_norm = 1.447
I0331 22:23:13.391507 140688727717696 submission_runner.py:371] Before eval at step 6784: RAM USED (GB) 100.092960768
I0331 22:23:13.391746 140688727717696 spec.py:298] Evaluating on the training split.
I0331 22:23:55.222808 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 22:24:48.136448 140688727717696 spec.py:326] Evaluating on the test split.
I0331 22:24:49.527516 140688727717696 submission_runner.py:380] Time since start: 3043.10s, 	Step: 6784, 	{'train/accuracy': 0.5029894770408163, 'train/loss': 2.286790653150909, 'validation/accuracy': 0.45642, 'validation/loss': 2.51895828125, 'validation/num_examples': 50000, 'test/accuracy': 0.3581, 'test/loss': 3.1534560546875, 'test/num_examples': 10000}
I0331 22:24:49.527870 140688727717696 submission_runner.py:390] After eval at step 6784: RAM USED (GB) 100.076130304
I0331 22:24:49.536056 140636212684544 logging_writer.py:48] [6784] global_step=6784, preemption_count=0, score=2547.635496, test/accuracy=0.358100, test/loss=3.153456, test/num_examples=10000, total_duration=3043.103759, train/accuracy=0.502989, train/loss=2.286791, validation/accuracy=0.456420, validation/loss=2.518958, validation/num_examples=50000
I0331 22:24:49.978621 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_6784.
I0331 22:24:49.979744 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 6784: RAM USED (GB) 100.074655744
I0331 22:26:11.024882 140636204291840 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.921643, loss=3.272586
I0331 22:26:11.029224 140688727717696 submission.py:119] 7000) loss = 3.273, grad_norm = 1.922
I0331 22:29:18.604302 140636212684544 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.984492, loss=3.177673
I0331 22:29:18.608432 140688727717696 submission.py:119] 7500) loss = 3.178, grad_norm = 0.984
I0331 22:32:26.525130 140636204291840 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.512394, loss=3.152108
I0331 22:32:26.528802 140688727717696 submission.py:119] 8000) loss = 3.152, grad_norm = 1.512
I0331 22:33:20.251528 140688727717696 submission_runner.py:371] Before eval at step 8145: RAM USED (GB) 99.708612608
I0331 22:33:20.251732 140688727717696 spec.py:298] Evaluating on the training split.
I0331 22:34:02.769750 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 22:34:48.858063 140688727717696 spec.py:326] Evaluating on the test split.
I0331 22:34:50.217816 140688727717696 submission_runner.py:380] Time since start: 3649.96s, 	Step: 8145, 	{'train/accuracy': 0.5415138711734694, 'train/loss': 2.0556927116549746, 'validation/accuracy': 0.49296, 'validation/loss': 2.3083834375, 'validation/num_examples': 50000, 'test/accuracy': 0.371, 'test/loss': 3.0446119140625, 'test/num_examples': 10000}
I0331 22:34:50.218208 140688727717696 submission_runner.py:390] After eval at step 8145: RAM USED (GB) 99.881955328
I0331 22:34:50.227534 140636212684544 logging_writer.py:48] [8145] global_step=8145, preemption_count=0, score=3055.738572, test/accuracy=0.371000, test/loss=3.044612, test/num_examples=10000, total_duration=3649.963875, train/accuracy=0.541514, train/loss=2.055693, validation/accuracy=0.492960, validation/loss=2.308383, validation/num_examples=50000
I0331 22:34:50.688895 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_8145.
I0331 22:34:50.689766 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 8145: RAM USED (GB) 99.879743488
I0331 22:37:03.822322 140636204291840 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.221611, loss=3.065614
I0331 22:37:03.828845 140688727717696 submission.py:119] 8500) loss = 3.066, grad_norm = 1.222
I0331 22:40:12.616922 140636212684544 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.054644, loss=3.074970
I0331 22:40:12.621436 140688727717696 submission.py:119] 9000) loss = 3.075, grad_norm = 1.055
I0331 22:43:19.195022 140636204291840 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.891821, loss=3.017612
I0331 22:43:19.199025 140688727717696 submission.py:119] 9500) loss = 3.018, grad_norm = 0.892
I0331 22:43:20.696048 140688727717696 submission_runner.py:371] Before eval at step 9505: RAM USED (GB) 100.079824896
I0331 22:43:20.696280 140688727717696 spec.py:298] Evaluating on the training split.
I0331 22:44:04.198273 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 22:44:49.729918 140688727717696 spec.py:326] Evaluating on the test split.
I0331 22:44:51.090633 140688727717696 submission_runner.py:380] Time since start: 4250.41s, 	Step: 9505, 	{'train/accuracy': 0.5991111288265306, 'train/loss': 1.7929536469128666, 'validation/accuracy': 0.5441, 'validation/loss': 2.05903140625, 'validation/num_examples': 50000, 'test/accuracy': 0.4187, 'test/loss': 2.7736751953125, 'test/num_examples': 10000}
I0331 22:44:51.091007 140688727717696 submission_runner.py:390] After eval at step 9505: RAM USED (GB) 99.996286976
I0331 22:44:51.099724 140636212684544 logging_writer.py:48] [9505] global_step=9505, preemption_count=0, score=3563.586604, test/accuracy=0.418700, test/loss=2.773675, test/num_examples=10000, total_duration=4250.408110, train/accuracy=0.599111, train/loss=1.792954, validation/accuracy=0.544100, validation/loss=2.059031, validation/num_examples=50000
I0331 22:44:51.539666 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_9505.
I0331 22:44:51.540406 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 9505: RAM USED (GB) 99.99505408
I0331 22:47:57.792460 140636204291840 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.106063, loss=2.934869
I0331 22:47:57.797066 140688727717696 submission.py:119] 10000) loss = 2.935, grad_norm = 1.106
I0331 22:51:05.913000 140636212684544 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.131989, loss=2.951926
I0331 22:51:05.916837 140688727717696 submission.py:119] 10500) loss = 2.952, grad_norm = 1.132
I0331 22:53:21.786806 140688727717696 submission_runner.py:371] Before eval at step 10865: RAM USED (GB) 99.90223872
I0331 22:53:21.787003 140688727717696 spec.py:298] Evaluating on the training split.
I0331 22:54:05.280887 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 22:54:51.120688 140688727717696 spec.py:326] Evaluating on the test split.
I0331 22:54:52.469140 140688727717696 submission_runner.py:380] Time since start: 4851.50s, 	Step: 10865, 	{'train/accuracy': 0.6325334821428571, 'train/loss': 1.6058860311702805, 'validation/accuracy': 0.5726, 'validation/loss': 1.90156828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4407, 'test/loss': 2.6444544921875, 'test/num_examples': 10000}
I0331 22:54:52.469532 140688727717696 submission_runner.py:390] After eval at step 10865: RAM USED (GB) 99.907833856
I0331 22:54:52.478916 140636204291840 logging_writer.py:48] [10865] global_step=10865, preemption_count=0, score=4071.603575, test/accuracy=0.440700, test/loss=2.644454, test/num_examples=10000, total_duration=4851.498982, train/accuracy=0.632533, train/loss=1.605886, validation/accuracy=0.572600, validation/loss=1.901568, validation/num_examples=50000
I0331 22:54:52.920860 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_10865.
I0331 22:54:52.921697 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 10865: RAM USED (GB) 99.906113536
I0331 22:55:43.892276 140636212684544 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.947554, loss=2.807594
I0331 22:55:43.896047 140688727717696 submission.py:119] 11000) loss = 2.808, grad_norm = 0.948
I0331 22:58:52.620639 140636204291840 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.585935, loss=2.764763
I0331 22:58:52.625056 140688727717696 submission.py:119] 11500) loss = 2.765, grad_norm = 0.586
I0331 23:01:59.174444 140636212684544 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.774134, loss=2.760960
I0331 23:01:59.178469 140688727717696 submission.py:119] 12000) loss = 2.761, grad_norm = 0.774
I0331 23:03:23.211317 140688727717696 submission_runner.py:371] Before eval at step 12226: RAM USED (GB) 99.964489728
I0331 23:03:23.211510 140688727717696 spec.py:298] Evaluating on the training split.
I0331 23:04:05.606271 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 23:04:59.792702 140688727717696 spec.py:326] Evaluating on the test split.
I0331 23:05:01.145656 140688727717696 submission_runner.py:380] Time since start: 5452.92s, 	Step: 12226, 	{'train/accuracy': 0.6468829719387755, 'train/loss': 1.5697932340660874, 'validation/accuracy': 0.58088, 'validation/loss': 1.88274390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4539, 'test/loss': 2.5900640625, 'test/num_examples': 10000}
I0331 23:05:01.146042 140688727717696 submission_runner.py:390] After eval at step 12226: RAM USED (GB) 99.984310272
I0331 23:05:01.156051 140636204291840 logging_writer.py:48] [12226] global_step=12226, preemption_count=0, score=4579.720565, test/accuracy=0.453900, test/loss=2.590064, test/num_examples=10000, total_duration=5452.923475, train/accuracy=0.646883, train/loss=1.569793, validation/accuracy=0.580880, validation/loss=1.882744, validation/num_examples=50000
I0331 23:05:01.615574 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_12226.
I0331 23:05:01.616365 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 12226: RAM USED (GB) 99.980918784
I0331 23:06:45.013622 140636212684544 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.717063, loss=2.831390
I0331 23:06:45.018627 140688727717696 submission.py:119] 12500) loss = 2.831, grad_norm = 0.717
I0331 23:09:53.187793 140636204291840 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.809222, loss=2.707474
I0331 23:09:53.192201 140688727717696 submission.py:119] 13000) loss = 2.707, grad_norm = 0.809
I0331 23:13:00.023899 140636212684544 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.642023, loss=2.677328
I0331 23:13:00.029156 140688727717696 submission.py:119] 13500) loss = 2.677, grad_norm = 0.642
I0331 23:13:31.968090 140688727717696 submission_runner.py:371] Before eval at step 13586: RAM USED (GB) 99.921465344
I0331 23:13:31.968297 140688727717696 spec.py:298] Evaluating on the training split.
I0331 23:14:14.929606 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 23:15:01.902621 140688727717696 spec.py:326] Evaluating on the test split.
I0331 23:15:03.261070 140688727717696 submission_runner.py:380] Time since start: 6061.68s, 	Step: 13586, 	{'train/accuracy': 0.6667928890306123, 'train/loss': 1.4690212327606824, 'validation/accuracy': 0.59836, 'validation/loss': 1.80166375, 'validation/num_examples': 50000, 'test/accuracy': 0.462, 'test/loss': 2.495459765625, 'test/num_examples': 10000}
I0331 23:15:03.261388 140688727717696 submission_runner.py:390] After eval at step 13586: RAM USED (GB) 99.97060096
I0331 23:15:03.270160 140636204291840 logging_writer.py:48] [13586] global_step=13586, preemption_count=0, score=5087.864877, test/accuracy=0.462000, test/loss=2.495460, test/num_examples=10000, total_duration=6061.680347, train/accuracy=0.666793, train/loss=1.469021, validation/accuracy=0.598360, validation/loss=1.801664, validation/num_examples=50000
I0331 23:15:03.708910 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_13586.
I0331 23:15:03.709686 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 13586: RAM USED (GB) 99.962806272
I0331 23:17:40.309830 140688727717696 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 100.108210176
I0331 23:17:40.310058 140688727717696 spec.py:298] Evaluating on the training split.
I0331 23:18:21.973598 140688727717696 spec.py:310] Evaluating on the validation split.
I0331 23:19:06.069001 140688727717696 spec.py:326] Evaluating on the test split.
I0331 23:19:07.415573 140688727717696 submission_runner.py:380] Time since start: 6310.02s, 	Step: 14000, 	{'train/accuracy': 0.6855668048469388, 'train/loss': 1.3763879269969708, 'validation/accuracy': 0.61082, 'validation/loss': 1.7307846875, 'validation/num_examples': 50000, 'test/accuracy': 0.478, 'test/loss': 2.447826953125, 'test/num_examples': 10000}
I0331 23:19:07.415918 140688727717696 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 100.024979456
I0331 23:19:07.424315 140636212684544 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5243.781220, test/accuracy=0.478000, test/loss=2.447827, test/num_examples=10000, total_duration=6310.022059, train/accuracy=0.685567, train/loss=1.376388, validation/accuracy=0.610820, validation/loss=1.730785, validation/num_examples=50000
I0331 23:19:07.870952 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0331 23:19:07.871708 140688727717696 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.023775232
I0331 23:19:07.880219 140636204291840 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5243.781220
I0331 23:19:09.104001 140688727717696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0331 23:19:09.441347 140688727717696 submission_runner.py:543] Tuning trial 1/1
I0331 23:19:09.441541 140688727717696 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0331 23:19:09.442208 140688727717696 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008569834183673469, 'train/loss': 6.923496791294643, 'validation/accuracy': 0.00108, 'validation/loss': 6.923540625, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92712265625, 'test/num_examples': 10000, 'score': 7.838140487670898, 'total_duration': 7.8399574756622314, 'global_step': 1, 'preemption_count': 0}), (1350, {'train/accuracy': 0.11220503826530612, 'train/loss': 4.836207798549107, 'validation/accuracy': 0.10164, 'validation/loss': 4.911870625, 'validation/num_examples': 50000, 'test/accuracy': 0.0734, 'test/loss': 5.284830078125, 'test/num_examples': 10000, 'score': 515.8919432163239, 'total_duration': 623.6713826656342, 'global_step': 1350, 'preemption_count': 0}), (2707, {'train/accuracy': 0.24001514668367346, 'train/loss': 3.72224083725287, 'validation/accuracy': 0.21892, 'validation/loss': 3.864601875, 'validation/num_examples': 50000, 'test/accuracy': 0.1469, 'test/loss': 4.5024109375, 'test/num_examples': 10000, 'score': 1023.6625761985779, 'total_duration': 1220.935007572174, 'global_step': 2707, 'preemption_count': 0}), (4066, {'train/accuracy': 0.36324139030612246, 'train/loss': 2.9780475850007972, 'validation/accuracy': 0.3385, 'validation/loss': 3.1435728125, 'validation/num_examples': 50000, 'test/accuracy': 0.2514, 'test/loss': 3.802466796875, 'test/num_examples': 10000, 'score': 1531.536463022232, 'total_duration': 1828.8396451473236, 'global_step': 4066, 'preemption_count': 0}), (5424, {'train/accuracy': 0.4326969068877551, 'train/loss': 2.5791337927993463, 'validation/accuracy': 0.393, 'validation/loss': 2.7994503125, 'validation/num_examples': 50000, 'test/accuracy': 0.2899, 'test/loss': 3.551389453125, 'test/num_examples': 10000, 'score': 2039.5247704982758, 'total_duration': 2438.4452435970306, 'global_step': 5424, 'preemption_count': 0}), (6784, {'train/accuracy': 0.5029894770408163, 'train/loss': 2.286790653150909, 'validation/accuracy': 0.45642, 'validation/loss': 2.51895828125, 'validation/num_examples': 50000, 'test/accuracy': 0.3581, 'test/loss': 3.1534560546875, 'test/num_examples': 10000, 'score': 2547.635495901108, 'total_duration': 3043.1037590503693, 'global_step': 6784, 'preemption_count': 0}), (8145, {'train/accuracy': 0.5415138711734694, 'train/loss': 2.0556927116549746, 'validation/accuracy': 0.49296, 'validation/loss': 2.3083834375, 'validation/num_examples': 50000, 'test/accuracy': 0.371, 'test/loss': 3.0446119140625, 'test/num_examples': 10000, 'score': 3055.738572359085, 'total_duration': 3649.9638748168945, 'global_step': 8145, 'preemption_count': 0}), (9505, {'train/accuracy': 0.5991111288265306, 'train/loss': 1.7929536469128666, 'validation/accuracy': 0.5441, 'validation/loss': 2.05903140625, 'validation/num_examples': 50000, 'test/accuracy': 0.4187, 'test/loss': 2.7736751953125, 'test/num_examples': 10000, 'score': 3563.5866038799286, 'total_duration': 4250.408109903336, 'global_step': 9505, 'preemption_count': 0}), (10865, {'train/accuracy': 0.6325334821428571, 'train/loss': 1.6058860311702805, 'validation/accuracy': 0.5726, 'validation/loss': 1.90156828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4407, 'test/loss': 2.6444544921875, 'test/num_examples': 10000, 'score': 4071.603574991226, 'total_duration': 4851.498982191086, 'global_step': 10865, 'preemption_count': 0}), (12226, {'train/accuracy': 0.6468829719387755, 'train/loss': 1.5697932340660874, 'validation/accuracy': 0.58088, 'validation/loss': 1.88274390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4539, 'test/loss': 2.5900640625, 'test/num_examples': 10000, 'score': 4579.7205646038055, 'total_duration': 5452.923475265503, 'global_step': 12226, 'preemption_count': 0}), (13586, {'train/accuracy': 0.6667928890306123, 'train/loss': 1.4690212327606824, 'validation/accuracy': 0.59836, 'validation/loss': 1.80166375, 'validation/num_examples': 50000, 'test/accuracy': 0.462, 'test/loss': 2.495459765625, 'test/num_examples': 10000, 'score': 5087.864876747131, 'total_duration': 6061.68034696579, 'global_step': 13586, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6855668048469388, 'train/loss': 1.3763879269969708, 'validation/accuracy': 0.61082, 'validation/loss': 1.7307846875, 'validation/num_examples': 50000, 'test/accuracy': 0.478, 'test/loss': 2.447826953125, 'test/num_examples': 10000, 'score': 5243.78121972084, 'total_duration': 6310.022058963776, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0331 23:19:09.442309 140688727717696 submission_runner.py:546] Timing: 5243.78121972084
I0331 23:19:09.442364 140688727717696 submission_runner.py:547] ====================
I0331 23:19:09.442495 140688727717696 submission_runner.py:606] Final imagenet_resnet score: 5243.78121972084
