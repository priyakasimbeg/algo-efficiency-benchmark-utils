WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 11:30:45.502940 140655539320640 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 11:30:45.502968 140581703296832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 11:30:45.502984 140418466846528 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 11:30:45.503000 139660176025408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 11:30:45.503572 139658016347968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 11:30:45.503814 140395536697152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 11:30:45.503958 140085848139584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 11:30:45.504263 140085848139584 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.504231 140165137565504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 11:30:45.504760 140165137565504 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.513734 140655539320640 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.513760 140581703296832 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.513802 140418466846528 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.513820 139660176025408 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.514258 139658016347968 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:45.514365 140395536697152 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 11:30:46.031244 140165137565504 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/librispeech_conformer_pytorch.
W0402 11:30:46.038078 140655539320640 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.038563 139658016347968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.039427 140085848139584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.040163 140418466846528 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.041355 140395536697152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.041457 140581703296832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.041470 139660176025408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 11:30:46.065654 140165137565504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 11:30:46.069115 140165137565504 submission_runner.py:511] Using RNG seed 2075448786
I0402 11:30:46.070213 140165137565504 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 11:30:46.070334 140165137565504 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1.
I0402 11:30:46.070554 140165137565504 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/hparams.json.
I0402 11:30:46.071529 140165137565504 submission_runner.py:230] Starting train once: RAM USED (GB) 5.895499776
I0402 11:30:46.071629 140165137565504 submission_runner.py:231] Initializing dataset.
I0402 11:30:46.071716 140165137565504 input_pipeline.py:20] Loading split = train-clean-100
I0402 11:30:46.102084 140165137565504 input_pipeline.py:20] Loading split = train-clean-360
I0402 11:30:46.426118 140165137565504 input_pipeline.py:20] Loading split = train-other-500
I0402 11:30:46.851230 140165137565504 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 6.096818176
I0402 11:30:46.851402 140165137565504 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0402 11:30:53.923125 140165137565504 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.584045056
I0402 11:30:53.923334 140165137565504 submission_runner.py:252] Initializing optimizer.
I0402 11:30:53.924396 140165137565504 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.315642368
I0402 11:30:53.924510 140165137565504 submission_runner.py:261] Initializing metrics bundle.
I0402 11:30:53.924566 140165137565504 submission_runner.py:276] Initializing checkpoint and logger.
I0402 11:30:53.925723 140165137565504 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 11:30:53.925819 140165137565504 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 11:30:54.689806 140165137565504 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0402 11:30:54.690748 140165137565504 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0402 11:30:54.695013 140165137565504 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.318267904
I0402 11:30:54.696098 140165137565504 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.318267904
I0402 11:30:54.696205 140165137565504 submission_runner.py:313] Starting training loop.
I0402 11:30:56.386097 140165137565504 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 25.18339584
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0402 11:31:01.255923 140138793391872 logging_writer.py:48] [0] global_step=0, grad_norm=64.161575, loss=31.789568
I0402 11:31:01.269348 140165137565504 submission.py:296] 0) loss = 31.790, grad_norm = 64.162
I0402 11:31:01.270195 140165137565504 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 33.127346176
I0402 11:31:01.289130 140165137565504 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 33.12838656
I0402 11:31:01.289270 140165137565504 spec.py:298] Evaluating on the training split.
I0402 11:31:01.290115 140165137565504 input_pipeline.py:20] Loading split = train-clean-100
I0402 11:31:01.319602 140165137565504 input_pipeline.py:20] Loading split = train-clean-360
I0402 11:31:01.796763 140165137565504 input_pipeline.py:20] Loading split = train-other-500
I0402 11:31:13.690379 140165137565504 spec.py:310] Evaluating on the validation split.
I0402 11:31:13.691586 140165137565504 input_pipeline.py:20] Loading split = dev-clean
I0402 11:31:13.694864 140165137565504 input_pipeline.py:20] Loading split = dev-other
I0402 11:31:23.401139 140165137565504 spec.py:326] Evaluating on the test split.
I0402 11:31:23.406134 140165137565504 input_pipeline.py:20] Loading split = test-clean
I0402 11:31:28.494474 140165137565504 submission_runner.py:382] Time since start: 6.59s, 	Step: 1, 	{'train/ctc_loss': 30.8289556110131, 'train/wer': 0.9897307437891061, 'validation/ctc_loss': 29.4308883363472, 'validation/wer': 0.9130787428185198, 'validation/num_examples': 5348, 'test/ctc_loss': 29.452219498032132, 'test/wer': 0.9283001239006358, 'test/num_examples': 2472}
I0402 11:31:28.495269 140165137565504 submission_runner.py:396] After eval at step 1: RAM USED (GB) 45.876486144
I0402 11:31:28.512967 140124394440448 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.591480, test/ctc_loss=29.452219, test/num_examples=2472, test/wer=0.928300, total_duration=6.593562, train/ctc_loss=30.828956, train/wer=0.989731, validation/ctc_loss=29.430888, validation/num_examples=5348, validation/wer=0.913079
I0402 11:31:29.097229 140165137565504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0402 11:31:29.097901 140165137565504 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.90819328
I0402 11:31:29.101954 140165137565504 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.910253568
I0402 11:31:29.140602 140165137565504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.141004 140655539320640 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.141005 139660176025408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.141042 139658016347968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.141543 140581703296832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.141435 140395536697152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.141780 140085848139584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:29.142529 140418466846528 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 11:31:30.229511 140123308029696 logging_writer.py:48] [1] global_step=1, grad_norm=63.792706, loss=31.256624
I0402 11:31:30.232928 140165137565504 submission.py:296] 1) loss = 31.257, grad_norm = 63.793
I0402 11:31:30.233687 140165137565504 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 46.084845568
I0402 11:31:31.098440 140124394440448 logging_writer.py:48] [2] global_step=2, grad_norm=63.558132, loss=31.638300
I0402 11:31:31.102232 140165137565504 submission.py:296] 2) loss = 31.638, grad_norm = 63.558
I0402 11:31:32.087503 140123308029696 logging_writer.py:48] [3] global_step=3, grad_norm=67.757088, loss=31.607258
I0402 11:31:32.091149 140165137565504 submission.py:296] 3) loss = 31.607, grad_norm = 67.757
I0402 11:31:32.898689 140124394440448 logging_writer.py:48] [4] global_step=4, grad_norm=63.675346, loss=31.112022
I0402 11:31:32.902237 140165137565504 submission.py:296] 4) loss = 31.112, grad_norm = 63.675
I0402 11:31:33.707700 140123308029696 logging_writer.py:48] [5] global_step=5, grad_norm=66.900627, loss=31.231432
I0402 11:31:33.711571 140165137565504 submission.py:296] 5) loss = 31.231, grad_norm = 66.901
I0402 11:31:34.516142 140124394440448 logging_writer.py:48] [6] global_step=6, grad_norm=72.265617, loss=31.036009
I0402 11:31:34.519416 140165137565504 submission.py:296] 6) loss = 31.036, grad_norm = 72.266
I0402 11:31:35.326746 140123308029696 logging_writer.py:48] [7] global_step=7, grad_norm=75.142982, loss=29.699089
I0402 11:31:35.330303 140165137565504 submission.py:296] 7) loss = 29.699, grad_norm = 75.143
I0402 11:31:36.139613 140124394440448 logging_writer.py:48] [8] global_step=8, grad_norm=80.675079, loss=29.731686
I0402 11:31:36.143378 140165137565504 submission.py:296] 8) loss = 29.732, grad_norm = 80.675
I0402 11:31:36.944327 140123308029696 logging_writer.py:48] [9] global_step=9, grad_norm=86.499535, loss=29.054874
I0402 11:31:36.947579 140165137565504 submission.py:296] 9) loss = 29.055, grad_norm = 86.500
I0402 11:31:37.748300 140124394440448 logging_writer.py:48] [10] global_step=10, grad_norm=97.959427, loss=28.444553
I0402 11:31:37.751522 140165137565504 submission.py:296] 10) loss = 28.445, grad_norm = 97.959
I0402 11:31:38.562334 140123308029696 logging_writer.py:48] [11] global_step=11, grad_norm=103.768906, loss=28.328449
I0402 11:31:38.565482 140165137565504 submission.py:296] 11) loss = 28.328, grad_norm = 103.769
I0402 11:31:39.374542 140124394440448 logging_writer.py:48] [12] global_step=12, grad_norm=106.114441, loss=27.886503
I0402 11:31:39.377566 140165137565504 submission.py:296] 12) loss = 27.887, grad_norm = 106.114
I0402 11:31:40.187585 140123308029696 logging_writer.py:48] [13] global_step=13, grad_norm=103.294853, loss=26.781668
I0402 11:31:40.190570 140165137565504 submission.py:296] 13) loss = 26.782, grad_norm = 103.295
I0402 11:31:40.994251 140124394440448 logging_writer.py:48] [14] global_step=14, grad_norm=108.299461, loss=25.835909
I0402 11:31:40.997396 140165137565504 submission.py:296] 14) loss = 25.836, grad_norm = 108.299
I0402 11:31:41.798156 140123308029696 logging_writer.py:48] [15] global_step=15, grad_norm=102.663612, loss=24.177406
I0402 11:31:41.801346 140165137565504 submission.py:296] 15) loss = 24.177, grad_norm = 102.664
I0402 11:31:42.605528 140124394440448 logging_writer.py:48] [16] global_step=16, grad_norm=93.244392, loss=24.420008
I0402 11:31:42.608670 140165137565504 submission.py:296] 16) loss = 24.420, grad_norm = 93.244
I0402 11:31:43.414768 140123308029696 logging_writer.py:48] [17] global_step=17, grad_norm=100.200943, loss=23.172836
I0402 11:31:43.418461 140165137565504 submission.py:296] 17) loss = 23.173, grad_norm = 100.201
I0402 11:31:44.226625 140124394440448 logging_writer.py:48] [18] global_step=18, grad_norm=90.230301, loss=21.615808
I0402 11:31:44.229945 140165137565504 submission.py:296] 18) loss = 21.616, grad_norm = 90.230
I0402 11:31:45.036406 140123308029696 logging_writer.py:48] [19] global_step=19, grad_norm=92.065170, loss=20.761826
I0402 11:31:45.039484 140165137565504 submission.py:296] 19) loss = 20.762, grad_norm = 92.065
I0402 11:31:45.845754 140124394440448 logging_writer.py:48] [20] global_step=20, grad_norm=84.321991, loss=19.333426
I0402 11:31:45.848727 140165137565504 submission.py:296] 20) loss = 19.333, grad_norm = 84.322
I0402 11:31:46.654864 140123308029696 logging_writer.py:48] [21] global_step=21, grad_norm=92.744888, loss=18.106630
I0402 11:31:46.658590 140165137565504 submission.py:296] 21) loss = 18.107, grad_norm = 92.745
I0402 11:31:47.467764 140124394440448 logging_writer.py:48] [22] global_step=22, grad_norm=97.612015, loss=18.040989
I0402 11:31:47.471061 140165137565504 submission.py:296] 22) loss = 18.041, grad_norm = 97.612
I0402 11:31:48.278863 140123308029696 logging_writer.py:48] [23] global_step=23, grad_norm=87.641487, loss=16.964184
I0402 11:31:48.282232 140165137565504 submission.py:296] 23) loss = 16.964, grad_norm = 87.641
I0402 11:31:49.089083 140124394440448 logging_writer.py:48] [24] global_step=24, grad_norm=82.575569, loss=15.722905
I0402 11:31:49.093311 140165137565504 submission.py:296] 24) loss = 15.723, grad_norm = 82.576
I0402 11:31:49.894840 140123308029696 logging_writer.py:48] [25] global_step=25, grad_norm=59.758423, loss=15.678489
I0402 11:31:49.898043 140165137565504 submission.py:296] 25) loss = 15.678, grad_norm = 59.758
I0402 11:31:50.701669 140124394440448 logging_writer.py:48] [26] global_step=26, grad_norm=48.722294, loss=14.524514
I0402 11:31:50.704869 140165137565504 submission.py:296] 26) loss = 14.525, grad_norm = 48.722
I0402 11:31:51.508987 140123308029696 logging_writer.py:48] [27] global_step=27, grad_norm=26.506409, loss=14.470968
I0402 11:31:51.512805 140165137565504 submission.py:296] 27) loss = 14.471, grad_norm = 26.506
I0402 11:31:52.320458 140124394440448 logging_writer.py:48] [28] global_step=28, grad_norm=16.777048, loss=14.154973
I0402 11:31:52.324389 140165137565504 submission.py:296] 28) loss = 14.155, grad_norm = 16.777
I0402 11:31:53.130757 140123308029696 logging_writer.py:48] [29] global_step=29, grad_norm=10.108788, loss=13.676727
I0402 11:31:53.134511 140165137565504 submission.py:296] 29) loss = 13.677, grad_norm = 10.109
I0402 11:31:53.944869 140124394440448 logging_writer.py:48] [30] global_step=30, grad_norm=11.878938, loss=12.848910
I0402 11:31:53.948484 140165137565504 submission.py:296] 30) loss = 12.849, grad_norm = 11.879
I0402 11:31:54.753914 140123308029696 logging_writer.py:48] [31] global_step=31, grad_norm=22.066366, loss=13.295247
I0402 11:31:54.756940 140165137565504 submission.py:296] 31) loss = 13.295, grad_norm = 22.066
I0402 11:31:55.558616 140124394440448 logging_writer.py:48] [32] global_step=32, grad_norm=29.139629, loss=13.955901
I0402 11:31:55.561758 140165137565504 submission.py:296] 32) loss = 13.956, grad_norm = 29.140
I0402 11:31:56.370202 140123308029696 logging_writer.py:48] [33] global_step=33, grad_norm=36.245884, loss=15.017509
I0402 11:31:56.373970 140165137565504 submission.py:296] 33) loss = 15.018, grad_norm = 36.246
I0402 11:31:57.178673 140124394440448 logging_writer.py:48] [34] global_step=34, grad_norm=44.614452, loss=15.887154
I0402 11:31:57.182256 140165137565504 submission.py:296] 34) loss = 15.887, grad_norm = 44.614
I0402 11:31:57.988879 140123308029696 logging_writer.py:48] [35] global_step=35, grad_norm=43.149014, loss=15.465310
I0402 11:31:57.992606 140165137565504 submission.py:296] 35) loss = 15.465, grad_norm = 43.149
I0402 11:31:58.796267 140124394440448 logging_writer.py:48] [36] global_step=36, grad_norm=33.365768, loss=13.085023
I0402 11:31:58.799883 140165137565504 submission.py:296] 36) loss = 13.085, grad_norm = 33.366
I0402 11:31:59.602171 140123308029696 logging_writer.py:48] [37] global_step=37, grad_norm=37.999302, loss=13.950199
I0402 11:31:59.605282 140165137565504 submission.py:296] 37) loss = 13.950, grad_norm = 37.999
I0402 11:32:00.410549 140124394440448 logging_writer.py:48] [38] global_step=38, grad_norm=39.859821, loss=14.374316
I0402 11:32:00.413652 140165137565504 submission.py:296] 38) loss = 14.374, grad_norm = 39.860
I0402 11:32:01.225137 140123308029696 logging_writer.py:48] [39] global_step=39, grad_norm=46.682251, loss=15.720951
I0402 11:32:01.228295 140165137565504 submission.py:296] 39) loss = 15.721, grad_norm = 46.682
I0402 11:32:02.035176 140124394440448 logging_writer.py:48] [40] global_step=40, grad_norm=40.203121, loss=14.308915
I0402 11:32:02.038981 140165137565504 submission.py:296] 40) loss = 14.309, grad_norm = 40.203
I0402 11:32:02.845792 140123308029696 logging_writer.py:48] [41] global_step=41, grad_norm=44.400814, loss=14.552704
I0402 11:32:02.849629 140165137565504 submission.py:296] 41) loss = 14.553, grad_norm = 44.401
I0402 11:32:03.651520 140124394440448 logging_writer.py:48] [42] global_step=42, grad_norm=43.382927, loss=14.656865
I0402 11:32:03.655127 140165137565504 submission.py:296] 42) loss = 14.657, grad_norm = 43.383
I0402 11:32:04.456535 140123308029696 logging_writer.py:48] [43] global_step=43, grad_norm=42.357628, loss=14.271092
I0402 11:32:04.459510 140165137565504 submission.py:296] 43) loss = 14.271, grad_norm = 42.358
I0402 11:32:05.265534 140124394440448 logging_writer.py:48] [44] global_step=44, grad_norm=41.796139, loss=14.164700
I0402 11:32:05.269015 140165137565504 submission.py:296] 44) loss = 14.165, grad_norm = 41.796
I0402 11:32:06.072230 140123308029696 logging_writer.py:48] [45] global_step=45, grad_norm=33.168358, loss=12.928574
I0402 11:32:06.075410 140165137565504 submission.py:296] 45) loss = 12.929, grad_norm = 33.168
I0402 11:32:06.878602 140124394440448 logging_writer.py:48] [46] global_step=46, grad_norm=30.299721, loss=12.721675
I0402 11:32:06.882585 140165137565504 submission.py:296] 46) loss = 12.722, grad_norm = 30.300
I0402 11:32:07.690269 140123308029696 logging_writer.py:48] [47] global_step=47, grad_norm=31.811310, loss=12.783201
I0402 11:32:07.693898 140165137565504 submission.py:296] 47) loss = 12.783, grad_norm = 31.811
I0402 11:32:08.499844 140124394440448 logging_writer.py:48] [48] global_step=48, grad_norm=29.530378, loss=13.133392
I0402 11:32:08.503434 140165137565504 submission.py:296] 48) loss = 13.133, grad_norm = 29.530
I0402 11:32:09.310523 140123308029696 logging_writer.py:48] [49] global_step=49, grad_norm=35.332302, loss=13.768236
I0402 11:32:09.314053 140165137565504 submission.py:296] 49) loss = 13.768, grad_norm = 35.332
I0402 11:32:10.119266 140124394440448 logging_writer.py:48] [50] global_step=50, grad_norm=23.006575, loss=12.210557
I0402 11:32:10.122662 140165137565504 submission.py:296] 50) loss = 12.211, grad_norm = 23.007
I0402 11:32:10.932636 140123308029696 logging_writer.py:48] [51] global_step=51, grad_norm=18.973793, loss=11.139739
I0402 11:32:10.936491 140165137565504 submission.py:296] 51) loss = 11.140, grad_norm = 18.974
I0402 11:32:11.740916 140124394440448 logging_writer.py:48] [52] global_step=52, grad_norm=23.274925, loss=11.742168
I0402 11:32:11.744596 140165137565504 submission.py:296] 52) loss = 11.742, grad_norm = 23.275
I0402 11:32:12.547069 140123308029696 logging_writer.py:48] [53] global_step=53, grad_norm=27.503265, loss=11.806177
I0402 11:32:12.550677 140165137565504 submission.py:296] 53) loss = 11.806, grad_norm = 27.503
I0402 11:32:13.352330 140124394440448 logging_writer.py:48] [54] global_step=54, grad_norm=30.657961, loss=11.977839
I0402 11:32:13.355765 140165137565504 submission.py:296] 54) loss = 11.978, grad_norm = 30.658
I0402 11:32:14.159899 140123308029696 logging_writer.py:48] [55] global_step=55, grad_norm=34.458523, loss=11.496272
I0402 11:32:14.163102 140165137565504 submission.py:296] 55) loss = 11.496, grad_norm = 34.459
I0402 11:32:14.973861 140124394440448 logging_writer.py:48] [56] global_step=56, grad_norm=35.021019, loss=11.578055
I0402 11:32:14.977522 140165137565504 submission.py:296] 56) loss = 11.578, grad_norm = 35.021
I0402 11:32:15.788861 140123308029696 logging_writer.py:48] [57] global_step=57, grad_norm=35.004742, loss=11.337004
I0402 11:32:15.792526 140165137565504 submission.py:296] 57) loss = 11.337, grad_norm = 35.005
I0402 11:32:16.597404 140124394440448 logging_writer.py:48] [58] global_step=58, grad_norm=34.953560, loss=10.980773
I0402 11:32:16.601181 140165137565504 submission.py:296] 58) loss = 10.981, grad_norm = 34.954
I0402 11:32:17.407940 140123308029696 logging_writer.py:48] [59] global_step=59, grad_norm=35.514614, loss=10.752869
I0402 11:32:17.411169 140165137565504 submission.py:296] 59) loss = 10.753, grad_norm = 35.515
I0402 11:32:18.212970 140124394440448 logging_writer.py:48] [60] global_step=60, grad_norm=32.827248, loss=10.674976
I0402 11:32:18.216354 140165137565504 submission.py:296] 60) loss = 10.675, grad_norm = 32.827
I0402 11:32:19.023846 140123308029696 logging_writer.py:48] [61] global_step=61, grad_norm=28.648867, loss=10.364728
I0402 11:32:19.027218 140165137565504 submission.py:296] 61) loss = 10.365, grad_norm = 28.649
I0402 11:32:19.833669 140124394440448 logging_writer.py:48] [62] global_step=62, grad_norm=25.244110, loss=10.049599
I0402 11:32:19.836828 140165137565504 submission.py:296] 62) loss = 10.050, grad_norm = 25.244
I0402 11:32:20.644466 140123308029696 logging_writer.py:48] [63] global_step=63, grad_norm=23.305490, loss=9.709703
I0402 11:32:20.647957 140165137565504 submission.py:296] 63) loss = 9.710, grad_norm = 23.305
I0402 11:32:21.453118 140124394440448 logging_writer.py:48] [64] global_step=64, grad_norm=20.864458, loss=9.332998
I0402 11:32:21.456399 140165137565504 submission.py:296] 64) loss = 9.333, grad_norm = 20.864
I0402 11:32:22.264920 140123308029696 logging_writer.py:48] [65] global_step=65, grad_norm=36.747494, loss=9.153463
I0402 11:32:22.268588 140165137565504 submission.py:296] 65) loss = 9.153, grad_norm = 36.747
I0402 11:32:23.076011 140124394440448 logging_writer.py:48] [66] global_step=66, grad_norm=52.717335, loss=8.577832
I0402 11:32:23.079396 140165137565504 submission.py:296] 66) loss = 8.578, grad_norm = 52.717
I0402 11:32:23.890042 140123308029696 logging_writer.py:48] [67] global_step=67, grad_norm=45.237648, loss=7.907520
I0402 11:32:23.893516 140165137565504 submission.py:296] 67) loss = 7.908, grad_norm = 45.238
I0402 11:32:24.701634 140124394440448 logging_writer.py:48] [68] global_step=68, grad_norm=25.901423, loss=7.458278
I0402 11:32:24.705107 140165137565504 submission.py:296] 68) loss = 7.458, grad_norm = 25.901
I0402 11:32:25.512833 140123308029696 logging_writer.py:48] [69] global_step=69, grad_norm=14.029053, loss=7.275425
I0402 11:32:25.517079 140165137565504 submission.py:296] 69) loss = 7.275, grad_norm = 14.029
I0402 11:32:26.322035 140124394440448 logging_writer.py:48] [70] global_step=70, grad_norm=10.734011, loss=7.208690
I0402 11:32:26.326009 140165137565504 submission.py:296] 70) loss = 7.209, grad_norm = 10.734
I0402 11:32:27.131525 140123308029696 logging_writer.py:48] [71] global_step=71, grad_norm=14.104299, loss=7.299759
I0402 11:32:27.134804 140165137565504 submission.py:296] 71) loss = 7.300, grad_norm = 14.104
I0402 11:32:27.940806 140124394440448 logging_writer.py:48] [72] global_step=72, grad_norm=17.722385, loss=7.422917
I0402 11:32:27.944302 140165137565504 submission.py:296] 72) loss = 7.423, grad_norm = 17.722
I0402 11:32:28.748172 140123308029696 logging_writer.py:48] [73] global_step=73, grad_norm=19.915550, loss=7.543158
I0402 11:32:28.751450 140165137565504 submission.py:296] 73) loss = 7.543, grad_norm = 19.916
I0402 11:32:29.563964 140124394440448 logging_writer.py:48] [74] global_step=74, grad_norm=21.455851, loss=7.691832
I0402 11:32:29.567392 140165137565504 submission.py:296] 74) loss = 7.692, grad_norm = 21.456
I0402 11:32:30.375276 140123308029696 logging_writer.py:48] [75] global_step=75, grad_norm=22.269329, loss=7.757428
I0402 11:32:30.378366 140165137565504 submission.py:296] 75) loss = 7.757, grad_norm = 22.269
I0402 11:32:31.186532 140124394440448 logging_writer.py:48] [76] global_step=76, grad_norm=22.917343, loss=7.816939
I0402 11:32:31.189546 140165137565504 submission.py:296] 76) loss = 7.817, grad_norm = 22.917
I0402 11:32:31.994822 140123308029696 logging_writer.py:48] [77] global_step=77, grad_norm=23.190149, loss=7.869162
I0402 11:32:31.997951 140165137565504 submission.py:296] 77) loss = 7.869, grad_norm = 23.190
I0402 11:32:32.803068 140124394440448 logging_writer.py:48] [78] global_step=78, grad_norm=23.398411, loss=7.891057
I0402 11:32:32.806545 140165137565504 submission.py:296] 78) loss = 7.891, grad_norm = 23.398
I0402 11:32:33.613547 140123308029696 logging_writer.py:48] [79] global_step=79, grad_norm=23.339031, loss=7.873796
I0402 11:32:33.616939 140165137565504 submission.py:296] 79) loss = 7.874, grad_norm = 23.339
I0402 11:32:34.424288 140124394440448 logging_writer.py:48] [80] global_step=80, grad_norm=23.146997, loss=7.814253
I0402 11:32:34.427915 140165137565504 submission.py:296] 80) loss = 7.814, grad_norm = 23.147
I0402 11:32:35.232607 140123308029696 logging_writer.py:48] [81] global_step=81, grad_norm=22.911968, loss=7.755388
I0402 11:32:35.236257 140165137565504 submission.py:296] 81) loss = 7.755, grad_norm = 22.912
I0402 11:32:36.042025 140124394440448 logging_writer.py:48] [82] global_step=82, grad_norm=22.542204, loss=7.678403
I0402 11:32:36.045039 140165137565504 submission.py:296] 82) loss = 7.678, grad_norm = 22.542
I0402 11:32:36.850765 140123308029696 logging_writer.py:48] [83] global_step=83, grad_norm=21.931759, loss=7.591803
I0402 11:32:36.854548 140165137565504 submission.py:296] 83) loss = 7.592, grad_norm = 21.932
I0402 11:32:37.664305 140124394440448 logging_writer.py:48] [84] global_step=84, grad_norm=21.286016, loss=7.474670
I0402 11:32:37.667738 140165137565504 submission.py:296] 84) loss = 7.475, grad_norm = 21.286
I0402 11:32:38.478360 140123308029696 logging_writer.py:48] [85] global_step=85, grad_norm=20.207134, loss=7.332565
I0402 11:32:38.482134 140165137565504 submission.py:296] 85) loss = 7.333, grad_norm = 20.207
I0402 11:32:39.288638 140124394440448 logging_writer.py:48] [86] global_step=86, grad_norm=19.132948, loss=7.202356
I0402 11:32:39.292402 140165137565504 submission.py:296] 86) loss = 7.202, grad_norm = 19.133
I0402 11:32:40.096097 140123308029696 logging_writer.py:48] [87] global_step=87, grad_norm=17.624454, loss=7.074702
I0402 11:32:40.099337 140165137565504 submission.py:296] 87) loss = 7.075, grad_norm = 17.624
I0402 11:32:40.904966 140124394440448 logging_writer.py:48] [88] global_step=88, grad_norm=15.341397, loss=6.939355
I0402 11:32:40.908709 140165137565504 submission.py:296] 88) loss = 6.939, grad_norm = 15.341
I0402 11:32:41.714190 140123308029696 logging_writer.py:48] [89] global_step=89, grad_norm=12.668217, loss=6.834737
I0402 11:32:41.717601 140165137565504 submission.py:296] 89) loss = 6.835, grad_norm = 12.668
I0402 11:32:42.524390 140124394440448 logging_writer.py:48] [90] global_step=90, grad_norm=9.925832, loss=6.732610
I0402 11:32:42.527523 140165137565504 submission.py:296] 90) loss = 6.733, grad_norm = 9.926
I0402 11:32:43.336108 140123308029696 logging_writer.py:48] [91] global_step=91, grad_norm=5.759287, loss=6.642218
I0402 11:32:43.339354 140165137565504 submission.py:296] 91) loss = 6.642, grad_norm = 5.759
I0402 11:32:44.146810 140124394440448 logging_writer.py:48] [92] global_step=92, grad_norm=2.365348, loss=6.614534
I0402 11:32:44.150314 140165137565504 submission.py:296] 92) loss = 6.615, grad_norm = 2.365
I0402 11:32:44.959445 140123308029696 logging_writer.py:48] [93] global_step=93, grad_norm=4.027008, loss=6.607686
I0402 11:32:44.962884 140165137565504 submission.py:296] 93) loss = 6.608, grad_norm = 4.027
I0402 11:32:45.765812 140124394440448 logging_writer.py:48] [94] global_step=94, grad_norm=8.217860, loss=6.634116
I0402 11:32:45.769646 140165137565504 submission.py:296] 94) loss = 6.634, grad_norm = 8.218
I0402 11:32:46.576321 140123308029696 logging_writer.py:48] [95] global_step=95, grad_norm=12.802371, loss=6.687404
I0402 11:32:46.579507 140165137565504 submission.py:296] 95) loss = 6.687, grad_norm = 12.802
I0402 11:32:47.387032 140124394440448 logging_writer.py:48] [96] global_step=96, grad_norm=17.434526, loss=6.771759
I0402 11:32:47.390381 140165137565504 submission.py:296] 96) loss = 6.772, grad_norm = 17.435
I0402 11:32:48.196035 140123308029696 logging_writer.py:48] [97] global_step=97, grad_norm=19.675200, loss=6.804869
I0402 11:32:48.199695 140165137565504 submission.py:296] 97) loss = 6.805, grad_norm = 19.675
I0402 11:32:49.003209 140124394440448 logging_writer.py:48] [98] global_step=98, grad_norm=22.119572, loss=6.846991
I0402 11:32:49.006616 140165137565504 submission.py:296] 98) loss = 6.847, grad_norm = 22.120
I0402 11:32:49.811985 140123308029696 logging_writer.py:48] [99] global_step=99, grad_norm=23.128473, loss=6.865450
I0402 11:32:49.815603 140165137565504 submission.py:296] 99) loss = 6.865, grad_norm = 23.128
I0402 11:32:50.621287 140124394440448 logging_writer.py:48] [100] global_step=100, grad_norm=21.928120, loss=6.824143
I0402 11:32:50.624883 140165137565504 submission.py:296] 100) loss = 6.824, grad_norm = 21.928
I0402 11:38:09.544516 140123308029696 logging_writer.py:48] [500] global_step=500, grad_norm=0.374961, loss=5.803257
I0402 11:38:09.549492 140165137565504 submission.py:296] 500) loss = 5.803, grad_norm = 0.375
I0402 11:44:48.522551 140124394440448 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.388954, loss=5.682059
I0402 11:44:48.527043 140165137565504 submission.py:296] 1000) loss = 5.682, grad_norm = 3.389
I0402 11:51:29.445838 140130622895872 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.283328, loss=3.916384
I0402 11:51:29.453036 140165137565504 submission.py:296] 1500) loss = 3.916, grad_norm = 1.283
I0402 11:58:08.499342 140130614503168 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.228261, loss=3.046168
I0402 11:58:08.503401 140165137565504 submission.py:296] 2000) loss = 3.046, grad_norm = 1.228
I0402 12:04:48.757423 140130622895872 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.066254, loss=2.765598
I0402 12:04:48.764349 140165137565504 submission.py:296] 2500) loss = 2.766, grad_norm = 1.066
I0402 12:11:26.687826 140130614503168 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.222388, loss=2.527291
I0402 12:11:26.692731 140165137565504 submission.py:296] 3000) loss = 2.527, grad_norm = 1.222
I0402 12:11:29.877456 140165137565504 submission_runner.py:373] Before eval at step 3005: RAM USED (GB) 40.493101056
I0402 12:11:29.877687 140165137565504 spec.py:298] Evaluating on the training split.
I0402 12:11:40.349187 140165137565504 spec.py:310] Evaluating on the validation split.
I0402 12:11:50.052771 140165137565504 spec.py:326] Evaluating on the test split.
I0402 12:11:55.399833 140165137565504 submission_runner.py:382] Time since start: 2435.18s, 	Step: 3005, 	{'train/ctc_loss': 3.4842019361982386, 'train/wer': 0.7168851730158213, 'validation/ctc_loss': 3.6707310660287322, 'validation/wer': 0.7227924491865012, 'validation/num_examples': 5348, 'test/ctc_loss': 3.3814338828311503, 'test/wer': 0.6691446793817155, 'test/num_examples': 2472}
I0402 12:11:55.400629 140165137565504 submission_runner.py:396] After eval at step 3005: RAM USED (GB) 39.629750272
I0402 12:11:55.415546 140130614503168 logging_writer.py:48] [3005] global_step=3005, preemption_count=0, score=2403.450325, test/ctc_loss=3.381434, test/num_examples=2472, test/wer=0.669145, total_duration=2435.180216, train/ctc_loss=3.484202, train/wer=0.716885, validation/ctc_loss=3.670731, validation/num_examples=5348, validation/wer=0.722792
I0402 12:11:55.998096 140165137565504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_3005.
I0402 12:11:55.998782 140165137565504 submission_runner.py:416] After logging and checkpointing eval at step 3005: RAM USED (GB) 39.63656192
I0402 12:18:32.473475 140130614503168 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.007866, loss=2.364491
I0402 12:18:32.480613 140165137565504 submission.py:296] 3500) loss = 2.364, grad_norm = 1.008
I0402 12:25:10.059589 140130606110464 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.958519, loss=2.201917
I0402 12:25:10.063851 140165137565504 submission.py:296] 4000) loss = 2.202, grad_norm = 0.959
I0402 12:31:49.379469 140130614503168 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.850779, loss=2.080748
I0402 12:31:49.394996 140165137565504 submission.py:296] 4500) loss = 2.081, grad_norm = 0.851
I0402 12:38:26.503665 140130606110464 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.127897, loss=2.074638
I0402 12:38:26.507869 140165137565504 submission.py:296] 5000) loss = 2.075, grad_norm = 1.128
I0402 12:45:05.230737 140130614503168 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.683583, loss=2.049487
I0402 12:45:05.241868 140165137565504 submission.py:296] 5500) loss = 2.049, grad_norm = 0.684
I0402 12:51:42.312043 140130606110464 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.608743, loss=1.899258
I0402 12:51:42.316756 140165137565504 submission.py:296] 6000) loss = 1.899, grad_norm = 0.609
I0402 12:51:56.604377 140165137565504 submission_runner.py:373] Before eval at step 6019: RAM USED (GB) 39.721086976
I0402 12:51:56.604600 140165137565504 spec.py:298] Evaluating on the training split.
I0402 12:52:07.787062 140165137565504 spec.py:310] Evaluating on the validation split.
I0402 12:52:18.038524 140165137565504 spec.py:326] Evaluating on the test split.
I0402 12:52:23.606046 140165137565504 submission_runner.py:382] Time since start: 4861.90s, 	Step: 6019, 	{'train/ctc_loss': 0.7325238892594838, 'train/wer': 0.2428402572193847, 'validation/ctc_loss': 0.9614898454766928, 'validation/wer': 0.2825954714430551, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6679809483676367, 'test/wer': 0.22040095058192677, 'test/num_examples': 2472}
I0402 12:52:23.606860 140165137565504 submission_runner.py:396] After eval at step 6019: RAM USED (GB) 39.525793792
I0402 12:52:23.624028 140130606110464 logging_writer.py:48] [6019] global_step=6019, preemption_count=0, score=4800.106432, test/ctc_loss=0.667981, test/num_examples=2472, test/wer=0.220401, total_duration=4861.904756, train/ctc_loss=0.732524, train/wer=0.242840, validation/ctc_loss=0.961490, validation/num_examples=5348, validation/wer=0.282595
I0402 12:52:24.223988 140165137565504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_6019.
I0402 12:52:24.224626 140165137565504 submission_runner.py:416] After logging and checkpointing eval at step 6019: RAM USED (GB) 39.532023808
I0402 12:58:49.058813 140130606110464 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.550205, loss=1.922981
I0402 12:58:49.065126 140165137565504 submission.py:296] 6500) loss = 1.923, grad_norm = 0.550
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 13:05:25.995218 140130597717760 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.674450, loss=1.800071
I0402 13:05:26.000003 140165137565504 submission.py:296] 7000) loss = 1.800, grad_norm = 0.674
I0402 13:12:04.938638 140130606110464 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.804701, loss=1.805084
I0402 13:12:04.944580 140165137565504 submission.py:296] 7500) loss = 1.805, grad_norm = 0.805
I0402 13:18:41.949867 140130597717760 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.792179, loss=1.684679
I0402 13:18:41.954929 140165137565504 submission.py:296] 8000) loss = 1.685, grad_norm = 0.792
I0402 13:25:20.868735 140130606110464 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.496834, loss=1.678464
I0402 13:25:20.875573 140165137565504 submission.py:296] 8500) loss = 1.678, grad_norm = 0.497
I0402 13:31:57.774644 140130597717760 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.034562, loss=1.727329
I0402 13:31:57.779149 140165137565504 submission.py:296] 9000) loss = 1.727, grad_norm = 2.035
I0402 13:32:24.787508 140165137565504 submission_runner.py:373] Before eval at step 9035: RAM USED (GB) 39.581282304
I0402 13:32:24.787747 140165137565504 spec.py:298] Evaluating on the training split.
I0402 13:32:35.946295 140165137565504 spec.py:310] Evaluating on the validation split.
I0402 13:32:45.831666 140165137565504 spec.py:326] Evaluating on the test split.
I0402 13:32:51.286578 140165137565504 submission_runner.py:382] Time since start: 7290.09s, 	Step: 9035, 	{'train/ctc_loss': 0.5523231500638288, 'train/wer': 0.18954205840571695, 'validation/ctc_loss': 0.7869563272302592, 'validation/wer': 0.2353015014725052, 'validation/num_examples': 5348, 'test/ctc_loss': 0.511620558181173, 'test/wer': 0.17094225417910752, 'test/num_examples': 2472}
I0402 13:32:51.287324 140165137565504 submission_runner.py:396] After eval at step 9035: RAM USED (GB) 39.475150848
I0402 13:32:51.304007 140130597717760 logging_writer.py:48] [9035] global_step=9035, preemption_count=0, score=7196.759858, test/ctc_loss=0.511621, test/num_examples=2472, test/wer=0.170942, total_duration=7290.087879, train/ctc_loss=0.552323, train/wer=0.189542, validation/ctc_loss=0.786956, validation/num_examples=5348, validation/wer=0.235302
I0402 13:32:51.882933 140165137565504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_9035.
I0402 13:32:51.883634 140165137565504 submission_runner.py:416] After logging and checkpointing eval at step 9035: RAM USED (GB) 39.481868288
I0402 13:39:03.654219 140130597717760 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.416233, loss=1.671907
I0402 13:39:03.660628 140165137565504 submission.py:296] 9500) loss = 1.672, grad_norm = 0.416
I0402 13:45:39.946215 140165137565504 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 39.565357056
I0402 13:45:39.946507 140165137565504 spec.py:298] Evaluating on the training split.
I0402 13:45:50.994211 140165137565504 spec.py:310] Evaluating on the validation split.
I0402 13:46:00.900770 140165137565504 spec.py:326] Evaluating on the test split.
I0402 13:46:06.449069 140165137565504 submission_runner.py:382] Time since start: 8085.25s, 	Step: 10000, 	{'train/ctc_loss': 0.48266727137589266, 'train/wer': 0.17009878872659054, 'validation/ctc_loss': 0.7228206782449267, 'validation/wer': 0.21871288562738378, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45715000766178465, 'test/wer': 0.1561960473666037, 'test/num_examples': 2472}
I0402 13:46:06.449809 140165137565504 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 39.454941184
I0402 13:46:06.466914 140130589325056 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7963.568863, test/ctc_loss=0.457150, test/num_examples=2472, test/wer=0.156196, total_duration=8085.248637, train/ctc_loss=0.482667, train/wer=0.170099, validation/ctc_loss=0.722821, validation/num_examples=5348, validation/wer=0.218713
I0402 13:46:07.046553 140165137565504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0402 13:46:07.047195 140165137565504 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 39.460859904
I0402 13:46:07.058082 140130580932352 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7963.568863
I0402 13:46:08.188307 140165137565504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0402 13:46:08.319992 140165137565504 submission_runner.py:550] Tuning trial 1/1
I0402 13:46:08.320211 140165137565504 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 13:46:08.320603 140165137565504 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 30.8289556110131, 'train/wer': 0.9897307437891061, 'validation/ctc_loss': 29.4308883363472, 'validation/wer': 0.9130787428185198, 'validation/num_examples': 5348, 'test/ctc_loss': 29.452219498032132, 'test/wer': 0.9283001239006358, 'test/num_examples': 2472, 'score': 6.591480493545532, 'total_duration': 6.593562364578247, 'global_step': 1, 'preemption_count': 0}), (3005, {'train/ctc_loss': 3.4842019361982386, 'train/wer': 0.7168851730158213, 'validation/ctc_loss': 3.6707310660287322, 'validation/wer': 0.7227924491865012, 'validation/num_examples': 5348, 'test/ctc_loss': 3.3814338828311503, 'test/wer': 0.6691446793817155, 'test/num_examples': 2472, 'score': 2403.450325012207, 'total_duration': 2435.1802163124084, 'global_step': 3005, 'preemption_count': 0}), (6019, {'train/ctc_loss': 0.7325238892594838, 'train/wer': 0.2428402572193847, 'validation/ctc_loss': 0.9614898454766928, 'validation/wer': 0.2825954714430551, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6679809483676367, 'test/wer': 0.22040095058192677, 'test/num_examples': 2472, 'score': 4800.106431722641, 'total_duration': 4861.904756069183, 'global_step': 6019, 'preemption_count': 0}), (9035, {'train/ctc_loss': 0.5523231500638288, 'train/wer': 0.18954205840571695, 'validation/ctc_loss': 0.7869563272302592, 'validation/wer': 0.2353015014725052, 'validation/num_examples': 5348, 'test/ctc_loss': 0.511620558181173, 'test/wer': 0.17094225417910752, 'test/num_examples': 2472, 'score': 7196.7598576545715, 'total_duration': 7290.087879419327, 'global_step': 9035, 'preemption_count': 0}), (10000, {'train/ctc_loss': 0.48266727137589266, 'train/wer': 0.17009878872659054, 'validation/ctc_loss': 0.7228206782449267, 'validation/wer': 0.21871288562738378, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45715000766178465, 'test/wer': 0.1561960473666037, 'test/num_examples': 2472, 'score': 7963.568862915039, 'total_duration': 8085.24863743782, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0402 13:46:08.320683 140165137565504 submission_runner.py:553] Timing: 7963.568862915039
I0402 13:46:08.320728 140165137565504 submission_runner.py:554] ====================
I0402 13:46:08.320879 140165137565504 submission_runner.py:613] Final librispeech_conformer score: 7963.568862915039
