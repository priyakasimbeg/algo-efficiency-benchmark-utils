WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 06:06:22.702644 139682584803136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 06:06:22.702678 139979417651008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 06:06:22.702707 140485574223680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 06:06:22.703382 140635581605696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 06:06:22.703708 140635581605696 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.703593 139773604169536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 06:06:22.703611 140446820050752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 06:06:22.703637 139985874179904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 06:06:22.703666 140196286560064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 06:06:22.703911 139773604169536 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.703935 139985874179904 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.703991 140446820050752 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.704020 140196286560064 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.713464 139682584803136 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.713488 139979417651008 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:22.713517 140485574223680 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:06:23.264197 140196286560064 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/fastmri_pytorch.
W0402 06:06:23.303091 140196286560064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.303081 139682584803136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.303121 139773604169536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.303536 139979417651008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.303576 139985874179904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.303690 140485574223680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.304844 140635581605696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:06:23.304893 140446820050752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 06:06:23.311367 140196286560064 submission_runner.py:511] Using RNG seed 3405769858
I0402 06:06:23.312395 140196286560064 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 06:06:23.312541 140196286560064 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1.
I0402 06:06:23.312757 140196286560064 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/hparams.json.
I0402 06:06:23.313699 140196286560064 submission_runner.py:230] Starting train once: RAM USED (GB) 5.780189184
I0402 06:06:23.313801 140196286560064 submission_runner.py:231] Initializing dataset.
I0402 06:06:23.313982 140196286560064 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.780189184
I0402 06:06:23.314053 140196286560064 submission_runner.py:240] Initializing model.
I0402 06:06:27.453179 140196286560064 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.416967168
I0402 06:06:27.453355 140196286560064 submission_runner.py:252] Initializing optimizer.
I0402 06:06:27.454096 140196286560064 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.416967168
I0402 06:06:27.454206 140196286560064 submission_runner.py:261] Initializing metrics bundle.
I0402 06:06:27.454257 140196286560064 submission_runner.py:276] Initializing checkpoint and logger.
I0402 06:06:27.457467 140196286560064 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 06:06:27.457564 140196286560064 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 06:06:28.035555 140196286560064 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0402 06:06:28.036389 140196286560064 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/flags_0.json.
I0402 06:06:28.079241 140196286560064 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.467577344
I0402 06:06:28.080386 140196286560064 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.467577344
I0402 06:06:28.080513 140196286560064 submission_runner.py:313] Starting training loop.
I0402 06:07:12.854336 140196286560064 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 24.044539904
I0402 06:07:16.830008 140154010334976 logging_writer.py:48] [0] global_step=0, grad_norm=4.829504, loss=1.076770
I0402 06:07:16.838330 140196286560064 submission.py:296] 0) loss = 1.077, grad_norm = 4.830
I0402 06:07:16.840375 140196286560064 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.690061312
I0402 06:07:16.875159 140196286560064 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.698683392
I0402 06:07:16.875315 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:08:57.692211 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:10:00.232601 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:11:03.592365 140196286560064 submission_runner.py:382] Time since start: 48.79s, 	Step: 1, 	{'train/ssim': 0.17093840667179652, 'train/loss': 1.0958151136125838, 'validation/ssim': 0.16222392325328855, 'validation/loss': 1.0968789018535452, 'validation/num_examples': 3554, 'test/ssim': 0.18376277698814053, 'test/loss': 1.0957503032497906, 'test/num_examples': 3581}
I0402 06:11:03.592756 140196286560064 submission_runner.py:396] After eval at step 1: RAM USED (GB) 69.02882304
I0402 06:11:03.602034 140130346067712 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=48.792915, test/loss=1.095750, test/num_examples=3581, test/ssim=0.183763, total_duration=48.794952, train/loss=1.095815, train/ssim=0.170938, validation/loss=1.096879, validation/num_examples=3554, validation/ssim=0.162224
I0402 06:11:03.750524 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1.
I0402 06:11:03.751053 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 69.029507072
I0402 06:11:03.761734 140196286560064 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 69.057343488
I0402 06:11:03.764420 140196286560064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764435 140446820050752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764433 140485574223680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764446 139682584803136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764439 139985874179904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764446 139773604169536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764439 139979417651008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.764600 140635581605696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 06:11:03.826186 140130337675008 logging_writer.py:48] [1] global_step=1, grad_norm=4.874581, loss=1.091791
I0402 06:11:03.830265 140196286560064 submission.py:296] 1) loss = 1.092, grad_norm = 4.875
I0402 06:11:03.831036 140196286560064 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 69.05890816
I0402 06:11:03.903174 140130346067712 logging_writer.py:48] [2] global_step=2, grad_norm=4.971717, loss=1.104894
I0402 06:11:03.907616 140196286560064 submission.py:296] 2) loss = 1.105, grad_norm = 4.972
I0402 06:11:03.981448 140130337675008 logging_writer.py:48] [3] global_step=3, grad_norm=4.534154, loss=1.070406
I0402 06:11:03.984624 140196286560064 submission.py:296] 3) loss = 1.070, grad_norm = 4.534
I0402 06:11:04.048325 140130346067712 logging_writer.py:48] [4] global_step=4, grad_norm=4.892568, loss=1.067383
I0402 06:11:04.051865 140196286560064 submission.py:296] 4) loss = 1.067, grad_norm = 4.893
I0402 06:11:04.117564 140130337675008 logging_writer.py:48] [5] global_step=5, grad_norm=4.196437, loss=1.079958
I0402 06:11:04.124972 140196286560064 submission.py:296] 5) loss = 1.080, grad_norm = 4.196
I0402 06:11:04.195974 140130346067712 logging_writer.py:48] [6] global_step=6, grad_norm=4.876697, loss=1.031581
I0402 06:11:04.201213 140196286560064 submission.py:296] 6) loss = 1.032, grad_norm = 4.877
I0402 06:11:04.271901 140130337675008 logging_writer.py:48] [7] global_step=7, grad_norm=4.390243, loss=1.016424
I0402 06:11:04.276978 140196286560064 submission.py:296] 7) loss = 1.016, grad_norm = 4.390
I0402 06:11:04.345923 140130346067712 logging_writer.py:48] [8] global_step=8, grad_norm=4.350387, loss=1.029928
I0402 06:11:04.351323 140196286560064 submission.py:296] 8) loss = 1.030, grad_norm = 4.350
I0402 06:11:04.423216 140130337675008 logging_writer.py:48] [9] global_step=9, grad_norm=4.554586, loss=1.028273
I0402 06:11:04.428694 140196286560064 submission.py:296] 9) loss = 1.028, grad_norm = 4.555
I0402 06:11:04.506187 140130346067712 logging_writer.py:48] [10] global_step=10, grad_norm=4.512874, loss=1.019819
I0402 06:11:04.511433 140196286560064 submission.py:296] 10) loss = 1.020, grad_norm = 4.513
I0402 06:11:04.579990 140130337675008 logging_writer.py:48] [11] global_step=11, grad_norm=4.447545, loss=0.993077
I0402 06:11:04.583866 140196286560064 submission.py:296] 11) loss = 0.993, grad_norm = 4.448
I0402 06:11:04.653878 140130346067712 logging_writer.py:48] [12] global_step=12, grad_norm=4.635255, loss=0.969605
I0402 06:11:04.658891 140196286560064 submission.py:296] 12) loss = 0.970, grad_norm = 4.635
I0402 06:11:04.735488 140130337675008 logging_writer.py:48] [13] global_step=13, grad_norm=4.654193, loss=0.979940
I0402 06:11:04.740940 140196286560064 submission.py:296] 13) loss = 0.980, grad_norm = 4.654
I0402 06:11:04.852894 140130346067712 logging_writer.py:48] [14] global_step=14, grad_norm=4.070764, loss=0.965242
I0402 06:11:04.856393 140196286560064 submission.py:296] 14) loss = 0.965, grad_norm = 4.071
I0402 06:11:05.079433 140130337675008 logging_writer.py:48] [15] global_step=15, grad_norm=4.051898, loss=0.977765
I0402 06:11:05.084708 140196286560064 submission.py:296] 15) loss = 0.978, grad_norm = 4.052
I0402 06:11:05.364738 140130346067712 logging_writer.py:48] [16] global_step=16, grad_norm=3.977213, loss=0.928473
I0402 06:11:05.368396 140196286560064 submission.py:296] 16) loss = 0.928, grad_norm = 3.977
I0402 06:11:05.589308 140130337675008 logging_writer.py:48] [17] global_step=17, grad_norm=4.338301, loss=0.909284
I0402 06:11:05.595816 140196286560064 submission.py:296] 17) loss = 0.909, grad_norm = 4.338
I0402 06:11:05.894595 140130346067712 logging_writer.py:48] [18] global_step=18, grad_norm=3.641633, loss=0.962529
I0402 06:11:05.897695 140196286560064 submission.py:296] 18) loss = 0.963, grad_norm = 3.642
I0402 06:11:06.089999 140130337675008 logging_writer.py:48] [19] global_step=19, grad_norm=3.785646, loss=0.843756
I0402 06:11:06.095089 140196286560064 submission.py:296] 19) loss = 0.844, grad_norm = 3.786
I0402 06:11:06.390931 140130346067712 logging_writer.py:48] [20] global_step=20, grad_norm=3.544511, loss=0.884733
I0402 06:11:06.396059 140196286560064 submission.py:296] 20) loss = 0.885, grad_norm = 3.545
I0402 06:11:06.668625 140130337675008 logging_writer.py:48] [21] global_step=21, grad_norm=3.750283, loss=0.844452
I0402 06:11:06.674404 140196286560064 submission.py:296] 21) loss = 0.844, grad_norm = 3.750
I0402 06:11:06.941003 140130346067712 logging_writer.py:48] [22] global_step=22, grad_norm=3.040322, loss=0.789919
I0402 06:11:06.948415 140196286560064 submission.py:296] 22) loss = 0.790, grad_norm = 3.040
I0402 06:11:07.176478 140130337675008 logging_writer.py:48] [23] global_step=23, grad_norm=2.869420, loss=0.786502
I0402 06:11:07.183178 140196286560064 submission.py:296] 23) loss = 0.787, grad_norm = 2.869
I0402 06:11:07.466585 140130346067712 logging_writer.py:48] [24] global_step=24, grad_norm=2.751235, loss=0.760626
I0402 06:11:07.472775 140196286560064 submission.py:296] 24) loss = 0.761, grad_norm = 2.751
I0402 06:11:07.716594 140130337675008 logging_writer.py:48] [25] global_step=25, grad_norm=2.604330, loss=0.757758
I0402 06:11:07.722313 140196286560064 submission.py:296] 25) loss = 0.758, grad_norm = 2.604
I0402 06:11:07.943516 140130346067712 logging_writer.py:48] [26] global_step=26, grad_norm=2.768638, loss=0.766710
I0402 06:11:07.947452 140196286560064 submission.py:296] 26) loss = 0.767, grad_norm = 2.769
I0402 06:11:08.275975 140130337675008 logging_writer.py:48] [27] global_step=27, grad_norm=2.376779, loss=0.718350
I0402 06:11:08.281778 140196286560064 submission.py:296] 27) loss = 0.718, grad_norm = 2.377
I0402 06:11:08.563793 140130346067712 logging_writer.py:48] [28] global_step=28, grad_norm=2.258632, loss=0.680476
I0402 06:11:08.568809 140196286560064 submission.py:296] 28) loss = 0.680, grad_norm = 2.259
I0402 06:11:08.772920 140130337675008 logging_writer.py:48] [29] global_step=29, grad_norm=2.020742, loss=0.696357
I0402 06:11:08.776175 140196286560064 submission.py:296] 29) loss = 0.696, grad_norm = 2.021
I0402 06:11:09.047465 140130346067712 logging_writer.py:48] [30] global_step=30, grad_norm=1.962557, loss=0.679688
I0402 06:11:09.050913 140196286560064 submission.py:296] 30) loss = 0.680, grad_norm = 1.963
I0402 06:11:09.274598 140130337675008 logging_writer.py:48] [31] global_step=31, grad_norm=1.559202, loss=0.683369
I0402 06:11:09.279214 140196286560064 submission.py:296] 31) loss = 0.683, grad_norm = 1.559
I0402 06:11:09.537937 140130346067712 logging_writer.py:48] [32] global_step=32, grad_norm=1.537516, loss=0.695209
I0402 06:11:09.541758 140196286560064 submission.py:296] 32) loss = 0.695, grad_norm = 1.538
I0402 06:11:09.813714 140130337675008 logging_writer.py:48] [33] global_step=33, grad_norm=1.382173, loss=0.692129
I0402 06:11:09.817220 140196286560064 submission.py:296] 33) loss = 0.692, grad_norm = 1.382
I0402 06:11:10.069941 140130346067712 logging_writer.py:48] [34] global_step=34, grad_norm=1.318770, loss=0.652035
I0402 06:11:10.075078 140196286560064 submission.py:296] 34) loss = 0.652, grad_norm = 1.319
I0402 06:11:10.317249 140130337675008 logging_writer.py:48] [35] global_step=35, grad_norm=1.253688, loss=0.682303
I0402 06:11:10.322282 140196286560064 submission.py:296] 35) loss = 0.682, grad_norm = 1.254
I0402 06:11:10.600931 140130346067712 logging_writer.py:48] [36] global_step=36, grad_norm=1.299149, loss=0.653830
I0402 06:11:10.605570 140196286560064 submission.py:296] 36) loss = 0.654, grad_norm = 1.299
I0402 06:11:10.844284 140130337675008 logging_writer.py:48] [37] global_step=37, grad_norm=1.382953, loss=0.639819
I0402 06:11:10.849458 140196286560064 submission.py:296] 37) loss = 0.640, grad_norm = 1.383
I0402 06:11:11.090705 140130346067712 logging_writer.py:48] [38] global_step=38, grad_norm=1.365118, loss=0.642494
I0402 06:11:11.095070 140196286560064 submission.py:296] 38) loss = 0.642, grad_norm = 1.365
I0402 06:11:11.384140 140130337675008 logging_writer.py:48] [39] global_step=39, grad_norm=1.632019, loss=0.572856
I0402 06:11:11.389462 140196286560064 submission.py:296] 39) loss = 0.573, grad_norm = 1.632
I0402 06:11:11.606527 140130346067712 logging_writer.py:48] [40] global_step=40, grad_norm=1.393813, loss=0.618401
I0402 06:11:11.611984 140196286560064 submission.py:296] 40) loss = 0.618, grad_norm = 1.394
I0402 06:11:11.883276 140130337675008 logging_writer.py:48] [41] global_step=41, grad_norm=1.582311, loss=0.597664
I0402 06:11:11.888587 140196286560064 submission.py:296] 41) loss = 0.598, grad_norm = 1.582
I0402 06:11:12.105933 140130346067712 logging_writer.py:48] [42] global_step=42, grad_norm=1.485197, loss=0.633575
I0402 06:11:12.109472 140196286560064 submission.py:296] 42) loss = 0.634, grad_norm = 1.485
I0402 06:11:12.372700 140130337675008 logging_writer.py:48] [43] global_step=43, grad_norm=1.542280, loss=0.646530
I0402 06:11:12.376052 140196286560064 submission.py:296] 43) loss = 0.647, grad_norm = 1.542
I0402 06:11:12.667981 140130346067712 logging_writer.py:48] [44] global_step=44, grad_norm=1.710358, loss=0.623962
I0402 06:11:12.671692 140196286560064 submission.py:296] 44) loss = 0.624, grad_norm = 1.710
I0402 06:11:12.966930 140130337675008 logging_writer.py:48] [45] global_step=45, grad_norm=1.540362, loss=0.626949
I0402 06:11:12.970554 140196286560064 submission.py:296] 45) loss = 0.627, grad_norm = 1.540
I0402 06:11:13.286767 140130346067712 logging_writer.py:48] [46] global_step=46, grad_norm=1.455607, loss=0.600720
I0402 06:11:13.290312 140196286560064 submission.py:296] 46) loss = 0.601, grad_norm = 1.456
I0402 06:11:13.518335 140130337675008 logging_writer.py:48] [47] global_step=47, grad_norm=1.485723, loss=0.618330
I0402 06:11:13.521735 140196286560064 submission.py:296] 47) loss = 0.618, grad_norm = 1.486
I0402 06:11:13.799082 140130346067712 logging_writer.py:48] [48] global_step=48, grad_norm=1.392172, loss=0.647217
I0402 06:11:13.803203 140196286560064 submission.py:296] 48) loss = 0.647, grad_norm = 1.392
I0402 06:11:14.051027 140130337675008 logging_writer.py:48] [49] global_step=49, grad_norm=1.360925, loss=0.646617
I0402 06:11:14.057439 140196286560064 submission.py:296] 49) loss = 0.647, grad_norm = 1.361
I0402 06:11:14.342904 140130346067712 logging_writer.py:48] [50] global_step=50, grad_norm=1.301267, loss=0.605304
I0402 06:11:14.346440 140196286560064 submission.py:296] 50) loss = 0.605, grad_norm = 1.301
I0402 06:11:14.632658 140130337675008 logging_writer.py:48] [51] global_step=51, grad_norm=1.382537, loss=0.545229
I0402 06:11:14.638839 140196286560064 submission.py:296] 51) loss = 0.545, grad_norm = 1.383
I0402 06:11:14.843226 140130346067712 logging_writer.py:48] [52] global_step=52, grad_norm=1.195334, loss=0.548120
I0402 06:11:14.850184 140196286560064 submission.py:296] 52) loss = 0.548, grad_norm = 1.195
I0402 06:11:15.149197 140130337675008 logging_writer.py:48] [53] global_step=53, grad_norm=1.245010, loss=0.574912
I0402 06:11:15.156457 140196286560064 submission.py:296] 53) loss = 0.575, grad_norm = 1.245
I0402 06:11:15.425230 140130346067712 logging_writer.py:48] [54] global_step=54, grad_norm=1.125041, loss=0.622541
I0402 06:11:15.429840 140196286560064 submission.py:296] 54) loss = 0.623, grad_norm = 1.125
I0402 06:11:15.673355 140130337675008 logging_writer.py:48] [55] global_step=55, grad_norm=1.122610, loss=0.526175
I0402 06:11:15.678634 140196286560064 submission.py:296] 55) loss = 0.526, grad_norm = 1.123
I0402 06:11:15.921175 140130346067712 logging_writer.py:48] [56] global_step=56, grad_norm=1.101401, loss=0.547339
I0402 06:11:15.926776 140196286560064 submission.py:296] 56) loss = 0.547, grad_norm = 1.101
I0402 06:11:16.184964 140130337675008 logging_writer.py:48] [57] global_step=57, grad_norm=0.995511, loss=0.562505
I0402 06:11:16.190418 140196286560064 submission.py:296] 57) loss = 0.563, grad_norm = 0.996
I0402 06:11:16.437122 140130346067712 logging_writer.py:48] [58] global_step=58, grad_norm=0.997424, loss=0.573719
I0402 06:11:16.442703 140196286560064 submission.py:296] 58) loss = 0.574, grad_norm = 0.997
I0402 06:11:16.764243 140130337675008 logging_writer.py:48] [59] global_step=59, grad_norm=0.971869, loss=0.525848
I0402 06:11:16.769002 140196286560064 submission.py:296] 59) loss = 0.526, grad_norm = 0.972
I0402 06:11:17.027141 140130346067712 logging_writer.py:48] [60] global_step=60, grad_norm=1.043539, loss=0.485327
I0402 06:11:17.032585 140196286560064 submission.py:296] 60) loss = 0.485, grad_norm = 1.044
I0402 06:11:17.304649 140130337675008 logging_writer.py:48] [61] global_step=61, grad_norm=0.865259, loss=0.514118
I0402 06:11:17.309922 140196286560064 submission.py:296] 61) loss = 0.514, grad_norm = 0.865
I0402 06:11:17.569360 140130346067712 logging_writer.py:48] [62] global_step=62, grad_norm=0.914487, loss=0.466077
I0402 06:11:17.574572 140196286560064 submission.py:296] 62) loss = 0.466, grad_norm = 0.914
I0402 06:11:17.838898 140130337675008 logging_writer.py:48] [63] global_step=63, grad_norm=1.042398, loss=0.455038
I0402 06:11:17.842770 140196286560064 submission.py:296] 63) loss = 0.455, grad_norm = 1.042
I0402 06:11:18.090451 140130346067712 logging_writer.py:48] [64] global_step=64, grad_norm=0.838574, loss=0.471444
I0402 06:11:18.094138 140196286560064 submission.py:296] 64) loss = 0.471, grad_norm = 0.839
I0402 06:11:18.353971 140130337675008 logging_writer.py:48] [65] global_step=65, grad_norm=0.904338, loss=0.578371
I0402 06:11:18.359331 140196286560064 submission.py:296] 65) loss = 0.578, grad_norm = 0.904
I0402 06:11:18.602984 140130346067712 logging_writer.py:48] [66] global_step=66, grad_norm=1.162292, loss=0.429876
I0402 06:11:18.608968 140196286560064 submission.py:296] 66) loss = 0.430, grad_norm = 1.162
I0402 06:11:18.869379 140130337675008 logging_writer.py:48] [67] global_step=67, grad_norm=1.016095, loss=0.544638
I0402 06:11:18.873838 140196286560064 submission.py:296] 67) loss = 0.545, grad_norm = 1.016
I0402 06:11:19.153134 140130346067712 logging_writer.py:48] [68] global_step=68, grad_norm=0.894040, loss=0.532566
I0402 06:11:19.158185 140196286560064 submission.py:296] 68) loss = 0.533, grad_norm = 0.894
I0402 06:11:19.379695 140130337675008 logging_writer.py:48] [69] global_step=69, grad_norm=1.005466, loss=0.530820
I0402 06:11:19.383435 140196286560064 submission.py:296] 69) loss = 0.531, grad_norm = 1.005
I0402 06:11:19.670795 140130346067712 logging_writer.py:48] [70] global_step=70, grad_norm=0.864555, loss=0.442678
I0402 06:11:19.674136 140196286560064 submission.py:296] 70) loss = 0.443, grad_norm = 0.865
I0402 06:11:19.972676 140130337675008 logging_writer.py:48] [71] global_step=71, grad_norm=0.766259, loss=0.476939
I0402 06:11:19.978314 140196286560064 submission.py:296] 71) loss = 0.477, grad_norm = 0.766
I0402 06:11:20.265800 140130346067712 logging_writer.py:48] [72] global_step=72, grad_norm=0.804451, loss=0.498184
I0402 06:11:20.269031 140196286560064 submission.py:296] 72) loss = 0.498, grad_norm = 0.804
I0402 06:11:20.553168 140130337675008 logging_writer.py:48] [73] global_step=73, grad_norm=0.917325, loss=0.390945
I0402 06:11:20.558370 140196286560064 submission.py:296] 73) loss = 0.391, grad_norm = 0.917
I0402 06:11:20.840154 140130346067712 logging_writer.py:48] [74] global_step=74, grad_norm=0.917211, loss=0.437270
I0402 06:11:20.845698 140196286560064 submission.py:296] 74) loss = 0.437, grad_norm = 0.917
I0402 06:11:21.108353 140130337675008 logging_writer.py:48] [75] global_step=75, grad_norm=0.863338, loss=0.439285
I0402 06:11:21.114278 140196286560064 submission.py:296] 75) loss = 0.439, grad_norm = 0.863
I0402 06:11:21.421866 140130346067712 logging_writer.py:48] [76] global_step=76, grad_norm=0.849937, loss=0.409921
I0402 06:11:21.427329 140196286560064 submission.py:296] 76) loss = 0.410, grad_norm = 0.850
I0402 06:11:21.631610 140130337675008 logging_writer.py:48] [77] global_step=77, grad_norm=0.804818, loss=0.398211
I0402 06:11:21.636916 140196286560064 submission.py:296] 77) loss = 0.398, grad_norm = 0.805
I0402 06:11:21.849923 140130346067712 logging_writer.py:48] [78] global_step=78, grad_norm=0.807285, loss=0.360338
I0402 06:11:21.854688 140196286560064 submission.py:296] 78) loss = 0.360, grad_norm = 0.807
I0402 06:11:22.147132 140130337675008 logging_writer.py:48] [79] global_step=79, grad_norm=0.847891, loss=0.372900
I0402 06:11:22.153054 140196286560064 submission.py:296] 79) loss = 0.373, grad_norm = 0.848
I0402 06:11:22.403591 140130346067712 logging_writer.py:48] [80] global_step=80, grad_norm=0.769325, loss=0.359631
I0402 06:11:22.406833 140196286560064 submission.py:296] 80) loss = 0.360, grad_norm = 0.769
I0402 06:11:22.647859 140130337675008 logging_writer.py:48] [81] global_step=81, grad_norm=0.692653, loss=0.459741
I0402 06:11:22.651295 140196286560064 submission.py:296] 81) loss = 0.460, grad_norm = 0.693
I0402 06:11:22.945613 140130346067712 logging_writer.py:48] [82] global_step=82, grad_norm=0.773166, loss=0.351135
I0402 06:11:22.949655 140196286560064 submission.py:296] 82) loss = 0.351, grad_norm = 0.773
I0402 06:11:23.253519 140130337675008 logging_writer.py:48] [83] global_step=83, grad_norm=0.695286, loss=0.352142
I0402 06:11:23.257009 140196286560064 submission.py:296] 83) loss = 0.352, grad_norm = 0.695
I0402 06:11:23.540167 140130346067712 logging_writer.py:48] [84] global_step=84, grad_norm=0.709838, loss=0.389895
I0402 06:11:23.546056 140196286560064 submission.py:296] 84) loss = 0.390, grad_norm = 0.710
I0402 06:11:23.823585 140130337675008 logging_writer.py:48] [85] global_step=85, grad_norm=0.662135, loss=0.372012
I0402 06:11:23.828292 140196286560064 submission.py:296] 85) loss = 0.372, grad_norm = 0.662
I0402 06:11:24.035061 140130346067712 logging_writer.py:48] [86] global_step=86, grad_norm=0.613276, loss=0.394135
I0402 06:11:24.038343 140196286560064 submission.py:296] 86) loss = 0.394, grad_norm = 0.613
I0402 06:11:24.285887 140130337675008 logging_writer.py:48] [87] global_step=87, grad_norm=0.638558, loss=0.343873
I0402 06:11:24.289142 140196286560064 submission.py:296] 87) loss = 0.344, grad_norm = 0.639
I0402 06:11:24.568111 140130346067712 logging_writer.py:48] [88] global_step=88, grad_norm=0.533122, loss=0.330360
I0402 06:11:24.571990 140196286560064 submission.py:296] 88) loss = 0.330, grad_norm = 0.533
I0402 06:11:24.866997 140130337675008 logging_writer.py:48] [89] global_step=89, grad_norm=0.516717, loss=0.341567
I0402 06:11:24.870250 140196286560064 submission.py:296] 89) loss = 0.342, grad_norm = 0.517
I0402 06:11:25.148653 140130346067712 logging_writer.py:48] [90] global_step=90, grad_norm=0.632125, loss=0.325699
I0402 06:11:25.152486 140196286560064 submission.py:296] 90) loss = 0.326, grad_norm = 0.632
I0402 06:11:25.396296 140130337675008 logging_writer.py:48] [91] global_step=91, grad_norm=0.453413, loss=0.302071
I0402 06:11:25.403095 140196286560064 submission.py:296] 91) loss = 0.302, grad_norm = 0.453
I0402 06:11:25.678692 140130346067712 logging_writer.py:48] [92] global_step=92, grad_norm=0.813154, loss=0.296689
I0402 06:11:25.682786 140196286560064 submission.py:296] 92) loss = 0.297, grad_norm = 0.813
I0402 06:11:25.957873 140130337675008 logging_writer.py:48] [93] global_step=93, grad_norm=0.350911, loss=0.411874
I0402 06:11:25.962964 140196286560064 submission.py:296] 93) loss = 0.412, grad_norm = 0.351
I0402 06:11:26.277805 140130346067712 logging_writer.py:48] [94] global_step=94, grad_norm=0.477920, loss=0.297440
I0402 06:11:26.280918 140196286560064 submission.py:296] 94) loss = 0.297, grad_norm = 0.478
I0402 06:11:26.546095 140130337675008 logging_writer.py:48] [95] global_step=95, grad_norm=0.540653, loss=0.354290
I0402 06:11:26.549536 140196286560064 submission.py:296] 95) loss = 0.354, grad_norm = 0.541
I0402 06:11:26.833127 140130346067712 logging_writer.py:48] [96] global_step=96, grad_norm=0.515954, loss=0.320907
I0402 06:11:26.836883 140196286560064 submission.py:296] 96) loss = 0.321, grad_norm = 0.516
I0402 06:11:27.144789 140130337675008 logging_writer.py:48] [97] global_step=97, grad_norm=0.419926, loss=0.359576
I0402 06:11:27.151087 140196286560064 submission.py:296] 97) loss = 0.360, grad_norm = 0.420
I0402 06:11:27.385737 140130346067712 logging_writer.py:48] [98] global_step=98, grad_norm=0.560375, loss=0.280971
I0402 06:11:27.390607 140196286560064 submission.py:296] 98) loss = 0.281, grad_norm = 0.560
I0402 06:11:27.703969 140130337675008 logging_writer.py:48] [99] global_step=99, grad_norm=0.442990, loss=0.325644
I0402 06:11:27.709372 140196286560064 submission.py:296] 99) loss = 0.326, grad_norm = 0.443
I0402 06:11:27.934738 140130346067712 logging_writer.py:48] [100] global_step=100, grad_norm=0.361178, loss=0.293118
I0402 06:11:27.939666 140196286560064 submission.py:296] 100) loss = 0.293, grad_norm = 0.361
I0402 06:12:24.065402 140196286560064 submission_runner.py:373] Before eval at step 309: RAM USED (GB) 86.91058688
I0402 06:12:24.065683 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:12:26.129664 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:12:32.117524 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:12:35.077110 140196286560064 submission_runner.py:382] Time since start: 355.97s, 	Step: 309, 	{'train/ssim': 0.698620115007673, 'train/loss': 0.3060086454663958, 'validation/ssim': 0.6775584481174382, 'validation/loss': 0.3252568285184827, 'validation/num_examples': 3554, 'test/ssim': 0.6962827220704761, 'test/loss': 0.3265593229675719, 'test/num_examples': 3581}
I0402 06:12:35.077474 140196286560064 submission_runner.py:396] After eval at step 309: RAM USED (GB) 87.980576768
I0402 06:12:35.088454 140130337675008 logging_writer.py:48] [309] global_step=309, preemption_count=0, score=125.789669, test/loss=0.326559, test/num_examples=3581, test/ssim=0.696283, total_duration=355.968193, train/loss=0.306009, train/ssim=0.698620, validation/loss=0.325257, validation/num_examples=3554, validation/ssim=0.677558
I0402 06:12:35.245552 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_309.
I0402 06:12:35.246248 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 309: RAM USED (GB) 88.001785856
I0402 06:13:44.404587 140130346067712 logging_writer.py:48] [500] global_step=500, grad_norm=0.179010, loss=0.305620
I0402 06:13:44.409004 140196286560064 submission.py:296] 500) loss = 0.306, grad_norm = 0.179
I0402 06:13:55.404911 140196286560064 submission_runner.py:373] Before eval at step 530: RAM USED (GB) 104.923611136
I0402 06:13:55.405106 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:13:57.426070 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:14:02.657838 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:14:06.827423 140196286560064 submission_runner.py:382] Time since start: 447.30s, 	Step: 530, 	{'train/ssim': 0.7146573066711426, 'train/loss': 0.29219147137233187, 'validation/ssim': 0.6940855456659749, 'validation/loss': 0.31069659489088, 'validation/num_examples': 3554, 'test/ssim': 0.7117583470050265, 'test/loss': 0.31251090826584754, 'test/num_examples': 3581}
I0402 06:14:06.827782 140196286560064 submission_runner.py:396] After eval at step 530: RAM USED (GB) 106.103451648
I0402 06:14:06.842442 140130337675008 logging_writer.py:48] [530] global_step=530, preemption_count=0, score=201.945520, test/loss=0.312511, test/num_examples=3581, test/ssim=0.711758, total_duration=447.304833, train/loss=0.292191, train/ssim=0.714657, validation/loss=0.310697, validation/num_examples=3554, validation/ssim=0.694086
I0402 06:14:07.006356 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_530.
I0402 06:14:07.007033 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 530: RAM USED (GB) 106.12826112
I0402 06:15:27.380712 140196286560064 submission_runner.py:373] Before eval at step 751: RAM USED (GB) 122.903515136
I0402 06:15:27.380935 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:15:29.430466 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:15:33.495020 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:15:35.765829 140196286560064 submission_runner.py:382] Time since start: 539.29s, 	Step: 751, 	{'train/ssim': 0.7173272541591099, 'train/loss': 0.28967680249895367, 'validation/ssim': 0.6983698220754784, 'validation/loss': 0.30745080916748735, 'validation/num_examples': 3554, 'test/ssim': 0.715770475360409, 'test/loss': 0.3094095860204028, 'test/num_examples': 3581}
I0402 06:15:35.766228 140196286560064 submission_runner.py:396] After eval at step 751: RAM USED (GB) 124.02210816
I0402 06:15:35.775926 140130346067712 logging_writer.py:48] [751] global_step=751, preemption_count=0, score=278.888988, test/loss=0.309410, test/num_examples=3581, test/ssim=0.715770, total_duration=539.293012, train/loss=0.289677, train/ssim=0.717327, validation/loss=0.307451, validation/num_examples=3554, validation/ssim=0.698370
I0402 06:15:35.933386 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_751.
I0402 06:15:35.935142 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 751: RAM USED (GB) 124.045254656
I0402 06:16:56.152935 140196286560064 submission_runner.py:373] Before eval at step 968: RAM USED (GB) 140.906274816
I0402 06:16:56.153255 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:16:58.222946 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:17:02.854610 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:17:04.973191 140196286560064 submission_runner.py:382] Time since start: 628.06s, 	Step: 968, 	{'train/ssim': 0.7270622253417969, 'train/loss': 0.2824565512793405, 'validation/ssim': 0.706571751789357, 'validation/loss': 0.30093481678874157, 'validation/num_examples': 3554, 'test/ssim': 0.7239626511885646, 'test/loss': 0.3026219857734397, 'test/num_examples': 3581}
I0402 06:17:04.973745 140196286560064 submission_runner.py:396] After eval at step 968: RAM USED (GB) 141.942980608
I0402 06:17:04.984094 140130337675008 logging_writer.py:48] [968] global_step=968, preemption_count=0, score=355.417572, test/loss=0.302622, test/num_examples=3581, test/ssim=0.723963, total_duration=628.060277, train/loss=0.282457, train/ssim=0.727062, validation/loss=0.300935, validation/num_examples=3554, validation/ssim=0.706572
I0402 06:17:05.141487 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_968.
I0402 06:17:05.142122 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 968: RAM USED (GB) 141.964455936
I0402 06:17:11.452307 140130346067712 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.126666, loss=0.382669
I0402 06:17:11.455556 140196286560064 submission.py:296] 1000) loss = 0.383, grad_norm = 0.127
I0402 06:18:25.317156 140196286560064 submission_runner.py:373] Before eval at step 1281: RAM USED (GB) 143.504678912
I0402 06:18:25.317461 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:18:27.342262 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:18:29.404022 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:18:31.412951 140196286560064 submission_runner.py:382] Time since start: 717.22s, 	Step: 1281, 	{'train/ssim': 0.7257396153041294, 'train/loss': 0.2831718070166452, 'validation/ssim': 0.7052762402398706, 'validation/loss': 0.30193264026888716, 'validation/num_examples': 3554, 'test/ssim': 0.7219019434166434, 'test/loss': 0.3039005027074316, 'test/num_examples': 3581}
I0402 06:18:31.413329 140196286560064 submission_runner.py:396] After eval at step 1281: RAM USED (GB) 143.515877376
I0402 06:18:31.421485 140130337675008 logging_writer.py:48] [1281] global_step=1281, preemption_count=0, score=429.214030, test/loss=0.303901, test/num_examples=3581, test/ssim=0.721902, total_duration=717.220012, train/loss=0.283172, train/ssim=0.725740, validation/loss=0.301933, validation/num_examples=3554, validation/ssim=0.705276
I0402 06:18:31.563731 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1281.
I0402 06:18:31.564333 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 1281: RAM USED (GB) 143.524421632
I0402 06:19:27.187997 140130346067712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.246399, loss=0.355984
I0402 06:19:27.191987 140196286560064 submission.py:296] 1500) loss = 0.356, grad_norm = 0.246
I0402 06:19:51.610917 140196286560064 submission_runner.py:373] Before eval at step 1592: RAM USED (GB) 143.566794752
I0402 06:19:51.611215 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:19:53.597056 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:19:56.008498 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:19:58.034984 140196286560064 submission_runner.py:382] Time since start: 803.52s, 	Step: 1592, 	{'train/ssim': 0.7175218718392509, 'train/loss': 0.2933340072631836, 'validation/ssim': 0.6976832881963985, 'validation/loss': 0.3113408128934827, 'validation/num_examples': 3554, 'test/ssim': 0.7150435076183329, 'test/loss': 0.3133015530098087, 'test/num_examples': 3581}
I0402 06:19:58.035357 140196286560064 submission_runner.py:396] After eval at step 1592: RAM USED (GB) 143.530409984
I0402 06:19:58.043426 140130337675008 logging_writer.py:48] [1592] global_step=1592, preemption_count=0, score=502.316688, test/loss=0.313302, test/num_examples=3581, test/ssim=0.715044, total_duration=803.516854, train/loss=0.293334, train/ssim=0.717522, validation/loss=0.311341, validation/num_examples=3554, validation/ssim=0.697683
I0402 06:19:58.185382 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1592.
I0402 06:19:58.185942 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 1592: RAM USED (GB) 143.530115072
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 06:21:18.281046 140196286560064 submission_runner.py:373] Before eval at step 1904: RAM USED (GB) 143.602798592
I0402 06:21:18.281328 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:21:20.255558 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:21:22.251261 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:21:24.256505 140196286560064 submission_runner.py:382] Time since start: 890.18s, 	Step: 1904, 	{'train/ssim': 0.7230431011744908, 'train/loss': 0.2846576826913016, 'validation/ssim': 0.7040219454531865, 'validation/loss': 0.30240295787976573, 'validation/num_examples': 3554, 'test/ssim': 0.7212988526685982, 'test/loss': 0.3040224366666085, 'test/num_examples': 3581}
I0402 06:21:24.256868 140196286560064 submission_runner.py:396] After eval at step 1904: RAM USED (GB) 143.54417664
I0402 06:21:24.265154 140130346067712 logging_writer.py:48] [1904] global_step=1904, preemption_count=0, score=575.431664, test/loss=0.304022, test/num_examples=3581, test/ssim=0.721299, total_duration=890.182259, train/loss=0.284658, train/ssim=0.723043, validation/loss=0.302403, validation/num_examples=3554, validation/ssim=0.704022
I0402 06:21:24.406159 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1904.
I0402 06:21:24.406777 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 1904: RAM USED (GB) 143.543386112
I0402 06:21:47.883716 140130337675008 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.177001, loss=0.259546
I0402 06:21:47.887690 140196286560064 submission.py:296] 2000) loss = 0.260, grad_norm = 0.177
I0402 06:22:44.557159 140196286560064 submission_runner.py:373] Before eval at step 2213: RAM USED (GB) 143.800262656
I0402 06:22:44.557375 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:22:46.537546 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:22:48.740238 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:22:51.425052 140196286560064 submission_runner.py:382] Time since start: 976.45s, 	Step: 2213, 	{'train/ssim': 0.7285834721156529, 'train/loss': 0.2813143048967634, 'validation/ssim': 0.7073466956246482, 'validation/loss': 0.300242581257474, 'validation/num_examples': 3554, 'test/ssim': 0.724385551020141, 'test/loss': 0.30206201676382294, 'test/num_examples': 3581}
I0402 06:22:51.425422 140196286560064 submission_runner.py:396] After eval at step 2213: RAM USED (GB) 143.751487488
I0402 06:22:51.434765 140130346067712 logging_writer.py:48] [2213] global_step=2213, preemption_count=0, score=648.746026, test/loss=0.302062, test/num_examples=3581, test/ssim=0.724386, total_duration=976.454252, train/loss=0.281314, train/ssim=0.728583, validation/loss=0.300243, validation/num_examples=3554, validation/ssim=0.707347
I0402 06:22:51.576933 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2213.
I0402 06:22:51.577499 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 2213: RAM USED (GB) 143.751979008
I0402 06:24:05.263765 140130337675008 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.144457, loss=0.242797
I0402 06:24:05.267330 140196286560064 submission.py:296] 2500) loss = 0.243, grad_norm = 0.144
I0402 06:24:11.650772 140196286560064 submission_runner.py:373] Before eval at step 2525: RAM USED (GB) 143.821615104
I0402 06:24:11.650959 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:24:13.617432 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:24:15.712082 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:24:17.731678 140196286560064 submission_runner.py:382] Time since start: 1063.55s, 	Step: 2525, 	{'train/ssim': 0.728510856628418, 'train/loss': 0.2805591991969517, 'validation/ssim': 0.7075746930175506, 'validation/loss': 0.29902545025191685, 'validation/num_examples': 3554, 'test/ssim': 0.7246026936871683, 'test/loss': 0.30081980390210483, 'test/num_examples': 3581}
I0402 06:24:17.732039 140196286560064 submission_runner.py:396] After eval at step 2525: RAM USED (GB) 143.820480512
I0402 06:24:17.739998 140130346067712 logging_writer.py:48] [2525] global_step=2525, preemption_count=0, score=721.828610, test/loss=0.300820, test/num_examples=3581, test/ssim=0.724603, total_duration=1063.550801, train/loss=0.280559, train/ssim=0.728511, validation/loss=0.299025, validation/num_examples=3554, validation/ssim=0.707575
I0402 06:24:17.882383 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2525.
I0402 06:24:17.882963 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 2525: RAM USED (GB) 143.820406784
I0402 06:25:05.555836 140196286560064 submission_runner.py:373] Before eval at step 2714: RAM USED (GB) 143.843995648
I0402 06:25:05.556068 140196286560064 spec.py:298] Evaluating on the training split.
I0402 06:25:07.543632 140196286560064 spec.py:310] Evaluating on the validation split.
I0402 06:25:09.723562 140196286560064 spec.py:326] Evaluating on the test split.
I0402 06:25:11.747037 140196286560064 submission_runner.py:382] Time since start: 1117.45s, 	Step: 2714, 	{'train/ssim': 0.7308521270751953, 'train/loss': 0.27825547967638287, 'validation/ssim': 0.7103340865925718, 'validation/loss': 0.29649759184193863, 'validation/num_examples': 3554, 'test/ssim': 0.7275222228645979, 'test/loss': 0.29817267457588664, 'test/num_examples': 3581}
I0402 06:25:11.747439 140196286560064 submission_runner.py:396] After eval at step 2714: RAM USED (GB) 143.764062208
I0402 06:25:11.755465 140130337675008 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=765.332268, test/loss=0.298173, test/num_examples=3581, test/ssim=0.727522, total_duration=1117.452758, train/loss=0.278255, train/ssim=0.730852, validation/loss=0.296498, validation/num_examples=3554, validation/ssim=0.710334
I0402 06:25:11.898473 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0402 06:25:11.899008 140196286560064 submission_runner.py:416] After logging and checkpointing eval at step 2714: RAM USED (GB) 143.763075072
I0402 06:25:11.906809 140130346067712 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=765.332268
I0402 06:25:12.152793 140196286560064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0402 06:25:13.647586 140196286560064 submission_runner.py:550] Tuning trial 1/1
I0402 06:25:13.647831 140196286560064 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 06:25:13.654820 140196286560064 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ssim': 0.17093840667179652, 'train/loss': 1.0958151136125838, 'validation/ssim': 0.16222392325328855, 'validation/loss': 1.0968789018535452, 'validation/num_examples': 3554, 'test/ssim': 0.18376277698814053, 'test/loss': 1.0957503032497906, 'test/num_examples': 3581, 'score': 48.79291534423828, 'total_duration': 48.794952154159546, 'global_step': 1, 'preemption_count': 0}), (309, {'train/ssim': 0.698620115007673, 'train/loss': 0.3060086454663958, 'validation/ssim': 0.6775584481174382, 'validation/loss': 0.3252568285184827, 'validation/num_examples': 3554, 'test/ssim': 0.6962827220704761, 'test/loss': 0.3265593229675719, 'test/num_examples': 3581, 'score': 125.78966927528381, 'total_duration': 355.9681930541992, 'global_step': 309, 'preemption_count': 0}), (530, {'train/ssim': 0.7146573066711426, 'train/loss': 0.29219147137233187, 'validation/ssim': 0.6940855456659749, 'validation/loss': 0.31069659489088, 'validation/num_examples': 3554, 'test/ssim': 0.7117583470050265, 'test/loss': 0.31251090826584754, 'test/num_examples': 3581, 'score': 201.94552040100098, 'total_duration': 447.3048326969147, 'global_step': 530, 'preemption_count': 0}), (751, {'train/ssim': 0.7173272541591099, 'train/loss': 0.28967680249895367, 'validation/ssim': 0.6983698220754784, 'validation/loss': 0.30745080916748735, 'validation/num_examples': 3554, 'test/ssim': 0.715770475360409, 'test/loss': 0.3094095860204028, 'test/num_examples': 3581, 'score': 278.8889877796173, 'total_duration': 539.2930119037628, 'global_step': 751, 'preemption_count': 0}), (968, {'train/ssim': 0.7270622253417969, 'train/loss': 0.2824565512793405, 'validation/ssim': 0.706571751789357, 'validation/loss': 0.30093481678874157, 'validation/num_examples': 3554, 'test/ssim': 0.7239626511885646, 'test/loss': 0.3026219857734397, 'test/num_examples': 3581, 'score': 355.41757249832153, 'total_duration': 628.0602774620056, 'global_step': 968, 'preemption_count': 0}), (1281, {'train/ssim': 0.7257396153041294, 'train/loss': 0.2831718070166452, 'validation/ssim': 0.7052762402398706, 'validation/loss': 0.30193264026888716, 'validation/num_examples': 3554, 'test/ssim': 0.7219019434166434, 'test/loss': 0.3039005027074316, 'test/num_examples': 3581, 'score': 429.21402978897095, 'total_duration': 717.2200117111206, 'global_step': 1281, 'preemption_count': 0}), (1592, {'train/ssim': 0.7175218718392509, 'train/loss': 0.2933340072631836, 'validation/ssim': 0.6976832881963985, 'validation/loss': 0.3113408128934827, 'validation/num_examples': 3554, 'test/ssim': 0.7150435076183329, 'test/loss': 0.3133015530098087, 'test/num_examples': 3581, 'score': 502.3166882991791, 'total_duration': 803.5168540477753, 'global_step': 1592, 'preemption_count': 0}), (1904, {'train/ssim': 0.7230431011744908, 'train/loss': 0.2846576826913016, 'validation/ssim': 0.7040219454531865, 'validation/loss': 0.30240295787976573, 'validation/num_examples': 3554, 'test/ssim': 0.7212988526685982, 'test/loss': 0.3040224366666085, 'test/num_examples': 3581, 'score': 575.4316642284393, 'total_duration': 890.1822590827942, 'global_step': 1904, 'preemption_count': 0}), (2213, {'train/ssim': 0.7285834721156529, 'train/loss': 0.2813143048967634, 'validation/ssim': 0.7073466956246482, 'validation/loss': 0.300242581257474, 'validation/num_examples': 3554, 'test/ssim': 0.724385551020141, 'test/loss': 0.30206201676382294, 'test/num_examples': 3581, 'score': 648.7460260391235, 'total_duration': 976.4542517662048, 'global_step': 2213, 'preemption_count': 0}), (2525, {'train/ssim': 0.728510856628418, 'train/loss': 0.2805591991969517, 'validation/ssim': 0.7075746930175506, 'validation/loss': 0.29902545025191685, 'validation/num_examples': 3554, 'test/ssim': 0.7246026936871683, 'test/loss': 0.30081980390210483, 'test/num_examples': 3581, 'score': 721.8286099433899, 'total_duration': 1063.550801038742, 'global_step': 2525, 'preemption_count': 0}), (2714, {'train/ssim': 0.7308521270751953, 'train/loss': 0.27825547967638287, 'validation/ssim': 0.7103340865925718, 'validation/loss': 0.29649759184193863, 'validation/num_examples': 3554, 'test/ssim': 0.7275222228645979, 'test/loss': 0.29817267457588664, 'test/num_examples': 3581, 'score': 765.3322677612305, 'total_duration': 1117.4527575969696, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0402 06:25:13.654969 140196286560064 submission_runner.py:553] Timing: 765.3322677612305
I0402 06:25:13.655020 140196286560064 submission_runner.py:554] ====================
I0402 06:25:13.655116 140196286560064 submission_runner.py:613] Final fastmri score: 765.3322677612305
