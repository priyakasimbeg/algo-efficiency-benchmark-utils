python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_jax_upgrade_b/shampoo --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-31-2023-07-27-11.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0531 07:27:33.968245 139662639298368 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax.
I0531 07:27:34.899817 139662639298368 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0531 07:27:34.900501 139662639298368 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 07:27:34.900676 139662639298368 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0531 07:27:34.905757 139662639298368 submission_runner.py:549] Using RNG seed 1210241894
I0531 07:27:40.055391 139662639298368 submission_runner.py:558] --- Tuning run 1/1 ---
I0531 07:27:40.055600 139662639298368 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1.
I0531 07:27:40.055797 139662639298368 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1/hparams.json.
I0531 07:27:40.234348 139662639298368 submission_runner.py:243] Initializing dataset.
I0531 07:27:40.234585 139662639298368 submission_runner.py:250] Initializing model.
I0531 07:27:42.375968 139662639298368 submission_runner.py:260] Initializing optimizer.
I0531 07:27:47.022928 139662639298368 submission_runner.py:267] Initializing metrics bundle.
I0531 07:27:47.023125 139662639298368 submission_runner.py:285] Initializing checkpoint and logger.
I0531 07:27:47.024551 139662639298368 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0531 07:27:47.024821 139662639298368 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0531 07:27:47.024897 139662639298368 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0531 07:27:47.656868 139662639298368 submission_runner.py:306] Saving meta data to /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0531 07:27:47.657905 139662639298368 submission_runner.py:309] Saving flags to /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0531 07:27:47.664824 139662639298368 submission_runner.py:321] Starting training loop.
I0531 07:27:47.956022 139662639298368 input_pipeline.py:20] Loading split = train-clean-100
I0531 07:27:47.989855 139662639298368 input_pipeline.py:20] Loading split = train-clean-360
I0531 07:27:48.334844 139662639298368 input_pipeline.py:20] Loading split = train-other-500
2023-05-31 07:29:10.312823: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-31 07:29:10.666912: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/array_methods.py:795: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  return getattr(self.aval, name).fun(self, *args, **kwargs)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:593: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:594: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:477: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  power = jnp.eye(mat_m.shape[0], dtype=_MAT_INV_PTH_ROOT_DTYPE)
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0531 07:29:16.314785 139501234022144 logging_writer.py:48] [0] global_step=0, grad_norm=39.38363265991211, loss=32.90290069580078
I0531 07:29:16.343908 139662639298368 spec.py:298] Evaluating on the training split.
I0531 07:29:16.613003 139662639298368 input_pipeline.py:20] Loading split = train-clean-100
I0531 07:29:16.835620 139662639298368 input_pipeline.py:20] Loading split = train-clean-360
I0531 07:29:16.955425 139662639298368 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0531 07:31:00.868678 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 07:31:01.070347 139662639298368 input_pipeline.py:20] Loading split = dev-clean
I0531 07:31:01.075635 139662639298368 input_pipeline.py:20] Loading split = dev-other
I0531 07:31:57.481582 139662639298368 spec.py:326] Evaluating on the test split.
I0531 07:31:57.693001 139662639298368 input_pipeline.py:20] Loading split = test-clean
I0531 07:32:36.925493 139662639298368 submission_runner.py:426] Time since start: 289.26s, 	Step: 1, 	{'train/ctc_loss': Array(31.970522, dtype=float32), 'train/wer': 3.105360972867168, 'validation/ctc_loss': Array(30.90734, dtype=float32), 'validation/wer': 2.644174087545466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.940115, dtype=float32), 'test/wer': 3.0247395039912255, 'test/num_examples': 2472, 'score': 88.67890858650208, 'total_duration': 289.25870633125305, 'accumulated_submission_time': 88.67890858650208, 'accumulated_data_selection_time': 4.427462816238403, 'accumulated_eval_time': 200.57964491844177, 'accumulated_logging_time': 0}
I0531 07:32:36.946003 139496209250048 logging_writer.py:48] [1] accumulated_data_selection_time=4.427463, accumulated_eval_time=200.579645, accumulated_logging_time=0, accumulated_submission_time=88.678909, global_step=1, preemption_count=0, score=88.678909, test/ctc_loss=30.940114974975586, test/num_examples=2472, test/wer=3.024740, total_duration=289.258706, train/ctc_loss=31.970521926879883, train/wer=3.105361, validation/ctc_loss=30.907339096069336, validation/num_examples=5348, validation/wer=2.644174
I0531 07:34:47.464915 139503947851520 logging_writer.py:48] [100] global_step=100, grad_norm=2.130167007446289, loss=5.871892929077148
I0531 07:36:20.171283 139503956244224 logging_writer.py:48] [200] global_step=200, grad_norm=2.3461859226226807, loss=5.744994163513184
I0531 07:37:53.828545 139503947851520 logging_writer.py:48] [300] global_step=300, grad_norm=0.7951125502586365, loss=5.2782745361328125
I0531 07:39:27.271178 139503956244224 logging_writer.py:48] [400] global_step=400, grad_norm=1.5224682092666626, loss=4.4801106452941895
I0531 07:41:01.559113 139503947851520 logging_writer.py:48] [500] global_step=500, grad_norm=2.628746271133423, loss=3.976980686187744
I0531 07:42:35.597648 139503956244224 logging_writer.py:48] [600] global_step=600, grad_norm=4.049501895904541, loss=3.6713500022888184
I0531 07:44:09.320058 139503947851520 logging_writer.py:48] [700] global_step=700, grad_norm=2.5341193675994873, loss=3.4935247898101807
I0531 07:45:43.861891 139503956244224 logging_writer.py:48] [800] global_step=800, grad_norm=2.5968124866485596, loss=3.233034133911133
I0531 07:47:18.008312 139503947851520 logging_writer.py:48] [900] global_step=900, grad_norm=2.498439073562622, loss=3.0858612060546875
I0531 07:48:53.594541 139503956244224 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.463334321975708, loss=3.0367724895477295
I0531 07:50:32.804743 139504098920192 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.771277666091919, loss=2.8553450107574463
I0531 07:52:06.474372 139504090527488 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.3087422847747803, loss=2.773514986038208
I0531 07:53:40.401615 139504098920192 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.1003522872924805, loss=2.7214131355285645
I0531 07:55:14.228064 139504090527488 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.477555990219116, loss=2.6852595806121826
I0531 07:56:49.059489 139504098920192 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.1957223415374756, loss=2.545100450515747
I0531 07:58:24.383453 139504090527488 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.9431653022766113, loss=2.5510029792785645
I0531 07:59:58.411478 139504098920192 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.480330228805542, loss=2.4946064949035645
I0531 08:01:32.657732 139504090527488 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.762547731399536, loss=2.4481961727142334
I0531 08:03:06.736566 139504098920192 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.5104665756225586, loss=2.3493728637695312
I0531 08:04:40.959847 139504090527488 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.9688808917999268, loss=2.32214093208313
I0531 08:06:19.090000 139504098920192 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.2613353729248047, loss=2.3313148021698
I0531 08:07:54.941209 139504090527488 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.5496437549591064, loss=2.261037826538086
I0531 08:09:28.817079 139504098920192 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.6512715816497803, loss=2.1978530883789062
I0531 08:11:02.991566 139504090527488 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.7050957679748535, loss=2.2431094646453857
I0531 08:12:37.872812 139504098920192 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.6497738361358643, loss=2.206159830093384
I0531 08:12:37.882880 139662639298368 spec.py:298] Evaluating on the training split.
I0531 08:13:23.434342 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 08:14:04.754890 139662639298368 spec.py:326] Evaluating on the test split.
I0531 08:14:26.264702 139662639298368 submission_runner.py:426] Time since start: 2798.60s, 	Step: 2501, 	{'train/ctc_loss': Array(2.4473372, dtype=float32), 'train/wer': 0.5556219233383453, 'validation/ctc_loss': Array(2.8303134, dtype=float32), 'validation/wer': 0.600411002518114, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2768626, dtype=float32), 'test/wer': 0.521926350212256, 'test/num_examples': 2472, 'score': 2489.575747728348, 'total_duration': 2798.595618247986, 'accumulated_submission_time': 2489.575747728348, 'accumulated_data_selection_time': 291.822074174881, 'accumulated_eval_time': 308.95723724365234, 'accumulated_logging_time': 0.03281426429748535}
I0531 08:14:26.285981 139504098920192 logging_writer.py:48] [2501] accumulated_data_selection_time=291.822074, accumulated_eval_time=308.957237, accumulated_logging_time=0.032814, accumulated_submission_time=2489.575748, global_step=2501, preemption_count=0, score=2489.575748, test/ctc_loss=2.276862621307373, test/num_examples=2472, test/wer=0.521926, total_duration=2798.595618, train/ctc_loss=2.4473371505737305, train/wer=0.555622, validation/ctc_loss=2.8303134441375732, validation/num_examples=5348, validation/wer=0.600411
I0531 08:16:01.195241 139504090527488 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.193657398223877, loss=2.2127645015716553
I0531 08:17:35.554756 139504098920192 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.161243438720703, loss=2.1722910404205322
I0531 08:19:10.802935 139504090527488 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.5078201293945312, loss=2.088733434677124
I0531 08:20:45.568869 139504098920192 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.8670332431793213, loss=2.164437770843506
I0531 08:22:20.208687 139504090527488 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.0230469703674316, loss=2.016254186630249
I0531 08:23:57.825335 139504754280192 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.9645214080810547, loss=2.111100673675537
I0531 08:25:31.927125 139504745887488 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.450166702270508, loss=1.9944812059402466
I0531 08:27:05.783159 139504754280192 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.387763261795044, loss=2.03224515914917
I0531 08:28:39.524317 139504745887488 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.665040016174316, loss=1.9894484281539917
I0531 08:30:14.456739 139504754280192 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.1370856761932373, loss=2.0263640880584717
I0531 08:31:48.487157 139504745887488 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.799854278564453, loss=2.03037166595459
I0531 08:33:23.195606 139504754280192 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.240727663040161, loss=2.0278711318969727
I0531 08:34:57.228546 139504745887488 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.45969820022583, loss=2.03834867477417
I0531 08:36:31.181163 139504754280192 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.4233481884002686, loss=1.9662375450134277
I0531 08:38:05.541572 139504745887488 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.047090768814087, loss=1.977527379989624
I0531 08:39:40.304222 139504754280192 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.374772787094116, loss=1.8908929824829102
I0531 08:41:19.618025 139504098920192 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.8072454929351807, loss=1.858467698097229
I0531 08:42:54.107049 139504090527488 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.7949464321136475, loss=1.972314715385437
I0531 08:44:28.071358 139504098920192 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.4222874641418457, loss=1.9488033056259155
I0531 08:46:02.155758 139504090527488 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.1356258392333984, loss=1.895519733428955
I0531 08:47:37.260390 139504098920192 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.829280853271484, loss=1.8989239931106567
I0531 08:49:11.757697 139504090527488 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.8938679695129395, loss=1.938141942024231
I0531 08:50:45.905131 139504098920192 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.706178903579712, loss=1.8803168535232544
I0531 08:52:20.220688 139504090527488 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.5907557010650635, loss=1.8839111328125
I0531 08:53:54.530751 139504098920192 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.9518840312957764, loss=1.9178309440612793
I0531 08:54:26.683164 139662639298368 spec.py:298] Evaluating on the training split.
I0531 08:55:13.151747 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 08:55:54.825790 139662639298368 spec.py:326] Evaluating on the test split.
I0531 08:56:17.343945 139662639298368 submission_runner.py:426] Time since start: 5309.68s, 	Step: 5035, 	{'train/ctc_loss': Array(0.69535136, dtype=float32), 'train/wer': 0.23360946115121328, 'validation/ctc_loss': Array(1.1133963, dtype=float32), 'validation/wer': 0.3070265993883202, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7363255, dtype=float32), 'test/wer': 0.2348627952795889, 'test/num_examples': 2472, 'score': 4889.928608417511, 'total_duration': 5309.6751844882965, 'accumulated_submission_time': 4889.928608417511, 'accumulated_data_selection_time': 604.3080639839172, 'accumulated_eval_time': 419.6141517162323, 'accumulated_logging_time': 0.07032966613769531}
I0531 08:56:17.365642 139504098920192 logging_writer.py:48] [5035] accumulated_data_selection_time=604.308064, accumulated_eval_time=419.614152, accumulated_logging_time=0.070330, accumulated_submission_time=4889.928608, global_step=5035, preemption_count=0, score=4889.928608, test/ctc_loss=0.7363255023956299, test/num_examples=2472, test/wer=0.234863, total_duration=5309.675184, train/ctc_loss=0.6953513622283936, train/wer=0.233609, validation/ctc_loss=1.1133962869644165, validation/num_examples=5348, validation/wer=0.307027
I0531 08:57:20.704199 139504090527488 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.9938440322875977, loss=2.0421254634857178
I0531 08:58:59.795266 139504098920192 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.2397408485412598, loss=1.8408044576644897
I0531 09:00:35.113721 139504090527488 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.3215548992156982, loss=1.8629329204559326
I0531 09:02:09.749061 139504098920192 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.571384906768799, loss=1.8653361797332764
I0531 09:03:44.692987 139504090527488 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.828210353851318, loss=1.8261531591415405
I0531 09:05:18.648177 139504098920192 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.2955758571624756, loss=1.9606252908706665
I0531 09:06:53.892168 139504090527488 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.9125983715057373, loss=1.7812610864639282
I0531 09:08:30.507302 139504098920192 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.369311571121216, loss=1.8560175895690918
I0531 09:10:05.033098 139504090527488 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.057964563369751, loss=1.8434165716171265
I0531 09:11:40.052168 139504098920192 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.2727839946746826, loss=1.9560108184814453
I0531 09:13:17.082274 139504090527488 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.255401849746704, loss=1.9009546041488647
I0531 09:14:56.372634 139504098920192 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.1300103664398193, loss=1.8660486936569214
I0531 09:16:32.312815 139504090527488 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.996419668197632, loss=1.816968560218811
I0531 09:18:06.599491 139504098920192 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.643702745437622, loss=1.7864471673965454
I0531 09:19:41.779475 139504090527488 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.551557779312134, loss=1.8366750478744507
I0531 09:21:15.891761 139504098920192 logging_writer.py:48] [6600] global_step=6600, grad_norm=8.332540512084961, loss=1.897111415863037
I0531 09:22:49.954500 139504090527488 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.2147810459136963, loss=1.834921956062317
I0531 09:24:24.455932 139504098920192 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.50846529006958, loss=1.8721171617507935
I0531 09:25:59.012878 139504090527488 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.208858013153076, loss=1.887396216392517
I0531 09:27:33.074697 139504098920192 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.651233196258545, loss=1.741979718208313
I0531 09:29:08.393199 139504090527488 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.866292476654053, loss=1.8277932405471802
I0531 09:30:42.350233 139504098920192 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.921270847320557, loss=1.7842130661010742
I0531 09:32:21.418274 139504098920192 logging_writer.py:48] [7300] global_step=7300, grad_norm=4.931681156158447, loss=1.87355637550354
I0531 09:33:56.885332 139504090527488 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.1705665588378906, loss=1.8464601039886475
I0531 09:35:31.757915 139504098920192 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.446916103363037, loss=1.8186149597167969
I0531 09:36:17.620136 139662639298368 spec.py:298] Evaluating on the training split.
I0531 09:37:05.138195 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 09:37:47.701218 139662639298368 spec.py:326] Evaluating on the test split.
I0531 09:38:09.609024 139662639298368 submission_runner.py:426] Time since start: 7821.94s, 	Step: 7550, 	{'train/ctc_loss': Array(0.50636894, dtype=float32), 'train/wer': 0.17850836272013748, 'validation/ctc_loss': Array(0.9455484, dtype=float32), 'validation/wer': 0.26218294436029294, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.60111153, dtype=float32), 'test/wer': 0.19070542116060366, 'test/num_examples': 2472, 'score': 7290.138382196426, 'total_duration': 7821.940096378326, 'accumulated_submission_time': 7290.138382196426, 'accumulated_data_selection_time': 927.3380451202393, 'accumulated_eval_time': 531.5990126132965, 'accumulated_logging_time': 0.10799002647399902}
I0531 09:38:09.631147 139504754280192 logging_writer.py:48] [7550] accumulated_data_selection_time=927.338045, accumulated_eval_time=531.599013, accumulated_logging_time=0.107990, accumulated_submission_time=7290.138382, global_step=7550, preemption_count=0, score=7290.138382, test/ctc_loss=0.6011115312576294, test/num_examples=2472, test/wer=0.190705, total_duration=7821.940096, train/ctc_loss=0.5063689351081848, train/wer=0.178508, validation/ctc_loss=0.945548415184021, validation/num_examples=5348, validation/wer=0.262183
I0531 09:38:58.680352 139504745887488 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.9152426719665527, loss=1.741269588470459
I0531 09:40:32.777948 139504754280192 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.391630172729492, loss=1.7798545360565186
I0531 09:42:07.343227 139504745887488 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.1456990242004395, loss=1.765976905822754
I0531 09:43:42.055972 139504754280192 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.89642333984375, loss=1.7951526641845703
I0531 09:45:17.604499 139504745887488 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.51119327545166, loss=1.759470820426941
I0531 09:46:52.312868 139504754280192 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.8914852142333984, loss=1.776870846748352
I0531 09:48:27.585309 139504745887488 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.447033882141113, loss=1.7641026973724365
I0531 09:50:07.679119 139504098920192 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.338561773300171, loss=1.7658653259277344
I0531 09:51:42.817210 139504090527488 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.520580291748047, loss=1.7119144201278687
I0531 09:53:18.154170 139504098920192 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.594466209411621, loss=1.7522295713424683
I0531 09:54:53.470174 139504090527488 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.168165922164917, loss=1.7229803800582886
I0531 09:56:28.313943 139504098920192 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.7596848011016846, loss=1.7094186544418335
I0531 09:58:04.017217 139504090527488 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.6141507625579834, loss=1.7711862325668335
I0531 09:59:38.459798 139504098920192 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.313054084777832, loss=1.8101073503494263
I0531 10:01:14.153111 139504090527488 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.871917724609375, loss=1.7198355197906494
I0531 10:02:50.933831 139504098920192 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.0749974250793457, loss=1.7880841493606567
I0531 10:04:27.569918 139504090527488 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.507077693939209, loss=1.7562670707702637
I0531 10:06:05.647316 139504098920192 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.346933841705322, loss=1.7230747938156128
I0531 10:07:40.677479 139504090527488 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.574018955230713, loss=1.7287964820861816
I0531 10:09:15.135106 139504098920192 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.4024224281311035, loss=1.7089017629623413
I0531 10:10:50.315331 139504090527488 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.239335060119629, loss=1.6466007232666016
I0531 10:12:25.435935 139504098920192 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.720057725906372, loss=1.7123451232910156
I0531 10:14:00.096694 139504090527488 logging_writer.py:48] [9800] global_step=9800, grad_norm=5.763805389404297, loss=1.7460036277770996
I0531 10:15:35.465965 139504098920192 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.7849981784820557, loss=1.7113971710205078
I0531 10:17:11.121545 139504090527488 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.533905506134033, loss=1.744379997253418
I0531 10:18:10.018205 139662639298368 spec.py:298] Evaluating on the training split.
I0531 10:18:57.440169 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 10:19:41.638917 139662639298368 spec.py:326] Evaluating on the test split.
I0531 10:20:03.004481 139662639298368 submission_runner.py:426] Time since start: 10335.34s, 	Step: 10063, 	{'train/ctc_loss': Array(0.4309836, dtype=float32), 'train/wer': 0.14667658054854485, 'validation/ctc_loss': Array(0.78869724, dtype=float32), 'validation/wer': 0.22291580237146524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48237324, dtype=float32), 'test/wer': 0.15672414843702395, 'test/num_examples': 2472, 'score': 9690.479616165161, 'total_duration': 10335.335806369781, 'accumulated_submission_time': 9690.479616165161, 'accumulated_data_selection_time': 1248.5485014915466, 'accumulated_eval_time': 644.5815079212189, 'accumulated_logging_time': 0.1476132869720459}
I0531 10:20:03.027320 139504754280192 logging_writer.py:48] [10063] accumulated_data_selection_time=1248.548501, accumulated_eval_time=644.581508, accumulated_logging_time=0.147613, accumulated_submission_time=9690.479616, global_step=10063, preemption_count=0, score=9690.479616, test/ctc_loss=0.4823732376098633, test/num_examples=2472, test/wer=0.156724, total_duration=10335.335806, train/ctc_loss=0.43098360300064087, train/wer=0.146677, validation/ctc_loss=0.7886972427368164, validation/num_examples=5348, validation/wer=0.222916
I0531 10:20:39.717328 139504745887488 logging_writer.py:48] [10100] global_step=10100, grad_norm=8.597278594970703, loss=1.7207934856414795
I0531 10:22:18.327087 139504754280192 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.6098387241363525, loss=1.6808998584747314
I0531 10:23:58.204420 139504754280192 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.3596785068511963, loss=1.7470167875289917
I0531 10:25:34.118181 139504745887488 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.6253082752227783, loss=1.6966735124588013
I0531 10:27:08.681912 139504754280192 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.396700620651245, loss=1.7244675159454346
I0531 10:28:43.516635 139504745887488 logging_writer.py:48] [10600] global_step=10600, grad_norm=4.0355224609375, loss=1.7067655324935913
I0531 10:30:19.096511 139504754280192 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.941875696182251, loss=1.6521140336990356
I0531 10:31:54.233794 139504745887488 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.2488443851470947, loss=1.7025549411773682
I0531 10:33:30.352419 139504754280192 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.616093635559082, loss=1.6607621908187866
I0531 10:35:06.010424 139504745887488 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.022422790527344, loss=1.612671136856079
I0531 10:36:41.131394 139504754280192 logging_writer.py:48] [11100] global_step=11100, grad_norm=9.48243236541748, loss=1.604225516319275
I0531 10:38:16.516827 139504745887488 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.8500304222106934, loss=1.6640539169311523
I0531 10:39:51.385302 139504754280192 logging_writer.py:48] [11300] global_step=11300, grad_norm=6.101681709289551, loss=1.6573066711425781
I0531 10:41:32.215539 139504754280192 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.4466607570648193, loss=1.601994276046753
I0531 10:43:08.472089 139504745887488 logging_writer.py:48] [11500] global_step=11500, grad_norm=5.347492218017578, loss=1.5737591981887817
I0531 10:44:43.804761 139504754280192 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.981094837188721, loss=1.7001240253448486
I0531 10:46:18.706360 139504745887488 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.212085723876953, loss=1.7319191694259644
I0531 10:47:54.929357 139504754280192 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.313638925552368, loss=1.6911582946777344
I0531 10:49:29.738966 139504745887488 logging_writer.py:48] [11900] global_step=11900, grad_norm=6.696540355682373, loss=1.6604902744293213
I0531 10:51:05.373796 139504754280192 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.244601249694824, loss=1.6724343299865723
I0531 10:52:40.493870 139504745887488 logging_writer.py:48] [12100] global_step=12100, grad_norm=11.509113311767578, loss=1.7192810773849487
I0531 10:54:16.049157 139504754280192 logging_writer.py:48] [12200] global_step=12200, grad_norm=9.694990158081055, loss=1.694964051246643
I0531 10:55:51.442750 139504745887488 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.747086524963379, loss=1.6491097211837769
I0531 10:57:30.513300 139504754280192 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.149967193603516, loss=1.6518492698669434
I0531 10:59:06.146889 139504745887488 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.877319097518921, loss=1.627028465270996
I0531 11:00:03.922899 139662639298368 spec.py:298] Evaluating on the training split.
I0531 11:00:50.838107 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 11:01:35.605191 139662639298368 spec.py:326] Evaluating on the test split.
I0531 11:01:57.003199 139662639298368 submission_runner.py:426] Time since start: 12849.33s, 	Step: 12561, 	{'train/ctc_loss': Array(0.37684247, dtype=float32), 'train/wer': 0.13090086759877362, 'validation/ctc_loss': Array(0.7194192, dtype=float32), 'validation/wer': 0.20359096566295865, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43170878, dtype=float32), 'test/wer': 0.13838279202973616, 'test/num_examples': 2472, 'score': 12091.330996990204, 'total_duration': 12849.334196090698, 'accumulated_submission_time': 12091.330996990204, 'accumulated_data_selection_time': 1575.572368144989, 'accumulated_eval_time': 757.657693862915, 'accumulated_logging_time': 0.18575215339660645}
I0531 11:01:57.026357 139504754280192 logging_writer.py:48] [12561] accumulated_data_selection_time=1575.572368, accumulated_eval_time=757.657694, accumulated_logging_time=0.185752, accumulated_submission_time=12091.330997, global_step=12561, preemption_count=0, score=12091.330997, test/ctc_loss=0.43170878291130066, test/num_examples=2472, test/wer=0.138383, total_duration=12849.334196, train/ctc_loss=0.3768424689769745, train/wer=0.130901, validation/ctc_loss=0.7194191813468933, validation/num_examples=5348, validation/wer=0.203591
I0531 11:02:35.373753 139504745887488 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.749984264373779, loss=1.7359219789505005
I0531 11:04:11.964270 139504754280192 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.0528945922851562, loss=1.6146994829177856
I0531 11:05:46.727798 139504745887488 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.8918509483337402, loss=1.6295185089111328
I0531 11:07:21.397103 139504754280192 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.7848398685455322, loss=1.6553415060043335
I0531 11:08:55.907629 139504745887488 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.8796467781066895, loss=1.6959173679351807
I0531 11:10:30.916603 139504754280192 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.7797908782958984, loss=1.5434030294418335
I0531 11:12:05.987845 139504745887488 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.826788425445557, loss=1.6977304220199585
I0531 11:13:40.633800 139504754280192 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.442452430725098, loss=1.6045832633972168
I0531 11:15:18.769446 139504754280192 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.588222980499268, loss=1.6425832509994507
I0531 11:16:53.817257 139504745887488 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.5595946311950684, loss=1.6846684217453003
I0531 11:18:29.041475 139504754280192 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.143075942993164, loss=1.6377958059310913
I0531 11:20:04.137615 139504745887488 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.1907033920288086, loss=1.586675763130188
I0531 11:21:40.025692 139504754280192 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.242742538452148, loss=1.589199185371399
I0531 11:23:15.563758 139504745887488 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.772883415222168, loss=1.6280170679092407
I0531 11:24:52.410702 139504754280192 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.7172060012817383, loss=1.614640712738037
I0531 11:26:30.723719 139504745887488 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.937809467315674, loss=1.6074764728546143
I0531 11:28:06.672002 139504754280192 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.5435402393341064, loss=1.653018593788147
I0531 11:29:41.823285 139504745887488 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.6999521255493164, loss=1.585584044456482
I0531 11:31:16.445221 139504754280192 logging_writer.py:48] [14400] global_step=14400, grad_norm=6.724349498748779, loss=1.693396806716919
I0531 11:32:55.883619 139504098920192 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.50493049621582, loss=1.6824318170547485
I0531 11:34:31.162856 139504090527488 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.601072311401367, loss=1.6152567863464355
I0531 11:36:05.964814 139504098920192 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.2140235900878906, loss=1.6315127611160278
I0531 11:37:41.451558 139504090527488 logging_writer.py:48] [14800] global_step=14800, grad_norm=7.205456256866455, loss=1.5818690061569214
I0531 11:39:17.463599 139504098920192 logging_writer.py:48] [14900] global_step=14900, grad_norm=6.346884727478027, loss=1.640418529510498
I0531 11:40:54.249879 139504090527488 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.004619598388672, loss=1.600412130355835
I0531 11:41:57.678221 139662639298368 spec.py:298] Evaluating on the training split.
I0531 11:42:45.441690 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 11:43:29.626888 139662639298368 spec.py:326] Evaluating on the test split.
I0531 11:43:51.557975 139662639298368 submission_runner.py:426] Time since start: 15363.89s, 	Step: 15068, 	{'train/ctc_loss': Array(0.3745714, dtype=float32), 'train/wer': 0.12615030061189325, 'validation/ctc_loss': Array(0.69116414, dtype=float32), 'validation/wer': 0.1970207141409951, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4033859, dtype=float32), 'test/wer': 0.13100968862348425, 'test/num_examples': 2472, 'score': 14491.937796115875, 'total_duration': 15363.888649702072, 'accumulated_submission_time': 14491.937796115875, 'accumulated_data_selection_time': 1899.2993032932281, 'accumulated_eval_time': 871.5330171585083, 'accumulated_logging_time': 0.22539138793945312}
I0531 11:43:51.584227 139504754280192 logging_writer.py:48] [15068] accumulated_data_selection_time=1899.299303, accumulated_eval_time=871.533017, accumulated_logging_time=0.225391, accumulated_submission_time=14491.937796, global_step=15068, preemption_count=0, score=14491.937796, test/ctc_loss=0.4033859074115753, test/num_examples=2472, test/wer=0.131010, total_duration=15363.888650, train/ctc_loss=0.37457141280174255, train/wer=0.126150, validation/ctc_loss=0.6911641359329224, validation/num_examples=5348, validation/wer=0.197021
I0531 11:44:23.824274 139504745887488 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.3260576725006104, loss=1.6297742128372192
I0531 11:45:59.026623 139504754280192 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.7937088012695312, loss=1.60396409034729
I0531 11:47:34.301099 139504745887488 logging_writer.py:48] [15300] global_step=15300, grad_norm=6.513667106628418, loss=1.6358882188796997
I0531 11:49:08.972278 139504754280192 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.8621535301208496, loss=1.623105764389038
I0531 11:50:48.296919 139504754280192 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.798454523086548, loss=1.6026159524917603
I0531 11:52:24.590330 139504745887488 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.170993328094482, loss=1.6066702604293823
I0531 11:54:00.804374 139504754280192 logging_writer.py:48] [15700] global_step=15700, grad_norm=6.680484771728516, loss=1.6416518688201904
I0531 11:55:39.997486 139504745887488 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.469169616699219, loss=1.6220526695251465
I0531 11:57:16.530038 139504754280192 logging_writer.py:48] [15900] global_step=15900, grad_norm=5.516539096832275, loss=1.6127219200134277
I0531 11:58:50.878914 139662639298368 spec.py:298] Evaluating on the training split.
I0531 11:59:38.539764 139662639298368 spec.py:310] Evaluating on the validation split.
I0531 12:00:21.542304 139662639298368 spec.py:326] Evaluating on the test split.
I0531 12:00:43.198682 139662639298368 submission_runner.py:426] Time since start: 16375.53s, 	Step: 16000, 	{'train/ctc_loss': Array(0.3444786, dtype=float32), 'train/wer': 0.11709527152064322, 'validation/ctc_loss': Array(0.67374545, dtype=float32), 'validation/wer': 0.1915985682447491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39600092, dtype=float32), 'test/wer': 0.12952694331038125, 'test/num_examples': 2472, 'score': 15391.204306602478, 'total_duration': 16375.531425476074, 'accumulated_submission_time': 15391.204306602478, 'accumulated_data_selection_time': 2024.1246349811554, 'accumulated_eval_time': 983.8504133224487, 'accumulated_logging_time': 0.26792025566101074}
I0531 12:00:43.219144 139504754280192 logging_writer.py:48] [16000] accumulated_data_selection_time=2024.124635, accumulated_eval_time=983.850413, accumulated_logging_time=0.267920, accumulated_submission_time=15391.204307, global_step=16000, preemption_count=0, score=15391.204307, test/ctc_loss=0.3960009217262268, test/num_examples=2472, test/wer=0.129527, total_duration=16375.531425, train/ctc_loss=0.3444786071777344, train/wer=0.117095, validation/ctc_loss=0.6737454533576965, validation/num_examples=5348, validation/wer=0.191599
I0531 12:00:43.239089 139504745887488 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=15391.204307
I0531 12:00:43.779942 139662639298368 checkpoints.py:490] Saving checkpoint at step: 16000
I0531 12:00:45.240153 139662639298368 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0531 12:00:45.273502 139662639298368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_jax_upgrade_b/shampoo/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0531 12:00:46.580110 139662639298368 submission_runner.py:589] Tuning trial 1/1
I0531 12:00:46.580349 139662639298368 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0531 12:00:46.595729 139662639298368 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.970522, dtype=float32), 'train/wer': 3.105360972867168, 'validation/ctc_loss': Array(30.90734, dtype=float32), 'validation/wer': 2.644174087545466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.940115, dtype=float32), 'test/wer': 3.0247395039912255, 'test/num_examples': 2472, 'score': 88.67890858650208, 'total_duration': 289.25870633125305, 'accumulated_submission_time': 88.67890858650208, 'accumulated_data_selection_time': 4.427462816238403, 'accumulated_eval_time': 200.57964491844177, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2501, {'train/ctc_loss': Array(2.4473372, dtype=float32), 'train/wer': 0.5556219233383453, 'validation/ctc_loss': Array(2.8303134, dtype=float32), 'validation/wer': 0.600411002518114, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2768626, dtype=float32), 'test/wer': 0.521926350212256, 'test/num_examples': 2472, 'score': 2489.575747728348, 'total_duration': 2798.595618247986, 'accumulated_submission_time': 2489.575747728348, 'accumulated_data_selection_time': 291.822074174881, 'accumulated_eval_time': 308.95723724365234, 'accumulated_logging_time': 0.03281426429748535, 'global_step': 2501, 'preemption_count': 0}), (5035, {'train/ctc_loss': Array(0.69535136, dtype=float32), 'train/wer': 0.23360946115121328, 'validation/ctc_loss': Array(1.1133963, dtype=float32), 'validation/wer': 0.3070265993883202, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7363255, dtype=float32), 'test/wer': 0.2348627952795889, 'test/num_examples': 2472, 'score': 4889.928608417511, 'total_duration': 5309.6751844882965, 'accumulated_submission_time': 4889.928608417511, 'accumulated_data_selection_time': 604.3080639839172, 'accumulated_eval_time': 419.6141517162323, 'accumulated_logging_time': 0.07032966613769531, 'global_step': 5035, 'preemption_count': 0}), (7550, {'train/ctc_loss': Array(0.50636894, dtype=float32), 'train/wer': 0.17850836272013748, 'validation/ctc_loss': Array(0.9455484, dtype=float32), 'validation/wer': 0.26218294436029294, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.60111153, dtype=float32), 'test/wer': 0.19070542116060366, 'test/num_examples': 2472, 'score': 7290.138382196426, 'total_duration': 7821.940096378326, 'accumulated_submission_time': 7290.138382196426, 'accumulated_data_selection_time': 927.3380451202393, 'accumulated_eval_time': 531.5990126132965, 'accumulated_logging_time': 0.10799002647399902, 'global_step': 7550, 'preemption_count': 0}), (10063, {'train/ctc_loss': Array(0.4309836, dtype=float32), 'train/wer': 0.14667658054854485, 'validation/ctc_loss': Array(0.78869724, dtype=float32), 'validation/wer': 0.22291580237146524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48237324, dtype=float32), 'test/wer': 0.15672414843702395, 'test/num_examples': 2472, 'score': 9690.479616165161, 'total_duration': 10335.335806369781, 'accumulated_submission_time': 9690.479616165161, 'accumulated_data_selection_time': 1248.5485014915466, 'accumulated_eval_time': 644.5815079212189, 'accumulated_logging_time': 0.1476132869720459, 'global_step': 10063, 'preemption_count': 0}), (12561, {'train/ctc_loss': Array(0.37684247, dtype=float32), 'train/wer': 0.13090086759877362, 'validation/ctc_loss': Array(0.7194192, dtype=float32), 'validation/wer': 0.20359096566295865, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43170878, dtype=float32), 'test/wer': 0.13838279202973616, 'test/num_examples': 2472, 'score': 12091.330996990204, 'total_duration': 12849.334196090698, 'accumulated_submission_time': 12091.330996990204, 'accumulated_data_selection_time': 1575.572368144989, 'accumulated_eval_time': 757.657693862915, 'accumulated_logging_time': 0.18575215339660645, 'global_step': 12561, 'preemption_count': 0}), (15068, {'train/ctc_loss': Array(0.3745714, dtype=float32), 'train/wer': 0.12615030061189325, 'validation/ctc_loss': Array(0.69116414, dtype=float32), 'validation/wer': 0.1970207141409951, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4033859, dtype=float32), 'test/wer': 0.13100968862348425, 'test/num_examples': 2472, 'score': 14491.937796115875, 'total_duration': 15363.888649702072, 'accumulated_submission_time': 14491.937796115875, 'accumulated_data_selection_time': 1899.2993032932281, 'accumulated_eval_time': 871.5330171585083, 'accumulated_logging_time': 0.22539138793945312, 'global_step': 15068, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.3444786, dtype=float32), 'train/wer': 0.11709527152064322, 'validation/ctc_loss': Array(0.67374545, dtype=float32), 'validation/wer': 0.1915985682447491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39600092, dtype=float32), 'test/wer': 0.12952694331038125, 'test/num_examples': 2472, 'score': 15391.204306602478, 'total_duration': 16375.531425476074, 'accumulated_submission_time': 15391.204306602478, 'accumulated_data_selection_time': 2024.1246349811554, 'accumulated_eval_time': 983.8504133224487, 'accumulated_logging_time': 0.26792025566101074, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0531 12:00:46.595898 139662639298368 submission_runner.py:592] Timing: 15391.204306602478
I0531 12:00:46.595957 139662639298368 submission_runner.py:593] ====================
I0531 12:00:46.596868 139662639298368 submission_runner.py:661] Final librispeech_deepspeech score: 15391.204306602478
