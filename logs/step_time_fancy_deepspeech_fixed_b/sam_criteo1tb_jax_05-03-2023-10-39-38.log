python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_jax_05-03-2023-10-39-38.log
I0503 10:39:58.175742 140175231719232 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax.
I0503 10:39:58.321562 140175231719232 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0503 10:39:59.224566 140175231719232 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0503 10:39:59.225557 140175231719232 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0503 10:39:59.230280 140175231719232 submission_runner.py:538] Using RNG seed 2779975996
I0503 10:40:01.956115 140175231719232 submission_runner.py:547] --- Tuning run 1/1 ---
I0503 10:40:01.956365 140175231719232 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1.
I0503 10:40:01.956638 140175231719232 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1/hparams.json.
I0503 10:40:02.095727 140175231719232 submission_runner.py:241] Initializing dataset.
I0503 10:40:02.095953 140175231719232 submission_runner.py:248] Initializing model.
I0503 10:40:08.393095 140175231719232 submission_runner.py:258] Initializing optimizer.
I0503 10:40:11.244130 140175231719232 submission_runner.py:265] Initializing metrics bundle.
I0503 10:40:11.244329 140175231719232 submission_runner.py:282] Initializing checkpoint and logger.
I0503 10:40:11.248811 140175231719232 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1 with prefix checkpoint_
I0503 10:40:11.249123 140175231719232 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0503 10:40:11.249191 140175231719232 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0503 10:40:12.118434 140175231719232 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1/meta_data_0.json.
I0503 10:40:12.121227 140175231719232 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1/flags_0.json.
I0503 10:40:12.171375 140175231719232 submission_runner.py:318] Starting training loop.
I0503 10:40:37.397070 139998780188416 logging_writer.py:48] [0] global_step=0, grad_norm=10.492799758911133, loss=1.3374654054641724
I0503 10:40:37.406213 140175231719232 spec.py:298] Evaluating on the training split.
I0503 10:45:02.034462 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 10:49:22.233925 140175231719232 spec.py:326] Evaluating on the test split.
I0503 10:53:42.980124 140175231719232 submission_runner.py:415] Time since start: 810.81s, 	Step: 1, 	{'train/loss': 1.3363766477246597, 'validation/loss': 1.338281797752809, 'validation/num_examples': 89000000, 'test/loss': 1.3380548385763809, 'test/num_examples': 89274637, 'score': 25.23462700843811, 'total_duration': 810.8086378574371, 'accumulated_submission_time': 25.23462700843811, 'accumulated_eval_time': 785.5738339424133, 'accumulated_logging_time': 0}
I0503 10:53:42.997566 139985197987584 logging_writer.py:48] [1] accumulated_eval_time=785.573834, accumulated_logging_time=0, accumulated_submission_time=25.234627, global_step=1, preemption_count=0, score=25.234627, test/loss=1.338055, test/num_examples=89274637, total_duration=810.808638, train/loss=1.336377, validation/loss=1.338282, validation/num_examples=89000000
I0503 10:54:41.368643 139985189594880 logging_writer.py:48] [100] global_step=100, grad_norm=0.13666047155857086, loss=0.1436716616153717
I0503 10:55:43.066147 140175231719232 spec.py:298] Evaluating on the training split.
I0503 10:59:57.972688 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 11:04:04.613779 140175231719232 spec.py:326] Evaluating on the test split.
I0503 11:08:12.518013 140175231719232 submission_runner.py:415] Time since start: 1680.35s, 	Step: 180, 	{'train/loss': 0.129661947422996, 'validation/loss': 0.1294209775280899, 'validation/num_examples': 89000000, 'test/loss': 0.13249972665808768, 'test/num_examples': 89274637, 'score': 145.2929539680481, 'total_duration': 1680.3465270996094, 'accumulated_submission_time': 145.2929539680481, 'accumulated_eval_time': 1535.02565574646, 'accumulated_logging_time': 0.025297880172729492}
I0503 11:08:12.526218 139985197987584 logging_writer.py:48] [180] accumulated_eval_time=1535.025656, accumulated_logging_time=0.025298, accumulated_submission_time=145.292954, global_step=180, preemption_count=0, score=145.292954, test/loss=0.132500, test/num_examples=89274637, total_duration=1680.346527, train/loss=0.129662, validation/loss=0.129421, validation/num_examples=89000000
I0503 11:08:16.393192 139985189594880 logging_writer.py:48] [200] global_step=200, grad_norm=0.03715844824910164, loss=0.12306305766105652
I0503 11:09:29.533980 139985197987584 logging_writer.py:48] [300] global_step=300, grad_norm=0.05937091261148453, loss=0.12972283363342285
I0503 11:10:12.623157 140175231719232 spec.py:298] Evaluating on the training split.
I0503 11:14:37.555226 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 11:18:45.715658 140175231719232 spec.py:326] Evaluating on the test split.
I0503 11:22:39.599272 140175231719232 submission_runner.py:415] Time since start: 2547.43s, 	Step: 357, 	{'train/loss': 0.1301886619584713, 'validation/loss': 0.12908250561797752, 'validation/num_examples': 89000000, 'test/loss': 0.13155027446373152, 'test/num_examples': 89274637, 'score': 265.38097381591797, 'total_duration': 2547.427776813507, 'accumulated_submission_time': 265.38097381591797, 'accumulated_eval_time': 2282.0017008781433, 'accumulated_logging_time': 0.0402224063873291}
I0503 11:22:39.610522 139985189594880 logging_writer.py:48] [357] accumulated_eval_time=2282.001701, accumulated_logging_time=0.040222, accumulated_submission_time=265.380974, global_step=357, preemption_count=0, score=265.380974, test/loss=0.131550, test/num_examples=89274637, total_duration=2547.427777, train/loss=0.130189, validation/loss=0.129083, validation/num_examples=89000000
I0503 11:22:55.244062 139985197987584 logging_writer.py:48] [400] global_step=400, grad_norm=0.07118137180805206, loss=0.1269446611404419
I0503 11:24:13.316677 139985189594880 logging_writer.py:48] [500] global_step=500, grad_norm=0.01770971715450287, loss=0.13892409205436707
I0503 11:24:40.307065 140175231719232 spec.py:298] Evaluating on the training split.
I0503 11:28:56.030431 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 11:33:04.871799 140175231719232 spec.py:326] Evaluating on the test split.
I0503 11:37:12.854257 140175231719232 submission_runner.py:415] Time since start: 3420.68s, 	Step: 536, 	{'train/loss': 0.1274345459702964, 'validation/loss': 0.1273522808988764, 'validation/num_examples': 89000000, 'test/loss': 0.13024256822237204, 'test/num_examples': 89274637, 'score': 386.06801891326904, 'total_duration': 3420.6827726364136, 'accumulated_submission_time': 386.06801891326904, 'accumulated_eval_time': 3034.5488197803497, 'accumulated_logging_time': 0.058443307876586914}
I0503 11:37:12.862946 139985197987584 logging_writer.py:48] [536] accumulated_eval_time=3034.548820, accumulated_logging_time=0.058443, accumulated_submission_time=386.068019, global_step=536, preemption_count=0, score=386.068019, test/loss=0.130243, test/num_examples=89274637, total_duration=3420.682773, train/loss=0.127435, validation/loss=0.127352, validation/num_examples=89000000
I0503 11:37:45.101675 139985189594880 logging_writer.py:48] [600] global_step=600, grad_norm=0.014919132925570011, loss=0.12109920382499695
I0503 11:39:03.892652 139985197987584 logging_writer.py:48] [700] global_step=700, grad_norm=0.028576945886015892, loss=0.12562164664268494
I0503 11:39:12.956324 140175231719232 spec.py:298] Evaluating on the training split.
I0503 11:43:33.006716 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 11:47:42.490596 140175231719232 spec.py:326] Evaluating on the test split.
I0503 11:51:37.313895 140175231719232 submission_runner.py:415] Time since start: 4285.14s, 	Step: 712, 	{'train/loss': 0.12673851389320043, 'validation/loss': 0.12678540449438203, 'validation/num_examples': 89000000, 'test/loss': 0.12968079612577982, 'test/num_examples': 89274637, 'score': 506.1522595882416, 'total_duration': 4285.142414331436, 'accumulated_submission_time': 506.1522595882416, 'accumulated_eval_time': 3778.9063334465027, 'accumulated_logging_time': 0.07392263412475586}
I0503 11:51:37.321975 139985189594880 logging_writer.py:48] [712] accumulated_eval_time=3778.906333, accumulated_logging_time=0.073923, accumulated_submission_time=506.152260, global_step=712, preemption_count=0, score=506.152260, test/loss=0.129681, test/num_examples=89274637, total_duration=4285.142414, train/loss=0.126739, validation/loss=0.126785, validation/num_examples=89000000
I0503 11:52:28.187755 139985197987584 logging_writer.py:48] [800] global_step=800, grad_norm=0.06548410654067993, loss=0.12481178343296051
I0503 11:53:37.949027 140175231719232 spec.py:298] Evaluating on the training split.
I0503 11:57:50.723354 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 12:01:58.380297 140175231719232 spec.py:326] Evaluating on the test split.
I0503 12:05:53.983982 140175231719232 submission_runner.py:415] Time since start: 5141.81s, 	Step: 890, 	{'train/loss': 0.12454246088719877, 'validation/loss': 0.1262574831460674, 'validation/num_examples': 89000000, 'test/loss': 0.1291006537500679, 'test/num_examples': 89274637, 'score': 626.7701337337494, 'total_duration': 5141.812485218048, 'accumulated_submission_time': 626.7701337337494, 'accumulated_eval_time': 4514.9412133693695, 'accumulated_logging_time': 0.08880758285522461}
I0503 12:05:53.993109 139985189594880 logging_writer.py:48] [890] accumulated_eval_time=4514.941213, accumulated_logging_time=0.088808, accumulated_submission_time=626.770134, global_step=890, preemption_count=0, score=626.770134, test/loss=0.129101, test/num_examples=89274637, total_duration=5141.812485, train/loss=0.124542, validation/loss=0.126257, validation/num_examples=89000000
I0503 12:05:56.023728 139985197987584 logging_writer.py:48] [900] global_step=900, grad_norm=0.0810149684548378, loss=0.12758435308933258
I0503 12:07:01.837315 139985189594880 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.025071438401937485, loss=0.13399924337863922
I0503 12:07:54.417143 140175231719232 spec.py:298] Evaluating on the training split.
I0503 12:12:15.632446 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 12:16:25.699957 140175231719232 spec.py:326] Evaluating on the test split.
I0503 12:20:20.864626 140175231719232 submission_runner.py:415] Time since start: 6008.69s, 	Step: 1068, 	{'train/loss': 0.1267266571741055, 'validation/loss': 0.12621167415730336, 'validation/num_examples': 89000000, 'test/loss': 0.12880262957551986, 'test/num_examples': 89274637, 'score': 747.1848349571228, 'total_duration': 6008.693133592606, 'accumulated_submission_time': 747.1848349571228, 'accumulated_eval_time': 5261.38861989975, 'accumulated_logging_time': 0.10500884056091309}
I0503 12:20:20.873614 139985197987584 logging_writer.py:48] [1068] accumulated_eval_time=5261.388620, accumulated_logging_time=0.105009, accumulated_submission_time=747.184835, global_step=1068, preemption_count=0, score=747.184835, test/loss=0.128803, test/num_examples=89274637, total_duration=6008.693134, train/loss=0.126727, validation/loss=0.126212, validation/num_examples=89000000
I0503 12:20:28.280471 139985189594880 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.008342462591826916, loss=0.12022998929023743
I0503 12:21:45.997640 139985197987584 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.03107854165136814, loss=0.13249832391738892
I0503 12:22:21.080816 140175231719232 spec.py:298] Evaluating on the training split.
I0503 12:26:33.459690 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 12:30:42.628219 140175231719232 spec.py:326] Evaluating on the test split.
I0503 12:34:53.512965 140175231719232 submission_runner.py:415] Time since start: 6881.34s, 	Step: 1247, 	{'train/loss': 0.12450379181469601, 'validation/loss': 0.1259903595505618, 'validation/num_examples': 89000000, 'test/loss': 0.12861356131865315, 'test/num_examples': 89274637, 'score': 867.3830873966217, 'total_duration': 6881.341468334198, 'accumulated_submission_time': 867.3830873966217, 'accumulated_eval_time': 6013.820707559586, 'accumulated_logging_time': 0.12061333656311035}
I0503 12:34:53.522313 139985189594880 logging_writer.py:48] [1247] accumulated_eval_time=6013.820708, accumulated_logging_time=0.120613, accumulated_submission_time=867.383087, global_step=1247, preemption_count=0, score=867.383087, test/loss=0.128614, test/num_examples=89274637, total_duration=6881.341468, train/loss=0.124504, validation/loss=0.125990, validation/num_examples=89000000
I0503 12:35:17.396770 139985197987584 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.037603121250867844, loss=0.12528106570243835
I0503 12:36:35.669906 139985189594880 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.006855058949440718, loss=0.11879946291446686
I0503 12:36:54.298569 140175231719232 spec.py:298] Evaluating on the training split.
I0503 12:41:18.563188 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 12:45:28.985004 140175231719232 spec.py:326] Evaluating on the test split.
I0503 12:49:25.309642 140175231719232 submission_runner.py:415] Time since start: 7753.14s, 	Step: 1425, 	{'train/loss': 0.12411848212797208, 'validation/loss': 0.12575108988764044, 'validation/num_examples': 89000000, 'test/loss': 0.12843409265276542, 'test/num_examples': 89274637, 'score': 988.1496088504791, 'total_duration': 7753.138160228729, 'accumulated_submission_time': 988.1496088504791, 'accumulated_eval_time': 6764.831708192825, 'accumulated_logging_time': 0.13737964630126953}
I0503 12:49:25.319760 139985197987584 logging_writer.py:48] [1425] accumulated_eval_time=6764.831708, accumulated_logging_time=0.137380, accumulated_submission_time=988.149609, global_step=1425, preemption_count=0, score=988.149609, test/loss=0.128434, test/num_examples=89274637, total_duration=7753.138160, train/loss=0.124118, validation/loss=0.125751, validation/num_examples=89000000
I0503 12:50:06.214047 139985189594880 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.022220036014914513, loss=0.12211708724498749
I0503 12:51:23.882543 140175231719232 spec.py:298] Evaluating on the training split.
I0503 12:55:48.033888 140175231719232 spec.py:310] Evaluating on the validation split.
I0503 12:59:56.552632 140175231719232 spec.py:326] Evaluating on the test split.
I0503 13:04:06.692148 140175231719232 submission_runner.py:415] Time since start: 8634.52s, 	Step: 1600, 	{'train/loss': 0.12422771888940627, 'validation/loss': 0.12554552808988764, 'validation/num_examples': 89000000, 'test/loss': 0.12821289881021863, 'test/num_examples': 89274637, 'score': 1106.7029876708984, 'total_duration': 8634.520668029785, 'accumulated_submission_time': 1106.7029876708984, 'accumulated_eval_time': 7527.641267538071, 'accumulated_logging_time': 0.15460562705993652}
I0503 13:04:06.700492 139985197987584 logging_writer.py:48] [1600] accumulated_eval_time=7527.641268, accumulated_logging_time=0.154606, accumulated_submission_time=1106.702988, global_step=1600, preemption_count=0, score=1106.702988, test/loss=0.128213, test/num_examples=89274637, total_duration=8634.520668, train/loss=0.124228, validation/loss=0.125546, validation/num_examples=89000000
I0503 13:04:06.712251 139985189594880 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1106.702988
I0503 13:04:09.934082 140175231719232 checkpoints.py:356] Saving checkpoint at step: 1600
I0503 13:04:30.405061 140175231719232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1/checkpoint_1600
I0503 13:04:30.442462 140175231719232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_sam/criteo1tb_jax/trial_1/checkpoint_1600.
I0503 13:04:30.532372 140175231719232 submission_runner.py:578] Tuning trial 1/1
I0503 13:04:30.532571 140175231719232 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0503 13:04:30.533936 140175231719232 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/loss': 1.3363766477246597, 'validation/loss': 1.338281797752809, 'validation/num_examples': 89000000, 'test/loss': 1.3380548385763809, 'test/num_examples': 89274637, 'score': 25.23462700843811, 'total_duration': 810.8086378574371, 'accumulated_submission_time': 25.23462700843811, 'accumulated_eval_time': 785.5738339424133, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (180, {'train/loss': 0.129661947422996, 'validation/loss': 0.1294209775280899, 'validation/num_examples': 89000000, 'test/loss': 0.13249972665808768, 'test/num_examples': 89274637, 'score': 145.2929539680481, 'total_duration': 1680.3465270996094, 'accumulated_submission_time': 145.2929539680481, 'accumulated_eval_time': 1535.02565574646, 'accumulated_logging_time': 0.025297880172729492, 'global_step': 180, 'preemption_count': 0}), (357, {'train/loss': 0.1301886619584713, 'validation/loss': 0.12908250561797752, 'validation/num_examples': 89000000, 'test/loss': 0.13155027446373152, 'test/num_examples': 89274637, 'score': 265.38097381591797, 'total_duration': 2547.427776813507, 'accumulated_submission_time': 265.38097381591797, 'accumulated_eval_time': 2282.0017008781433, 'accumulated_logging_time': 0.0402224063873291, 'global_step': 357, 'preemption_count': 0}), (536, {'train/loss': 0.1274345459702964, 'validation/loss': 0.1273522808988764, 'validation/num_examples': 89000000, 'test/loss': 0.13024256822237204, 'test/num_examples': 89274637, 'score': 386.06801891326904, 'total_duration': 3420.6827726364136, 'accumulated_submission_time': 386.06801891326904, 'accumulated_eval_time': 3034.5488197803497, 'accumulated_logging_time': 0.058443307876586914, 'global_step': 536, 'preemption_count': 0}), (712, {'train/loss': 0.12673851389320043, 'validation/loss': 0.12678540449438203, 'validation/num_examples': 89000000, 'test/loss': 0.12968079612577982, 'test/num_examples': 89274637, 'score': 506.1522595882416, 'total_duration': 4285.142414331436, 'accumulated_submission_time': 506.1522595882416, 'accumulated_eval_time': 3778.9063334465027, 'accumulated_logging_time': 0.07392263412475586, 'global_step': 712, 'preemption_count': 0}), (890, {'train/loss': 0.12454246088719877, 'validation/loss': 0.1262574831460674, 'validation/num_examples': 89000000, 'test/loss': 0.1291006537500679, 'test/num_examples': 89274637, 'score': 626.7701337337494, 'total_duration': 5141.812485218048, 'accumulated_submission_time': 626.7701337337494, 'accumulated_eval_time': 4514.9412133693695, 'accumulated_logging_time': 0.08880758285522461, 'global_step': 890, 'preemption_count': 0}), (1068, {'train/loss': 0.1267266571741055, 'validation/loss': 0.12621167415730336, 'validation/num_examples': 89000000, 'test/loss': 0.12880262957551986, 'test/num_examples': 89274637, 'score': 747.1848349571228, 'total_duration': 6008.693133592606, 'accumulated_submission_time': 747.1848349571228, 'accumulated_eval_time': 5261.38861989975, 'accumulated_logging_time': 0.10500884056091309, 'global_step': 1068, 'preemption_count': 0}), (1247, {'train/loss': 0.12450379181469601, 'validation/loss': 0.1259903595505618, 'validation/num_examples': 89000000, 'test/loss': 0.12861356131865315, 'test/num_examples': 89274637, 'score': 867.3830873966217, 'total_duration': 6881.341468334198, 'accumulated_submission_time': 867.3830873966217, 'accumulated_eval_time': 6013.820707559586, 'accumulated_logging_time': 0.12061333656311035, 'global_step': 1247, 'preemption_count': 0}), (1425, {'train/loss': 0.12411848212797208, 'validation/loss': 0.12575108988764044, 'validation/num_examples': 89000000, 'test/loss': 0.12843409265276542, 'test/num_examples': 89274637, 'score': 988.1496088504791, 'total_duration': 7753.138160228729, 'accumulated_submission_time': 988.1496088504791, 'accumulated_eval_time': 6764.831708192825, 'accumulated_logging_time': 0.13737964630126953, 'global_step': 1425, 'preemption_count': 0}), (1600, {'train/loss': 0.12422771888940627, 'validation/loss': 0.12554552808988764, 'validation/num_examples': 89000000, 'test/loss': 0.12821289881021863, 'test/num_examples': 89274637, 'score': 1106.7029876708984, 'total_duration': 8634.520668029785, 'accumulated_submission_time': 1106.7029876708984, 'accumulated_eval_time': 7527.641267538071, 'accumulated_logging_time': 0.15460562705993652, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0503 13:04:30.534063 140175231719232 submission_runner.py:581] Timing: 1106.7029876708984
I0503 13:04:30.534113 140175231719232 submission_runner.py:582] ====================
I0503 13:04:30.534220 140175231719232 submission_runner.py:645] Final criteo1tb score: 1106.7029876708984
