python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_jax_upgrade_b/sam --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-31-2023-07-28-15.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0531 07:28:38.328854 139927933790016 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax.
I0531 07:28:39.284168 139927933790016 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0531 07:28:39.284818 139927933790016 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 07:28:39.284952 139927933790016 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0531 07:28:39.290674 139927933790016 submission_runner.py:549] Using RNG seed 3442569298
I0531 07:28:44.485258 139927933790016 submission_runner.py:558] --- Tuning run 1/1 ---
I0531 07:28:44.485524 139927933790016 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1.
I0531 07:28:44.487693 139927933790016 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1/hparams.json.
I0531 07:28:44.676050 139927933790016 submission_runner.py:243] Initializing dataset.
I0531 07:28:44.676303 139927933790016 submission_runner.py:250] Initializing model.
I0531 07:28:47.208437 139927933790016 submission_runner.py:260] Initializing optimizer.
I0531 07:28:47.923354 139927933790016 submission_runner.py:267] Initializing metrics bundle.
I0531 07:28:47.923597 139927933790016 submission_runner.py:285] Initializing checkpoint and logger.
I0531 07:28:47.924779 139927933790016 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0531 07:28:47.925084 139927933790016 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0531 07:28:47.925157 139927933790016 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0531 07:28:48.703176 139927933790016 submission_runner.py:306] Saving meta data to /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0531 07:28:48.704385 139927933790016 submission_runner.py:309] Saving flags to /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0531 07:28:48.711725 139927933790016 submission_runner.py:321] Starting training loop.
I0531 07:28:49.011934 139927933790016 input_pipeline.py:20] Loading split = train-clean-100
I0531 07:28:49.050669 139927933790016 input_pipeline.py:20] Loading split = train-clean-360
I0531 07:28:49.455585 139927933790016 input_pipeline.py:20] Loading split = train-other-500
2023-05-31 07:29:50.235266: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-31 07:29:50.534104: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0531 07:29:56.235681 139762017543936 logging_writer.py:48] [0] global_step=0, grad_norm=22.92038917541504, loss=33.369972229003906
I0531 07:29:56.259452 139927933790016 spec.py:298] Evaluating on the training split.
I0531 07:29:56.540795 139927933790016 input_pipeline.py:20] Loading split = train-clean-100
I0531 07:29:56.577397 139927933790016 input_pipeline.py:20] Loading split = train-clean-360
I0531 07:29:56.972523 139927933790016 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0531 07:31:49.710231 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 07:31:49.905451 139927933790016 input_pipeline.py:20] Loading split = dev-clean
I0531 07:31:49.911059 139927933790016 input_pipeline.py:20] Loading split = dev-other
I0531 07:32:49.464461 139927933790016 spec.py:326] Evaluating on the test split.
I0531 07:32:49.680859 139927933790016 input_pipeline.py:20] Loading split = test-clean
I0531 07:33:29.792483 139927933790016 submission_runner.py:426] Time since start: 281.08s, 	Step: 1, 	{'train/ctc_loss': Array(31.706896, dtype=float32), 'train/wer': 3.5298398683963934, 'validation/ctc_loss': Array(30.587967, dtype=float32), 'validation/wer': 3.3663711179075535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.682737, dtype=float32), 'test/wer': 3.3815327117989966, 'test/num_examples': 2472, 'score': 67.54744935035706, 'total_duration': 281.0784583091736, 'accumulated_submission_time': 67.54744935035706, 'accumulated_data_selection_time': 4.842672824859619, 'accumulated_eval_time': 213.53076004981995, 'accumulated_logging_time': 0}
I0531 07:33:29.814198 139760131626752 logging_writer.py:48] [1] accumulated_data_selection_time=4.842673, accumulated_eval_time=213.530760, accumulated_logging_time=0, accumulated_submission_time=67.547449, global_step=1, preemption_count=0, score=67.547449, test/ctc_loss=30.682737350463867, test/num_examples=2472, test/wer=3.381533, total_duration=281.078458, train/ctc_loss=31.70689582824707, train/wer=3.529840, validation/ctc_loss=30.587966918945312, validation/num_examples=5348, validation/wer=3.366371
I0531 07:36:14.133831 139768624187136 logging_writer.py:48] [100] global_step=100, grad_norm=6.674790859222412, loss=8.799871444702148
I0531 07:38:45.343886 139768632579840 logging_writer.py:48] [200] global_step=200, grad_norm=1.21625816822052, loss=6.4339599609375
I0531 07:41:14.358427 139768624187136 logging_writer.py:48] [300] global_step=300, grad_norm=0.8349905610084534, loss=6.024291515350342
I0531 07:43:46.642735 139768632579840 logging_writer.py:48] [400] global_step=400, grad_norm=1.2790472507476807, loss=5.908777236938477
I0531 07:46:18.152669 139768624187136 logging_writer.py:48] [500] global_step=500, grad_norm=0.8809933662414551, loss=5.844090938568115
I0531 07:48:49.589015 139768632579840 logging_writer.py:48] [600] global_step=600, grad_norm=1.6585054397583008, loss=5.804501056671143
I0531 07:51:18.734161 139768624187136 logging_writer.py:48] [700] global_step=700, grad_norm=0.884641170501709, loss=5.740571975708008
I0531 07:53:50.583722 139768632579840 logging_writer.py:48] [800] global_step=800, grad_norm=1.4125633239746094, loss=5.667715549468994
I0531 07:56:21.160086 139768624187136 logging_writer.py:48] [900] global_step=900, grad_norm=1.375388264656067, loss=5.566644668579102
I0531 07:58:51.038496 139768632579840 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7146250009536743, loss=5.471258640289307
I0531 08:01:23.755915 139769985263360 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1289535760879517, loss=5.326887130737305
I0531 08:03:52.476028 139769976870656 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9861502647399902, loss=4.9734392166137695
I0531 08:06:21.066827 139769985263360 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.268757700920105, loss=4.560790538787842
I0531 08:08:49.820015 139769976870656 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.5086817741394043, loss=4.316222667694092
I0531 08:11:18.456803 139769985263360 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.1502866744995117, loss=4.0098557472229
I0531 08:13:30.300144 139927933790016 spec.py:298] Evaluating on the training split.
I0531 08:14:00.510175 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 08:14:36.909630 139927933790016 spec.py:326] Evaluating on the test split.
I0531 08:14:55.111080 139927933790016 submission_runner.py:426] Time since start: 2766.40s, 	Step: 1590, 	{'train/ctc_loss': Array(6.1107783, dtype=float32), 'train/wer': 0.9444553150295566, 'validation/ctc_loss': Array(6.081579, dtype=float32), 'validation/wer': 0.8959179538635201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9402027, dtype=float32), 'test/wer': 0.8994779924034693, 'test/num_examples': 2472, 'score': 2467.9976222515106, 'total_duration': 2766.396758556366, 'accumulated_submission_time': 2467.9976222515106, 'accumulated_data_selection_time': 177.5426948070526, 'accumulated_eval_time': 298.3391728401184, 'accumulated_logging_time': 0.033190011978149414}
I0531 08:14:55.132827 139769985263360 logging_writer.py:48] [1590] accumulated_data_selection_time=177.542695, accumulated_eval_time=298.339173, accumulated_logging_time=0.033190, accumulated_submission_time=2467.997622, global_step=1590, preemption_count=0, score=2467.997622, test/ctc_loss=5.940202713012695, test/num_examples=2472, test/wer=0.899478, total_duration=2766.396759, train/ctc_loss=6.110778331756592, train/wer=0.944455, validation/ctc_loss=6.081579208374023, validation/num_examples=5348, validation/wer=0.895918
I0531 08:15:11.493971 139769976870656 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.4772615432739258, loss=3.772181510925293
I0531 08:17:39.866334 139769985263360 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.351753830909729, loss=3.617433547973633
I0531 08:20:07.869246 139769976870656 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.4467616081237793, loss=3.4825806617736816
I0531 08:22:37.291560 139769985263360 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1885778903961182, loss=3.310784339904785
I0531 08:25:06.374195 139769976870656 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2445324659347534, loss=3.3159263134002686
I0531 08:27:39.047706 139769657583360 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3463071584701538, loss=3.090463876724243
I0531 08:30:07.141898 139769649190656 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.8332302570343018, loss=3.1438581943511963
I0531 08:32:34.930561 139769657583360 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.291805386543274, loss=2.984541893005371
I0531 08:35:03.925425 139769649190656 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6256153583526611, loss=2.927248477935791
I0531 08:37:32.637293 139769657583360 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.725170612335205, loss=2.8615353107452393
I0531 08:40:00.941431 139769649190656 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.6115059852600098, loss=2.8170790672302246
I0531 08:42:30.648073 139769657583360 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.0313706398010254, loss=2.8085222244262695
I0531 08:44:59.099102 139769649190656 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.5875380039215088, loss=2.6960763931274414
I0531 08:47:26.322314 139769657583360 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.7706704139709473, loss=2.648214817047119
I0531 08:49:55.041999 139769649190656 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5573742389678955, loss=2.5966176986694336
I0531 08:52:26.656849 139769985263360 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.3244030475616455, loss=2.4602909088134766
I0531 08:54:53.366693 139769976870656 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.9609408378601074, loss=2.4876630306243896
I0531 08:54:56.151302 139927933790016 spec.py:298] Evaluating on the training split.
I0531 08:55:36.518861 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 08:56:17.561601 139927933790016 spec.py:326] Evaluating on the test split.
I0531 08:56:37.934635 139927933790016 submission_runner.py:426] Time since start: 5269.22s, 	Step: 3203, 	{'train/ctc_loss': Array(4.1658964, dtype=float32), 'train/wer': 0.7598433795914019, 'validation/ctc_loss': Array(4.354117, dtype=float32), 'validation/wer': 0.7678221690513174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.0394535, dtype=float32), 'test/wer': 0.7351979363435094, 'test/num_examples': 2472, 'score': 4868.97563791275, 'total_duration': 5269.2203540802, 'accumulated_submission_time': 4868.97563791275, 'accumulated_data_selection_time': 380.6711537837982, 'accumulated_eval_time': 400.12002420425415, 'accumulated_logging_time': 0.07041501998901367}
I0531 08:56:37.956236 139769985263360 logging_writer.py:48] [3203] accumulated_data_selection_time=380.671154, accumulated_eval_time=400.120024, accumulated_logging_time=0.070415, accumulated_submission_time=4868.975638, global_step=3203, preemption_count=0, score=4868.975638, test/ctc_loss=4.039453506469727, test/num_examples=2472, test/wer=0.735198, total_duration=5269.220354, train/ctc_loss=4.165896415710449, train/wer=0.759843, validation/ctc_loss=4.354116916656494, validation/num_examples=5348, validation/wer=0.767822
I0531 08:59:01.824981 139769976870656 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2209370136260986, loss=2.4596445560455322
I0531 09:01:28.843262 139769985263360 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.6355262994766235, loss=2.411993980407715
I0531 09:03:56.050392 139769976870656 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.662498116493225, loss=2.3933427333831787
I0531 09:06:23.900310 139769985263360 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.0357677936553955, loss=2.373404026031494
I0531 09:08:50.586826 139769976870656 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.7879050970077515, loss=2.324300765991211
I0531 09:11:17.694593 139769985263360 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.8366875648498535, loss=2.3719115257263184
I0531 09:13:44.601441 139769976870656 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.634031057357788, loss=2.2788689136505127
I0531 09:16:11.982456 139769985263360 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.8796738386154175, loss=2.261767864227295
I0531 09:18:39.318899 139769976870656 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.3899946212768555, loss=2.1978647708892822
I0531 09:21:09.958336 139769985263360 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.7122143507003784, loss=2.2723758220672607
I0531 09:23:36.996610 139769976870656 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.887465000152588, loss=2.2577078342437744
I0531 09:26:04.128362 139769985263360 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.6289688348770142, loss=2.1426315307617188
I0531 09:28:30.308503 139769976870656 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.7209937572479248, loss=2.1197800636291504
I0531 09:30:56.686336 139769985263360 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.5723780393600464, loss=2.184647560119629
I0531 09:33:23.033413 139769976870656 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.653737187385559, loss=2.111931324005127
I0531 09:35:49.701553 139769985263360 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.6754295825958252, loss=2.030078411102295
I0531 09:36:38.106530 139927933790016 spec.py:298] Evaluating on the training split.
I0531 09:37:26.115333 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 09:38:09.056415 139927933790016 spec.py:326] Evaluating on the test split.
I0531 09:38:30.627550 139927933790016 submission_runner.py:426] Time since start: 7781.91s, 	Step: 4834, 	{'train/ctc_loss': Array(0.8117652, dtype=float32), 'train/wer': 0.26127636246712105, 'validation/ctc_loss': Array(1.2305324, dtype=float32), 'validation/wer': 0.33435923163754594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.86933064, dtype=float32), 'test/wer': 0.26602075843438344, 'test/num_examples': 2472, 'score': 7269.083954572678, 'total_duration': 7781.911602497101, 'accumulated_submission_time': 7269.083954572678, 'accumulated_data_selection_time': 590.3777830600739, 'accumulated_eval_time': 512.636884689331, 'accumulated_logging_time': 0.10801815986633301}
I0531 09:38:30.649367 139769985263360 logging_writer.py:48] [4834] accumulated_data_selection_time=590.377783, accumulated_eval_time=512.636885, accumulated_logging_time=0.108018, accumulated_submission_time=7269.083955, global_step=4834, preemption_count=0, score=7269.083955, test/ctc_loss=0.869330644607544, test/num_examples=2472, test/wer=0.266021, total_duration=7781.911602, train/ctc_loss=0.811765193939209, train/wer=0.261276, validation/ctc_loss=1.2305324077606201, validation/num_examples=5348, validation/wer=0.334359
I0531 09:40:08.456071 139769976870656 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.4169716835021973, loss=2.0674824714660645
I0531 09:42:35.287293 139769985263360 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.6793850660324097, loss=2.0420587062835693
I0531 09:45:01.889849 139769976870656 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.419212818145752, loss=2.094991683959961
I0531 09:47:33.016695 139769985263360 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.552976131439209, loss=2.01177978515625
I0531 09:49:59.000805 139769976870656 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.3659199476242065, loss=1.9310979843139648
I0531 09:52:25.070977 139769985263360 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.3770712614059448, loss=2.032930374145508
I0531 09:54:51.039591 139769976870656 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.8992695808410645, loss=1.9598233699798584
I0531 09:57:19.371975 139769985263360 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.3803738355636597, loss=2.0220112800598145
I0531 09:59:45.403863 139769976870656 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.6394712924957275, loss=1.999916434288025
I0531 10:02:11.698606 139769985263360 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.5113919973373413, loss=1.9699205160140991
I0531 10:04:39.605453 139769976870656 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.544735312461853, loss=2.0026965141296387
I0531 10:07:05.512518 139769985263360 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.560701847076416, loss=1.962562084197998
I0531 10:09:32.315920 139769976870656 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.3854193687438965, loss=1.941724181175232
I0531 10:12:02.035590 139769985263360 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.8486275672912598, loss=1.8854808807373047
I0531 10:14:29.099385 139769976870656 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.1385866403579712, loss=1.851081132888794
I0531 10:16:55.246257 139769985263360 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.5572433471679688, loss=1.8668662309646606
I0531 10:18:31.594882 139927933790016 spec.py:298] Evaluating on the training split.
I0531 10:19:20.223739 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 10:20:02.658536 139927933790016 spec.py:326] Evaluating on the test split.
I0531 10:20:24.381049 139927933790016 submission_runner.py:426] Time since start: 10295.67s, 	Step: 6467, 	{'train/ctc_loss': Array(0.6133573, dtype=float32), 'train/wer': 0.2053011341620501, 'validation/ctc_loss': Array(0.98163384, dtype=float32), 'validation/wer': 0.2779766326737354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64855486, dtype=float32), 'test/wer': 0.20888428493083908, 'test/num_examples': 2472, 'score': 9669.987254142761, 'total_duration': 10295.666693925858, 'accumulated_submission_time': 9669.987254142761, 'accumulated_data_selection_time': 808.2260210514069, 'accumulated_eval_time': 625.4205026626587, 'accumulated_logging_time': 0.14506316184997559}
I0531 10:20:24.402652 139769555183360 logging_writer.py:48] [6467] accumulated_data_selection_time=808.226021, accumulated_eval_time=625.420503, accumulated_logging_time=0.145063, accumulated_submission_time=9669.987254, global_step=6467, preemption_count=0, score=9669.987254, test/ctc_loss=0.6485548615455627, test/num_examples=2472, test/wer=0.208884, total_duration=10295.666694, train/ctc_loss=0.6133573055267334, train/wer=0.205301, validation/ctc_loss=0.9816338419914246, validation/num_examples=5348, validation/wer=0.277977
I0531 10:21:14.237730 139769546790656 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.8401744365692139, loss=1.93650221824646
I0531 10:23:41.208985 139769555183360 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.6584781408309937, loss=1.853602409362793
I0531 10:26:07.496805 139769546790656 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.314976692199707, loss=1.8118655681610107
I0531 10:28:35.419730 139769555183360 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.5844883918762207, loss=1.8518482446670532
I0531 10:31:01.703525 139769546790656 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.4178999662399292, loss=1.8531196117401123
I0531 10:33:28.529961 139769555183360 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.564652919769287, loss=1.867742896080017
I0531 10:35:55.141211 139769546790656 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.5425387620925903, loss=1.8959912061691284
I0531 10:38:21.927113 139769555183360 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.5071614980697632, loss=1.9157978296279907
I0531 10:40:52.079984 139768899823360 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.4688711166381836, loss=1.8355921506881714
I0531 10:43:18.201784 139768891430656 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.4925601482391357, loss=1.8272327184677124
I0531 10:45:45.934165 139768899823360 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.3215968608856201, loss=1.7504345178604126
I0531 10:48:13.604606 139768891430656 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.3653773069381714, loss=1.821784257888794
I0531 10:50:40.633300 139768899823360 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.4535871744155884, loss=1.8074322938919067
I0531 10:53:06.767504 139768891430656 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.532965064048767, loss=1.7896877527236938
I0531 10:55:33.463808 139768899823360 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.5506985187530518, loss=1.7859593629837036
I0531 10:58:01.073491 139768891430656 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.2116678953170776, loss=1.7718801498413086
I0531 11:00:25.838542 139927933790016 spec.py:298] Evaluating on the training split.
I0531 11:01:13.181565 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 11:01:57.174497 139927933790016 spec.py:326] Evaluating on the test split.
I0531 11:02:18.996713 139927933790016 submission_runner.py:426] Time since start: 12810.28s, 	Step: 8100, 	{'train/ctc_loss': Array(0.50916165, dtype=float32), 'train/wer': 0.17388397225423471, 'validation/ctc_loss': Array(0.87118906, dtype=float32), 'validation/wer': 0.24741193836891817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5485132, dtype=float32), 'test/wer': 0.1797574797391993, 'test/num_examples': 2472, 'score': 12071.380446434021, 'total_duration': 12810.280030488968, 'accumulated_submission_time': 12071.380446434021, 'accumulated_data_selection_time': 1026.037944316864, 'accumulated_eval_time': 738.5738236904144, 'accumulated_logging_time': 0.1824631690979004}
I0531 11:02:19.019785 139769985263360 logging_writer.py:48] [8100] accumulated_data_selection_time=1026.037944, accumulated_eval_time=738.573824, accumulated_logging_time=0.182463, accumulated_submission_time=12071.380446, global_step=8100, preemption_count=0, score=12071.380446, test/ctc_loss=0.5485131740570068, test/num_examples=2472, test/wer=0.179757, total_duration=12810.280030, train/ctc_loss=0.509161651134491, train/wer=0.173884, validation/ctc_loss=0.8711890578269958, validation/num_examples=5348, validation/wer=0.247412
I0531 11:02:20.570534 139769976870656 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.888221025466919, loss=1.7903300523757935
I0531 11:04:47.208554 139769985263360 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2248890399932861, loss=1.7741153240203857
I0531 11:07:17.614282 139769657583360 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.2604292631149292, loss=1.7526739835739136
I0531 11:09:44.470595 139769649190656 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.578016519546509, loss=1.710157036781311
I0531 11:12:11.762848 139769657583360 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.263023853302002, loss=1.789646863937378
I0531 11:14:37.466389 139769649190656 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.161956787109375, loss=1.7821508646011353
I0531 11:17:04.968610 139769657583360 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9984188079833984, loss=1.6862047910690308
I0531 11:19:32.704159 139769649190656 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.4598976373672485, loss=1.7142527103424072
I0531 11:21:58.383680 139769657583360 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.618381142616272, loss=1.7651935815811157
I0531 11:24:23.960044 139769649190656 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4934064149856567, loss=1.7586402893066406
I0531 11:26:49.988822 139769657583360 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1346076726913452, loss=1.7314409017562866
I0531 11:29:16.161820 139769649190656 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.4360743761062622, loss=1.7612274885177612
I0531 11:31:46.206209 139769985263360 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.6450673341751099, loss=1.6586240530014038
I0531 11:34:12.486048 139769976870656 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.4714704751968384, loss=1.6905040740966797
I0531 11:36:38.885567 139769985263360 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.6221323013305664, loss=1.7115398645401
I0531 11:39:04.918408 139769976870656 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.6704556941986084, loss=1.75209379196167
I0531 11:41:32.776094 139769985263360 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.9982529878616333, loss=1.7374012470245361
I0531 11:42:19.303018 139927933790016 spec.py:298] Evaluating on the training split.
I0531 11:43:07.050343 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 11:43:49.862582 139927933790016 spec.py:326] Evaluating on the test split.
I0531 11:44:11.794164 139927933790016 submission_runner.py:426] Time since start: 15323.08s, 	Step: 9733, 	{'train/ctc_loss': Array(0.4594445, dtype=float32), 'train/wer': 0.15516588335084902, 'validation/ctc_loss': Array(0.7804235, dtype=float32), 'validation/wer': 0.22362010246119113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47423607, dtype=float32), 'test/wer': 0.15463205573497452, 'test/num_examples': 2472, 'score': 14471.62058520317, 'total_duration': 15323.080006361008, 'accumulated_submission_time': 14471.62058520317, 'accumulated_data_selection_time': 1249.297127008438, 'accumulated_eval_time': 851.0626120567322, 'accumulated_logging_time': 0.22306275367736816}
I0531 11:44:11.816715 139769985263360 logging_writer.py:48] [9733] accumulated_data_selection_time=1249.297127, accumulated_eval_time=851.062612, accumulated_logging_time=0.223063, accumulated_submission_time=14471.620585, global_step=9733, preemption_count=0, score=14471.620585, test/ctc_loss=0.47423607110977173, test/num_examples=2472, test/wer=0.154632, total_duration=15323.080006, train/ctc_loss=0.45944449305534363, train/wer=0.155166, validation/ctc_loss=0.7804235219955444, validation/num_examples=5348, validation/wer=0.223620
I0531 11:45:52.096846 139769976870656 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.3100553750991821, loss=1.739362120628357
I0531 11:48:18.586550 139769985263360 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.0648841857910156, loss=1.6722910404205322
I0531 11:50:46.301874 139769976870656 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.096769094467163, loss=1.721927285194397
I0531 11:53:14.655869 139769985263360 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.5196601152420044, loss=1.67822265625
I0531 11:55:43.234107 139769976870656 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.1495896577835083, loss=1.6620749235153198
I0531 11:58:13.655618 139769985263360 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.3676131963729858, loss=1.648119568824768
I0531 12:00:40.078444 139769976870656 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.7321213483810425, loss=1.6560715436935425
I0531 12:03:05.959434 139769985263360 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0566883087158203, loss=1.641133189201355
I0531 12:05:32.464875 139769976870656 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.6742713451385498, loss=1.6425206661224365
I0531 12:07:59.577539 139769985263360 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.1920387744903564, loss=1.6567074060440063
I0531 12:10:25.939335 139769976870656 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.980521559715271, loss=1.7052477598190308
I0531 12:12:54.032381 139769985263360 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.4701852798461914, loss=1.5941333770751953
I0531 12:15:20.862381 139769976870656 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.4861302375793457, loss=1.6182501316070557
I0531 12:17:47.311112 139769985263360 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.3895480632781982, loss=1.6598021984100342
I0531 12:20:13.973674 139769976870656 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.816245436668396, loss=1.6843218803405762
I0531 12:22:39.943545 139769985263360 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.3666863441467285, loss=1.597825527191162
I0531 12:24:12.650108 139927933790016 spec.py:298] Evaluating on the training split.
I0531 12:25:00.520655 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 12:25:44.770270 139927933790016 spec.py:326] Evaluating on the test split.
I0531 12:26:06.828715 139927933790016 submission_runner.py:426] Time since start: 17838.11s, 	Step: 11362, 	{'train/ctc_loss': Array(0.40929842, dtype=float32), 'train/wer': 0.14108863505838812, 'validation/ctc_loss': Array(0.7481957, dtype=float32), 'validation/wer': 0.21556406718829896, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45182276, dtype=float32), 'test/wer': 0.15184936932545243, 'test/num_examples': 2472, 'score': 16872.411393880844, 'total_duration': 17838.113131284714, 'accumulated_submission_time': 16872.411393880844, 'accumulated_data_selection_time': 1473.6049880981445, 'accumulated_eval_time': 965.2376129627228, 'accumulated_logging_time': 0.2613377571105957}
I0531 12:26:06.850508 139769985263360 logging_writer.py:48] [11362] accumulated_data_selection_time=1473.604988, accumulated_eval_time=965.237613, accumulated_logging_time=0.261338, accumulated_submission_time=16872.411394, global_step=11362, preemption_count=0, score=16872.411394, test/ctc_loss=0.45182275772094727, test/num_examples=2472, test/wer=0.151849, total_duration=17838.113131, train/ctc_loss=0.4092984199523926, train/wer=0.141089, validation/ctc_loss=0.7481957077980042, validation/num_examples=5348, validation/wer=0.215564
I0531 12:27:04.546280 139769976870656 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.5598170757293701, loss=1.6308486461639404
I0531 12:29:31.887083 139769985263360 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.3774352073669434, loss=1.6379868984222412
I0531 12:31:58.051198 139769976870656 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.3807884454727173, loss=1.6808255910873413
I0531 12:34:24.937075 139769985263360 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.471205234527588, loss=1.6366968154907227
I0531 12:36:52.774648 139769976870656 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.9309632778167725, loss=1.643782615661621
I0531 12:39:19.157484 139769985263360 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.3808035850524902, loss=1.6748661994934082
I0531 12:41:45.952711 139769976870656 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.5644463300704956, loss=1.6035505533218384
I0531 12:44:12.878555 139769985263360 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.0430359840393066, loss=1.6191436052322388
I0531 12:46:40.116522 139769976870656 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.115375518798828, loss=1.6202691793441772
I0531 12:49:07.805025 139769985263360 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.5367331504821777, loss=1.6849923133850098
I0531 12:51:38.064044 139769985263360 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.2937521934509277, loss=1.572823166847229
I0531 12:54:04.401904 139769976870656 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.430863857269287, loss=1.6011459827423096
I0531 12:56:30.306004 139769985263360 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.436389446258545, loss=1.6056768894195557
I0531 12:58:57.037403 139769976870656 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4996051788330078, loss=1.5934075117111206
I0531 13:01:24.059522 139769985263360 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.160553216934204, loss=1.582902193069458
I0531 13:03:50.643060 139769976870656 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9517000913619995, loss=1.5196224451065063
I0531 13:06:07.348066 139927933790016 spec.py:298] Evaluating on the training split.
I0531 13:06:55.865387 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 13:07:39.511811 139927933790016 spec.py:326] Evaluating on the test split.
I0531 13:08:02.173178 139927933790016 submission_runner.py:426] Time since start: 20353.46s, 	Step: 12994, 	{'train/ctc_loss': Array(0.34094182, dtype=float32), 'train/wer': 0.12036194388829663, 'validation/ctc_loss': Array(0.69502985, dtype=float32), 'validation/wer': 0.20053256664319, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4182817, dtype=float32), 'test/wer': 0.13905307415757723, 'test/num_examples': 2472, 'score': 19272.86445093155, 'total_duration': 20353.457109689713, 'accumulated_submission_time': 19272.86445093155, 'accumulated_data_selection_time': 1693.9201724529266, 'accumulated_eval_time': 1080.058475971222, 'accumulated_logging_time': 0.3004932403564453}
I0531 13:08:02.199465 139769693423360 logging_writer.py:48] [12994] accumulated_data_selection_time=1693.920172, accumulated_eval_time=1080.058476, accumulated_logging_time=0.300493, accumulated_submission_time=19272.864451, global_step=12994, preemption_count=0, score=19272.864451, test/ctc_loss=0.4182817041873932, test/num_examples=2472, test/wer=0.139053, total_duration=20353.457110, train/ctc_loss=0.34094181656837463, train/wer=0.120362, validation/ctc_loss=0.6950298547744751, validation/num_examples=5348, validation/wer=0.200533
I0531 13:08:12.449380 139769685030656 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.191031813621521, loss=1.6286063194274902
I0531 13:10:38.660174 139769693423360 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.1495171785354614, loss=1.5507676601409912
I0531 13:13:04.293656 139769685030656 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.3663142919540405, loss=1.5829973220825195
I0531 13:15:30.951967 139769693423360 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.6380069255828857, loss=1.6368026733398438
I0531 13:18:00.903991 139769693423360 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.1215147972106934, loss=1.574305534362793
I0531 13:20:27.152863 139769685030656 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.4755789041519165, loss=1.5471080541610718
I0531 13:22:53.544691 139769693423360 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.2501496076583862, loss=1.5800838470458984
I0531 13:25:20.304119 139769685030656 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.5310752391815186, loss=1.587958574295044
I0531 13:27:46.587267 139769693423360 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.735449194908142, loss=1.81710684299469
I0531 13:30:12.696082 139769685030656 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3017549514770508, loss=1.586307406425476
I0531 13:32:39.151897 139769693423360 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.603484869003296, loss=1.6647605895996094
I0531 13:35:05.107178 139769685030656 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.660000801086426, loss=1.7144027948379517
I0531 13:37:31.267136 139769693423360 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.214571475982666, loss=1.637981653213501
I0531 13:39:57.955324 139769685030656 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.536102056503296, loss=1.5069931745529175
I0531 13:42:23.679873 139769693423360 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4481843709945679, loss=1.5693151950836182
I0531 13:44:55.028067 139769693423360 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.534696340560913, loss=1.636013388633728
I0531 13:47:22.899140 139769685030656 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.7942562103271484, loss=1.631529450416565
I0531 13:48:02.557127 139927933790016 spec.py:298] Evaluating on the training split.
I0531 13:48:50.264477 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 13:49:35.079859 139927933790016 spec.py:326] Evaluating on the test split.
I0531 13:49:56.836694 139927933790016 submission_runner.py:426] Time since start: 22868.12s, 	Step: 14628, 	{'train/ctc_loss': Array(0.3267977, dtype=float32), 'train/wer': 0.11358255882618942, 'validation/ctc_loss': Array(0.6798721, dtype=float32), 'validation/wer': 0.19742592789124835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40910178, dtype=float32), 'test/wer': 0.13326427396258608, 'test/num_examples': 2472, 'score': 21673.175567150116, 'total_duration': 22868.120924949646, 'accumulated_submission_time': 21673.175567150116, 'accumulated_data_selection_time': 1919.8110327720642, 'accumulated_eval_time': 1194.3340756893158, 'accumulated_logging_time': 0.34671926498413086}
I0531 13:49:56.863876 139769693423360 logging_writer.py:48] [14628] accumulated_data_selection_time=1919.811033, accumulated_eval_time=1194.334076, accumulated_logging_time=0.346719, accumulated_submission_time=21673.175567, global_step=14628, preemption_count=0, score=21673.175567, test/ctc_loss=0.40910178422927856, test/num_examples=2472, test/wer=0.133264, total_duration=22868.120925, train/ctc_loss=0.3267976939678192, train/wer=0.113583, validation/ctc_loss=0.6798720955848694, validation/num_examples=5348, validation/wer=0.197426
I0531 13:51:43.747150 139769685030656 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.2734075784683228, loss=1.5699577331542969
I0531 13:54:09.647304 139769693423360 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.0856504440307617, loss=1.6283632516860962
I0531 13:56:36.446278 139769685030656 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.7997182607650757, loss=1.608864426612854
I0531 13:59:03.143435 139769693423360 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.5924307107925415, loss=1.577867865562439
I0531 14:01:29.057869 139769685030656 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.3397880792617798, loss=1.4935927391052246
I0531 14:03:55.832576 139769693423360 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.4289120435714722, loss=1.5447394847869873
I0531 14:06:22.578775 139769685030656 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.1791353225708008, loss=1.5519980192184448
I0531 14:08:48.702196 139769693423360 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5517475605010986, loss=1.5341931581497192
I0531 14:11:18.610808 139769693423360 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5722836256027222, loss=1.5417001247406006
I0531 14:13:45.283262 139769685030656 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.283316731452942, loss=1.5737360715866089
I0531 14:16:11.367289 139769693423360 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.969574213027954, loss=1.5632786750793457
I0531 14:18:38.322001 139769685030656 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.2767528295516968, loss=1.5405042171478271
I0531 14:21:04.390954 139769693423360 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.3481090068817139, loss=1.5312494039535522
I0531 14:23:29.575025 139927933790016 spec.py:298] Evaluating on the training split.
I0531 14:24:17.596484 139927933790016 spec.py:310] Evaluating on the validation split.
I0531 14:25:02.325995 139927933790016 spec.py:326] Evaluating on the test split.
I0531 14:25:24.773512 139927933790016 submission_runner.py:426] Time since start: 24996.06s, 	Step: 16000, 	{'train/ctc_loss': Array(0.3210961, dtype=float32), 'train/wer': 0.1135107016254887, 'validation/ctc_loss': Array(0.65960807, dtype=float32), 'validation/wer': 0.19033468726181632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38977787, dtype=float32), 'test/wer': 0.1265005179452806, 'test/num_examples': 2472, 'score': 23685.848528385162, 'total_duration': 24996.059401750565, 'accumulated_submission_time': 23685.848528385162, 'accumulated_data_selection_time': 2108.2067000865936, 'accumulated_eval_time': 1309.5302565097809, 'accumulated_logging_time': 0.3883380889892578}
I0531 14:25:24.797136 139769693423360 logging_writer.py:48] [16000] accumulated_data_selection_time=2108.206700, accumulated_eval_time=1309.530257, accumulated_logging_time=0.388338, accumulated_submission_time=23685.848528, global_step=16000, preemption_count=0, score=23685.848528, test/ctc_loss=0.38977786898612976, test/num_examples=2472, test/wer=0.126501, total_duration=24996.059402, train/ctc_loss=0.3210960924625397, train/wer=0.113511, validation/ctc_loss=0.6596080660820007, validation/num_examples=5348, validation/wer=0.190335
I0531 14:25:24.821201 139769685030656 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=23685.848528
I0531 14:25:24.998629 139927933790016 checkpoints.py:490] Saving checkpoint at step: 16000
I0531 14:25:25.948045 139927933790016 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0531 14:25:25.971475 139927933790016 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_jax_upgrade_b/sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0531 14:25:27.268315 139927933790016 submission_runner.py:589] Tuning trial 1/1
I0531 14:25:27.268547 139927933790016 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0531 14:25:27.275615 139927933790016 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.706896, dtype=float32), 'train/wer': 3.5298398683963934, 'validation/ctc_loss': Array(30.587967, dtype=float32), 'validation/wer': 3.3663711179075535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.682737, dtype=float32), 'test/wer': 3.3815327117989966, 'test/num_examples': 2472, 'score': 67.54744935035706, 'total_duration': 281.0784583091736, 'accumulated_submission_time': 67.54744935035706, 'accumulated_data_selection_time': 4.842672824859619, 'accumulated_eval_time': 213.53076004981995, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1590, {'train/ctc_loss': Array(6.1107783, dtype=float32), 'train/wer': 0.9444553150295566, 'validation/ctc_loss': Array(6.081579, dtype=float32), 'validation/wer': 0.8959179538635201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9402027, dtype=float32), 'test/wer': 0.8994779924034693, 'test/num_examples': 2472, 'score': 2467.9976222515106, 'total_duration': 2766.396758556366, 'accumulated_submission_time': 2467.9976222515106, 'accumulated_data_selection_time': 177.5426948070526, 'accumulated_eval_time': 298.3391728401184, 'accumulated_logging_time': 0.033190011978149414, 'global_step': 1590, 'preemption_count': 0}), (3203, {'train/ctc_loss': Array(4.1658964, dtype=float32), 'train/wer': 0.7598433795914019, 'validation/ctc_loss': Array(4.354117, dtype=float32), 'validation/wer': 0.7678221690513174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.0394535, dtype=float32), 'test/wer': 0.7351979363435094, 'test/num_examples': 2472, 'score': 4868.97563791275, 'total_duration': 5269.2203540802, 'accumulated_submission_time': 4868.97563791275, 'accumulated_data_selection_time': 380.6711537837982, 'accumulated_eval_time': 400.12002420425415, 'accumulated_logging_time': 0.07041501998901367, 'global_step': 3203, 'preemption_count': 0}), (4834, {'train/ctc_loss': Array(0.8117652, dtype=float32), 'train/wer': 0.26127636246712105, 'validation/ctc_loss': Array(1.2305324, dtype=float32), 'validation/wer': 0.33435923163754594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.86933064, dtype=float32), 'test/wer': 0.26602075843438344, 'test/num_examples': 2472, 'score': 7269.083954572678, 'total_duration': 7781.911602497101, 'accumulated_submission_time': 7269.083954572678, 'accumulated_data_selection_time': 590.3777830600739, 'accumulated_eval_time': 512.636884689331, 'accumulated_logging_time': 0.10801815986633301, 'global_step': 4834, 'preemption_count': 0}), (6467, {'train/ctc_loss': Array(0.6133573, dtype=float32), 'train/wer': 0.2053011341620501, 'validation/ctc_loss': Array(0.98163384, dtype=float32), 'validation/wer': 0.2779766326737354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64855486, dtype=float32), 'test/wer': 0.20888428493083908, 'test/num_examples': 2472, 'score': 9669.987254142761, 'total_duration': 10295.666693925858, 'accumulated_submission_time': 9669.987254142761, 'accumulated_data_selection_time': 808.2260210514069, 'accumulated_eval_time': 625.4205026626587, 'accumulated_logging_time': 0.14506316184997559, 'global_step': 6467, 'preemption_count': 0}), (8100, {'train/ctc_loss': Array(0.50916165, dtype=float32), 'train/wer': 0.17388397225423471, 'validation/ctc_loss': Array(0.87118906, dtype=float32), 'validation/wer': 0.24741193836891817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5485132, dtype=float32), 'test/wer': 0.1797574797391993, 'test/num_examples': 2472, 'score': 12071.380446434021, 'total_duration': 12810.280030488968, 'accumulated_submission_time': 12071.380446434021, 'accumulated_data_selection_time': 1026.037944316864, 'accumulated_eval_time': 738.5738236904144, 'accumulated_logging_time': 0.1824631690979004, 'global_step': 8100, 'preemption_count': 0}), (9733, {'train/ctc_loss': Array(0.4594445, dtype=float32), 'train/wer': 0.15516588335084902, 'validation/ctc_loss': Array(0.7804235, dtype=float32), 'validation/wer': 0.22362010246119113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47423607, dtype=float32), 'test/wer': 0.15463205573497452, 'test/num_examples': 2472, 'score': 14471.62058520317, 'total_duration': 15323.080006361008, 'accumulated_submission_time': 14471.62058520317, 'accumulated_data_selection_time': 1249.297127008438, 'accumulated_eval_time': 851.0626120567322, 'accumulated_logging_time': 0.22306275367736816, 'global_step': 9733, 'preemption_count': 0}), (11362, {'train/ctc_loss': Array(0.40929842, dtype=float32), 'train/wer': 0.14108863505838812, 'validation/ctc_loss': Array(0.7481957, dtype=float32), 'validation/wer': 0.21556406718829896, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45182276, dtype=float32), 'test/wer': 0.15184936932545243, 'test/num_examples': 2472, 'score': 16872.411393880844, 'total_duration': 17838.113131284714, 'accumulated_submission_time': 16872.411393880844, 'accumulated_data_selection_time': 1473.6049880981445, 'accumulated_eval_time': 965.2376129627228, 'accumulated_logging_time': 0.2613377571105957, 'global_step': 11362, 'preemption_count': 0}), (12994, {'train/ctc_loss': Array(0.34094182, dtype=float32), 'train/wer': 0.12036194388829663, 'validation/ctc_loss': Array(0.69502985, dtype=float32), 'validation/wer': 0.20053256664319, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4182817, dtype=float32), 'test/wer': 0.13905307415757723, 'test/num_examples': 2472, 'score': 19272.86445093155, 'total_duration': 20353.457109689713, 'accumulated_submission_time': 19272.86445093155, 'accumulated_data_selection_time': 1693.9201724529266, 'accumulated_eval_time': 1080.058475971222, 'accumulated_logging_time': 0.3004932403564453, 'global_step': 12994, 'preemption_count': 0}), (14628, {'train/ctc_loss': Array(0.3267977, dtype=float32), 'train/wer': 0.11358255882618942, 'validation/ctc_loss': Array(0.6798721, dtype=float32), 'validation/wer': 0.19742592789124835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40910178, dtype=float32), 'test/wer': 0.13326427396258608, 'test/num_examples': 2472, 'score': 21673.175567150116, 'total_duration': 22868.120924949646, 'accumulated_submission_time': 21673.175567150116, 'accumulated_data_selection_time': 1919.8110327720642, 'accumulated_eval_time': 1194.3340756893158, 'accumulated_logging_time': 0.34671926498413086, 'global_step': 14628, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.3210961, dtype=float32), 'train/wer': 0.1135107016254887, 'validation/ctc_loss': Array(0.65960807, dtype=float32), 'validation/wer': 0.19033468726181632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38977787, dtype=float32), 'test/wer': 0.1265005179452806, 'test/num_examples': 2472, 'score': 23685.848528385162, 'total_duration': 24996.059401750565, 'accumulated_submission_time': 23685.848528385162, 'accumulated_data_selection_time': 2108.2067000865936, 'accumulated_eval_time': 1309.5302565097809, 'accumulated_logging_time': 0.3883380889892578, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0531 14:25:27.275777 139927933790016 submission_runner.py:592] Timing: 23685.848528385162
I0531 14:25:27.275856 139927933790016 submission_runner.py:593] ====================
I0531 14:25:27.277553 139927933790016 submission_runner.py:661] Final librispeech_deepspeech score: 23685.848528385162
