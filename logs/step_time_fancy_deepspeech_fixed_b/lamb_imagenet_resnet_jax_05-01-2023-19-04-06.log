python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_05-01-2023-19-04-06.log
I0501 19:04:28.064049 140676718925632 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax.
I0501 19:04:28.136492 140676718925632 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 19:04:29.007766 140676718925632 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0501 19:04:29.008496 140676718925632 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 19:04:29.012338 140676718925632 submission_runner.py:538] Using RNG seed 880148209
I0501 19:04:31.694814 140676718925632 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 19:04:31.695024 140676718925632 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1.
I0501 19:04:31.695324 140676718925632 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1/hparams.json.
I0501 19:04:31.818491 140676718925632 submission_runner.py:241] Initializing dataset.
I0501 19:04:31.830796 140676718925632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:04:31.837948 140676718925632 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:04:31.838089 140676718925632 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:04:32.098362 140676718925632 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:04:33.163227 140676718925632 submission_runner.py:248] Initializing model.
I0501 19:04:44.709448 140676718925632 submission_runner.py:258] Initializing optimizer.
I0501 19:04:45.838700 140676718925632 submission_runner.py:265] Initializing metrics bundle.
I0501 19:04:45.838896 140676718925632 submission_runner.py:282] Initializing checkpoint and logger.
I0501 19:04:45.840029 140676718925632 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0501 19:04:46.799443 140676718925632 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0501 19:04:46.800454 140676718925632 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1/flags_0.json.
I0501 19:04:46.805681 140676718925632 submission_runner.py:318] Starting training loop.
I0501 19:05:53.095168 140492096399104 logging_writer.py:48] [0] global_step=0, grad_norm=0.5898441076278687, loss=6.924770832061768
I0501 19:05:53.111204 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:05:53.610371 140676718925632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:05:53.617208 140676718925632 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:05:53.617322 140676718925632 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:05:53.678282 140676718925632 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:06:05.995395 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:06:06.743070 140676718925632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:06:06.763054 140676718925632 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:06:06.763416 140676718925632 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:06:06.828340 140676718925632 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:06:25.432012 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:06:25.853983 140676718925632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:06:25.858757 140676718925632 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0501 19:06:25.889455 140676718925632 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:06:35.112787 140676718925632 submission_runner.py:415] Time since start: 108.31s, 	Step: 1, 	{'train/accuracy': 0.0008370535797439516, 'train/loss': 6.911266803741455, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.911518096923828, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.9111247062683105, 'test/num_examples': 10000, 'score': 66.30537223815918, 'total_duration': 108.30703520774841, 'accumulated_submission_time': 66.30537223815918, 'accumulated_eval_time': 42.00152921676636, 'accumulated_logging_time': 0}
I0501 19:06:35.128822 140472022439680 logging_writer.py:48] [1] accumulated_eval_time=42.001529, accumulated_logging_time=0, accumulated_submission_time=66.305372, global_step=1, preemption_count=0, score=66.305372, test/accuracy=0.001000, test/loss=6.911125, test/num_examples=10000, total_duration=108.307035, train/accuracy=0.000837, train/loss=6.911267, validation/accuracy=0.001000, validation/loss=6.911518, validation/num_examples=50000
I0501 19:07:11.442530 140472030832384 logging_writer.py:48] [100] global_step=100, grad_norm=0.6080451011657715, loss=6.919145584106445
I0501 19:07:47.859312 140472022439680 logging_writer.py:48] [200] global_step=200, grad_norm=0.608105480670929, loss=6.908480167388916
I0501 19:08:24.179040 140472030832384 logging_writer.py:48] [300] global_step=300, grad_norm=0.6072322726249695, loss=6.906421184539795
I0501 19:09:00.561864 140472022439680 logging_writer.py:48] [400] global_step=400, grad_norm=0.6191819310188293, loss=6.877525329589844
I0501 19:09:36.896223 140472030832384 logging_writer.py:48] [500] global_step=500, grad_norm=0.6262691617012024, loss=6.860269546508789
I0501 19:10:13.257313 140472022439680 logging_writer.py:48] [600] global_step=600, grad_norm=0.6690092086791992, loss=6.820385456085205
I0501 19:10:49.527110 140472030832384 logging_writer.py:48] [700] global_step=700, grad_norm=0.671281099319458, loss=6.796656608581543
I0501 19:11:25.852191 140472022439680 logging_writer.py:48] [800] global_step=800, grad_norm=0.7192108035087585, loss=6.731367111206055
I0501 19:12:02.197364 140472030832384 logging_writer.py:48] [900] global_step=900, grad_norm=0.7532675862312317, loss=6.699277400970459
I0501 19:12:38.579473 140472022439680 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7519726753234863, loss=6.678204536437988
I0501 19:13:14.988342 140472030832384 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8108140230178833, loss=6.656481742858887
I0501 19:13:51.234467 140472022439680 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8007329702377319, loss=6.610157012939453
I0501 19:14:27.560641 140472030832384 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8424469232559204, loss=6.587204933166504
I0501 19:15:03.834795 140472022439680 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8568258881568909, loss=6.544328212738037
I0501 19:15:05.149640 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:15:12.143850 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:15:20.090221 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:15:22.464097 140676718925632 submission_runner.py:415] Time since start: 635.66s, 	Step: 1405, 	{'train/accuracy': 0.022002549842000008, 'train/loss': 6.4482831954956055, 'validation/accuracy': 0.020099999383091927, 'validation/loss': 6.47771692276001, 'validation/num_examples': 50000, 'test/accuracy': 0.01510000042617321, 'test/loss': 6.542073726654053, 'test/num_examples': 10000, 'score': 576.3000509738922, 'total_duration': 635.6583120822906, 'accumulated_submission_time': 576.3000509738922, 'accumulated_eval_time': 59.315924644470215, 'accumulated_logging_time': 0.02403879165649414}
I0501 19:15:22.474085 140472299267840 logging_writer.py:48] [1405] accumulated_eval_time=59.315925, accumulated_logging_time=0.024039, accumulated_submission_time=576.300051, global_step=1405, preemption_count=0, score=576.300051, test/accuracy=0.015100, test/loss=6.542074, test/num_examples=10000, total_duration=635.658312, train/accuracy=0.022003, train/loss=6.448283, validation/accuracy=0.020100, validation/loss=6.477717, validation/num_examples=50000
I0501 19:15:57.312532 140472307660544 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9079097509384155, loss=6.521961212158203
I0501 19:16:33.586594 140472299267840 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8985452651977539, loss=6.481339454650879
I0501 19:17:09.899296 140472307660544 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9321203827857971, loss=6.464334964752197
I0501 19:17:46.226788 140472299267840 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9512279629707336, loss=6.4240922927856445
I0501 19:18:22.630754 140472307660544 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.010776400566101, loss=6.409064769744873
I0501 19:18:58.885659 140472299267840 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0466175079345703, loss=6.3710737228393555
I0501 19:19:35.221966 140472307660544 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0991737842559814, loss=6.349830627441406
I0501 19:20:11.487709 140472299267840 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.339537262916565, loss=6.297175407409668
I0501 19:20:47.783357 140472307660544 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1230225563049316, loss=6.251193046569824
I0501 19:21:24.097886 140472299267840 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2080186605453491, loss=6.31742000579834
I0501 19:22:00.449438 140472307660544 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.742030382156372, loss=6.256336688995361
I0501 19:22:36.821579 140472299267840 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.8078218698501587, loss=6.166554927825928
I0501 19:23:13.160042 140472307660544 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.7844663858413696, loss=6.1738739013671875
I0501 19:23:49.473978 140472299267840 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.4594461917877197, loss=6.1268534660339355
I0501 19:23:52.582416 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:23:59.567720 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:24:07.773408 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:24:10.099270 140676718925632 submission_runner.py:415] Time since start: 1163.29s, 	Step: 2810, 	{'train/accuracy': 0.0489676333963871, 'train/loss': 5.865507125854492, 'validation/accuracy': 0.04623999819159508, 'validation/loss': 5.9181036949157715, 'validation/num_examples': 50000, 'test/accuracy': 0.0333000011742115, 'test/loss': 6.073990345001221, 'test/num_examples': 10000, 'score': 1086.3819971084595, 'total_duration': 1163.2935228347778, 'accumulated_submission_time': 1086.3819971084595, 'accumulated_eval_time': 76.8327488899231, 'accumulated_logging_time': 0.04265332221984863}
I0501 19:24:10.107289 140472307660544 logging_writer.py:48] [2810] accumulated_eval_time=76.832749, accumulated_logging_time=0.042653, accumulated_submission_time=1086.381997, global_step=2810, preemption_count=0, score=1086.381997, test/accuracy=0.033300, test/loss=6.073990, test/num_examples=10000, total_duration=1163.293523, train/accuracy=0.048968, train/loss=5.865507, validation/accuracy=0.046240, validation/loss=5.918104, validation/num_examples=50000
I0501 19:24:43.149948 140472299267840 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.581201434135437, loss=6.104002952575684
I0501 19:25:19.539505 140472307660544 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.7607471942901611, loss=6.062653541564941
I0501 19:25:55.856949 140472299267840 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.7989023923873901, loss=6.07127571105957
I0501 19:26:32.209501 140472307660544 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.072119951248169, loss=5.967592239379883
I0501 19:27:08.557011 140472299267840 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.2219035625457764, loss=5.995143413543701
I0501 19:27:44.947510 140472307660544 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.7263169288635254, loss=5.9678473472595215
I0501 19:28:21.358607 140472299267840 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.9028608798980713, loss=5.989656448364258
I0501 19:28:57.729763 140472307660544 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.0125482082366943, loss=5.92251443862915
I0501 19:29:34.106764 140472299267840 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.7809767723083496, loss=5.901666164398193
I0501 19:30:10.522495 140472307660544 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.703212022781372, loss=5.863755702972412
I0501 19:30:46.869879 140472299267840 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.1562917232513428, loss=5.924269676208496
I0501 19:31:23.254813 140472307660544 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.05666446685791, loss=5.916095733642578
I0501 19:31:59.646638 140472299267840 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.5978171825408936, loss=5.891263961791992
I0501 19:32:36.013285 140472307660544 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.419992446899414, loss=5.81928825378418
I0501 19:32:40.233829 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:32:47.215563 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:32:55.315421 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:32:57.548629 140676718925632 submission_runner.py:415] Time since start: 1690.74s, 	Step: 4213, 	{'train/accuracy': 0.07920120656490326, 'train/loss': 5.4193572998046875, 'validation/accuracy': 0.07215999811887741, 'validation/loss': 5.520287990570068, 'validation/num_examples': 50000, 'test/accuracy': 0.04930000379681587, 'test/loss': 5.755897521972656, 'test/num_examples': 10000, 'score': 1596.4833886623383, 'total_duration': 1690.7428572177887, 'accumulated_submission_time': 1596.4833886623383, 'accumulated_eval_time': 94.14749574661255, 'accumulated_logging_time': 0.05832695960998535}
I0501 19:32:57.558337 140472299267840 logging_writer.py:48] [4213] accumulated_eval_time=94.147496, accumulated_logging_time=0.058327, accumulated_submission_time=1596.483389, global_step=4213, preemption_count=0, score=1596.483389, test/accuracy=0.049300, test/loss=5.755898, test/num_examples=10000, total_duration=1690.742857, train/accuracy=0.079201, train/loss=5.419357, validation/accuracy=0.072160, validation/loss=5.520288, validation/num_examples=50000
I0501 19:33:29.555548 140472307660544 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.040412187576294, loss=5.880010604858398
I0501 19:34:05.949383 140472299267840 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.385118007659912, loss=5.717600345611572
I0501 19:34:42.314654 140472307660544 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.054950475692749, loss=5.793889045715332
I0501 19:35:18.738652 140472299267840 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.6231002807617188, loss=5.853590965270996
I0501 19:35:55.126779 140472307660544 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.5865585803985596, loss=5.752646446228027
I0501 19:36:31.507732 140472299267840 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.284256935119629, loss=5.741421699523926
I0501 19:37:07.909815 140472307660544 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.803415536880493, loss=5.708470344543457
I0501 19:37:44.274461 140472299267840 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.8852851390838623, loss=5.797403335571289
I0501 19:38:20.668454 140472307660544 logging_writer.py:48] [5100] global_step=5100, grad_norm=4.234422206878662, loss=5.6877851486206055
I0501 19:38:57.051202 140472299267840 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.1597635746002197, loss=5.729913711547852
I0501 19:39:33.391825 140472307660544 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.8951427936553955, loss=5.663373947143555
I0501 19:40:09.776645 140472299267840 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.612860679626465, loss=5.719830513000488
I0501 19:40:46.161241 140472307660544 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.518707036972046, loss=5.705555438995361
I0501 19:41:22.561632 140472299267840 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.0079619884490967, loss=5.698276996612549
I0501 19:41:27.874166 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:41:34.751622 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:41:42.797019 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:41:44.863659 140676718925632 submission_runner.py:415] Time since start: 2218.06s, 	Step: 5616, 	{'train/accuracy': 0.09773596376180649, 'train/loss': 5.190073013305664, 'validation/accuracy': 0.0894399955868721, 'validation/loss': 5.286595344543457, 'validation/num_examples': 50000, 'test/accuracy': 0.061500001698732376, 'test/loss': 5.575964450836182, 'test/num_examples': 10000, 'score': 2106.7729983329773, 'total_duration': 2218.0579133033752, 'accumulated_submission_time': 2106.7729983329773, 'accumulated_eval_time': 111.136967420578, 'accumulated_logging_time': 0.07622694969177246}
I0501 19:41:44.872666 140472307660544 logging_writer.py:48] [5616] accumulated_eval_time=111.136967, accumulated_logging_time=0.076227, accumulated_submission_time=2106.772998, global_step=5616, preemption_count=0, score=2106.772998, test/accuracy=0.061500, test/loss=5.575964, test/num_examples=10000, total_duration=2218.057913, train/accuracy=0.097736, train/loss=5.190073, validation/accuracy=0.089440, validation/loss=5.286595, validation/num_examples=50000
I0501 19:42:15.796583 140472299267840 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.74973201751709, loss=5.725687026977539
I0501 19:42:52.239766 140472307660544 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.189164638519287, loss=5.681784152984619
I0501 19:43:28.615586 140472299267840 logging_writer.py:48] [5900] global_step=5900, grad_norm=4.223130226135254, loss=5.721238613128662
I0501 19:44:05.008518 140472307660544 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.058981895446777, loss=5.61134672164917
I0501 19:44:41.371931 140472299267840 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.4560749530792236, loss=5.642194747924805
I0501 19:45:17.758851 140472307660544 logging_writer.py:48] [6200] global_step=6200, grad_norm=4.257322311401367, loss=5.619309902191162
I0501 19:45:54.163664 140472299267840 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.65415620803833, loss=5.650383949279785
I0501 19:46:30.540914 140472307660544 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.160146474838257, loss=5.673282623291016
I0501 19:47:06.912708 140472299267840 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.188194751739502, loss=5.659204959869385
I0501 19:47:43.306143 140472307660544 logging_writer.py:48] [6600] global_step=6600, grad_norm=4.211882591247559, loss=5.603768348693848
I0501 19:48:19.661464 140472299267840 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.32112455368042, loss=5.519346237182617
I0501 19:48:56.031871 140472307660544 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.0911736488342285, loss=5.587501049041748
I0501 19:49:32.362632 140472299267840 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.638421058654785, loss=5.564208984375
I0501 19:50:08.711917 140472307660544 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.3988614082336426, loss=5.646683692932129
I0501 19:50:15.116885 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:50:22.229213 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:50:30.220185 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:50:32.579040 140676718925632 submission_runner.py:415] Time since start: 2745.77s, 	Step: 7019, 	{'train/accuracy': 0.11292251199483871, 'train/loss': 5.01508092880249, 'validation/accuracy': 0.10227999836206436, 'validation/loss': 5.134513854980469, 'validation/num_examples': 50000, 'test/accuracy': 0.07129999995231628, 'test/loss': 5.489673614501953, 'test/num_examples': 10000, 'score': 2616.9900267124176, 'total_duration': 2745.7732882499695, 'accumulated_submission_time': 2616.9900267124176, 'accumulated_eval_time': 128.59909343719482, 'accumulated_logging_time': 0.09452009201049805}
I0501 19:50:32.588843 140472299267840 logging_writer.py:48] [7019] accumulated_eval_time=128.599093, accumulated_logging_time=0.094520, accumulated_submission_time=2616.990027, global_step=7019, preemption_count=0, score=2616.990027, test/accuracy=0.071300, test/loss=5.489674, test/num_examples=10000, total_duration=2745.773288, train/accuracy=0.112923, train/loss=5.015081, validation/accuracy=0.102280, validation/loss=5.134514, validation/num_examples=50000
I0501 19:51:02.386556 140472307660544 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.7874436378479004, loss=5.561683654785156
I0501 19:51:38.754501 140472299267840 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.4022653102874756, loss=5.517384052276611
I0501 19:52:15.247671 140472307660544 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.9210197925567627, loss=5.4744768142700195
I0501 19:52:51.600147 140472299267840 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.9554312229156494, loss=5.562705039978027
I0501 19:53:27.946732 140472307660544 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.574556827545166, loss=5.60693359375
I0501 19:54:04.306122 140472299267840 logging_writer.py:48] [7600] global_step=7600, grad_norm=5.282060146331787, loss=5.508238792419434
I0501 19:54:40.616818 140472307660544 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.971092462539673, loss=5.4968342781066895
I0501 19:55:16.938000 140472299267840 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.320079326629639, loss=5.576165199279785
I0501 19:55:53.307195 140472307660544 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.8768832683563232, loss=5.566433906555176
I0501 19:56:29.603861 140472299267840 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.222797393798828, loss=5.530340194702148
I0501 19:57:05.976101 140472307660544 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.812203407287598, loss=5.5443196296691895
I0501 19:57:42.358074 140472299267840 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.076280117034912, loss=5.48652458190918
I0501 19:58:18.747160 140472307660544 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.8645637035369873, loss=5.490948677062988
I0501 19:58:55.036245 140472299267840 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.6475143432617188, loss=5.511361122131348
I0501 19:59:02.881162 140676718925632 spec.py:298] Evaluating on the training split.
I0501 19:59:10.040587 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 19:59:18.735431 140676718925632 spec.py:326] Evaluating on the test split.
I0501 19:59:21.053184 140676718925632 submission_runner.py:415] Time since start: 3274.25s, 	Step: 8423, 	{'train/accuracy': 0.12922511994838715, 'train/loss': 4.895474910736084, 'validation/accuracy': 0.11219999939203262, 'validation/loss': 5.0270891189575195, 'validation/num_examples': 50000, 'test/accuracy': 0.07820000499486923, 'test/loss': 5.392067909240723, 'test/num_examples': 10000, 'score': 3127.257213830948, 'total_duration': 3274.2474353313446, 'accumulated_submission_time': 3127.257213830948, 'accumulated_eval_time': 146.77109003067017, 'accumulated_logging_time': 0.11184430122375488}
I0501 19:59:21.063762 140472307660544 logging_writer.py:48] [8423] accumulated_eval_time=146.771090, accumulated_logging_time=0.111844, accumulated_submission_time=3127.257214, global_step=8423, preemption_count=0, score=3127.257214, test/accuracy=0.078200, test/loss=5.392068, test/num_examples=10000, total_duration=3274.247435, train/accuracy=0.129225, train/loss=4.895475, validation/accuracy=0.112200, validation/loss=5.027089, validation/num_examples=50000
I0501 19:59:49.402897 140472299267840 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.4574382305145264, loss=5.508196830749512
I0501 20:00:25.726123 140472307660544 logging_writer.py:48] [8600] global_step=8600, grad_norm=4.767140865325928, loss=5.489736557006836
I0501 20:01:02.044184 140472299267840 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.794088125228882, loss=5.471899509429932
I0501 20:01:38.400924 140472307660544 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.25559139251709, loss=5.504571914672852
I0501 20:02:14.765227 140472299267840 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.74735689163208, loss=5.500454425811768
I0501 20:02:51.132526 140472307660544 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.2243218421936035, loss=5.426560401916504
I0501 20:03:27.558749 140472299267840 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.8842074871063232, loss=5.400545597076416
I0501 20:04:03.889485 140472307660544 logging_writer.py:48] [9200] global_step=9200, grad_norm=4.0076584815979, loss=5.442999839782715
I0501 20:04:40.167647 140472299267840 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.6791982650756836, loss=5.361910343170166
I0501 20:05:16.547798 140472307660544 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.054397106170654, loss=5.339298725128174
I0501 20:05:52.865800 140472299267840 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.5735392570495605, loss=5.3332719802856445
I0501 20:06:29.263722 140472307660544 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.9747464656829834, loss=5.278451442718506
I0501 20:07:05.647372 140472299267840 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.5271191596984863, loss=5.2194318771362305
I0501 20:07:41.991740 140472307660544 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.507648944854736, loss=5.301591873168945
I0501 20:07:51.264880 140676718925632 spec.py:298] Evaluating on the training split.
I0501 20:07:58.328237 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 20:08:08.312285 140676718925632 spec.py:326] Evaluating on the test split.
I0501 20:08:10.457750 140676718925632 submission_runner.py:415] Time since start: 3803.65s, 	Step: 9827, 	{'train/accuracy': 0.15684789419174194, 'train/loss': 4.54622745513916, 'validation/accuracy': 0.14093999564647675, 'validation/loss': 4.682422161102295, 'validation/num_examples': 50000, 'test/accuracy': 0.09760000556707382, 'test/loss': 5.122074127197266, 'test/num_examples': 10000, 'score': 3637.433202266693, 'total_duration': 3803.6520023345947, 'accumulated_submission_time': 3637.433202266693, 'accumulated_eval_time': 165.96393203735352, 'accumulated_logging_time': 0.1300067901611328}
I0501 20:08:10.466873 140472299267840 logging_writer.py:48] [9827] accumulated_eval_time=165.963932, accumulated_logging_time=0.130007, accumulated_submission_time=3637.433202, global_step=9827, preemption_count=0, score=3637.433202, test/accuracy=0.097600, test/loss=5.122074, test/num_examples=10000, total_duration=3803.652002, train/accuracy=0.156848, train/loss=4.546227, validation/accuracy=0.140940, validation/loss=4.682422, validation/num_examples=50000
I0501 20:08:37.368846 140472307660544 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.536506175994873, loss=5.187436580657959
I0501 20:09:13.820486 140472299267840 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.783751964569092, loss=5.131941795349121
I0501 20:09:50.195878 140472307660544 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.333194732666016, loss=5.075473308563232
I0501 20:10:26.518863 140472299267840 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.901160955429077, loss=5.116888046264648
I0501 20:11:02.857923 140472307660544 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.589017391204834, loss=5.0380120277404785
I0501 20:11:39.174671 140472299267840 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.970773220062256, loss=5.046563148498535
I0501 20:12:15.504228 140472307660544 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.6145496368408203, loss=4.922595024108887
I0501 20:12:51.897195 140472299267840 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.864382266998291, loss=4.960168838500977
I0501 20:13:28.305151 140472307660544 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.549732208251953, loss=4.885077476501465
I0501 20:14:04.683388 140472299267840 logging_writer.py:48] [10800] global_step=10800, grad_norm=4.729914665222168, loss=4.777037620544434
I0501 20:14:41.042962 140472307660544 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.947624206542969, loss=4.8236918449401855
I0501 20:15:17.327167 140472299267840 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.8466150760650635, loss=4.737906455993652
I0501 20:15:53.676901 140472307660544 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.8346993923187256, loss=4.734777927398682
I0501 20:16:30.060404 140472299267840 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.25782585144043, loss=4.6886796951293945
I0501 20:16:40.811174 140676718925632 spec.py:298] Evaluating on the training split.
I0501 20:16:47.987613 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 20:16:58.304304 140676718925632 spec.py:326] Evaluating on the test split.
I0501 20:17:00.455780 140676718925632 submission_runner.py:415] Time since start: 4333.65s, 	Step: 11231, 	{'train/accuracy': 0.24451929330825806, 'train/loss': 3.800215721130371, 'validation/accuracy': 0.22655999660491943, 'validation/loss': 3.9290382862091064, 'validation/num_examples': 50000, 'test/accuracy': 0.15820001065731049, 'test/loss': 4.473752021789551, 'test/num_examples': 10000, 'score': 4147.749961853027, 'total_duration': 4333.649039506912, 'accumulated_submission_time': 4147.749961853027, 'accumulated_eval_time': 185.60751867294312, 'accumulated_logging_time': 0.1493680477142334}
I0501 20:17:00.465254 140472307660544 logging_writer.py:48] [11231] accumulated_eval_time=185.607519, accumulated_logging_time=0.149368, accumulated_submission_time=4147.749962, global_step=11231, preemption_count=0, score=4147.749962, test/accuracy=0.158200, test/loss=4.473752, test/num_examples=10000, total_duration=4333.649040, train/accuracy=0.244519, train/loss=3.800216, validation/accuracy=0.226560, validation/loss=3.929038, validation/num_examples=50000
I0501 20:17:25.931825 140472299267840 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.20010232925415, loss=4.582611560821533
I0501 20:18:02.332728 140472307660544 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.693544387817383, loss=4.622683048248291
I0501 20:18:38.726759 140472299267840 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.7647647857666016, loss=4.4779767990112305
I0501 20:19:15.119461 140472307660544 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.008703708648682, loss=4.503256320953369
I0501 20:19:51.482030 140472299267840 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.484295845031738, loss=4.4958930015563965
I0501 20:20:27.864980 140472307660544 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.4555907249450684, loss=4.447880744934082
I0501 20:21:04.357273 140472299267840 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.005360126495361, loss=4.419392108917236
I0501 20:21:40.712489 140472307660544 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.634768962860107, loss=4.27936315536499
I0501 20:22:17.089562 140472299267840 logging_writer.py:48] [12100] global_step=12100, grad_norm=4.014038562774658, loss=4.300455570220947
I0501 20:22:53.458427 140472307660544 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.754941940307617, loss=4.475701332092285
I0501 20:23:29.785689 140472299267840 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.8833067417144775, loss=4.245048522949219
I0501 20:24:06.135215 140472307660544 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.975940704345703, loss=4.2347822189331055
I0501 20:24:42.513522 140472299267840 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.239037990570068, loss=4.298872947692871
I0501 20:25:18.838138 140472307660544 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.94099497795105, loss=4.1669111251831055
I0501 20:25:30.701799 140676718925632 spec.py:298] Evaluating on the training split.
I0501 20:25:38.692160 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 20:25:49.076569 140676718925632 spec.py:326] Evaluating on the test split.
I0501 20:25:51.338849 140676718925632 submission_runner.py:415] Time since start: 4864.53s, 	Step: 12634, 	{'train/accuracy': 0.31475207209587097, 'train/loss': 3.3164684772491455, 'validation/accuracy': 0.2825999855995178, 'validation/loss': 3.485480546951294, 'validation/num_examples': 50000, 'test/accuracy': 0.20740000903606415, 'test/loss': 4.1016974449157715, 'test/num_examples': 10000, 'score': 4657.960319757462, 'total_duration': 4864.53219294548, 'accumulated_submission_time': 4657.960319757462, 'accumulated_eval_time': 206.24364757537842, 'accumulated_logging_time': 0.1668405532836914}
I0501 20:25:51.353832 140472299267840 logging_writer.py:48] [12634] accumulated_eval_time=206.243648, accumulated_logging_time=0.166841, accumulated_submission_time=4657.960320, global_step=12634, preemption_count=0, score=4657.960320, test/accuracy=0.207400, test/loss=4.101697, test/num_examples=10000, total_duration=4864.532193, train/accuracy=0.314752, train/loss=3.316468, validation/accuracy=0.282600, validation/loss=3.485481, validation/num_examples=50000
I0501 20:26:15.737733 140472307660544 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.8036162853240967, loss=4.306506156921387
I0501 20:26:52.239083 140472299267840 logging_writer.py:48] [12800] global_step=12800, grad_norm=4.9371209144592285, loss=4.258546829223633
I0501 20:27:28.643306 140472307660544 logging_writer.py:48] [12900] global_step=12900, grad_norm=4.743399620056152, loss=4.181973457336426
I0501 20:28:05.005341 140472299267840 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.416057109832764, loss=4.130297660827637
I0501 20:28:41.364550 140472307660544 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.1284356117248535, loss=4.099146842956543
I0501 20:29:17.761936 140472299267840 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.449145793914795, loss=4.083386421203613
I0501 20:29:54.171338 140472307660544 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.110444068908691, loss=4.018104553222656
I0501 20:30:30.549834 140472299267840 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.654837131500244, loss=4.088961124420166
I0501 20:31:06.890846 140472307660544 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.71852445602417, loss=3.9471166133880615
I0501 20:31:43.268522 140472299267840 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.8621926307678223, loss=4.0381622314453125
I0501 20:32:19.620555 140472307660544 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.5832111835479736, loss=4.015253067016602
I0501 20:32:56.109308 140472299267840 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.5283126831054688, loss=3.8907861709594727
I0501 20:33:32.473421 140472307660544 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.161213397979736, loss=3.9847326278686523
I0501 20:34:08.852012 140472299267840 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.216670513153076, loss=3.9701762199401855
I0501 20:34:21.407653 140676718925632 spec.py:298] Evaluating on the training split.
I0501 20:34:28.711324 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 20:34:38.803100 140676718925632 spec.py:326] Evaluating on the test split.
I0501 20:34:40.893642 140676718925632 submission_runner.py:415] Time since start: 5394.09s, 	Step: 14036, 	{'train/accuracy': 0.3905054032802582, 'train/loss': 2.8370370864868164, 'validation/accuracy': 0.35986000299453735, 'validation/loss': 2.9939615726470947, 'validation/num_examples': 50000, 'test/accuracy': 0.2687000036239624, 'test/loss': 3.6394169330596924, 'test/num_examples': 10000, 'score': 5167.985907316208, 'total_duration': 5394.086795806885, 'accumulated_submission_time': 5167.985907316208, 'accumulated_eval_time': 225.72851371765137, 'accumulated_logging_time': 0.19259929656982422}
I0501 20:34:40.904778 140472307660544 logging_writer.py:48] [14036] accumulated_eval_time=225.728514, accumulated_logging_time=0.192599, accumulated_submission_time=5167.985907, global_step=14036, preemption_count=0, score=5167.985907, test/accuracy=0.268700, test/loss=3.639417, test/num_examples=10000, total_duration=5394.086796, train/accuracy=0.390505, train/loss=2.837037, validation/accuracy=0.359860, validation/loss=2.993962, validation/num_examples=50000
I0501 20:35:04.560019 140472299267840 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.116910934448242, loss=3.945127010345459
I0501 20:35:40.963769 140472307660544 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.676756858825684, loss=3.9238271713256836
I0501 20:36:17.256350 140472299267840 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.1514971256256104, loss=3.812917947769165
I0501 20:36:53.620726 140472307660544 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.7936341762542725, loss=3.787341594696045
I0501 20:37:30.007322 140472299267840 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.206942558288574, loss=3.7596969604492188
I0501 20:38:06.364064 140472307660544 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.1457295417785645, loss=3.9020919799804688
I0501 20:38:42.830898 140472299267840 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.117562770843506, loss=3.826364755630493
I0501 20:39:19.202043 140472307660544 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.7350683212280273, loss=3.7506322860717773
I0501 20:39:55.560204 140472299267840 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.5065524578094482, loss=3.704664707183838
I0501 20:40:31.967368 140472307660544 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.838222026824951, loss=3.7523372173309326
I0501 20:41:08.361799 140472299267840 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.86266827583313, loss=3.644167184829712
I0501 20:41:44.707281 140472307660544 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.141982555389404, loss=3.672102451324463
I0501 20:42:21.087858 140472299267840 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.6812546253204346, loss=3.778444766998291
I0501 20:42:57.467493 140472307660544 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.864555835723877, loss=3.6548447608947754
I0501 20:43:11.146959 140676718925632 spec.py:298] Evaluating on the training split.
I0501 20:43:19.022485 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 20:43:29.089331 140676718925632 spec.py:326] Evaluating on the test split.
I0501 20:43:31.343041 140676718925632 submission_runner.py:415] Time since start: 5924.54s, 	Step: 15439, 	{'train/accuracy': 0.429408460855484, 'train/loss': 2.6316182613372803, 'validation/accuracy': 0.39937999844551086, 'validation/loss': 2.8073737621307373, 'validation/num_examples': 50000, 'test/accuracy': 0.29360002279281616, 'test/loss': 3.4545373916625977, 'test/num_examples': 10000, 'score': 5678.201397180557, 'total_duration': 5924.536305189133, 'accumulated_submission_time': 5678.201397180557, 'accumulated_eval_time': 245.92358303070068, 'accumulated_logging_time': 0.21259856224060059}
I0501 20:43:31.357968 140472299267840 logging_writer.py:48] [15439] accumulated_eval_time=245.923583, accumulated_logging_time=0.212599, accumulated_submission_time=5678.201397, global_step=15439, preemption_count=0, score=5678.201397, test/accuracy=0.293600, test/loss=3.454537, test/num_examples=10000, total_duration=5924.536305, train/accuracy=0.429408, train/loss=2.631618, validation/accuracy=0.399380, validation/loss=2.807374, validation/num_examples=50000
I0501 20:43:53.899585 140472307660544 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.2018680572509766, loss=3.7166085243225098
I0501 20:44:30.423503 140472299267840 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.503650426864624, loss=3.678727626800537
I0501 20:45:06.829172 140472307660544 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.8208155632019043, loss=3.5640482902526855
I0501 20:45:43.171992 140472299267840 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.030117511749268, loss=3.6173877716064453
I0501 20:46:19.509101 140472307660544 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.795596122741699, loss=3.630023717880249
I0501 20:46:55.895966 140472299267840 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.3118205070495605, loss=3.5907135009765625
I0501 20:47:32.256965 140472307660544 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.254397392272949, loss=3.595404863357544
I0501 20:48:08.595747 140472299267840 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.266710042953491, loss=3.5791118144989014
I0501 20:48:44.988968 140472307660544 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.286252021789551, loss=3.5487051010131836
I0501 20:49:21.305566 140472299267840 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.923827886581421, loss=3.4855103492736816
I0501 20:49:57.663698 140472307660544 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.3259356021881104, loss=3.43709397315979
I0501 20:50:34.194792 140472299267840 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.295012474060059, loss=3.5230417251586914
I0501 20:51:10.594474 140472307660544 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.9254047870635986, loss=3.432506561279297
I0501 20:51:46.924610 140472299267840 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.7834410667419434, loss=3.421398162841797
I0501 20:52:01.639706 140676718925632 spec.py:298] Evaluating on the training split.
I0501 20:52:09.223247 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 20:52:19.371130 140676718925632 spec.py:326] Evaluating on the test split.
I0501 20:52:21.649233 140676718925632 submission_runner.py:415] Time since start: 6454.84s, 	Step: 16842, 	{'train/accuracy': 0.4826211631298065, 'train/loss': 2.3545384407043457, 'validation/accuracy': 0.44742000102996826, 'validation/loss': 2.5501890182495117, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.221362352371216, 'test/num_examples': 10000, 'score': 6188.4553554058075, 'total_duration': 6454.842549800873, 'accumulated_submission_time': 6188.4553554058075, 'accumulated_eval_time': 265.93215012550354, 'accumulated_logging_time': 0.2372586727142334}
I0501 20:52:21.659665 140472307660544 logging_writer.py:48] [16842] accumulated_eval_time=265.932150, accumulated_logging_time=0.237259, accumulated_submission_time=6188.455355, global_step=16842, preemption_count=0, score=6188.455355, test/accuracy=0.342700, test/loss=3.221362, test/num_examples=10000, total_duration=6454.842550, train/accuracy=0.482621, train/loss=2.354538, validation/accuracy=0.447420, validation/loss=2.550189, validation/num_examples=50000
I0501 20:52:43.128588 140472299267840 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.311544418334961, loss=3.4480369091033936
I0501 20:53:19.475336 140472307660544 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.2542943954467773, loss=3.47304630279541
I0501 20:53:55.857977 140472299267840 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.8864660263061523, loss=3.362301826477051
I0501 20:54:32.259011 140472307660544 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.8425168991088867, loss=3.4920337200164795
I0501 20:55:08.611337 140472299267840 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.336822748184204, loss=3.4607999324798584
I0501 20:55:45.008761 140472307660544 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.6061534881591797, loss=3.484636068344116
I0501 20:56:21.404568 140472299267840 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.79331636428833, loss=3.5653884410858154
I0501 20:56:57.811122 140472307660544 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.056683540344238, loss=3.5682852268218994
I0501 20:57:34.101533 140472299267840 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.430511236190796, loss=3.423948049545288
I0501 20:58:10.434752 140472307660544 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.9368362426757812, loss=3.399338722229004
I0501 20:58:46.786282 140472299267840 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.6036577224731445, loss=3.322420120239258
I0501 20:59:23.154956 140472307660544 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.507733106613159, loss=3.2706313133239746
I0501 20:59:59.547951 140472299267840 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.45202898979187, loss=3.3154826164245605
I0501 21:00:35.897589 140472307660544 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.064253330230713, loss=3.3911375999450684
I0501 21:00:51.754684 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:01:00.021378 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:01:10.194175 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:01:12.379423 140676718925632 submission_runner.py:415] Time since start: 6985.57s, 	Step: 18245, 	{'train/accuracy': 0.5407366156578064, 'train/loss': 2.0840752124786377, 'validation/accuracy': 0.4781999886035919, 'validation/loss': 2.4126811027526855, 'validation/num_examples': 50000, 'test/accuracy': 0.3638000190258026, 'test/loss': 3.0911612510681152, 'test/num_examples': 10000, 'score': 6698.523548603058, 'total_duration': 6985.572681188583, 'accumulated_submission_time': 6698.523548603058, 'accumulated_eval_time': 286.5558683872223, 'accumulated_logging_time': 0.256666898727417}
I0501 21:01:12.395943 140472299267840 logging_writer.py:48] [18245] accumulated_eval_time=286.555868, accumulated_logging_time=0.256667, accumulated_submission_time=6698.523549, global_step=18245, preemption_count=0, score=6698.523549, test/accuracy=0.363800, test/loss=3.091161, test/num_examples=10000, total_duration=6985.572681, train/accuracy=0.540737, train/loss=2.084075, validation/accuracy=0.478200, validation/loss=2.412681, validation/num_examples=50000
I0501 21:01:32.786078 140472307660544 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.270622491836548, loss=3.31156063079834
I0501 21:02:09.088567 140472299267840 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.539762496948242, loss=3.3600056171417236
I0501 21:02:45.526005 140472307660544 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.091009140014648, loss=3.270504951477051
I0501 21:03:21.901281 140472299267840 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.935375452041626, loss=3.288724899291992
I0501 21:03:58.246817 140472307660544 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.4112539291381836, loss=3.420994520187378
I0501 21:04:34.559097 140472299267840 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.824322462081909, loss=3.3167338371276855
I0501 21:05:10.939942 140472307660544 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.2959282398223877, loss=3.293869972229004
I0501 21:05:47.321947 140472299267840 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.577848434448242, loss=3.390176773071289
I0501 21:06:23.660228 140472307660544 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.0885164737701416, loss=3.340798854827881
I0501 21:06:59.998617 140472299267840 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.021512746810913, loss=3.1979148387908936
I0501 21:07:36.365949 140472307660544 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.539614677429199, loss=3.297513246536255
I0501 21:08:12.710565 140472299267840 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.253577709197998, loss=3.3906548023223877
I0501 21:08:49.106778 140472307660544 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.168755292892456, loss=3.1755857467651367
I0501 21:09:25.499338 140472299267840 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.3204612731933594, loss=3.234985828399658
I0501 21:09:42.447518 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:09:50.178931 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:10:00.480327 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:10:02.678055 140676718925632 submission_runner.py:415] Time since start: 7515.87s, 	Step: 19648, 	{'train/accuracy': 0.5539500713348389, 'train/loss': 1.9456920623779297, 'validation/accuracy': 0.4918399751186371, 'validation/loss': 2.2660341262817383, 'validation/num_examples': 50000, 'test/accuracy': 0.37050002813339233, 'test/loss': 2.99126935005188, 'test/num_examples': 10000, 'score': 7208.545796394348, 'total_duration': 7515.872306108475, 'accumulated_submission_time': 7208.545796394348, 'accumulated_eval_time': 306.78637957572937, 'accumulated_logging_time': 0.28446292877197266}
I0501 21:10:02.688006 140472307660544 logging_writer.py:48] [19648] accumulated_eval_time=306.786380, accumulated_logging_time=0.284463, accumulated_submission_time=7208.545796, global_step=19648, preemption_count=0, score=7208.545796, test/accuracy=0.370500, test/loss=2.991269, test/num_examples=10000, total_duration=7515.872306, train/accuracy=0.553950, train/loss=1.945692, validation/accuracy=0.491840, validation/loss=2.266034, validation/num_examples=50000
I0501 21:10:21.945245 140472299267840 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.0201900005340576, loss=3.138862133026123
I0501 21:10:58.274124 140472307660544 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.228200674057007, loss=3.267063617706299
I0501 21:11:34.612631 140472299267840 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.7765252590179443, loss=3.1668918132781982
I0501 21:12:10.911613 140472307660544 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.0791850090026855, loss=3.1921746730804443
I0501 21:12:47.219810 140472299267840 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.8482697010040283, loss=3.2006824016571045
I0501 21:13:23.513078 140472307660544 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.810788154602051, loss=3.2781472206115723
I0501 21:13:59.876683 140472299267840 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.169344663619995, loss=3.1895017623901367
I0501 21:14:36.286841 140472307660544 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.3700053691864014, loss=3.183633804321289
I0501 21:15:12.633509 140472299267840 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.142794370651245, loss=3.2193796634674072
I0501 21:15:49.016047 140472307660544 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.263932943344116, loss=3.0443639755249023
I0501 21:16:25.292504 140472299267840 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.2598767280578613, loss=3.187495231628418
I0501 21:17:01.566915 140472307660544 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.0665154457092285, loss=3.2320570945739746
I0501 21:17:37.903057 140472299267840 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.9229464530944824, loss=3.1580586433410645
I0501 21:18:14.233273 140472307660544 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.0397233963012695, loss=3.1999876499176025
I0501 21:18:32.951984 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:18:40.807515 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:18:51.010609 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:18:53.301033 140676718925632 submission_runner.py:415] Time since start: 8046.49s, 	Step: 21053, 	{'train/accuracy': 0.570711076259613, 'train/loss': 1.8872711658477783, 'validation/accuracy': 0.5190199613571167, 'validation/loss': 2.150937080383301, 'validation/num_examples': 50000, 'test/accuracy': 0.39810001850128174, 'test/loss': 2.8562216758728027, 'test/num_examples': 10000, 'score': 7718.783116817474, 'total_duration': 8046.494191408157, 'accumulated_submission_time': 7718.783116817474, 'accumulated_eval_time': 327.13430976867676, 'accumulated_logging_time': 0.3032972812652588}
I0501 21:18:53.311468 140472299267840 logging_writer.py:48] [21053] accumulated_eval_time=327.134310, accumulated_logging_time=0.303297, accumulated_submission_time=7718.783117, global_step=21053, preemption_count=0, score=7718.783117, test/accuracy=0.398100, test/loss=2.856222, test/num_examples=10000, total_duration=8046.494191, train/accuracy=0.570711, train/loss=1.887271, validation/accuracy=0.519020, validation/loss=2.150937, validation/num_examples=50000
I0501 21:19:10.792317 140472307660544 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.7692549228668213, loss=3.1574840545654297
I0501 21:19:47.167327 140472299267840 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.966482400894165, loss=3.189572811126709
I0501 21:20:23.495327 140472307660544 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.2582156658172607, loss=3.0863444805145264
I0501 21:20:59.952891 140472299267840 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.7450897693634033, loss=3.144937038421631
I0501 21:21:36.294663 140472307660544 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.0466079711914062, loss=3.0759925842285156
I0501 21:22:12.672939 140472299267840 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.9873082637786865, loss=3.014880895614624
I0501 21:22:48.987143 140472307660544 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.7736644744873047, loss=3.1347124576568604
I0501 21:23:25.370563 140472299267840 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.940516710281372, loss=3.1497085094451904
I0501 21:24:01.704264 140472307660544 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.217179536819458, loss=3.078223466873169
I0501 21:24:38.122811 140472299267840 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.7864348888397217, loss=3.092971086502075
I0501 21:25:14.491891 140472307660544 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.726203441619873, loss=3.0866541862487793
I0501 21:25:50.878197 140472299267840 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.7167909145355225, loss=3.1348743438720703
I0501 21:26:27.249890 140472307660544 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.69262433052063, loss=3.2166552543640137
I0501 21:27:03.770270 140472299267840 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.168377161026001, loss=3.0647172927856445
I0501 21:27:23.625097 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:27:31.756062 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:27:42.149966 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:27:44.219821 140676718925632 submission_runner.py:415] Time since start: 8577.41s, 	Step: 22456, 	{'train/accuracy': 0.5912587642669678, 'train/loss': 1.8076483011245728, 'validation/accuracy': 0.5346400141716003, 'validation/loss': 2.0981714725494385, 'validation/num_examples': 50000, 'test/accuracy': 0.40800002217292786, 'test/loss': 2.8230860233306885, 'test/num_examples': 10000, 'score': 8229.070605754852, 'total_duration': 8577.413085460663, 'accumulated_submission_time': 8229.070605754852, 'accumulated_eval_time': 347.72804594039917, 'accumulated_logging_time': 0.3221592903137207}
I0501 21:27:44.230103 140472307660544 logging_writer.py:48] [22456] accumulated_eval_time=347.728046, accumulated_logging_time=0.322159, accumulated_submission_time=8229.070606, global_step=22456, preemption_count=0, score=8229.070606, test/accuracy=0.408000, test/loss=2.823086, test/num_examples=10000, total_duration=8577.413085, train/accuracy=0.591259, train/loss=1.807648, validation/accuracy=0.534640, validation/loss=2.098171, validation/num_examples=50000
I0501 21:28:00.583944 140472299267840 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.8529231548309326, loss=3.067859649658203
I0501 21:28:36.951560 140472307660544 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.0152924060821533, loss=3.1048355102539062
I0501 21:29:13.312097 140472299267840 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.84238338470459, loss=3.044830322265625
I0501 21:29:49.632447 140472307660544 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.891491174697876, loss=2.9916741847991943
I0501 21:30:25.961339 140472299267840 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.729828119277954, loss=3.0851328372955322
I0501 21:31:02.291680 140472307660544 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.6713340282440186, loss=2.931284189224243
I0501 21:31:38.622943 140472299267840 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.0665154457092285, loss=3.115691661834717
I0501 21:32:14.906639 140472307660544 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.786252498626709, loss=2.9898858070373535
I0501 21:32:51.223642 140472299267840 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.5847878456115723, loss=2.9699716567993164
I0501 21:33:27.670023 140472307660544 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.806126594543457, loss=3.071941375732422
I0501 21:34:04.042961 140472299267840 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.9060120582580566, loss=3.039318561553955
I0501 21:34:40.386287 140472307660544 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.7345643043518066, loss=2.960977077484131
I0501 21:35:16.723322 140472299267840 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.6329901218414307, loss=2.9407541751861572
I0501 21:35:53.067018 140472307660544 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.7500293254852295, loss=3.0003912448883057
I0501 21:36:14.375068 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:36:22.636587 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:36:33.003154 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:36:35.285798 140676718925632 submission_runner.py:415] Time since start: 9108.48s, 	Step: 23860, 	{'train/accuracy': 0.6051099896430969, 'train/loss': 1.7409166097640991, 'validation/accuracy': 0.5496799945831299, 'validation/loss': 1.9972022771835327, 'validation/num_examples': 50000, 'test/accuracy': 0.4278000295162201, 'test/loss': 2.7050833702087402, 'test/num_examples': 10000, 'score': 8739.189260005951, 'total_duration': 9108.478843688965, 'accumulated_submission_time': 8739.189260005951, 'accumulated_eval_time': 368.6375529766083, 'accumulated_logging_time': 0.34098076820373535}
I0501 21:36:35.295885 140472299267840 logging_writer.py:48] [23860] accumulated_eval_time=368.637553, accumulated_logging_time=0.340981, accumulated_submission_time=8739.189260, global_step=23860, preemption_count=0, score=8739.189260, test/accuracy=0.427800, test/loss=2.705083, test/num_examples=10000, total_duration=9108.478844, train/accuracy=0.605110, train/loss=1.740917, validation/accuracy=0.549680, validation/loss=1.997202, validation/num_examples=50000
I0501 21:36:50.191420 140472307660544 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.955430269241333, loss=3.082956314086914
I0501 21:37:26.555047 140472299267840 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.768091917037964, loss=3.0890274047851562
I0501 21:38:02.849386 140472307660544 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.7728734016418457, loss=3.045360565185547
I0501 21:38:39.168720 140472299267840 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.658538818359375, loss=2.943540334701538
I0501 21:39:15.574782 140472307660544 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.569345235824585, loss=2.9649901390075684
I0501 21:39:51.890146 140472299267840 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.768385887145996, loss=2.85650634765625
I0501 21:40:28.211735 140472307660544 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.871568202972412, loss=2.980790853500366
I0501 21:41:04.514501 140472299267840 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.7145988941192627, loss=3.034672498703003
I0501 21:41:40.807083 140472307660544 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.520723819732666, loss=3.07853364944458
I0501 21:42:17.130109 140472299267840 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.615236520767212, loss=2.93972110748291
I0501 21:42:53.431714 140472307660544 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.5486106872558594, loss=2.8789024353027344
I0501 21:43:29.673953 140472299267840 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.613640785217285, loss=2.9340338706970215
I0501 21:44:05.976967 140472307660544 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.7801756858825684, loss=3.014430522918701
I0501 21:44:42.323770 140472299267840 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.9655861854553223, loss=2.946031093597412
I0501 21:45:05.398813 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:45:14.103270 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:45:24.443826 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:45:26.715043 140676718925632 submission_runner.py:415] Time since start: 9639.91s, 	Step: 25265, 	{'train/accuracy': 0.6118462681770325, 'train/loss': 1.7116851806640625, 'validation/accuracy': 0.5546199679374695, 'validation/loss': 1.9860296249389648, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.6940345764160156, 'test/num_examples': 10000, 'score': 9249.265469551086, 'total_duration': 9639.909292459488, 'accumulated_submission_time': 9249.265469551086, 'accumulated_eval_time': 389.95375514030457, 'accumulated_logging_time': 0.3598804473876953}
I0501 21:45:26.725501 140472307660544 logging_writer.py:48] [25265] accumulated_eval_time=389.953755, accumulated_logging_time=0.359880, accumulated_submission_time=9249.265470, global_step=25265, preemption_count=0, score=9249.265470, test/accuracy=0.431200, test/loss=2.694035, test/num_examples=10000, total_duration=9639.909292, train/accuracy=0.611846, train/loss=1.711685, validation/accuracy=0.554620, validation/loss=1.986030, validation/num_examples=50000
I0501 21:45:39.797471 140472299267840 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.625100612640381, loss=3.0174219608306885
I0501 21:46:16.131222 140472307660544 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.862928867340088, loss=2.972360134124756
I0501 21:46:52.380501 140472299267840 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.6314761638641357, loss=2.98757266998291
I0501 21:47:28.662756 140472307660544 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.096626043319702, loss=2.980210781097412
I0501 21:48:04.898131 140472299267840 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.6130645275115967, loss=2.8967952728271484
I0501 21:48:41.149993 140472307660544 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.471087694168091, loss=2.8423638343811035
I0501 21:49:17.495194 140472299267840 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.9345614910125732, loss=2.9973013401031494
I0501 21:49:53.753955 140472307660544 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.4579811096191406, loss=2.919651508331299
I0501 21:50:30.066207 140472299267840 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.5722274780273438, loss=2.92138409614563
I0501 21:51:06.396479 140472307660544 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.58504056930542, loss=2.9103281497955322
I0501 21:51:42.798957 140472299267840 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.649237632751465, loss=2.837179183959961
I0501 21:52:19.091818 140472307660544 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.8506696224212646, loss=2.911663055419922
I0501 21:52:55.373093 140472299267840 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.704420566558838, loss=2.893317222595215
I0501 21:53:31.627489 140472307660544 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.5043766498565674, loss=2.921952486038208
I0501 21:53:56.782294 140676718925632 spec.py:298] Evaluating on the training split.
I0501 21:54:05.228239 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 21:54:15.996252 140676718925632 spec.py:326] Evaluating on the test split.
I0501 21:54:18.021179 140676718925632 submission_runner.py:415] Time since start: 10171.22s, 	Step: 26671, 	{'train/accuracy': 0.6272720098495483, 'train/loss': 1.6545472145080566, 'validation/accuracy': 0.5691199898719788, 'validation/loss': 1.9326534271240234, 'validation/num_examples': 50000, 'test/accuracy': 0.4399000108242035, 'test/loss': 2.6454710960388184, 'test/num_examples': 10000, 'score': 9759.295982122421, 'total_duration': 10171.21542596817, 'accumulated_submission_time': 9759.295982122421, 'accumulated_eval_time': 411.1926121711731, 'accumulated_logging_time': 0.37895989418029785}
I0501 21:54:18.031309 140472299267840 logging_writer.py:48] [26671] accumulated_eval_time=411.192612, accumulated_logging_time=0.378960, accumulated_submission_time=9759.295982, global_step=26671, preemption_count=0, score=9759.295982, test/accuracy=0.439900, test/loss=2.645471, test/num_examples=10000, total_duration=10171.215426, train/accuracy=0.627272, train/loss=1.654547, validation/accuracy=0.569120, validation/loss=1.932653, validation/num_examples=50000
I0501 21:54:28.937448 140472307660544 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.4203953742980957, loss=2.951846122741699
I0501 21:55:05.268310 140472299267840 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.8119418621063232, loss=2.937666654586792
I0501 21:55:41.568788 140472307660544 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.366971015930176, loss=2.8284912109375
I0501 21:56:17.872748 140472299267840 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.5512568950653076, loss=2.8985543251037598
I0501 21:56:54.156564 140472307660544 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.482184410095215, loss=2.8701908588409424
I0501 21:57:30.568001 140472299267840 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.0788516998291016, loss=2.9495620727539062
I0501 21:58:06.864680 140472307660544 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.6447255611419678, loss=2.893117904663086
I0501 21:58:43.193360 140472299267840 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.790694236755371, loss=2.8149304389953613
I0501 21:59:19.535907 140472307660544 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.4740331172943115, loss=2.9457435607910156
I0501 21:59:55.892472 140472299267840 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.420217514038086, loss=2.8739538192749023
I0501 22:00:32.237366 140472307660544 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.360116958618164, loss=2.956479072570801
I0501 22:01:08.567073 140472299267840 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.7388081550598145, loss=2.888953447341919
I0501 22:01:44.904401 140472307660544 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.543397903442383, loss=2.809412717819214
I0501 22:02:20.650481 140676718925632 spec.py:298] Evaluating on the training split.
I0501 22:02:28.703997 140676718925632 spec.py:310] Evaluating on the validation split.
I0501 22:02:39.341863 140676718925632 spec.py:326] Evaluating on the test split.
I0501 22:02:41.361423 140676718925632 submission_runner.py:415] Time since start: 10674.55s, 	Step: 28000, 	{'train/accuracy': 0.6354033946990967, 'train/loss': 1.5815608501434326, 'validation/accuracy': 0.5773199796676636, 'validation/loss': 1.8788278102874756, 'validation/num_examples': 50000, 'test/accuracy': 0.445900022983551, 'test/loss': 2.6040096282958984, 'test/num_examples': 10000, 'score': 10241.890132665634, 'total_duration': 10674.554406166077, 'accumulated_submission_time': 10241.890132665634, 'accumulated_eval_time': 431.90226221084595, 'accumulated_logging_time': 0.39748668670654297}
I0501 22:02:41.371887 140472299267840 logging_writer.py:48] [28000] accumulated_eval_time=431.902262, accumulated_logging_time=0.397487, accumulated_submission_time=10241.890133, global_step=28000, preemption_count=0, score=10241.890133, test/accuracy=0.445900, test/loss=2.604010, test/num_examples=10000, total_duration=10674.554406, train/accuracy=0.635403, train/loss=1.581561, validation/accuracy=0.577320, validation/loss=1.878828, validation/num_examples=50000
I0501 22:02:41.387362 140472307660544 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10241.890133
I0501 22:02:41.644938 140676718925632 checkpoints.py:356] Saving checkpoint at step: 28000
I0501 22:02:42.739759 140676718925632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1/checkpoint_28000
I0501 22:02:42.764971 140676718925632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0501 22:02:43.187139 140676718925632 submission_runner.py:578] Tuning trial 1/1
I0501 22:02:43.187373 140676718925632 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0501 22:02:43.191712 140676718925632 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008370535797439516, 'train/loss': 6.911266803741455, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.911518096923828, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.9111247062683105, 'test/num_examples': 10000, 'score': 66.30537223815918, 'total_duration': 108.30703520774841, 'accumulated_submission_time': 66.30537223815918, 'accumulated_eval_time': 42.00152921676636, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1405, {'train/accuracy': 0.022002549842000008, 'train/loss': 6.4482831954956055, 'validation/accuracy': 0.020099999383091927, 'validation/loss': 6.47771692276001, 'validation/num_examples': 50000, 'test/accuracy': 0.01510000042617321, 'test/loss': 6.542073726654053, 'test/num_examples': 10000, 'score': 576.3000509738922, 'total_duration': 635.6583120822906, 'accumulated_submission_time': 576.3000509738922, 'accumulated_eval_time': 59.315924644470215, 'accumulated_logging_time': 0.02403879165649414, 'global_step': 1405, 'preemption_count': 0}), (2810, {'train/accuracy': 0.0489676333963871, 'train/loss': 5.865507125854492, 'validation/accuracy': 0.04623999819159508, 'validation/loss': 5.9181036949157715, 'validation/num_examples': 50000, 'test/accuracy': 0.0333000011742115, 'test/loss': 6.073990345001221, 'test/num_examples': 10000, 'score': 1086.3819971084595, 'total_duration': 1163.2935228347778, 'accumulated_submission_time': 1086.3819971084595, 'accumulated_eval_time': 76.8327488899231, 'accumulated_logging_time': 0.04265332221984863, 'global_step': 2810, 'preemption_count': 0}), (4213, {'train/accuracy': 0.07920120656490326, 'train/loss': 5.4193572998046875, 'validation/accuracy': 0.07215999811887741, 'validation/loss': 5.520287990570068, 'validation/num_examples': 50000, 'test/accuracy': 0.04930000379681587, 'test/loss': 5.755897521972656, 'test/num_examples': 10000, 'score': 1596.4833886623383, 'total_duration': 1690.7428572177887, 'accumulated_submission_time': 1596.4833886623383, 'accumulated_eval_time': 94.14749574661255, 'accumulated_logging_time': 0.05832695960998535, 'global_step': 4213, 'preemption_count': 0}), (5616, {'train/accuracy': 0.09773596376180649, 'train/loss': 5.190073013305664, 'validation/accuracy': 0.0894399955868721, 'validation/loss': 5.286595344543457, 'validation/num_examples': 50000, 'test/accuracy': 0.061500001698732376, 'test/loss': 5.575964450836182, 'test/num_examples': 10000, 'score': 2106.7729983329773, 'total_duration': 2218.0579133033752, 'accumulated_submission_time': 2106.7729983329773, 'accumulated_eval_time': 111.136967420578, 'accumulated_logging_time': 0.07622694969177246, 'global_step': 5616, 'preemption_count': 0}), (7019, {'train/accuracy': 0.11292251199483871, 'train/loss': 5.01508092880249, 'validation/accuracy': 0.10227999836206436, 'validation/loss': 5.134513854980469, 'validation/num_examples': 50000, 'test/accuracy': 0.07129999995231628, 'test/loss': 5.489673614501953, 'test/num_examples': 10000, 'score': 2616.9900267124176, 'total_duration': 2745.7732882499695, 'accumulated_submission_time': 2616.9900267124176, 'accumulated_eval_time': 128.59909343719482, 'accumulated_logging_time': 0.09452009201049805, 'global_step': 7019, 'preemption_count': 0}), (8423, {'train/accuracy': 0.12922511994838715, 'train/loss': 4.895474910736084, 'validation/accuracy': 0.11219999939203262, 'validation/loss': 5.0270891189575195, 'validation/num_examples': 50000, 'test/accuracy': 0.07820000499486923, 'test/loss': 5.392067909240723, 'test/num_examples': 10000, 'score': 3127.257213830948, 'total_duration': 3274.2474353313446, 'accumulated_submission_time': 3127.257213830948, 'accumulated_eval_time': 146.77109003067017, 'accumulated_logging_time': 0.11184430122375488, 'global_step': 8423, 'preemption_count': 0}), (9827, {'train/accuracy': 0.15684789419174194, 'train/loss': 4.54622745513916, 'validation/accuracy': 0.14093999564647675, 'validation/loss': 4.682422161102295, 'validation/num_examples': 50000, 'test/accuracy': 0.09760000556707382, 'test/loss': 5.122074127197266, 'test/num_examples': 10000, 'score': 3637.433202266693, 'total_duration': 3803.6520023345947, 'accumulated_submission_time': 3637.433202266693, 'accumulated_eval_time': 165.96393203735352, 'accumulated_logging_time': 0.1300067901611328, 'global_step': 9827, 'preemption_count': 0}), (11231, {'train/accuracy': 0.24451929330825806, 'train/loss': 3.800215721130371, 'validation/accuracy': 0.22655999660491943, 'validation/loss': 3.9290382862091064, 'validation/num_examples': 50000, 'test/accuracy': 0.15820001065731049, 'test/loss': 4.473752021789551, 'test/num_examples': 10000, 'score': 4147.749961853027, 'total_duration': 4333.649039506912, 'accumulated_submission_time': 4147.749961853027, 'accumulated_eval_time': 185.60751867294312, 'accumulated_logging_time': 0.1493680477142334, 'global_step': 11231, 'preemption_count': 0}), (12634, {'train/accuracy': 0.31475207209587097, 'train/loss': 3.3164684772491455, 'validation/accuracy': 0.2825999855995178, 'validation/loss': 3.485480546951294, 'validation/num_examples': 50000, 'test/accuracy': 0.20740000903606415, 'test/loss': 4.1016974449157715, 'test/num_examples': 10000, 'score': 4657.960319757462, 'total_duration': 4864.53219294548, 'accumulated_submission_time': 4657.960319757462, 'accumulated_eval_time': 206.24364757537842, 'accumulated_logging_time': 0.1668405532836914, 'global_step': 12634, 'preemption_count': 0}), (14036, {'train/accuracy': 0.3905054032802582, 'train/loss': 2.8370370864868164, 'validation/accuracy': 0.35986000299453735, 'validation/loss': 2.9939615726470947, 'validation/num_examples': 50000, 'test/accuracy': 0.2687000036239624, 'test/loss': 3.6394169330596924, 'test/num_examples': 10000, 'score': 5167.985907316208, 'total_duration': 5394.086795806885, 'accumulated_submission_time': 5167.985907316208, 'accumulated_eval_time': 225.72851371765137, 'accumulated_logging_time': 0.19259929656982422, 'global_step': 14036, 'preemption_count': 0}), (15439, {'train/accuracy': 0.429408460855484, 'train/loss': 2.6316182613372803, 'validation/accuracy': 0.39937999844551086, 'validation/loss': 2.8073737621307373, 'validation/num_examples': 50000, 'test/accuracy': 0.29360002279281616, 'test/loss': 3.4545373916625977, 'test/num_examples': 10000, 'score': 5678.201397180557, 'total_duration': 5924.536305189133, 'accumulated_submission_time': 5678.201397180557, 'accumulated_eval_time': 245.92358303070068, 'accumulated_logging_time': 0.21259856224060059, 'global_step': 15439, 'preemption_count': 0}), (16842, {'train/accuracy': 0.4826211631298065, 'train/loss': 2.3545384407043457, 'validation/accuracy': 0.44742000102996826, 'validation/loss': 2.5501890182495117, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.221362352371216, 'test/num_examples': 10000, 'score': 6188.4553554058075, 'total_duration': 6454.842549800873, 'accumulated_submission_time': 6188.4553554058075, 'accumulated_eval_time': 265.93215012550354, 'accumulated_logging_time': 0.2372586727142334, 'global_step': 16842, 'preemption_count': 0}), (18245, {'train/accuracy': 0.5407366156578064, 'train/loss': 2.0840752124786377, 'validation/accuracy': 0.4781999886035919, 'validation/loss': 2.4126811027526855, 'validation/num_examples': 50000, 'test/accuracy': 0.3638000190258026, 'test/loss': 3.0911612510681152, 'test/num_examples': 10000, 'score': 6698.523548603058, 'total_duration': 6985.572681188583, 'accumulated_submission_time': 6698.523548603058, 'accumulated_eval_time': 286.5558683872223, 'accumulated_logging_time': 0.256666898727417, 'global_step': 18245, 'preemption_count': 0}), (19648, {'train/accuracy': 0.5539500713348389, 'train/loss': 1.9456920623779297, 'validation/accuracy': 0.4918399751186371, 'validation/loss': 2.2660341262817383, 'validation/num_examples': 50000, 'test/accuracy': 0.37050002813339233, 'test/loss': 2.99126935005188, 'test/num_examples': 10000, 'score': 7208.545796394348, 'total_duration': 7515.872306108475, 'accumulated_submission_time': 7208.545796394348, 'accumulated_eval_time': 306.78637957572937, 'accumulated_logging_time': 0.28446292877197266, 'global_step': 19648, 'preemption_count': 0}), (21053, {'train/accuracy': 0.570711076259613, 'train/loss': 1.8872711658477783, 'validation/accuracy': 0.5190199613571167, 'validation/loss': 2.150937080383301, 'validation/num_examples': 50000, 'test/accuracy': 0.39810001850128174, 'test/loss': 2.8562216758728027, 'test/num_examples': 10000, 'score': 7718.783116817474, 'total_duration': 8046.494191408157, 'accumulated_submission_time': 7718.783116817474, 'accumulated_eval_time': 327.13430976867676, 'accumulated_logging_time': 0.3032972812652588, 'global_step': 21053, 'preemption_count': 0}), (22456, {'train/accuracy': 0.5912587642669678, 'train/loss': 1.8076483011245728, 'validation/accuracy': 0.5346400141716003, 'validation/loss': 2.0981714725494385, 'validation/num_examples': 50000, 'test/accuracy': 0.40800002217292786, 'test/loss': 2.8230860233306885, 'test/num_examples': 10000, 'score': 8229.070605754852, 'total_duration': 8577.413085460663, 'accumulated_submission_time': 8229.070605754852, 'accumulated_eval_time': 347.72804594039917, 'accumulated_logging_time': 0.3221592903137207, 'global_step': 22456, 'preemption_count': 0}), (23860, {'train/accuracy': 0.6051099896430969, 'train/loss': 1.7409166097640991, 'validation/accuracy': 0.5496799945831299, 'validation/loss': 1.9972022771835327, 'validation/num_examples': 50000, 'test/accuracy': 0.4278000295162201, 'test/loss': 2.7050833702087402, 'test/num_examples': 10000, 'score': 8739.189260005951, 'total_duration': 9108.478843688965, 'accumulated_submission_time': 8739.189260005951, 'accumulated_eval_time': 368.6375529766083, 'accumulated_logging_time': 0.34098076820373535, 'global_step': 23860, 'preemption_count': 0}), (25265, {'train/accuracy': 0.6118462681770325, 'train/loss': 1.7116851806640625, 'validation/accuracy': 0.5546199679374695, 'validation/loss': 1.9860296249389648, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.6940345764160156, 'test/num_examples': 10000, 'score': 9249.265469551086, 'total_duration': 9639.909292459488, 'accumulated_submission_time': 9249.265469551086, 'accumulated_eval_time': 389.95375514030457, 'accumulated_logging_time': 0.3598804473876953, 'global_step': 25265, 'preemption_count': 0}), (26671, {'train/accuracy': 0.6272720098495483, 'train/loss': 1.6545472145080566, 'validation/accuracy': 0.5691199898719788, 'validation/loss': 1.9326534271240234, 'validation/num_examples': 50000, 'test/accuracy': 0.4399000108242035, 'test/loss': 2.6454710960388184, 'test/num_examples': 10000, 'score': 9759.295982122421, 'total_duration': 10171.21542596817, 'accumulated_submission_time': 9759.295982122421, 'accumulated_eval_time': 411.1926121711731, 'accumulated_logging_time': 0.37895989418029785, 'global_step': 26671, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6354033946990967, 'train/loss': 1.5815608501434326, 'validation/accuracy': 0.5773199796676636, 'validation/loss': 1.8788278102874756, 'validation/num_examples': 50000, 'test/accuracy': 0.445900022983551, 'test/loss': 2.6040096282958984, 'test/num_examples': 10000, 'score': 10241.890132665634, 'total_duration': 10674.554406166077, 'accumulated_submission_time': 10241.890132665634, 'accumulated_eval_time': 431.90226221084595, 'accumulated_logging_time': 0.39748668670654297, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0501 22:02:43.191863 140676718925632 submission_runner.py:581] Timing: 10241.890132665634
I0501 22:02:43.191908 140676718925632 submission_runner.py:582] ====================
I0501 22:02:43.192039 140676718925632 submission_runner.py:645] Final imagenet_resnet score: 10241.890132665634
