python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_05-01-2023-21-58-31.log
I0501 21:58:53.357978 139674332673856 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax.
I0501 21:58:53.428158 139674332673856 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 21:58:54.278406 139674332673856 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0501 21:58:54.279219 139674332673856 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 21:58:54.283819 139674332673856 submission_runner.py:538] Using RNG seed 1818051108
I0501 21:58:56.873294 139674332673856 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 21:58:56.873506 139674332673856 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1.
I0501 21:58:56.873707 139674332673856 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1/hparams.json.
I0501 21:58:57.006927 139674332673856 submission_runner.py:241] Initializing dataset.
I0501 21:58:57.025104 139674332673856 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 21:58:57.035181 139674332673856 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 21:58:57.035301 139674332673856 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 21:58:57.320040 139674332673856 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 21:59:05.592379 139674332673856 submission_runner.py:248] Initializing model.
I0501 21:59:17.716943 139674332673856 submission_runner.py:258] Initializing optimizer.
I0501 21:59:20.101134 139674332673856 submission_runner.py:265] Initializing metrics bundle.
I0501 21:59:20.101337 139674332673856 submission_runner.py:282] Initializing checkpoint and logger.
I0501 21:59:20.102266 139674332673856 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0501 21:59:20.985920 139674332673856 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1/meta_data_0.json.
I0501 21:59:20.987063 139674332673856 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1/flags_0.json.
I0501 21:59:20.991726 139674332673856 submission_runner.py:318] Starting training loop.
I0501 22:01:16.153615 139497475925760 logging_writer.py:48] [0] global_step=0, grad_norm=0.33126306533813477, loss=6.907756805419922
I0501 22:01:16.177359 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:01:16.186632 139674332673856 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:01:16.194801 139674332673856 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 22:01:16.194937 139674332673856 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 22:01:16.272346 139674332673856 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:01:36.883493 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:01:36.893853 139674332673856 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:01:36.913522 139674332673856 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 22:01:36.913891 139674332673856 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 22:01:36.987246 139674332673856 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:01:56.762565 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:01:56.769984 139674332673856 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 22:01:56.776350 139674332673856 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0501 22:01:56.815438 139674332673856 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 22:02:08.495074 139674332673856 submission_runner.py:415] Time since start: 167.50s, 	Step: 1, 	{'train/accuracy': 0.0022070312406867743, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0019600000232458115, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0014000000664964318, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 115.18545484542847, 'total_duration': 167.50329041481018, 'accumulated_submission_time': 115.18545484542847, 'accumulated_eval_time': 52.317665815353394, 'accumulated_logging_time': 0}
I0501 22:02:08.512979 139444334081792 logging_writer.py:48] [1] accumulated_eval_time=52.317666, accumulated_logging_time=0, accumulated_submission_time=115.185455, global_step=1, preemption_count=0, score=115.185455, test/accuracy=0.001400, test/loss=6.907757, test/num_examples=10000, total_duration=167.503290, train/accuracy=0.002207, train/loss=6.907756, validation/accuracy=0.001960, validation/loss=6.907756, validation/num_examples=50000
I0501 22:04:19.186728 139492228572928 logging_writer.py:48] [100] global_step=100, grad_norm=0.35227277874946594, loss=6.90151309967041
I0501 22:05:08.067351 139492236965632 logging_writer.py:48] [200] global_step=200, grad_norm=0.39748135209083557, loss=6.873222827911377
I0501 22:05:56.618721 139492228572928 logging_writer.py:48] [300] global_step=300, grad_norm=0.6683528423309326, loss=6.770504951477051
I0501 22:06:45.404899 139492236965632 logging_writer.py:48] [400] global_step=400, grad_norm=0.7860800623893738, loss=6.789732933044434
I0501 22:07:33.893576 139492228572928 logging_writer.py:48] [500] global_step=500, grad_norm=1.018208384513855, loss=6.625789165496826
I0501 22:08:22.680630 139492236965632 logging_writer.py:48] [600] global_step=600, grad_norm=0.8175302147865295, loss=6.706057548522949
I0501 22:09:08.816839 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:09:15.386517 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:09:22.305989 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:09:24.152992 139674332673856 submission_runner.py:415] Time since start: 603.16s, 	Step: 696, 	{'train/accuracy': 0.017519531771540642, 'train/loss': 6.262195110321045, 'validation/accuracy': 0.018459999933838844, 'validation/loss': 6.277126789093018, 'validation/num_examples': 50000, 'test/accuracy': 0.013000000268220901, 'test/loss': 6.339540004730225, 'test/num_examples': 10000, 'score': 535.4674348831177, 'total_duration': 603.1611816883087, 'accumulated_submission_time': 535.4674348831177, 'accumulated_eval_time': 67.65379047393799, 'accumulated_logging_time': 0.026494503021240234}
I0501 22:09:24.165655 139444661233408 logging_writer.py:48] [696] accumulated_eval_time=67.653790, accumulated_logging_time=0.026495, accumulated_submission_time=535.467435, global_step=696, preemption_count=0, score=535.467435, test/accuracy=0.013000, test/loss=6.339540, test/num_examples=10000, total_duration=603.161182, train/accuracy=0.017520, train/loss=6.262195, validation/accuracy=0.018460, validation/loss=6.277127, validation/num_examples=50000
I0501 22:09:26.811810 139444669626112 logging_writer.py:48] [700] global_step=700, grad_norm=1.0854319334030151, loss=6.472169399261475
I0501 22:10:15.708486 139444661233408 logging_writer.py:48] [800] global_step=800, grad_norm=1.0699833631515503, loss=6.479794979095459
I0501 22:11:04.264135 139444669626112 logging_writer.py:48] [900] global_step=900, grad_norm=0.9822989106178284, loss=6.706644535064697
I0501 22:11:53.176846 139444661233408 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3170406818389893, loss=6.661484718322754
I0501 22:12:41.872391 139444669626112 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8887231945991516, loss=6.351930618286133
I0501 22:13:30.844468 139444661233408 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0102826356887817, loss=6.264255523681641
I0501 22:14:19.433218 139444669626112 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9454468488693237, loss=6.52167272567749
I0501 22:15:08.329357 139444661233408 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0314165353775024, loss=6.178672790527344
I0501 22:15:56.982839 139444669626112 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9061204791069031, loss=6.054715156555176
I0501 22:16:24.223516 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:16:30.781046 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:16:37.685910 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:16:39.431344 139674332673856 submission_runner.py:415] Time since start: 1038.44s, 	Step: 1557, 	{'train/accuracy': 0.05316406115889549, 'train/loss': 5.635303974151611, 'validation/accuracy': 0.05107999965548515, 'validation/loss': 5.677877426147461, 'validation/num_examples': 50000, 'test/accuracy': 0.03710000216960907, 'test/loss': 5.828189849853516, 'test/num_examples': 10000, 'score': 955.4949903488159, 'total_duration': 1038.439501285553, 'accumulated_submission_time': 955.4949903488159, 'accumulated_eval_time': 82.86155104637146, 'accumulated_logging_time': 0.05360054969787598}
I0501 22:16:39.447136 139444661233408 logging_writer.py:48] [1557] accumulated_eval_time=82.861551, accumulated_logging_time=0.053601, accumulated_submission_time=955.494990, global_step=1557, preemption_count=0, score=955.494990, test/accuracy=0.037100, test/loss=5.828190, test/num_examples=10000, total_duration=1038.439501, train/accuracy=0.053164, train/loss=5.635304, validation/accuracy=0.051080, validation/loss=5.677877, validation/num_examples=50000
I0501 22:17:01.302387 139444669626112 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8176578879356384, loss=6.107048511505127
I0501 22:17:49.791448 139444661233408 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.1074005365371704, loss=6.105449676513672
I0501 22:18:38.382663 139444669626112 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0796411037445068, loss=6.696401119232178
I0501 22:19:27.265733 139444661233408 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9374995231628418, loss=6.5132975578308105
I0501 22:20:16.117222 139444669626112 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9029698371887207, loss=6.058178424835205
I0501 22:21:04.632072 139444661233408 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7635306715965271, loss=6.049469947814941
I0501 22:21:53.503005 139444669626112 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7433268427848816, loss=5.981166362762451
I0501 22:22:42.247758 139444661233408 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7133554220199585, loss=6.412214279174805
I0501 22:23:31.208762 139444669626112 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8008664846420288, loss=5.901595115661621
I0501 22:23:39.548443 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:23:46.113043 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:23:53.038832 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:23:54.794197 139674332673856 submission_runner.py:415] Time since start: 1473.80s, 	Step: 2418, 	{'train/accuracy': 0.08955077826976776, 'train/loss': 5.159768104553223, 'validation/accuracy': 0.0830800011754036, 'validation/loss': 5.210400104522705, 'validation/num_examples': 50000, 'test/accuracy': 0.06270000338554382, 'test/loss': 5.441117286682129, 'test/num_examples': 10000, 'score': 1375.5669481754303, 'total_duration': 1473.8023755550385, 'accumulated_submission_time': 1375.5669481754303, 'accumulated_eval_time': 98.10727071762085, 'accumulated_logging_time': 0.08298063278198242}
I0501 22:23:54.805002 139444661233408 logging_writer.py:48] [2418] accumulated_eval_time=98.107271, accumulated_logging_time=0.082981, accumulated_submission_time=1375.566948, global_step=2418, preemption_count=0, score=1375.566948, test/accuracy=0.062700, test/loss=5.441117, test/num_examples=10000, total_duration=1473.802376, train/accuracy=0.089551, train/loss=5.159768, validation/accuracy=0.083080, validation/loss=5.210400, validation/num_examples=50000
I0501 22:24:35.681111 139444669626112 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8215505480766296, loss=5.834915637969971
I0501 22:25:24.584072 139444661233408 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8661072254180908, loss=6.255743980407715
I0501 22:26:13.000104 139444669626112 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6898595094680786, loss=5.943826198577881
I0501 22:27:01.753160 139444661233408 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.760221540927887, loss=5.904610633850098
I0501 22:27:50.386944 139444669626112 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6867664456367493, loss=5.880218029022217
I0501 22:28:39.344018 139444661233408 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7674770355224609, loss=5.6572065353393555
I0501 22:29:27.922375 139444669626112 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9445176124572754, loss=5.854786396026611
I0501 22:30:16.788483 139444661233408 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.866922914981842, loss=5.718600749969482
I0501 22:30:55.184631 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:31:01.772092 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:31:08.512851 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:31:10.264680 139674332673856 submission_runner.py:415] Time since start: 1909.27s, 	Step: 3280, 	{'train/accuracy': 0.12218749523162842, 'train/loss': 4.872628211975098, 'validation/accuracy': 0.1138399988412857, 'validation/loss': 4.9308671951293945, 'validation/num_examples': 50000, 'test/accuracy': 0.08890000730752945, 'test/loss': 5.186919689178467, 'test/num_examples': 10000, 'score': 1795.9175248146057, 'total_duration': 1909.2728300094604, 'accumulated_submission_time': 1795.9175248146057, 'accumulated_eval_time': 113.18725848197937, 'accumulated_logging_time': 0.10706710815429688}
I0501 22:31:10.280145 139444669626112 logging_writer.py:48] [3280] accumulated_eval_time=113.187258, accumulated_logging_time=0.107067, accumulated_submission_time=1795.917525, global_step=3280, preemption_count=0, score=1795.917525, test/accuracy=0.088900, test/loss=5.186920, test/num_examples=10000, total_duration=1909.272830, train/accuracy=0.122187, train/loss=4.872628, validation/accuracy=0.113840, validation/loss=4.930867, validation/num_examples=50000
I0501 22:31:20.744590 139444661233408 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7876377105712891, loss=5.853298664093018
I0501 22:32:09.738492 139444669626112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.897267758846283, loss=5.700910568237305
I0501 22:32:58.287821 139444661233408 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9278535842895508, loss=5.488275527954102
I0501 22:33:47.467129 139444669626112 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8263042569160461, loss=5.653067588806152
I0501 22:34:36.193526 139444661233408 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6894484758377075, loss=6.015467643737793
I0501 22:35:25.335315 139444669626112 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8006762266159058, loss=5.6577839851379395
I0501 22:36:14.348685 139444661233408 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7322219610214233, loss=5.495146751403809
I0501 22:37:03.184313 139444669626112 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7757003903388977, loss=5.56581974029541
I0501 22:37:51.861226 139444661233408 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7881931066513062, loss=5.321094512939453
I0501 22:38:10.611799 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:38:17.181598 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:38:23.963315 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:38:25.713910 139674332673856 submission_runner.py:415] Time since start: 2344.72s, 	Step: 4139, 	{'train/accuracy': 0.16751952469348907, 'train/loss': 4.410518646240234, 'validation/accuracy': 0.15015999972820282, 'validation/loss': 4.536258220672607, 'validation/num_examples': 50000, 'test/accuracy': 0.11500000208616257, 'test/loss': 4.887967586517334, 'test/num_examples': 10000, 'score': 2216.219784259796, 'total_duration': 2344.7220344543457, 'accumulated_submission_time': 2216.219784259796, 'accumulated_eval_time': 128.28926539421082, 'accumulated_logging_time': 0.13607454299926758}
I0501 22:38:25.729750 139444669626112 logging_writer.py:48] [4139] accumulated_eval_time=128.289265, accumulated_logging_time=0.136075, accumulated_submission_time=2216.219784, global_step=4139, preemption_count=0, score=2216.219784, test/accuracy=0.115000, test/loss=4.887968, test/num_examples=10000, total_duration=2344.722034, train/accuracy=0.167520, train/loss=4.410519, validation/accuracy=0.150160, validation/loss=4.536258, validation/num_examples=50000
I0501 22:38:56.438269 139444661233408 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6435214877128601, loss=5.931866645812988
I0501 22:39:45.239673 139444669626112 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8606062531471252, loss=5.289238929748535
I0501 22:40:34.235508 139444661233408 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7709458470344543, loss=5.299031734466553
I0501 22:41:23.073394 139444669626112 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7164668440818787, loss=5.3540849685668945
I0501 22:42:12.088837 139444661233408 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7761729955673218, loss=6.031617164611816
I0501 22:43:01.028047 139444669626112 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8424779772758484, loss=5.441598892211914
I0501 22:43:50.341407 139444661233408 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8188397288322449, loss=5.241904258728027
I0501 22:44:39.305735 139444669626112 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7705538868904114, loss=5.214205741882324
I0501 22:45:25.793792 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:45:32.387426 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:45:39.481285 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:45:41.234620 139674332673856 submission_runner.py:415] Time since start: 2780.24s, 	Step: 4996, 	{'train/accuracy': 0.21482421457767487, 'train/loss': 4.016725063323975, 'validation/accuracy': 0.19829998910427094, 'validation/loss': 4.110166549682617, 'validation/num_examples': 50000, 'test/accuracy': 0.14920000731945038, 'test/loss': 4.530595302581787, 'test/num_examples': 10000, 'score': 2636.253886461258, 'total_duration': 2780.2427973747253, 'accumulated_submission_time': 2636.253886461258, 'accumulated_eval_time': 143.73006224632263, 'accumulated_logging_time': 0.16613507270812988}
I0501 22:45:41.248771 139444661233408 logging_writer.py:48] [4996] accumulated_eval_time=143.730062, accumulated_logging_time=0.166135, accumulated_submission_time=2636.253886, global_step=4996, preemption_count=0, score=2636.253886, test/accuracy=0.149200, test/loss=4.530595, test/num_examples=10000, total_duration=2780.242797, train/accuracy=0.214824, train/loss=4.016725, validation/accuracy=0.198300, validation/loss=4.110167, validation/num_examples=50000
I0501 22:45:43.811772 139444669626112 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8064460754394531, loss=5.257443904876709
I0501 22:46:32.863825 139444661233408 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8221021294593811, loss=5.121213912963867
I0501 22:47:22.308103 139444669626112 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7959232330322266, loss=5.164099216461182
I0501 22:48:11.341270 139444661233408 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7065074443817139, loss=6.072582721710205
I0501 22:49:00.517910 139444669626112 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7156014442443848, loss=6.189248085021973
I0501 22:49:49.823238 139444661233408 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7836624979972839, loss=5.089917182922363
I0501 22:50:38.797136 139444669626112 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8165817260742188, loss=5.623495101928711
I0501 22:51:27.849683 139444661233408 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7743416428565979, loss=5.02059268951416
I0501 22:52:17.062273 139444669626112 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9366572499275208, loss=5.051619529724121
I0501 22:52:41.687997 139674332673856 spec.py:298] Evaluating on the training split.
I0501 22:52:48.249308 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 22:52:55.088473 139674332673856 spec.py:326] Evaluating on the test split.
I0501 22:52:56.837766 139674332673856 submission_runner.py:415] Time since start: 3215.85s, 	Step: 5851, 	{'train/accuracy': 0.2528710961341858, 'train/loss': 3.7637510299682617, 'validation/accuracy': 0.2340799868106842, 'validation/loss': 3.878903865814209, 'validation/num_examples': 50000, 'test/accuracy': 0.17250001430511475, 'test/loss': 4.353720664978027, 'test/num_examples': 10000, 'score': 3056.6637723445892, 'total_duration': 3215.84592962265, 'accumulated_submission_time': 3056.6637723445892, 'accumulated_eval_time': 158.87976837158203, 'accumulated_logging_time': 0.19379734992980957}
I0501 22:52:56.853505 139444661233408 logging_writer.py:48] [5851] accumulated_eval_time=158.879768, accumulated_logging_time=0.193797, accumulated_submission_time=3056.663772, global_step=5851, preemption_count=0, score=3056.663772, test/accuracy=0.172500, test/loss=4.353721, test/num_examples=10000, total_duration=3215.845930, train/accuracy=0.252871, train/loss=3.763751, validation/accuracy=0.234080, validation/loss=3.878904, validation/num_examples=50000
I0501 22:53:22.028963 139444669626112 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7489732503890991, loss=5.031874656677246
I0501 22:54:12.141078 139444661233408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6817640066146851, loss=5.106386661529541
I0501 22:55:02.395353 139444669626112 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6911501288414001, loss=6.2074761390686035
I0501 22:55:52.328485 139444661233408 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7025725245475769, loss=6.234616279602051
I0501 22:56:42.283741 139444669626112 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.803439736366272, loss=4.8604736328125
I0501 22:57:31.915937 139444661233408 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7725670337677002, loss=4.914407730102539
I0501 22:58:21.916628 139444669626112 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7105472683906555, loss=5.427105903625488
I0501 22:59:11.736384 139444661233408 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7399054169654846, loss=4.861142158508301
I0501 22:59:56.901507 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:00:03.526545 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:00:10.682499 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:00:12.435030 139674332673856 submission_runner.py:415] Time since start: 3651.44s, 	Step: 6691, 	{'train/accuracy': 0.2962304651737213, 'train/loss': 3.4981274604797363, 'validation/accuracy': 0.26187998056411743, 'validation/loss': 3.6747686862945557, 'validation/num_examples': 50000, 'test/accuracy': 0.1997000128030777, 'test/loss': 4.172412872314453, 'test/num_examples': 10000, 'score': 3476.677853345871, 'total_duration': 3651.4431920051575, 'accumulated_submission_time': 3476.677853345871, 'accumulated_eval_time': 174.4132227897644, 'accumulated_logging_time': 0.22710204124450684}
I0501 23:00:12.450732 139444669626112 logging_writer.py:48] [6691] accumulated_eval_time=174.413223, accumulated_logging_time=0.227102, accumulated_submission_time=3476.677853, global_step=6691, preemption_count=0, score=3476.677853, test/accuracy=0.199700, test/loss=4.172413, test/num_examples=10000, total_duration=3651.443192, train/accuracy=0.296230, train/loss=3.498127, validation/accuracy=0.261880, validation/loss=3.674769, validation/num_examples=50000
I0501 23:00:17.593153 139444661233408 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6541929244995117, loss=4.870718002319336
I0501 23:01:07.820206 139444669626112 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6933562755584717, loss=4.768269062042236
I0501 23:01:57.927319 139444661233408 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6669428944587708, loss=5.084268569946289
I0501 23:02:48.751702 139444669626112 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6305655837059021, loss=5.766858100891113
I0501 23:03:39.011323 139444661233408 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5967387557029724, loss=6.254602432250977
I0501 23:04:29.918044 139444669626112 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.77494215965271, loss=4.71169900894165
I0501 23:05:20.192432 139444661233408 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7494845986366272, loss=5.221803665161133
I0501 23:06:10.771668 139444669626112 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6931132674217224, loss=4.944293975830078
I0501 23:07:00.966477 139444661233408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6961135864257812, loss=4.659064769744873
I0501 23:07:12.465836 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:07:19.091952 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:07:26.185223 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:07:27.941183 139674332673856 submission_runner.py:415] Time since start: 4086.95s, 	Step: 7523, 	{'train/accuracy': 0.31550779938697815, 'train/loss': 3.331088066101074, 'validation/accuracy': 0.293179988861084, 'validation/loss': 3.4600484371185303, 'validation/num_examples': 50000, 'test/accuracy': 0.22180001437664032, 'test/loss': 3.9861233234405518, 'test/num_examples': 10000, 'score': 3896.662455558777, 'total_duration': 4086.94788980484, 'accumulated_submission_time': 3896.662455558777, 'accumulated_eval_time': 189.88712120056152, 'accumulated_logging_time': 0.2565450668334961}
I0501 23:07:27.957554 139444669626112 logging_writer.py:48] [7523] accumulated_eval_time=189.887121, accumulated_logging_time=0.256545, accumulated_submission_time=3896.662456, global_step=7523, preemption_count=0, score=3896.662456, test/accuracy=0.221800, test/loss=3.986123, test/num_examples=10000, total_duration=4086.947890, train/accuracy=0.315508, train/loss=3.331088, validation/accuracy=0.293180, validation/loss=3.460048, validation/num_examples=50000
I0501 23:08:07.318788 139444661233408 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7790572643280029, loss=4.669065475463867
I0501 23:08:58.218377 139444669626112 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7069074511528015, loss=5.640380859375
I0501 23:09:48.937849 139444661233408 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8353341221809387, loss=4.6995463371276855
I0501 23:10:39.750898 139444669626112 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6181827187538147, loss=4.782768249511719
I0501 23:11:30.028644 139444661233408 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.713508129119873, loss=6.009506702423096
I0501 23:12:20.656873 139444669626112 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7491152882575989, loss=4.704604625701904
I0501 23:13:11.156429 139444661233408 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7242142558097839, loss=4.603302955627441
I0501 23:14:02.164677 139444669626112 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7766478061676025, loss=4.701581954956055
I0501 23:14:28.332124 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:14:35.240322 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:14:42.487485 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:14:44.265723 139674332673856 submission_runner.py:415] Time since start: 4523.27s, 	Step: 8353, 	{'train/accuracy': 0.3450585901737213, 'train/loss': 3.1688172817230225, 'validation/accuracy': 0.3148599863052368, 'validation/loss': 3.3324038982391357, 'validation/num_examples': 50000, 'test/accuracy': 0.24370001256465912, 'test/loss': 3.8671798706054688, 'test/num_examples': 10000, 'score': 4317.006311655045, 'total_duration': 4523.273879766464, 'accumulated_submission_time': 4317.006311655045, 'accumulated_eval_time': 205.8206489086151, 'accumulated_logging_time': 0.28713083267211914}
I0501 23:14:44.282682 139444661233408 logging_writer.py:48] [8353] accumulated_eval_time=205.820649, accumulated_logging_time=0.287131, accumulated_submission_time=4317.006312, global_step=8353, preemption_count=0, score=4317.006312, test/accuracy=0.243700, test/loss=3.867180, test/num_examples=10000, total_duration=4523.273880, train/accuracy=0.345059, train/loss=3.168817, validation/accuracy=0.314860, validation/loss=3.332404, validation/num_examples=50000
I0501 23:15:09.212606 139444669626112 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6397311091423035, loss=5.159402847290039
I0501 23:16:00.056281 139444661233408 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5959640145301819, loss=4.993796348571777
I0501 23:16:50.310262 139444669626112 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8831204175949097, loss=4.516236305236816
I0501 23:17:41.265093 139444661233408 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7205279469490051, loss=6.125268936157227
I0501 23:18:31.742621 139444669626112 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7794720530509949, loss=4.52341365814209
I0501 23:19:22.581701 139444661233408 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0302637815475464, loss=4.621211051940918
I0501 23:20:13.063863 139444669626112 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8980438709259033, loss=4.706616401672363
I0501 23:21:03.850561 139444661233408 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7640750408172607, loss=4.695977687835693
I0501 23:21:44.638337 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:21:51.583205 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:21:58.659306 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:22:00.445507 139674332673856 submission_runner.py:415] Time since start: 4959.45s, 	Step: 9182, 	{'train/accuracy': 0.34882810711860657, 'train/loss': 3.1703295707702637, 'validation/accuracy': 0.3268599808216095, 'validation/loss': 3.2917654514312744, 'validation/num_examples': 50000, 'test/accuracy': 0.24890001118183136, 'test/loss': 3.81809401512146, 'test/num_examples': 10000, 'score': 4737.328916549683, 'total_duration': 4959.453677654266, 'accumulated_submission_time': 4737.328916549683, 'accumulated_eval_time': 221.627788066864, 'accumulated_logging_time': 0.3207366466522217}
I0501 23:22:00.463108 139444669626112 logging_writer.py:48] [9182] accumulated_eval_time=221.627788, accumulated_logging_time=0.320737, accumulated_submission_time=4737.328917, global_step=9182, preemption_count=0, score=4737.328917, test/accuracy=0.248900, test/loss=3.818094, test/num_examples=10000, total_duration=4959.453678, train/accuracy=0.348828, train/loss=3.170330, validation/accuracy=0.326860, validation/loss=3.291765, validation/num_examples=50000
I0501 23:22:10.869359 139444661233408 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7726041078567505, loss=4.5257062911987305
I0501 23:23:01.882408 139444669626112 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7664300203323364, loss=4.406254768371582
I0501 23:23:52.367237 139444661233408 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6928872466087341, loss=4.548678874969482
I0501 23:24:43.135937 139444669626112 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.63972008228302, loss=4.581533432006836
I0501 23:25:33.556755 139444661233408 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.935786247253418, loss=4.550078868865967
I0501 23:26:24.253872 139444669626112 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8903909921646118, loss=5.459530830383301
I0501 23:27:15.012241 139444661233408 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.925767719745636, loss=5.635725975036621
I0501 23:28:05.865373 139444669626112 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7683925628662109, loss=4.4238200187683105
I0501 23:28:56.330043 139444661233408 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8071263432502747, loss=4.499711513519287
I0501 23:29:00.860146 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:29:07.790029 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:29:14.889585 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:29:16.679656 139674332673856 submission_runner.py:415] Time since start: 5395.69s, 	Step: 10010, 	{'train/accuracy': 0.3848632872104645, 'train/loss': 2.9989681243896484, 'validation/accuracy': 0.3562600016593933, 'validation/loss': 3.1473562717437744, 'validation/num_examples': 50000, 'test/accuracy': 0.2703000009059906, 'test/loss': 3.701571226119995, 'test/num_examples': 10000, 'score': 5157.6530418396, 'total_duration': 5395.6878089904785, 'accumulated_submission_time': 5157.6530418396, 'accumulated_eval_time': 237.44723105430603, 'accumulated_logging_time': 0.39409971237182617}
I0501 23:29:16.696686 139444669626112 logging_writer.py:48] [10010] accumulated_eval_time=237.447231, accumulated_logging_time=0.394100, accumulated_submission_time=5157.653042, global_step=10010, preemption_count=0, score=5157.653042, test/accuracy=0.270300, test/loss=3.701571, test/num_examples=10000, total_duration=5395.687809, train/accuracy=0.384863, train/loss=2.998968, validation/accuracy=0.356260, validation/loss=3.147356, validation/num_examples=50000
I0501 23:30:03.511307 139444661233408 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6922035217285156, loss=5.6900224685668945
I0501 23:30:53.831019 139444669626112 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.913475513458252, loss=5.962433815002441
I0501 23:31:44.561138 139444661233408 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6957054138183594, loss=5.719883918762207
I0501 23:32:35.003007 139444669626112 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7349749803543091, loss=4.3218841552734375
I0501 23:33:25.799221 139444661233408 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9781841039657593, loss=4.618648529052734
I0501 23:34:15.992926 139444669626112 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8533094525337219, loss=4.65576696395874
I0501 23:35:06.671985 139444661233408 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8548741340637207, loss=5.913005828857422
I0501 23:35:57.126128 139444669626112 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.98628830909729, loss=4.968372821807861
I0501 23:36:16.872328 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:36:24.449605 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:36:31.706814 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:36:33.472834 139674332673856 submission_runner.py:415] Time since start: 5832.48s, 	Step: 10840, 	{'train/accuracy': 0.42769530415534973, 'train/loss': 2.7138445377349854, 'validation/accuracy': 0.3832399845123291, 'validation/loss': 2.9353668689727783, 'validation/num_examples': 50000, 'test/accuracy': 0.28850001096725464, 'test/loss': 3.5299103260040283, 'test/num_examples': 10000, 'score': 5577.7686223983765, 'total_duration': 5832.4809827804565, 'accumulated_submission_time': 5577.7686223983765, 'accumulated_eval_time': 254.04766058921814, 'accumulated_logging_time': 0.45459413528442383}
I0501 23:36:33.492903 139444661233408 logging_writer.py:48] [10840] accumulated_eval_time=254.047661, accumulated_logging_time=0.454594, accumulated_submission_time=5577.768622, global_step=10840, preemption_count=0, score=5577.768622, test/accuracy=0.288500, test/loss=3.529910, test/num_examples=10000, total_duration=5832.480983, train/accuracy=0.427695, train/loss=2.713845, validation/accuracy=0.383240, validation/loss=2.935367, validation/num_examples=50000
I0501 23:37:04.917516 139444669626112 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7721322774887085, loss=4.225292682647705
I0501 23:37:55.516389 139444661233408 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9412029385566711, loss=4.606808185577393
I0501 23:38:46.486119 139444669626112 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9336262345314026, loss=4.193284034729004
I0501 23:39:37.035304 139444661233408 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9730218052864075, loss=4.124152183532715
I0501 23:40:27.833786 139444669626112 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.777661144733429, loss=5.508809566497803
I0501 23:41:18.047557 139444661233408 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9038578867912292, loss=4.302769184112549
I0501 23:42:08.777888 139444669626112 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9502304196357727, loss=5.512792587280273
I0501 23:42:59.130332 139444661233408 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8153684735298157, loss=5.295161247253418
I0501 23:43:33.803212 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:43:40.894865 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:43:48.180215 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:43:49.931859 139674332673856 submission_runner.py:415] Time since start: 6268.94s, 	Step: 11669, 	{'train/accuracy': 0.4272851347923279, 'train/loss': 2.72206449508667, 'validation/accuracy': 0.39574000239372253, 'validation/loss': 2.8797767162323, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.4793076515197754, 'test/num_examples': 10000, 'score': 5998.043658256531, 'total_duration': 6268.940042972565, 'accumulated_submission_time': 5998.043658256531, 'accumulated_eval_time': 270.176255941391, 'accumulated_logging_time': 0.4910240173339844}
I0501 23:43:49.947365 139444669626112 logging_writer.py:48] [11669] accumulated_eval_time=270.176256, accumulated_logging_time=0.491024, accumulated_submission_time=5998.043658, global_step=11669, preemption_count=0, score=5998.043658, test/accuracy=0.299200, test/loss=3.479308, test/num_examples=10000, total_duration=6268.940043, train/accuracy=0.427285, train/loss=2.722064, validation/accuracy=0.395740, validation/loss=2.879777, validation/num_examples=50000
I0501 23:44:06.550710 139444661233408 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8011792302131653, loss=5.825037002563477
I0501 23:44:57.628791 139444669626112 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9956282377243042, loss=4.300421714782715
I0501 23:45:48.496602 139444661233408 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9043247103691101, loss=4.853405952453613
I0501 23:46:38.985433 139444669626112 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0857291221618652, loss=4.097109794616699
I0501 23:47:29.736293 139444661233408 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0588622093200684, loss=4.093194484710693
I0501 23:48:19.986132 139444669626112 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0042885541915894, loss=4.197597980499268
I0501 23:49:10.744281 139444661233408 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0832709074020386, loss=4.185415267944336
I0501 23:50:01.343064 139444669626112 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.071950078010559, loss=5.980997085571289
I0501 23:50:50.123326 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:50:58.371820 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:51:05.908024 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:51:07.653480 139674332673856 submission_runner.py:415] Time since start: 6706.66s, 	Step: 12497, 	{'train/accuracy': 0.4516015648841858, 'train/loss': 2.5749576091766357, 'validation/accuracy': 0.42093998193740845, 'validation/loss': 2.755376100540161, 'validation/num_examples': 50000, 'test/accuracy': 0.32510000467300415, 'test/loss': 3.3381431102752686, 'test/num_examples': 10000, 'score': 6418.182667016983, 'total_duration': 6706.661639928818, 'accumulated_submission_time': 6418.182667016983, 'accumulated_eval_time': 287.70635509490967, 'accumulated_logging_time': 0.526430606842041}
I0501 23:51:07.670706 139444661233408 logging_writer.py:48] [12497] accumulated_eval_time=287.706355, accumulated_logging_time=0.526431, accumulated_submission_time=6418.182667, global_step=12497, preemption_count=0, score=6418.182667, test/accuracy=0.325100, test/loss=3.338143, test/num_examples=10000, total_duration=6706.661640, train/accuracy=0.451602, train/loss=2.574958, validation/accuracy=0.420940, validation/loss=2.755376, validation/num_examples=50000
I0501 23:51:09.782358 139444669626112 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9282820820808411, loss=4.253450393676758
I0501 23:52:00.665640 139444661233408 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9780355095863342, loss=4.064114570617676
I0501 23:52:51.327984 139444669626112 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1111876964569092, loss=4.059534549713135
I0501 23:53:41.789175 139444661233408 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.03316330909729, loss=5.86079216003418
I0501 23:54:32.594679 139444669626112 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9173116683959961, loss=4.205020904541016
I0501 23:55:22.935428 139444661233408 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9323969483375549, loss=4.234220504760742
I0501 23:56:13.738325 139444669626112 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0493319034576416, loss=5.370195388793945
I0501 23:57:04.255394 139444661233408 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.0284996032714844, loss=4.1045241355896
I0501 23:57:55.217263 139444669626112 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.2714357376098633, loss=4.028176307678223
I0501 23:58:07.896170 139674332673856 spec.py:298] Evaluating on the training split.
I0501 23:58:15.612797 139674332673856 spec.py:310] Evaluating on the validation split.
I0501 23:58:23.070107 139674332673856 spec.py:326] Evaluating on the test split.
I0501 23:58:24.818834 139674332673856 submission_runner.py:415] Time since start: 7143.83s, 	Step: 13326, 	{'train/accuracy': 0.4691406190395355, 'train/loss': 2.4858171939849854, 'validation/accuracy': 0.42857998609542847, 'validation/loss': 2.688340187072754, 'validation/num_examples': 50000, 'test/accuracy': 0.3254000246524811, 'test/loss': 3.3119332790374756, 'test/num_examples': 10000, 'score': 6838.375488996506, 'total_duration': 7143.825047016144, 'accumulated_submission_time': 6838.375488996506, 'accumulated_eval_time': 304.6270191669464, 'accumulated_logging_time': 0.5598948001861572}
I0501 23:58:24.835150 139444661233408 logging_writer.py:48] [13326] accumulated_eval_time=304.627019, accumulated_logging_time=0.559895, accumulated_submission_time=6838.375489, global_step=13326, preemption_count=0, score=6838.375489, test/accuracy=0.325400, test/loss=3.311933, test/num_examples=10000, total_duration=7143.825047, train/accuracy=0.469141, train/loss=2.485817, validation/accuracy=0.428580, validation/loss=2.688340, validation/num_examples=50000
I0501 23:59:03.174352 139444669626112 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0333980321884155, loss=4.1728515625
I0501 23:59:54.155401 139444661233408 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0749115943908691, loss=4.107110500335693
I0502 00:00:44.633637 139444669626112 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9129867553710938, loss=3.9839885234832764
I0502 00:01:35.777738 139444661233408 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9479190707206726, loss=4.045563697814941
I0502 00:02:26.293526 139444669626112 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9337360858917236, loss=3.8839004039764404
I0502 00:03:17.062261 139444661233408 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9855267405509949, loss=4.036714553833008
I0502 00:04:07.722598 139444669626112 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.098737359046936, loss=5.856544494628906
I0502 00:04:58.143326 139444661233408 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0299696922302246, loss=4.765011310577393
I0502 00:05:25.195967 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:05:33.763926 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:05:41.301762 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:05:43.059092 139674332673856 submission_runner.py:415] Time since start: 7582.07s, 	Step: 14155, 	{'train/accuracy': 0.4715234339237213, 'train/loss': 2.4577696323394775, 'validation/accuracy': 0.43605998158454895, 'validation/loss': 2.6347227096557617, 'validation/num_examples': 50000, 'test/accuracy': 0.3362000286579132, 'test/loss': 3.2475883960723877, 'test/num_examples': 10000, 'score': 7258.70299077034, 'total_duration': 7582.065094470978, 'accumulated_submission_time': 7258.70299077034, 'accumulated_eval_time': 322.48793959617615, 'accumulated_logging_time': 0.5929391384124756}
I0502 00:05:43.077035 139444669626112 logging_writer.py:48] [14155] accumulated_eval_time=322.487940, accumulated_logging_time=0.592939, accumulated_submission_time=7258.702991, global_step=14155, preemption_count=0, score=7258.702991, test/accuracy=0.336200, test/loss=3.247588, test/num_examples=10000, total_duration=7582.065094, train/accuracy=0.471523, train/loss=2.457770, validation/accuracy=0.436060, validation/loss=2.634723, validation/num_examples=50000
I0502 00:06:07.086163 139444661233408 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0000455379486084, loss=4.101877689361572
I0502 00:06:57.728171 139444669626112 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.1172865629196167, loss=4.0463032722473145
I0502 00:07:48.307547 139444661233408 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9741604328155518, loss=4.33178186416626
I0502 00:08:39.510673 139444669626112 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0249269008636475, loss=4.727544784545898
I0502 00:09:30.089275 139444661233408 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0116055011749268, loss=4.675342559814453
I0502 00:10:21.116298 139444669626112 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.1436359882354736, loss=3.9117250442504883
I0502 00:11:11.907919 139444661233408 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.3646756410598755, loss=5.934026718139648
I0502 00:12:02.837836 139444669626112 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.2117787599563599, loss=4.122200012207031
I0502 00:12:43.188984 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:12:51.938262 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:12:59.862758 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:13:01.613327 139674332673856 submission_runner.py:415] Time since start: 8020.62s, 	Step: 14981, 	{'train/accuracy': 0.49779295921325684, 'train/loss': 2.3374860286712646, 'validation/accuracy': 0.45325997471809387, 'validation/loss': 2.5486438274383545, 'validation/num_examples': 50000, 'test/accuracy': 0.34700000286102295, 'test/loss': 3.1736245155334473, 'test/num_examples': 10000, 'score': 7678.770076751709, 'total_duration': 8020.621518611908, 'accumulated_submission_time': 7678.770076751709, 'accumulated_eval_time': 340.9122576713562, 'accumulated_logging_time': 0.639080286026001}
I0502 00:13:01.630507 139444661233408 logging_writer.py:48] [14981] accumulated_eval_time=340.912258, accumulated_logging_time=0.639080, accumulated_submission_time=7678.770077, global_step=14981, preemption_count=0, score=7678.770077, test/accuracy=0.347000, test/loss=3.173625, test/num_examples=10000, total_duration=8020.621519, train/accuracy=0.497793, train/loss=2.337486, validation/accuracy=0.453260, validation/loss=2.548644, validation/num_examples=50000
I0502 00:13:12.136713 139444669626112 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.064716100692749, loss=4.0103960037231445
I0502 00:14:03.296382 139444661233408 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0251654386520386, loss=4.969699382781982
I0502 00:14:54.327043 139444669626112 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9847918152809143, loss=4.417060852050781
I0502 00:15:44.906368 139444661233408 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.1259846687316895, loss=3.9133684635162354
I0502 00:16:35.881218 139444669626112 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.2541732788085938, loss=3.9789583683013916
I0502 00:17:26.340258 139444661233408 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.104836344718933, loss=3.857388734817505
I0502 00:18:17.141354 139444669626112 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9574349522590637, loss=3.766465187072754
I0502 00:19:07.623172 139444661233408 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.2643392086029053, loss=3.91802978515625
I0502 00:19:58.353290 139444669626112 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0815287828445435, loss=3.7977828979492188
I0502 00:20:01.894688 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:20:10.393078 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:20:18.849202 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:20:20.598765 139674332673856 submission_runner.py:415] Time since start: 8459.61s, 	Step: 15808, 	{'train/accuracy': 0.49199217557907104, 'train/loss': 2.328792095184326, 'validation/accuracy': 0.45802000164985657, 'validation/loss': 2.5054256916046143, 'validation/num_examples': 50000, 'test/accuracy': 0.35350000858306885, 'test/loss': 3.127781629562378, 'test/num_examples': 10000, 'score': 8098.9833953380585, 'total_duration': 8459.606926202774, 'accumulated_submission_time': 8098.9833953380585, 'accumulated_eval_time': 359.6162667274475, 'accumulated_logging_time': 0.6910319328308105}
I0502 00:20:20.616855 139444661233408 logging_writer.py:48] [15808] accumulated_eval_time=359.616267, accumulated_logging_time=0.691032, accumulated_submission_time=8098.983395, global_step=15808, preemption_count=0, score=8098.983395, test/accuracy=0.353500, test/loss=3.127782, test/num_examples=10000, total_duration=8459.606926, train/accuracy=0.491992, train/loss=2.328792, validation/accuracy=0.458020, validation/loss=2.505426, validation/num_examples=50000
I0502 00:21:08.042626 139444669626112 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.1244125366210938, loss=3.8504531383514404
I0502 00:21:58.509518 139444661233408 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.2577311992645264, loss=3.808600425720215
I0502 00:22:49.332805 139444669626112 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.1188832521438599, loss=4.028182506561279
I0502 00:23:39.805007 139444661233408 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.068834900856018, loss=4.310609817504883
I0502 00:24:30.764666 139444669626112 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0884039402008057, loss=4.112490653991699
I0502 00:25:21.384171 139444661233408 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.271628975868225, loss=3.7732603549957275
I0502 00:26:12.423617 139444669626112 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.3375253677368164, loss=3.73803448677063
I0502 00:27:02.860519 139444661233408 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.2086046934127808, loss=5.526180267333984
I0502 00:27:20.883262 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:27:29.346516 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:27:38.153298 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:27:39.896692 139674332673856 submission_runner.py:415] Time since start: 8898.90s, 	Step: 16636, 	{'train/accuracy': 0.5077539086341858, 'train/loss': 2.348529815673828, 'validation/accuracy': 0.4653799831867218, 'validation/loss': 2.5411765575408936, 'validation/num_examples': 50000, 'test/accuracy': 0.358100026845932, 'test/loss': 3.164151668548584, 'test/num_examples': 10000, 'score': 8519.220742464066, 'total_duration': 8898.90488409996, 'accumulated_submission_time': 8519.220742464066, 'accumulated_eval_time': 378.6296739578247, 'accumulated_logging_time': 0.721747636795044}
I0502 00:27:39.909993 139444669626112 logging_writer.py:48] [16636] accumulated_eval_time=378.629674, accumulated_logging_time=0.721748, accumulated_submission_time=8519.220742, global_step=16636, preemption_count=0, score=8519.220742, test/accuracy=0.358100, test/loss=3.164152, test/num_examples=10000, total_duration=8898.904884, train/accuracy=0.507754, train/loss=2.348530, validation/accuracy=0.465380, validation/loss=2.541177, validation/num_examples=50000
I0502 00:28:13.383530 139444661233408 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0933130979537964, loss=3.902423858642578
I0502 00:29:04.255103 139444669626112 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.292860984802246, loss=4.036932468414307
I0502 00:29:54.766685 139444661233408 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.2865999937057495, loss=5.724806785583496
I0502 00:30:45.208142 139444669626112 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3148409128189087, loss=3.897874116897583
I0502 00:31:35.688295 139444661233408 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.0828241109848022, loss=3.7790231704711914
I0502 00:32:26.623713 139444669626112 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.1650323867797852, loss=3.843684434890747
I0502 00:33:17.198787 139444661233408 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.2110631465911865, loss=3.7686374187469482
I0502 00:34:08.216763 139444669626112 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.1350668668746948, loss=4.1553192138671875
I0502 00:34:40.155377 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:34:48.817728 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:34:57.613656 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:34:59.356683 139674332673856 submission_runner.py:415] Time since start: 9338.36s, 	Step: 17464, 	{'train/accuracy': 0.5338085889816284, 'train/loss': 2.2077438831329346, 'validation/accuracy': 0.48037999868392944, 'validation/loss': 2.464365005493164, 'validation/num_examples': 50000, 'test/accuracy': 0.3742000162601471, 'test/loss': 3.0653090476989746, 'test/num_examples': 10000, 'score': 8939.434975385666, 'total_duration': 9338.364850282669, 'accumulated_submission_time': 8939.434975385666, 'accumulated_eval_time': 397.830947637558, 'accumulated_logging_time': 0.7493588924407959}
I0502 00:34:59.375422 139444661233408 logging_writer.py:48] [17464] accumulated_eval_time=397.830948, accumulated_logging_time=0.749359, accumulated_submission_time=8939.434975, global_step=17464, preemption_count=0, score=8939.434975, test/accuracy=0.374200, test/loss=3.065309, test/num_examples=10000, total_duration=9338.364850, train/accuracy=0.533809, train/loss=2.207744, validation/accuracy=0.480380, validation/loss=2.464365, validation/num_examples=50000
I0502 00:35:19.690542 139444669626112 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.1898459196090698, loss=5.313638687133789
I0502 00:36:11.639981 139444661233408 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.260316252708435, loss=3.8096506595611572
I0502 00:37:03.406718 139444669626112 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.3009517192840576, loss=3.945594310760498
I0502 00:37:55.044564 139444661233408 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.2608412504196167, loss=3.816810131072998
I0502 00:38:46.534599 139444669626112 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.1750366687774658, loss=3.825068950653076
I0502 00:39:38.301252 139444661233408 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0068693161010742, loss=4.740259647369385
I0502 00:40:29.819766 139444669626112 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.15879487991333, loss=3.715096950531006
I0502 00:41:21.863076 139444661233408 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.1169875860214233, loss=3.7506818771362305
I0502 00:41:59.598343 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:42:07.349022 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:42:15.900805 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:42:17.661508 139674332673856 submission_runner.py:415] Time since start: 9776.67s, 	Step: 18274, 	{'train/accuracy': 0.5253320336341858, 'train/loss': 2.1683266162872314, 'validation/accuracy': 0.48733997344970703, 'validation/loss': 2.36962628364563, 'validation/num_examples': 50000, 'test/accuracy': 0.37630000710487366, 'test/loss': 3.015493392944336, 'test/num_examples': 10000, 'score': 9359.629138231277, 'total_duration': 9776.66967344284, 'accumulated_submission_time': 9359.629138231277, 'accumulated_eval_time': 415.89405632019043, 'accumulated_logging_time': 0.7808575630187988}
I0502 00:42:17.678617 139444669626112 logging_writer.py:48] [18274] accumulated_eval_time=415.894056, accumulated_logging_time=0.780858, accumulated_submission_time=9359.629138, global_step=18274, preemption_count=0, score=9359.629138, test/accuracy=0.376300, test/loss=3.015493, test/num_examples=10000, total_duration=9776.669673, train/accuracy=0.525332, train/loss=2.168327, validation/accuracy=0.487340, validation/loss=2.369626, validation/num_examples=50000
I0502 00:42:31.916694 139444661233408 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.2213377952575684, loss=5.7763824462890625
I0502 00:43:22.965190 139444669626112 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1177632808685303, loss=3.726116180419922
I0502 00:44:13.545527 139444661233408 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1508567333221436, loss=3.7735204696655273
I0502 00:45:04.743044 139444669626112 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.5883828401565552, loss=5.681680202484131
I0502 00:45:55.299288 139444661233408 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.2923945188522339, loss=3.6430418491363525
I0502 00:46:46.483723 139444669626112 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.2054314613342285, loss=3.7004899978637695
I0502 00:47:36.879483 139444661233408 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2718408107757568, loss=3.667381525039673
I0502 00:48:27.571066 139444669626112 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.416088342666626, loss=5.836420059204102
I0502 00:49:17.936700 139444661233408 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.2182151079177856, loss=4.8101911544799805
I0502 00:49:17.962444 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:49:27.201465 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:49:35.756040 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:49:37.529116 139674332673856 submission_runner.py:415] Time since start: 10216.54s, 	Step: 19101, 	{'train/accuracy': 0.5433593392372131, 'train/loss': 2.100349187850952, 'validation/accuracy': 0.4976399838924408, 'validation/loss': 2.3239128589630127, 'validation/num_examples': 50000, 'test/accuracy': 0.38430002331733704, 'test/loss': 2.948103189468384, 'test/num_examples': 10000, 'score': 9779.884184360504, 'total_duration': 10216.537297010422, 'accumulated_submission_time': 9779.884184360504, 'accumulated_eval_time': 435.4606990814209, 'accumulated_logging_time': 0.8098547458648682}
I0502 00:49:37.546706 139444669626112 logging_writer.py:48] [19101] accumulated_eval_time=435.460699, accumulated_logging_time=0.809855, accumulated_submission_time=9779.884184, global_step=19101, preemption_count=0, score=9779.884184, test/accuracy=0.384300, test/loss=2.948103, test/num_examples=10000, total_duration=10216.537297, train/accuracy=0.543359, train/loss=2.100349, validation/accuracy=0.497640, validation/loss=2.323913, validation/num_examples=50000
I0502 00:50:28.930669 139444661233408 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1851282119750977, loss=4.715259075164795
I0502 00:51:19.462976 139444669626112 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.1868990659713745, loss=3.6705586910247803
I0502 00:52:10.299609 139444661233408 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.431772232055664, loss=3.703813076019287
I0502 00:53:00.620110 139444669626112 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.1878432035446167, loss=4.1142988204956055
I0502 00:53:51.647956 139444661233408 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.9156028032302856, loss=5.703287601470947
I0502 00:54:41.857201 139444669626112 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.2872178554534912, loss=3.7599098682403564
I0502 00:55:32.657539 139444661233408 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.2153964042663574, loss=4.254632472991943
I0502 00:56:23.019587 139444669626112 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.2301594018936157, loss=3.805767059326172
I0502 00:56:37.733144 139674332673856 spec.py:298] Evaluating on the training split.
I0502 00:56:45.753573 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 00:56:54.288229 139674332673856 spec.py:326] Evaluating on the test split.
I0502 00:56:56.038311 139674332673856 submission_runner.py:415] Time since start: 10655.05s, 	Step: 19930, 	{'train/accuracy': 0.5494921803474426, 'train/loss': 2.0221712589263916, 'validation/accuracy': 0.5053200125694275, 'validation/loss': 2.235727071762085, 'validation/num_examples': 50000, 'test/accuracy': 0.3937000334262848, 'test/loss': 2.8814697265625, 'test/num_examples': 10000, 'score': 10200.017252445221, 'total_duration': 10655.046504497528, 'accumulated_submission_time': 10200.017252445221, 'accumulated_eval_time': 453.76585626602173, 'accumulated_logging_time': 0.8638229370117188}
I0502 00:56:56.058586 139444661233408 logging_writer.py:48] [19930] accumulated_eval_time=453.765856, accumulated_logging_time=0.863823, accumulated_submission_time=10200.017252, global_step=19930, preemption_count=0, score=10200.017252, test/accuracy=0.393700, test/loss=2.881470, test/num_examples=10000, total_duration=10655.046504, train/accuracy=0.549492, train/loss=2.022171, validation/accuracy=0.505320, validation/loss=2.235727, validation/num_examples=50000
I0502 00:57:32.856057 139444669626112 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1355161666870117, loss=3.6242990493774414
I0502 00:58:23.506306 139444661233408 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.2748535871505737, loss=5.481021404266357
I0502 00:59:14.383279 139444669626112 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.3548206090927124, loss=3.645690679550171
I0502 01:00:04.783889 139444661233408 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.3761417865753174, loss=5.528965473175049
I0502 01:00:55.636287 139444669626112 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.5593336820602417, loss=3.7232401371002197
I0502 01:01:45.673559 139444661233408 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.4397050142288208, loss=3.602370500564575
I0502 01:02:36.358157 139444669626112 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.377669095993042, loss=4.190327167510986
I0502 01:03:26.886291 139444661233408 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.2341852188110352, loss=3.7475473880767822
I0502 01:03:56.511466 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:04:04.465367 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:04:12.880609 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:04:14.628187 139674332673856 submission_runner.py:415] Time since start: 11093.64s, 	Step: 20759, 	{'train/accuracy': 0.553027331829071, 'train/loss': 2.0009143352508545, 'validation/accuracy': 0.5134599804878235, 'validation/loss': 2.207019090652466, 'validation/num_examples': 50000, 'test/accuracy': 0.39730000495910645, 'test/loss': 2.8512375354766846, 'test/num_examples': 10000, 'score': 10620.44093990326, 'total_duration': 11093.63636636734, 'accumulated_submission_time': 10620.44093990326, 'accumulated_eval_time': 471.8825533390045, 'accumulated_logging_time': 0.8967299461364746}
I0502 01:04:14.642148 139444669626112 logging_writer.py:48] [20759] accumulated_eval_time=471.882553, accumulated_logging_time=0.896730, accumulated_submission_time=10620.440940, global_step=20759, preemption_count=0, score=10620.440940, test/accuracy=0.397300, test/loss=2.851238, test/num_examples=10000, total_duration=11093.636366, train/accuracy=0.553027, train/loss=2.000914, validation/accuracy=0.513460, validation/loss=2.207019, validation/num_examples=50000
I0502 01:04:36.184878 139444661233408 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.1953167915344238, loss=3.882894515991211
I0502 01:05:27.162266 139444669626112 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.3038339614868164, loss=3.5871739387512207
I0502 01:06:17.778288 139444661233408 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.3259937763214111, loss=5.25822639465332
I0502 01:07:08.312202 139444669626112 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.2688895463943481, loss=4.623007774353027
I0502 01:07:59.244580 139444661233408 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.097811698913574, loss=5.126582145690918
I0502 01:08:49.733760 139444669626112 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.2168350219726562, loss=3.4524054527282715
I0502 01:09:40.276916 139444661233408 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.3748141527175903, loss=4.914051055908203
I0502 01:10:30.725662 139444669626112 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.3012008666992188, loss=4.242650032043457
I0502 01:11:14.676877 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:11:22.534459 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:11:31.067781 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:11:32.813174 139674332673856 submission_runner.py:415] Time since start: 11531.82s, 	Step: 21587, 	{'train/accuracy': 0.5649218559265137, 'train/loss': 1.9944509267807007, 'validation/accuracy': 0.5170599818229675, 'validation/loss': 2.2341129779815674, 'validation/num_examples': 50000, 'test/accuracy': 0.40460002422332764, 'test/loss': 2.8678736686706543, 'test/num_examples': 10000, 'score': 11040.445036172867, 'total_duration': 11531.8213596344, 'accumulated_submission_time': 11040.445036172867, 'accumulated_eval_time': 490.01882314682007, 'accumulated_logging_time': 0.92466139793396}
I0502 01:11:32.831467 139444661233408 logging_writer.py:48] [21587] accumulated_eval_time=490.018823, accumulated_logging_time=0.924661, accumulated_submission_time=11040.445036, global_step=21587, preemption_count=0, score=11040.445036, test/accuracy=0.404600, test/loss=2.867874, test/num_examples=10000, total_duration=11531.821360, train/accuracy=0.564922, train/loss=1.994451, validation/accuracy=0.517060, validation/loss=2.234113, validation/num_examples=50000
I0502 01:11:39.935384 139444669626112 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.469669222831726, loss=3.759687662124634
I0502 01:12:30.821977 139444661233408 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2925817966461182, loss=4.902216911315918
I0502 01:13:21.622860 139444669626112 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.4302806854248047, loss=3.563166856765747
I0502 01:14:12.080285 139444661233408 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.243956446647644, loss=4.3083319664001465
I0502 01:15:02.771934 139444669626112 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.3887537717819214, loss=4.926673889160156
I0502 01:15:53.196188 139444661233408 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3143631219863892, loss=3.598680019378662
I0502 01:16:44.115017 139444669626112 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.4723068475723267, loss=5.5853271484375
I0502 01:17:34.444851 139444661233408 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3746123313903809, loss=3.5900683403015137
I0502 01:18:25.184874 139444669626112 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.5921064615249634, loss=5.672095775604248
I0502 01:18:33.327698 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:18:41.427177 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:18:50.061965 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:18:51.816477 139674332673856 submission_runner.py:415] Time since start: 11970.82s, 	Step: 22417, 	{'train/accuracy': 0.568066418170929, 'train/loss': 1.9755398035049438, 'validation/accuracy': 0.5228399634361267, 'validation/loss': 2.1831674575805664, 'validation/num_examples': 50000, 'test/accuracy': 0.4053000211715698, 'test/loss': 2.8376688957214355, 'test/num_examples': 10000, 'score': 11460.9106798172, 'total_duration': 11970.82464647293, 'accumulated_submission_time': 11460.9106798172, 'accumulated_eval_time': 508.50755858421326, 'accumulated_logging_time': 0.9571468830108643}
I0502 01:18:51.836742 139444661233408 logging_writer.py:48] [22417] accumulated_eval_time=508.507559, accumulated_logging_time=0.957147, accumulated_submission_time=11460.910680, global_step=22417, preemption_count=0, score=11460.910680, test/accuracy=0.405300, test/loss=2.837669, test/num_examples=10000, total_duration=11970.824646, train/accuracy=0.568066, train/loss=1.975540, validation/accuracy=0.522840, validation/loss=2.183167, validation/num_examples=50000
I0502 01:19:34.711704 139444669626112 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3006882667541504, loss=3.971560478210449
I0502 01:20:25.686556 139444661233408 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.1252641677856445, loss=3.504657745361328
I0502 01:21:16.294436 139444669626112 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.3267103433609009, loss=3.5779964923858643
I0502 01:22:07.040657 139444661233408 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.339850902557373, loss=3.441168785095215
I0502 01:22:57.565516 139444669626112 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.4638586044311523, loss=3.5559041500091553
I0502 01:23:48.541104 139444661233408 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.3682432174682617, loss=4.676424503326416
I0502 01:24:38.997819 139444669626112 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.6817137002944946, loss=5.431567668914795
I0502 01:25:29.793188 139444661233408 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.266209363937378, loss=3.785613775253296
I0502 01:25:52.012547 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:26:00.040506 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:26:08.722777 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:26:10.464548 139674332673856 submission_runner.py:415] Time since start: 12409.47s, 	Step: 23245, 	{'train/accuracy': 0.5717968344688416, 'train/loss': 1.9142189025878906, 'validation/accuracy': 0.5251799821853638, 'validation/loss': 2.148688793182373, 'validation/num_examples': 50000, 'test/accuracy': 0.4123000204563141, 'test/loss': 2.7936477661132812, 'test/num_examples': 10000, 'score': 11881.056750774384, 'total_duration': 12409.47264957428, 'accumulated_submission_time': 11881.056750774384, 'accumulated_eval_time': 526.9594578742981, 'accumulated_logging_time': 0.9905316829681396}
I0502 01:26:10.484929 139444669626112 logging_writer.py:48] [23245] accumulated_eval_time=526.959458, accumulated_logging_time=0.990532, accumulated_submission_time=11881.056751, global_step=23245, preemption_count=0, score=11881.056751, test/accuracy=0.412300, test/loss=2.793648, test/num_examples=10000, total_duration=12409.472650, train/accuracy=0.571797, train/loss=1.914219, validation/accuracy=0.525180, validation/loss=2.148689, validation/num_examples=50000
I0502 01:26:38.991663 139444661233408 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5531047582626343, loss=3.4988579750061035
I0502 01:27:30.099192 139444669626112 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.379544973373413, loss=3.697373390197754
I0502 01:28:20.567604 139444661233408 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.521571397781372, loss=5.263891220092773
I0502 01:29:11.524337 139444669626112 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.4419194459915161, loss=4.642879962921143
I0502 01:30:01.846665 139444661233408 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3748719692230225, loss=3.5892279148101807
I0502 01:30:52.651386 139444669626112 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.344800591468811, loss=3.659554958343506
I0502 01:31:42.767870 139444661233408 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4110623598098755, loss=3.5439388751983643
I0502 01:32:33.273278 139444669626112 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2666939496994019, loss=4.722455024719238
I0502 01:33:10.993033 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:33:18.766367 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:33:27.438007 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:33:29.183517 139674332673856 submission_runner.py:415] Time since start: 12848.19s, 	Step: 24076, 	{'train/accuracy': 0.5941601395606995, 'train/loss': 1.7937589883804321, 'validation/accuracy': 0.5328199863433838, 'validation/loss': 2.098494291305542, 'validation/num_examples': 50000, 'test/accuracy': 0.41620001196861267, 'test/loss': 2.7552809715270996, 'test/num_examples': 10000, 'score': 12301.535136938095, 'total_duration': 12848.191681623459, 'accumulated_submission_time': 12301.535136938095, 'accumulated_eval_time': 545.1499035358429, 'accumulated_logging_time': 1.0239295959472656}
I0502 01:33:29.204403 139444661233408 logging_writer.py:48] [24076] accumulated_eval_time=545.149904, accumulated_logging_time=1.023930, accumulated_submission_time=12301.535137, global_step=24076, preemption_count=0, score=12301.535137, test/accuracy=0.416200, test/loss=2.755281, test/num_examples=10000, total_duration=12848.191682, train/accuracy=0.594160, train/loss=1.793759, validation/accuracy=0.532820, validation/loss=2.098494, validation/num_examples=50000
I0502 01:33:42.035372 139444669626112 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.314828634262085, loss=3.5042667388916016
I0502 01:34:33.067518 139444661233408 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.330156683921814, loss=3.832517147064209
I0502 01:35:23.321846 139444669626112 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.420992136001587, loss=5.191378593444824
I0502 01:36:13.902847 139444661233408 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.4760818481445312, loss=5.212706565856934
I0502 01:37:04.105695 139444669626112 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.2684417963027954, loss=3.3929150104522705
I0502 01:37:54.975790 139444661233408 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.288100004196167, loss=3.4306676387786865
I0502 01:38:45.534285 139444669626112 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.3746895790100098, loss=3.407491445541382
I0502 01:39:36.426179 139444661233408 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.1612151861190796, loss=3.419816732406616
I0502 01:40:26.898981 139444669626112 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.195632815361023, loss=3.462986946105957
I0502 01:40:29.422102 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:40:37.399505 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:40:46.135457 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:40:47.895717 139674332673856 submission_runner.py:415] Time since start: 13286.90s, 	Step: 24906, 	{'train/accuracy': 0.5870312452316284, 'train/loss': 1.8558127880096436, 'validation/accuracy': 0.5435999631881714, 'validation/loss': 2.051222801208496, 'validation/num_examples': 50000, 'test/accuracy': 0.4296000301837921, 'test/loss': 2.7143967151641846, 'test/num_examples': 10000, 'score': 12721.723054409027, 'total_duration': 13286.903891801834, 'accumulated_submission_time': 12721.723054409027, 'accumulated_eval_time': 563.6234745979309, 'accumulated_logging_time': 1.0578880310058594}
I0502 01:40:47.910610 139444661233408 logging_writer.py:48] [24906] accumulated_eval_time=563.623475, accumulated_logging_time=1.057888, accumulated_submission_time=12721.723054, global_step=24906, preemption_count=0, score=12721.723054, test/accuracy=0.429600, test/loss=2.714397, test/num_examples=10000, total_duration=13286.903892, train/accuracy=0.587031, train/loss=1.855813, validation/accuracy=0.543600, validation/loss=2.051223, validation/num_examples=50000
I0502 01:41:36.628784 139444669626112 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.5057293176651, loss=3.6007559299468994
I0502 01:42:27.249414 139444661233408 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.2948737144470215, loss=3.603137969970703
I0502 01:43:18.247398 139444669626112 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.3483057022094727, loss=3.8219964504241943
I0502 01:44:08.786717 139444661233408 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.3452986478805542, loss=3.367652416229248
I0502 01:44:59.865008 139444669626112 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.316246747970581, loss=3.463747501373291
I0502 01:45:50.659798 139444661233408 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.5014636516571045, loss=3.467102289199829
I0502 01:46:41.258019 139444669626112 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.4062554836273193, loss=3.3663413524627686
I0502 01:47:31.583703 139444661233408 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.3668859004974365, loss=3.742142677307129
I0502 01:47:48.134783 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:47:56.380613 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:48:05.000482 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:48:06.746670 139674332673856 submission_runner.py:415] Time since start: 13725.75s, 	Step: 25733, 	{'train/accuracy': 0.595410168170929, 'train/loss': 1.7982745170593262, 'validation/accuracy': 0.5427199602127075, 'validation/loss': 2.047550916671753, 'validation/num_examples': 50000, 'test/accuracy': 0.428600013256073, 'test/loss': 2.692305088043213, 'test/num_examples': 10000, 'score': 13141.918692350388, 'total_duration': 13725.75484251976, 'accumulated_submission_time': 13141.918692350388, 'accumulated_eval_time': 582.2353172302246, 'accumulated_logging_time': 1.0846836566925049}
I0502 01:48:06.767264 139444669626112 logging_writer.py:48] [25733] accumulated_eval_time=582.235317, accumulated_logging_time=1.084684, accumulated_submission_time=13141.918692, global_step=25733, preemption_count=0, score=13141.918692, test/accuracy=0.428600, test/loss=2.692305, test/num_examples=10000, total_duration=13725.754843, train/accuracy=0.595410, train/loss=1.798275, validation/accuracy=0.542720, validation/loss=2.047551, validation/num_examples=50000
I0502 01:48:41.566339 139444661233408 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.3599939346313477, loss=3.608849287033081
I0502 01:49:32.384062 139444669626112 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.87556791305542, loss=5.660292625427246
I0502 01:50:23.064813 139444661233408 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.324479579925537, loss=3.515717029571533
I0502 01:51:13.886581 139444669626112 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.3100378513336182, loss=3.3418290615081787
I0502 01:52:04.594294 139444661233408 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.3233915567398071, loss=5.604244232177734
I0502 01:52:55.245787 139444669626112 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.4136961698532104, loss=3.5089030265808105
I0502 01:53:45.768658 139444661233408 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.1183507442474365, loss=5.5727949142456055
I0502 01:54:36.560374 139444669626112 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.2543351650238037, loss=3.7847986221313477
I0502 01:55:06.841378 139674332673856 spec.py:298] Evaluating on the training split.
I0502 01:55:14.928356 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 01:55:23.529390 139674332673856 spec.py:326] Evaluating on the test split.
I0502 01:55:25.268048 139674332673856 submission_runner.py:415] Time since start: 14164.28s, 	Step: 26561, 	{'train/accuracy': 0.5968359112739563, 'train/loss': 1.7940090894699097, 'validation/accuracy': 0.5536400079727173, 'validation/loss': 2.0078365802764893, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.6519391536712646, 'test/num_examples': 10000, 'score': 13561.960940361023, 'total_duration': 14164.276211500168, 'accumulated_submission_time': 13561.960940361023, 'accumulated_eval_time': 600.6619396209717, 'accumulated_logging_time': 1.1202125549316406}
I0502 01:55:25.287992 139444661233408 logging_writer.py:48] [26561] accumulated_eval_time=600.661940, accumulated_logging_time=1.120213, accumulated_submission_time=13561.960940, global_step=26561, preemption_count=0, score=13561.960940, test/accuracy=0.434000, test/loss=2.651939, test/num_examples=10000, total_duration=14164.276212, train/accuracy=0.596836, train/loss=1.794009, validation/accuracy=0.553640, validation/loss=2.007837, validation/num_examples=50000
I0502 01:55:45.607180 139444669626112 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.3628301620483398, loss=3.4444429874420166
I0502 01:56:36.587150 139444661233408 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8628321886062622, loss=5.564863204956055
I0502 01:57:27.126121 139444669626112 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.0812697410583496, loss=5.445051670074463
I0502 01:58:18.021456 139444661233408 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.4352424144744873, loss=3.4727914333343506
I0502 01:59:08.884734 139444669626112 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.308403491973877, loss=3.368239164352417
I0502 01:59:59.779066 139444661233408 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2975494861602783, loss=3.5755105018615723
I0502 02:00:50.502146 139444669626112 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.3706978559494019, loss=4.152740478515625
I0502 02:01:41.267824 139444661233408 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.447187066078186, loss=3.5894429683685303
I0502 02:02:25.644359 139674332673856 spec.py:298] Evaluating on the training split.
I0502 02:02:33.825812 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 02:02:42.268153 139674332673856 spec.py:326] Evaluating on the test split.
I0502 02:02:44.003564 139674332673856 submission_runner.py:415] Time since start: 14603.01s, 	Step: 27389, 	{'train/accuracy': 0.6099609136581421, 'train/loss': 1.7598166465759277, 'validation/accuracy': 0.5626400113105774, 'validation/loss': 1.9834723472595215, 'validation/num_examples': 50000, 'test/accuracy': 0.4386000335216522, 'test/loss': 2.6456127166748047, 'test/num_examples': 10000, 'score': 13982.287277936935, 'total_duration': 14603.011756896973, 'accumulated_submission_time': 13982.287277936935, 'accumulated_eval_time': 619.0211308002472, 'accumulated_logging_time': 1.1531453132629395}
I0502 02:02:44.020157 139444669626112 logging_writer.py:48] [27389] accumulated_eval_time=619.021131, accumulated_logging_time=1.153145, accumulated_submission_time=13982.287278, global_step=27389, preemption_count=0, score=13982.287278, test/accuracy=0.438600, test/loss=2.645613, test/num_examples=10000, total_duration=14603.011757, train/accuracy=0.609961, train/loss=1.759817, validation/accuracy=0.562640, validation/loss=1.983472, validation/num_examples=50000
I0502 02:02:50.202824 139444661233408 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.4611793756484985, loss=4.56561279296875
I0502 02:03:41.758388 139444669626112 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5505326986312866, loss=5.122161865234375
I0502 02:04:32.042893 139444661233408 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.5819607973098755, loss=4.485635757446289
I0502 02:05:22.663877 139444669626112 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.4690691232681274, loss=4.979671001434326
I0502 02:06:12.657823 139444661233408 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.2467740774154663, loss=3.3955721855163574
I0502 02:07:03.118408 139444669626112 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.3315675258636475, loss=3.827643632888794
I0502 02:07:52.723150 139674332673856 spec.py:298] Evaluating on the training split.
I0502 02:08:00.538406 139674332673856 spec.py:310] Evaluating on the validation split.
I0502 02:08:09.118200 139674332673856 spec.py:326] Evaluating on the test split.
I0502 02:08:10.864423 139674332673856 submission_runner.py:415] Time since start: 14929.87s, 	Step: 28000, 	{'train/accuracy': 0.6187109351158142, 'train/loss': 1.7497375011444092, 'validation/accuracy': 0.558739960193634, 'validation/loss': 2.011604070663452, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.666518449783325, 'test/num_examples': 10000, 'score': 14290.964758872986, 'total_duration': 14929.872590303421, 'accumulated_submission_time': 14290.964758872986, 'accumulated_eval_time': 637.1623675823212, 'accumulated_logging_time': 1.182924509048462}
I0502 02:08:10.886507 139444661233408 logging_writer.py:48] [28000] accumulated_eval_time=637.162368, accumulated_logging_time=1.182925, accumulated_submission_time=14290.964759, global_step=28000, preemption_count=0, score=14290.964759, test/accuracy=0.441000, test/loss=2.666518, test/num_examples=10000, total_duration=14929.872590, train/accuracy=0.618711, train/loss=1.749738, validation/accuracy=0.558740, validation/loss=2.011604, validation/num_examples=50000
I0502 02:08:10.917469 139444669626112 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=14290.964759
I0502 02:08:11.089061 139674332673856 checkpoints.py:356] Saving checkpoint at step: 28000
I0502 02:08:11.753622 139674332673856 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1/checkpoint_28000
I0502 02:08:11.766520 139674332673856 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_vit_jax/trial_1/checkpoint_28000.
I0502 02:08:12.377786 139674332673856 submission_runner.py:578] Tuning trial 1/1
I0502 02:08:12.378055 139674332673856 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 02:08:12.389990 139674332673856 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0022070312406867743, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0019600000232458115, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0014000000664964318, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 115.18545484542847, 'total_duration': 167.50329041481018, 'accumulated_submission_time': 115.18545484542847, 'accumulated_eval_time': 52.317665815353394, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (696, {'train/accuracy': 0.017519531771540642, 'train/loss': 6.262195110321045, 'validation/accuracy': 0.018459999933838844, 'validation/loss': 6.277126789093018, 'validation/num_examples': 50000, 'test/accuracy': 0.013000000268220901, 'test/loss': 6.339540004730225, 'test/num_examples': 10000, 'score': 535.4674348831177, 'total_duration': 603.1611816883087, 'accumulated_submission_time': 535.4674348831177, 'accumulated_eval_time': 67.65379047393799, 'accumulated_logging_time': 0.026494503021240234, 'global_step': 696, 'preemption_count': 0}), (1557, {'train/accuracy': 0.05316406115889549, 'train/loss': 5.635303974151611, 'validation/accuracy': 0.05107999965548515, 'validation/loss': 5.677877426147461, 'validation/num_examples': 50000, 'test/accuracy': 0.03710000216960907, 'test/loss': 5.828189849853516, 'test/num_examples': 10000, 'score': 955.4949903488159, 'total_duration': 1038.439501285553, 'accumulated_submission_time': 955.4949903488159, 'accumulated_eval_time': 82.86155104637146, 'accumulated_logging_time': 0.05360054969787598, 'global_step': 1557, 'preemption_count': 0}), (2418, {'train/accuracy': 0.08955077826976776, 'train/loss': 5.159768104553223, 'validation/accuracy': 0.0830800011754036, 'validation/loss': 5.210400104522705, 'validation/num_examples': 50000, 'test/accuracy': 0.06270000338554382, 'test/loss': 5.441117286682129, 'test/num_examples': 10000, 'score': 1375.5669481754303, 'total_duration': 1473.8023755550385, 'accumulated_submission_time': 1375.5669481754303, 'accumulated_eval_time': 98.10727071762085, 'accumulated_logging_time': 0.08298063278198242, 'global_step': 2418, 'preemption_count': 0}), (3280, {'train/accuracy': 0.12218749523162842, 'train/loss': 4.872628211975098, 'validation/accuracy': 0.1138399988412857, 'validation/loss': 4.9308671951293945, 'validation/num_examples': 50000, 'test/accuracy': 0.08890000730752945, 'test/loss': 5.186919689178467, 'test/num_examples': 10000, 'score': 1795.9175248146057, 'total_duration': 1909.2728300094604, 'accumulated_submission_time': 1795.9175248146057, 'accumulated_eval_time': 113.18725848197937, 'accumulated_logging_time': 0.10706710815429688, 'global_step': 3280, 'preemption_count': 0}), (4139, {'train/accuracy': 0.16751952469348907, 'train/loss': 4.410518646240234, 'validation/accuracy': 0.15015999972820282, 'validation/loss': 4.536258220672607, 'validation/num_examples': 50000, 'test/accuracy': 0.11500000208616257, 'test/loss': 4.887967586517334, 'test/num_examples': 10000, 'score': 2216.219784259796, 'total_duration': 2344.7220344543457, 'accumulated_submission_time': 2216.219784259796, 'accumulated_eval_time': 128.28926539421082, 'accumulated_logging_time': 0.13607454299926758, 'global_step': 4139, 'preemption_count': 0}), (4996, {'train/accuracy': 0.21482421457767487, 'train/loss': 4.016725063323975, 'validation/accuracy': 0.19829998910427094, 'validation/loss': 4.110166549682617, 'validation/num_examples': 50000, 'test/accuracy': 0.14920000731945038, 'test/loss': 4.530595302581787, 'test/num_examples': 10000, 'score': 2636.253886461258, 'total_duration': 2780.2427973747253, 'accumulated_submission_time': 2636.253886461258, 'accumulated_eval_time': 143.73006224632263, 'accumulated_logging_time': 0.16613507270812988, 'global_step': 4996, 'preemption_count': 0}), (5851, {'train/accuracy': 0.2528710961341858, 'train/loss': 3.7637510299682617, 'validation/accuracy': 0.2340799868106842, 'validation/loss': 3.878903865814209, 'validation/num_examples': 50000, 'test/accuracy': 0.17250001430511475, 'test/loss': 4.353720664978027, 'test/num_examples': 10000, 'score': 3056.6637723445892, 'total_duration': 3215.84592962265, 'accumulated_submission_time': 3056.6637723445892, 'accumulated_eval_time': 158.87976837158203, 'accumulated_logging_time': 0.19379734992980957, 'global_step': 5851, 'preemption_count': 0}), (6691, {'train/accuracy': 0.2962304651737213, 'train/loss': 3.4981274604797363, 'validation/accuracy': 0.26187998056411743, 'validation/loss': 3.6747686862945557, 'validation/num_examples': 50000, 'test/accuracy': 0.1997000128030777, 'test/loss': 4.172412872314453, 'test/num_examples': 10000, 'score': 3476.677853345871, 'total_duration': 3651.4431920051575, 'accumulated_submission_time': 3476.677853345871, 'accumulated_eval_time': 174.4132227897644, 'accumulated_logging_time': 0.22710204124450684, 'global_step': 6691, 'preemption_count': 0}), (7523, {'train/accuracy': 0.31550779938697815, 'train/loss': 3.331088066101074, 'validation/accuracy': 0.293179988861084, 'validation/loss': 3.4600484371185303, 'validation/num_examples': 50000, 'test/accuracy': 0.22180001437664032, 'test/loss': 3.9861233234405518, 'test/num_examples': 10000, 'score': 3896.662455558777, 'total_duration': 4086.94788980484, 'accumulated_submission_time': 3896.662455558777, 'accumulated_eval_time': 189.88712120056152, 'accumulated_logging_time': 0.2565450668334961, 'global_step': 7523, 'preemption_count': 0}), (8353, {'train/accuracy': 0.3450585901737213, 'train/loss': 3.1688172817230225, 'validation/accuracy': 0.3148599863052368, 'validation/loss': 3.3324038982391357, 'validation/num_examples': 50000, 'test/accuracy': 0.24370001256465912, 'test/loss': 3.8671798706054688, 'test/num_examples': 10000, 'score': 4317.006311655045, 'total_duration': 4523.273879766464, 'accumulated_submission_time': 4317.006311655045, 'accumulated_eval_time': 205.8206489086151, 'accumulated_logging_time': 0.28713083267211914, 'global_step': 8353, 'preemption_count': 0}), (9182, {'train/accuracy': 0.34882810711860657, 'train/loss': 3.1703295707702637, 'validation/accuracy': 0.3268599808216095, 'validation/loss': 3.2917654514312744, 'validation/num_examples': 50000, 'test/accuracy': 0.24890001118183136, 'test/loss': 3.81809401512146, 'test/num_examples': 10000, 'score': 4737.328916549683, 'total_duration': 4959.453677654266, 'accumulated_submission_time': 4737.328916549683, 'accumulated_eval_time': 221.627788066864, 'accumulated_logging_time': 0.3207366466522217, 'global_step': 9182, 'preemption_count': 0}), (10010, {'train/accuracy': 0.3848632872104645, 'train/loss': 2.9989681243896484, 'validation/accuracy': 0.3562600016593933, 'validation/loss': 3.1473562717437744, 'validation/num_examples': 50000, 'test/accuracy': 0.2703000009059906, 'test/loss': 3.701571226119995, 'test/num_examples': 10000, 'score': 5157.6530418396, 'total_duration': 5395.6878089904785, 'accumulated_submission_time': 5157.6530418396, 'accumulated_eval_time': 237.44723105430603, 'accumulated_logging_time': 0.39409971237182617, 'global_step': 10010, 'preemption_count': 0}), (10840, {'train/accuracy': 0.42769530415534973, 'train/loss': 2.7138445377349854, 'validation/accuracy': 0.3832399845123291, 'validation/loss': 2.9353668689727783, 'validation/num_examples': 50000, 'test/accuracy': 0.28850001096725464, 'test/loss': 3.5299103260040283, 'test/num_examples': 10000, 'score': 5577.7686223983765, 'total_duration': 5832.4809827804565, 'accumulated_submission_time': 5577.7686223983765, 'accumulated_eval_time': 254.04766058921814, 'accumulated_logging_time': 0.45459413528442383, 'global_step': 10840, 'preemption_count': 0}), (11669, {'train/accuracy': 0.4272851347923279, 'train/loss': 2.72206449508667, 'validation/accuracy': 0.39574000239372253, 'validation/loss': 2.8797767162323, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.4793076515197754, 'test/num_examples': 10000, 'score': 5998.043658256531, 'total_duration': 6268.940042972565, 'accumulated_submission_time': 5998.043658256531, 'accumulated_eval_time': 270.176255941391, 'accumulated_logging_time': 0.4910240173339844, 'global_step': 11669, 'preemption_count': 0}), (12497, {'train/accuracy': 0.4516015648841858, 'train/loss': 2.5749576091766357, 'validation/accuracy': 0.42093998193740845, 'validation/loss': 2.755376100540161, 'validation/num_examples': 50000, 'test/accuracy': 0.32510000467300415, 'test/loss': 3.3381431102752686, 'test/num_examples': 10000, 'score': 6418.182667016983, 'total_duration': 6706.661639928818, 'accumulated_submission_time': 6418.182667016983, 'accumulated_eval_time': 287.70635509490967, 'accumulated_logging_time': 0.526430606842041, 'global_step': 12497, 'preemption_count': 0}), (13326, {'train/accuracy': 0.4691406190395355, 'train/loss': 2.4858171939849854, 'validation/accuracy': 0.42857998609542847, 'validation/loss': 2.688340187072754, 'validation/num_examples': 50000, 'test/accuracy': 0.3254000246524811, 'test/loss': 3.3119332790374756, 'test/num_examples': 10000, 'score': 6838.375488996506, 'total_duration': 7143.825047016144, 'accumulated_submission_time': 6838.375488996506, 'accumulated_eval_time': 304.6270191669464, 'accumulated_logging_time': 0.5598948001861572, 'global_step': 13326, 'preemption_count': 0}), (14155, {'train/accuracy': 0.4715234339237213, 'train/loss': 2.4577696323394775, 'validation/accuracy': 0.43605998158454895, 'validation/loss': 2.6347227096557617, 'validation/num_examples': 50000, 'test/accuracy': 0.3362000286579132, 'test/loss': 3.2475883960723877, 'test/num_examples': 10000, 'score': 7258.70299077034, 'total_duration': 7582.065094470978, 'accumulated_submission_time': 7258.70299077034, 'accumulated_eval_time': 322.48793959617615, 'accumulated_logging_time': 0.5929391384124756, 'global_step': 14155, 'preemption_count': 0}), (14981, {'train/accuracy': 0.49779295921325684, 'train/loss': 2.3374860286712646, 'validation/accuracy': 0.45325997471809387, 'validation/loss': 2.5486438274383545, 'validation/num_examples': 50000, 'test/accuracy': 0.34700000286102295, 'test/loss': 3.1736245155334473, 'test/num_examples': 10000, 'score': 7678.770076751709, 'total_duration': 8020.621518611908, 'accumulated_submission_time': 7678.770076751709, 'accumulated_eval_time': 340.9122576713562, 'accumulated_logging_time': 0.639080286026001, 'global_step': 14981, 'preemption_count': 0}), (15808, {'train/accuracy': 0.49199217557907104, 'train/loss': 2.328792095184326, 'validation/accuracy': 0.45802000164985657, 'validation/loss': 2.5054256916046143, 'validation/num_examples': 50000, 'test/accuracy': 0.35350000858306885, 'test/loss': 3.127781629562378, 'test/num_examples': 10000, 'score': 8098.9833953380585, 'total_duration': 8459.606926202774, 'accumulated_submission_time': 8098.9833953380585, 'accumulated_eval_time': 359.6162667274475, 'accumulated_logging_time': 0.6910319328308105, 'global_step': 15808, 'preemption_count': 0}), (16636, {'train/accuracy': 0.5077539086341858, 'train/loss': 2.348529815673828, 'validation/accuracy': 0.4653799831867218, 'validation/loss': 2.5411765575408936, 'validation/num_examples': 50000, 'test/accuracy': 0.358100026845932, 'test/loss': 3.164151668548584, 'test/num_examples': 10000, 'score': 8519.220742464066, 'total_duration': 8898.90488409996, 'accumulated_submission_time': 8519.220742464066, 'accumulated_eval_time': 378.6296739578247, 'accumulated_logging_time': 0.721747636795044, 'global_step': 16636, 'preemption_count': 0}), (17464, {'train/accuracy': 0.5338085889816284, 'train/loss': 2.2077438831329346, 'validation/accuracy': 0.48037999868392944, 'validation/loss': 2.464365005493164, 'validation/num_examples': 50000, 'test/accuracy': 0.3742000162601471, 'test/loss': 3.0653090476989746, 'test/num_examples': 10000, 'score': 8939.434975385666, 'total_duration': 9338.364850282669, 'accumulated_submission_time': 8939.434975385666, 'accumulated_eval_time': 397.830947637558, 'accumulated_logging_time': 0.7493588924407959, 'global_step': 17464, 'preemption_count': 0}), (18274, {'train/accuracy': 0.5253320336341858, 'train/loss': 2.1683266162872314, 'validation/accuracy': 0.48733997344970703, 'validation/loss': 2.36962628364563, 'validation/num_examples': 50000, 'test/accuracy': 0.37630000710487366, 'test/loss': 3.015493392944336, 'test/num_examples': 10000, 'score': 9359.629138231277, 'total_duration': 9776.66967344284, 'accumulated_submission_time': 9359.629138231277, 'accumulated_eval_time': 415.89405632019043, 'accumulated_logging_time': 0.7808575630187988, 'global_step': 18274, 'preemption_count': 0}), (19101, {'train/accuracy': 0.5433593392372131, 'train/loss': 2.100349187850952, 'validation/accuracy': 0.4976399838924408, 'validation/loss': 2.3239128589630127, 'validation/num_examples': 50000, 'test/accuracy': 0.38430002331733704, 'test/loss': 2.948103189468384, 'test/num_examples': 10000, 'score': 9779.884184360504, 'total_duration': 10216.537297010422, 'accumulated_submission_time': 9779.884184360504, 'accumulated_eval_time': 435.4606990814209, 'accumulated_logging_time': 0.8098547458648682, 'global_step': 19101, 'preemption_count': 0}), (19930, {'train/accuracy': 0.5494921803474426, 'train/loss': 2.0221712589263916, 'validation/accuracy': 0.5053200125694275, 'validation/loss': 2.235727071762085, 'validation/num_examples': 50000, 'test/accuracy': 0.3937000334262848, 'test/loss': 2.8814697265625, 'test/num_examples': 10000, 'score': 10200.017252445221, 'total_duration': 10655.046504497528, 'accumulated_submission_time': 10200.017252445221, 'accumulated_eval_time': 453.76585626602173, 'accumulated_logging_time': 0.8638229370117188, 'global_step': 19930, 'preemption_count': 0}), (20759, {'train/accuracy': 0.553027331829071, 'train/loss': 2.0009143352508545, 'validation/accuracy': 0.5134599804878235, 'validation/loss': 2.207019090652466, 'validation/num_examples': 50000, 'test/accuracy': 0.39730000495910645, 'test/loss': 2.8512375354766846, 'test/num_examples': 10000, 'score': 10620.44093990326, 'total_duration': 11093.63636636734, 'accumulated_submission_time': 10620.44093990326, 'accumulated_eval_time': 471.8825533390045, 'accumulated_logging_time': 0.8967299461364746, 'global_step': 20759, 'preemption_count': 0}), (21587, {'train/accuracy': 0.5649218559265137, 'train/loss': 1.9944509267807007, 'validation/accuracy': 0.5170599818229675, 'validation/loss': 2.2341129779815674, 'validation/num_examples': 50000, 'test/accuracy': 0.40460002422332764, 'test/loss': 2.8678736686706543, 'test/num_examples': 10000, 'score': 11040.445036172867, 'total_duration': 11531.8213596344, 'accumulated_submission_time': 11040.445036172867, 'accumulated_eval_time': 490.01882314682007, 'accumulated_logging_time': 0.92466139793396, 'global_step': 21587, 'preemption_count': 0}), (22417, {'train/accuracy': 0.568066418170929, 'train/loss': 1.9755398035049438, 'validation/accuracy': 0.5228399634361267, 'validation/loss': 2.1831674575805664, 'validation/num_examples': 50000, 'test/accuracy': 0.4053000211715698, 'test/loss': 2.8376688957214355, 'test/num_examples': 10000, 'score': 11460.9106798172, 'total_duration': 11970.82464647293, 'accumulated_submission_time': 11460.9106798172, 'accumulated_eval_time': 508.50755858421326, 'accumulated_logging_time': 0.9571468830108643, 'global_step': 22417, 'preemption_count': 0}), (23245, {'train/accuracy': 0.5717968344688416, 'train/loss': 1.9142189025878906, 'validation/accuracy': 0.5251799821853638, 'validation/loss': 2.148688793182373, 'validation/num_examples': 50000, 'test/accuracy': 0.4123000204563141, 'test/loss': 2.7936477661132812, 'test/num_examples': 10000, 'score': 11881.056750774384, 'total_duration': 12409.47264957428, 'accumulated_submission_time': 11881.056750774384, 'accumulated_eval_time': 526.9594578742981, 'accumulated_logging_time': 0.9905316829681396, 'global_step': 23245, 'preemption_count': 0}), (24076, {'train/accuracy': 0.5941601395606995, 'train/loss': 1.7937589883804321, 'validation/accuracy': 0.5328199863433838, 'validation/loss': 2.098494291305542, 'validation/num_examples': 50000, 'test/accuracy': 0.41620001196861267, 'test/loss': 2.7552809715270996, 'test/num_examples': 10000, 'score': 12301.535136938095, 'total_duration': 12848.191681623459, 'accumulated_submission_time': 12301.535136938095, 'accumulated_eval_time': 545.1499035358429, 'accumulated_logging_time': 1.0239295959472656, 'global_step': 24076, 'preemption_count': 0}), (24906, {'train/accuracy': 0.5870312452316284, 'train/loss': 1.8558127880096436, 'validation/accuracy': 0.5435999631881714, 'validation/loss': 2.051222801208496, 'validation/num_examples': 50000, 'test/accuracy': 0.4296000301837921, 'test/loss': 2.7143967151641846, 'test/num_examples': 10000, 'score': 12721.723054409027, 'total_duration': 13286.903891801834, 'accumulated_submission_time': 12721.723054409027, 'accumulated_eval_time': 563.6234745979309, 'accumulated_logging_time': 1.0578880310058594, 'global_step': 24906, 'preemption_count': 0}), (25733, {'train/accuracy': 0.595410168170929, 'train/loss': 1.7982745170593262, 'validation/accuracy': 0.5427199602127075, 'validation/loss': 2.047550916671753, 'validation/num_examples': 50000, 'test/accuracy': 0.428600013256073, 'test/loss': 2.692305088043213, 'test/num_examples': 10000, 'score': 13141.918692350388, 'total_duration': 13725.75484251976, 'accumulated_submission_time': 13141.918692350388, 'accumulated_eval_time': 582.2353172302246, 'accumulated_logging_time': 1.0846836566925049, 'global_step': 25733, 'preemption_count': 0}), (26561, {'train/accuracy': 0.5968359112739563, 'train/loss': 1.7940090894699097, 'validation/accuracy': 0.5536400079727173, 'validation/loss': 2.0078365802764893, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.6519391536712646, 'test/num_examples': 10000, 'score': 13561.960940361023, 'total_duration': 14164.276211500168, 'accumulated_submission_time': 13561.960940361023, 'accumulated_eval_time': 600.6619396209717, 'accumulated_logging_time': 1.1202125549316406, 'global_step': 26561, 'preemption_count': 0}), (27389, {'train/accuracy': 0.6099609136581421, 'train/loss': 1.7598166465759277, 'validation/accuracy': 0.5626400113105774, 'validation/loss': 1.9834723472595215, 'validation/num_examples': 50000, 'test/accuracy': 0.4386000335216522, 'test/loss': 2.6456127166748047, 'test/num_examples': 10000, 'score': 13982.287277936935, 'total_duration': 14603.011756896973, 'accumulated_submission_time': 13982.287277936935, 'accumulated_eval_time': 619.0211308002472, 'accumulated_logging_time': 1.1531453132629395, 'global_step': 27389, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6187109351158142, 'train/loss': 1.7497375011444092, 'validation/accuracy': 0.558739960193634, 'validation/loss': 2.011604070663452, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.666518449783325, 'test/num_examples': 10000, 'score': 14290.964758872986, 'total_duration': 14929.872590303421, 'accumulated_submission_time': 14290.964758872986, 'accumulated_eval_time': 637.1623675823212, 'accumulated_logging_time': 1.182924509048462, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0502 02:08:12.390151 139674332673856 submission_runner.py:581] Timing: 14290.964758872986
I0502 02:08:12.390207 139674332673856 submission_runner.py:582] ====================
I0502 02:08:12.390394 139674332673856 submission_runner.py:645] Final imagenet_vit score: 14290.964758872986
