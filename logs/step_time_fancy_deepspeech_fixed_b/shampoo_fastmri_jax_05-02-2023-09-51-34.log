python3 submission_runner.py --framework=jax --workload=fastmri --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_shampoo --overwrite=True --save_checkpoints=False --max_global_steps=5428 2>&1 | tee -a /logs/fastmri_jax_05-02-2023-09-51-34.log
I0502 09:51:53.637533 140129979877184 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax.
I0502 09:51:53.832232 140129979877184 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 09:51:54.674066 140129979877184 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0502 09:51:54.674833 140129979877184 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 09:51:54.679008 140129979877184 submission_runner.py:538] Using RNG seed 2482368099
I0502 09:51:57.596323 140129979877184 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 09:51:57.596520 140129979877184 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1.
I0502 09:51:57.596816 140129979877184 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1/hparams.json.
I0502 09:51:57.719627 140129979877184 submission_runner.py:241] Initializing dataset.
I0502 09:52:01.742691 140129979877184 submission_runner.py:248] Initializing model.
I0502 09:52:09.068176 140129979877184 submission_runner.py:258] Initializing optimizer.
I0502 09:52:19.029872 140129979877184 submission_runner.py:265] Initializing metrics bundle.
I0502 09:52:19.030097 140129979877184 submission_runner.py:282] Initializing checkpoint and logger.
I0502 09:52:19.032112 140129979877184 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1 with prefix checkpoint_
I0502 09:52:19.032362 140129979877184 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 09:52:19.032428 140129979877184 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 09:52:19.885277 140129979877184 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1/meta_data_0.json.
I0502 09:52:19.886704 140129979877184 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1/flags_0.json.
I0502 09:52:19.892876 140129979877184 submission_runner.py:318] Starting training loop.
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:812: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:813: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:814: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
I0502 09:53:45.542569 139953010894592 logging_writer.py:48] [0] global_step=0, grad_norm=4.216468811035156, loss=0.877657413482666
I0502 09:53:45.560503 140129979877184 spec.py:298] Evaluating on the training split.
I0502 09:55:15.802496 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 09:56:20.382318 140129979877184 spec.py:326] Evaluating on the test split.
I0502 09:57:22.063181 140129979877184 submission_runner.py:415] Time since start: 302.17s, 	Step: 1, 	{'train/ssim': 0.23053409372057235, 'train/loss': 0.936103343963623, 'validation/ssim': 0.2202281361560038, 'validation/loss': 0.9508626118897721, 'validation/num_examples': 3554, 'test/ssim': 0.242457905683861, 'test/loss': 0.9508195380131248, 'test/num_examples': 3581, 'score': 85.66740083694458, 'total_duration': 302.17018961906433, 'accumulated_submission_time': 85.66740083694458, 'accumulated_eval_time': 216.50258827209473, 'accumulated_logging_time': 0}
I0502 09:57:22.081435 139924086957824 logging_writer.py:48] [1] accumulated_eval_time=216.502588, accumulated_logging_time=0, accumulated_submission_time=85.667401, global_step=1, preemption_count=0, score=85.667401, test/loss=0.950820, test/num_examples=3581, test/ssim=0.242458, total_duration=302.170190, train/loss=0.936103, train/ssim=0.230534, validation/loss=0.950863, validation/num_examples=3554, validation/ssim=0.220228
I0502 09:57:46.037398 139924078565120 logging_writer.py:48] [100] global_step=100, grad_norm=0.3940720558166504, loss=0.3024951219558716
I0502 09:58:11.779892 139924086957824 logging_writer.py:48] [200] global_step=200, grad_norm=0.19564229249954224, loss=0.3756345808506012
I0502 09:58:37.277988 139924078565120 logging_writer.py:48] [300] global_step=300, grad_norm=0.14565707743167877, loss=0.365388423204422
I0502 09:58:42.175917 140129979877184 spec.py:298] Evaluating on the training split.
I0502 09:58:43.963831 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 09:58:45.309459 140129979877184 spec.py:326] Evaluating on the test split.
I0502 09:58:46.655448 140129979877184 submission_runner.py:415] Time since start: 386.76s, 	Step: 317, 	{'train/ssim': 0.7208073479788644, 'train/loss': 0.28603659357343403, 'validation/ssim': 0.7006410030951041, 'validation/loss': 0.30636752959913127, 'validation/num_examples': 3554, 'test/ssim': 0.7173248350670204, 'test/loss': 0.3085642295055501, 'test/num_examples': 3581, 'score': 165.748441696167, 'total_duration': 386.76248049736023, 'accumulated_submission_time': 165.748441696167, 'accumulated_eval_time': 220.9821105003357, 'accumulated_logging_time': 0.02681732177734375}
I0502 09:58:46.667475 139924086957824 logging_writer.py:48] [317] accumulated_eval_time=220.982111, accumulated_logging_time=0.026817, accumulated_submission_time=165.748442, global_step=317, preemption_count=0, score=165.748442, test/loss=0.308564, test/num_examples=3581, test/ssim=0.717325, total_duration=386.762480, train/loss=0.286037, train/ssim=0.720807, validation/loss=0.306368, validation/num_examples=3554, validation/ssim=0.700641
I0502 09:59:15.326855 139924078565120 logging_writer.py:48] [400] global_step=400, grad_norm=0.15602168440818787, loss=0.2957587242126465
I0502 09:59:55.420222 139924086957824 logging_writer.py:48] [500] global_step=500, grad_norm=0.27012911438941956, loss=0.3099428713321686
I0502 10:00:07.033551 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:00:08.375923 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:00:09.723548 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:00:11.068346 140129979877184 submission_runner.py:415] Time since start: 471.18s, 	Step: 533, 	{'train/ssim': 0.7327751432146344, 'train/loss': 0.27835117067609516, 'validation/ssim': 0.7099452064355304, 'validation/loss': 0.29932890866805006, 'validation/num_examples': 3554, 'test/ssim': 0.7269072693774434, 'test/loss': 0.3014320644111282, 'test/num_examples': 3581, 'score': 246.09798288345337, 'total_duration': 471.1753816604614, 'accumulated_submission_time': 246.09798288345337, 'accumulated_eval_time': 225.01683855056763, 'accumulated_logging_time': 0.05229926109313965}
I0502 10:00:11.079489 139924078565120 logging_writer.py:48] [533] accumulated_eval_time=225.016839, accumulated_logging_time=0.052299, accumulated_submission_time=246.097983, global_step=533, preemption_count=0, score=246.097983, test/loss=0.301432, test/num_examples=3581, test/ssim=0.726907, total_duration=471.175382, train/loss=0.278351, train/ssim=0.732775, validation/loss=0.299329, validation/num_examples=3554, validation/ssim=0.709945
I0502 10:00:35.263714 139924086957824 logging_writer.py:48] [600] global_step=600, grad_norm=0.20734626054763794, loss=0.3267502188682556
I0502 10:01:15.640251 139924078565120 logging_writer.py:48] [700] global_step=700, grad_norm=0.1740851253271103, loss=0.22471536695957184
I0502 10:01:31.928387 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:01:33.272408 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:01:34.617304 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:01:35.962919 140129979877184 submission_runner.py:415] Time since start: 556.07s, 	Step: 741, 	{'train/ssim': 0.7351674352373395, 'train/loss': 0.2752456154142107, 'validation/ssim': 0.7134544703151379, 'validation/loss': 0.2959043452360017, 'validation/num_examples': 3554, 'test/ssim': 0.7303895287192823, 'test/loss': 0.2976956083867111, 'test/num_examples': 3581, 'score': 326.9285452365875, 'total_duration': 556.0699486732483, 'accumulated_submission_time': 326.9285452365875, 'accumulated_eval_time': 229.05130052566528, 'accumulated_logging_time': 0.07875394821166992}
I0502 10:01:35.974957 139924086957824 logging_writer.py:48] [741] accumulated_eval_time=229.051301, accumulated_logging_time=0.078754, accumulated_submission_time=326.928545, global_step=741, preemption_count=0, score=326.928545, test/loss=0.297696, test/num_examples=3581, test/ssim=0.730390, total_duration=556.069949, train/loss=0.275246, train/ssim=0.735167, validation/loss=0.295904, validation/num_examples=3554, validation/ssim=0.713454
I0502 10:01:55.751540 139924078565120 logging_writer.py:48] [800] global_step=800, grad_norm=0.1903282105922699, loss=0.21357205510139465
I0502 10:02:35.651547 139924086957824 logging_writer.py:48] [900] global_step=900, grad_norm=0.15924015641212463, loss=0.3654292821884155
I0502 10:02:56.328047 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:02:57.673214 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:02:59.019888 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:03:00.364460 140129979877184 submission_runner.py:415] Time since start: 640.47s, 	Step: 956, 	{'train/ssim': 0.737365586417062, 'train/loss': 0.2739224433898926, 'validation/ssim': 0.71482080600204, 'validation/loss': 0.2945187750597918, 'validation/num_examples': 3554, 'test/ssim': 0.7320889683616657, 'test/loss': 0.29620727777680816, 'test/num_examples': 3581, 'score': 407.2681622505188, 'total_duration': 640.4714977741241, 'accumulated_submission_time': 407.2681622505188, 'accumulated_eval_time': 233.08764958381653, 'accumulated_logging_time': 0.10121893882751465}
I0502 10:03:00.377797 139924078565120 logging_writer.py:48] [956] accumulated_eval_time=233.087650, accumulated_logging_time=0.101219, accumulated_submission_time=407.268162, global_step=956, preemption_count=0, score=407.268162, test/loss=0.296207, test/num_examples=3581, test/ssim=0.732089, total_duration=640.471498, train/loss=0.273922, train/ssim=0.737366, validation/loss=0.294519, validation/num_examples=3554, validation/ssim=0.714821
I0502 10:03:10.677163 139924086957824 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.15068066120147705, loss=0.2669063210487366
I0502 10:03:36.455089 139924078565120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.14396266639232635, loss=0.27339041233062744
I0502 10:04:02.009699 139924086957824 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.20773401856422424, loss=0.24867767095565796
I0502 10:04:20.509994 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:04:21.857308 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:04:23.207731 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:04:24.558948 140129979877184 submission_runner.py:415] Time since start: 724.67s, 	Step: 1278, 	{'train/ssim': 0.7388089043753487, 'train/loss': 0.2713191338947841, 'validation/ssim': 0.7163611451225732, 'validation/loss': 0.29240912940304936, 'validation/num_examples': 3554, 'test/ssim': 0.7334276171111421, 'test/loss': 0.2939544481726473, 'test/num_examples': 3581, 'score': 487.38143610954285, 'total_duration': 724.6659700870514, 'accumulated_submission_time': 487.38143610954285, 'accumulated_eval_time': 237.13654136657715, 'accumulated_logging_time': 0.12822461128234863}
I0502 10:04:24.568959 139924078565120 logging_writer.py:48] [1278] accumulated_eval_time=237.136541, accumulated_logging_time=0.128225, accumulated_submission_time=487.381436, global_step=1278, preemption_count=0, score=487.381436, test/loss=0.293954, test/num_examples=3581, test/ssim=0.733428, total_duration=724.665970, train/loss=0.271319, train/ssim=0.738809, validation/loss=0.292409, validation/num_examples=3554, validation/ssim=0.716361
I0502 10:04:29.328423 139924086957824 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.14379271864891052, loss=0.26682108640670776
I0502 10:04:54.667477 139924078565120 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.1255100816488266, loss=0.2306404560804367
I0502 10:05:20.141829 139924086957824 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1895495057106018, loss=0.2465583235025406
I0502 10:05:45.451976 139924078565120 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.1894996613264084, loss=0.25084829330444336
I0502 10:05:45.466074 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:05:46.810265 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:05:48.162050 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:05:49.510646 140129979877184 submission_runner.py:415] Time since start: 809.62s, 	Step: 1601, 	{'train/ssim': 0.7430562973022461, 'train/loss': 0.2691326141357422, 'validation/ssim': 0.7196477697884426, 'validation/loss': 0.290388580648565, 'validation/num_examples': 3554, 'test/ssim': 0.7366651904146886, 'test/loss': 0.29196092850068067, 'test/num_examples': 3581, 'score': 568.2654356956482, 'total_duration': 809.6176927089691, 'accumulated_submission_time': 568.2654356956482, 'accumulated_eval_time': 241.18103861808777, 'accumulated_logging_time': 0.14597868919372559}
I0502 10:05:49.519848 139924086957824 logging_writer.py:48] [1601] accumulated_eval_time=241.181039, accumulated_logging_time=0.145979, accumulated_submission_time=568.265436, global_step=1601, preemption_count=0, score=568.265436, test/loss=0.291961, test/num_examples=3581, test/ssim=0.736665, total_duration=809.617693, train/loss=0.269133, train/ssim=0.743056, validation/loss=0.290389, validation/num_examples=3554, validation/ssim=0.719648
I0502 10:06:13.302627 139924078565120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.12322726100683212, loss=0.33071663975715637
I0502 10:06:38.637683 139924086957824 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.14322148263454437, loss=0.2507312297821045
I0502 10:07:04.138375 139924078565120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.1930648237466812, loss=0.31586524844169617
I0502 10:07:09.530248 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:07:10.874380 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:07:12.228469 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:07:13.576805 140129979877184 submission_runner.py:415] Time since start: 893.68s, 	Step: 1925, 	{'train/ssim': 0.7431612695966449, 'train/loss': 0.26871749332972933, 'validation/ssim': 0.7192730407199635, 'validation/loss': 0.2903490125562746, 'validation/num_examples': 3554, 'test/ssim': 0.7365146563459928, 'test/loss': 0.2918125419968235, 'test/num_examples': 3581, 'score': 648.2632210254669, 'total_duration': 893.6838080883026, 'accumulated_submission_time': 648.2632210254669, 'accumulated_eval_time': 245.22750997543335, 'accumulated_logging_time': 0.16245532035827637}
I0502 10:07:13.585054 139924086957824 logging_writer.py:48] [1925] accumulated_eval_time=245.227510, accumulated_logging_time=0.162455, accumulated_submission_time=648.263221, global_step=1925, preemption_count=0, score=648.263221, test/loss=0.291813, test/num_examples=3581, test/ssim=0.736515, total_duration=893.683808, train/loss=0.268717, train/ssim=0.743161, validation/loss=0.290349, validation/num_examples=3554, validation/ssim=0.719273
I0502 10:07:31.466205 139924078565120 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.11335676163434982, loss=0.2800733149051666
I0502 10:07:57.434210 139924086957824 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.10529906302690506, loss=0.2868790924549103
I0502 10:08:22.974400 139924078565120 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.1255216747522354, loss=0.22933650016784668
I0502 10:08:33.601753 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:08:34.950807 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:08:36.300855 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:08:37.650782 140129979877184 submission_runner.py:415] Time since start: 977.76s, 	Step: 2245, 	{'train/ssim': 0.7459299904959542, 'train/loss': 0.2672916650772095, 'validation/ssim': 0.7214099237874578, 'validation/loss': 0.2896400498887697, 'validation/num_examples': 3554, 'test/ssim': 0.7385865450904077, 'test/loss': 0.29106358728139836, 'test/num_examples': 3581, 'score': 728.2673318386078, 'total_duration': 977.7578113079071, 'accumulated_submission_time': 728.2673318386078, 'accumulated_eval_time': 249.2764711380005, 'accumulated_logging_time': 0.17804694175720215}
I0502 10:08:37.659451 139924086957824 logging_writer.py:48] [2245] accumulated_eval_time=249.276471, accumulated_logging_time=0.178047, accumulated_submission_time=728.267332, global_step=2245, preemption_count=0, score=728.267332, test/loss=0.291064, test/num_examples=3581, test/ssim=0.738587, total_duration=977.757811, train/loss=0.267292, train/ssim=0.745930, validation/loss=0.289640, validation/num_examples=3554, validation/ssim=0.721410
I0502 10:08:50.398419 139924078565120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.08315809816122055, loss=0.3391108810901642
I0502 10:09:15.726299 139924086957824 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.10472127795219421, loss=0.21664422750473022
I0502 10:09:40.944788 139924078565120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.09677214175462723, loss=0.24193598330020905
I0502 10:09:57.800509 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:09:59.141609 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:10:00.494371 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:10:01.841574 140129979877184 submission_runner.py:415] Time since start: 1061.95s, 	Step: 2571, 	{'train/ssim': 0.7447992052350726, 'train/loss': 0.26684628214154926, 'validation/ssim': 0.7214215331756472, 'validation/loss': 0.2883723940014772, 'validation/num_examples': 3554, 'test/ssim': 0.7386706069140953, 'test/loss': 0.2897538795247487, 'test/num_examples': 3581, 'score': 808.3955051898956, 'total_duration': 1061.9485957622528, 'accumulated_submission_time': 808.3955051898956, 'accumulated_eval_time': 253.3174638748169, 'accumulated_logging_time': 0.19421005249023438}
I0502 10:10:01.850839 139924086957824 logging_writer.py:48] [2571] accumulated_eval_time=253.317464, accumulated_logging_time=0.194210, accumulated_submission_time=808.395505, global_step=2571, preemption_count=0, score=808.395505, test/loss=0.289754, test/num_examples=3581, test/ssim=0.738671, total_duration=1061.948596, train/loss=0.266846, train/ssim=0.744799, validation/loss=0.288372, validation/num_examples=3554, validation/ssim=0.721422
I0502 10:10:07.861612 139924078565120 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.10024025291204453, loss=0.37110623717308044
I0502 10:10:33.139597 139924086957824 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.09209449589252472, loss=0.28937849402427673
I0502 10:10:58.411506 139924078565120 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.05992545187473297, loss=0.3211377263069153
I0502 10:11:21.971029 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:11:23.314131 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:11:24.667572 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:11:26.018687 140129979877184 submission_runner.py:415] Time since start: 1146.13s, 	Step: 2896, 	{'train/ssim': 0.7462560789925712, 'train/loss': 0.26618594782693045, 'validation/ssim': 0.7219124935152293, 'validation/loss': 0.28831778179076395, 'validation/num_examples': 3554, 'test/ssim': 0.7392447907576445, 'test/loss': 0.28961469687020036, 'test/num_examples': 3581, 'score': 888.502414226532, 'total_duration': 1146.1257238388062, 'accumulated_submission_time': 888.502414226532, 'accumulated_eval_time': 257.3650665283203, 'accumulated_logging_time': 0.21136927604675293}
I0502 10:11:26.028097 139924086957824 logging_writer.py:48] [2896] accumulated_eval_time=257.365067, accumulated_logging_time=0.211369, accumulated_submission_time=888.502414, global_step=2896, preemption_count=0, score=888.502414, test/loss=0.289615, test/num_examples=3581, test/ssim=0.739245, total_duration=1146.125724, train/loss=0.266186, train/ssim=0.746256, validation/loss=0.288318, validation/num_examples=3554, validation/ssim=0.721912
I0502 10:11:27.498920 139924078565120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.1575692892074585, loss=0.240060955286026
I0502 10:11:51.355413 139924086957824 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.115608349442482, loss=0.33299770951271057
I0502 10:12:17.182226 139924078565120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.13049469888210297, loss=0.27581700682640076
I0502 10:12:42.608865 139924086957824 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.1418302357196808, loss=0.2550320625305176
I0502 10:12:46.110958 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:12:47.454431 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:12:48.806272 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:12:50.156999 140129979877184 submission_runner.py:415] Time since start: 1230.26s, 	Step: 3218, 	{'train/ssim': 0.7464042391095843, 'train/loss': 0.2658578668321882, 'validation/ssim': 0.7224142389077448, 'validation/loss': 0.2878384480130223, 'validation/num_examples': 3554, 'test/ssim': 0.7397229818617356, 'test/loss': 0.2892328734772061, 'test/num_examples': 3581, 'score': 968.572104215622, 'total_duration': 1230.2640392780304, 'accumulated_submission_time': 968.572104215622, 'accumulated_eval_time': 261.4110481739044, 'accumulated_logging_time': 0.2285914421081543}
I0502 10:12:50.165690 139924078565120 logging_writer.py:48] [3218] accumulated_eval_time=261.411048, accumulated_logging_time=0.228591, accumulated_submission_time=968.572104, global_step=3218, preemption_count=0, score=968.572104, test/loss=0.289233, test/num_examples=3581, test/ssim=0.739723, total_duration=1230.264039, train/loss=0.265858, train/ssim=0.746404, validation/loss=0.287838, validation/num_examples=3554, validation/ssim=0.722414
I0502 10:13:10.327313 139924086957824 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.03986933454871178, loss=0.2552543878555298
I0502 10:13:35.543210 139924078565120 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.05513797327876091, loss=0.2750558853149414
I0502 10:14:00.727705 139924086957824 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.052196573466062546, loss=0.29153972864151
I0502 10:14:10.854038 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:14:12.199033 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:14:13.550224 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:14:14.900780 140129979877184 submission_runner.py:415] Time since start: 1315.01s, 	Step: 3541, 	{'train/ssim': 0.7461259024483817, 'train/loss': 0.26534838335854666, 'validation/ssim': 0.7220192449308878, 'validation/loss': 0.28730664873096334, 'validation/num_examples': 3554, 'test/ssim': 0.7393772580110305, 'test/loss': 0.28859112656206365, 'test/num_examples': 3581, 'score': 1049.247876882553, 'total_duration': 1315.0078189373016, 'accumulated_submission_time': 1049.247876882553, 'accumulated_eval_time': 265.4577271938324, 'accumulated_logging_time': 0.24448680877685547}
I0502 10:14:14.909281 139924078565120 logging_writer.py:48] [3541] accumulated_eval_time=265.457727, accumulated_logging_time=0.244487, accumulated_submission_time=1049.247877, global_step=3541, preemption_count=0, score=1049.247877, test/loss=0.288591, test/num_examples=3581, test/ssim=0.739377, total_duration=1315.007819, train/loss=0.265348, train/ssim=0.746126, validation/loss=0.287307, validation/num_examples=3554, validation/ssim=0.722019
I0502 10:14:28.359753 139924086957824 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0439881756901741, loss=0.33522993326187134
I0502 10:14:53.516401 139924078565120 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.08775803446769714, loss=0.21461905539035797
I0502 10:15:18.884205 139924086957824 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.07002457976341248, loss=0.24258047342300415
I0502 10:15:35.082878 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:15:36.429092 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:15:37.778596 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:15:39.128223 140129979877184 submission_runner.py:415] Time since start: 1399.24s, 	Step: 3868, 	{'train/ssim': 0.7468570981706891, 'train/loss': 0.26515793800354004, 'validation/ssim': 0.7227957687420864, 'validation/loss': 0.28704607292179585, 'validation/num_examples': 3554, 'test/ssim': 0.7401274739946943, 'test/loss': 0.28837077959194357, 'test/num_examples': 3581, 'score': 1129.4085009098053, 'total_duration': 1399.2352771759033, 'accumulated_submission_time': 1129.4085009098053, 'accumulated_eval_time': 269.5030303001404, 'accumulated_logging_time': 0.26044511795043945}
I0502 10:15:39.137026 139924078565120 logging_writer.py:48] [3868] accumulated_eval_time=269.503030, accumulated_logging_time=0.260445, accumulated_submission_time=1129.408501, global_step=3868, preemption_count=0, score=1129.408501, test/loss=0.288371, test/num_examples=3581, test/ssim=0.740127, total_duration=1399.235277, train/loss=0.265158, train/ssim=0.746857, validation/loss=0.287046, validation/num_examples=3554, validation/ssim=0.722796
I0502 10:15:46.117239 139924086957824 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.05852329730987549, loss=0.3113517165184021
I0502 10:16:11.285418 139924078565120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0505702905356884, loss=0.2759171426296234
I0502 10:16:36.260936 139924086957824 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.10776633024215698, loss=0.28699249029159546
I0502 10:16:59.327370 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:17:00.672610 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:17:02.025322 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:17:03.375800 140129979877184 submission_runner.py:415] Time since start: 1483.48s, 	Step: 4194, 	{'train/ssim': 0.7480990546090263, 'train/loss': 0.26488288811274935, 'validation/ssim': 0.7240654511202167, 'validation/loss': 0.28675963359396983, 'validation/num_examples': 3554, 'test/ssim': 0.7413254742913991, 'test/loss': 0.2880500083993647, 'test/num_examples': 3581, 'score': 1209.585619688034, 'total_duration': 1483.482827425003, 'accumulated_submission_time': 1209.585619688034, 'accumulated_eval_time': 273.5513937473297, 'accumulated_logging_time': 0.27704429626464844}
I0502 10:17:03.384763 139924078565120 logging_writer.py:48] [4194] accumulated_eval_time=273.551394, accumulated_logging_time=0.277044, accumulated_submission_time=1209.585620, global_step=4194, preemption_count=0, score=1209.585620, test/loss=0.288050, test/num_examples=3581, test/ssim=0.741325, total_duration=1483.482827, train/loss=0.264883, train/ssim=0.748099, validation/loss=0.286760, validation/num_examples=3554, validation/ssim=0.724065
I0502 10:17:05.074767 139924086957824 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.07362601906061172, loss=0.22333738207817078
I0502 10:17:28.838288 139924078565120 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.11647067964076996, loss=0.27077779173851013
I0502 10:17:54.221144 139924086957824 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.03769481182098389, loss=0.3259936273097992
I0502 10:18:19.355880 139924078565120 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.05886538699269295, loss=0.23916657269001007
I0502 10:18:24.320500 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:18:25.671711 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:18:27.021003 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:18:28.375598 140129979877184 submission_runner.py:415] Time since start: 1568.48s, 	Step: 4521, 	{'train/ssim': 0.7487756184169224, 'train/loss': 0.2641328402927944, 'validation/ssim': 0.7242957154350732, 'validation/loss': 0.28672145656742404, 'validation/num_examples': 3554, 'test/ssim': 0.7414980294217747, 'test/loss': 0.2880502470176801, 'test/num_examples': 3581, 'score': 1290.508192539215, 'total_duration': 1568.482634305954, 'accumulated_submission_time': 1290.508192539215, 'accumulated_eval_time': 277.60642647743225, 'accumulated_logging_time': 0.29378223419189453}
I0502 10:18:28.384917 139924086957824 logging_writer.py:48] [4521] accumulated_eval_time=277.606426, accumulated_logging_time=0.293782, accumulated_submission_time=1290.508193, global_step=4521, preemption_count=0, score=1290.508193, test/loss=0.288050, test/num_examples=3581, test/ssim=0.741498, total_duration=1568.482634, train/loss=0.264133, train/ssim=0.748776, validation/loss=0.286721, validation/num_examples=3554, validation/ssim=0.724296
I0502 10:18:47.067539 139924078565120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.07930731028318405, loss=0.2814839780330658
I0502 10:19:12.368829 139924086957824 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.05434475839138031, loss=0.2673570215702057
I0502 10:19:37.314758 139924078565120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1092071533203125, loss=0.23451882600784302
I0502 10:19:48.656337 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:19:50.002589 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:19:51.352166 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:19:52.700500 140129979877184 submission_runner.py:415] Time since start: 1652.81s, 	Step: 4849, 	{'train/ssim': 0.7471113204956055, 'train/loss': 0.2644621304103306, 'validation/ssim': 0.7225479873030388, 'validation/loss': 0.28653148163820696, 'validation/num_examples': 3554, 'test/ssim': 0.7399166035805291, 'test/loss': 0.2878553299423171, 'test/num_examples': 3581, 'score': 1370.7666976451874, 'total_duration': 1652.8074979782104, 'accumulated_submission_time': 1370.7666976451874, 'accumulated_eval_time': 281.6505024433136, 'accumulated_logging_time': 0.3105196952819824}
I0502 10:19:52.711327 139924086957824 logging_writer.py:48] [4849] accumulated_eval_time=281.650502, accumulated_logging_time=0.310520, accumulated_submission_time=1370.766698, global_step=4849, preemption_count=0, score=1370.766698, test/loss=0.287855, test/num_examples=3581, test/ssim=0.739917, total_duration=1652.807498, train/loss=0.264462, train/ssim=0.747111, validation/loss=0.286531, validation/num_examples=3554, validation/ssim=0.722548
I0502 10:20:04.403099 139924078565120 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.08962680399417877, loss=0.3013525903224945
I0502 10:20:29.424924 139924086957824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.06975361704826355, loss=0.31828194856643677
I0502 10:20:54.543564 139924078565120 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.043612804263830185, loss=0.21754151582717896
I0502 10:21:12.993976 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:21:14.341729 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:21:15.693264 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:21:17.044074 140129979877184 submission_runner.py:415] Time since start: 1737.15s, 	Step: 5177, 	{'train/ssim': 0.7492414883204869, 'train/loss': 0.26414685589926584, 'validation/ssim': 0.7248968619205473, 'validation/loss': 0.28626984106266706, 'validation/num_examples': 3554, 'test/ssim': 0.7420779401048939, 'test/loss': 0.2875959859217921, 'test/num_examples': 3581, 'score': 1451.0357098579407, 'total_duration': 1737.1511163711548, 'accumulated_submission_time': 1451.0357098579407, 'accumulated_eval_time': 285.700541973114, 'accumulated_logging_time': 0.32953715324401855}
I0502 10:21:17.053318 139924086957824 logging_writer.py:48] [5177] accumulated_eval_time=285.700542, accumulated_logging_time=0.329537, accumulated_submission_time=1451.035710, global_step=5177, preemption_count=0, score=1451.035710, test/loss=0.287596, test/num_examples=3581, test/ssim=0.742078, total_duration=1737.151116, train/loss=0.264147, train/ssim=0.749241, validation/loss=0.286270, validation/num_examples=3554, validation/ssim=0.724897
I0502 10:21:21.906261 139924078565120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.037902090698480606, loss=0.3003639876842499
I0502 10:21:47.451804 139924086957824 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.056821394711732864, loss=0.2640002965927124
I0502 10:22:12.807367 139924078565120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.07087080180644989, loss=0.2744167447090149
I0502 10:22:18.995472 140129979877184 spec.py:298] Evaluating on the training split.
I0502 10:22:20.343149 140129979877184 spec.py:310] Evaluating on the validation split.
I0502 10:22:21.696905 140129979877184 spec.py:326] Evaluating on the test split.
I0502 10:22:23.044164 140129979877184 submission_runner.py:415] Time since start: 1803.15s, 	Step: 5428, 	{'train/ssim': 0.7477006912231445, 'train/loss': 0.2641615356717791, 'validation/ssim': 0.7235122534688028, 'validation/loss': 0.28623805263435564, 'validation/num_examples': 3554, 'test/ssim': 0.7407968324577632, 'test/loss': 0.28756165897270314, 'test/num_examples': 3581, 'score': 1512.966007232666, 'total_duration': 1803.151204586029, 'accumulated_submission_time': 1512.966007232666, 'accumulated_eval_time': 289.74918484687805, 'accumulated_logging_time': 0.3464639186859131}
I0502 10:22:23.053625 139924086957824 logging_writer.py:48] [5428] accumulated_eval_time=289.749185, accumulated_logging_time=0.346464, accumulated_submission_time=1512.966007, global_step=5428, preemption_count=0, score=1512.966007, test/loss=0.287562, test/num_examples=3581, test/ssim=0.740797, total_duration=1803.151205, train/loss=0.264162, train/ssim=0.747701, validation/loss=0.286238, validation/num_examples=3554, validation/ssim=0.723512
I0502 10:22:23.068164 139924078565120 logging_writer.py:48] [5428] global_step=5428, preemption_count=0, score=1512.966007
I0502 10:22:23.246624 140129979877184 checkpoints.py:356] Saving checkpoint at step: 5428
I0502 10:22:23.845050 140129979877184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1/checkpoint_5428
I0502 10:22:23.846043 140129979877184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_shampoo/fastmri_jax/trial_1/checkpoint_5428.
I0502 10:22:24.445310 140129979877184 submission_runner.py:578] Tuning trial 1/1
I0502 10:22:24.445576 140129979877184 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0502 10:22:24.446350 140129979877184 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ssim': 0.23053409372057235, 'train/loss': 0.936103343963623, 'validation/ssim': 0.2202281361560038, 'validation/loss': 0.9508626118897721, 'validation/num_examples': 3554, 'test/ssim': 0.242457905683861, 'test/loss': 0.9508195380131248, 'test/num_examples': 3581, 'score': 85.66740083694458, 'total_duration': 302.17018961906433, 'accumulated_submission_time': 85.66740083694458, 'accumulated_eval_time': 216.50258827209473, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (317, {'train/ssim': 0.7208073479788644, 'train/loss': 0.28603659357343403, 'validation/ssim': 0.7006410030951041, 'validation/loss': 0.30636752959913127, 'validation/num_examples': 3554, 'test/ssim': 0.7173248350670204, 'test/loss': 0.3085642295055501, 'test/num_examples': 3581, 'score': 165.748441696167, 'total_duration': 386.76248049736023, 'accumulated_submission_time': 165.748441696167, 'accumulated_eval_time': 220.9821105003357, 'accumulated_logging_time': 0.02681732177734375, 'global_step': 317, 'preemption_count': 0}), (533, {'train/ssim': 0.7327751432146344, 'train/loss': 0.27835117067609516, 'validation/ssim': 0.7099452064355304, 'validation/loss': 0.29932890866805006, 'validation/num_examples': 3554, 'test/ssim': 0.7269072693774434, 'test/loss': 0.3014320644111282, 'test/num_examples': 3581, 'score': 246.09798288345337, 'total_duration': 471.1753816604614, 'accumulated_submission_time': 246.09798288345337, 'accumulated_eval_time': 225.01683855056763, 'accumulated_logging_time': 0.05229926109313965, 'global_step': 533, 'preemption_count': 0}), (741, {'train/ssim': 0.7351674352373395, 'train/loss': 0.2752456154142107, 'validation/ssim': 0.7134544703151379, 'validation/loss': 0.2959043452360017, 'validation/num_examples': 3554, 'test/ssim': 0.7303895287192823, 'test/loss': 0.2976956083867111, 'test/num_examples': 3581, 'score': 326.9285452365875, 'total_duration': 556.0699486732483, 'accumulated_submission_time': 326.9285452365875, 'accumulated_eval_time': 229.05130052566528, 'accumulated_logging_time': 0.07875394821166992, 'global_step': 741, 'preemption_count': 0}), (956, {'train/ssim': 0.737365586417062, 'train/loss': 0.2739224433898926, 'validation/ssim': 0.71482080600204, 'validation/loss': 0.2945187750597918, 'validation/num_examples': 3554, 'test/ssim': 0.7320889683616657, 'test/loss': 0.29620727777680816, 'test/num_examples': 3581, 'score': 407.2681622505188, 'total_duration': 640.4714977741241, 'accumulated_submission_time': 407.2681622505188, 'accumulated_eval_time': 233.08764958381653, 'accumulated_logging_time': 0.10121893882751465, 'global_step': 956, 'preemption_count': 0}), (1278, {'train/ssim': 0.7388089043753487, 'train/loss': 0.2713191338947841, 'validation/ssim': 0.7163611451225732, 'validation/loss': 0.29240912940304936, 'validation/num_examples': 3554, 'test/ssim': 0.7334276171111421, 'test/loss': 0.2939544481726473, 'test/num_examples': 3581, 'score': 487.38143610954285, 'total_duration': 724.6659700870514, 'accumulated_submission_time': 487.38143610954285, 'accumulated_eval_time': 237.13654136657715, 'accumulated_logging_time': 0.12822461128234863, 'global_step': 1278, 'preemption_count': 0}), (1601, {'train/ssim': 0.7430562973022461, 'train/loss': 0.2691326141357422, 'validation/ssim': 0.7196477697884426, 'validation/loss': 0.290388580648565, 'validation/num_examples': 3554, 'test/ssim': 0.7366651904146886, 'test/loss': 0.29196092850068067, 'test/num_examples': 3581, 'score': 568.2654356956482, 'total_duration': 809.6176927089691, 'accumulated_submission_time': 568.2654356956482, 'accumulated_eval_time': 241.18103861808777, 'accumulated_logging_time': 0.14597868919372559, 'global_step': 1601, 'preemption_count': 0}), (1925, {'train/ssim': 0.7431612695966449, 'train/loss': 0.26871749332972933, 'validation/ssim': 0.7192730407199635, 'validation/loss': 0.2903490125562746, 'validation/num_examples': 3554, 'test/ssim': 0.7365146563459928, 'test/loss': 0.2918125419968235, 'test/num_examples': 3581, 'score': 648.2632210254669, 'total_duration': 893.6838080883026, 'accumulated_submission_time': 648.2632210254669, 'accumulated_eval_time': 245.22750997543335, 'accumulated_logging_time': 0.16245532035827637, 'global_step': 1925, 'preemption_count': 0}), (2245, {'train/ssim': 0.7459299904959542, 'train/loss': 0.2672916650772095, 'validation/ssim': 0.7214099237874578, 'validation/loss': 0.2896400498887697, 'validation/num_examples': 3554, 'test/ssim': 0.7385865450904077, 'test/loss': 0.29106358728139836, 'test/num_examples': 3581, 'score': 728.2673318386078, 'total_duration': 977.7578113079071, 'accumulated_submission_time': 728.2673318386078, 'accumulated_eval_time': 249.2764711380005, 'accumulated_logging_time': 0.17804694175720215, 'global_step': 2245, 'preemption_count': 0}), (2571, {'train/ssim': 0.7447992052350726, 'train/loss': 0.26684628214154926, 'validation/ssim': 0.7214215331756472, 'validation/loss': 0.2883723940014772, 'validation/num_examples': 3554, 'test/ssim': 0.7386706069140953, 'test/loss': 0.2897538795247487, 'test/num_examples': 3581, 'score': 808.3955051898956, 'total_duration': 1061.9485957622528, 'accumulated_submission_time': 808.3955051898956, 'accumulated_eval_time': 253.3174638748169, 'accumulated_logging_time': 0.19421005249023438, 'global_step': 2571, 'preemption_count': 0}), (2896, {'train/ssim': 0.7462560789925712, 'train/loss': 0.26618594782693045, 'validation/ssim': 0.7219124935152293, 'validation/loss': 0.28831778179076395, 'validation/num_examples': 3554, 'test/ssim': 0.7392447907576445, 'test/loss': 0.28961469687020036, 'test/num_examples': 3581, 'score': 888.502414226532, 'total_duration': 1146.1257238388062, 'accumulated_submission_time': 888.502414226532, 'accumulated_eval_time': 257.3650665283203, 'accumulated_logging_time': 0.21136927604675293, 'global_step': 2896, 'preemption_count': 0}), (3218, {'train/ssim': 0.7464042391095843, 'train/loss': 0.2658578668321882, 'validation/ssim': 0.7224142389077448, 'validation/loss': 0.2878384480130223, 'validation/num_examples': 3554, 'test/ssim': 0.7397229818617356, 'test/loss': 0.2892328734772061, 'test/num_examples': 3581, 'score': 968.572104215622, 'total_duration': 1230.2640392780304, 'accumulated_submission_time': 968.572104215622, 'accumulated_eval_time': 261.4110481739044, 'accumulated_logging_time': 0.2285914421081543, 'global_step': 3218, 'preemption_count': 0}), (3541, {'train/ssim': 0.7461259024483817, 'train/loss': 0.26534838335854666, 'validation/ssim': 0.7220192449308878, 'validation/loss': 0.28730664873096334, 'validation/num_examples': 3554, 'test/ssim': 0.7393772580110305, 'test/loss': 0.28859112656206365, 'test/num_examples': 3581, 'score': 1049.247876882553, 'total_duration': 1315.0078189373016, 'accumulated_submission_time': 1049.247876882553, 'accumulated_eval_time': 265.4577271938324, 'accumulated_logging_time': 0.24448680877685547, 'global_step': 3541, 'preemption_count': 0}), (3868, {'train/ssim': 0.7468570981706891, 'train/loss': 0.26515793800354004, 'validation/ssim': 0.7227957687420864, 'validation/loss': 0.28704607292179585, 'validation/num_examples': 3554, 'test/ssim': 0.7401274739946943, 'test/loss': 0.28837077959194357, 'test/num_examples': 3581, 'score': 1129.4085009098053, 'total_duration': 1399.2352771759033, 'accumulated_submission_time': 1129.4085009098053, 'accumulated_eval_time': 269.5030303001404, 'accumulated_logging_time': 0.26044511795043945, 'global_step': 3868, 'preemption_count': 0}), (4194, {'train/ssim': 0.7480990546090263, 'train/loss': 0.26488288811274935, 'validation/ssim': 0.7240654511202167, 'validation/loss': 0.28675963359396983, 'validation/num_examples': 3554, 'test/ssim': 0.7413254742913991, 'test/loss': 0.2880500083993647, 'test/num_examples': 3581, 'score': 1209.585619688034, 'total_duration': 1483.482827425003, 'accumulated_submission_time': 1209.585619688034, 'accumulated_eval_time': 273.5513937473297, 'accumulated_logging_time': 0.27704429626464844, 'global_step': 4194, 'preemption_count': 0}), (4521, {'train/ssim': 0.7487756184169224, 'train/loss': 0.2641328402927944, 'validation/ssim': 0.7242957154350732, 'validation/loss': 0.28672145656742404, 'validation/num_examples': 3554, 'test/ssim': 0.7414980294217747, 'test/loss': 0.2880502470176801, 'test/num_examples': 3581, 'score': 1290.508192539215, 'total_duration': 1568.482634305954, 'accumulated_submission_time': 1290.508192539215, 'accumulated_eval_time': 277.60642647743225, 'accumulated_logging_time': 0.29378223419189453, 'global_step': 4521, 'preemption_count': 0}), (4849, {'train/ssim': 0.7471113204956055, 'train/loss': 0.2644621304103306, 'validation/ssim': 0.7225479873030388, 'validation/loss': 0.28653148163820696, 'validation/num_examples': 3554, 'test/ssim': 0.7399166035805291, 'test/loss': 0.2878553299423171, 'test/num_examples': 3581, 'score': 1370.7666976451874, 'total_duration': 1652.8074979782104, 'accumulated_submission_time': 1370.7666976451874, 'accumulated_eval_time': 281.6505024433136, 'accumulated_logging_time': 0.3105196952819824, 'global_step': 4849, 'preemption_count': 0}), (5177, {'train/ssim': 0.7492414883204869, 'train/loss': 0.26414685589926584, 'validation/ssim': 0.7248968619205473, 'validation/loss': 0.28626984106266706, 'validation/num_examples': 3554, 'test/ssim': 0.7420779401048939, 'test/loss': 0.2875959859217921, 'test/num_examples': 3581, 'score': 1451.0357098579407, 'total_duration': 1737.1511163711548, 'accumulated_submission_time': 1451.0357098579407, 'accumulated_eval_time': 285.700541973114, 'accumulated_logging_time': 0.32953715324401855, 'global_step': 5177, 'preemption_count': 0}), (5428, {'train/ssim': 0.7477006912231445, 'train/loss': 0.2641615356717791, 'validation/ssim': 0.7235122534688028, 'validation/loss': 0.28623805263435564, 'validation/num_examples': 3554, 'test/ssim': 0.7407968324577632, 'test/loss': 0.28756165897270314, 'test/num_examples': 3581, 'score': 1512.966007232666, 'total_duration': 1803.151204586029, 'accumulated_submission_time': 1512.966007232666, 'accumulated_eval_time': 289.74918484687805, 'accumulated_logging_time': 0.3464639186859131, 'global_step': 5428, 'preemption_count': 0})], 'global_step': 5428}
I0502 10:22:24.446481 140129979877184 submission_runner.py:581] Timing: 1512.966007232666
I0502 10:22:24.446526 140129979877184 submission_runner.py:582] ====================
I0502 10:22:24.446631 140129979877184 submission_runner.py:645] Final fastmri score: 1512.966007232666
