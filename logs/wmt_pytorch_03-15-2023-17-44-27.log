WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0315 17:44:46.288348 140570348128064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0315 17:44:47.299932 140221790058304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0315 17:44:47.300838 140181012973376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0315 17:44:47.300859 140329820518208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0315 17:44:47.300891 140229114566464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0315 17:44:47.300910 140546311149376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0315 17:44:47.301213 139633563592512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0315 17:44:47.311405 140329820518208 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.311396 140181012973376 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.311269 140515296233280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0315 17:44:47.311443 140229114566464 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.311463 140546311149376 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.311670 140515296233280 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.311724 139633563592512 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.320307 140570348128064 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:47.320720 140221790058304 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 17:44:51.677491 140515296233280 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/wmt_pytorch.
W0315 17:44:51.763689 140570348128064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.765251 140515296233280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.765779 140221790058304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.766245 140546311149376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.766710 139633563592512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.767980 140329820518208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.768771 140229114566464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 17:44:51.770051 140181012973376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0315 17:44:51.771124 140515296233280 submission_runner.py:484] Using RNG seed 2050882465
I0315 17:44:51.772546 140515296233280 submission_runner.py:493] --- Tuning run 1/1 ---
I0315 17:44:51.772681 140515296233280 submission_runner.py:498] Creating tuning directory at /experiment_runs/timing/wmt_pytorch/trial_1.
I0315 17:44:51.772879 140515296233280 logger_utils.py:84] Saving hparams to /experiment_runs/timing/wmt_pytorch/trial_1/hparams.json.
I0315 17:44:51.773967 140515296233280 submission_runner.py:230] Initializing dataset.
I0315 17:44:51.774079 140515296233280 submission_runner.py:237] Initializing model.
I0315 17:44:55.366415 140515296233280 submission_runner.py:247] Initializing optimizer.
I0315 17:44:55.367934 140515296233280 submission_runner.py:254] Initializing metrics bundle.
I0315 17:44:55.368069 140515296233280 submission_runner.py:268] Initializing checkpoint and logger.
I0315 17:44:55.370872 140515296233280 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0315 17:44:55.371039 140515296233280 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:44:56.025887 140515296233280 submission_runner.py:289] Saving meta data to /experiment_runs/timing/wmt_pytorch/trial_1/meta_data_0.json.
I0315 17:44:56.027811 140515296233280 submission_runner.py:292] Saving flags to /experiment_runs/timing/wmt_pytorch/trial_1/flags_0.json.
I0315 17:44:56.065218 140515296233280 submission_runner.py:302] Starting training loop.
I0315 17:44:56.077964 140515296233280 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0315 17:44:56.081747 140515296233280 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0315 17:44:56.081876 140515296233280 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0315 17:44:56.137602 140515296233280 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0315 17:45:00.244871 140475126249216 logging_writer.py:48] [0] global_step=0, grad_norm=4.749248, loss=11.045822
I0315 17:45:00.249051 140515296233280 pytorch_submission_base.py:86] 0) loss = 11.046, grad_norm = 4.749
I0315 17:45:00.250201 140515296233280 spec.py:298] Evaluating on the training split.
I0315 17:45:00.252895 140515296233280 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0315 17:45:00.255557 140515296233280 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0315 17:45:00.255661 140515296233280 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0315 17:45:00.284241 140515296233280 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0315 17:45:04.443186 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 17:49:39.206267 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 17:49:39.209012 140515296233280 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0315 17:49:39.212224 140515296233280 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0315 17:49:39.212358 140515296233280 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0315 17:49:39.247208 140515296233280 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0315 17:49:43.098443 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 17:54:10.964076 140515296233280 spec.py:326] Evaluating on the test split.
I0315 17:54:10.966771 140515296233280 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0315 17:54:10.969841 140515296233280 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0315 17:54:10.969959 140515296233280 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0315 17:54:10.997800 140515296233280 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0315 17:54:14.911702 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 17:58:48.055404 140515296233280 submission_runner.py:362] Time since start: 4.19s, 	Step: 1, 	{'train/accuracy': 0.0006215755789861411, 'train/loss': 11.07326966020535, 'train/bleu': 8.618773875820979e-11, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.0789830566267, 'validation/bleu': 1.0550295602636623e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.062961913892279, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0315 17:58:48.065450 140457232099072 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=4.183402, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.062962, test/num_examples=3003, total_duration=4.185377, train/accuracy=0.000622, train/bleu=0.000000, train/loss=11.073270, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.078983, validation/num_examples=3000
I0315 17:58:50.194167 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_1.
I0315 17:58:50.204760 139633563592512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.204827 140221790058304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.204820 140181012973376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.204875 140515296233280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.204838 140570348128064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.204874 140546311149376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.204917 140229114566464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.205051 140329820518208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 17:58:50.647797 140457223706368 logging_writer.py:48] [1] global_step=1, grad_norm=4.770947, loss=11.045828
I0315 17:58:50.651814 140515296233280 pytorch_submission_base.py:86] 1) loss = 11.046, grad_norm = 4.771
I0315 17:58:51.104654 140457232099072 logging_writer.py:48] [2] global_step=2, grad_norm=4.716418, loss=11.009341
I0315 17:58:51.108550 140515296233280 pytorch_submission_base.py:86] 2) loss = 11.009, grad_norm = 4.716
I0315 17:58:51.568196 140457223706368 logging_writer.py:48] [3] global_step=3, grad_norm=4.561864, loss=10.964977
I0315 17:58:51.571959 140515296233280 pytorch_submission_base.py:86] 3) loss = 10.965, grad_norm = 4.562
I0315 17:58:52.026106 140457232099072 logging_writer.py:48] [4] global_step=4, grad_norm=4.501201, loss=10.887428
I0315 17:58:52.029583 140515296233280 pytorch_submission_base.py:86] 4) loss = 10.887, grad_norm = 4.501
I0315 17:58:52.496931 140457223706368 logging_writer.py:48] [5] global_step=5, grad_norm=4.356817, loss=10.788953
I0315 17:58:52.500436 140515296233280 pytorch_submission_base.py:86] 5) loss = 10.789, grad_norm = 4.357
I0315 17:58:52.952243 140457232099072 logging_writer.py:48] [6] global_step=6, grad_norm=4.118663, loss=10.697286
I0315 17:58:52.956588 140515296233280 pytorch_submission_base.py:86] 6) loss = 10.697, grad_norm = 4.119
I0315 17:58:53.410986 140457223706368 logging_writer.py:48] [7] global_step=7, grad_norm=3.839347, loss=10.595502
I0315 17:58:53.414743 140515296233280 pytorch_submission_base.py:86] 7) loss = 10.596, grad_norm = 3.839
I0315 17:58:53.870878 140457232099072 logging_writer.py:48] [8] global_step=8, grad_norm=3.603987, loss=10.471857
I0315 17:58:53.874540 140515296233280 pytorch_submission_base.py:86] 8) loss = 10.472, grad_norm = 3.604
I0315 17:58:54.328255 140457223706368 logging_writer.py:48] [9] global_step=9, grad_norm=3.312116, loss=10.353788
I0315 17:58:54.332024 140515296233280 pytorch_submission_base.py:86] 9) loss = 10.354, grad_norm = 3.312
I0315 17:58:54.785956 140457232099072 logging_writer.py:48] [10] global_step=10, grad_norm=3.011818, loss=10.222914
I0315 17:58:54.789687 140515296233280 pytorch_submission_base.py:86] 10) loss = 10.223, grad_norm = 3.012
I0315 17:58:55.243234 140457223706368 logging_writer.py:48] [11] global_step=11, grad_norm=2.801330, loss=10.095486
I0315 17:58:55.246908 140515296233280 pytorch_submission_base.py:86] 11) loss = 10.095, grad_norm = 2.801
I0315 17:58:55.701164 140457232099072 logging_writer.py:48] [12] global_step=12, grad_norm=2.453402, loss=10.018727
I0315 17:58:55.704765 140515296233280 pytorch_submission_base.py:86] 12) loss = 10.019, grad_norm = 2.453
I0315 17:58:56.157474 140457223706368 logging_writer.py:48] [13] global_step=13, grad_norm=2.265161, loss=9.876835
I0315 17:58:56.161167 140515296233280 pytorch_submission_base.py:86] 13) loss = 9.877, grad_norm = 2.265
I0315 17:58:56.616534 140457232099072 logging_writer.py:48] [14] global_step=14, grad_norm=2.065468, loss=9.769730
I0315 17:58:56.620242 140515296233280 pytorch_submission_base.py:86] 14) loss = 9.770, grad_norm = 2.065
I0315 17:58:57.072641 140457223706368 logging_writer.py:48] [15] global_step=15, grad_norm=1.848680, loss=9.690877
I0315 17:58:57.076246 140515296233280 pytorch_submission_base.py:86] 15) loss = 9.691, grad_norm = 1.849
I0315 17:58:57.527516 140457232099072 logging_writer.py:48] [16] global_step=16, grad_norm=1.699870, loss=9.612700
I0315 17:58:57.531319 140515296233280 pytorch_submission_base.py:86] 16) loss = 9.613, grad_norm = 1.700
I0315 17:58:57.985362 140457223706368 logging_writer.py:48] [17] global_step=17, grad_norm=1.537206, loss=9.537024
I0315 17:58:57.989274 140515296233280 pytorch_submission_base.py:86] 17) loss = 9.537, grad_norm = 1.537
I0315 17:58:58.444338 140457232099072 logging_writer.py:48] [18] global_step=18, grad_norm=1.431914, loss=9.442634
I0315 17:58:58.448011 140515296233280 pytorch_submission_base.py:86] 18) loss = 9.443, grad_norm = 1.432
I0315 17:58:58.903130 140457223706368 logging_writer.py:48] [19] global_step=19, grad_norm=1.328983, loss=9.356174
I0315 17:58:58.907381 140515296233280 pytorch_submission_base.py:86] 19) loss = 9.356, grad_norm = 1.329
I0315 17:58:59.361557 140457232099072 logging_writer.py:48] [20] global_step=20, grad_norm=1.216703, loss=9.321109
I0315 17:58:59.365094 140515296233280 pytorch_submission_base.py:86] 20) loss = 9.321, grad_norm = 1.217
I0315 17:58:59.819104 140457223706368 logging_writer.py:48] [21] global_step=21, grad_norm=1.141023, loss=9.240422
I0315 17:58:59.823035 140515296233280 pytorch_submission_base.py:86] 21) loss = 9.240, grad_norm = 1.141
I0315 17:59:00.275855 140457232099072 logging_writer.py:48] [22] global_step=22, grad_norm=1.061140, loss=9.174065
I0315 17:59:00.279796 140515296233280 pytorch_submission_base.py:86] 22) loss = 9.174, grad_norm = 1.061
I0315 17:59:00.731168 140457223706368 logging_writer.py:48] [23] global_step=23, grad_norm=0.961630, loss=9.152628
I0315 17:59:00.734671 140515296233280 pytorch_submission_base.py:86] 23) loss = 9.153, grad_norm = 0.962
I0315 17:59:01.188421 140457232099072 logging_writer.py:48] [24] global_step=24, grad_norm=0.889501, loss=9.044861
I0315 17:59:01.192169 140515296233280 pytorch_submission_base.py:86] 24) loss = 9.045, grad_norm = 0.890
I0315 17:59:01.648344 140457223706368 logging_writer.py:48] [25] global_step=25, grad_norm=0.808231, loss=9.010694
I0315 17:59:01.652034 140515296233280 pytorch_submission_base.py:86] 25) loss = 9.011, grad_norm = 0.808
I0315 17:59:02.106518 140457232099072 logging_writer.py:48] [26] global_step=26, grad_norm=0.752637, loss=8.964204
I0315 17:59:02.110479 140515296233280 pytorch_submission_base.py:86] 26) loss = 8.964, grad_norm = 0.753
I0315 17:59:02.563541 140457223706368 logging_writer.py:48] [27] global_step=27, grad_norm=0.690723, loss=8.969712
I0315 17:59:02.567434 140515296233280 pytorch_submission_base.py:86] 27) loss = 8.970, grad_norm = 0.691
I0315 17:59:03.024094 140457232099072 logging_writer.py:48] [28] global_step=28, grad_norm=0.665383, loss=8.912150
I0315 17:59:03.027945 140515296233280 pytorch_submission_base.py:86] 28) loss = 8.912, grad_norm = 0.665
I0315 17:59:03.483363 140457223706368 logging_writer.py:48] [29] global_step=29, grad_norm=0.619197, loss=8.889805
I0315 17:59:03.486965 140515296233280 pytorch_submission_base.py:86] 29) loss = 8.890, grad_norm = 0.619
I0315 17:59:03.939609 140457232099072 logging_writer.py:48] [30] global_step=30, grad_norm=0.581522, loss=8.823548
I0315 17:59:03.943525 140515296233280 pytorch_submission_base.py:86] 30) loss = 8.824, grad_norm = 0.582
I0315 17:59:04.397028 140457223706368 logging_writer.py:48] [31] global_step=31, grad_norm=0.555409, loss=8.789032
I0315 17:59:04.400702 140515296233280 pytorch_submission_base.py:86] 31) loss = 8.789, grad_norm = 0.555
I0315 17:59:04.853035 140457232099072 logging_writer.py:48] [32] global_step=32, grad_norm=0.503921, loss=8.746021
I0315 17:59:04.856612 140515296233280 pytorch_submission_base.py:86] 32) loss = 8.746, grad_norm = 0.504
I0315 17:59:05.310789 140457223706368 logging_writer.py:48] [33] global_step=33, grad_norm=0.461706, loss=8.757409
I0315 17:59:05.314583 140515296233280 pytorch_submission_base.py:86] 33) loss = 8.757, grad_norm = 0.462
I0315 17:59:05.768644 140457232099072 logging_writer.py:48] [34] global_step=34, grad_norm=0.427903, loss=8.708024
I0315 17:59:05.772085 140515296233280 pytorch_submission_base.py:86] 34) loss = 8.708, grad_norm = 0.428
I0315 17:59:06.224929 140457223706368 logging_writer.py:48] [35] global_step=35, grad_norm=0.393717, loss=8.735483
I0315 17:59:06.228628 140515296233280 pytorch_submission_base.py:86] 35) loss = 8.735, grad_norm = 0.394
I0315 17:59:06.685496 140457232099072 logging_writer.py:48] [36] global_step=36, grad_norm=0.391169, loss=8.676181
I0315 17:59:06.689042 140515296233280 pytorch_submission_base.py:86] 36) loss = 8.676, grad_norm = 0.391
I0315 17:59:07.145720 140457223706368 logging_writer.py:48] [37] global_step=37, grad_norm=0.362235, loss=8.700035
I0315 17:59:07.149277 140515296233280 pytorch_submission_base.py:86] 37) loss = 8.700, grad_norm = 0.362
I0315 17:59:07.605594 140457232099072 logging_writer.py:48] [38] global_step=38, grad_norm=0.323904, loss=8.669719
I0315 17:59:07.609358 140515296233280 pytorch_submission_base.py:86] 38) loss = 8.670, grad_norm = 0.324
I0315 17:59:08.066988 140457223706368 logging_writer.py:48] [39] global_step=39, grad_norm=0.319697, loss=8.648915
I0315 17:59:08.070387 140515296233280 pytorch_submission_base.py:86] 39) loss = 8.649, grad_norm = 0.320
I0315 17:59:08.523360 140457232099072 logging_writer.py:48] [40] global_step=40, grad_norm=0.296414, loss=8.623127
I0315 17:59:08.527233 140515296233280 pytorch_submission_base.py:86] 40) loss = 8.623, grad_norm = 0.296
I0315 17:59:08.977534 140457223706368 logging_writer.py:48] [41] global_step=41, grad_norm=0.281714, loss=8.597672
I0315 17:59:08.981281 140515296233280 pytorch_submission_base.py:86] 41) loss = 8.598, grad_norm = 0.282
I0315 17:59:09.437528 140457232099072 logging_writer.py:48] [42] global_step=42, grad_norm=0.270933, loss=8.629293
I0315 17:59:09.441227 140515296233280 pytorch_submission_base.py:86] 42) loss = 8.629, grad_norm = 0.271
I0315 17:59:09.901258 140457223706368 logging_writer.py:48] [43] global_step=43, grad_norm=0.272559, loss=8.549949
I0315 17:59:09.904991 140515296233280 pytorch_submission_base.py:86] 43) loss = 8.550, grad_norm = 0.273
I0315 17:59:10.361810 140457232099072 logging_writer.py:48] [44] global_step=44, grad_norm=0.240055, loss=8.550949
I0315 17:59:10.365672 140515296233280 pytorch_submission_base.py:86] 44) loss = 8.551, grad_norm = 0.240
I0315 17:59:10.819697 140457223706368 logging_writer.py:48] [45] global_step=45, grad_norm=0.237081, loss=8.543085
I0315 17:59:10.823099 140515296233280 pytorch_submission_base.py:86] 45) loss = 8.543, grad_norm = 0.237
I0315 17:59:11.280294 140457232099072 logging_writer.py:48] [46] global_step=46, grad_norm=0.237243, loss=8.535849
I0315 17:59:11.283592 140515296233280 pytorch_submission_base.py:86] 46) loss = 8.536, grad_norm = 0.237
I0315 17:59:11.736437 140457223706368 logging_writer.py:48] [47] global_step=47, grad_norm=0.215601, loss=8.515676
I0315 17:59:11.740167 140515296233280 pytorch_submission_base.py:86] 47) loss = 8.516, grad_norm = 0.216
I0315 17:59:12.194657 140457232099072 logging_writer.py:48] [48] global_step=48, grad_norm=0.212399, loss=8.564720
I0315 17:59:12.198250 140515296233280 pytorch_submission_base.py:86] 48) loss = 8.565, grad_norm = 0.212
I0315 17:59:12.649237 140457223706368 logging_writer.py:48] [49] global_step=49, grad_norm=0.199790, loss=8.522272
I0315 17:59:12.652602 140515296233280 pytorch_submission_base.py:86] 49) loss = 8.522, grad_norm = 0.200
I0315 17:59:13.106786 140457232099072 logging_writer.py:48] [50] global_step=50, grad_norm=0.193387, loss=8.509610
I0315 17:59:13.110505 140515296233280 pytorch_submission_base.py:86] 50) loss = 8.510, grad_norm = 0.193
I0315 17:59:13.566005 140457223706368 logging_writer.py:48] [51] global_step=51, grad_norm=0.198195, loss=8.488633
I0315 17:59:13.569151 140515296233280 pytorch_submission_base.py:86] 51) loss = 8.489, grad_norm = 0.198
I0315 17:59:14.022241 140457232099072 logging_writer.py:48] [52] global_step=52, grad_norm=0.187655, loss=8.475080
I0315 17:59:14.025992 140515296233280 pytorch_submission_base.py:86] 52) loss = 8.475, grad_norm = 0.188
I0315 17:59:14.480796 140457223706368 logging_writer.py:48] [53] global_step=53, grad_norm=0.178483, loss=8.494604
I0315 17:59:14.484681 140515296233280 pytorch_submission_base.py:86] 53) loss = 8.495, grad_norm = 0.178
I0315 17:59:14.943148 140457232099072 logging_writer.py:48] [54] global_step=54, grad_norm=0.174264, loss=8.517178
I0315 17:59:14.946862 140515296233280 pytorch_submission_base.py:86] 54) loss = 8.517, grad_norm = 0.174
I0315 17:59:15.402274 140457223706368 logging_writer.py:48] [55] global_step=55, grad_norm=0.170322, loss=8.501184
I0315 17:59:15.405948 140515296233280 pytorch_submission_base.py:86] 55) loss = 8.501, grad_norm = 0.170
I0315 17:59:15.861117 140457232099072 logging_writer.py:48] [56] global_step=56, grad_norm=0.173334, loss=8.482235
I0315 17:59:15.864932 140515296233280 pytorch_submission_base.py:86] 56) loss = 8.482, grad_norm = 0.173
I0315 17:59:16.321020 140457223706368 logging_writer.py:48] [57] global_step=57, grad_norm=0.165392, loss=8.490508
I0315 17:59:16.324696 140515296233280 pytorch_submission_base.py:86] 57) loss = 8.491, grad_norm = 0.165
I0315 17:59:16.780543 140457232099072 logging_writer.py:48] [58] global_step=58, grad_norm=0.175512, loss=8.448359
I0315 17:59:16.784332 140515296233280 pytorch_submission_base.py:86] 58) loss = 8.448, grad_norm = 0.176
I0315 17:59:17.241713 140457223706368 logging_writer.py:48] [59] global_step=59, grad_norm=0.163630, loss=8.503182
I0315 17:59:17.245310 140515296233280 pytorch_submission_base.py:86] 59) loss = 8.503, grad_norm = 0.164
I0315 17:59:17.700082 140457232099072 logging_writer.py:48] [60] global_step=60, grad_norm=0.161066, loss=8.435437
I0315 17:59:17.703450 140515296233280 pytorch_submission_base.py:86] 60) loss = 8.435, grad_norm = 0.161
I0315 17:59:18.159004 140457223706368 logging_writer.py:48] [61] global_step=61, grad_norm=0.165227, loss=8.443990
I0315 17:59:18.162288 140515296233280 pytorch_submission_base.py:86] 61) loss = 8.444, grad_norm = 0.165
I0315 17:59:18.617132 140457232099072 logging_writer.py:48] [62] global_step=62, grad_norm=0.165210, loss=8.472758
I0315 17:59:18.620833 140515296233280 pytorch_submission_base.py:86] 62) loss = 8.473, grad_norm = 0.165
I0315 17:59:19.107704 140457223706368 logging_writer.py:48] [63] global_step=63, grad_norm=0.160639, loss=8.423870
I0315 17:59:19.111621 140515296233280 pytorch_submission_base.py:86] 63) loss = 8.424, grad_norm = 0.161
I0315 17:59:19.587548 140457232099072 logging_writer.py:48] [64] global_step=64, grad_norm=0.153428, loss=8.431059
I0315 17:59:19.591744 140515296233280 pytorch_submission_base.py:86] 64) loss = 8.431, grad_norm = 0.153
I0315 17:59:20.064295 140457223706368 logging_writer.py:48] [65] global_step=65, grad_norm=0.163431, loss=8.440531
I0315 17:59:20.067720 140515296233280 pytorch_submission_base.py:86] 65) loss = 8.441, grad_norm = 0.163
I0315 17:59:20.543297 140457232099072 logging_writer.py:48] [66] global_step=66, grad_norm=0.156810, loss=8.428159
I0315 17:59:20.546771 140515296233280 pytorch_submission_base.py:86] 66) loss = 8.428, grad_norm = 0.157
I0315 17:59:21.005309 140457223706368 logging_writer.py:48] [67] global_step=67, grad_norm=0.181042, loss=8.392904
I0315 17:59:21.008566 140515296233280 pytorch_submission_base.py:86] 67) loss = 8.393, grad_norm = 0.181
I0315 17:59:21.462827 140457232099072 logging_writer.py:48] [68] global_step=68, grad_norm=0.156475, loss=8.400941
I0315 17:59:21.466181 140515296233280 pytorch_submission_base.py:86] 68) loss = 8.401, grad_norm = 0.156
I0315 17:59:21.921716 140457223706368 logging_writer.py:48] [69] global_step=69, grad_norm=0.167273, loss=8.411254
I0315 17:59:21.924893 140515296233280 pytorch_submission_base.py:86] 69) loss = 8.411, grad_norm = 0.167
I0315 17:59:22.378313 140457232099072 logging_writer.py:48] [70] global_step=70, grad_norm=0.159853, loss=8.372885
I0315 17:59:22.381641 140515296233280 pytorch_submission_base.py:86] 70) loss = 8.373, grad_norm = 0.160
I0315 17:59:22.836132 140457223706368 logging_writer.py:48] [71] global_step=71, grad_norm=0.159345, loss=8.399918
I0315 17:59:22.839406 140515296233280 pytorch_submission_base.py:86] 71) loss = 8.400, grad_norm = 0.159
I0315 17:59:23.297878 140457232099072 logging_writer.py:48] [72] global_step=72, grad_norm=0.163512, loss=8.376536
I0315 17:59:23.301271 140515296233280 pytorch_submission_base.py:86] 72) loss = 8.377, grad_norm = 0.164
I0315 17:59:23.755116 140457223706368 logging_writer.py:48] [73] global_step=73, grad_norm=0.159083, loss=8.367005
I0315 17:59:23.758294 140515296233280 pytorch_submission_base.py:86] 73) loss = 8.367, grad_norm = 0.159
I0315 17:59:24.212070 140457232099072 logging_writer.py:48] [74] global_step=74, grad_norm=0.156095, loss=8.360196
I0315 17:59:24.215556 140515296233280 pytorch_submission_base.py:86] 74) loss = 8.360, grad_norm = 0.156
I0315 17:59:24.670992 140457223706368 logging_writer.py:48] [75] global_step=75, grad_norm=0.160568, loss=8.367690
I0315 17:59:24.674341 140515296233280 pytorch_submission_base.py:86] 75) loss = 8.368, grad_norm = 0.161
I0315 17:59:25.129164 140457232099072 logging_writer.py:48] [76] global_step=76, grad_norm=0.167325, loss=8.314550
I0315 17:59:25.132479 140515296233280 pytorch_submission_base.py:86] 76) loss = 8.315, grad_norm = 0.167
I0315 17:59:25.585222 140457223706368 logging_writer.py:48] [77] global_step=77, grad_norm=0.165845, loss=8.343263
I0315 17:59:25.588541 140515296233280 pytorch_submission_base.py:86] 77) loss = 8.343, grad_norm = 0.166
I0315 17:59:26.042943 140457232099072 logging_writer.py:48] [78] global_step=78, grad_norm=0.166224, loss=8.338040
I0315 17:59:26.046124 140515296233280 pytorch_submission_base.py:86] 78) loss = 8.338, grad_norm = 0.166
I0315 17:59:26.501877 140457223706368 logging_writer.py:48] [79] global_step=79, grad_norm=0.172003, loss=8.327924
I0315 17:59:26.505251 140515296233280 pytorch_submission_base.py:86] 79) loss = 8.328, grad_norm = 0.172
I0315 17:59:26.960084 140457232099072 logging_writer.py:48] [80] global_step=80, grad_norm=0.167171, loss=8.326605
I0315 17:59:26.963388 140515296233280 pytorch_submission_base.py:86] 80) loss = 8.327, grad_norm = 0.167
I0315 17:59:27.416552 140457223706368 logging_writer.py:48] [81] global_step=81, grad_norm=0.151231, loss=8.300261
I0315 17:59:27.419669 140515296233280 pytorch_submission_base.py:86] 81) loss = 8.300, grad_norm = 0.151
I0315 17:59:27.877698 140457232099072 logging_writer.py:48] [82] global_step=82, grad_norm=0.155465, loss=8.256769
I0315 17:59:27.881286 140515296233280 pytorch_submission_base.py:86] 82) loss = 8.257, grad_norm = 0.155
I0315 17:59:28.338236 140457223706368 logging_writer.py:48] [83] global_step=83, grad_norm=0.152984, loss=8.278613
I0315 17:59:28.341455 140515296233280 pytorch_submission_base.py:86] 83) loss = 8.279, grad_norm = 0.153
I0315 17:59:28.797192 140457232099072 logging_writer.py:48] [84] global_step=84, grad_norm=0.155642, loss=8.296515
I0315 17:59:28.800595 140515296233280 pytorch_submission_base.py:86] 84) loss = 8.297, grad_norm = 0.156
I0315 17:59:29.255821 140457223706368 logging_writer.py:48] [85] global_step=85, grad_norm=0.164071, loss=8.243655
I0315 17:59:29.259450 140515296233280 pytorch_submission_base.py:86] 85) loss = 8.244, grad_norm = 0.164
I0315 17:59:29.716054 140457232099072 logging_writer.py:48] [86] global_step=86, grad_norm=0.164916, loss=8.312979
I0315 17:59:29.720100 140515296233280 pytorch_submission_base.py:86] 86) loss = 8.313, grad_norm = 0.165
I0315 17:59:30.174713 140457223706368 logging_writer.py:48] [87] global_step=87, grad_norm=0.151666, loss=8.239102
I0315 17:59:30.178268 140515296233280 pytorch_submission_base.py:86] 87) loss = 8.239, grad_norm = 0.152
I0315 17:59:30.631062 140457232099072 logging_writer.py:48] [88] global_step=88, grad_norm=0.198770, loss=8.198410
I0315 17:59:30.634835 140515296233280 pytorch_submission_base.py:86] 88) loss = 8.198, grad_norm = 0.199
I0315 17:59:31.089103 140457223706368 logging_writer.py:48] [89] global_step=89, grad_norm=0.162741, loss=8.240663
I0315 17:59:31.092699 140515296233280 pytorch_submission_base.py:86] 89) loss = 8.241, grad_norm = 0.163
I0315 17:59:31.559261 140457232099072 logging_writer.py:48] [90] global_step=90, grad_norm=0.166995, loss=8.267583
I0315 17:59:31.563022 140515296233280 pytorch_submission_base.py:86] 90) loss = 8.268, grad_norm = 0.167
I0315 17:59:32.019418 140457223706368 logging_writer.py:48] [91] global_step=91, grad_norm=0.174831, loss=8.231445
I0315 17:59:32.023117 140515296233280 pytorch_submission_base.py:86] 91) loss = 8.231, grad_norm = 0.175
I0315 17:59:32.479355 140457232099072 logging_writer.py:48] [92] global_step=92, grad_norm=0.150329, loss=8.201907
I0315 17:59:32.482684 140515296233280 pytorch_submission_base.py:86] 92) loss = 8.202, grad_norm = 0.150
I0315 17:59:32.937958 140457223706368 logging_writer.py:48] [93] global_step=93, grad_norm=0.162965, loss=8.250115
I0315 17:59:32.941303 140515296233280 pytorch_submission_base.py:86] 93) loss = 8.250, grad_norm = 0.163
I0315 17:59:33.397161 140457232099072 logging_writer.py:48] [94] global_step=94, grad_norm=0.182235, loss=8.206442
I0315 17:59:33.400307 140515296233280 pytorch_submission_base.py:86] 94) loss = 8.206, grad_norm = 0.182
I0315 17:59:33.855152 140457223706368 logging_writer.py:48] [95] global_step=95, grad_norm=0.173904, loss=8.193697
I0315 17:59:33.858652 140515296233280 pytorch_submission_base.py:86] 95) loss = 8.194, grad_norm = 0.174
I0315 17:59:34.311988 140457232099072 logging_writer.py:48] [96] global_step=96, grad_norm=0.178016, loss=8.208867
I0315 17:59:34.315558 140515296233280 pytorch_submission_base.py:86] 96) loss = 8.209, grad_norm = 0.178
I0315 17:59:34.773685 140457223706368 logging_writer.py:48] [97] global_step=97, grad_norm=0.160103, loss=8.193547
I0315 17:59:34.777698 140515296233280 pytorch_submission_base.py:86] 97) loss = 8.194, grad_norm = 0.160
I0315 17:59:35.232500 140457232099072 logging_writer.py:48] [98] global_step=98, grad_norm=0.158292, loss=8.185570
I0315 17:59:35.236129 140515296233280 pytorch_submission_base.py:86] 98) loss = 8.186, grad_norm = 0.158
I0315 17:59:35.689372 140457223706368 logging_writer.py:48] [99] global_step=99, grad_norm=0.170474, loss=8.191714
I0315 17:59:35.692953 140515296233280 pytorch_submission_base.py:86] 99) loss = 8.192, grad_norm = 0.170
I0315 17:59:36.145565 140457232099072 logging_writer.py:48] [100] global_step=100, grad_norm=0.163546, loss=8.155763
I0315 17:59:36.149159 140515296233280 pytorch_submission_base.py:86] 100) loss = 8.156, grad_norm = 0.164
I0315 18:02:34.704344 140457223706368 logging_writer.py:48] [500] global_step=500, grad_norm=0.619194, loss=5.648260
I0315 18:02:34.708451 140515296233280 pytorch_submission_base.py:86] 500) loss = 5.648, grad_norm = 0.619
I0315 18:06:17.941953 140457232099072 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.657842, loss=4.310694
I0315 18:06:17.946337 140515296233280 pytorch_submission_base.py:86] 1000) loss = 4.311, grad_norm = 0.658
I0315 18:10:01.234639 140457223706368 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.512028, loss=3.510592
I0315 18:10:01.239468 140515296233280 pytorch_submission_base.py:86] 1500) loss = 3.511, grad_norm = 0.512
I0315 18:12:50.445926 140515296233280 spec.py:298] Evaluating on the training split.
I0315 18:12:54.334177 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:15:28.827821 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 18:15:32.582053 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:18:01.016752 140515296233280 spec.py:326] Evaluating on the test split.
I0315 18:18:04.839215 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:20:17.294057 140515296233280 submission_runner.py:362] Time since start: 1674.38s, 	Step: 1880, 	{'train/accuracy': 0.49743297204791787, 'train/loss': 3.0247368796349114, 'train/bleu': 20.52722740944265, 'validation/accuracy': 0.4927527247027315, 'validation/loss': 3.041989172483912, 'validation/bleu': 16.26766595962541, 'validation/num_examples': 3000, 'test/accuracy': 0.4902794724304224, 'test/loss': 3.107095752716286, 'test/bleu': 15.146659926047317, 'test/num_examples': 3003}
I0315 18:20:17.303430 140457232099072 logging_writer.py:48] [1880] global_step=1880, preemption_count=0, score=841.429967, test/accuracy=0.490279, test/bleu=15.146660, test/loss=3.107096, test/num_examples=3003, total_duration=1674.381091, train/accuracy=0.497433, train/bleu=20.527227, train/loss=3.024737, validation/accuracy=0.492753, validation/bleu=16.267666, validation/loss=3.041989, validation/num_examples=3000
I0315 18:20:19.460696 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_1880.
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 18:21:13.427990 140457223706368 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.308447, loss=2.970363
I0315 18:21:13.431769 140515296233280 pytorch_submission_base.py:86] 2000) loss = 2.970, grad_norm = 0.308
I0315 18:24:56.583869 140457232099072 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.239684, loss=2.801966
I0315 18:24:56.587249 140515296233280 pytorch_submission_base.py:86] 2500) loss = 2.802, grad_norm = 0.240
I0315 18:28:39.733968 140457223706368 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.206977, loss=2.622661
I0315 18:28:39.737762 140515296233280 pytorch_submission_base.py:86] 3000) loss = 2.623, grad_norm = 0.207
I0315 18:32:22.732278 140457232099072 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.233304, loss=2.425745
I0315 18:32:22.735986 140515296233280 pytorch_submission_base.py:86] 3500) loss = 2.426, grad_norm = 0.233
I0315 18:34:19.657122 140515296233280 spec.py:298] Evaluating on the training split.
I0315 18:34:23.563052 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:36:45.264880 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 18:36:49.008715 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:38:58.558711 140515296233280 spec.py:326] Evaluating on the test split.
I0315 18:39:02.367249 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:41:02.209324 140515296233280 submission_runner.py:362] Time since start: 2963.59s, 	Step: 3763, 	{'train/accuracy': 0.5649170010944911, 'train/loss': 2.3638860346930866, 'train/bleu': 25.997323666152543, 'validation/accuracy': 0.5792612614846685, 'validation/loss': 2.2553893473112545, 'validation/bleu': 22.491922136302023, 'validation/num_examples': 3000, 'test/accuracy': 0.5814421009819302, 'test/loss': 2.240032718900703, 'test/bleu': 21.43944740788008, 'test/num_examples': 3003}
I0315 18:41:02.219329 140457223706368 logging_writer.py:48] [3763] global_step=3763, preemption_count=0, score=1678.597128, test/accuracy=0.581442, test/bleu=21.439447, test/loss=2.240033, test/num_examples=3003, total_duration=2963.592274, train/accuracy=0.564917, train/bleu=25.997324, train/loss=2.363886, validation/accuracy=0.579261, validation/bleu=22.491922, validation/loss=2.255389, validation/num_examples=3000
I0315 18:41:04.351516 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_3763.
I0315 18:42:50.353088 140457232099072 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.187165, loss=2.460001
I0315 18:42:50.356981 140515296233280 pytorch_submission_base.py:86] 4000) loss = 2.460, grad_norm = 0.187
I0315 18:46:33.724740 140457223706368 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.191553, loss=2.369494
I0315 18:46:33.728839 140515296233280 pytorch_submission_base.py:86] 4500) loss = 2.369, grad_norm = 0.192
I0315 18:50:17.017375 140457232099072 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.167700, loss=2.160035
I0315 18:50:17.021823 140515296233280 pytorch_submission_base.py:86] 5000) loss = 2.160, grad_norm = 0.168
I0315 18:54:00.109780 140457223706368 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.160305, loss=2.326672
I0315 18:54:00.113659 140515296233280 pytorch_submission_base.py:86] 5500) loss = 2.327, grad_norm = 0.160
I0315 18:55:04.437696 140515296233280 spec.py:298] Evaluating on the training split.
I0315 18:55:08.336601 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 18:58:01.372635 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 18:58:05.111225 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:00:18.134203 140515296233280 spec.py:326] Evaluating on the test split.
I0315 19:00:21.941513 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:02:27.006843 140515296233280 submission_runner.py:362] Time since start: 4208.37s, 	Step: 5645, 	{'train/accuracy': 0.591108071135431, 'train/loss': 2.132342075638395, 'train/bleu': 28.232287752538152, 'validation/accuracy': 0.6068740623178882, 'validation/loss': 2.0010837590358457, 'validation/bleu': 24.901913227485654, 'validation/num_examples': 3000, 'test/accuracy': 0.6118645052582651, 'test/loss': 1.9652197359246992, 'test/bleu': 23.62578021473685, 'test/num_examples': 3003}
I0315 19:02:27.017820 140457232099072 logging_writer.py:48] [5645] global_step=5645, preemption_count=0, score=2515.657758, test/accuracy=0.611865, test/bleu=23.625780, test/loss=1.965220, test/num_examples=3003, total_duration=4208.372835, train/accuracy=0.591108, train/bleu=28.232288, train/loss=2.132342, validation/accuracy=0.606874, validation/bleu=24.901913, validation/loss=2.001084, validation/num_examples=3000
I0315 19:02:29.145984 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_5645.
I0315 19:05:07.992168 140457223706368 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.137192, loss=2.152225
I0315 19:05:07.995884 140515296233280 pytorch_submission_base.py:86] 6000) loss = 2.152, grad_norm = 0.137
I0315 19:08:51.069608 140457232099072 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.151522, loss=2.196413
I0315 19:08:51.073617 140515296233280 pytorch_submission_base.py:86] 6500) loss = 2.196, grad_norm = 0.152
I0315 19:12:34.266530 140457223706368 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.146335, loss=2.235971
I0315 19:12:34.270568 140515296233280 pytorch_submission_base.py:86] 7000) loss = 2.236, grad_norm = 0.146
I0315 19:16:17.502641 140457232099072 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.173272, loss=2.035793
I0315 19:16:17.507257 140515296233280 pytorch_submission_base.py:86] 7500) loss = 2.036, grad_norm = 0.173
I0315 19:16:29.575504 140515296233280 spec.py:298] Evaluating on the training split.
I0315 19:16:33.458253 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:19:04.459470 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 19:19:08.190199 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:21:17.132019 140515296233280 spec.py:326] Evaluating on the test split.
I0315 19:21:20.942784 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:23:15.661307 140515296233280 submission_runner.py:362] Time since start: 5493.51s, 	Step: 7528, 	{'train/accuracy': 0.6080287542496079, 'train/loss': 1.9882593849085977, 'train/bleu': 29.40981254221714, 'validation/accuracy': 0.6207858551040905, 'validation/loss': 1.885847664629081, 'validation/bleu': 25.31446233219718, 'validation/num_examples': 3000, 'test/accuracy': 0.6274243216547557, 'test/loss': 1.837021926384289, 'test/bleu': 24.590162980484852, 'test/num_examples': 3003}
I0315 19:23:15.671656 140457223706368 logging_writer.py:48] [7528] global_step=7528, preemption_count=0, score=3353.007484, test/accuracy=0.627424, test/bleu=24.590163, test/loss=1.837022, test/num_examples=3003, total_duration=5493.510672, train/accuracy=0.608029, train/bleu=29.409813, train/loss=1.988259, validation/accuracy=0.620786, validation/bleu=25.314462, validation/loss=1.885848, validation/num_examples=3000
I0315 19:23:17.767251 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_7528.
I0315 19:26:48.333724 140457232099072 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.142214, loss=2.099610
I0315 19:26:48.337850 140515296233280 pytorch_submission_base.py:86] 8000) loss = 2.100, grad_norm = 0.142
I0315 19:30:31.351418 140457223706368 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.143647, loss=2.005998
I0315 19:30:31.355284 140515296233280 pytorch_submission_base.py:86] 8500) loss = 2.006, grad_norm = 0.144
I0315 19:34:14.418949 140457232099072 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.156332, loss=2.072218
I0315 19:34:14.423073 140515296233280 pytorch_submission_base.py:86] 9000) loss = 2.072, grad_norm = 0.156
I0315 19:37:17.820550 140515296233280 spec.py:298] Evaluating on the training split.
I0315 19:37:21.718556 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:39:50.958647 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 19:39:54.693547 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:42:12.347115 140515296233280 spec.py:326] Evaluating on the test split.
I0315 19:42:16.150689 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 19:44:33.623616 140515296233280 submission_runner.py:362] Time since start: 6741.76s, 	Step: 9412, 	{'train/accuracy': 0.6176886900141546, 'train/loss': 1.9140728448472673, 'train/bleu': 29.96804324387166, 'validation/accuracy': 0.6317466615417043, 'validation/loss': 1.8025218224200568, 'validation/bleu': 25.85691164719861, 'validation/num_examples': 3000, 'test/accuracy': 0.6390796583580268, 'test/loss': 1.7482373191563534, 'test/bleu': 25.134911808003622, 'test/num_examples': 3003}
I0315 19:44:33.635301 140457223706368 logging_writer.py:48] [9412] global_step=9412, preemption_count=0, score=4190.024913, test/accuracy=0.639080, test/bleu=25.134912, test/loss=1.748237, test/num_examples=3003, total_duration=6741.755692, train/accuracy=0.617689, train/bleu=29.968043, train/loss=1.914073, validation/accuracy=0.631747, validation/bleu=25.856912, validation/loss=1.802522, validation/num_examples=3000
I0315 19:44:35.783424 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_9412.
I0315 19:45:15.439595 140457232099072 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.161699, loss=2.068193
I0315 19:45:15.443508 140515296233280 pytorch_submission_base.py:86] 9500) loss = 2.068, grad_norm = 0.162
I0315 19:48:58.336402 140457223706368 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.186640, loss=2.003053
I0315 19:48:58.340461 140515296233280 pytorch_submission_base.py:86] 10000) loss = 2.003, grad_norm = 0.187
I0315 19:52:41.335267 140457232099072 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.146531, loss=2.006179
I0315 19:52:41.339674 140515296233280 pytorch_submission_base.py:86] 10500) loss = 2.006, grad_norm = 0.147
I0315 19:56:24.202205 140457223706368 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.170104, loss=1.997707
I0315 19:56:24.206778 140515296233280 pytorch_submission_base.py:86] 11000) loss = 1.998, grad_norm = 0.170
I0315 19:58:36.181623 140515296233280 spec.py:298] Evaluating on the training split.
I0315 19:58:40.063231 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:01:02.144500 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 20:01:05.896236 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:03:06.109143 140515296233280 spec.py:326] Evaluating on the test split.
I0315 20:03:09.922959 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:05:03.866668 140515296233280 submission_runner.py:362] Time since start: 8020.12s, 	Step: 11297, 	{'train/accuracy': 0.6205177792358785, 'train/loss': 1.9049848338340758, 'train/bleu': 30.363084981515268, 'validation/accuracy': 0.6363343293945518, 'validation/loss': 1.7627761125094543, 'validation/bleu': 26.68394796520704, 'validation/num_examples': 3000, 'test/accuracy': 0.6442159084306548, 'test/loss': 1.6984998692696531, 'test/bleu': 25.76829539435683, 'test/num_examples': 3003}
I0315 20:05:03.877793 140457232099072 logging_writer.py:48] [11297] global_step=11297, preemption_count=0, score=5027.360723, test/accuracy=0.644216, test/bleu=25.768295, test/loss=1.698500, test/num_examples=3003, total_duration=8020.116785, train/accuracy=0.620518, train/bleu=30.363085, train/loss=1.904985, validation/accuracy=0.636334, validation/bleu=26.683948, validation/loss=1.762776, validation/num_examples=3000
I0315 20:05:05.910863 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_11297.
I0315 20:06:36.648172 140457223706368 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.157897, loss=1.986807
I0315 20:06:36.652530 140515296233280 pytorch_submission_base.py:86] 11500) loss = 1.987, grad_norm = 0.158
I0315 20:10:19.649052 140457232099072 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.179148, loss=1.992498
I0315 20:10:19.653359 140515296233280 pytorch_submission_base.py:86] 12000) loss = 1.992, grad_norm = 0.179
I0315 20:14:02.714042 140457223706368 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.200720, loss=2.014943
I0315 20:14:02.718213 140515296233280 pytorch_submission_base.py:86] 12500) loss = 2.015, grad_norm = 0.201
I0315 20:17:45.818215 140457232099072 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.156095, loss=1.918169
I0315 20:17:45.821831 140515296233280 pytorch_submission_base.py:86] 13000) loss = 1.918, grad_norm = 0.156
I0315 20:19:06.062672 140515296233280 spec.py:298] Evaluating on the training split.
I0315 20:19:09.939643 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:21:30.317391 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 20:21:34.060811 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:23:39.674086 140515296233280 spec.py:326] Evaluating on the test split.
I0315 20:23:43.491105 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:25:43.928155 140515296233280 submission_runner.py:362] Time since start: 9250.00s, 	Step: 13181, 	{'train/accuracy': 0.635065084667665, 'train/loss': 1.7839318626886302, 'train/bleu': 30.303004377868557, 'validation/accuracy': 0.641492355953429, 'validation/loss': 1.7211949479857658, 'validation/bleu': 26.895621604215595, 'validation/num_examples': 3000, 'test/accuracy': 0.6506768926849108, 'test/loss': 1.6558378362675032, 'test/bleu': 25.879161306474046, 'test/num_examples': 3003}
I0315 20:25:43.941385 140457223706368 logging_writer.py:48] [13181] global_step=13181, preemption_count=0, score=5864.462736, test/accuracy=0.650677, test/bleu=25.879161, test/loss=1.655838, test/num_examples=3003, total_duration=9249.997861, train/accuracy=0.635065, train/bleu=30.303004, train/loss=1.783932, validation/accuracy=0.641492, validation/bleu=26.895622, validation/loss=1.721195, validation/num_examples=3000
I0315 20:25:46.457647 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_13181.
I0315 20:28:08.994063 140457232099072 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.149279, loss=1.944443
I0315 20:28:08.997514 140515296233280 pytorch_submission_base.py:86] 13500) loss = 1.944, grad_norm = 0.149
I0315 20:31:51.994835 140457223706368 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.181188, loss=1.935069
I0315 20:31:51.998199 140515296233280 pytorch_submission_base.py:86] 14000) loss = 1.935, grad_norm = 0.181
I0315 20:35:35.150568 140457232099072 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.153932, loss=1.919484
I0315 20:35:35.153860 140515296233280 pytorch_submission_base.py:86] 14500) loss = 1.919, grad_norm = 0.154
I0315 20:39:18.039360 140457223706368 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.157572, loss=1.845937
I0315 20:39:18.043855 140515296233280 pytorch_submission_base.py:86] 15000) loss = 1.846, grad_norm = 0.158
I0315 20:39:46.550794 140515296233280 spec.py:298] Evaluating on the training split.
I0315 20:39:50.470192 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:42:10.961690 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 20:42:14.707114 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:44:16.606113 140515296233280 spec.py:326] Evaluating on the test split.
I0315 20:44:20.431780 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 20:46:11.469050 140515296233280 submission_runner.py:362] Time since start: 10490.49s, 	Step: 15065, 	{'train/accuracy': 0.6277439860971371, 'train/loss': 1.826714955581725, 'train/bleu': 30.243204632185584, 'validation/accuracy': 0.6455716606117717, 'validation/loss': 1.6961295814683017, 'validation/bleu': 27.18406075423392, 'validation/num_examples': 3000, 'test/accuracy': 0.6541630352681425, 'test/loss': 1.6337200772761606, 'test/bleu': 26.15211267921952, 'test/num_examples': 3003}
I0315 20:46:11.481370 140457232099072 logging_writer.py:48] [15065] global_step=15065, preemption_count=0, score=6701.520494, test/accuracy=0.654163, test/bleu=26.152113, test/loss=1.633720, test/num_examples=3003, total_duration=10490.485874, train/accuracy=0.627744, train/bleu=30.243205, train/loss=1.826715, validation/accuracy=0.645572, validation/bleu=27.184061, validation/loss=1.696130, validation/num_examples=3000
I0315 20:46:14.007143 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_15065.
I0315 20:49:28.450735 140457223706368 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.177655, loss=1.893666
I0315 20:49:28.454501 140515296233280 pytorch_submission_base.py:86] 15500) loss = 1.894, grad_norm = 0.178
I0315 20:53:11.695923 140457232099072 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.172862, loss=1.905656
I0315 20:53:11.699810 140515296233280 pytorch_submission_base.py:86] 16000) loss = 1.906, grad_norm = 0.173
I0315 20:56:55.050544 140457223706368 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.155067, loss=1.874537
I0315 20:56:55.053992 140515296233280 pytorch_submission_base.py:86] 16500) loss = 1.875, grad_norm = 0.155
I0315 21:00:14.299001 140515296233280 spec.py:298] Evaluating on the training split.
I0315 21:00:18.184889 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:03:31.325679 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 21:03:35.059000 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:05:38.737851 140515296233280 spec.py:326] Evaluating on the test split.
I0315 21:05:42.549205 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:07:43.616893 140515296233280 submission_runner.py:362] Time since start: 11718.23s, 	Step: 16948, 	{'train/accuracy': 0.633547459580343, 'train/loss': 1.7974581541024384, 'train/bleu': 30.368111584640875, 'validation/accuracy': 0.6489566155410348, 'validation/loss': 1.6712073858352656, 'validation/bleu': 27.385469398600947, 'validation/num_examples': 3000, 'test/accuracy': 0.65748649119749, 'test/loss': 1.606301928998896, 'test/bleu': 26.4282240866204, 'test/num_examples': 3003}
I0315 21:07:43.628687 140457232099072 logging_writer.py:48] [16948] global_step=16948, preemption_count=0, score=7538.793017, test/accuracy=0.657486, test/bleu=26.428224, test/loss=1.606302, test/num_examples=3003, total_duration=11718.234112, train/accuracy=0.633547, train/bleu=30.368112, train/loss=1.797458, validation/accuracy=0.648957, validation/bleu=27.385469, validation/loss=1.671207, validation/num_examples=3000
I0315 21:07:45.905441 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_16948.
I0315 21:08:09.556742 140457223706368 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.162320, loss=1.955305
I0315 21:08:09.559660 140515296233280 pytorch_submission_base.py:86] 17000) loss = 1.955, grad_norm = 0.162
I0315 21:11:52.397741 140457232099072 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.202375, loss=1.929073
I0315 21:11:52.401265 140515296233280 pytorch_submission_base.py:86] 17500) loss = 1.929, grad_norm = 0.202
I0315 21:15:35.381357 140457223706368 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.177807, loss=1.910795
I0315 21:15:35.384915 140515296233280 pytorch_submission_base.py:86] 18000) loss = 1.911, grad_norm = 0.178
I0315 21:19:18.589981 140457232099072 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.174883, loss=1.941508
I0315 21:19:18.593802 140515296233280 pytorch_submission_base.py:86] 18500) loss = 1.942, grad_norm = 0.175
I0315 21:21:46.028430 140515296233280 spec.py:298] Evaluating on the training split.
I0315 21:21:49.914135 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:24:26.976938 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 21:24:30.716933 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:26:30.591494 140515296233280 spec.py:326] Evaluating on the test split.
I0315 21:26:34.410091 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:28:27.077308 140515296233280 submission_runner.py:362] Time since start: 13009.96s, 	Step: 18832, 	{'train/accuracy': 0.6640485950830779, 'train/loss': 1.5656629649086546, 'train/bleu': 32.90391735508681, 'validation/accuracy': 0.6498865482139093, 'validation/loss': 1.65164179613396, 'validation/bleu': 27.249080942212753, 'validation/num_examples': 3000, 'test/accuracy': 0.65939224914299, 'test/loss': 1.5874751249201093, 'test/bleu': 26.234322949103575, 'test/num_examples': 3003}
I0315 21:28:27.088478 140457223706368 logging_writer.py:48] [18832] global_step=18832, preemption_count=0, score=8375.910771, test/accuracy=0.659392, test/bleu=26.234323, test/loss=1.587475, test/num_examples=3003, total_duration=13009.963544, train/accuracy=0.664049, train/bleu=32.903917, train/loss=1.565663, validation/accuracy=0.649887, validation/bleu=27.249081, validation/loss=1.651642, validation/num_examples=3000
I0315 21:28:29.364330 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_18832.
I0315 21:29:44.470824 140457232099072 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.239033, loss=1.895193
I0315 21:29:44.474480 140515296233280 pytorch_submission_base.py:86] 19000) loss = 1.895, grad_norm = 0.239
I0315 21:33:27.361191 140457223706368 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.183219, loss=1.878467
I0315 21:33:27.364946 140515296233280 pytorch_submission_base.py:86] 19500) loss = 1.878, grad_norm = 0.183
I0315 21:37:10.576520 140457232099072 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.194205, loss=1.881609
I0315 21:37:10.579860 140515296233280 pytorch_submission_base.py:86] 20000) loss = 1.882, grad_norm = 0.194
I0315 21:40:53.504201 140457223706368 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.195608, loss=1.903813
I0315 21:40:53.507454 140515296233280 pytorch_submission_base.py:86] 20500) loss = 1.904, grad_norm = 0.196
I0315 21:42:29.421681 140515296233280 spec.py:298] Evaluating on the training split.
I0315 21:42:33.328701 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:45:27.601266 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 21:45:31.335492 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:47:45.668800 140515296233280 spec.py:326] Evaluating on the test split.
I0315 21:47:49.489867 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 21:49:56.245873 140515296233280 submission_runner.py:362] Time since start: 14253.36s, 	Step: 20716, 	{'train/accuracy': 0.6326977465559712, 'train/loss': 1.7846968798467293, 'train/bleu': 31.069751621826267, 'validation/accuracy': 0.652329171367993, 'validation/loss': 1.6372586437241943, 'validation/bleu': 27.71484554816888, 'validation/num_examples': 3000, 'test/accuracy': 0.6624716750915113, 'test/loss': 1.5725963773168323, 'test/bleu': 26.766986118637664, 'test/num_examples': 3003}
I0315 21:49:56.259508 140457232099072 logging_writer.py:48] [20716] global_step=20716, preemption_count=0, score=9212.968909, test/accuracy=0.662472, test/bleu=26.766986, test/loss=1.572596, test/num_examples=3003, total_duration=14253.356784, train/accuracy=0.632698, train/bleu=31.069752, train/loss=1.784697, validation/accuracy=0.652329, validation/bleu=27.714846, validation/loss=1.637259, validation/num_examples=3000
I0315 21:49:58.799674 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_20716.
I0315 21:52:05.591763 140457223706368 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.244491, loss=1.823557
I0315 21:52:05.595774 140515296233280 pytorch_submission_base.py:86] 21000) loss = 1.824, grad_norm = 0.244
I0315 21:55:48.360301 140457232099072 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.266931, loss=1.835734
I0315 21:55:48.363554 140515296233280 pytorch_submission_base.py:86] 21500) loss = 1.836, grad_norm = 0.267
I0315 21:59:30.955798 140457223706368 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.194607, loss=1.863601
I0315 21:59:30.959124 140515296233280 pytorch_submission_base.py:86] 22000) loss = 1.864, grad_norm = 0.195
I0315 22:03:13.925332 140457232099072 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.170934, loss=1.851807
I0315 22:03:13.929417 140515296233280 pytorch_submission_base.py:86] 22500) loss = 1.852, grad_norm = 0.171
I0315 22:03:58.984991 140515296233280 spec.py:298] Evaluating on the training split.
I0315 22:04:02.874439 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:08:07.195995 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 22:08:10.938625 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:10:29.765815 140515296233280 spec.py:326] Evaluating on the test split.
I0315 22:10:33.564420 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:12:56.541584 140515296233280 submission_runner.py:362] Time since start: 15542.92s, 	Step: 22602, 	{'train/accuracy': 0.6362604608506248, 'train/loss': 1.7624987461309183, 'train/bleu': 30.81292269442534, 'validation/accuracy': 0.6548709873405165, 'validation/loss': 1.6265328390224547, 'validation/bleu': 27.360154841242807, 'validation/num_examples': 3000, 'test/accuracy': 0.6649584568008832, 'test/loss': 1.5498051754691766, 'test/bleu': 27.05637108543675, 'test/num_examples': 3003}
I0315 22:12:56.553542 140457223706368 logging_writer.py:48] [22602] global_step=22602, preemption_count=0, score=10050.139518, test/accuracy=0.664958, test/bleu=27.056371, test/loss=1.549805, test/num_examples=3003, total_duration=15542.920112, train/accuracy=0.636260, train/bleu=30.812923, train/loss=1.762499, validation/accuracy=0.654871, validation/bleu=27.360155, validation/loss=1.626533, validation/num_examples=3000
I0315 22:12:58.820306 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_22602.
I0315 22:15:56.628308 140457232099072 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.165097, loss=1.922959
I0315 22:15:56.632430 140515296233280 pytorch_submission_base.py:86] 23000) loss = 1.923, grad_norm = 0.165
I0315 22:19:40.236299 140457223706368 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.268836, loss=1.839570
I0315 22:19:40.239626 140515296233280 pytorch_submission_base.py:86] 23500) loss = 1.840, grad_norm = 0.269
I0315 22:23:23.344350 140457232099072 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.187510, loss=1.860583
I0315 22:23:23.347928 140515296233280 pytorch_submission_base.py:86] 24000) loss = 1.861, grad_norm = 0.188
I0315 22:26:58.873077 140515296233280 spec.py:298] Evaluating on the training split.
I0315 22:27:02.769487 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:30:17.879787 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 22:30:21.617775 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:33:40.922973 140515296233280 spec.py:326] Evaluating on the test split.
I0315 22:33:44.735001 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:36:32.070784 140515296233280 submission_runner.py:362] Time since start: 16922.81s, 	Step: 24484, 	{'train/accuracy': 0.639632164834405, 'train/loss': 1.743467166217244, 'train/bleu': 30.945332085897736, 'validation/accuracy': 0.6566936553793505, 'validation/loss': 1.6154009637202267, 'validation/bleu': 27.60159964228423, 'validation/num_examples': 3000, 'test/accuracy': 0.6682819127302306, 'test/loss': 1.5415887731683227, 'test/bleu': 27.04761601112965, 'test/num_examples': 3003}
I0315 22:36:32.081293 140457223706368 logging_writer.py:48] [24484] global_step=24484, preemption_count=0, score=10887.133499, test/accuracy=0.668282, test/bleu=27.047616, test/loss=1.541589, test/num_examples=3003, total_duration=16922.808219, train/accuracy=0.639632, train/bleu=30.945332, train/loss=1.743467, validation/accuracy=0.656694, validation/bleu=27.601600, validation/loss=1.615401, validation/num_examples=3000
I0315 22:36:34.329997 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_24484.
I0315 22:36:41.904256 140457232099072 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.316115, loss=1.725243
I0315 22:36:41.907189 140515296233280 pytorch_submission_base.py:86] 24500) loss = 1.725, grad_norm = 0.316
I0315 22:40:24.674396 140457223706368 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.250451, loss=1.763515
I0315 22:40:24.678559 140515296233280 pytorch_submission_base.py:86] 25000) loss = 1.764, grad_norm = 0.250
I0315 22:44:07.654417 140457232099072 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.178812, loss=1.716897
I0315 22:44:07.658112 140515296233280 pytorch_submission_base.py:86] 25500) loss = 1.717, grad_norm = 0.179
I0315 22:47:50.702494 140457223706368 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.167281, loss=1.779236
I0315 22:47:50.706128 140515296233280 pytorch_submission_base.py:86] 26000) loss = 1.779, grad_norm = 0.167
I0315 22:50:34.419938 140515296233280 spec.py:298] Evaluating on the training split.
I0315 22:50:38.317070 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:54:30.107445 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 22:54:33.828994 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:56:39.024969 140515296233280 spec.py:326] Evaluating on the test split.
I0315 22:56:42.843624 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 22:58:44.950437 140515296233280 submission_runner.py:362] Time since start: 18338.36s, 	Step: 26368, 	{'train/accuracy': 0.6435315422024305, 'train/loss': 1.7101494254182659, 'train/bleu': 31.84420835723053, 'validation/accuracy': 0.6583675341905246, 'validation/loss': 1.6013241078845892, 'validation/bleu': 27.97728314014912, 'validation/num_examples': 3000, 'test/accuracy': 0.6686770088896636, 'test/loss': 1.5315568531752948, 'test/bleu': 27.326146927344798, 'test/num_examples': 3003}
I0315 22:58:44.962072 140457232099072 logging_writer.py:48] [26368] global_step=26368, preemption_count=0, score=11724.204772, test/accuracy=0.668677, test/bleu=27.326147, test/loss=1.531557, test/num_examples=3003, total_duration=18338.355080, train/accuracy=0.643532, train/bleu=31.844208, train/loss=1.710149, validation/accuracy=0.658368, validation/bleu=27.977283, validation/loss=1.601324, validation/num_examples=3000
I0315 22:58:47.192260 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_26368.
I0315 22:59:46.364915 140457223706368 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.236307, loss=1.801984
I0315 22:59:46.369042 140515296233280 pytorch_submission_base.py:86] 26500) loss = 1.802, grad_norm = 0.236
I0315 23:03:29.081817 140457232099072 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.178546, loss=1.759521
I0315 23:03:29.085294 140515296233280 pytorch_submission_base.py:86] 27000) loss = 1.760, grad_norm = 0.179
I0315 23:07:11.868612 140457223706368 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.181986, loss=1.823702
I0315 23:07:11.871894 140515296233280 pytorch_submission_base.py:86] 27500) loss = 1.824, grad_norm = 0.182
I0315 23:10:54.600922 140457232099072 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.230810, loss=1.874865
I0315 23:10:54.604130 140515296233280 pytorch_submission_base.py:86] 28000) loss = 1.875, grad_norm = 0.231
I0315 23:12:47.314621 140515296233280 spec.py:298] Evaluating on the training split.
I0315 23:12:51.186115 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:16:13.053918 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 23:16:16.791758 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:18:20.102273 140515296233280 spec.py:326] Evaluating on the test split.
I0315 23:18:23.940233 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:20:29.494213 140515296233280 submission_runner.py:362] Time since start: 19671.25s, 	Step: 28254, 	{'train/accuracy': 0.641522881151357, 'train/loss': 1.7329014618992333, 'train/bleu': 31.15583572630349, 'validation/accuracy': 0.6602893950477985, 'validation/loss': 1.5915061499547434, 'validation/bleu': 28.33024579429472, 'validation/num_examples': 3000, 'test/accuracy': 0.6723258381267794, 'test/loss': 1.5165065219917495, 'test/bleu': 27.894723976455527, 'test/num_examples': 3003}
I0315 23:20:29.505302 140457223706368 logging_writer.py:48] [28254] global_step=28254, preemption_count=0, score=12561.351468, test/accuracy=0.672326, test/bleu=27.894724, test/loss=1.516507, test/num_examples=3003, total_duration=19671.249791, train/accuracy=0.641523, train/bleu=31.155836, train/loss=1.732901, validation/accuracy=0.660289, validation/bleu=28.330246, validation/loss=1.591506, validation/num_examples=3000
I0315 23:20:31.739183 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_28254.
I0315 23:22:21.777223 140457232099072 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.241643, loss=1.843072
I0315 23:22:21.780710 140515296233280 pytorch_submission_base.py:86] 28500) loss = 1.843, grad_norm = 0.242
I0315 23:26:04.345413 140457223706368 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.207014, loss=1.837481
I0315 23:26:04.348768 140515296233280 pytorch_submission_base.py:86] 29000) loss = 1.837, grad_norm = 0.207
I0315 23:29:47.009450 140457232099072 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.210595, loss=1.811209
I0315 23:29:47.012789 140515296233280 pytorch_submission_base.py:86] 29500) loss = 1.811, grad_norm = 0.211
I0315 23:33:29.831889 140457223706368 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.227511, loss=1.755954
I0315 23:33:29.835382 140515296233280 pytorch_submission_base.py:86] 30000) loss = 1.756, grad_norm = 0.228
I0315 23:34:31.748975 140515296233280 spec.py:298] Evaluating on the training split.
I0315 23:34:35.640490 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:37:38.808059 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 23:37:42.564013 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:40:03.773951 140515296233280 spec.py:326] Evaluating on the test split.
I0315 23:40:07.583141 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:42:04.986210 140515296233280 submission_runner.py:362] Time since start: 20975.68s, 	Step: 30140, 	{'train/accuracy': 0.6369845715860042, 'train/loss': 1.7452464212737626, 'train/bleu': 31.45834537771507, 'validation/accuracy': 0.6611821304137581, 'validation/loss': 1.591758975400181, 'validation/bleu': 28.20256328892751, 'validation/num_examples': 3000, 'test/accuracy': 0.670989483469874, 'test/loss': 1.5102646926384289, 'test/bleu': 27.383034951732817, 'test/num_examples': 3003}
I0315 23:42:04.997421 140457232099072 logging_writer.py:48] [30140] global_step=30140, preemption_count=0, score=13398.354297, test/accuracy=0.670989, test/bleu=27.383035, test/loss=1.510265, test/num_examples=3003, total_duration=20975.684143, train/accuracy=0.636985, train/bleu=31.458345, train/loss=1.745246, validation/accuracy=0.661182, validation/bleu=28.202563, validation/loss=1.591759, validation/num_examples=3000
I0315 23:42:07.180323 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_30140.
I0315 23:44:47.930146 140457223706368 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.174524, loss=1.824997
I0315 23:44:47.933669 140515296233280 pytorch_submission_base.py:86] 30500) loss = 1.825, grad_norm = 0.175
I0315 23:48:30.629627 140457232099072 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.165551, loss=1.799812
I0315 23:48:30.632797 140515296233280 pytorch_submission_base.py:86] 31000) loss = 1.800, grad_norm = 0.166
I0315 23:52:13.673048 140457223706368 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.232913, loss=1.824005
I0315 23:52:13.676606 140515296233280 pytorch_submission_base.py:86] 31500) loss = 1.824, grad_norm = 0.233
I0315 23:55:56.633016 140457232099072 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.185246, loss=1.791426
I0315 23:55:56.636228 140515296233280 pytorch_submission_base.py:86] 32000) loss = 1.791, grad_norm = 0.185
I0315 23:56:07.332446 140515296233280 spec.py:298] Evaluating on the training split.
I0315 23:56:11.204616 140515296233280 workload.py:130] Translating evaluation dataset.
I0315 23:59:16.987048 140515296233280 spec.py:310] Evaluating on the validation split.
I0315 23:59:20.727260 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:01:41.331321 140515296233280 spec.py:326] Evaluating on the test split.
I0316 00:01:45.151307 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:03:50.923934 140515296233280 submission_runner.py:362] Time since start: 22271.27s, 	Step: 32025, 	{'train/accuracy': 0.6486923050486304, 'train/loss': 1.6533754553734061, 'train/bleu': 31.824115465390307, 'validation/accuracy': 0.6633271751125218, 'validation/loss': 1.5779239020594908, 'validation/bleu': 28.419839896241655, 'validation/num_examples': 3000, 'test/accuracy': 0.6742664574981116, 'test/loss': 1.5012134318168613, 'test/bleu': 27.903679986035026, 'test/num_examples': 3003}
I0316 00:03:50.935319 140457223706368 logging_writer.py:48] [32025] global_step=32025, preemption_count=0, score=14235.516757, test/accuracy=0.674266, test/bleu=27.903680, test/loss=1.501213, test/num_examples=3003, total_duration=22271.267604, train/accuracy=0.648692, train/bleu=31.824115, train/loss=1.653375, validation/accuracy=0.663327, validation/bleu=28.419840, validation/loss=1.577924, validation/num_examples=3000
I0316 00:03:53.162724 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_32025.
I0316 00:07:25.156433 140457232099072 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.222577, loss=1.720929
I0316 00:07:25.159880 140515296233280 pytorch_submission_base.py:86] 32500) loss = 1.721, grad_norm = 0.223
I0316 00:11:08.004190 140457223706368 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.287558, loss=1.787466
I0316 00:11:08.008271 140515296233280 pytorch_submission_base.py:86] 33000) loss = 1.787, grad_norm = 0.288
I0316 00:14:50.916632 140457232099072 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.190899, loss=1.840627
I0316 00:14:50.921340 140515296233280 pytorch_submission_base.py:86] 33500) loss = 1.841, grad_norm = 0.191
I0316 00:17:53.491230 140515296233280 spec.py:298] Evaluating on the training split.
I0316 00:17:57.396255 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:22:07.490843 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 00:22:11.247978 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:25:12.993813 140515296233280 spec.py:326] Evaluating on the test split.
I0316 00:25:16.811691 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:28:19.238168 140515296233280 submission_runner.py:362] Time since start: 23577.43s, 	Step: 33911, 	{'train/accuracy': 0.6416286779941981, 'train/loss': 1.7091760271331216, 'train/bleu': 31.39044707695118, 'validation/accuracy': 0.6643810988084463, 'validation/loss': 1.5696378144722323, 'validation/bleu': 28.72874202223925, 'validation/num_examples': 3000, 'test/accuracy': 0.6757073964325141, 'test/loss': 1.4945248496601011, 'test/bleu': 27.844046875534083, 'test/num_examples': 3003}
I0316 00:28:19.250350 140457223706368 logging_writer.py:48] [33911] global_step=33911, preemption_count=0, score=15072.860785, test/accuracy=0.675707, test/bleu=27.844047, test/loss=1.494525, test/num_examples=3003, total_duration=23577.426116, train/accuracy=0.641629, train/bleu=31.390447, train/loss=1.709176, validation/accuracy=0.664381, validation/bleu=28.728742, validation/loss=1.569638, validation/num_examples=3000
I0316 00:28:21.444891 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_33911.
I0316 00:29:01.392600 140457232099072 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.206370, loss=1.762495
I0316 00:29:01.395591 140515296233280 pytorch_submission_base.py:86] 34000) loss = 1.762, grad_norm = 0.206
I0316 00:32:44.143127 140457223706368 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.176054, loss=1.774091
I0316 00:32:44.146520 140515296233280 pytorch_submission_base.py:86] 34500) loss = 1.774, grad_norm = 0.176
I0316 00:36:27.371922 140457232099072 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.192867, loss=1.719565
I0316 00:36:27.375278 140515296233280 pytorch_submission_base.py:86] 35000) loss = 1.720, grad_norm = 0.193
I0316 00:40:10.257594 140457223706368 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.199702, loss=1.757296
I0316 00:40:10.261544 140515296233280 pytorch_submission_base.py:86] 35500) loss = 1.757, grad_norm = 0.200
I0316 00:42:21.550011 140515296233280 spec.py:298] Evaluating on the training split.
I0316 00:42:25.427315 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:46:21.964415 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 00:46:25.693331 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:49:35.426807 140515296233280 spec.py:326] Evaluating on the test split.
I0316 00:49:39.234575 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 00:52:53.176979 140515296233280 submission_runner.py:362] Time since start: 25045.49s, 	Step: 35796, 	{'train/accuracy': 0.6428457743116346, 'train/loss': 1.712694117914554, 'train/bleu': 31.421648340938546, 'validation/accuracy': 0.664703475468376, 'validation/loss': 1.56033064143656, 'validation/bleu': 28.29975325954969, 'validation/num_examples': 3000, 'test/accuracy': 0.6755679507291849, 'test/loss': 1.487943666114694, 'test/bleu': 27.785051570551563, 'test/num_examples': 3003}
I0316 00:52:53.190459 140457232099072 logging_writer.py:48] [35796] global_step=35796, preemption_count=0, score=15909.992571, test/accuracy=0.675568, test/bleu=27.785052, test/loss=1.487944, test/num_examples=3003, total_duration=25045.485096, train/accuracy=0.642846, train/bleu=31.421648, train/loss=1.712694, validation/accuracy=0.664703, validation/bleu=28.299753, validation/loss=1.560331, validation/num_examples=3000
I0316 00:52:55.637380 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_35796.
I0316 00:54:26.935873 140457223706368 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.289721, loss=1.739168
I0316 00:54:26.940334 140515296233280 pytorch_submission_base.py:86] 36000) loss = 1.739, grad_norm = 0.290
I0316 00:58:09.827487 140457232099072 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.207855, loss=1.741457
I0316 00:58:09.831292 140515296233280 pytorch_submission_base.py:86] 36500) loss = 1.741, grad_norm = 0.208
I0316 01:01:52.764317 140457223706368 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.219503, loss=1.747483
I0316 01:01:52.768115 140515296233280 pytorch_submission_base.py:86] 37000) loss = 1.747, grad_norm = 0.220
I0316 01:05:35.664010 140457232099072 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.221916, loss=1.722049
I0316 01:05:35.667670 140515296233280 pytorch_submission_base.py:86] 37500) loss = 1.722, grad_norm = 0.222
I0316 01:06:55.952996 140515296233280 spec.py:298] Evaluating on the training split.
I0316 01:06:59.830492 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:09:55.598172 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 01:09:59.337270 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:12:01.453784 140515296233280 spec.py:326] Evaluating on the test split.
I0316 01:12:05.291776 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:14:11.720250 140515296233280 submission_runner.py:362] Time since start: 26519.89s, 	Step: 37681, 	{'train/accuracy': 0.671919935580352, 'train/loss': 1.5098912199470838, 'train/bleu': 33.61026640561844, 'validation/accuracy': 0.6660921749265353, 'validation/loss': 1.5491124567581307, 'validation/bleu': 28.443428893862272, 'validation/num_examples': 3000, 'test/accuracy': 0.6765208297019348, 'test/loss': 1.474751793910871, 'test/bleu': 27.80606736956398, 'test/num_examples': 3003}
I0316 01:14:11.730843 140457223706368 logging_writer.py:48] [37681] global_step=37681, preemption_count=0, score=16747.352998, test/accuracy=0.676521, test/bleu=27.806067, test/loss=1.474752, test/num_examples=3003, total_duration=26519.888184, train/accuracy=0.671920, train/bleu=33.610266, train/loss=1.509891, validation/accuracy=0.666092, validation/bleu=28.443429, validation/loss=1.549112, validation/num_examples=3000
I0316 01:14:13.994216 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_37681.
I0316 01:16:36.417155 140457232099072 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.243170, loss=1.757997
I0316 01:16:36.420549 140515296233280 pytorch_submission_base.py:86] 38000) loss = 1.758, grad_norm = 0.243
I0316 01:20:19.268467 140457223706368 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.200451, loss=1.816128
I0316 01:20:19.272400 140515296233280 pytorch_submission_base.py:86] 38500) loss = 1.816, grad_norm = 0.200
I0316 01:24:01.936902 140457232099072 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.175785, loss=1.720273
I0316 01:24:01.940875 140515296233280 pytorch_submission_base.py:86] 39000) loss = 1.720, grad_norm = 0.176
I0316 01:27:44.622715 140457223706368 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.212145, loss=1.763658
I0316 01:27:44.626309 140515296233280 pytorch_submission_base.py:86] 39500) loss = 1.764, grad_norm = 0.212
I0316 01:28:14.419206 140515296233280 spec.py:298] Evaluating on the training split.
I0316 01:28:18.288553 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:32:20.290898 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 01:32:24.044039 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:34:37.453652 140515296233280 spec.py:326] Evaluating on the test split.
I0316 01:34:41.270445 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:36:52.808058 140515296233280 submission_runner.py:362] Time since start: 27798.35s, 	Step: 39568, 	{'train/accuracy': 0.6508180193391687, 'train/loss': 1.6613638739631547, 'train/bleu': 31.339324881152066, 'validation/accuracy': 0.6661789686426702, 'validation/loss': 1.5512650571598616, 'validation/bleu': 28.764847312019285, 'validation/num_examples': 3000, 'test/accuracy': 0.6782057986171635, 'test/loss': 1.4734995061298006, 'test/bleu': 28.26857949831251, 'test/num_examples': 3003}
I0316 01:36:52.821233 140457232099072 logging_writer.py:48] [39568] global_step=39568, preemption_count=0, score=17584.782005, test/accuracy=0.678206, test/bleu=28.268579, test/loss=1.473500, test/num_examples=3003, total_duration=27798.354406, train/accuracy=0.650818, train/bleu=31.339325, train/loss=1.661364, validation/accuracy=0.666179, validation/bleu=28.764847, validation/loss=1.551265, validation/num_examples=3000
I0316 01:36:55.015429 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_39568.
I0316 01:40:07.725479 140457223706368 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.192029, loss=1.725547
I0316 01:40:07.729529 140515296233280 pytorch_submission_base.py:86] 40000) loss = 1.726, grad_norm = 0.192
I0316 01:43:50.377429 140457232099072 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.204052, loss=1.808510
I0316 01:43:50.380820 140515296233280 pytorch_submission_base.py:86] 40500) loss = 1.809, grad_norm = 0.204
I0316 01:47:33.014816 140457223706368 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.182019, loss=1.738467
I0316 01:47:33.018215 140515296233280 pytorch_submission_base.py:86] 41000) loss = 1.738, grad_norm = 0.182
I0316 01:50:55.428600 140515296233280 spec.py:298] Evaluating on the training split.
I0316 01:50:59.321081 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:54:51.338301 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 01:54:55.081707 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 01:58:44.623916 140515296233280 spec.py:326] Evaluating on the test split.
I0316 01:58:48.448718 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:02:29.691129 140515296233280 submission_runner.py:362] Time since start: 29159.36s, 	Step: 41455, 	{'train/accuracy': 0.6525691200605984, 'train/loss': 1.6488270893826538, 'train/bleu': 31.48487368474806, 'validation/accuracy': 0.6677784528400144, 'validation/loss': 1.538154362624146, 'validation/bleu': 28.775147697427983, 'validation/num_examples': 3000, 'test/accuracy': 0.6802161408401604, 'test/loss': 1.4610410856429028, 'test/bleu': 28.31390810819476, 'test/num_examples': 3003}
I0316 02:02:29.702127 140457232099072 logging_writer.py:48] [41455] global_step=41455, preemption_count=0, score=18422.202786, test/accuracy=0.680216, test/bleu=28.313908, test/loss=1.461041, test/num_examples=3003, total_duration=29159.363748, train/accuracy=0.652569, train/bleu=31.484874, train/loss=1.648827, validation/accuracy=0.667778, validation/bleu=28.775148, validation/loss=1.538154, validation/num_examples=3000
I0316 02:02:31.979406 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_41455.
I0316 02:02:52.464106 140457223706368 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.190347, loss=1.717922
I0316 02:02:52.467329 140515296233280 pytorch_submission_base.py:86] 41500) loss = 1.718, grad_norm = 0.190
I0316 02:06:35.198876 140457232099072 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.192083, loss=1.733452
I0316 02:06:35.202492 140515296233280 pytorch_submission_base.py:86] 42000) loss = 1.733, grad_norm = 0.192
I0316 02:10:17.888782 140457223706368 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.213810, loss=1.652944
I0316 02:10:17.892249 140515296233280 pytorch_submission_base.py:86] 42500) loss = 1.653, grad_norm = 0.214
I0316 02:14:00.634563 140457232099072 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.195264, loss=1.764563
I0316 02:14:00.639037 140515296233280 pytorch_submission_base.py:86] 43000) loss = 1.765, grad_norm = 0.195
I0316 02:16:32.221048 140515296233280 spec.py:298] Evaluating on the training split.
I0316 02:16:36.129436 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:19:40.011892 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 02:19:43.741975 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:21:51.186266 140515296233280 spec.py:326] Evaluating on the test split.
I0316 02:21:55.001673 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:24:04.852999 140515296233280 submission_runner.py:362] Time since start: 30696.16s, 	Step: 43341, 	{'train/accuracy': 0.6498793040626708, 'train/loss': 1.6697952011978503, 'train/bleu': 31.682639234649624, 'validation/accuracy': 0.6696631163903733, 'validation/loss': 1.5334567496063285, 'validation/bleu': 28.857259466198116, 'validation/num_examples': 3000, 'test/accuracy': 0.6799837313346115, 'test/loss': 1.4507416223636047, 'test/bleu': 28.296255170102853, 'test/num_examples': 3003}
I0316 02:24:04.863370 140457223706368 logging_writer.py:48] [43341] global_step=43341, preemption_count=0, score=19259.466382, test/accuracy=0.679984, test/bleu=28.296255, test/loss=1.450742, test/num_examples=3003, total_duration=30696.156211, train/accuracy=0.649879, train/bleu=31.682639, train/loss=1.669795, validation/accuracy=0.669663, validation/bleu=28.857259, validation/loss=1.533457, validation/num_examples=3000
I0316 02:24:07.045328 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_43341.
I0316 02:25:18.242360 140457232099072 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.188746, loss=1.764259
I0316 02:25:18.246298 140515296233280 pytorch_submission_base.py:86] 43500) loss = 1.764, grad_norm = 0.189
I0316 02:29:00.862605 140457223706368 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.235846, loss=1.774064
I0316 02:29:00.865965 140515296233280 pytorch_submission_base.py:86] 44000) loss = 1.774, grad_norm = 0.236
I0316 02:32:43.584476 140457232099072 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.188611, loss=1.862029
I0316 02:32:43.587712 140515296233280 pytorch_submission_base.py:86] 44500) loss = 1.862, grad_norm = 0.189
I0316 02:36:26.507745 140457223706368 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.213508, loss=1.729389
I0316 02:36:26.511165 140515296233280 pytorch_submission_base.py:86] 45000) loss = 1.729, grad_norm = 0.214
I0316 02:38:07.254482 140515296233280 spec.py:298] Evaluating on the training split.
I0316 02:38:11.143885 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:41:41.276795 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 02:41:45.021379 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:43:51.822710 140515296233280 spec.py:326] Evaluating on the test split.
I0316 02:43:55.638866 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 02:45:56.808654 140515296233280 submission_runner.py:362] Time since start: 31991.19s, 	Step: 45227, 	{'train/accuracy': 0.6576458073592826, 'train/loss': 1.6054222379814016, 'train/bleu': 32.688935887767606, 'validation/accuracy': 0.6708162329047377, 'validation/loss': 1.5217309766772886, 'validation/bleu': 29.02580262527768, 'validation/num_examples': 3000, 'test/accuracy': 0.6822032421126024, 'test/loss': 1.4457235743129393, 'test/bleu': 28.438519967009317, 'test/num_examples': 3003}
I0316 02:45:56.820599 140457232099072 logging_writer.py:48] [45227] global_step=45227, preemption_count=0, score=20096.693793, test/accuracy=0.682203, test/bleu=28.438520, test/loss=1.445724, test/num_examples=3003, total_duration=31991.189598, train/accuracy=0.657646, train/bleu=32.688936, train/loss=1.605422, validation/accuracy=0.670816, validation/bleu=29.025803, validation/loss=1.521731, validation/num_examples=3000
I0316 02:45:59.024099 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_45227.
I0316 02:48:01.058584 140457223706368 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.181691, loss=1.696334
I0316 02:48:01.062109 140515296233280 pytorch_submission_base.py:86] 45500) loss = 1.696, grad_norm = 0.182
I0316 02:51:43.715772 140457232099072 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.183772, loss=1.706267
I0316 02:51:43.719166 140515296233280 pytorch_submission_base.py:86] 46000) loss = 1.706, grad_norm = 0.184
I0316 02:55:26.634255 140457223706368 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.188806, loss=1.722493
I0316 02:55:26.639074 140515296233280 pytorch_submission_base.py:86] 46500) loss = 1.722, grad_norm = 0.189
I0316 02:59:09.157593 140457232099072 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.178708, loss=1.663820
I0316 02:59:09.161243 140515296233280 pytorch_submission_base.py:86] 47000) loss = 1.664, grad_norm = 0.179
I0316 02:59:59.346358 140515296233280 spec.py:298] Evaluating on the training split.
I0316 03:00:03.236013 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:03:39.799702 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 03:03:43.540322 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:06:07.163650 140515296233280 spec.py:326] Evaluating on the test split.
I0316 03:06:10.969500 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:08:43.050482 140515296233280 submission_runner.py:362] Time since start: 33303.28s, 	Step: 47114, 	{'train/accuracy': 0.6555906334461378, 'train/loss': 1.6351791929750845, 'train/bleu': 31.780830959813994, 'validation/accuracy': 0.6721429368513719, 'validation/loss': 1.51631605621753, 'validation/bleu': 28.986638466231803, 'validation/num_examples': 3000, 'test/accuracy': 0.6838417291267213, 'test/loss': 1.4315676565859043, 'test/bleu': 28.45079716450238, 'test/num_examples': 3003}
I0316 03:08:43.063263 140457223706368 logging_writer.py:48] [47114] global_step=47114, preemption_count=0, score=20934.003564, test/accuracy=0.683842, test/bleu=28.450797, test/loss=1.431568, test/num_examples=3003, total_duration=33303.281521, train/accuracy=0.655591, train/bleu=31.780831, train/loss=1.635179, validation/accuracy=0.672143, validation/bleu=28.986638, validation/loss=1.516316, validation/num_examples=3000
I0316 03:08:45.254100 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_47114.
I0316 03:11:37.730440 140457232099072 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.188467, loss=1.683476
I0316 03:11:37.733769 140515296233280 pytorch_submission_base.py:86] 47500) loss = 1.683, grad_norm = 0.188
I0316 03:15:20.491445 140457223706368 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.175122, loss=1.682854
I0316 03:15:20.494965 140515296233280 pytorch_submission_base.py:86] 48000) loss = 1.683, grad_norm = 0.175
I0316 03:19:03.252967 140457232099072 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.183068, loss=1.641723
I0316 03:19:03.256516 140515296233280 pytorch_submission_base.py:86] 48500) loss = 1.642, grad_norm = 0.183
I0316 03:22:45.671769 140515296233280 spec.py:298] Evaluating on the training split.
I0316 03:22:49.561332 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:26:28.865292 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 03:26:32.612256 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:28:49.658556 140515296233280 spec.py:326] Evaluating on the test split.
I0316 03:28:53.477227 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:31:07.900353 140515296233280 submission_runner.py:362] Time since start: 34669.61s, 	Step: 49000, 	{'train/accuracy': 0.6520108403563138, 'train/loss': 1.6498292615293135, 'train/bleu': 32.037927753609026, 'validation/accuracy': 0.6714733853269024, 'validation/loss': 1.5145914185812948, 'validation/bleu': 28.814124929990705, 'validation/num_examples': 3000, 'test/accuracy': 0.6838998315031085, 'test/loss': 1.4304014329498576, 'test/bleu': 28.539512504185197, 'test/num_examples': 3003}
I0316 03:31:07.913239 140457223706368 logging_writer.py:48] [49000] global_step=49000, preemption_count=0, score=21771.408381, test/accuracy=0.683900, test/bleu=28.539513, test/loss=1.430401, test/num_examples=3003, total_duration=34669.606892, train/accuracy=0.652011, train/bleu=32.037928, train/loss=1.649829, validation/accuracy=0.671473, validation/bleu=28.814125, validation/loss=1.514591, validation/num_examples=3000
I0316 03:31:10.092660 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_49000.
I0316 03:31:10.546197 140457232099072 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.200063, loss=1.661875
I0316 03:31:10.549327 140515296233280 pytorch_submission_base.py:86] 49000) loss = 1.662, grad_norm = 0.200
I0316 03:34:53.318723 140457223706368 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.201645, loss=1.780429
I0316 03:34:53.323467 140515296233280 pytorch_submission_base.py:86] 49500) loss = 1.780, grad_norm = 0.202
I0316 03:38:36.045359 140457232099072 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.194871, loss=1.690703
I0316 03:38:36.049957 140515296233280 pytorch_submission_base.py:86] 50000) loss = 1.691, grad_norm = 0.195
I0316 03:42:18.502545 140457223706368 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.197630, loss=1.645887
I0316 03:42:18.505924 140515296233280 pytorch_submission_base.py:86] 50500) loss = 1.646, grad_norm = 0.198
I0316 03:45:10.543940 140515296233280 spec.py:298] Evaluating on the training split.
I0316 03:45:14.420675 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:48:26.467002 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 03:48:30.212594 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:50:46.741428 140515296233280 spec.py:326] Evaluating on the test split.
I0316 03:50:50.567637 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 03:53:01.898914 140515296233280 submission_runner.py:362] Time since start: 36014.48s, 	Step: 50887, 	{'train/accuracy': 0.6649973582044979, 'train/loss': 1.5588849397555764, 'train/bleu': 33.24875537321265, 'validation/accuracy': 0.6747591474377255, 'validation/loss': 1.5004754280790071, 'validation/bleu': 29.048216384175547, 'validation/num_examples': 3000, 'test/accuracy': 0.6865376793910871, 'test/loss': 1.4172163696182674, 'test/bleu': 28.927441072849195, 'test/num_examples': 3003}
I0316 03:53:01.911731 140457232099072 logging_writer.py:48] [50887] global_step=50887, preemption_count=0, score=22608.923993, test/accuracy=0.686538, test/bleu=28.927441, test/loss=1.417216, test/num_examples=3003, total_duration=36014.479019, train/accuracy=0.664997, train/bleu=33.248755, train/loss=1.558885, validation/accuracy=0.674759, validation/bleu=29.048216, validation/loss=1.500475, validation/num_examples=3000
I0316 03:53:04.176665 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_50887.
I0316 03:53:54.938070 140457223706368 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.205123, loss=1.669587
I0316 03:53:54.941755 140515296233280 pytorch_submission_base.py:86] 51000) loss = 1.670, grad_norm = 0.205
I0316 03:57:37.702718 140457232099072 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.202729, loss=1.698491
I0316 03:57:37.706128 140515296233280 pytorch_submission_base.py:86] 51500) loss = 1.698, grad_norm = 0.203
I0316 04:01:20.381859 140457223706368 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.192523, loss=1.643688
I0316 04:01:20.385166 140515296233280 pytorch_submission_base.py:86] 52000) loss = 1.644, grad_norm = 0.193
I0316 04:05:03.104691 140457232099072 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.188235, loss=1.674455
I0316 04:05:03.108304 140515296233280 pytorch_submission_base.py:86] 52500) loss = 1.674, grad_norm = 0.188
I0316 04:07:04.308171 140515296233280 spec.py:298] Evaluating on the training split.
I0316 04:07:08.217664 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:10:46.643665 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 04:10:50.382376 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:13:32.545931 140515296233280 spec.py:326] Evaluating on the test split.
I0316 04:13:36.352928 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:16:33.927211 140515296233280 submission_runner.py:362] Time since start: 37328.24s, 	Step: 52773, 	{'train/accuracy': 0.6537821637760102, 'train/loss': 1.6232979764455182, 'train/bleu': 32.69315989250041, 'validation/accuracy': 0.6747467483354205, 'validation/loss': 1.4932340617289308, 'validation/bleu': 29.18004195542679, 'validation/num_examples': 3000, 'test/accuracy': 0.6880367207018767, 'test/loss': 1.4092450867177968, 'test/bleu': 28.81291253458141, 'test/num_examples': 3003}
I0316 04:16:33.939110 140457223706368 logging_writer.py:48] [52773] global_step=52773, preemption_count=0, score=23446.132752, test/accuracy=0.688037, test/bleu=28.812913, test/loss=1.409245, test/num_examples=3003, total_duration=37328.243356, train/accuracy=0.653782, train/bleu=32.693160, train/loss=1.623298, validation/accuracy=0.674747, validation/bleu=29.180042, validation/loss=1.493234, validation/num_examples=3000
I0316 04:16:36.159727 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_52773.
I0316 04:18:17.508863 140457232099072 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.182845, loss=1.720986
I0316 04:18:17.512943 140515296233280 pytorch_submission_base.py:86] 53000) loss = 1.721, grad_norm = 0.183
I0316 04:22:00.134492 140457223706368 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.207089, loss=1.588051
I0316 04:22:00.138347 140515296233280 pytorch_submission_base.py:86] 53500) loss = 1.588, grad_norm = 0.207
I0316 04:25:43.044371 140457232099072 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.222985, loss=1.699516
I0316 04:25:43.047557 140515296233280 pytorch_submission_base.py:86] 54000) loss = 1.700, grad_norm = 0.223
I0316 04:29:25.778990 140457223706368 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.193085, loss=1.624947
I0316 04:29:25.782771 140515296233280 pytorch_submission_base.py:86] 54500) loss = 1.625, grad_norm = 0.193
I0316 04:30:36.206812 140515296233280 spec.py:298] Evaluating on the training split.
I0316 04:30:40.113579 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:34:22.097859 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 04:34:25.836887 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:36:47.936154 140515296233280 spec.py:326] Evaluating on the test split.
I0316 04:36:51.748399 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:39:15.504473 140515296233280 submission_runner.py:362] Time since start: 38740.14s, 	Step: 54659, 	{'train/accuracy': 0.6599579659158404, 'train/loss': 1.6021398618472154, 'train/bleu': 32.783600043441396, 'validation/accuracy': 0.6782680933900386, 'validation/loss': 1.484899249544333, 'validation/bleu': 29.385626929144074, 'validation/num_examples': 3000, 'test/accuracy': 0.6904305386090291, 'test/loss': 1.397283078409157, 'test/bleu': 29.021750824657104, 'test/num_examples': 3003}
I0316 04:39:15.519041 140457232099072 logging_writer.py:48] [54659] global_step=54659, preemption_count=0, score=24283.192143, test/accuracy=0.690431, test/bleu=29.021751, test/loss=1.397283, test/num_examples=3003, total_duration=38740.142006, train/accuracy=0.659958, train/bleu=32.783600, train/loss=1.602140, validation/accuracy=0.678268, validation/bleu=29.385627, validation/loss=1.484899, validation/num_examples=3000
I0316 04:39:17.982141 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_54659.
I0316 04:41:50.000624 140457223706368 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.477540, loss=1.625634
I0316 04:41:50.004344 140515296233280 pytorch_submission_base.py:86] 55000) loss = 1.626, grad_norm = 0.478
I0316 04:45:32.635466 140457232099072 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.200171, loss=1.750955
I0316 04:45:32.639225 140515296233280 pytorch_submission_base.py:86] 55500) loss = 1.751, grad_norm = 0.200
I0316 04:49:15.330698 140457223706368 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.203269, loss=1.692427
I0316 04:49:15.334397 140515296233280 pytorch_submission_base.py:86] 56000) loss = 1.692, grad_norm = 0.203
I0316 04:52:58.115284 140457232099072 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.197412, loss=1.674559
I0316 04:52:58.118400 140515296233280 pytorch_submission_base.py:86] 56500) loss = 1.675, grad_norm = 0.197
I0316 04:53:18.206562 140515296233280 spec.py:298] Evaluating on the training split.
I0316 04:53:22.104297 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:56:09.333855 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 04:56:13.077845 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 04:58:25.977784 140515296233280 spec.py:326] Evaluating on the test split.
I0316 04:58:29.796103 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:00:33.252119 140515296233280 submission_runner.py:362] Time since start: 40102.14s, 	Step: 56546, 	{'train/accuracy': 0.6764106672770974, 'train/loss': 1.4814878533821678, 'train/bleu': 34.03051830462946, 'validation/accuracy': 0.6771149768756742, 'validation/loss': 1.4801188143978377, 'validation/bleu': 29.34683276584154, 'validation/num_examples': 3000, 'test/accuracy': 0.6911045261751205, 'test/loss': 1.3910838272035326, 'test/bleu': 29.163399445160433, 'test/num_examples': 3003}
I0316 05:00:33.265434 140457223706368 logging_writer.py:48] [56546] global_step=56546, preemption_count=0, score=25120.425232, test/accuracy=0.691105, test/bleu=29.163399, test/loss=1.391084, test/num_examples=3003, total_duration=40102.141684, train/accuracy=0.676411, train/bleu=34.030518, train/loss=1.481488, validation/accuracy=0.677115, validation/bleu=29.346833, validation/loss=1.480119, validation/num_examples=3000
I0316 05:00:35.456590 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_56546.
I0316 05:03:57.826349 140457232099072 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.190768, loss=1.703094
I0316 05:03:57.830355 140515296233280 pytorch_submission_base.py:86] 57000) loss = 1.703, grad_norm = 0.191
I0316 05:07:40.278066 140457223706368 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.196474, loss=1.717743
I0316 05:07:40.281549 140515296233280 pytorch_submission_base.py:86] 57500) loss = 1.718, grad_norm = 0.196
I0316 05:11:23.225068 140457232099072 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.968611, loss=1.703089
I0316 05:11:23.229136 140515296233280 pytorch_submission_base.py:86] 58000) loss = 1.703, grad_norm = 0.969
I0316 05:14:35.637107 140515296233280 spec.py:298] Evaluating on the training split.
I0316 05:14:39.531110 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:18:32.241934 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 05:18:35.989485 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:21:25.308461 140515296233280 spec.py:326] Evaluating on the test split.
I0316 05:21:29.112193 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:24:40.418448 140515296233280 submission_runner.py:362] Time since start: 41379.57s, 	Step: 58433, 	{'train/accuracy': 0.6656634494528431, 'train/loss': 1.549663215945388, 'train/bleu': 33.205141534789924, 'validation/accuracy': 0.6780325104462437, 'validation/loss': 1.4712561685533967, 'validation/bleu': 29.12705554202531, 'validation/num_examples': 3000, 'test/accuracy': 0.6911858695020626, 'test/loss': 1.384688752832491, 'test/bleu': 29.15487267926438, 'test/num_examples': 3003}
I0316 05:24:40.431459 140457223706368 logging_writer.py:48] [58433] global_step=58433, preemption_count=0, score=25957.606014, test/accuracy=0.691186, test/bleu=29.154873, test/loss=1.384689, test/num_examples=3003, total_duration=41379.572278, train/accuracy=0.665663, train/bleu=33.205142, train/loss=1.549663, validation/accuracy=0.678033, validation/bleu=29.127056, validation/loss=1.471256, validation/num_examples=3000
I0316 05:24:42.648093 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_58433.
I0316 05:25:12.871350 140457232099072 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.204172, loss=1.615907
I0316 05:25:12.874571 140515296233280 pytorch_submission_base.py:86] 58500) loss = 1.616, grad_norm = 0.204
I0316 05:28:55.772545 140457223706368 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.191462, loss=1.687909
I0316 05:28:55.777016 140515296233280 pytorch_submission_base.py:86] 59000) loss = 1.688, grad_norm = 0.191
I0316 05:32:38.131357 140457232099072 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.193481, loss=1.649526
I0316 05:32:38.134643 140515296233280 pytorch_submission_base.py:86] 59500) loss = 1.650, grad_norm = 0.193
I0316 05:36:20.957528 140457223706368 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.198725, loss=1.609850
I0316 05:36:20.960743 140515296233280 pytorch_submission_base.py:86] 60000) loss = 1.610, grad_norm = 0.199
I0316 05:38:42.736844 140515296233280 spec.py:298] Evaluating on the training split.
I0316 05:38:46.657175 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:42:29.193362 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 05:42:32.923771 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:45:02.931231 140515296233280 spec.py:326] Evaluating on the test split.
I0316 05:45:06.746726 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 05:47:40.102742 140515296233280 submission_runner.py:362] Time since start: 42826.67s, 	Step: 60319, 	{'train/accuracy': 0.6641775660260668, 'train/loss': 1.5701677279703141, 'train/bleu': 33.01858437448892, 'validation/accuracy': 0.6799667704058227, 'validation/loss': 1.4605778291651685, 'validation/bleu': 29.50704644527715, 'validation/num_examples': 3000, 'test/accuracy': 0.6941490906978095, 'test/loss': 1.3729910195514496, 'test/bleu': 29.397786145019566, 'test/num_examples': 3003}
I0316 05:47:40.115946 140457232099072 logging_writer.py:48] [60319] global_step=60319, preemption_count=0, score=26794.706814, test/accuracy=0.694149, test/bleu=29.397786, test/loss=1.372991, test/num_examples=3003, total_duration=42826.672015, train/accuracy=0.664178, train/bleu=33.018584, train/loss=1.570168, validation/accuracy=0.679967, validation/bleu=29.507046, validation/loss=1.460578, validation/num_examples=3000
I0316 05:47:42.295121 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_60319.
I0316 05:49:03.298773 140457223706368 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.198588, loss=1.607632
I0316 05:49:03.302325 140515296233280 pytorch_submission_base.py:86] 60500) loss = 1.608, grad_norm = 0.199
I0316 05:52:45.913674 140457232099072 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.198023, loss=1.666856
I0316 05:52:45.917101 140515296233280 pytorch_submission_base.py:86] 61000) loss = 1.667, grad_norm = 0.198
I0316 05:56:28.462752 140457223706368 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.191412, loss=1.658245
I0316 05:56:28.466252 140515296233280 pytorch_submission_base.py:86] 61500) loss = 1.658, grad_norm = 0.191
I0316 06:00:11.126032 140457232099072 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.200498, loss=1.711442
I0316 06:00:11.129973 140515296233280 pytorch_submission_base.py:86] 62000) loss = 1.711, grad_norm = 0.200
I0316 06:01:42.637737 140515296233280 spec.py:298] Evaluating on the training split.
I0316 06:01:46.534101 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:05:26.375564 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 06:05:30.114236 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:07:45.022021 140515296233280 spec.py:326] Evaluating on the test split.
I0316 06:07:48.839291 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:09:49.463908 140515296233280 submission_runner.py:362] Time since start: 44206.57s, 	Step: 62207, 	{'train/accuracy': 0.6632315347393408, 'train/loss': 1.5727017926024833, 'train/bleu': 33.26628953667495, 'validation/accuracy': 0.682868160345191, 'validation/loss': 1.4571556769289904, 'validation/bleu': 30.05583602851255, 'validation/num_examples': 3000, 'test/accuracy': 0.6960664691185869, 'test/loss': 1.362349569316135, 'test/bleu': 29.914898432391468, 'test/num_examples': 3003}
I0316 06:09:49.478134 140457223706368 logging_writer.py:48] [62207] global_step=62207, preemption_count=0, score=27632.065144, test/accuracy=0.696066, test/bleu=29.914898, test/loss=1.362350, test/num_examples=3003, total_duration=44206.572842, train/accuracy=0.663232, train/bleu=33.266290, train/loss=1.572702, validation/accuracy=0.682868, validation/bleu=30.055836, validation/loss=1.457156, validation/num_examples=3000
I0316 06:09:51.659089 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_62207.
I0316 06:12:02.628500 140457232099072 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.205005, loss=1.587740
I0316 06:12:02.632025 140515296233280 pytorch_submission_base.py:86] 62500) loss = 1.588, grad_norm = 0.205
I0316 06:15:45.135151 140457223706368 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.244407, loss=1.609072
I0316 06:15:45.138606 140515296233280 pytorch_submission_base.py:86] 63000) loss = 1.609, grad_norm = 0.244
I0316 06:19:27.998409 140457232099072 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.198645, loss=1.627471
I0316 06:19:28.001767 140515296233280 pytorch_submission_base.py:86] 63500) loss = 1.627, grad_norm = 0.199
I0316 06:23:10.921489 140457223706368 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.194515, loss=1.590035
I0316 06:23:10.926752 140515296233280 pytorch_submission_base.py:86] 64000) loss = 1.590, grad_norm = 0.195
I0316 06:23:51.984904 140515296233280 spec.py:298] Evaluating on the training split.
I0316 06:23:55.874364 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:27:43.625852 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 06:27:47.368082 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:30:05.572920 140515296233280 spec.py:326] Evaluating on the test split.
I0316 06:30:09.381496 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:32:27.768429 140515296233280 submission_runner.py:362] Time since start: 45535.92s, 	Step: 64093, 	{'train/accuracy': 0.6705376245666624, 'train/loss': 1.5086785493632942, 'train/bleu': 33.6690085223432, 'validation/accuracy': 0.6832153352097308, 'validation/loss': 1.4497123408265242, 'validation/bleu': 29.596739206528152, 'validation/num_examples': 3000, 'test/accuracy': 0.6956481320085992, 'test/loss': 1.358672869095346, 'test/bleu': 29.694214431085353, 'test/num_examples': 3003}
I0316 06:32:27.779516 140457232099072 logging_writer.py:48] [64093] global_step=64093, preemption_count=0, score=28469.380394, test/accuracy=0.695648, test/bleu=29.694214, test/loss=1.358673, test/num_examples=3003, total_duration=45535.920101, train/accuracy=0.670538, train/bleu=33.669009, train/loss=1.508679, validation/accuracy=0.683215, validation/bleu=29.596739, validation/loss=1.449712, validation/num_examples=3000
I0316 06:32:29.982107 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_64093.
I0316 06:35:31.673963 140457223706368 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.185483, loss=1.640454
I0316 06:35:31.677442 140515296233280 pytorch_submission_base.py:86] 64500) loss = 1.640, grad_norm = 0.185
I0316 06:39:14.556946 140457232099072 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.201714, loss=1.615357
I0316 06:39:14.560453 140515296233280 pytorch_submission_base.py:86] 65000) loss = 1.615, grad_norm = 0.202
I0316 06:42:57.168621 140457223706368 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.203266, loss=1.619565
I0316 06:42:57.172017 140515296233280 pytorch_submission_base.py:86] 65500) loss = 1.620, grad_norm = 0.203
I0316 06:46:30.079132 140515296233280 spec.py:298] Evaluating on the training split.
I0316 06:46:33.980485 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:50:20.535503 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 06:50:24.277994 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:53:23.396470 140515296233280 spec.py:326] Evaluating on the test split.
I0316 06:53:27.208282 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 06:56:35.052973 140515296233280 submission_runner.py:362] Time since start: 46894.01s, 	Step: 65979, 	{'train/accuracy': 0.6692088250856972, 'train/loss': 1.5390015701566706, 'train/bleu': 33.304574360100595, 'validation/accuracy': 0.6841204696779953, 'validation/loss': 1.4440724929015138, 'validation/bleu': 29.811458152345917, 'validation/num_examples': 3000, 'test/accuracy': 0.6994364069490442, 'test/loss': 1.3471531651269537, 'test/bleu': 29.822457678398095, 'test/num_examples': 3003}
I0316 06:56:35.065295 140457232099072 logging_writer.py:48] [65979] global_step=65979, preemption_count=0, score=29306.474890, test/accuracy=0.699436, test/bleu=29.822458, test/loss=1.347153, test/num_examples=3003, total_duration=46894.014262, train/accuracy=0.669209, train/bleu=33.304574, train/loss=1.539002, validation/accuracy=0.684120, validation/bleu=29.811458, validation/loss=1.444072, validation/num_examples=3000
I0316 06:56:37.287844 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_65979.
I0316 06:56:47.090692 140457223706368 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.183389, loss=1.634461
I0316 06:56:47.093935 140515296233280 pytorch_submission_base.py:86] 66000) loss = 1.634, grad_norm = 0.183
I0316 07:00:29.653486 140457232099072 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.202652, loss=1.609323
I0316 07:00:29.657104 140515296233280 pytorch_submission_base.py:86] 66500) loss = 1.609, grad_norm = 0.203
I0316 07:04:12.320126 140457223706368 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.193744, loss=1.582238
I0316 07:04:12.323717 140515296233280 pytorch_submission_base.py:86] 67000) loss = 1.582, grad_norm = 0.194
I0316 07:07:55.110093 140457232099072 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.200181, loss=1.599242
I0316 07:07:55.115204 140515296233280 pytorch_submission_base.py:86] 67500) loss = 1.599, grad_norm = 0.200
I0316 07:10:37.386842 140515296233280 spec.py:298] Evaluating on the training split.
I0316 07:10:41.281175 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:13:57.715715 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 07:14:01.453058 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:16:24.360565 140515296233280 spec.py:326] Evaluating on the test split.
I0316 07:16:28.168057 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:18:33.487907 140515296233280 submission_runner.py:362] Time since start: 48341.32s, 	Step: 67865, 	{'train/accuracy': 0.6701912931785862, 'train/loss': 1.527198602860223, 'train/bleu': 33.442483166407065, 'validation/accuracy': 0.684591635565585, 'validation/loss': 1.433192474364856, 'validation/bleu': 29.955462694439838, 'validation/num_examples': 3000, 'test/accuracy': 0.6992737202951601, 'test/loss': 1.3414754735343675, 'test/bleu': 29.711195470527247, 'test/num_examples': 3003}
I0316 07:18:33.499510 140457223706368 logging_writer.py:48] [67865] global_step=67865, preemption_count=0, score=30143.577477, test/accuracy=0.699274, test/bleu=29.711195, test/loss=1.341475, test/num_examples=3003, total_duration=48341.322000, train/accuracy=0.670191, train/bleu=33.442483, train/loss=1.527199, validation/accuracy=0.684592, validation/bleu=29.955463, validation/loss=1.433192, validation/num_examples=3000
I0316 07:18:35.691270 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_67865.
I0316 07:19:36.120855 140457232099072 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.186574, loss=1.528367
I0316 07:19:36.124058 140515296233280 pytorch_submission_base.py:86] 68000) loss = 1.528, grad_norm = 0.187
I0316 07:23:18.769514 140457223706368 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.200065, loss=1.540674
I0316 07:23:18.772817 140515296233280 pytorch_submission_base.py:86] 68500) loss = 1.541, grad_norm = 0.200
I0316 07:27:01.410785 140457232099072 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.202756, loss=1.646305
I0316 07:27:01.414762 140515296233280 pytorch_submission_base.py:86] 69000) loss = 1.646, grad_norm = 0.203
I0316 07:30:44.184646 140457223706368 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.208407, loss=1.579292
I0316 07:30:44.188207 140515296233280 pytorch_submission_base.py:86] 69500) loss = 1.579, grad_norm = 0.208
I0316 07:32:36.114087 140515296233280 spec.py:298] Evaluating on the training split.
I0316 07:32:40.027006 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:36:13.282888 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 07:36:17.005408 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:38:53.031087 140515296233280 spec.py:326] Evaluating on the test split.
I0316 07:38:56.833219 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:41:35.046283 140515296233280 submission_runner.py:362] Time since start: 49660.05s, 	Step: 69752, 	{'train/accuracy': 0.6795271900042297, 'train/loss': 1.4685161885409879, 'train/bleu': 34.25280051311348, 'validation/accuracy': 0.6868854694920088, 'validation/loss': 1.4277715480899182, 'validation/bleu': 29.864097863626423, 'validation/num_examples': 3000, 'test/accuracy': 0.700145255940968, 'test/loss': 1.3338085018302248, 'test/bleu': 30.137383723897436, 'test/num_examples': 3003}
I0316 07:41:35.058576 140457232099072 logging_writer.py:48] [69752] global_step=69752, preemption_count=0, score=30981.053390, test/accuracy=0.700145, test/bleu=30.137384, test/loss=1.333809, test/num_examples=3003, total_duration=49660.049259, train/accuracy=0.679527, train/bleu=34.252801, train/loss=1.468516, validation/accuracy=0.686885, validation/bleu=29.864098, validation/loss=1.427772, validation/num_examples=3000
I0316 07:41:37.210875 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_69752.
I0316 07:43:28.053294 140457223706368 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.210318, loss=1.537945
I0316 07:43:28.056383 140515296233280 pytorch_submission_base.py:86] 70000) loss = 1.538, grad_norm = 0.210
I0316 07:47:10.475813 140457232099072 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.206388, loss=1.647982
I0316 07:47:10.480226 140515296233280 pytorch_submission_base.py:86] 70500) loss = 1.648, grad_norm = 0.206
I0316 07:50:53.250899 140457223706368 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.209199, loss=1.675178
I0316 07:50:53.254272 140515296233280 pytorch_submission_base.py:86] 71000) loss = 1.675, grad_norm = 0.209
I0316 07:54:35.968334 140457232099072 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.204837, loss=1.524206
I0316 07:54:35.972522 140515296233280 pytorch_submission_base.py:86] 71500) loss = 1.524, grad_norm = 0.205
I0316 07:55:37.479147 140515296233280 spec.py:298] Evaluating on the training split.
I0316 07:55:41.378400 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 07:59:18.767097 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 07:59:22.492410 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:01:46.384120 140515296233280 spec.py:326] Evaluating on the test split.
I0316 08:01:50.207573 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:04:28.376222 140515296233280 submission_runner.py:362] Time since start: 51041.41s, 	Step: 71639, 	{'train/accuracy': 0.6774223045182823, 'train/loss': 1.4866812720742035, 'train/bleu': 34.104412110220125, 'validation/accuracy': 0.686389505399809, 'validation/loss': 1.424724178094506, 'validation/bleu': 30.129032875389193, 'validation/num_examples': 3000, 'test/accuracy': 0.7015048515484283, 'test/loss': 1.327286328510836, 'test/bleu': 29.81887507986993, 'test/num_examples': 3003}
I0316 08:04:28.388165 140457223706368 logging_writer.py:48] [71639] global_step=71639, preemption_count=0, score=31818.373950, test/accuracy=0.701505, test/bleu=29.818875, test/loss=1.327286, test/num_examples=3003, total_duration=51041.414298, train/accuracy=0.677422, train/bleu=34.104412, train/loss=1.486681, validation/accuracy=0.686390, validation/bleu=30.129033, validation/loss=1.424724, validation/num_examples=3000
I0316 08:04:30.526547 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_71639.
I0316 08:07:11.882043 140457232099072 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.206936, loss=1.549013
I0316 08:07:11.885138 140515296233280 pytorch_submission_base.py:86] 72000) loss = 1.549, grad_norm = 0.207
I0316 08:10:54.748233 140457223706368 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.202571, loss=1.534592
I0316 08:10:54.751693 140515296233280 pytorch_submission_base.py:86] 72500) loss = 1.535, grad_norm = 0.203
I0316 08:14:37.691174 140457232099072 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.208023, loss=1.536983
I0316 08:14:37.694412 140515296233280 pytorch_submission_base.py:86] 73000) loss = 1.537, grad_norm = 0.208
I0316 08:18:20.703207 140457223706368 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.211439, loss=1.500179
I0316 08:18:20.706282 140515296233280 pytorch_submission_base.py:86] 73500) loss = 1.500, grad_norm = 0.211
I0316 08:18:30.961412 140515296233280 spec.py:298] Evaluating on the training split.
I0316 08:18:34.871672 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:22:10.135468 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 08:22:13.884732 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:24:23.191964 140515296233280 spec.py:326] Evaluating on the test split.
I0316 08:24:26.994707 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:26:15.754824 140515296233280 submission_runner.py:362] Time since start: 52414.90s, 	Step: 73524, 	{'train/accuracy': 0.6775757990711262, 'train/loss': 1.4831811895918203, 'train/bleu': 33.56239893551465, 'validation/accuracy': 0.6870466578219737, 'validation/loss': 1.417836961104016, 'validation/bleu': 30.38759937487197, 'validation/num_examples': 3000, 'test/accuracy': 0.7041891813375167, 'test/loss': 1.31781700293417, 'test/bleu': 30.14620272293587, 'test/num_examples': 3003}
I0316 08:26:15.768136 140457232099072 logging_writer.py:48] [73524] global_step=73524, preemption_count=0, score=32655.835476, test/accuracy=0.704189, test/bleu=30.146203, test/loss=1.317817, test/num_examples=3003, total_duration=52414.896576, train/accuracy=0.677576, train/bleu=33.562399, train/loss=1.483181, validation/accuracy=0.687047, validation/bleu=30.387599, validation/loss=1.417837, validation/num_examples=3000
I0316 08:26:17.922965 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_73524.
I0316 08:29:50.324594 140457223706368 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.200958, loss=1.526885
I0316 08:29:50.327885 140515296233280 pytorch_submission_base.py:86] 74000) loss = 1.527, grad_norm = 0.201
I0316 08:33:33.401165 140457232099072 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.203934, loss=1.592285
I0316 08:33:33.405284 140515296233280 pytorch_submission_base.py:86] 74500) loss = 1.592, grad_norm = 0.204
I0316 08:37:16.075077 140457223706368 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.206932, loss=1.523928
I0316 08:37:16.078626 140515296233280 pytorch_submission_base.py:86] 75000) loss = 1.524, grad_norm = 0.207
I0316 08:40:18.182821 140515296233280 spec.py:298] Evaluating on the training split.
I0316 08:40:22.078703 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:43:45.609056 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 08:43:49.344716 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:46:03.046941 140515296233280 spec.py:326] Evaluating on the test split.
I0316 08:46:06.863806 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 08:48:16.914638 140515296233280 submission_runner.py:362] Time since start: 53722.12s, 	Step: 75410, 	{'train/accuracy': 0.6901934790807173, 'train/loss': 1.4040593608219147, 'train/bleu': 34.71339255725569, 'validation/accuracy': 0.6889809177815526, 'validation/loss': 1.4110492275359263, 'validation/bleu': 30.461296298804637, 'validation/num_examples': 3000, 'test/accuracy': 0.7050026146069375, 'test/loss': 1.311849798094242, 'test/bleu': 30.252368522856493, 'test/num_examples': 3003}
I0316 08:48:16.928565 140457232099072 logging_writer.py:48] [75410] global_step=75410, preemption_count=0, score=33493.085453, test/accuracy=0.705003, test/bleu=30.252369, test/loss=1.311850, test/num_examples=3003, total_duration=53722.117998, train/accuracy=0.690193, train/bleu=34.713393, train/loss=1.404059, validation/accuracy=0.688981, validation/bleu=30.461296, validation/loss=1.411049, validation/num_examples=3000
I0316 08:48:19.117116 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_75410.
I0316 08:48:59.629229 140457223706368 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.208493, loss=1.542684
I0316 08:48:59.632420 140515296233280 pytorch_submission_base.py:86] 75500) loss = 1.543, grad_norm = 0.208
I0316 08:52:42.289471 140457232099072 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.215274, loss=1.550338
I0316 08:52:42.293601 140515296233280 pytorch_submission_base.py:86] 76000) loss = 1.550, grad_norm = 0.215
I0316 08:56:24.892958 140457223706368 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.195846, loss=1.511068
I0316 08:56:24.896723 140515296233280 pytorch_submission_base.py:86] 76500) loss = 1.511, grad_norm = 0.196
I0316 09:00:07.652950 140457232099072 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.207538, loss=1.517904
I0316 09:00:07.656658 140515296233280 pytorch_submission_base.py:86] 77000) loss = 1.518, grad_norm = 0.208
I0316 09:02:19.288997 140515296233280 spec.py:298] Evaluating on the training split.
I0316 09:02:23.196922 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:06:08.175160 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 09:06:11.919803 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:08:44.845683 140515296233280 spec.py:326] Evaluating on the test split.
I0316 09:08:48.667256 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:11:30.902995 140515296233280 submission_runner.py:362] Time since start: 55043.22s, 	Step: 77296, 	{'train/accuracy': 0.6826583999815613, 'train/loss': 1.4543589885795285, 'train/bleu': 34.424077391178855, 'validation/accuracy': 0.6892908953391774, 'validation/loss': 1.4044031149644767, 'validation/bleu': 30.529621984216252, 'validation/num_examples': 3000, 'test/accuracy': 0.7059903550055198, 'test/loss': 1.3042375697228517, 'test/bleu': 30.433690732885772, 'test/num_examples': 3003}
I0316 09:11:30.918032 140457223706368 logging_writer.py:48] [77296] global_step=77296, preemption_count=0, score=34330.242558, test/accuracy=0.705990, test/bleu=30.433691, test/loss=1.304238, test/num_examples=3003, total_duration=55043.224169, train/accuracy=0.682658, train/bleu=34.424077, train/loss=1.454359, validation/accuracy=0.689291, validation/bleu=30.529622, validation/loss=1.404403, validation/num_examples=3000
I0316 09:11:33.320475 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_77296.
I0316 09:13:04.519768 140457232099072 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.218845, loss=1.615925
I0316 09:13:04.524240 140515296233280 pytorch_submission_base.py:86] 77500) loss = 1.616, grad_norm = 0.219
I0316 09:16:47.412175 140457223706368 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.211854, loss=1.600444
I0316 09:16:47.416589 140515296233280 pytorch_submission_base.py:86] 78000) loss = 1.600, grad_norm = 0.212
I0316 09:20:30.282164 140457232099072 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.217690, loss=1.596563
I0316 09:20:30.285325 140515296233280 pytorch_submission_base.py:86] 78500) loss = 1.597, grad_norm = 0.218
I0316 09:24:13.069974 140457223706368 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.211792, loss=1.465017
I0316 09:24:13.072972 140515296233280 pytorch_submission_base.py:86] 79000) loss = 1.465, grad_norm = 0.212
I0316 09:25:33.664866 140515296233280 spec.py:298] Evaluating on the training split.
I0316 09:25:37.591420 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:28:47.096305 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 09:28:50.838311 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:31:43.056399 140515296233280 spec.py:326] Evaluating on the test split.
I0316 09:31:46.882716 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:34:55.998652 140515296233280 submission_runner.py:362] Time since start: 56437.60s, 	Step: 79182, 	{'train/accuracy': 0.6818124491223243, 'train/loss': 1.4520943687728591, 'train/bleu': 34.37674920217998, 'validation/accuracy': 0.6899852450682571, 'validation/loss': 1.400145379474526, 'validation/bleu': 30.336275416897152, 'validation/num_examples': 3000, 'test/accuracy': 0.7070478182557667, 'test/loss': 1.297778582737784, 'test/bleu': 30.606031842070642, 'test/num_examples': 3003}
I0316 09:34:56.012861 140457232099072 logging_writer.py:48] [79182] global_step=79182, preemption_count=0, score=35167.603321, test/accuracy=0.707048, test/bleu=30.606032, test/loss=1.297779, test/num_examples=3003, total_duration=56437.600014, train/accuracy=0.681812, train/bleu=34.376749, train/loss=1.452094, validation/accuracy=0.689985, validation/bleu=30.336275, validation/loss=1.400145, validation/num_examples=3000
I0316 09:34:58.353238 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_79182.
I0316 09:37:20.504346 140457223706368 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.211079, loss=1.532730
I0316 09:37:20.508063 140515296233280 pytorch_submission_base.py:86] 79500) loss = 1.533, grad_norm = 0.211
I0316 09:41:03.428604 140457232099072 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.220772, loss=1.533736
I0316 09:41:03.431869 140515296233280 pytorch_submission_base.py:86] 80000) loss = 1.534, grad_norm = 0.221
I0316 09:44:46.452373 140457223706368 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.392588, loss=1.515947
I0316 09:44:46.455636 140515296233280 pytorch_submission_base.py:86] 80500) loss = 1.516, grad_norm = 0.393
I0316 09:48:29.117864 140457232099072 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.212063, loss=1.544371
I0316 09:48:29.121079 140515296233280 pytorch_submission_base.py:86] 81000) loss = 1.544, grad_norm = 0.212
I0316 09:48:58.517760 140515296233280 spec.py:298] Evaluating on the training split.
I0316 09:49:02.417539 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:52:57.735448 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 09:53:01.474514 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:55:19.961892 140515296233280 spec.py:326] Evaluating on the test split.
I0316 09:55:23.776674 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 09:58:04.783130 140515296233280 submission_runner.py:362] Time since start: 57842.45s, 	Step: 81067, 	{'train/accuracy': 0.6804222758518146, 'train/loss': 1.4670655466882456, 'train/bleu': 34.355315337154224, 'validation/accuracy': 0.6913119490148913, 'validation/loss': 1.3955473467470956, 'validation/bleu': 30.51700880394276, 'validation/num_examples': 3000, 'test/accuracy': 0.7067340654232758, 'test/loss': 1.2923299416071117, 'test/bleu': 30.45410239126915, 'test/num_examples': 3003}
I0316 09:58:04.797283 140457223706368 logging_writer.py:48] [81067] global_step=81067, preemption_count=0, score=36004.744053, test/accuracy=0.706734, test/bleu=30.454102, test/loss=1.292330, test/num_examples=3003, total_duration=57842.452931, train/accuracy=0.680422, train/bleu=34.355315, train/loss=1.467066, validation/accuracy=0.691312, validation/bleu=30.517009, validation/loss=1.395547, validation/num_examples=3000
I0316 09:58:06.957880 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_81067.
I0316 10:01:20.072489 140457232099072 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.209350, loss=1.575264
I0316 10:01:20.075489 140515296233280 pytorch_submission_base.py:86] 81500) loss = 1.575, grad_norm = 0.209
I0316 10:05:02.600683 140457223706368 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.212596, loss=1.572739
I0316 10:05:02.604086 140515296233280 pytorch_submission_base.py:86] 82000) loss = 1.573, grad_norm = 0.213
I0316 10:08:45.419137 140457232099072 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.212020, loss=1.459369
I0316 10:08:45.422515 140515296233280 pytorch_submission_base.py:86] 82500) loss = 1.459, grad_norm = 0.212
I0316 10:12:07.324482 140515296233280 spec.py:298] Evaluating on the training split.
I0316 10:12:11.212963 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 10:15:06.393321 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 10:15:10.132509 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 10:17:39.819892 140515296233280 spec.py:326] Evaluating on the test split.
I0316 10:17:43.631508 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 10:19:46.607268 140515296233280 submission_runner.py:362] Time since start: 59231.26s, 	Step: 82954, 	{'train/accuracy': 0.6871974858348817, 'train/loss': 1.4204561583855206, 'train/bleu': 34.50536646949436, 'validation/accuracy': 0.691820312209396, 'validation/loss': 1.3914246452306853, 'validation/bleu': 30.67833187300776, 'validation/num_examples': 3000, 'test/accuracy': 0.7079193539015746, 'test/loss': 1.2890810201324734, 'test/bleu': 30.620050897445417, 'test/num_examples': 3003}
I0316 10:19:46.621580 140457223706368 logging_writer.py:48] [82954] global_step=82954, preemption_count=0, score=36842.168894, test/accuracy=0.707919, test/bleu=30.620051, test/loss=1.289081, test/num_examples=3003, total_duration=59231.259644, train/accuracy=0.687197, train/bleu=34.505366, train/loss=1.420456, validation/accuracy=0.691820, validation/bleu=30.678332, validation/loss=1.391425, validation/num_examples=3000
I0316 10:19:48.782041 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_82954.
I0316 10:20:09.691374 140457232099072 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.208832, loss=1.465634
I0316 10:20:09.694433 140515296233280 pytorch_submission_base.py:86] 83000) loss = 1.466, grad_norm = 0.209
I0316 10:23:52.356391 140457223706368 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.209924, loss=1.573635
I0316 10:23:52.359757 140515296233280 pytorch_submission_base.py:86] 83500) loss = 1.574, grad_norm = 0.210
I0316 10:27:35.066263 140457232099072 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.208051, loss=1.556182
I0316 10:27:35.069518 140515296233280 pytorch_submission_base.py:86] 84000) loss = 1.556, grad_norm = 0.208
I0316 10:31:17.850605 140457223706368 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.210052, loss=1.472142
I0316 10:31:17.854326 140515296233280 pytorch_submission_base.py:86] 84500) loss = 1.472, grad_norm = 0.210
I0316 10:33:48.865288 140515296233280 spec.py:298] Evaluating on the training split.
I0316 10:33:52.744906 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 10:37:43.860579 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 10:37:47.601431 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 10:40:26.442243 140515296233280 spec.py:326] Evaluating on the test split.
I0316 10:40:30.255736 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 10:43:03.726841 140515296233280 submission_runner.py:362] Time since start: 60532.80s, 	Step: 84840, 	{'train/accuracy': 0.68675340926207, 'train/loss': 1.421291884392126, 'train/bleu': 34.74114605682637, 'validation/accuracy': 0.6932586080767752, 'validation/loss': 1.3844049128343108, 'validation/bleu': 30.63959953742076, 'validation/num_examples': 3000, 'test/accuracy': 0.7098134913717971, 'test/loss': 1.2796984849805357, 'test/bleu': 30.72195270285195, 'test/num_examples': 3003}
I0316 10:43:03.742049 140457232099072 logging_writer.py:48] [84840] global_step=84840, preemption_count=0, score=37679.320273, test/accuracy=0.709813, test/bleu=30.721953, test/loss=1.279698, test/num_examples=3003, total_duration=60532.800432, train/accuracy=0.686753, train/bleu=34.741146, train/loss=1.421292, validation/accuracy=0.693259, validation/bleu=30.639600, validation/loss=1.384405, validation/num_examples=3000
I0316 10:43:05.932112 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_84840.
I0316 10:44:17.606665 140457223706368 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.202573, loss=1.358355
I0316 10:44:17.609960 140515296233280 pytorch_submission_base.py:86] 85000) loss = 1.358, grad_norm = 0.203
I0316 10:48:00.394346 140457232099072 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.213844, loss=1.476833
I0316 10:48:00.398112 140515296233280 pytorch_submission_base.py:86] 85500) loss = 1.477, grad_norm = 0.214
I0316 10:51:43.332498 140457223706368 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.224611, loss=1.443891
I0316 10:51:43.336652 140515296233280 pytorch_submission_base.py:86] 86000) loss = 1.444, grad_norm = 0.225
I0316 10:55:25.889957 140457232099072 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.343725, loss=1.550453
I0316 10:55:25.893330 140515296233280 pytorch_submission_base.py:86] 86500) loss = 1.550, grad_norm = 0.344
I0316 10:57:06.027291 140515296233280 spec.py:298] Evaluating on the training split.
I0316 10:57:09.906481 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:01:00.247918 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 11:01:04.004233 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:03:32.251276 140515296233280 spec.py:326] Evaluating on the test split.
I0316 11:03:36.083399 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:06:05.714755 140515296233280 submission_runner.py:362] Time since start: 61929.96s, 	Step: 86726, 	{'train/accuracy': 0.6846408409577575, 'train/loss': 1.430961149789875, 'train/bleu': 34.58290761942374, 'validation/accuracy': 0.6922790789946808, 'validation/loss': 1.3826513116700352, 'validation/bleu': 30.66834502775578, 'validation/num_examples': 3000, 'test/accuracy': 0.7096159432920807, 'test/loss': 1.2784579992446692, 'test/bleu': 30.872431032250816, 'test/num_examples': 3003}
I0316 11:06:05.726974 140457223706368 logging_writer.py:48] [86726] global_step=86726, preemption_count=0, score=38516.441675, test/accuracy=0.709616, test/bleu=30.872431, test/loss=1.278458, test/num_examples=3003, total_duration=61929.962465, train/accuracy=0.684641, train/bleu=34.582908, train/loss=1.430961, validation/accuracy=0.692279, validation/bleu=30.668345, validation/loss=1.382651, validation/num_examples=3000
I0316 11:06:07.905504 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_86726.
I0316 11:08:10.270794 140457232099072 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.219953, loss=1.526025
I0316 11:08:10.274148 140515296233280 pytorch_submission_base.py:86] 87000) loss = 1.526, grad_norm = 0.220
I0316 11:11:53.151344 140457223706368 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.206104, loss=1.443541
I0316 11:11:53.154638 140515296233280 pytorch_submission_base.py:86] 87500) loss = 1.444, grad_norm = 0.206
I0316 11:15:36.085765 140457232099072 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.221559, loss=1.455126
I0316 11:15:36.089524 140515296233280 pytorch_submission_base.py:86] 88000) loss = 1.455, grad_norm = 0.222
I0316 11:19:19.009839 140457223706368 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.217702, loss=1.447173
I0316 11:19:19.013521 140515296233280 pytorch_submission_base.py:86] 88500) loss = 1.447, grad_norm = 0.218
I0316 11:20:08.082227 140515296233280 spec.py:298] Evaluating on the training split.
I0316 11:20:11.983099 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:23:20.997329 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 11:23:24.736732 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:26:15.662661 140515296233280 spec.py:326] Evaluating on the test split.
I0316 11:26:19.476952 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:29:19.511451 140515296233280 submission_runner.py:362] Time since start: 63312.02s, 	Step: 88611, 	{'train/accuracy': 0.6934860168028542, 'train/loss': 1.3810592955173207, 'train/bleu': 34.965156857212136, 'validation/accuracy': 0.6939653569081599, 'validation/loss': 1.3805424955983188, 'validation/bleu': 30.71567532633703, 'validation/num_examples': 3000, 'test/accuracy': 0.7098832142234618, 'test/loss': 1.276646385305909, 'test/bleu': 30.765114074347082, 'test/num_examples': 3003}
I0316 11:29:19.526828 140457232099072 logging_writer.py:48] [88611] global_step=88611, preemption_count=0, score=39353.671068, test/accuracy=0.709883, test/bleu=30.765114, test/loss=1.276646, test/num_examples=3003, total_duration=63312.017384, train/accuracy=0.693486, train/bleu=34.965157, train/loss=1.381059, validation/accuracy=0.693965, validation/bleu=30.715675, validation/loss=1.380542, validation/num_examples=3000
I0316 11:29:21.677764 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_88611.
I0316 11:32:15.519021 140457223706368 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.247061, loss=1.463092
I0316 11:32:15.522736 140515296233280 pytorch_submission_base.py:86] 89000) loss = 1.463, grad_norm = 0.247
I0316 11:35:58.051637 140457232099072 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.225430, loss=1.484138
I0316 11:35:58.054942 140515296233280 pytorch_submission_base.py:86] 89500) loss = 1.484, grad_norm = 0.225
I0316 11:39:40.937882 140457223706368 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.217336, loss=1.494201
I0316 11:39:40.941201 140515296233280 pytorch_submission_base.py:86] 90000) loss = 1.494, grad_norm = 0.217
I0316 11:43:22.039828 140515296233280 spec.py:298] Evaluating on the training split.
I0316 11:43:25.921979 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:47:23.937225 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 11:47:27.682319 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:50:01.399776 140515296233280 spec.py:326] Evaluating on the test split.
I0316 11:50:05.216933 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 11:53:22.877296 140515296233280 submission_runner.py:362] Time since start: 64705.97s, 	Step: 90497, 	{'train/accuracy': 0.687704974428365, 'train/loss': 1.4131074159174828, 'train/bleu': 35.28538919280296, 'validation/accuracy': 0.6950812761156092, 'validation/loss': 1.377949920800734, 'validation/bleu': 30.585769657626205, 'validation/num_examples': 3000, 'test/accuracy': 0.7104177560862239, 'test/loss': 1.2749721290163267, 'test/bleu': 30.640379388437445, 'test/num_examples': 3003}
I0316 11:53:22.889708 140457232099072 logging_writer.py:48] [90497] global_step=90497, preemption_count=0, score=40191.033320, test/accuracy=0.710418, test/bleu=30.640379, test/loss=1.274972, test/num_examples=3003, total_duration=64705.974968, train/accuracy=0.687705, train/bleu=35.285389, train/loss=1.413107, validation/accuracy=0.695081, validation/bleu=30.585770, validation/loss=1.377950, validation/num_examples=3000
I0316 11:53:25.054847 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_90497.
I0316 11:53:26.828442 140457223706368 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.215583, loss=1.408218
I0316 11:53:26.831528 140515296233280 pytorch_submission_base.py:86] 90500) loss = 1.408, grad_norm = 0.216
I0316 11:57:09.418617 140457232099072 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.229569, loss=1.454525
I0316 11:57:09.422073 140515296233280 pytorch_submission_base.py:86] 91000) loss = 1.455, grad_norm = 0.230
I0316 12:00:52.385239 140457223706368 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.220881, loss=1.511554
I0316 12:00:52.389736 140515296233280 pytorch_submission_base.py:86] 91500) loss = 1.512, grad_norm = 0.221
I0316 12:04:35.519447 140457232099072 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.214528, loss=1.508966
I0316 12:04:35.523752 140515296233280 pytorch_submission_base.py:86] 92000) loss = 1.509, grad_norm = 0.215
I0316 12:07:25.212129 140515296233280 spec.py:298] Evaluating on the training split.
I0316 12:07:29.133319 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:11:23.488666 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 12:11:27.230314 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:13:45.476649 140515296233280 spec.py:326] Evaluating on the test split.
I0316 12:13:49.288675 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:16:28.969115 140515296233280 submission_runner.py:362] Time since start: 66149.15s, 	Step: 92382, 	{'train/accuracy': 0.690828436192349, 'train/loss': 1.4009928679429722, 'train/bleu': 34.86268077586935, 'validation/accuracy': 0.6951432716271342, 'validation/loss': 1.375137165069249, 'validation/bleu': 30.694537444259254, 'validation/num_examples': 3000, 'test/accuracy': 0.7113009122073093, 'test/loss': 1.2705107743594213, 'test/bleu': 30.855302734006802, 'test/num_examples': 3003}
I0316 12:16:28.983780 140457223706368 logging_writer.py:48] [92382] global_step=92382, preemption_count=0, score=41028.206060, test/accuracy=0.711301, test/bleu=30.855303, test/loss=1.270511, test/num_examples=3003, total_duration=66149.147317, train/accuracy=0.690828, train/bleu=34.862681, train/loss=1.400993, validation/accuracy=0.695143, validation/bleu=30.694537, validation/loss=1.375137, validation/num_examples=3000
I0316 12:16:31.145788 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_92382.
I0316 12:17:24.163600 140457232099072 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.227669, loss=1.502042
I0316 12:17:24.167353 140515296233280 pytorch_submission_base.py:86] 92500) loss = 1.502, grad_norm = 0.228
I0316 12:21:07.130368 140457223706368 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.214977, loss=1.513034
I0316 12:21:07.133782 140515296233280 pytorch_submission_base.py:86] 93000) loss = 1.513, grad_norm = 0.215
I0316 12:24:50.293743 140457232099072 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.220699, loss=1.487576
I0316 12:24:50.297811 140515296233280 pytorch_submission_base.py:86] 93500) loss = 1.488, grad_norm = 0.221
I0316 12:28:33.087864 140457223706368 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.214940, loss=1.493671
I0316 12:28:33.091150 140515296233280 pytorch_submission_base.py:86] 94000) loss = 1.494, grad_norm = 0.215
I0316 12:30:31.235894 140515296233280 spec.py:298] Evaluating on the training split.
I0316 12:30:35.131645 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:34:16.994902 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 12:34:20.740447 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:36:53.845279 140515296233280 spec.py:326] Evaluating on the test split.
I0316 12:36:57.673839 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:39:54.458503 140515296233280 submission_runner.py:362] Time since start: 67535.17s, 	Step: 94266, 	{'train/accuracy': 0.694521703447018, 'train/loss': 1.3807012156894036, 'train/bleu': 34.789253565509306, 'validation/accuracy': 0.6951680698317442, 'validation/loss': 1.3736450106012326, 'validation/bleu': 30.804896484404235, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938760095287, 'test/loss': 1.2700047389750742, 'test/bleu': 30.937181146546003, 'test/num_examples': 3003}
I0316 12:39:54.473891 140457232099072 logging_writer.py:48] [94266] global_step=94266, preemption_count=0, score=41865.315315, test/accuracy=0.711394, test/bleu=30.937181, test/loss=1.270005, test/num_examples=3003, total_duration=67535.171054, train/accuracy=0.694522, train/bleu=34.789254, train/loss=1.380701, validation/accuracy=0.695168, validation/bleu=30.804896, validation/loss=1.373645, validation/num_examples=3000
I0316 12:39:56.860207 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_94266.
I0316 12:41:41.405724 140457223706368 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.219748, loss=1.500259
I0316 12:41:41.408963 140515296233280 pytorch_submission_base.py:86] 94500) loss = 1.500, grad_norm = 0.220
I0316 12:45:23.969977 140457232099072 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.230658, loss=1.517984
I0316 12:45:23.973510 140515296233280 pytorch_submission_base.py:86] 95000) loss = 1.518, grad_norm = 0.231
I0316 12:49:06.520495 140457223706368 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.402275, loss=1.455068
I0316 12:49:06.523548 140515296233280 pytorch_submission_base.py:86] 95500) loss = 1.455, grad_norm = 0.402
I0316 12:52:49.234442 140457232099072 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.224080, loss=1.514318
I0316 12:52:49.238179 140515296233280 pytorch_submission_base.py:86] 96000) loss = 1.514, grad_norm = 0.224
I0316 12:53:56.893536 140515296233280 spec.py:298] Evaluating on the training split.
I0316 12:54:00.789486 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 12:57:31.509899 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 12:57:35.253458 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:00:04.763757 140515296233280 spec.py:326] Evaluating on the test split.
I0316 13:00:08.602235 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:02:48.705800 140515296233280 submission_runner.py:362] Time since start: 68940.83s, 	Step: 96153, 	{'train/accuracy': 0.6921132977324445, 'train/loss': 1.3967991718219346, 'train/bleu': 34.98858866771025, 'validation/accuracy': 0.6953912536732341, 'validation/loss': 1.3724778482287883, 'validation/bleu': 30.783272186813228, 'validation/num_examples': 3000, 'test/accuracy': 0.7113706350589739, 'test/loss': 1.2684933509093022, 'test/bleu': 31.05621347117347, 'test/num_examples': 3003}
I0316 13:02:48.719233 140457223706368 logging_writer.py:48] [96153] global_step=96153, preemption_count=0, score=42702.341250, test/accuracy=0.711371, test/bleu=31.056213, test/loss=1.268493, test/num_examples=3003, total_duration=68940.828635, train/accuracy=0.692113, train/bleu=34.988589, train/loss=1.396799, validation/accuracy=0.695391, validation/bleu=30.783272, validation/loss=1.372478, validation/num_examples=3000
I0316 13:02:50.868117 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_96153.
I0316 13:05:25.714133 140457232099072 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.214355, loss=1.498643
I0316 13:05:25.717316 140515296233280 pytorch_submission_base.py:86] 96500) loss = 1.499, grad_norm = 0.214
I0316 13:09:08.404091 140457223706368 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.208714, loss=1.391888
I0316 13:09:08.407262 140515296233280 pytorch_submission_base.py:86] 97000) loss = 1.392, grad_norm = 0.209
I0316 13:12:51.072777 140457232099072 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.202314, loss=1.458591
I0316 13:12:51.075987 140515296233280 pytorch_submission_base.py:86] 97500) loss = 1.459, grad_norm = 0.202
I0316 13:16:33.755796 140457223706368 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.224087, loss=1.552509
I0316 13:16:33.758968 140515296233280 pytorch_submission_base.py:86] 98000) loss = 1.553, grad_norm = 0.224
I0316 13:16:51.143484 140515296233280 spec.py:298] Evaluating on the training split.
I0316 13:16:55.039723 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:20:46.414298 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 13:20:50.157801 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:23:10.919946 140515296233280 spec.py:326] Evaluating on the test split.
I0316 13:23:14.737487 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:26:08.882354 140515296233280 submission_runner.py:362] Time since start: 70315.08s, 	Step: 98040, 	{'train/accuracy': 0.690156016145857, 'train/loss': 1.4037496208728286, 'train/bleu': 34.98944598731898, 'validation/accuracy': 0.6952300653432691, 'validation/loss': 1.3723590880770231, 'validation/bleu': 30.755706125476443, 'validation/num_examples': 3000, 'test/accuracy': 0.7115100807623032, 'test/loss': 1.2680101933356573, 'test/bleu': 31.042565142038715, 'test/num_examples': 3003}
I0316 13:26:08.896473 140457232099072 logging_writer.py:48] [98040] global_step=98040, preemption_count=0, score=43539.659498, test/accuracy=0.711510, test/bleu=31.042565, test/loss=1.268010, test/num_examples=3003, total_duration=70315.078677, train/accuracy=0.690156, train/bleu=34.989446, train/loss=1.403750, validation/accuracy=0.695230, validation/bleu=30.755706, validation/loss=1.372359, validation/num_examples=3000
I0316 13:26:11.041982 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_98040.
I0316 13:29:36.205093 140457223706368 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.212562, loss=1.520843
I0316 13:29:36.208898 140515296233280 pytorch_submission_base.py:86] 98500) loss = 1.521, grad_norm = 0.213
I0316 13:33:18.784751 140457232099072 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.245083, loss=1.465765
I0316 13:33:18.787935 140515296233280 pytorch_submission_base.py:86] 99000) loss = 1.466, grad_norm = 0.245
I0316 13:37:01.046871 140457223706368 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.224430, loss=1.458776
I0316 13:37:01.050317 140515296233280 pytorch_submission_base.py:86] 99500) loss = 1.459, grad_norm = 0.224
I0316 13:40:11.090400 140515296233280 spec.py:298] Evaluating on the training split.
I0316 13:40:14.982262 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:43:56.633888 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 13:44:00.399423 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:46:19.631409 140515296233280 spec.py:326] Evaluating on the test split.
I0316 13:46:23.447246 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 13:49:23.868142 140515296233280 submission_runner.py:362] Time since start: 71715.03s, 	Step: 99928, 	{'train/accuracy': 0.6915597491513722, 'train/loss': 1.4004351914446809, 'train/bleu': 35.114968149692174, 'validation/accuracy': 0.6952300653432691, 'validation/loss': 1.3725366470967502, 'validation/bleu': 30.723493736343755, 'validation/num_examples': 3000, 'test/accuracy': 0.7118005926442391, 'test/loss': 1.2678377019057578, 'test/bleu': 31.042933762858322, 'test/num_examples': 3003}
I0316 13:49:23.881810 140457232099072 logging_writer.py:48] [99928] global_step=99928, preemption_count=0, score=44376.738197, test/accuracy=0.711801, test/bleu=31.042934, test/loss=1.267838, test/num_examples=3003, total_duration=71715.025572, train/accuracy=0.691560, train/bleu=35.114968, train/loss=1.400435, validation/accuracy=0.695230, validation/bleu=30.723494, validation/loss=1.372537, validation/num_examples=3000
I0316 13:49:26.031018 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_99928.
I0316 13:49:58.483849 140457223706368 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.250713, loss=1.453981
I0316 13:49:58.486912 140515296233280 pytorch_submission_base.py:86] 100000) loss = 1.454, grad_norm = 0.251
I0316 13:53:41.214588 140457232099072 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.220923, loss=1.576370
I0316 13:53:41.218272 140515296233280 pytorch_submission_base.py:86] 100500) loss = 1.576, grad_norm = 0.221
I0316 13:57:23.906732 140457223706368 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.210235, loss=1.480417
I0316 13:57:23.910342 140515296233280 pytorch_submission_base.py:86] 101000) loss = 1.480, grad_norm = 0.210
I0316 14:01:06.649587 140457232099072 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.228501, loss=1.443134
I0316 14:01:06.653128 140515296233280 pytorch_submission_base.py:86] 101500) loss = 1.443, grad_norm = 0.229
I0316 14:03:26.159977 140515296233280 spec.py:298] Evaluating on the training split.
I0316 14:03:30.055611 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:07:15.988324 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 14:07:19.732251 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:09:40.372815 140515296233280 spec.py:326] Evaluating on the test split.
I0316 14:09:44.191717 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:12:44.400142 140515296233280 submission_runner.py:362] Time since start: 73110.10s, 	Step: 101814, 	{'train/accuracy': 0.6902809838414391, 'train/loss': 1.3996011885809805, 'train/bleu': 34.88556666678789, 'validation/accuracy': 0.695428450980149, 'validation/loss': 1.3727360982814845, 'validation/bleu': 30.746952220606925, 'validation/num_examples': 3000, 'test/accuracy': 0.7117773516936843, 'test/loss': 1.2681385632734878, 'test/bleu': 31.018640058088316, 'test/num_examples': 3003}
I0316 14:12:44.416332 140457223706368 logging_writer.py:48] [101814] global_step=101814, preemption_count=0, score=45213.891191, test/accuracy=0.711777, test/bleu=31.018640, test/loss=1.268139, test/num_examples=3003, total_duration=73110.095177, train/accuracy=0.690281, train/bleu=34.885567, train/loss=1.399601, validation/accuracy=0.695428, validation/bleu=30.746952, validation/loss=1.372736, validation/num_examples=3000
I0316 14:12:46.554579 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_101814.
I0316 14:14:09.663512 140457232099072 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.221956, loss=1.502514
I0316 14:14:09.666996 140515296233280 pytorch_submission_base.py:86] 102000) loss = 1.503, grad_norm = 0.222
I0316 14:17:52.674055 140457223706368 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.222270, loss=1.469803
I0316 14:17:52.677983 140515296233280 pytorch_submission_base.py:86] 102500) loss = 1.470, grad_norm = 0.222
I0316 14:21:35.553256 140457232099072 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.213296, loss=1.509892
I0316 14:21:35.556560 140515296233280 pytorch_submission_base.py:86] 103000) loss = 1.510, grad_norm = 0.213
I0316 14:25:18.553065 140457223706368 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.214949, loss=1.505365
I0316 14:25:18.556317 140515296233280 pytorch_submission_base.py:86] 103500) loss = 1.505, grad_norm = 0.215
I0316 14:26:46.678297 140515296233280 spec.py:298] Evaluating on the training split.
I0316 14:26:50.581678 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:30:38.206953 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 14:30:41.960934 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:33:10.906610 140515296233280 spec.py:326] Evaluating on the test split.
I0316 14:33:14.713474 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:36:01.480532 140515296233280 submission_runner.py:362] Time since start: 74510.61s, 	Step: 103699, 	{'train/accuracy': 0.6905433330669376, 'train/loss': 1.4021991203233282, 'train/bleu': 34.85612488584991, 'validation/accuracy': 0.6953292581617091, 'validation/loss': 1.3724982873739942, 'validation/bleu': 30.78517277616655, 'validation/num_examples': 3000, 'test/accuracy': 0.7117076288420197, 'test/loss': 1.2672779218232526, 'test/bleu': 31.089071774482054, 'test/num_examples': 3003}
I0316 14:36:01.495083 140457232099072 logging_writer.py:48] [103699] global_step=103699, preemption_count=0, score=46051.032849, test/accuracy=0.711708, test/bleu=31.089072, test/loss=1.267278, test/num_examples=3003, total_duration=74510.613470, train/accuracy=0.690543, train/bleu=34.856125, train/loss=1.402199, validation/accuracy=0.695329, validation/bleu=30.785173, validation/loss=1.372498, validation/num_examples=3000
I0316 14:36:03.626371 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_103699.
I0316 14:38:18.309223 140457223706368 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.208021, loss=1.484852
I0316 14:38:18.312816 140515296233280 pytorch_submission_base.py:86] 104000) loss = 1.485, grad_norm = 0.208
I0316 14:42:01.121472 140457232099072 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.206133, loss=1.423328
I0316 14:42:01.124847 140515296233280 pytorch_submission_base.py:86] 104500) loss = 1.423, grad_norm = 0.206
I0316 14:45:43.648429 140457223706368 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.216556, loss=1.444627
I0316 14:45:43.652057 140515296233280 pytorch_submission_base.py:86] 105000) loss = 1.445, grad_norm = 0.217
I0316 14:49:26.712391 140457232099072 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.223989, loss=1.445534
I0316 14:49:26.715774 140515296233280 pytorch_submission_base.py:86] 105500) loss = 1.446, grad_norm = 0.224
I0316 14:50:03.684927 140515296233280 spec.py:298] Evaluating on the training split.
I0316 14:50:07.576306 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:53:47.656145 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 14:53:51.382222 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:56:15.347704 140515296233280 spec.py:326] Evaluating on the test split.
I0316 14:56:19.160612 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 14:59:04.900956 140515296233280 submission_runner.py:362] Time since start: 75907.62s, 	Step: 105584, 	{'train/accuracy': 0.6929132048585973, 'train/loss': 1.3884049001103183, 'train/bleu': 35.04976320408652, 'validation/accuracy': 0.6949696841948643, 'validation/loss': 1.3726653846511512, 'validation/bleu': 30.690963762216814, 'validation/num_examples': 3000, 'test/accuracy': 0.7116843878914647, 'test/loss': 1.2674456924350705, 'test/bleu': 30.93839563377724, 'test/num_examples': 3003}
I0316 14:59:04.914189 140457223706368 logging_writer.py:48] [105584] global_step=105584, preemption_count=0, score=46888.108569, test/accuracy=0.711684, test/bleu=30.938396, test/loss=1.267446, test/num_examples=3003, total_duration=75907.620111, train/accuracy=0.692913, train/bleu=35.049763, train/loss=1.388405, validation/accuracy=0.694970, validation/bleu=30.690964, validation/loss=1.372665, validation/num_examples=3000
I0316 14:59:07.041102 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_105584.
I0316 15:02:13.012684 140457232099072 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.211727, loss=1.494299
I0316 15:02:13.016056 140515296233280 pytorch_submission_base.py:86] 106000) loss = 1.494, grad_norm = 0.212
I0316 15:05:55.805567 140457223706368 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.278109, loss=1.493163
I0316 15:05:55.808844 140515296233280 pytorch_submission_base.py:86] 106500) loss = 1.493, grad_norm = 0.278
I0316 15:09:38.530788 140457232099072 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.231240, loss=1.460747
I0316 15:09:38.534674 140515296233280 pytorch_submission_base.py:86] 107000) loss = 1.461, grad_norm = 0.231
I0316 15:13:07.171864 140515296233280 spec.py:298] Evaluating on the training split.
I0316 15:13:11.061780 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 15:16:43.650266 140515296233280 spec.py:310] Evaluating on the validation split.
I0316 15:16:47.382279 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 15:19:16.160171 140515296233280 spec.py:326] Evaluating on the test split.
I0316 15:19:19.976748 140515296233280 workload.py:130] Translating evaluation dataset.
I0316 15:22:20.054208 140515296233280 submission_runner.py:362] Time since start: 77291.11s, 	Step: 107469, 	{'train/accuracy': 0.6939958403891395, 'train/loss': 1.3823707721987977, 'train/bleu': 34.85290448733799, 'validation/accuracy': 0.695428450980149, 'validation/loss': 1.374405811769228, 'validation/bleu': 30.85186188009511, 'validation/num_examples': 3000, 'test/accuracy': 0.7112892917320318, 'test/loss': 1.269347365057231, 'test/bleu': 30.99902221060933, 'test/num_examples': 3003}
I0316 15:22:20.068077 140457223706368 logging_writer.py:48] [107469] global_step=107469, preemption_count=0, score=47725.237956, test/accuracy=0.711289, test/bleu=30.999022, test/loss=1.269347, test/num_examples=3003, total_duration=77291.106956, train/accuracy=0.693996, train/bleu=34.852904, train/loss=1.382371, validation/accuracy=0.695428, validation/bleu=30.851862, validation/loss=1.374406, validation/num_examples=3000
I0316 15:22:22.189099 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_107469.
I0316 15:22:22.201424 140457232099072 logging_writer.py:48] [107469] global_step=107469, preemption_count=0, score=47725.237956
I0316 15:22:26.776445 140515296233280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_pytorch/trial_1/checkpoint_107469.
I0316 15:22:26.799109 140515296233280 submission_runner.py:523] Tuning trial 1/1
I0316 15:22:26.799271 140515296233280 submission_runner.py:524] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, beta1=0.9326607383586145, beta2=0.9955159689799007, warmup_steps=1999, weight_decay=0.08121616522670176, label_smoothing=0.0)
I0316 15:22:26.801243 140515296233280 submission_runner.py:525] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006215755789861411, 'train/loss': 11.07326966020535, 'train/bleu': 8.618773875820979e-11, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.0789830566267, 'validation/bleu': 1.0550295602636623e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.062961913892279, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.183401584625244, 'total_duration': 4.1853766441345215, 'global_step': 1, 'preemption_count': 0}), (1880, {'train/accuracy': 0.49743297204791787, 'train/loss': 3.0247368796349114, 'train/bleu': 20.52722740944265, 'validation/accuracy': 0.4927527247027315, 'validation/loss': 3.041989172483912, 'validation/bleu': 16.26766595962541, 'validation/num_examples': 3000, 'test/accuracy': 0.4902794724304224, 'test/loss': 3.107095752716286, 'test/bleu': 15.146659926047317, 'test/num_examples': 3003, 'score': 841.4299666881561, 'total_duration': 1674.3810913562775, 'global_step': 1880, 'preemption_count': 0}), (3763, {'train/accuracy': 0.5649170010944911, 'train/loss': 2.3638860346930866, 'train/bleu': 25.997323666152543, 'validation/accuracy': 0.5792612614846685, 'validation/loss': 2.2553893473112545, 'validation/bleu': 22.491922136302023, 'validation/num_examples': 3000, 'test/accuracy': 0.5814421009819302, 'test/loss': 2.240032718900703, 'test/bleu': 21.43944740788008, 'test/num_examples': 3003, 'score': 1678.5971283912659, 'total_duration': 2963.592273950577, 'global_step': 3763, 'preemption_count': 0}), (5645, {'train/accuracy': 0.591108071135431, 'train/loss': 2.132342075638395, 'train/bleu': 28.232287752538152, 'validation/accuracy': 0.6068740623178882, 'validation/loss': 2.0010837590358457, 'validation/bleu': 24.901913227485654, 'validation/num_examples': 3000, 'test/accuracy': 0.6118645052582651, 'test/loss': 1.9652197359246992, 'test/bleu': 23.62578021473685, 'test/num_examples': 3003, 'score': 2515.6577582359314, 'total_duration': 4208.372834920883, 'global_step': 5645, 'preemption_count': 0}), (7528, {'train/accuracy': 0.6080287542496079, 'train/loss': 1.9882593849085977, 'train/bleu': 29.40981254221714, 'validation/accuracy': 0.6207858551040905, 'validation/loss': 1.885847664629081, 'validation/bleu': 25.31446233219718, 'validation/num_examples': 3000, 'test/accuracy': 0.6274243216547557, 'test/loss': 1.837021926384289, 'test/bleu': 24.590162980484852, 'test/num_examples': 3003, 'score': 3353.0074841976166, 'total_duration': 5493.510671615601, 'global_step': 7528, 'preemption_count': 0}), (9412, {'train/accuracy': 0.6176886900141546, 'train/loss': 1.9140728448472673, 'train/bleu': 29.96804324387166, 'validation/accuracy': 0.6317466615417043, 'validation/loss': 1.8025218224200568, 'validation/bleu': 25.85691164719861, 'validation/num_examples': 3000, 'test/accuracy': 0.6390796583580268, 'test/loss': 1.7482373191563534, 'test/bleu': 25.134911808003622, 'test/num_examples': 3003, 'score': 4190.024912595749, 'total_duration': 6741.755692243576, 'global_step': 9412, 'preemption_count': 0}), (11297, {'train/accuracy': 0.6205177792358785, 'train/loss': 1.9049848338340758, 'train/bleu': 30.363084981515268, 'validation/accuracy': 0.6363343293945518, 'validation/loss': 1.7627761125094543, 'validation/bleu': 26.68394796520704, 'validation/num_examples': 3000, 'test/accuracy': 0.6442159084306548, 'test/loss': 1.6984998692696531, 'test/bleu': 25.76829539435683, 'test/num_examples': 3003, 'score': 5027.360723018646, 'total_duration': 8020.116785287857, 'global_step': 11297, 'preemption_count': 0}), (13181, {'train/accuracy': 0.635065084667665, 'train/loss': 1.7839318626886302, 'train/bleu': 30.303004377868557, 'validation/accuracy': 0.641492355953429, 'validation/loss': 1.7211949479857658, 'validation/bleu': 26.895621604215595, 'validation/num_examples': 3000, 'test/accuracy': 0.6506768926849108, 'test/loss': 1.6558378362675032, 'test/bleu': 25.879161306474046, 'test/num_examples': 3003, 'score': 5864.462735891342, 'total_duration': 9249.997861385345, 'global_step': 13181, 'preemption_count': 0}), (15065, {'train/accuracy': 0.6277439860971371, 'train/loss': 1.826714955581725, 'train/bleu': 30.243204632185584, 'validation/accuracy': 0.6455716606117717, 'validation/loss': 1.6961295814683017, 'validation/bleu': 27.18406075423392, 'validation/num_examples': 3000, 'test/accuracy': 0.6541630352681425, 'test/loss': 1.6337200772761606, 'test/bleu': 26.15211267921952, 'test/num_examples': 3003, 'score': 6701.520494222641, 'total_duration': 10490.485873699188, 'global_step': 15065, 'preemption_count': 0}), (16948, {'train/accuracy': 0.633547459580343, 'train/loss': 1.7974581541024384, 'train/bleu': 30.368111584640875, 'validation/accuracy': 0.6489566155410348, 'validation/loss': 1.6712073858352656, 'validation/bleu': 27.385469398600947, 'validation/num_examples': 3000, 'test/accuracy': 0.65748649119749, 'test/loss': 1.606301928998896, 'test/bleu': 26.4282240866204, 'test/num_examples': 3003, 'score': 7538.793016910553, 'total_duration': 11718.234112262726, 'global_step': 16948, 'preemption_count': 0}), (18832, {'train/accuracy': 0.6640485950830779, 'train/loss': 1.5656629649086546, 'train/bleu': 32.90391735508681, 'validation/accuracy': 0.6498865482139093, 'validation/loss': 1.65164179613396, 'validation/bleu': 27.249080942212753, 'validation/num_examples': 3000, 'test/accuracy': 0.65939224914299, 'test/loss': 1.5874751249201093, 'test/bleu': 26.234322949103575, 'test/num_examples': 3003, 'score': 8375.910770893097, 'total_duration': 13009.963544368744, 'global_step': 18832, 'preemption_count': 0}), (20716, {'train/accuracy': 0.6326977465559712, 'train/loss': 1.7846968798467293, 'train/bleu': 31.069751621826267, 'validation/accuracy': 0.652329171367993, 'validation/loss': 1.6372586437241943, 'validation/bleu': 27.71484554816888, 'validation/num_examples': 3000, 'test/accuracy': 0.6624716750915113, 'test/loss': 1.5725963773168323, 'test/bleu': 26.766986118637664, 'test/num_examples': 3003, 'score': 9212.968908548355, 'total_duration': 14253.356783628464, 'global_step': 20716, 'preemption_count': 0}), (22602, {'train/accuracy': 0.6362604608506248, 'train/loss': 1.7624987461309183, 'train/bleu': 30.81292269442534, 'validation/accuracy': 0.6548709873405165, 'validation/loss': 1.6265328390224547, 'validation/bleu': 27.360154841242807, 'validation/num_examples': 3000, 'test/accuracy': 0.6649584568008832, 'test/loss': 1.5498051754691766, 'test/bleu': 27.05637108543675, 'test/num_examples': 3003, 'score': 10050.139518499374, 'total_duration': 15542.920112371445, 'global_step': 22602, 'preemption_count': 0}), (24484, {'train/accuracy': 0.639632164834405, 'train/loss': 1.743467166217244, 'train/bleu': 30.945332085897736, 'validation/accuracy': 0.6566936553793505, 'validation/loss': 1.6154009637202267, 'validation/bleu': 27.60159964228423, 'validation/num_examples': 3000, 'test/accuracy': 0.6682819127302306, 'test/loss': 1.5415887731683227, 'test/bleu': 27.04761601112965, 'test/num_examples': 3003, 'score': 10887.133499383926, 'total_duration': 16922.808218717575, 'global_step': 24484, 'preemption_count': 0}), (26368, {'train/accuracy': 0.6435315422024305, 'train/loss': 1.7101494254182659, 'train/bleu': 31.84420835723053, 'validation/accuracy': 0.6583675341905246, 'validation/loss': 1.6013241078845892, 'validation/bleu': 27.97728314014912, 'validation/num_examples': 3000, 'test/accuracy': 0.6686770088896636, 'test/loss': 1.5315568531752948, 'test/bleu': 27.326146927344798, 'test/num_examples': 3003, 'score': 11724.204772472382, 'total_duration': 18338.35507965088, 'global_step': 26368, 'preemption_count': 0}), (28254, {'train/accuracy': 0.641522881151357, 'train/loss': 1.7329014618992333, 'train/bleu': 31.15583572630349, 'validation/accuracy': 0.6602893950477985, 'validation/loss': 1.5915061499547434, 'validation/bleu': 28.33024579429472, 'validation/num_examples': 3000, 'test/accuracy': 0.6723258381267794, 'test/loss': 1.5165065219917495, 'test/bleu': 27.894723976455527, 'test/num_examples': 3003, 'score': 12561.351467847824, 'total_duration': 19671.249791383743, 'global_step': 28254, 'preemption_count': 0}), (30140, {'train/accuracy': 0.6369845715860042, 'train/loss': 1.7452464212737626, 'train/bleu': 31.45834537771507, 'validation/accuracy': 0.6611821304137581, 'validation/loss': 1.591758975400181, 'validation/bleu': 28.20256328892751, 'validation/num_examples': 3000, 'test/accuracy': 0.670989483469874, 'test/loss': 1.5102646926384289, 'test/bleu': 27.383034951732817, 'test/num_examples': 3003, 'score': 13398.354296922684, 'total_duration': 20975.684142827988, 'global_step': 30140, 'preemption_count': 0}), (32025, {'train/accuracy': 0.6486923050486304, 'train/loss': 1.6533754553734061, 'train/bleu': 31.824115465390307, 'validation/accuracy': 0.6633271751125218, 'validation/loss': 1.5779239020594908, 'validation/bleu': 28.419839896241655, 'validation/num_examples': 3000, 'test/accuracy': 0.6742664574981116, 'test/loss': 1.5012134318168613, 'test/bleu': 27.903679986035026, 'test/num_examples': 3003, 'score': 14235.516757249832, 'total_duration': 22271.267604112625, 'global_step': 32025, 'preemption_count': 0}), (33911, {'train/accuracy': 0.6416286779941981, 'train/loss': 1.7091760271331216, 'train/bleu': 31.39044707695118, 'validation/accuracy': 0.6643810988084463, 'validation/loss': 1.5696378144722323, 'validation/bleu': 28.72874202223925, 'validation/num_examples': 3000, 'test/accuracy': 0.6757073964325141, 'test/loss': 1.4945248496601011, 'test/bleu': 27.844046875534083, 'test/num_examples': 3003, 'score': 15072.860785007477, 'total_duration': 23577.426116228104, 'global_step': 33911, 'preemption_count': 0}), (35796, {'train/accuracy': 0.6428457743116346, 'train/loss': 1.712694117914554, 'train/bleu': 31.421648340938546, 'validation/accuracy': 0.664703475468376, 'validation/loss': 1.56033064143656, 'validation/bleu': 28.29975325954969, 'validation/num_examples': 3000, 'test/accuracy': 0.6755679507291849, 'test/loss': 1.487943666114694, 'test/bleu': 27.785051570551563, 'test/num_examples': 3003, 'score': 15909.992570638657, 'total_duration': 25045.485095739365, 'global_step': 35796, 'preemption_count': 0}), (37681, {'train/accuracy': 0.671919935580352, 'train/loss': 1.5098912199470838, 'train/bleu': 33.61026640561844, 'validation/accuracy': 0.6660921749265353, 'validation/loss': 1.5491124567581307, 'validation/bleu': 28.443428893862272, 'validation/num_examples': 3000, 'test/accuracy': 0.6765208297019348, 'test/loss': 1.474751793910871, 'test/bleu': 27.80606736956398, 'test/num_examples': 3003, 'score': 16747.352998018265, 'total_duration': 26519.888184309006, 'global_step': 37681, 'preemption_count': 0}), (39568, {'train/accuracy': 0.6508180193391687, 'train/loss': 1.6613638739631547, 'train/bleu': 31.339324881152066, 'validation/accuracy': 0.6661789686426702, 'validation/loss': 1.5512650571598616, 'validation/bleu': 28.764847312019285, 'validation/num_examples': 3000, 'test/accuracy': 0.6782057986171635, 'test/loss': 1.4734995061298006, 'test/bleu': 28.26857949831251, 'test/num_examples': 3003, 'score': 17584.78200507164, 'total_duration': 27798.35440635681, 'global_step': 39568, 'preemption_count': 0}), (41455, {'train/accuracy': 0.6525691200605984, 'train/loss': 1.6488270893826538, 'train/bleu': 31.48487368474806, 'validation/accuracy': 0.6677784528400144, 'validation/loss': 1.538154362624146, 'validation/bleu': 28.775147697427983, 'validation/num_examples': 3000, 'test/accuracy': 0.6802161408401604, 'test/loss': 1.4610410856429028, 'test/bleu': 28.31390810819476, 'test/num_examples': 3003, 'score': 18422.202785730362, 'total_duration': 29159.36374759674, 'global_step': 41455, 'preemption_count': 0}), (43341, {'train/accuracy': 0.6498793040626708, 'train/loss': 1.6697952011978503, 'train/bleu': 31.682639234649624, 'validation/accuracy': 0.6696631163903733, 'validation/loss': 1.5334567496063285, 'validation/bleu': 28.857259466198116, 'validation/num_examples': 3000, 'test/accuracy': 0.6799837313346115, 'test/loss': 1.4507416223636047, 'test/bleu': 28.296255170102853, 'test/num_examples': 3003, 'score': 19259.46638226509, 'total_duration': 30696.156210660934, 'global_step': 43341, 'preemption_count': 0}), (45227, {'train/accuracy': 0.6576458073592826, 'train/loss': 1.6054222379814016, 'train/bleu': 32.688935887767606, 'validation/accuracy': 0.6708162329047377, 'validation/loss': 1.5217309766772886, 'validation/bleu': 29.02580262527768, 'validation/num_examples': 3000, 'test/accuracy': 0.6822032421126024, 'test/loss': 1.4457235743129393, 'test/bleu': 28.438519967009317, 'test/num_examples': 3003, 'score': 20096.69379258156, 'total_duration': 31991.189598083496, 'global_step': 45227, 'preemption_count': 0}), (47114, {'train/accuracy': 0.6555906334461378, 'train/loss': 1.6351791929750845, 'train/bleu': 31.780830959813994, 'validation/accuracy': 0.6721429368513719, 'validation/loss': 1.51631605621753, 'validation/bleu': 28.986638466231803, 'validation/num_examples': 3000, 'test/accuracy': 0.6838417291267213, 'test/loss': 1.4315676565859043, 'test/bleu': 28.45079716450238, 'test/num_examples': 3003, 'score': 20934.00356388092, 'total_duration': 33303.281521081924, 'global_step': 47114, 'preemption_count': 0}), (49000, {'train/accuracy': 0.6520108403563138, 'train/loss': 1.6498292615293135, 'train/bleu': 32.037927753609026, 'validation/accuracy': 0.6714733853269024, 'validation/loss': 1.5145914185812948, 'validation/bleu': 28.814124929990705, 'validation/num_examples': 3000, 'test/accuracy': 0.6838998315031085, 'test/loss': 1.4304014329498576, 'test/bleu': 28.539512504185197, 'test/num_examples': 3003, 'score': 21771.408380508423, 'total_duration': 34669.606892347336, 'global_step': 49000, 'preemption_count': 0}), (50887, {'train/accuracy': 0.6649973582044979, 'train/loss': 1.5588849397555764, 'train/bleu': 33.24875537321265, 'validation/accuracy': 0.6747591474377255, 'validation/loss': 1.5004754280790071, 'validation/bleu': 29.048216384175547, 'validation/num_examples': 3000, 'test/accuracy': 0.6865376793910871, 'test/loss': 1.4172163696182674, 'test/bleu': 28.927441072849195, 'test/num_examples': 3003, 'score': 22608.923993110657, 'total_duration': 36014.4790186882, 'global_step': 50887, 'preemption_count': 0}), (52773, {'train/accuracy': 0.6537821637760102, 'train/loss': 1.6232979764455182, 'train/bleu': 32.69315989250041, 'validation/accuracy': 0.6747467483354205, 'validation/loss': 1.4932340617289308, 'validation/bleu': 29.18004195542679, 'validation/num_examples': 3000, 'test/accuracy': 0.6880367207018767, 'test/loss': 1.4092450867177968, 'test/bleu': 28.81291253458141, 'test/num_examples': 3003, 'score': 23446.132752418518, 'total_duration': 37328.243356227875, 'global_step': 52773, 'preemption_count': 0}), (54659, {'train/accuracy': 0.6599579659158404, 'train/loss': 1.6021398618472154, 'train/bleu': 32.783600043441396, 'validation/accuracy': 0.6782680933900386, 'validation/loss': 1.484899249544333, 'validation/bleu': 29.385626929144074, 'validation/num_examples': 3000, 'test/accuracy': 0.6904305386090291, 'test/loss': 1.397283078409157, 'test/bleu': 29.021750824657104, 'test/num_examples': 3003, 'score': 24283.19214296341, 'total_duration': 38740.14200592041, 'global_step': 54659, 'preemption_count': 0}), (56546, {'train/accuracy': 0.6764106672770974, 'train/loss': 1.4814878533821678, 'train/bleu': 34.03051830462946, 'validation/accuracy': 0.6771149768756742, 'validation/loss': 1.4801188143978377, 'validation/bleu': 29.34683276584154, 'validation/num_examples': 3000, 'test/accuracy': 0.6911045261751205, 'test/loss': 1.3910838272035326, 'test/bleu': 29.163399445160433, 'test/num_examples': 3003, 'score': 25120.42523241043, 'total_duration': 40102.14168381691, 'global_step': 56546, 'preemption_count': 0}), (58433, {'train/accuracy': 0.6656634494528431, 'train/loss': 1.549663215945388, 'train/bleu': 33.205141534789924, 'validation/accuracy': 0.6780325104462437, 'validation/loss': 1.4712561685533967, 'validation/bleu': 29.12705554202531, 'validation/num_examples': 3000, 'test/accuracy': 0.6911858695020626, 'test/loss': 1.384688752832491, 'test/bleu': 29.15487267926438, 'test/num_examples': 3003, 'score': 25957.60601425171, 'total_duration': 41379.57227778435, 'global_step': 58433, 'preemption_count': 0}), (60319, {'train/accuracy': 0.6641775660260668, 'train/loss': 1.5701677279703141, 'train/bleu': 33.01858437448892, 'validation/accuracy': 0.6799667704058227, 'validation/loss': 1.4605778291651685, 'validation/bleu': 29.50704644527715, 'validation/num_examples': 3000, 'test/accuracy': 0.6941490906978095, 'test/loss': 1.3729910195514496, 'test/bleu': 29.397786145019566, 'test/num_examples': 3003, 'score': 26794.706813573837, 'total_duration': 42826.67201542854, 'global_step': 60319, 'preemption_count': 0}), (62207, {'train/accuracy': 0.6632315347393408, 'train/loss': 1.5727017926024833, 'train/bleu': 33.26628953667495, 'validation/accuracy': 0.682868160345191, 'validation/loss': 1.4571556769289904, 'validation/bleu': 30.05583602851255, 'validation/num_examples': 3000, 'test/accuracy': 0.6960664691185869, 'test/loss': 1.362349569316135, 'test/bleu': 29.914898432391468, 'test/num_examples': 3003, 'score': 27632.065143823624, 'total_duration': 44206.572842121124, 'global_step': 62207, 'preemption_count': 0}), (64093, {'train/accuracy': 0.6705376245666624, 'train/loss': 1.5086785493632942, 'train/bleu': 33.6690085223432, 'validation/accuracy': 0.6832153352097308, 'validation/loss': 1.4497123408265242, 'validation/bleu': 29.596739206528152, 'validation/num_examples': 3000, 'test/accuracy': 0.6956481320085992, 'test/loss': 1.358672869095346, 'test/bleu': 29.694214431085353, 'test/num_examples': 3003, 'score': 28469.380393981934, 'total_duration': 45535.92010116577, 'global_step': 64093, 'preemption_count': 0}), (65979, {'train/accuracy': 0.6692088250856972, 'train/loss': 1.5390015701566706, 'train/bleu': 33.304574360100595, 'validation/accuracy': 0.6841204696779953, 'validation/loss': 1.4440724929015138, 'validation/bleu': 29.811458152345917, 'validation/num_examples': 3000, 'test/accuracy': 0.6994364069490442, 'test/loss': 1.3471531651269537, 'test/bleu': 29.822457678398095, 'test/num_examples': 3003, 'score': 29306.474890232086, 'total_duration': 46894.01426196098, 'global_step': 65979, 'preemption_count': 0}), (67865, {'train/accuracy': 0.6701912931785862, 'train/loss': 1.527198602860223, 'train/bleu': 33.442483166407065, 'validation/accuracy': 0.684591635565585, 'validation/loss': 1.433192474364856, 'validation/bleu': 29.955462694439838, 'validation/num_examples': 3000, 'test/accuracy': 0.6992737202951601, 'test/loss': 1.3414754735343675, 'test/bleu': 29.711195470527247, 'test/num_examples': 3003, 'score': 30143.57747745514, 'total_duration': 48341.32200026512, 'global_step': 67865, 'preemption_count': 0}), (69752, {'train/accuracy': 0.6795271900042297, 'train/loss': 1.4685161885409879, 'train/bleu': 34.25280051311348, 'validation/accuracy': 0.6868854694920088, 'validation/loss': 1.4277715480899182, 'validation/bleu': 29.864097863626423, 'validation/num_examples': 3000, 'test/accuracy': 0.700145255940968, 'test/loss': 1.3338085018302248, 'test/bleu': 30.137383723897436, 'test/num_examples': 3003, 'score': 30981.053389787674, 'total_duration': 49660.04925894737, 'global_step': 69752, 'preemption_count': 0}), (71639, {'train/accuracy': 0.6774223045182823, 'train/loss': 1.4866812720742035, 'train/bleu': 34.104412110220125, 'validation/accuracy': 0.686389505399809, 'validation/loss': 1.424724178094506, 'validation/bleu': 30.129032875389193, 'validation/num_examples': 3000, 'test/accuracy': 0.7015048515484283, 'test/loss': 1.327286328510836, 'test/bleu': 29.81887507986993, 'test/num_examples': 3003, 'score': 31818.37394952774, 'total_duration': 51041.41429781914, 'global_step': 71639, 'preemption_count': 0}), (73524, {'train/accuracy': 0.6775757990711262, 'train/loss': 1.4831811895918203, 'train/bleu': 33.56239893551465, 'validation/accuracy': 0.6870466578219737, 'validation/loss': 1.417836961104016, 'validation/bleu': 30.38759937487197, 'validation/num_examples': 3000, 'test/accuracy': 0.7041891813375167, 'test/loss': 1.31781700293417, 'test/bleu': 30.14620272293587, 'test/num_examples': 3003, 'score': 32655.83547616005, 'total_duration': 52414.89657640457, 'global_step': 73524, 'preemption_count': 0}), (75410, {'train/accuracy': 0.6901934790807173, 'train/loss': 1.4040593608219147, 'train/bleu': 34.71339255725569, 'validation/accuracy': 0.6889809177815526, 'validation/loss': 1.4110492275359263, 'validation/bleu': 30.461296298804637, 'validation/num_examples': 3000, 'test/accuracy': 0.7050026146069375, 'test/loss': 1.311849798094242, 'test/bleu': 30.252368522856493, 'test/num_examples': 3003, 'score': 33493.08545279503, 'total_duration': 53722.11799836159, 'global_step': 75410, 'preemption_count': 0}), (77296, {'train/accuracy': 0.6826583999815613, 'train/loss': 1.4543589885795285, 'train/bleu': 34.424077391178855, 'validation/accuracy': 0.6892908953391774, 'validation/loss': 1.4044031149644767, 'validation/bleu': 30.529621984216252, 'validation/num_examples': 3000, 'test/accuracy': 0.7059903550055198, 'test/loss': 1.3042375697228517, 'test/bleu': 30.433690732885772, 'test/num_examples': 3003, 'score': 34330.24255776405, 'total_duration': 55043.2241692543, 'global_step': 77296, 'preemption_count': 0}), (79182, {'train/accuracy': 0.6818124491223243, 'train/loss': 1.4520943687728591, 'train/bleu': 34.37674920217998, 'validation/accuracy': 0.6899852450682571, 'validation/loss': 1.400145379474526, 'validation/bleu': 30.336275416897152, 'validation/num_examples': 3000, 'test/accuracy': 0.7070478182557667, 'test/loss': 1.297778582737784, 'test/bleu': 30.606031842070642, 'test/num_examples': 3003, 'score': 35167.6033205986, 'total_duration': 56437.60001420975, 'global_step': 79182, 'preemption_count': 0}), (81067, {'train/accuracy': 0.6804222758518146, 'train/loss': 1.4670655466882456, 'train/bleu': 34.355315337154224, 'validation/accuracy': 0.6913119490148913, 'validation/loss': 1.3955473467470956, 'validation/bleu': 30.51700880394276, 'validation/num_examples': 3000, 'test/accuracy': 0.7067340654232758, 'test/loss': 1.2923299416071117, 'test/bleu': 30.45410239126915, 'test/num_examples': 3003, 'score': 36004.74405288696, 'total_duration': 57842.452931404114, 'global_step': 81067, 'preemption_count': 0}), (82954, {'train/accuracy': 0.6871974858348817, 'train/loss': 1.4204561583855206, 'train/bleu': 34.50536646949436, 'validation/accuracy': 0.691820312209396, 'validation/loss': 1.3914246452306853, 'validation/bleu': 30.67833187300776, 'validation/num_examples': 3000, 'test/accuracy': 0.7079193539015746, 'test/loss': 1.2890810201324734, 'test/bleu': 30.620050897445417, 'test/num_examples': 3003, 'score': 36842.16889357567, 'total_duration': 59231.259644031525, 'global_step': 82954, 'preemption_count': 0}), (84840, {'train/accuracy': 0.68675340926207, 'train/loss': 1.421291884392126, 'train/bleu': 34.74114605682637, 'validation/accuracy': 0.6932586080767752, 'validation/loss': 1.3844049128343108, 'validation/bleu': 30.63959953742076, 'validation/num_examples': 3000, 'test/accuracy': 0.7098134913717971, 'test/loss': 1.2796984849805357, 'test/bleu': 30.72195270285195, 'test/num_examples': 3003, 'score': 37679.3202726841, 'total_duration': 60532.80043172836, 'global_step': 84840, 'preemption_count': 0}), (86726, {'train/accuracy': 0.6846408409577575, 'train/loss': 1.430961149789875, 'train/bleu': 34.58290761942374, 'validation/accuracy': 0.6922790789946808, 'validation/loss': 1.3826513116700352, 'validation/bleu': 30.66834502775578, 'validation/num_examples': 3000, 'test/accuracy': 0.7096159432920807, 'test/loss': 1.2784579992446692, 'test/bleu': 30.872431032250816, 'test/num_examples': 3003, 'score': 38516.441675424576, 'total_duration': 61929.962465286255, 'global_step': 86726, 'preemption_count': 0}), (88611, {'train/accuracy': 0.6934860168028542, 'train/loss': 1.3810592955173207, 'train/bleu': 34.965156857212136, 'validation/accuracy': 0.6939653569081599, 'validation/loss': 1.3805424955983188, 'validation/bleu': 30.71567532633703, 'validation/num_examples': 3000, 'test/accuracy': 0.7098832142234618, 'test/loss': 1.276646385305909, 'test/bleu': 30.765114074347082, 'test/num_examples': 3003, 'score': 39353.67106842995, 'total_duration': 63312.01738405228, 'global_step': 88611, 'preemption_count': 0}), (90497, {'train/accuracy': 0.687704974428365, 'train/loss': 1.4131074159174828, 'train/bleu': 35.28538919280296, 'validation/accuracy': 0.6950812761156092, 'validation/loss': 1.377949920800734, 'validation/bleu': 30.585769657626205, 'validation/num_examples': 3000, 'test/accuracy': 0.7104177560862239, 'test/loss': 1.2749721290163267, 'test/bleu': 30.640379388437445, 'test/num_examples': 3003, 'score': 40191.033319711685, 'total_duration': 64705.97496795654, 'global_step': 90497, 'preemption_count': 0}), (92382, {'train/accuracy': 0.690828436192349, 'train/loss': 1.4009928679429722, 'train/bleu': 34.86268077586935, 'validation/accuracy': 0.6951432716271342, 'validation/loss': 1.375137165069249, 'validation/bleu': 30.694537444259254, 'validation/num_examples': 3000, 'test/accuracy': 0.7113009122073093, 'test/loss': 1.2705107743594213, 'test/bleu': 30.855302734006802, 'test/num_examples': 3003, 'score': 41028.206060409546, 'total_duration': 66149.14731693268, 'global_step': 92382, 'preemption_count': 0}), (94266, {'train/accuracy': 0.694521703447018, 'train/loss': 1.3807012156894036, 'train/bleu': 34.789253565509306, 'validation/accuracy': 0.6951680698317442, 'validation/loss': 1.3736450106012326, 'validation/bleu': 30.804896484404235, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938760095287, 'test/loss': 1.2700047389750742, 'test/bleu': 30.937181146546003, 'test/num_examples': 3003, 'score': 41865.315314531326, 'total_duration': 67535.17105436325, 'global_step': 94266, 'preemption_count': 0}), (96153, {'train/accuracy': 0.6921132977324445, 'train/loss': 1.3967991718219346, 'train/bleu': 34.98858866771025, 'validation/accuracy': 0.6953912536732341, 'validation/loss': 1.3724778482287883, 'validation/bleu': 30.783272186813228, 'validation/num_examples': 3000, 'test/accuracy': 0.7113706350589739, 'test/loss': 1.2684933509093022, 'test/bleu': 31.05621347117347, 'test/num_examples': 3003, 'score': 42702.3412501812, 'total_duration': 68940.82863497734, 'global_step': 96153, 'preemption_count': 0}), (98040, {'train/accuracy': 0.690156016145857, 'train/loss': 1.4037496208728286, 'train/bleu': 34.98944598731898, 'validation/accuracy': 0.6952300653432691, 'validation/loss': 1.3723590880770231, 'validation/bleu': 30.755706125476443, 'validation/num_examples': 3000, 'test/accuracy': 0.7115100807623032, 'test/loss': 1.2680101933356573, 'test/bleu': 31.042565142038715, 'test/num_examples': 3003, 'score': 43539.6594979763, 'total_duration': 70315.07867670059, 'global_step': 98040, 'preemption_count': 0}), (99928, {'train/accuracy': 0.6915597491513722, 'train/loss': 1.4004351914446809, 'train/bleu': 35.114968149692174, 'validation/accuracy': 0.6952300653432691, 'validation/loss': 1.3725366470967502, 'validation/bleu': 30.723493736343755, 'validation/num_examples': 3000, 'test/accuracy': 0.7118005926442391, 'test/loss': 1.2678377019057578, 'test/bleu': 31.042933762858322, 'test/num_examples': 3003, 'score': 44376.73819708824, 'total_duration': 71715.02557206154, 'global_step': 99928, 'preemption_count': 0}), (101814, {'train/accuracy': 0.6902809838414391, 'train/loss': 1.3996011885809805, 'train/bleu': 34.88556666678789, 'validation/accuracy': 0.695428450980149, 'validation/loss': 1.3727360982814845, 'validation/bleu': 30.746952220606925, 'validation/num_examples': 3000, 'test/accuracy': 0.7117773516936843, 'test/loss': 1.2681385632734878, 'test/bleu': 31.018640058088316, 'test/num_examples': 3003, 'score': 45213.89119100571, 'total_duration': 73110.09517669678, 'global_step': 101814, 'preemption_count': 0}), (103699, {'train/accuracy': 0.6905433330669376, 'train/loss': 1.4021991203233282, 'train/bleu': 34.85612488584991, 'validation/accuracy': 0.6953292581617091, 'validation/loss': 1.3724982873739942, 'validation/bleu': 30.78517277616655, 'validation/num_examples': 3000, 'test/accuracy': 0.7117076288420197, 'test/loss': 1.2672779218232526, 'test/bleu': 31.089071774482054, 'test/num_examples': 3003, 'score': 46051.03284859657, 'total_duration': 74510.61347007751, 'global_step': 103699, 'preemption_count': 0}), (105584, {'train/accuracy': 0.6929132048585973, 'train/loss': 1.3884049001103183, 'train/bleu': 35.04976320408652, 'validation/accuracy': 0.6949696841948643, 'validation/loss': 1.3726653846511512, 'validation/bleu': 30.690963762216814, 'validation/num_examples': 3000, 'test/accuracy': 0.7116843878914647, 'test/loss': 1.2674456924350705, 'test/bleu': 30.93839563377724, 'test/num_examples': 3003, 'score': 46888.10856938362, 'total_duration': 75907.62011122704, 'global_step': 105584, 'preemption_count': 0}), (107469, {'train/accuracy': 0.6939958403891395, 'train/loss': 1.3823707721987977, 'train/bleu': 34.85290448733799, 'validation/accuracy': 0.695428450980149, 'validation/loss': 1.374405811769228, 'validation/bleu': 30.85186188009511, 'validation/num_examples': 3000, 'test/accuracy': 0.7112892917320318, 'test/loss': 1.269347365057231, 'test/bleu': 30.99902221060933, 'test/num_examples': 3003, 'score': 47725.23795628548, 'total_duration': 77291.1069560051, 'global_step': 107469, 'preemption_count': 0})], 'global_step': 107469}
I0316 15:22:26.801353 140515296233280 submission_runner.py:526] Timing: 47725.23795628548
I0316 15:22:26.801400 140515296233280 submission_runner.py:527] ====================
I0316 15:22:26.801522 140515296233280 submission_runner.py:586] Final wmt score: 47725.23795628548
