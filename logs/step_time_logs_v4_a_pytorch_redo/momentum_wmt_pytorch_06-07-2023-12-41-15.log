torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_pytorch_06-07-2023-12-41-15.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 12:41:39.328299 140366398871360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 12:41:39.328330 140064360167232 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 12:41:39.328354 140136695260992 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 12:41:39.328426 140266544502592 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 12:41:39.328956 139637218592576 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 12:41:39.330001 140557269555008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 12:41:40.316377 139691226240832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 12:41:40.323025 139627451610944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 12:41:40.323495 139627451610944 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.327205 139691226240832 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.331592 140366398871360 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.331614 140064360167232 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.331636 140136695260992 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.331693 140557269555008 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.331674 140266544502592 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:40.331716 139637218592576 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:41:45.149507 139627451610944 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch because --overwrite was set.
I0607 12:41:45.194205 139627451610944 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch.
W0607 12:41:45.276811 140136695260992 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.278120 140366398871360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.278291 140064360167232 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.278841 139637218592576 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.279179 140557269555008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.279995 139627451610944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.280157 140266544502592 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:41:45.280569 139691226240832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 12:41:45.287456 139627451610944 submission_runner.py:541] Using RNG seed 3808407046
I0607 12:41:45.289189 139627451610944 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 12:41:45.289309 139627451610944 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch/trial_1.
I0607 12:41:45.289609 139627451610944 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch/trial_1/hparams.json.
I0607 12:41:45.290534 139627451610944 submission_runner.py:255] Initializing dataset.
I0607 12:41:45.290646 139627451610944 submission_runner.py:262] Initializing model.
I0607 12:41:48.987902 139627451610944 submission_runner.py:272] Initializing optimizer.
I0607 12:41:49.485759 139627451610944 submission_runner.py:279] Initializing metrics bundle.
I0607 12:41:49.485981 139627451610944 submission_runner.py:297] Initializing checkpoint and logger.
I0607 12:41:49.489772 139627451610944 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 12:41:49.489893 139627451610944 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 12:41:49.961989 139627451610944 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch/trial_1/meta_data_0.json.
I0607 12:41:49.962916 139627451610944 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch/trial_1/flags_0.json.
I0607 12:41:50.016249 139627451610944 submission_runner.py:332] Starting training loop.
I0607 12:41:50.029522 139627451610944 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:41:50.033480 139627451610944 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:41:50.033606 139627451610944 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:41:50.105798 139627451610944 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:41:54.541929 139587460519680 logging_writer.py:48] [0] global_step=0, grad_norm=4.559177, loss=11.067800
I0607 12:41:54.550737 139627451610944 spec.py:298] Evaluating on the training split.
I0607 12:41:54.553471 139627451610944 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:41:54.556308 139627451610944 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:41:54.556473 139627451610944 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:41:54.586302 139627451610944 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:41:58.773896 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 12:46:34.134799 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 12:46:34.138452 139627451610944 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:46:34.141913 139627451610944 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:46:34.142031 139627451610944 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:46:34.170461 139627451610944 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:46:38.005733 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 12:51:08.154296 139627451610944 spec.py:326] Evaluating on the test split.
I0607 12:51:08.157146 139627451610944 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:51:08.160039 139627451610944 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:51:08.160176 139627451610944 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:51:08.188443 139627451610944 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:51:12.119223 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 12:55:48.003150 139627451610944 submission_runner.py:419] Time since start: 837.99s, 	Step: 1, 	{'train/accuracy': 0.0005963029218843172, 'train/loss': 11.076557554699326, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.069143594003794, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.071717942013828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.534735918045044, 'total_duration': 837.9874379634857, 'accumulated_submission_time': 4.534735918045044, 'accumulated_eval_time': 833.4523410797119, 'accumulated_logging_time': 0}
I0607 12:55:48.025776 139569474049792 logging_writer.py:48] [1] accumulated_eval_time=833.452341, accumulated_logging_time=0, accumulated_submission_time=4.534736, global_step=1, preemption_count=0, score=4.534736, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.071718, test/num_examples=3003, total_duration=837.987438, train/accuracy=0.000596, train/bleu=0.000000, train/loss=11.076558, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.069144, validation/num_examples=3000
I0607 12:55:48.042909 139627451610944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.042889 140266544502592 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.042907 139691226240832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.042950 140557269555008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.043007 139637218592576 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.043150 140366398871360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.043206 140064360167232 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.043214 140136695260992 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:55:48.473666 139569465657088 logging_writer.py:48] [1] global_step=1, grad_norm=4.522840, loss=11.055329
I0607 12:55:48.909484 139569474049792 logging_writer.py:48] [2] global_step=2, grad_norm=4.555836, loss=11.054090
I0607 12:55:49.347111 139569465657088 logging_writer.py:48] [3] global_step=3, grad_norm=4.538776, loss=11.054854
I0607 12:55:49.784530 139569474049792 logging_writer.py:48] [4] global_step=4, grad_norm=4.470667, loss=11.037377
I0607 12:55:50.219559 139569465657088 logging_writer.py:48] [5] global_step=5, grad_norm=4.427149, loss=11.009559
I0607 12:55:50.662231 139569474049792 logging_writer.py:48] [6] global_step=6, grad_norm=4.334689, loss=10.983946
I0607 12:55:51.098769 139569465657088 logging_writer.py:48] [7] global_step=7, grad_norm=4.256533, loss=10.939573
I0607 12:55:51.538168 139569474049792 logging_writer.py:48] [8] global_step=8, grad_norm=4.162225, loss=10.886156
I0607 12:55:51.979614 139569465657088 logging_writer.py:48] [9] global_step=9, grad_norm=3.976912, loss=10.816782
I0607 12:55:52.415063 139569474049792 logging_writer.py:48] [10] global_step=10, grad_norm=3.723191, loss=10.753861
I0607 12:55:52.851559 139569465657088 logging_writer.py:48] [11] global_step=11, grad_norm=3.431749, loss=10.680799
I0607 12:55:53.289104 139569474049792 logging_writer.py:48] [12] global_step=12, grad_norm=3.235261, loss=10.586923
I0607 12:55:53.729544 139569465657088 logging_writer.py:48] [13] global_step=13, grad_norm=2.979350, loss=10.517411
I0607 12:55:54.168247 139569474049792 logging_writer.py:48] [14] global_step=14, grad_norm=2.804343, loss=10.444522
I0607 12:55:54.602484 139569465657088 logging_writer.py:48] [15] global_step=15, grad_norm=2.532325, loss=10.358260
I0607 12:55:55.042297 139569474049792 logging_writer.py:48] [16] global_step=16, grad_norm=2.343956, loss=10.295407
I0607 12:55:55.478716 139569465657088 logging_writer.py:48] [17] global_step=17, grad_norm=2.170802, loss=10.207906
I0607 12:55:55.914532 139569474049792 logging_writer.py:48] [18] global_step=18, grad_norm=2.021483, loss=10.158232
I0607 12:55:56.354898 139569465657088 logging_writer.py:48] [19] global_step=19, grad_norm=1.880019, loss=10.081789
I0607 12:55:56.790457 139569474049792 logging_writer.py:48] [20] global_step=20, grad_norm=1.768756, loss=10.041348
I0607 12:55:57.226172 139569465657088 logging_writer.py:48] [21] global_step=21, grad_norm=1.651359, loss=9.968578
I0607 12:55:57.664467 139569474049792 logging_writer.py:48] [22] global_step=22, grad_norm=1.550084, loss=9.924771
I0607 12:55:58.103884 139569465657088 logging_writer.py:48] [23] global_step=23, grad_norm=1.469791, loss=9.881781
I0607 12:55:58.542801 139569474049792 logging_writer.py:48] [24] global_step=24, grad_norm=1.388598, loss=9.833681
I0607 12:55:58.981180 139569465657088 logging_writer.py:48] [25] global_step=25, grad_norm=1.322649, loss=9.799794
I0607 12:55:59.418788 139569474049792 logging_writer.py:48] [26] global_step=26, grad_norm=1.254677, loss=9.748859
I0607 12:55:59.853671 139569465657088 logging_writer.py:48] [27] global_step=27, grad_norm=1.192302, loss=9.725909
I0607 12:56:00.295104 139569474049792 logging_writer.py:48] [28] global_step=28, grad_norm=1.132843, loss=9.701538
I0607 12:56:00.737974 139569465657088 logging_writer.py:48] [29] global_step=29, grad_norm=1.117597, loss=9.653377
I0607 12:56:01.175897 139569474049792 logging_writer.py:48] [30] global_step=30, grad_norm=1.067239, loss=9.609550
I0607 12:56:01.615708 139569465657088 logging_writer.py:48] [31] global_step=31, grad_norm=1.023262, loss=9.587598
I0607 12:56:02.065433 139569474049792 logging_writer.py:48] [32] global_step=32, grad_norm=0.963048, loss=9.542564
I0607 12:56:02.510104 139569465657088 logging_writer.py:48] [33] global_step=33, grad_norm=0.906383, loss=9.541539
I0607 12:56:02.949753 139569474049792 logging_writer.py:48] [34] global_step=34, grad_norm=0.864399, loss=9.519336
I0607 12:56:03.387791 139569465657088 logging_writer.py:48] [35] global_step=35, grad_norm=0.807312, loss=9.515195
I0607 12:56:03.826989 139569474049792 logging_writer.py:48] [36] global_step=36, grad_norm=0.770961, loss=9.487528
I0607 12:56:04.263909 139569465657088 logging_writer.py:48] [37] global_step=37, grad_norm=0.700930, loss=9.504488
I0607 12:56:04.703986 139569474049792 logging_writer.py:48] [38] global_step=38, grad_norm=0.688236, loss=9.435352
I0607 12:56:05.141612 139569465657088 logging_writer.py:48] [39] global_step=39, grad_norm=0.656537, loss=9.425319
I0607 12:56:05.578041 139569474049792 logging_writer.py:48] [40] global_step=40, grad_norm=0.647826, loss=9.405557
I0607 12:56:06.019589 139569465657088 logging_writer.py:48] [41] global_step=41, grad_norm=0.617391, loss=9.400242
I0607 12:56:06.459364 139569474049792 logging_writer.py:48] [42] global_step=42, grad_norm=0.595530, loss=9.394444
I0607 12:56:06.898525 139569465657088 logging_writer.py:48] [43] global_step=43, grad_norm=0.609299, loss=9.363392
I0607 12:56:07.337398 139569474049792 logging_writer.py:48] [44] global_step=44, grad_norm=0.556793, loss=9.369194
I0607 12:56:07.776916 139569465657088 logging_writer.py:48] [45] global_step=45, grad_norm=0.578552, loss=9.316332
I0607 12:56:08.214033 139569474049792 logging_writer.py:48] [46] global_step=46, grad_norm=0.556799, loss=9.306076
I0607 12:56:08.651324 139569465657088 logging_writer.py:48] [47] global_step=47, grad_norm=0.541530, loss=9.320805
I0607 12:56:09.087780 139569474049792 logging_writer.py:48] [48] global_step=48, grad_norm=0.513157, loss=9.319575
I0607 12:56:09.524075 139569465657088 logging_writer.py:48] [49] global_step=49, grad_norm=0.494955, loss=9.282631
I0607 12:56:09.959612 139569474049792 logging_writer.py:48] [50] global_step=50, grad_norm=0.498746, loss=9.274653
I0607 12:56:10.399240 139569465657088 logging_writer.py:48] [51] global_step=51, grad_norm=0.466137, loss=9.273865
I0607 12:56:10.839332 139569474049792 logging_writer.py:48] [52] global_step=52, grad_norm=0.451845, loss=9.291512
I0607 12:56:11.275358 139569465657088 logging_writer.py:48] [53] global_step=53, grad_norm=0.430885, loss=9.265107
I0607 12:56:11.711848 139569474049792 logging_writer.py:48] [54] global_step=54, grad_norm=0.422353, loss=9.240915
I0607 12:56:12.149054 139569465657088 logging_writer.py:48] [55] global_step=55, grad_norm=0.402455, loss=9.243598
I0607 12:56:12.587859 139569474049792 logging_writer.py:48] [56] global_step=56, grad_norm=0.387771, loss=9.253740
I0607 12:56:13.024878 139569465657088 logging_writer.py:48] [57] global_step=57, grad_norm=0.376209, loss=9.216166
I0607 12:56:13.466360 139569474049792 logging_writer.py:48] [58] global_step=58, grad_norm=0.361995, loss=9.228065
I0607 12:56:13.906903 139569465657088 logging_writer.py:48] [59] global_step=59, grad_norm=0.351804, loss=9.202557
I0607 12:56:14.346783 139569474049792 logging_writer.py:48] [60] global_step=60, grad_norm=0.343742, loss=9.211226
I0607 12:56:14.783955 139569465657088 logging_writer.py:48] [61] global_step=61, grad_norm=0.338158, loss=9.198891
I0607 12:56:15.221989 139569474049792 logging_writer.py:48] [62] global_step=62, grad_norm=0.341926, loss=9.193404
I0607 12:56:15.656566 139569465657088 logging_writer.py:48] [63] global_step=63, grad_norm=0.329099, loss=9.190610
I0607 12:56:16.095943 139569474049792 logging_writer.py:48] [64] global_step=64, grad_norm=0.339030, loss=9.203370
I0607 12:56:16.534611 139569465657088 logging_writer.py:48] [65] global_step=65, grad_norm=0.334986, loss=9.153563
I0607 12:56:16.969686 139569474049792 logging_writer.py:48] [66] global_step=66, grad_norm=0.327296, loss=9.167548
I0607 12:56:17.405763 139569465657088 logging_writer.py:48] [67] global_step=67, grad_norm=0.318046, loss=9.162155
I0607 12:56:17.840643 139569474049792 logging_writer.py:48] [68] global_step=68, grad_norm=0.293788, loss=9.148490
I0607 12:56:18.285969 139569465657088 logging_writer.py:48] [69] global_step=69, grad_norm=0.284926, loss=9.144851
I0607 12:56:18.730138 139569474049792 logging_writer.py:48] [70] global_step=70, grad_norm=0.267307, loss=9.139606
I0607 12:56:19.166642 139569465657088 logging_writer.py:48] [71] global_step=71, grad_norm=0.268926, loss=9.132474
I0607 12:56:19.608235 139569474049792 logging_writer.py:48] [72] global_step=72, grad_norm=0.263511, loss=9.132884
I0607 12:56:20.049374 139569465657088 logging_writer.py:48] [73] global_step=73, grad_norm=0.259086, loss=9.130727
I0607 12:56:20.484583 139569474049792 logging_writer.py:48] [74] global_step=74, grad_norm=0.256048, loss=9.098451
I0607 12:56:20.920213 139569465657088 logging_writer.py:48] [75] global_step=75, grad_norm=0.255945, loss=9.118859
I0607 12:56:21.355109 139569474049792 logging_writer.py:48] [76] global_step=76, grad_norm=0.246071, loss=9.115269
I0607 12:56:21.791148 139569465657088 logging_writer.py:48] [77] global_step=77, grad_norm=0.247492, loss=9.113481
I0607 12:56:22.227447 139569474049792 logging_writer.py:48] [78] global_step=78, grad_norm=0.241882, loss=9.094061
I0607 12:56:22.663892 139569465657088 logging_writer.py:48] [79] global_step=79, grad_norm=0.233370, loss=9.109542
I0607 12:56:23.100554 139569474049792 logging_writer.py:48] [80] global_step=80, grad_norm=0.236013, loss=9.093011
I0607 12:56:23.537269 139569465657088 logging_writer.py:48] [81] global_step=81, grad_norm=0.234324, loss=9.095121
I0607 12:56:23.978171 139569474049792 logging_writer.py:48] [82] global_step=82, grad_norm=0.229288, loss=9.076054
I0607 12:56:24.418286 139569465657088 logging_writer.py:48] [83] global_step=83, grad_norm=0.225927, loss=9.089521
I0607 12:56:24.856353 139569474049792 logging_writer.py:48] [84] global_step=84, grad_norm=0.220896, loss=9.095382
I0607 12:56:25.293583 139569465657088 logging_writer.py:48] [85] global_step=85, grad_norm=0.209446, loss=9.106234
I0607 12:56:25.728767 139569474049792 logging_writer.py:48] [86] global_step=86, grad_norm=0.212666, loss=9.082944
I0607 12:56:26.171793 139569465657088 logging_writer.py:48] [87] global_step=87, grad_norm=0.204188, loss=9.068411
I0607 12:56:26.615116 139569474049792 logging_writer.py:48] [88] global_step=88, grad_norm=0.207122, loss=9.039753
I0607 12:56:27.052920 139569465657088 logging_writer.py:48] [89] global_step=89, grad_norm=0.199188, loss=9.058938
I0607 12:56:27.488928 139569474049792 logging_writer.py:48] [90] global_step=90, grad_norm=0.186560, loss=9.080798
I0607 12:56:27.925420 139569465657088 logging_writer.py:48] [91] global_step=91, grad_norm=0.185823, loss=9.075209
I0607 12:56:28.366035 139569474049792 logging_writer.py:48] [92] global_step=92, grad_norm=0.184757, loss=9.031977
I0607 12:56:28.812079 139569465657088 logging_writer.py:48] [93] global_step=93, grad_norm=0.191976, loss=9.065481
I0607 12:56:29.248077 139569474049792 logging_writer.py:48] [94] global_step=94, grad_norm=0.183804, loss=9.059208
I0607 12:56:29.687157 139569465657088 logging_writer.py:48] [95] global_step=95, grad_norm=0.179965, loss=9.057531
I0607 12:56:30.128441 139569474049792 logging_writer.py:48] [96] global_step=96, grad_norm=0.185200, loss=9.016087
I0607 12:56:30.566508 139569465657088 logging_writer.py:48] [97] global_step=97, grad_norm=0.178348, loss=9.074381
I0607 12:56:31.009099 139569474049792 logging_writer.py:48] [98] global_step=98, grad_norm=0.168352, loss=9.054116
I0607 12:56:31.449168 139569465657088 logging_writer.py:48] [99] global_step=99, grad_norm=0.168513, loss=9.038870
I0607 12:56:31.890715 139569474049792 logging_writer.py:48] [100] global_step=100, grad_norm=0.169843, loss=9.041108
I0607 12:59:23.073728 139569465657088 logging_writer.py:48] [500] global_step=500, grad_norm=0.541827, loss=8.443109
I0607 13:02:57.309375 139569474049792 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.856055, loss=7.853391
I0607 13:06:31.587993 139569465657088 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.680836, loss=7.448571
I0607 13:09:48.218629 139627451610944 spec.py:298] Evaluating on the training split.
I0607 13:09:52.139079 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 13:14:10.526827 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 13:14:14.290798 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 13:18:36.367675 139627451610944 spec.py:326] Evaluating on the test split.
I0607 13:18:40.200665 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 13:23:09.611604 139627451610944 submission_runner.py:419] Time since start: 2479.60s, 	Step: 1960, 	{'train/accuracy': 0.27853436723286795, 'train/loss': 5.905021218229924, 'train/bleu': 5.289531557119159, 'validation/accuracy': 0.25689700065715243, 'validation/loss': 6.155588972858365, 'validation/bleu': 2.687248735256667, 'validation/num_examples': 3000, 'test/accuracy': 0.23548893149729824, 'test/loss': 6.4523575039219105, 'test/bleu': 1.894095097274457, 'test/num_examples': 3003, 'score': 844.0114667415619, 'total_duration': 2479.595888376236, 'accumulated_submission_time': 844.0114667415619, 'accumulated_eval_time': 1634.8452229499817, 'accumulated_logging_time': 0.03177785873413086}
I0607 13:23:09.621859 139569474049792 logging_writer.py:48] [1960] accumulated_eval_time=1634.845223, accumulated_logging_time=0.031778, accumulated_submission_time=844.011467, global_step=1960, preemption_count=0, score=844.011467, test/accuracy=0.235489, test/bleu=1.894095, test/loss=6.452358, test/num_examples=3003, total_duration=2479.595888, train/accuracy=0.278534, train/bleu=5.289532, train/loss=5.905021, validation/accuracy=0.256897, validation/bleu=2.687249, validation/loss=6.155589, validation/num_examples=3000
I0607 13:23:27.085826 139569465657088 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.600958, loss=7.106173
I0607 13:27:00.929596 139569474049792 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.721295, loss=6.789057
I0607 13:30:34.914204 139569465657088 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.742102, loss=6.429620
I0607 13:34:08.970383 139569474049792 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.703460, loss=6.163210
I0607 13:37:09.624438 139627451610944 spec.py:298] Evaluating on the training split.
I0607 13:37:13.546695 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 13:40:42.834188 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 13:40:46.590923 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 13:43:43.585002 139627451610944 spec.py:326] Evaluating on the test split.
I0607 13:43:47.419400 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 13:46:43.725075 139627451610944 submission_runner.py:419] Time since start: 3893.71s, 	Step: 3923, 	{'train/accuracy': 0.41702580047782895, 'train/loss': 4.320867722539123, 'train/bleu': 13.931903070081377, 'validation/accuracy': 0.40025542150748283, 'validation/loss': 4.456502786698243, 'validation/bleu': 9.689881919667974, 'validation/num_examples': 3000, 'test/accuracy': 0.38581139968624717, 'test/loss': 4.660569911684388, 'test/bleu': 7.977920252193421, 'test/num_examples': 3003, 'score': 1683.2941477298737, 'total_duration': 3893.7093143463135, 'accumulated_submission_time': 1683.2941477298737, 'accumulated_eval_time': 2208.9457592964172, 'accumulated_logging_time': 0.052085161209106445}
I0607 13:46:43.736557 139569465657088 logging_writer.py:48] [3923] accumulated_eval_time=2208.945759, accumulated_logging_time=0.052085, accumulated_submission_time=1683.294148, global_step=3923, preemption_count=0, score=1683.294148, test/accuracy=0.385811, test/bleu=7.977920, test/loss=4.660570, test/num_examples=3003, total_duration=3893.709314, train/accuracy=0.417026, train/bleu=13.931903, train/loss=4.320868, validation/accuracy=0.400255, validation/bleu=9.689882, validation/loss=4.456503, validation/num_examples=3000
I0607 13:47:17.087533 139569474049792 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.627609, loss=5.861990
I0607 13:50:51.269346 139569465657088 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.590588, loss=5.640150
I0607 13:54:25.355604 139569474049792 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.542287, loss=5.413083
I0607 13:57:59.463639 139569465657088 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.553108, loss=5.386522
I0607 14:00:43.959320 139627451610944 spec.py:298] Evaluating on the training split.
I0607 14:00:47.879706 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:03:52.949630 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 14:03:56.707654 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:06:39.986247 139627451610944 spec.py:326] Evaluating on the test split.
I0607 14:06:43.811429 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:09:30.213993 139627451610944 submission_runner.py:419] Time since start: 5260.20s, 	Step: 5885, 	{'train/accuracy': 0.5043446446538005, 'train/loss': 3.46171317627824, 'train/bleu': 21.80900412075067, 'validation/accuracy': 0.5020396523291714, 'validation/loss': 3.466288390720512, 'validation/bleu': 17.68594462956026, 'validation/num_examples': 3000, 'test/accuracy': 0.49834408227296495, 'test/loss': 3.5568724217070478, 'test/bleu': 15.720000491321827, 'test/num_examples': 3003, 'score': 2522.808267354965, 'total_duration': 5260.198267459869, 'accumulated_submission_time': 2522.808267354965, 'accumulated_eval_time': 2735.200346469879, 'accumulated_logging_time': 0.07354736328125}
I0607 14:09:30.224696 139569474049792 logging_writer.py:48] [5885] accumulated_eval_time=2735.200346, accumulated_logging_time=0.073547, accumulated_submission_time=2522.808267, global_step=5885, preemption_count=0, score=2522.808267, test/accuracy=0.498344, test/bleu=15.720000, test/loss=3.556872, test/num_examples=3003, total_duration=5260.198267, train/accuracy=0.504345, train/bleu=21.809004, train/loss=3.461713, validation/accuracy=0.502040, validation/bleu=17.685945, validation/loss=3.466288, validation/num_examples=3000
I0607 14:10:19.808878 139569465657088 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.472530, loss=5.161204
I0607 14:13:53.665616 139569474049792 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.446013, loss=5.101505
I0607 14:17:27.748692 139569465657088 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.477223, loss=5.165663
I0607 14:21:01.820649 139569474049792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.380323, loss=4.873701
I0607 14:23:30.397389 139627451610944 spec.py:298] Evaluating on the training split.
I0607 14:23:34.301439 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:25:51.361413 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 14:25:55.123033 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:28:04.406988 139627451610944 spec.py:326] Evaluating on the test split.
I0607 14:28:08.248560 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:30:15.338361 139627451610944 submission_runner.py:419] Time since start: 6505.32s, 	Step: 7848, 	{'train/accuracy': 0.5509773510024405, 'train/loss': 3.0060307517733733, 'train/bleu': 25.07449775144529, 'validation/accuracy': 0.5455605014196973, 'validation/loss': 3.005502682855761, 'validation/bleu': 20.2382666572735, 'validation/num_examples': 3000, 'test/accuracy': 0.5467201208529429, 'test/loss': 3.0429523633141597, 'test/bleu': 18.992377027673996, 'test/num_examples': 3003, 'score': 3362.271652698517, 'total_duration': 6505.322675704956, 'accumulated_submission_time': 3362.271652698517, 'accumulated_eval_time': 3140.1412954330444, 'accumulated_logging_time': 0.09270882606506348}
I0607 14:30:15.348738 139569465657088 logging_writer.py:48] [7848] accumulated_eval_time=3140.141295, accumulated_logging_time=0.092709, accumulated_submission_time=3362.271653, global_step=7848, preemption_count=0, score=3362.271653, test/accuracy=0.546720, test/bleu=18.992377, test/loss=3.042952, test/num_examples=3003, total_duration=6505.322676, train/accuracy=0.550977, train/bleu=25.074498, train/loss=3.006031, validation/accuracy=0.545561, validation/bleu=20.238267, validation/loss=3.005503, validation/num_examples=3000
I0607 14:31:20.714601 139569474049792 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.398263, loss=4.937007
I0607 14:34:54.586369 139569465657088 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.364635, loss=4.827590
I0607 14:38:28.614855 139569474049792 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.374855, loss=4.775063
I0607 14:42:02.479401 139569465657088 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.364131, loss=4.721747
I0607 14:44:15.549566 139627451610944 spec.py:298] Evaluating on the training split.
I0607 14:44:19.445886 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:46:36.435349 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 14:46:40.184051 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:48:48.443504 139627451610944 spec.py:326] Evaluating on the test split.
I0607 14:48:52.272433 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 14:50:57.082764 139627451610944 submission_runner.py:419] Time since start: 7747.07s, 	Step: 9812, 	{'train/accuracy': 0.5711585386809637, 'train/loss': 2.8148679775120002, 'train/bleu': 26.845260331862313, 'validation/accuracy': 0.5756035263046956, 'validation/loss': 2.7499757830033107, 'validation/bleu': 22.321697210231545, 'validation/num_examples': 3000, 'test/accuracy': 0.5740630991807565, 'test/loss': 2.771671823252571, 'test/bleu': 21.067521610185384, 'test/num_examples': 3003, 'score': 4201.763933181763, 'total_duration': 7747.067045688629, 'accumulated_submission_time': 4201.763933181763, 'accumulated_eval_time': 3541.6744542121887, 'accumulated_logging_time': 0.11171340942382812}
I0607 14:50:57.092993 139569474049792 logging_writer.py:48] [9812] accumulated_eval_time=3541.674454, accumulated_logging_time=0.111713, accumulated_submission_time=4201.763933, global_step=9812, preemption_count=0, score=4201.763933, test/accuracy=0.574063, test/bleu=21.067522, test/loss=2.771672, test/num_examples=3003, total_duration=7747.067046, train/accuracy=0.571159, train/bleu=26.845260, train/loss=2.814868, validation/accuracy=0.575604, validation/bleu=22.321697, validation/loss=2.749976, validation/num_examples=3000
I0607 14:52:17.746871 139569465657088 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.365491, loss=4.661985
I0607 14:55:51.538471 139569474049792 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.370115, loss=4.659462
I0607 14:59:25.344693 139569465657088 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.363921, loss=4.663294
I0607 15:02:59.188846 139569474049792 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.346352, loss=4.736985
I0607 15:04:57.249699 139627451610944 spec.py:298] Evaluating on the training split.
I0607 15:05:01.151424 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:07:22.137132 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 15:07:25.892740 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:09:38.553947 139627451610944 spec.py:326] Evaluating on the test split.
I0607 15:09:42.388226 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:11:48.276750 139627451610944 submission_runner.py:419] Time since start: 8998.26s, 	Step: 11777, 	{'train/accuracy': 0.5811271297509829, 'train/loss': 2.6985894851558494, 'train/bleu': 27.79748595684094, 'validation/accuracy': 0.589143346021748, 'validation/loss': 2.5887996584047315, 'validation/bleu': 23.49353604151781, 'validation/num_examples': 3000, 'test/accuracy': 0.5943640694904422, 'test/loss': 2.589274301318924, 'test/bleu': 22.409202413393906, 'test/num_examples': 3003, 'score': 5041.216021776199, 'total_duration': 8998.261040687561, 'accumulated_submission_time': 5041.216021776199, 'accumulated_eval_time': 3952.701439142227, 'accumulated_logging_time': 0.1312999725341797}
I0607 15:11:48.286988 139569465657088 logging_writer.py:48] [11777] accumulated_eval_time=3952.701439, accumulated_logging_time=0.131300, accumulated_submission_time=5041.216022, global_step=11777, preemption_count=0, score=5041.216022, test/accuracy=0.594364, test/bleu=22.409202, test/loss=2.589274, test/num_examples=3003, total_duration=8998.261041, train/accuracy=0.581127, train/bleu=27.797486, train/loss=2.698589, validation/accuracy=0.589143, validation/bleu=23.493536, validation/loss=2.588800, validation/num_examples=3000
I0607 15:13:24.012089 139569474049792 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.348396, loss=4.690498
I0607 15:16:58.000774 139569465657088 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.340437, loss=4.602633
I0607 15:20:31.951170 139569474049792 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.328460, loss=4.555067
I0607 15:24:05.860928 139569465657088 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.337299, loss=4.580882
I0607 15:25:48.569155 139627451610944 spec.py:298] Evaluating on the training split.
I0607 15:25:52.458147 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:28:11.682946 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 15:28:15.448774 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:30:30.611037 139627451610944 spec.py:326] Evaluating on the test split.
I0607 15:30:34.455068 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:32:39.831775 139627451610944 submission_runner.py:419] Time since start: 10249.82s, 	Step: 13741, 	{'train/accuracy': 0.6004226144676551, 'train/loss': 2.5154306676849227, 'train/bleu': 28.409107631373352, 'validation/accuracy': 0.6019888160097209, 'validation/loss': 2.4760732197988866, 'validation/bleu': 24.08849106087018, 'validation/num_examples': 3000, 'test/accuracy': 0.6049503224681889, 'test/loss': 2.4665762448434143, 'test/bleu': 23.14210165745972, 'test/num_examples': 3003, 'score': 5880.808818101883, 'total_duration': 10249.816081047058, 'accumulated_submission_time': 5880.808818101883, 'accumulated_eval_time': 4363.964024066925, 'accumulated_logging_time': 0.1500110626220703}
I0607 15:32:39.842464 139569474049792 logging_writer.py:48] [13741] accumulated_eval_time=4363.964024, accumulated_logging_time=0.150011, accumulated_submission_time=5880.808818, global_step=13741, preemption_count=0, score=5880.808818, test/accuracy=0.604950, test/bleu=23.142102, test/loss=2.466576, test/num_examples=3003, total_duration=10249.816081, train/accuracy=0.600423, train/bleu=28.409108, train/loss=2.515431, validation/accuracy=0.601989, validation/bleu=24.088491, validation/loss=2.476073, validation/num_examples=3000
I0607 15:34:30.908353 139569465657088 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.349091, loss=4.485756
I0607 15:38:04.778125 139569474049792 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.334238, loss=4.546173
I0607 15:41:38.689991 139569465657088 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.348801, loss=4.538788
I0607 15:45:12.504204 139569474049792 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.326780, loss=4.567532
I0607 15:46:40.180521 139627451610944 spec.py:298] Evaluating on the training split.
I0607 15:46:44.088180 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:49:02.045668 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 15:49:05.799682 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:51:12.898567 139627451610944 spec.py:326] Evaluating on the test split.
I0607 15:51:16.727516 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 15:53:20.346764 139627451610944 submission_runner.py:419] Time since start: 11490.33s, 	Step: 15706, 	{'train/accuracy': 0.6060456181920657, 'train/loss': 2.4631456754160963, 'train/bleu': 28.410205909798954, 'validation/accuracy': 0.6104450037817262, 'validation/loss': 2.398747380689638, 'validation/bleu': 24.827059231202256, 'validation/num_examples': 3000, 'test/accuracy': 0.613770263203765, 'test/loss': 2.377190459589797, 'test/bleu': 23.95572903202359, 'test/num_examples': 3003, 'score': 6720.478276729584, 'total_duration': 11490.331058263779, 'accumulated_submission_time': 6720.478276729584, 'accumulated_eval_time': 4764.130207538605, 'accumulated_logging_time': 0.16916155815124512}
I0607 15:53:20.357220 139569465657088 logging_writer.py:48] [15706] accumulated_eval_time=4764.130208, accumulated_logging_time=0.169162, accumulated_submission_time=6720.478277, global_step=15706, preemption_count=0, score=6720.478277, test/accuracy=0.613770, test/bleu=23.955729, test/loss=2.377190, test/num_examples=3003, total_duration=11490.331058, train/accuracy=0.606046, train/bleu=28.410206, train/loss=2.463146, validation/accuracy=0.610445, validation/bleu=24.827059, validation/loss=2.398747, validation/num_examples=3000
I0607 15:55:26.411737 139569474049792 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.327799, loss=4.496624
I0607 15:59:00.205878 139569465657088 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.322441, loss=4.485482
I0607 16:02:34.124115 139569474049792 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.316361, loss=4.441454
I0607 16:06:07.964399 139569465657088 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.324090, loss=4.482403
I0607 16:07:20.694166 139627451610944 spec.py:298] Evaluating on the training split.
I0607 16:07:24.598199 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:09:48.142962 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 16:09:51.906009 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:12:12.724028 139627451610944 spec.py:326] Evaluating on the test split.
I0607 16:12:16.558791 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:14:28.904440 139627451610944 submission_runner.py:419] Time since start: 12758.89s, 	Step: 17671, 	{'train/accuracy': 0.6054872932495029, 'train/loss': 2.4584275611774578, 'train/bleu': 29.070075747413885, 'validation/accuracy': 0.6140159452455642, 'validation/loss': 2.345952971754845, 'validation/bleu': 25.059465820374896, 'validation/num_examples': 3000, 'test/accuracy': 0.6199407355760851, 'test/loss': 2.326344706873511, 'test/bleu': 24.159290840432938, 'test/num_examples': 3003, 'score': 7560.144879579544, 'total_duration': 12758.888706207275, 'accumulated_submission_time': 7560.144879579544, 'accumulated_eval_time': 5192.340402126312, 'accumulated_logging_time': 0.18932700157165527}
I0607 16:14:28.914720 139569474049792 logging_writer.py:48] [17671] accumulated_eval_time=5192.340402, accumulated_logging_time=0.189327, accumulated_submission_time=7560.144880, global_step=17671, preemption_count=0, score=7560.144880, test/accuracy=0.619941, test/bleu=24.159291, test/loss=2.326345, test/num_examples=3003, total_duration=12758.888706, train/accuracy=0.605487, train/bleu=29.070076, train/loss=2.458428, validation/accuracy=0.614016, validation/bleu=25.059466, validation/loss=2.345953, validation/num_examples=3000
I0607 16:16:50.111251 139569465657088 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.312803, loss=4.483124
I0607 16:20:24.110731 139569474049792 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.309808, loss=4.478749
I0607 16:23:58.241443 139569465657088 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.312065, loss=4.406509
I0607 16:27:32.126070 139569474049792 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.309912, loss=4.367136
I0607 16:28:29.043901 139627451610944 spec.py:298] Evaluating on the training split.
I0607 16:28:32.944067 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:30:48.795065 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 16:30:52.545161 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:33:03.111808 139627451610944 spec.py:326] Evaluating on the test split.
I0607 16:33:06.951436 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:35:12.182718 139627451610944 submission_runner.py:419] Time since start: 14002.17s, 	Step: 19634, 	{'train/accuracy': 0.6215199917204264, 'train/loss': 2.322218796356988, 'train/bleu': 30.028192356893975, 'validation/accuracy': 0.6219885680276748, 'validation/loss': 2.282854327596682, 'validation/bleu': 25.526990867861667, 'validation/num_examples': 3000, 'test/accuracy': 0.626982743594213, 'test/loss': 2.2657595433153217, 'test/bleu': 24.645022747605235, 'test/num_examples': 3003, 'score': 8399.608613491058, 'total_duration': 14002.166983604431, 'accumulated_submission_time': 8399.608613491058, 'accumulated_eval_time': 5595.479131698608, 'accumulated_logging_time': 0.20876383781433105}
I0607 16:35:12.193321 139569465657088 logging_writer.py:48] [19634] accumulated_eval_time=5595.479132, accumulated_logging_time=0.208764, accumulated_submission_time=8399.608613, global_step=19634, preemption_count=0, score=8399.608613, test/accuracy=0.626983, test/bleu=24.645023, test/loss=2.265760, test/num_examples=3003, total_duration=14002.166984, train/accuracy=0.621520, train/bleu=30.028192, train/loss=2.322219, validation/accuracy=0.621989, validation/bleu=25.526991, validation/loss=2.282854, validation/num_examples=3000
I0607 16:37:48.666857 139627451610944 spec.py:298] Evaluating on the training split.
I0607 16:37:52.578548 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:40:08.685802 139627451610944 spec.py:310] Evaluating on the validation split.
I0607 16:40:12.449859 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:42:22.641103 139627451610944 spec.py:326] Evaluating on the test split.
I0607 16:42:26.474779 139627451610944 workload.py:130] Translating evaluation dataset.
I0607 16:44:30.877392 139627451610944 submission_runner.py:419] Time since start: 14560.86s, 	Step: 20000, 	{'train/accuracy': 0.6202049713017173, 'train/loss': 2.355967977547476, 'train/bleu': 30.04247628946544, 'validation/accuracy': 0.6226829177567544, 'validation/loss': 2.306977943546887, 'validation/bleu': 25.562116572900813, 'validation/num_examples': 3000, 'test/accuracy': 0.6283423392016734, 'test/loss': 2.2740917872290978, 'test/bleu': 24.696814777638235, 'test/num_examples': 3003, 'score': 8555.94378566742, 'total_duration': 14560.861699819565, 'accumulated_submission_time': 8555.94378566742, 'accumulated_eval_time': 5997.689708948135, 'accumulated_logging_time': 0.22771406173706055}
I0607 16:44:30.888206 139569474049792 logging_writer.py:48] [20000] accumulated_eval_time=5997.689709, accumulated_logging_time=0.227714, accumulated_submission_time=8555.943786, global_step=20000, preemption_count=0, score=8555.943786, test/accuracy=0.628342, test/bleu=24.696815, test/loss=2.274092, test/num_examples=3003, total_duration=14560.861700, train/accuracy=0.620205, train/bleu=30.042476, train/loss=2.355968, validation/accuracy=0.622683, validation/bleu=25.562117, validation/loss=2.306978, validation/num_examples=3000
I0607 16:44:30.905680 139569465657088 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=8555.943786
I0607 16:44:32.410880 139627451610944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/momentum/wmt_pytorch/trial_1/checkpoint_20000.
I0607 16:44:32.432549 139627451610944 submission_runner.py:581] Tuning trial 1/1
I0607 16:44:32.432748 139627451610944 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 16:44:32.433646 139627451610944 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005963029218843172, 'train/loss': 11.076557554699326, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.069143594003794, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.071717942013828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.534735918045044, 'total_duration': 837.9874379634857, 'accumulated_submission_time': 4.534735918045044, 'accumulated_eval_time': 833.4523410797119, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1960, {'train/accuracy': 0.27853436723286795, 'train/loss': 5.905021218229924, 'train/bleu': 5.289531557119159, 'validation/accuracy': 0.25689700065715243, 'validation/loss': 6.155588972858365, 'validation/bleu': 2.687248735256667, 'validation/num_examples': 3000, 'test/accuracy': 0.23548893149729824, 'test/loss': 6.4523575039219105, 'test/bleu': 1.894095097274457, 'test/num_examples': 3003, 'score': 844.0114667415619, 'total_duration': 2479.595888376236, 'accumulated_submission_time': 844.0114667415619, 'accumulated_eval_time': 1634.8452229499817, 'accumulated_logging_time': 0.03177785873413086, 'global_step': 1960, 'preemption_count': 0}), (3923, {'train/accuracy': 0.41702580047782895, 'train/loss': 4.320867722539123, 'train/bleu': 13.931903070081377, 'validation/accuracy': 0.40025542150748283, 'validation/loss': 4.456502786698243, 'validation/bleu': 9.689881919667974, 'validation/num_examples': 3000, 'test/accuracy': 0.38581139968624717, 'test/loss': 4.660569911684388, 'test/bleu': 7.977920252193421, 'test/num_examples': 3003, 'score': 1683.2941477298737, 'total_duration': 3893.7093143463135, 'accumulated_submission_time': 1683.2941477298737, 'accumulated_eval_time': 2208.9457592964172, 'accumulated_logging_time': 0.052085161209106445, 'global_step': 3923, 'preemption_count': 0}), (5885, {'train/accuracy': 0.5043446446538005, 'train/loss': 3.46171317627824, 'train/bleu': 21.80900412075067, 'validation/accuracy': 0.5020396523291714, 'validation/loss': 3.466288390720512, 'validation/bleu': 17.68594462956026, 'validation/num_examples': 3000, 'test/accuracy': 0.49834408227296495, 'test/loss': 3.5568724217070478, 'test/bleu': 15.720000491321827, 'test/num_examples': 3003, 'score': 2522.808267354965, 'total_duration': 5260.198267459869, 'accumulated_submission_time': 2522.808267354965, 'accumulated_eval_time': 2735.200346469879, 'accumulated_logging_time': 0.07354736328125, 'global_step': 5885, 'preemption_count': 0}), (7848, {'train/accuracy': 0.5509773510024405, 'train/loss': 3.0060307517733733, 'train/bleu': 25.07449775144529, 'validation/accuracy': 0.5455605014196973, 'validation/loss': 3.005502682855761, 'validation/bleu': 20.2382666572735, 'validation/num_examples': 3000, 'test/accuracy': 0.5467201208529429, 'test/loss': 3.0429523633141597, 'test/bleu': 18.992377027673996, 'test/num_examples': 3003, 'score': 3362.271652698517, 'total_duration': 6505.322675704956, 'accumulated_submission_time': 3362.271652698517, 'accumulated_eval_time': 3140.1412954330444, 'accumulated_logging_time': 0.09270882606506348, 'global_step': 7848, 'preemption_count': 0}), (9812, {'train/accuracy': 0.5711585386809637, 'train/loss': 2.8148679775120002, 'train/bleu': 26.845260331862313, 'validation/accuracy': 0.5756035263046956, 'validation/loss': 2.7499757830033107, 'validation/bleu': 22.321697210231545, 'validation/num_examples': 3000, 'test/accuracy': 0.5740630991807565, 'test/loss': 2.771671823252571, 'test/bleu': 21.067521610185384, 'test/num_examples': 3003, 'score': 4201.763933181763, 'total_duration': 7747.067045688629, 'accumulated_submission_time': 4201.763933181763, 'accumulated_eval_time': 3541.6744542121887, 'accumulated_logging_time': 0.11171340942382812, 'global_step': 9812, 'preemption_count': 0}), (11777, {'train/accuracy': 0.5811271297509829, 'train/loss': 2.6985894851558494, 'train/bleu': 27.79748595684094, 'validation/accuracy': 0.589143346021748, 'validation/loss': 2.5887996584047315, 'validation/bleu': 23.49353604151781, 'validation/num_examples': 3000, 'test/accuracy': 0.5943640694904422, 'test/loss': 2.589274301318924, 'test/bleu': 22.409202413393906, 'test/num_examples': 3003, 'score': 5041.216021776199, 'total_duration': 8998.261040687561, 'accumulated_submission_time': 5041.216021776199, 'accumulated_eval_time': 3952.701439142227, 'accumulated_logging_time': 0.1312999725341797, 'global_step': 11777, 'preemption_count': 0}), (13741, {'train/accuracy': 0.6004226144676551, 'train/loss': 2.5154306676849227, 'train/bleu': 28.409107631373352, 'validation/accuracy': 0.6019888160097209, 'validation/loss': 2.4760732197988866, 'validation/bleu': 24.08849106087018, 'validation/num_examples': 3000, 'test/accuracy': 0.6049503224681889, 'test/loss': 2.4665762448434143, 'test/bleu': 23.14210165745972, 'test/num_examples': 3003, 'score': 5880.808818101883, 'total_duration': 10249.816081047058, 'accumulated_submission_time': 5880.808818101883, 'accumulated_eval_time': 4363.964024066925, 'accumulated_logging_time': 0.1500110626220703, 'global_step': 13741, 'preemption_count': 0}), (15706, {'train/accuracy': 0.6060456181920657, 'train/loss': 2.4631456754160963, 'train/bleu': 28.410205909798954, 'validation/accuracy': 0.6104450037817262, 'validation/loss': 2.398747380689638, 'validation/bleu': 24.827059231202256, 'validation/num_examples': 3000, 'test/accuracy': 0.613770263203765, 'test/loss': 2.377190459589797, 'test/bleu': 23.95572903202359, 'test/num_examples': 3003, 'score': 6720.478276729584, 'total_duration': 11490.331058263779, 'accumulated_submission_time': 6720.478276729584, 'accumulated_eval_time': 4764.130207538605, 'accumulated_logging_time': 0.16916155815124512, 'global_step': 15706, 'preemption_count': 0}), (17671, {'train/accuracy': 0.6054872932495029, 'train/loss': 2.4584275611774578, 'train/bleu': 29.070075747413885, 'validation/accuracy': 0.6140159452455642, 'validation/loss': 2.345952971754845, 'validation/bleu': 25.059465820374896, 'validation/num_examples': 3000, 'test/accuracy': 0.6199407355760851, 'test/loss': 2.326344706873511, 'test/bleu': 24.159290840432938, 'test/num_examples': 3003, 'score': 7560.144879579544, 'total_duration': 12758.888706207275, 'accumulated_submission_time': 7560.144879579544, 'accumulated_eval_time': 5192.340402126312, 'accumulated_logging_time': 0.18932700157165527, 'global_step': 17671, 'preemption_count': 0}), (19634, {'train/accuracy': 0.6215199917204264, 'train/loss': 2.322218796356988, 'train/bleu': 30.028192356893975, 'validation/accuracy': 0.6219885680276748, 'validation/loss': 2.282854327596682, 'validation/bleu': 25.526990867861667, 'validation/num_examples': 3000, 'test/accuracy': 0.626982743594213, 'test/loss': 2.2657595433153217, 'test/bleu': 24.645022747605235, 'test/num_examples': 3003, 'score': 8399.608613491058, 'total_duration': 14002.166983604431, 'accumulated_submission_time': 8399.608613491058, 'accumulated_eval_time': 5595.479131698608, 'accumulated_logging_time': 0.20876383781433105, 'global_step': 19634, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6202049713017173, 'train/loss': 2.355967977547476, 'train/bleu': 30.04247628946544, 'validation/accuracy': 0.6226829177567544, 'validation/loss': 2.306977943546887, 'validation/bleu': 25.562116572900813, 'validation/num_examples': 3000, 'test/accuracy': 0.6283423392016734, 'test/loss': 2.2740917872290978, 'test/bleu': 24.696814777638235, 'test/num_examples': 3003, 'score': 8555.94378566742, 'total_duration': 14560.861699819565, 'accumulated_submission_time': 8555.94378566742, 'accumulated_eval_time': 5997.689708948135, 'accumulated_logging_time': 0.22771406173706055, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0607 16:44:32.433756 139627451610944 submission_runner.py:584] Timing: 8555.94378566742
I0607 16:44:32.433808 139627451610944 submission_runner.py:586] Total number of evals: 12
I0607 16:44:32.433860 139627451610944 submission_runner.py:587] ====================
I0607 16:44:32.433949 139627451610944 submission_runner.py:655] Final wmt score: 8555.94378566742
