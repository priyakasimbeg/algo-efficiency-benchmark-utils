torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-07-2023-06-52-15.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 06:52:38.540348 140601815115584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 06:52:38.540378 140676685031232 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 06:52:38.540392 140312391231296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 06:52:38.540994 140064644425536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 06:52:38.541659 139634100451136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 06:52:38.541700 140358143592256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 06:52:39.527423 139646377314112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 06:52:39.536653 139989027190592 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 06:52:39.537087 139989027190592 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.538034 139646377314112 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.544883 140601815115584 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.544914 140676685031232 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.544933 140312391231296 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.544949 140064644425536 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.544979 139634100451136 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:39.545032 140358143592256 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:52:41.850642 139989027190592 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch because --overwrite was set.
I0607 06:52:41.864758 139989027190592 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch.
W0607 06:52:41.936552 140601815115584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.936629 140676685031232 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.937184 139646377314112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.937455 140312391231296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.937564 140064644425536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.937654 140358143592256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.938208 139634100451136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:52:41.939334 139989027190592 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 06:52:41.944968 139989027190592 submission_runner.py:541] Using RNG seed 4085237444
I0607 06:52:41.946471 139989027190592 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 06:52:41.946585 139989027190592 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch/trial_1.
I0607 06:52:41.946913 139989027190592 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch/trial_1/hparams.json.
I0607 06:52:41.947804 139989027190592 submission_runner.py:255] Initializing dataset.
I0607 06:52:48.281715 139989027190592 submission_runner.py:262] Initializing model.
I0607 06:52:52.640891 139989027190592 submission_runner.py:272] Initializing optimizer.
I0607 06:52:53.100508 139989027190592 submission_runner.py:279] Initializing metrics bundle.
I0607 06:52:53.100709 139989027190592 submission_runner.py:297] Initializing checkpoint and logger.
I0607 06:52:53.606863 139989027190592 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0607 06:52:53.607842 139989027190592 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch/trial_1/flags_0.json.
I0607 06:52:53.658087 139989027190592 submission_runner.py:332] Starting training loop.
I0607 06:53:00.134877 139959822214912 logging_writer.py:48] [0] global_step=0, grad_norm=0.301783, loss=6.907755
I0607 06:53:00.171099 139989027190592 spec.py:298] Evaluating on the training split.
I0607 06:54:01.416634 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 06:54:55.258509 139989027190592 spec.py:326] Evaluating on the test split.
I0607 06:54:55.277059 139989027190592 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 06:54:55.283573 139989027190592 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0607 06:54:55.360655 139989027190592 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 06:55:08.079139 139989027190592 submission_runner.py:419] Time since start: 134.42s, 	Step: 1, 	{'train/accuracy': 0.0012109375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.51295280456543, 'total_duration': 134.42133784294128, 'accumulated_submission_time': 6.51295280456543, 'accumulated_eval_time': 127.9079225063324, 'accumulated_logging_time': 0}
I0607 06:55:08.099436 139954797672192 logging_writer.py:48] [1] accumulated_eval_time=127.907923, accumulated_logging_time=0, accumulated_submission_time=6.512953, global_step=1, preemption_count=0, score=6.512953, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=134.421338, train/accuracy=0.001211, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0607 06:55:08.119370 139989027190592 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119372 139646377314112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119404 140312391231296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119419 140064644425536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119441 140358143592256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119438 139634100451136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119444 140676685031232 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.119781 140601815115584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:55:08.705131 139954789279488 logging_writer.py:48] [1] global_step=1, grad_norm=0.312053, loss=6.907754
I0607 06:55:09.107547 139954797672192 logging_writer.py:48] [2] global_step=2, grad_norm=0.300465, loss=6.907755
I0607 06:55:09.500809 139954789279488 logging_writer.py:48] [3] global_step=3, grad_norm=0.307576, loss=6.907753
I0607 06:55:09.895783 139954797672192 logging_writer.py:48] [4] global_step=4, grad_norm=0.307448, loss=6.907754
I0607 06:55:10.293725 139954789279488 logging_writer.py:48] [5] global_step=5, grad_norm=0.307511, loss=6.907754
I0607 06:55:10.691794 139954797672192 logging_writer.py:48] [6] global_step=6, grad_norm=0.309857, loss=6.907748
I0607 06:55:11.092296 139954789279488 logging_writer.py:48] [7] global_step=7, grad_norm=0.312495, loss=6.907749
I0607 06:55:11.490865 139954797672192 logging_writer.py:48] [8] global_step=8, grad_norm=0.310810, loss=6.907738
I0607 06:55:11.888697 139954789279488 logging_writer.py:48] [9] global_step=9, grad_norm=0.314389, loss=6.907745
I0607 06:55:12.283539 139954797672192 logging_writer.py:48] [10] global_step=10, grad_norm=0.308509, loss=6.907738
I0607 06:55:12.686797 139954789279488 logging_writer.py:48] [11] global_step=11, grad_norm=0.311439, loss=6.907734
I0607 06:55:13.083686 139954797672192 logging_writer.py:48] [12] global_step=12, grad_norm=0.305471, loss=6.907755
I0607 06:55:13.478185 139954789279488 logging_writer.py:48] [13] global_step=13, grad_norm=0.305664, loss=6.907714
I0607 06:55:13.874650 139954797672192 logging_writer.py:48] [14] global_step=14, grad_norm=0.310497, loss=6.907741
I0607 06:55:14.275064 139954789279488 logging_writer.py:48] [15] global_step=15, grad_norm=0.313516, loss=6.907722
I0607 06:55:14.684285 139954797672192 logging_writer.py:48] [16] global_step=16, grad_norm=0.296983, loss=6.907707
I0607 06:55:15.083614 139954789279488 logging_writer.py:48] [17] global_step=17, grad_norm=0.303465, loss=6.907686
I0607 06:55:15.481278 139954797672192 logging_writer.py:48] [18] global_step=18, grad_norm=0.314369, loss=6.907663
I0607 06:55:15.876217 139954789279488 logging_writer.py:48] [19] global_step=19, grad_norm=0.304526, loss=6.907705
I0607 06:55:16.277636 139954797672192 logging_writer.py:48] [20] global_step=20, grad_norm=0.309167, loss=6.907637
I0607 06:55:16.673937 139954789279488 logging_writer.py:48] [21] global_step=21, grad_norm=0.306478, loss=6.907565
I0607 06:55:17.077337 139954797672192 logging_writer.py:48] [22] global_step=22, grad_norm=0.306585, loss=6.907624
I0607 06:55:17.478854 139954789279488 logging_writer.py:48] [23] global_step=23, grad_norm=0.305891, loss=6.907601
I0607 06:55:17.878153 139954797672192 logging_writer.py:48] [24] global_step=24, grad_norm=0.306411, loss=6.907496
I0607 06:55:18.276271 139954789279488 logging_writer.py:48] [25] global_step=25, grad_norm=0.305820, loss=6.907572
I0607 06:55:18.674016 139954797672192 logging_writer.py:48] [26] global_step=26, grad_norm=0.307134, loss=6.907533
I0607 06:55:19.071927 139954789279488 logging_writer.py:48] [27] global_step=27, grad_norm=0.304977, loss=6.907604
I0607 06:55:19.476459 139954797672192 logging_writer.py:48] [28] global_step=28, grad_norm=0.313075, loss=6.907501
I0607 06:55:19.875675 139954789279488 logging_writer.py:48] [29] global_step=29, grad_norm=0.308819, loss=6.907695
I0607 06:55:20.276610 139954797672192 logging_writer.py:48] [30] global_step=30, grad_norm=0.310130, loss=6.907362
I0607 06:55:20.674265 139954789279488 logging_writer.py:48] [31] global_step=31, grad_norm=0.302321, loss=6.907581
I0607 06:55:21.070666 139954797672192 logging_writer.py:48] [32] global_step=32, grad_norm=0.302897, loss=6.907534
I0607 06:55:21.473119 139954789279488 logging_writer.py:48] [33] global_step=33, grad_norm=0.312039, loss=6.907289
I0607 06:55:21.872821 139954797672192 logging_writer.py:48] [34] global_step=34, grad_norm=0.310541, loss=6.907410
I0607 06:55:22.274420 139954789279488 logging_writer.py:48] [35] global_step=35, grad_norm=0.301699, loss=6.907329
I0607 06:55:22.677468 139954797672192 logging_writer.py:48] [36] global_step=36, grad_norm=0.308360, loss=6.907420
I0607 06:55:23.075866 139954789279488 logging_writer.py:48] [37] global_step=37, grad_norm=0.306611, loss=6.907355
I0607 06:55:23.473671 139954797672192 logging_writer.py:48] [38] global_step=38, grad_norm=0.306644, loss=6.907248
I0607 06:55:23.873775 139954789279488 logging_writer.py:48] [39] global_step=39, grad_norm=0.307033, loss=6.907327
I0607 06:55:24.275666 139954797672192 logging_writer.py:48] [40] global_step=40, grad_norm=0.309567, loss=6.907213
I0607 06:55:24.679428 139954789279488 logging_writer.py:48] [41] global_step=41, grad_norm=0.301691, loss=6.907170
I0607 06:55:25.090265 139954797672192 logging_writer.py:48] [42] global_step=42, grad_norm=0.300141, loss=6.907485
I0607 06:55:25.497335 139954789279488 logging_writer.py:48] [43] global_step=43, grad_norm=0.310882, loss=6.907303
I0607 06:55:25.906855 139954797672192 logging_writer.py:48] [44] global_step=44, grad_norm=0.303073, loss=6.907065
I0607 06:55:26.311055 139954789279488 logging_writer.py:48] [45] global_step=45, grad_norm=0.300412, loss=6.906899
I0607 06:55:26.710661 139954797672192 logging_writer.py:48] [46] global_step=46, grad_norm=0.314166, loss=6.906739
I0607 06:55:27.152659 139954789279488 logging_writer.py:48] [47] global_step=47, grad_norm=0.298433, loss=6.906971
I0607 06:55:27.551420 139954797672192 logging_writer.py:48] [48] global_step=48, grad_norm=0.305083, loss=6.907119
I0607 06:55:27.950922 139954789279488 logging_writer.py:48] [49] global_step=49, grad_norm=0.308292, loss=6.907240
I0607 06:55:28.356375 139954797672192 logging_writer.py:48] [50] global_step=50, grad_norm=0.302550, loss=6.906750
I0607 06:55:28.882447 139954789279488 logging_writer.py:48] [51] global_step=51, grad_norm=0.303761, loss=6.907214
I0607 06:55:29.281684 139954797672192 logging_writer.py:48] [52] global_step=52, grad_norm=0.303814, loss=6.906879
I0607 06:55:29.688082 139954789279488 logging_writer.py:48] [53] global_step=53, grad_norm=0.302438, loss=6.906674
I0607 06:55:30.087010 139954797672192 logging_writer.py:48] [54] global_step=54, grad_norm=0.307723, loss=6.906737
I0607 06:55:30.489085 139954789279488 logging_writer.py:48] [55] global_step=55, grad_norm=0.307170, loss=6.907018
I0607 06:55:30.891900 139954797672192 logging_writer.py:48] [56] global_step=56, grad_norm=0.307769, loss=6.906574
I0607 06:55:31.293854 139954789279488 logging_writer.py:48] [57] global_step=57, grad_norm=0.310756, loss=6.907557
I0607 06:55:31.701568 139954797672192 logging_writer.py:48] [58] global_step=58, grad_norm=0.310534, loss=6.906151
I0607 06:55:32.108881 139954789279488 logging_writer.py:48] [59] global_step=59, grad_norm=0.304989, loss=6.906834
I0607 06:55:32.514622 139954797672192 logging_writer.py:48] [60] global_step=60, grad_norm=0.305179, loss=6.906106
I0607 06:55:32.921593 139954789279488 logging_writer.py:48] [61] global_step=61, grad_norm=0.308102, loss=6.905776
I0607 06:55:33.325883 139954797672192 logging_writer.py:48] [62] global_step=62, grad_norm=0.295240, loss=6.907334
I0607 06:55:33.733936 139954789279488 logging_writer.py:48] [63] global_step=63, grad_norm=0.312599, loss=6.906099
I0607 06:55:34.132884 139954797672192 logging_writer.py:48] [64] global_step=64, grad_norm=0.306817, loss=6.906348
I0607 06:55:34.547453 139954789279488 logging_writer.py:48] [65] global_step=65, grad_norm=0.302331, loss=6.906370
I0607 06:55:34.956055 139954797672192 logging_writer.py:48] [66] global_step=66, grad_norm=0.300927, loss=6.906624
I0607 06:55:35.357474 139954789279488 logging_writer.py:48] [67] global_step=67, grad_norm=0.308438, loss=6.906595
I0607 06:55:35.756239 139954797672192 logging_writer.py:48] [68] global_step=68, grad_norm=0.304255, loss=6.905991
I0607 06:55:36.165794 139954789279488 logging_writer.py:48] [69] global_step=69, grad_norm=0.296640, loss=6.905533
I0607 06:55:36.569847 139954797672192 logging_writer.py:48] [70] global_step=70, grad_norm=0.300439, loss=6.906114
I0607 06:55:36.970947 139954789279488 logging_writer.py:48] [71] global_step=71, grad_norm=0.307889, loss=6.906084
I0607 06:55:37.381406 139954797672192 logging_writer.py:48] [72] global_step=72, grad_norm=0.303064, loss=6.905354
I0607 06:55:37.787266 139954789279488 logging_writer.py:48] [73] global_step=73, grad_norm=0.310471, loss=6.905507
I0607 06:55:38.186854 139954797672192 logging_writer.py:48] [74] global_step=74, grad_norm=0.294672, loss=6.905768
I0607 06:55:38.585598 139954789279488 logging_writer.py:48] [75] global_step=75, grad_norm=0.294326, loss=6.905161
I0607 06:55:38.984294 139954797672192 logging_writer.py:48] [76] global_step=76, grad_norm=0.307578, loss=6.905582
I0607 06:55:39.393195 139954789279488 logging_writer.py:48] [77] global_step=77, grad_norm=0.302919, loss=6.905965
I0607 06:55:39.794861 139954797672192 logging_writer.py:48] [78] global_step=78, grad_norm=0.300632, loss=6.905273
I0607 06:55:40.194247 139954789279488 logging_writer.py:48] [79] global_step=79, grad_norm=0.312851, loss=6.906784
I0607 06:55:40.593738 139954797672192 logging_writer.py:48] [80] global_step=80, grad_norm=0.303549, loss=6.904419
I0607 06:55:41.000610 139954789279488 logging_writer.py:48] [81] global_step=81, grad_norm=0.299544, loss=6.905912
I0607 06:55:41.408015 139954797672192 logging_writer.py:48] [82] global_step=82, grad_norm=0.305859, loss=6.904722
I0607 06:55:41.813123 139954789279488 logging_writer.py:48] [83] global_step=83, grad_norm=0.314166, loss=6.904749
I0607 06:55:42.218439 139954797672192 logging_writer.py:48] [84] global_step=84, grad_norm=0.307635, loss=6.905473
I0607 06:55:42.619159 139954789279488 logging_writer.py:48] [85] global_step=85, grad_norm=0.301555, loss=6.905086
I0607 06:55:43.019753 139954797672192 logging_writer.py:48] [86] global_step=86, grad_norm=0.312318, loss=6.904120
I0607 06:55:43.418401 139954789279488 logging_writer.py:48] [87] global_step=87, grad_norm=0.300433, loss=6.904830
I0607 06:55:43.817010 139954797672192 logging_writer.py:48] [88] global_step=88, grad_norm=0.305991, loss=6.904099
I0607 06:55:44.215465 139954789279488 logging_writer.py:48] [89] global_step=89, grad_norm=0.308346, loss=6.904183
I0607 06:55:44.619109 139954797672192 logging_writer.py:48] [90] global_step=90, grad_norm=0.308843, loss=6.906205
I0607 06:55:45.018337 139954789279488 logging_writer.py:48] [91] global_step=91, grad_norm=0.302614, loss=6.903250
I0607 06:55:45.428712 139954797672192 logging_writer.py:48] [92] global_step=92, grad_norm=0.311069, loss=6.903078
I0607 06:55:45.830801 139954789279488 logging_writer.py:48] [93] global_step=93, grad_norm=0.302908, loss=6.904515
I0607 06:55:46.232309 139954797672192 logging_writer.py:48] [94] global_step=94, grad_norm=0.304147, loss=6.903743
I0607 06:55:46.630916 139954789279488 logging_writer.py:48] [95] global_step=95, grad_norm=0.303462, loss=6.903794
I0607 06:55:47.031850 139954797672192 logging_writer.py:48] [96] global_step=96, grad_norm=0.303973, loss=6.905134
I0607 06:55:47.434079 139954789279488 logging_writer.py:48] [97] global_step=97, grad_norm=0.310032, loss=6.902739
I0607 06:55:47.832074 139954797672192 logging_writer.py:48] [98] global_step=98, grad_norm=0.310846, loss=6.904655
I0607 06:55:48.231671 139954789279488 logging_writer.py:48] [99] global_step=99, grad_norm=0.305534, loss=6.903801
I0607 06:55:48.632316 139954797672192 logging_writer.py:48] [100] global_step=100, grad_norm=0.304474, loss=6.904141
I0607 06:58:31.070971 139954789279488 logging_writer.py:48] [500] global_step=500, grad_norm=0.630552, loss=6.757756
I0607 07:01:53.926774 139954797672192 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.779363, loss=6.574458
I0607 07:02:08.392092 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:02:50.876323 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:03:34.267691 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:03:35.745627 139989027190592 submission_runner.py:419] Time since start: 642.09s, 	Step: 1035, 	{'train/accuracy': 0.0437890625, 'train/loss': 5.957134399414063, 'validation/accuracy': 0.04298, 'validation/loss': 5.98637, 'validation/num_examples': 50000, 'test/accuracy': 0.0329, 'test/loss': 6.101221875, 'test/num_examples': 10000, 'score': 426.1436882019043, 'total_duration': 642.0879812240601, 'accumulated_submission_time': 426.1436882019043, 'accumulated_eval_time': 215.26170301437378, 'accumulated_logging_time': 0.02908945083618164}
I0607 07:03:35.755250 139945905731328 logging_writer.py:48] [1035] accumulated_eval_time=215.261703, accumulated_logging_time=0.029089, accumulated_submission_time=426.143688, global_step=1035, preemption_count=0, score=426.143688, test/accuracy=0.032900, test/loss=6.101222, test/num_examples=10000, total_duration=642.087981, train/accuracy=0.043789, train/loss=5.957134, validation/accuracy=0.042980, validation/loss=5.986370, validation/num_examples=50000
I0607 07:06:42.967643 139945914124032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.801057, loss=6.490467
I0607 07:09:57.677479 139945905731328 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.903009, loss=6.312371
I0607 07:10:35.845653 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:11:20.087318 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:12:05.333232 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:12:06.771276 139989027190592 submission_runner.py:419] Time since start: 1153.11s, 	Step: 2099, 	{'train/accuracy': 0.0801171875, 'train/loss': 5.418282470703125, 'validation/accuracy': 0.07614, 'validation/loss': 5.468415625, 'validation/num_examples': 50000, 'test/accuracy': 0.054, 'test/loss': 5.66853203125, 'test/num_examples': 10000, 'score': 845.5697410106659, 'total_duration': 1153.1136319637299, 'accumulated_submission_time': 845.5697410106659, 'accumulated_eval_time': 306.1875250339508, 'accumulated_logging_time': 0.046944618225097656}
I0607 07:12:06.781007 139945914124032 logging_writer.py:48] [2099] accumulated_eval_time=306.187525, accumulated_logging_time=0.046945, accumulated_submission_time=845.569741, global_step=2099, preemption_count=0, score=845.569741, test/accuracy=0.054000, test/loss=5.668532, test/num_examples=10000, total_duration=1153.113632, train/accuracy=0.080117, train/loss=5.418282, validation/accuracy=0.076140, validation/loss=5.468416, validation/num_examples=50000
I0607 07:14:48.195066 139945905731328 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.793926, loss=6.261061
I0607 07:18:04.528351 139945914124032 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.840137, loss=6.274615
I0607 07:19:06.814716 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:19:50.727456 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:20:45.283198 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:20:46.714239 139989027190592 submission_runner.py:419] Time since start: 1673.06s, 	Step: 3161, 	{'train/accuracy': 0.1084375, 'train/loss': 5.111151428222656, 'validation/accuracy': 0.10176, 'validation/loss': 5.1806028125, 'validation/num_examples': 50000, 'test/accuracy': 0.0744, 'test/loss': 5.434167578125, 'test/num_examples': 10000, 'score': 1264.943895816803, 'total_duration': 1673.0565927028656, 'accumulated_submission_time': 1264.943895816803, 'accumulated_eval_time': 406.08704924583435, 'accumulated_logging_time': 0.06462407112121582}
I0607 07:20:46.724709 139945905731328 logging_writer.py:48] [3161] accumulated_eval_time=406.087049, accumulated_logging_time=0.064624, accumulated_submission_time=1264.943896, global_step=3161, preemption_count=0, score=1264.943896, test/accuracy=0.074400, test/loss=5.434168, test/num_examples=10000, total_duration=1673.056593, train/accuracy=0.108438, train/loss=5.111151, validation/accuracy=0.101760, validation/loss=5.180603, validation/num_examples=50000
I0607 07:22:58.914273 139945914124032 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.730391, loss=6.158134
I0607 07:26:16.013809 139945905731328 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.566044, loss=6.262493
I0607 07:27:46.721565 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:28:30.663698 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:29:14.794983 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:29:16.228972 139989027190592 submission_runner.py:419] Time since start: 2182.57s, 	Step: 4234, 	{'train/accuracy': 0.14025390625, 'train/loss': 4.803804931640625, 'validation/accuracy': 0.12552, 'validation/loss': 4.8888453125, 'validation/num_examples': 50000, 'test/accuracy': 0.0937, 'test/loss': 5.1858453125, 'test/num_examples': 10000, 'score': 1684.2717118263245, 'total_duration': 2182.571363210678, 'accumulated_submission_time': 1684.2717118263245, 'accumulated_eval_time': 495.5945875644684, 'accumulated_logging_time': 0.08278656005859375}
I0607 07:29:16.239730 139945914124032 logging_writer.py:48] [4234] accumulated_eval_time=495.594588, accumulated_logging_time=0.082787, accumulated_submission_time=1684.271712, global_step=4234, preemption_count=0, score=1684.271712, test/accuracy=0.093700, test/loss=5.185845, test/num_examples=10000, total_duration=2182.571363, train/accuracy=0.140254, train/loss=4.803805, validation/accuracy=0.125520, validation/loss=4.888845, validation/num_examples=50000
I0607 07:31:00.261224 139945905731328 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.679285, loss=6.078600
I0607 07:34:18.169646 139945914124032 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.634928, loss=5.971496
I0607 07:36:16.476807 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:37:01.412083 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:37:46.624729 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:37:48.055656 139989027190592 submission_runner.py:419] Time since start: 2694.40s, 	Step: 5299, 	{'train/accuracy': 0.17234375, 'train/loss': 4.583239135742187, 'validation/accuracy': 0.15772, 'validation/loss': 4.66751625, 'validation/num_examples': 50000, 'test/accuracy': 0.1213, 'test/loss': 4.97475546875, 'test/num_examples': 10000, 'score': 2103.854108095169, 'total_duration': 2694.3962087631226, 'accumulated_submission_time': 2103.854108095169, 'accumulated_eval_time': 587.1718416213989, 'accumulated_logging_time': 0.10404109954833984}
I0607 07:37:48.065354 139945905731328 logging_writer.py:48] [5299] accumulated_eval_time=587.171842, accumulated_logging_time=0.104041, accumulated_submission_time=2103.854108, global_step=5299, preemption_count=0, score=2103.854108, test/accuracy=0.121300, test/loss=4.974755, test/num_examples=10000, total_duration=2694.396209, train/accuracy=0.172344, train/loss=4.583239, validation/accuracy=0.157720, validation/loss=4.667516, validation/num_examples=50000
I0607 07:39:06.612251 139945914124032 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.512415, loss=6.086754
I0607 07:42:21.662844 139945905731328 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.571219, loss=5.854463
I0607 07:44:48.349960 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:45:34.017036 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:46:19.912458 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:46:21.356008 139989027190592 submission_runner.py:419] Time since start: 3207.70s, 	Step: 6364, 	{'train/accuracy': 0.204375, 'train/loss': 4.289826965332031, 'validation/accuracy': 0.18584, 'validation/loss': 4.40291375, 'validation/num_examples': 50000, 'test/accuracy': 0.1403, 'test/loss': 4.76718984375, 'test/num_examples': 10000, 'score': 2523.4856894016266, 'total_duration': 3207.698268175125, 'accumulated_submission_time': 2523.4856894016266, 'accumulated_eval_time': 680.1778190135956, 'accumulated_logging_time': 0.12126827239990234}
I0607 07:46:21.367296 139945914124032 logging_writer.py:48] [6364] accumulated_eval_time=680.177819, accumulated_logging_time=0.121268, accumulated_submission_time=2523.485689, global_step=6364, preemption_count=0, score=2523.485689, test/accuracy=0.140300, test/loss=4.767190, test/num_examples=10000, total_duration=3207.698268, train/accuracy=0.204375, train/loss=4.289827, validation/accuracy=0.185840, validation/loss=4.402914, validation/num_examples=50000
I0607 07:47:14.561233 139945905731328 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.645195, loss=5.874269
I0607 07:50:29.243778 139945914124032 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.569958, loss=5.789591
I0607 07:53:21.385242 139989027190592 spec.py:298] Evaluating on the training split.
I0607 07:54:07.024215 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 07:54:56.537158 139989027190592 spec.py:326] Evaluating on the test split.
I0607 07:54:57.973234 139989027190592 submission_runner.py:419] Time since start: 3724.32s, 	Step: 7435, 	{'train/accuracy': 0.245859375, 'train/loss': 3.981947326660156, 'validation/accuracy': 0.22122, 'validation/loss': 4.1086925, 'validation/num_examples': 50000, 'test/accuracy': 0.1708, 'test/loss': 4.4966015625, 'test/num_examples': 10000, 'score': 2942.8394045829773, 'total_duration': 3724.3154344558716, 'accumulated_submission_time': 2942.8394045829773, 'accumulated_eval_time': 776.7658648490906, 'accumulated_logging_time': 0.14351105690002441}
I0607 07:54:57.985274 139945905731328 logging_writer.py:48] [7435] accumulated_eval_time=776.765865, accumulated_logging_time=0.143511, accumulated_submission_time=2942.839405, global_step=7435, preemption_count=0, score=2942.839405, test/accuracy=0.170800, test/loss=4.496602, test/num_examples=10000, total_duration=3724.315434, train/accuracy=0.245859, train/loss=3.981947, validation/accuracy=0.221220, validation/loss=4.108693, validation/num_examples=50000
I0607 07:55:24.102656 139945914124032 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.559165, loss=5.647282
I0607 07:58:40.827652 139945905731328 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.655247, loss=5.493335
I0607 08:01:56.624651 139945914124032 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.563048, loss=5.645772
I0607 08:01:58.236790 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:02:44.132306 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:03:29.936673 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:03:31.372812 139989027190592 submission_runner.py:419] Time since start: 4237.72s, 	Step: 8505, 	{'train/accuracy': 0.2765625, 'train/loss': 3.7673876953125, 'validation/accuracy': 0.25058, 'validation/loss': 3.900743125, 'validation/num_examples': 50000, 'test/accuracy': 0.1925, 'test/loss': 4.310465625, 'test/num_examples': 10000, 'score': 3362.427323579788, 'total_duration': 4237.7151782512665, 'accumulated_submission_time': 3362.427323579788, 'accumulated_eval_time': 869.9019396305084, 'accumulated_logging_time': 0.16523385047912598}
I0607 08:03:31.382624 139945905731328 logging_writer.py:48] [8505] accumulated_eval_time=869.901940, accumulated_logging_time=0.165234, accumulated_submission_time=3362.427324, global_step=8505, preemption_count=0, score=3362.427324, test/accuracy=0.192500, test/loss=4.310466, test/num_examples=10000, total_duration=4237.715178, train/accuracy=0.276562, train/loss=3.767388, validation/accuracy=0.250580, validation/loss=3.900743, validation/num_examples=50000
I0607 08:06:49.900920 139945914124032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.602830, loss=5.597997
I0607 08:10:04.959263 139945905731328 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.518729, loss=5.404372
I0607 08:10:31.458816 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:11:16.168768 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:12:02.215212 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:12:03.648382 139989027190592 submission_runner.py:419] Time since start: 4749.99s, 	Step: 9569, 	{'train/accuracy': 0.31384765625, 'train/loss': 3.5092242431640623, 'validation/accuracy': 0.2844, 'validation/loss': 3.6579265625, 'validation/num_examples': 50000, 'test/accuracy': 0.2149, 'test/loss': 4.137703515625, 'test/num_examples': 10000, 'score': 3781.8462586402893, 'total_duration': 4749.990668058395, 'accumulated_submission_time': 3781.8462586402893, 'accumulated_eval_time': 962.0914425849915, 'accumulated_logging_time': 0.18298816680908203}
I0607 08:12:03.658581 139945914124032 logging_writer.py:48] [9569] accumulated_eval_time=962.091443, accumulated_logging_time=0.182988, accumulated_submission_time=3781.846259, global_step=9569, preemption_count=0, score=3781.846259, test/accuracy=0.214900, test/loss=4.137704, test/num_examples=10000, total_duration=4749.990668, train/accuracy=0.313848, train/loss=3.509224, validation/accuracy=0.284400, validation/loss=3.657927, validation/num_examples=50000
I0607 08:14:55.990985 139945905731328 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.528592, loss=5.270213
I0607 08:18:13.590271 139945914124032 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.549554, loss=5.325079
I0607 08:19:03.969306 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:19:49.002410 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:20:34.365155 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:20:35.807477 139989027190592 submission_runner.py:419] Time since start: 5262.15s, 	Step: 10630, 	{'train/accuracy': 0.336328125, 'train/loss': 3.3631167602539063, 'validation/accuracy': 0.30742, 'validation/loss': 3.516368125, 'validation/num_examples': 50000, 'test/accuracy': 0.2369, 'test/loss': 3.9875234375, 'test/num_examples': 10000, 'score': 4201.509041547775, 'total_duration': 5262.149795770645, 'accumulated_submission_time': 4201.509041547775, 'accumulated_eval_time': 1053.929636001587, 'accumulated_logging_time': 0.20192193984985352}
I0607 08:20:35.817909 139945905731328 logging_writer.py:48] [10630] accumulated_eval_time=1053.929636, accumulated_logging_time=0.201922, accumulated_submission_time=4201.509042, global_step=10630, preemption_count=0, score=4201.509042, test/accuracy=0.236900, test/loss=3.987523, test/num_examples=10000, total_duration=5262.149796, train/accuracy=0.336328, train/loss=3.363117, validation/accuracy=0.307420, validation/loss=3.516368, validation/num_examples=50000
I0607 08:23:00.423676 139945914124032 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.504628, loss=5.336037
I0607 08:26:21.114177 139945905731328 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.568702, loss=5.189678
I0607 08:27:35.961235 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:28:21.182302 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:29:06.674091 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:29:08.114130 139989027190592 submission_runner.py:419] Time since start: 5774.46s, 	Step: 11693, 	{'train/accuracy': 0.36359375, 'train/loss': 3.16656005859375, 'validation/accuracy': 0.32978, 'validation/loss': 3.3330153125, 'validation/num_examples': 50000, 'test/accuracy': 0.2595, 'test/loss': 3.809386328125, 'test/num_examples': 10000, 'score': 4621.015128612518, 'total_duration': 5774.456464767456, 'accumulated_submission_time': 4621.015128612518, 'accumulated_eval_time': 1146.0826177597046, 'accumulated_logging_time': 0.22032809257507324}
I0607 08:29:08.124194 139945914124032 logging_writer.py:48] [11693] accumulated_eval_time=1146.082618, accumulated_logging_time=0.220328, accumulated_submission_time=4621.015129, global_step=11693, preemption_count=0, score=4621.015129, test/accuracy=0.259500, test/loss=3.809386, test/num_examples=10000, total_duration=5774.456465, train/accuracy=0.363594, train/loss=3.166560, validation/accuracy=0.329780, validation/loss=3.333015, validation/num_examples=50000
I0607 08:31:07.889934 139945905731328 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.598977, loss=5.009250
I0607 08:34:27.009775 139945914124032 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.557575, loss=5.295153
I0607 08:36:08.463271 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:36:53.481990 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:37:39.068229 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:37:40.504295 139989027190592 submission_runner.py:419] Time since start: 6286.85s, 	Step: 12755, 	{'train/accuracy': 0.39318359375, 'train/loss': 3.022621765136719, 'validation/accuracy': 0.36166, 'validation/loss': 3.196005, 'validation/num_examples': 50000, 'test/accuracy': 0.2757, 'test/loss': 3.739610546875, 'test/num_examples': 10000, 'score': 5040.703492879868, 'total_duration': 6286.846559762955, 'accumulated_submission_time': 5040.703492879868, 'accumulated_eval_time': 1238.1237270832062, 'accumulated_logging_time': 0.2383556365966797}
I0607 08:37:40.514507 139945905731328 logging_writer.py:48] [12755] accumulated_eval_time=1238.123727, accumulated_logging_time=0.238356, accumulated_submission_time=5040.703493, global_step=12755, preemption_count=0, score=5040.703493, test/accuracy=0.275700, test/loss=3.739611, test/num_examples=10000, total_duration=6286.846560, train/accuracy=0.393184, train/loss=3.022622, validation/accuracy=0.361660, validation/loss=3.196005, validation/num_examples=50000
I0607 08:39:16.431405 139945914124032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.547713, loss=5.330877
I0607 08:42:31.347352 139945905731328 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.526272, loss=5.031852
I0607 08:44:40.723137 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:45:25.881991 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:46:10.738374 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:46:12.171594 139989027190592 submission_runner.py:419] Time since start: 6798.51s, 	Step: 13818, 	{'train/accuracy': 0.41384765625, 'train/loss': 2.9016952514648438, 'validation/accuracy': 0.3759, 'validation/loss': 3.09055375, 'validation/num_examples': 50000, 'test/accuracy': 0.2921, 'test/loss': 3.60971015625, 'test/num_examples': 10000, 'score': 5460.2712507247925, 'total_duration': 6798.5139672756195, 'accumulated_submission_time': 5460.2712507247925, 'accumulated_eval_time': 1329.5722353458405, 'accumulated_logging_time': 0.25736451148986816}
I0607 08:46:12.182377 139945914124032 logging_writer.py:48] [13818] accumulated_eval_time=1329.572235, accumulated_logging_time=0.257365, accumulated_submission_time=5460.271251, global_step=13818, preemption_count=0, score=5460.271251, test/accuracy=0.292100, test/loss=3.609710, test/num_examples=10000, total_duration=6798.513967, train/accuracy=0.413848, train/loss=2.901695, validation/accuracy=0.375900, validation/loss=3.090554, validation/num_examples=50000
I0607 08:47:23.565648 139945905731328 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.549705, loss=5.427316
I0607 08:50:38.548970 139945914124032 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.625285, loss=4.737300
I0607 08:53:12.278290 139989027190592 spec.py:298] Evaluating on the training split.
I0607 08:53:58.019947 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 08:54:43.509424 139989027190592 spec.py:326] Evaluating on the test split.
I0607 08:54:44.940593 139989027190592 submission_runner.py:419] Time since start: 7311.28s, 	Step: 14889, 	{'train/accuracy': 0.42787109375, 'train/loss': 2.8932342529296875, 'validation/accuracy': 0.3908, 'validation/loss': 3.077198125, 'validation/num_examples': 50000, 'test/accuracy': 0.3037, 'test/loss': 3.60474921875, 'test/num_examples': 10000, 'score': 5879.701997756958, 'total_duration': 7311.282973527908, 'accumulated_submission_time': 5879.701997756958, 'accumulated_eval_time': 1422.2345299720764, 'accumulated_logging_time': 0.2780754566192627}
I0607 08:54:44.953412 139945905731328 logging_writer.py:48] [14889] accumulated_eval_time=1422.234530, accumulated_logging_time=0.278075, accumulated_submission_time=5879.701998, global_step=14889, preemption_count=0, score=5879.701998, test/accuracy=0.303700, test/loss=3.604749, test/num_examples=10000, total_duration=7311.282974, train/accuracy=0.427871, train/loss=2.893234, validation/accuracy=0.390800, validation/loss=3.077198, validation/num_examples=50000
I0607 08:55:29.813222 139945914124032 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.534937, loss=5.193404
I0607 08:58:47.035655 139945905731328 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.583181, loss=4.646547
I0607 09:01:45.287581 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:02:29.885704 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:03:15.345070 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:03:16.782041 139989027190592 submission_runner.py:419] Time since start: 7823.12s, 	Step: 15957, 	{'train/accuracy': 0.45201171875, 'train/loss': 2.725330810546875, 'validation/accuracy': 0.41268, 'validation/loss': 2.9218, 'validation/num_examples': 50000, 'test/accuracy': 0.3181, 'test/loss': 3.47887265625, 'test/num_examples': 10000, 'score': 6299.385862112045, 'total_duration': 7823.124364376068, 'accumulated_submission_time': 6299.385862112045, 'accumulated_eval_time': 1513.7289867401123, 'accumulated_logging_time': 0.2994375228881836}
I0607 09:03:16.792100 139945914124032 logging_writer.py:48] [15957] accumulated_eval_time=1513.728987, accumulated_logging_time=0.299438, accumulated_submission_time=6299.385862, global_step=15957, preemption_count=0, score=6299.385862, test/accuracy=0.318100, test/loss=3.478873, test/num_examples=10000, total_duration=7823.124364, train/accuracy=0.452012, train/loss=2.725331, validation/accuracy=0.412680, validation/loss=2.921800, validation/num_examples=50000
I0607 09:03:33.850282 139945905731328 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.587268, loss=5.038765
I0607 09:06:55.732066 139945914124032 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.647994, loss=4.650371
I0607 09:10:10.893629 139945905731328 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.561123, loss=4.909279
I0607 09:10:17.153927 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:11:02.392554 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:11:47.505441 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:11:48.942694 139989027190592 submission_runner.py:419] Time since start: 8335.29s, 	Step: 17017, 	{'train/accuracy': 0.4660546875, 'train/loss': 2.62017822265625, 'validation/accuracy': 0.42384, 'validation/loss': 2.8271321875, 'validation/num_examples': 50000, 'test/accuracy': 0.3245, 'test/loss': 3.406220703125, 'test/num_examples': 10000, 'score': 6719.104863166809, 'total_duration': 8335.285031318665, 'accumulated_submission_time': 6719.104863166809, 'accumulated_eval_time': 1605.5176692008972, 'accumulated_logging_time': 0.31922245025634766}
I0607 09:11:48.953992 139945914124032 logging_writer.py:48] [17017] accumulated_eval_time=1605.517669, accumulated_logging_time=0.319222, accumulated_submission_time=6719.104863, global_step=17017, preemption_count=0, score=6719.104863, test/accuracy=0.324500, test/loss=3.406221, test/num_examples=10000, total_duration=8335.285031, train/accuracy=0.466055, train/loss=2.620178, validation/accuracy=0.423840, validation/loss=2.827132, validation/num_examples=50000
I0607 09:15:00.989517 139945905731328 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.511413, loss=5.032372
I0607 09:18:18.218233 139945914124032 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.544694, loss=5.122507
I0607 09:18:49.070205 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:19:34.049021 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:20:19.724945 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:20:21.159221 139989027190592 submission_runner.py:419] Time since start: 8847.50s, 	Step: 18080, 	{'train/accuracy': 0.48859375, 'train/loss': 2.5167784118652343, 'validation/accuracy': 0.44384, 'validation/loss': 2.7273859375, 'validation/num_examples': 50000, 'test/accuracy': 0.3457, 'test/loss': 3.3038578125, 'test/num_examples': 10000, 'score': 7138.56667137146, 'total_duration': 8847.50152850151, 'accumulated_submission_time': 7138.56667137146, 'accumulated_eval_time': 1697.6066262722015, 'accumulated_logging_time': 0.3389284610748291}
I0607 09:20:21.169551 139945905731328 logging_writer.py:48] [18080] accumulated_eval_time=1697.606626, accumulated_logging_time=0.338928, accumulated_submission_time=7138.566671, global_step=18080, preemption_count=0, score=7138.566671, test/accuracy=0.345700, test/loss=3.303858, test/num_examples=10000, total_duration=8847.501529, train/accuracy=0.488594, train/loss=2.516778, validation/accuracy=0.443840, validation/loss=2.727386, validation/num_examples=50000
I0607 09:23:05.557415 139945914124032 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.593945, loss=4.956208
I0607 09:26:26.917975 139945905731328 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.612958, loss=4.831450
I0607 09:27:21.244622 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:28:06.671898 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:28:51.754895 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:28:53.195797 139989027190592 submission_runner.py:419] Time since start: 9359.54s, 	Step: 19140, 	{'train/accuracy': 0.50171875, 'train/loss': 2.4302407836914064, 'validation/accuracy': 0.45448, 'validation/loss': 2.6533515625, 'validation/num_examples': 50000, 'test/accuracy': 0.3505, 'test/loss': 3.238496875, 'test/num_examples': 10000, 'score': 7557.983078241348, 'total_duration': 9359.538076877594, 'accumulated_submission_time': 7557.983078241348, 'accumulated_eval_time': 1789.5577504634857, 'accumulated_logging_time': 0.3572392463684082}
I0607 09:28:53.207841 139945914124032 logging_writer.py:48] [19140] accumulated_eval_time=1789.557750, accumulated_logging_time=0.357239, accumulated_submission_time=7557.983078, global_step=19140, preemption_count=0, score=7557.983078, test/accuracy=0.350500, test/loss=3.238497, test/num_examples=10000, total_duration=9359.538077, train/accuracy=0.501719, train/loss=2.430241, validation/accuracy=0.454480, validation/loss=2.653352, validation/num_examples=50000
I0607 09:31:14.287454 139945905731328 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.695605, loss=4.477177
I0607 09:34:33.224117 139945914124032 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.579566, loss=4.778567
I0607 09:35:53.393419 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:36:38.111970 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:37:23.025562 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:37:24.463551 139989027190592 submission_runner.py:419] Time since start: 9870.81s, 	Step: 20200, 	{'train/accuracy': 0.52119140625, 'train/loss': 2.306282958984375, 'validation/accuracy': 0.47096, 'validation/loss': 2.53298421875, 'validation/num_examples': 50000, 'test/accuracy': 0.3654, 'test/loss': 3.13711640625, 'test/num_examples': 10000, 'score': 7977.5208649635315, 'total_duration': 9870.805797100067, 'accumulated_submission_time': 7977.5208649635315, 'accumulated_eval_time': 1880.6277930736542, 'accumulated_logging_time': 0.37806034088134766}
I0607 09:37:24.474951 139945905731328 logging_writer.py:48] [20200] accumulated_eval_time=1880.627793, accumulated_logging_time=0.378060, accumulated_submission_time=7977.520865, global_step=20200, preemption_count=0, score=7977.520865, test/accuracy=0.365400, test/loss=3.137116, test/num_examples=10000, total_duration=9870.805797, train/accuracy=0.521191, train/loss=2.306283, validation/accuracy=0.470960, validation/loss=2.532984, validation/num_examples=50000
I0607 09:39:21.645493 139945914124032 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.633571, loss=4.602851
I0607 09:42:37.352570 139945905731328 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.531468, loss=5.130307
I0607 09:44:26.056537 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:45:14.557776 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:46:02.484608 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:46:03.924360 139989027190592 submission_runner.py:419] Time since start: 10390.27s, 	Step: 21268, 	{'train/accuracy': 0.52189453125, 'train/loss': 2.353961944580078, 'validation/accuracy': 0.4726, 'validation/loss': 2.58099875, 'validation/num_examples': 50000, 'test/accuracy': 0.3671, 'test/loss': 3.1641763671875, 'test/num_examples': 10000, 'score': 8398.447336435318, 'total_duration': 10390.26659154892, 'accumulated_submission_time': 8398.447336435318, 'accumulated_eval_time': 1978.4955728054047, 'accumulated_logging_time': 0.39896392822265625}
I0607 09:46:03.937156 139945914124032 logging_writer.py:48] [21268] accumulated_eval_time=1978.495573, accumulated_logging_time=0.398964, accumulated_submission_time=8398.447336, global_step=21268, preemption_count=0, score=8398.447336, test/accuracy=0.367100, test/loss=3.164176, test/num_examples=10000, total_duration=10390.266592, train/accuracy=0.521895, train/loss=2.353962, validation/accuracy=0.472600, validation/loss=2.580999, validation/num_examples=50000
I0607 09:47:34.979977 139945905731328 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.588813, loss=4.712140
I0607 09:50:49.826307 139945914124032 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.618662, loss=4.603568
I0607 09:53:04.044236 139989027190592 spec.py:298] Evaluating on the training split.
I0607 09:53:48.529439 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 09:54:33.318265 139989027190592 spec.py:326] Evaluating on the test split.
I0607 09:54:34.752683 139989027190592 submission_runner.py:419] Time since start: 10901.10s, 	Step: 22342, 	{'train/accuracy': 0.53634765625, 'train/loss': 2.258028717041016, 'validation/accuracy': 0.4851, 'validation/loss': 2.4922884375, 'validation/num_examples': 50000, 'test/accuracy': 0.3798, 'test/loss': 3.0956224609375, 'test/num_examples': 10000, 'score': 8817.898244857788, 'total_duration': 10901.095012187958, 'accumulated_submission_time': 8817.898244857788, 'accumulated_eval_time': 2069.204158782959, 'accumulated_logging_time': 0.4212656021118164}
I0607 09:54:34.764517 139945905731328 logging_writer.py:48] [22342] accumulated_eval_time=2069.204159, accumulated_logging_time=0.421266, accumulated_submission_time=8817.898245, global_step=22342, preemption_count=0, score=8817.898245, test/accuracy=0.379800, test/loss=3.095622, test/num_examples=10000, total_duration=10901.095012, train/accuracy=0.536348, train/loss=2.258029, validation/accuracy=0.485100, validation/loss=2.492288, validation/num_examples=50000
I0607 09:55:38.776561 139945914124032 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.539964, loss=4.748875
I0607 09:58:56.495564 139945905731328 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.587310, loss=4.891487
I0607 10:01:34.955873 139989027190592 spec.py:298] Evaluating on the training split.
I0607 10:02:20.211821 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 10:03:05.883112 139989027190592 spec.py:326] Evaluating on the test split.
I0607 10:03:07.320894 139989027190592 submission_runner.py:419] Time since start: 11413.66s, 	Step: 23407, 	{'train/accuracy': 0.5525, 'train/loss': 2.2130686950683596, 'validation/accuracy': 0.49992, 'validation/loss': 2.45232109375, 'validation/num_examples': 50000, 'test/accuracy': 0.3919, 'test/loss': 3.046474609375, 'test/num_examples': 10000, 'score': 9237.432081699371, 'total_duration': 11413.66327381134, 'accumulated_submission_time': 9237.432081699371, 'accumulated_eval_time': 2161.569289445877, 'accumulated_logging_time': 0.4417691230773926}
I0607 10:03:07.331538 139945914124032 logging_writer.py:48] [23407] accumulated_eval_time=2161.569289, accumulated_logging_time=0.441769, accumulated_submission_time=9237.432082, global_step=23407, preemption_count=0, score=9237.432082, test/accuracy=0.391900, test/loss=3.046475, test/num_examples=10000, total_duration=11413.663274, train/accuracy=0.552500, train/loss=2.213069, validation/accuracy=0.499920, validation/loss=2.452321, validation/num_examples=50000
I0607 10:03:44.403913 139945905731328 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.678691, loss=4.504426
I0607 10:07:05.757665 139945914124032 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.623109, loss=4.376956
I0607 10:10:07.464390 139989027190592 spec.py:298] Evaluating on the training split.
I0607 10:10:52.483241 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 10:11:38.249344 139989027190592 spec.py:326] Evaluating on the test split.
I0607 10:11:39.687801 139989027190592 submission_runner.py:419] Time since start: 11926.03s, 	Step: 24466, 	{'train/accuracy': 0.5576171875, 'train/loss': 2.1414308166503906, 'validation/accuracy': 0.5017, 'validation/loss': 2.39850046875, 'validation/num_examples': 50000, 'test/accuracy': 0.3937, 'test/loss': 3.0115390625, 'test/num_examples': 10000, 'score': 9656.903904914856, 'total_duration': 11926.030137062073, 'accumulated_submission_time': 9656.903904914856, 'accumulated_eval_time': 2253.7928602695465, 'accumulated_logging_time': 0.46236324310302734}
I0607 10:11:39.698571 139945905731328 logging_writer.py:48] [24466] accumulated_eval_time=2253.792860, accumulated_logging_time=0.462363, accumulated_submission_time=9656.903905, global_step=24466, preemption_count=0, score=9656.903905, test/accuracy=0.393700, test/loss=3.011539, test/num_examples=10000, total_duration=11926.030137, train/accuracy=0.557617, train/loss=2.141431, validation/accuracy=0.501700, validation/loss=2.398500, validation/num_examples=50000
I0607 10:11:53.311291 139945914124032 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.633796, loss=4.581283
I0607 10:15:12.221767 139945905731328 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.585962, loss=4.697189
I0607 10:18:29.924393 139945914124032 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.588885, loss=4.635520
I0607 10:18:40.068335 139989027190592 spec.py:298] Evaluating on the training split.
I0607 10:19:25.540109 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 10:20:10.883310 139989027190592 spec.py:326] Evaluating on the test split.
I0607 10:20:12.318589 139989027190592 submission_runner.py:419] Time since start: 12438.66s, 	Step: 25527, 	{'train/accuracy': 0.56931640625, 'train/loss': 2.1410311889648437, 'validation/accuracy': 0.51466, 'validation/loss': 2.39132984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4011, 'test/loss': 2.9879015625, 'test/num_examples': 10000, 'score': 10076.61198925972, 'total_duration': 12438.6609416008, 'accumulated_submission_time': 10076.61198925972, 'accumulated_eval_time': 2346.0431690216064, 'accumulated_logging_time': 0.4815998077392578}
I0607 10:20:12.329804 139945905731328 logging_writer.py:48] [25527] accumulated_eval_time=2346.043169, accumulated_logging_time=0.481600, accumulated_submission_time=10076.611989, global_step=25527, preemption_count=0, score=10076.611989, test/accuracy=0.401100, test/loss=2.987902, test/num_examples=10000, total_duration=12438.660942, train/accuracy=0.569316, train/loss=2.141031, validation/accuracy=0.514660, validation/loss=2.391330, validation/num_examples=50000
I0607 10:23:17.523511 139945914124032 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.624260, loss=4.556948
I0607 10:26:38.231313 139945905731328 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.702996, loss=4.370811
I0607 10:27:12.642735 139989027190592 spec.py:298] Evaluating on the training split.
I0607 10:27:58.946063 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 10:28:44.738466 139989027190592 spec.py:326] Evaluating on the test split.
I0607 10:28:46.177322 139989027190592 submission_runner.py:419] Time since start: 12952.52s, 	Step: 26589, 	{'train/accuracy': 0.58044921875, 'train/loss': 2.0083384704589844, 'validation/accuracy': 0.52534, 'validation/loss': 2.27333015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4139, 'test/loss': 2.8756615234375, 'test/num_examples': 10000, 'score': 10496.263469934464, 'total_duration': 12952.51955485344, 'accumulated_submission_time': 10496.263469934464, 'accumulated_eval_time': 2439.5775997638702, 'accumulated_logging_time': 0.5005035400390625}
I0607 10:28:46.190178 139945914124032 logging_writer.py:48] [26589] accumulated_eval_time=2439.577600, accumulated_logging_time=0.500504, accumulated_submission_time=10496.263470, global_step=26589, preemption_count=0, score=10496.263470, test/accuracy=0.413900, test/loss=2.875662, test/num_examples=10000, total_duration=12952.519555, train/accuracy=0.580449, train/loss=2.008338, validation/accuracy=0.525340, validation/loss=2.273330, validation/num_examples=50000
I0607 10:31:27.336124 139945905731328 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.579293, loss=4.624967
I0607 10:34:46.143111 139945914124032 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.641973, loss=4.328166
I0607 10:35:46.283525 139989027190592 spec.py:298] Evaluating on the training split.
I0607 10:36:31.601777 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 10:37:17.368140 139989027190592 spec.py:326] Evaluating on the test split.
I0607 10:37:18.799854 139989027190592 submission_runner.py:419] Time since start: 13465.14s, 	Step: 27649, 	{'train/accuracy': 0.58712890625, 'train/loss': 2.0247796630859374, 'validation/accuracy': 0.52964, 'validation/loss': 2.28833140625, 'validation/num_examples': 50000, 'test/accuracy': 0.4156, 'test/loss': 2.892295703125, 'test/num_examples': 10000, 'score': 10915.69979429245, 'total_duration': 13465.142020225525, 'accumulated_submission_time': 10915.69979429245, 'accumulated_eval_time': 2532.093765258789, 'accumulated_logging_time': 0.5219442844390869}
I0607 10:37:18.811387 139945905731328 logging_writer.py:48] [27649] accumulated_eval_time=2532.093765, accumulated_logging_time=0.521944, accumulated_submission_time=10915.699794, global_step=27649, preemption_count=0, score=10915.699794, test/accuracy=0.415600, test/loss=2.892296, test/num_examples=10000, total_duration=13465.142020, train/accuracy=0.587129, train/loss=2.024780, validation/accuracy=0.529640, validation/loss=2.288331, validation/num_examples=50000
I0607 10:39:35.629809 139989027190592 spec.py:298] Evaluating on the training split.
I0607 10:40:20.634949 139989027190592 spec.py:310] Evaluating on the validation split.
I0607 10:41:05.914186 139989027190592 spec.py:326] Evaluating on the test split.
I0607 10:41:07.360211 139989027190592 submission_runner.py:419] Time since start: 13693.70s, 	Step: 28000, 	{'train/accuracy': 0.5923828125, 'train/loss': 1.997209014892578, 'validation/accuracy': 0.53652, 'validation/loss': 2.25481859375, 'validation/num_examples': 50000, 'test/accuracy': 0.421, 'test/loss': 2.8524482421875, 'test/num_examples': 10000, 'score': 11052.285992860794, 'total_duration': 13693.702577114105, 'accumulated_submission_time': 11052.285992860794, 'accumulated_eval_time': 2623.824229478836, 'accumulated_logging_time': 0.5422286987304688}
I0607 10:41:07.371175 139945914124032 logging_writer.py:48] [28000] accumulated_eval_time=2623.824229, accumulated_logging_time=0.542229, accumulated_submission_time=11052.285993, global_step=28000, preemption_count=0, score=11052.285993, test/accuracy=0.421000, test/loss=2.852448, test/num_examples=10000, total_duration=13693.702577, train/accuracy=0.592383, train/loss=1.997209, validation/accuracy=0.536520, validation/loss=2.254819, validation/num_examples=50000
I0607 10:41:07.389856 139945905731328 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11052.285993
I0607 10:41:07.916013 139989027190592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0607 10:41:08.173258 139989027190592 submission_runner.py:581] Tuning trial 1/1
I0607 10:41:08.173461 139989027190592 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 10:41:08.174482 139989027190592 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012109375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.51295280456543, 'total_duration': 134.42133784294128, 'accumulated_submission_time': 6.51295280456543, 'accumulated_eval_time': 127.9079225063324, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1035, {'train/accuracy': 0.0437890625, 'train/loss': 5.957134399414063, 'validation/accuracy': 0.04298, 'validation/loss': 5.98637, 'validation/num_examples': 50000, 'test/accuracy': 0.0329, 'test/loss': 6.101221875, 'test/num_examples': 10000, 'score': 426.1436882019043, 'total_duration': 642.0879812240601, 'accumulated_submission_time': 426.1436882019043, 'accumulated_eval_time': 215.26170301437378, 'accumulated_logging_time': 0.02908945083618164, 'global_step': 1035, 'preemption_count': 0}), (2099, {'train/accuracy': 0.0801171875, 'train/loss': 5.418282470703125, 'validation/accuracy': 0.07614, 'validation/loss': 5.468415625, 'validation/num_examples': 50000, 'test/accuracy': 0.054, 'test/loss': 5.66853203125, 'test/num_examples': 10000, 'score': 845.5697410106659, 'total_duration': 1153.1136319637299, 'accumulated_submission_time': 845.5697410106659, 'accumulated_eval_time': 306.1875250339508, 'accumulated_logging_time': 0.046944618225097656, 'global_step': 2099, 'preemption_count': 0}), (3161, {'train/accuracy': 0.1084375, 'train/loss': 5.111151428222656, 'validation/accuracy': 0.10176, 'validation/loss': 5.1806028125, 'validation/num_examples': 50000, 'test/accuracy': 0.0744, 'test/loss': 5.434167578125, 'test/num_examples': 10000, 'score': 1264.943895816803, 'total_duration': 1673.0565927028656, 'accumulated_submission_time': 1264.943895816803, 'accumulated_eval_time': 406.08704924583435, 'accumulated_logging_time': 0.06462407112121582, 'global_step': 3161, 'preemption_count': 0}), (4234, {'train/accuracy': 0.14025390625, 'train/loss': 4.803804931640625, 'validation/accuracy': 0.12552, 'validation/loss': 4.8888453125, 'validation/num_examples': 50000, 'test/accuracy': 0.0937, 'test/loss': 5.1858453125, 'test/num_examples': 10000, 'score': 1684.2717118263245, 'total_duration': 2182.571363210678, 'accumulated_submission_time': 1684.2717118263245, 'accumulated_eval_time': 495.5945875644684, 'accumulated_logging_time': 0.08278656005859375, 'global_step': 4234, 'preemption_count': 0}), (5299, {'train/accuracy': 0.17234375, 'train/loss': 4.583239135742187, 'validation/accuracy': 0.15772, 'validation/loss': 4.66751625, 'validation/num_examples': 50000, 'test/accuracy': 0.1213, 'test/loss': 4.97475546875, 'test/num_examples': 10000, 'score': 2103.854108095169, 'total_duration': 2694.3962087631226, 'accumulated_submission_time': 2103.854108095169, 'accumulated_eval_time': 587.1718416213989, 'accumulated_logging_time': 0.10404109954833984, 'global_step': 5299, 'preemption_count': 0}), (6364, {'train/accuracy': 0.204375, 'train/loss': 4.289826965332031, 'validation/accuracy': 0.18584, 'validation/loss': 4.40291375, 'validation/num_examples': 50000, 'test/accuracy': 0.1403, 'test/loss': 4.76718984375, 'test/num_examples': 10000, 'score': 2523.4856894016266, 'total_duration': 3207.698268175125, 'accumulated_submission_time': 2523.4856894016266, 'accumulated_eval_time': 680.1778190135956, 'accumulated_logging_time': 0.12126827239990234, 'global_step': 6364, 'preemption_count': 0}), (7435, {'train/accuracy': 0.245859375, 'train/loss': 3.981947326660156, 'validation/accuracy': 0.22122, 'validation/loss': 4.1086925, 'validation/num_examples': 50000, 'test/accuracy': 0.1708, 'test/loss': 4.4966015625, 'test/num_examples': 10000, 'score': 2942.8394045829773, 'total_duration': 3724.3154344558716, 'accumulated_submission_time': 2942.8394045829773, 'accumulated_eval_time': 776.7658648490906, 'accumulated_logging_time': 0.14351105690002441, 'global_step': 7435, 'preemption_count': 0}), (8505, {'train/accuracy': 0.2765625, 'train/loss': 3.7673876953125, 'validation/accuracy': 0.25058, 'validation/loss': 3.900743125, 'validation/num_examples': 50000, 'test/accuracy': 0.1925, 'test/loss': 4.310465625, 'test/num_examples': 10000, 'score': 3362.427323579788, 'total_duration': 4237.7151782512665, 'accumulated_submission_time': 3362.427323579788, 'accumulated_eval_time': 869.9019396305084, 'accumulated_logging_time': 0.16523385047912598, 'global_step': 8505, 'preemption_count': 0}), (9569, {'train/accuracy': 0.31384765625, 'train/loss': 3.5092242431640623, 'validation/accuracy': 0.2844, 'validation/loss': 3.6579265625, 'validation/num_examples': 50000, 'test/accuracy': 0.2149, 'test/loss': 4.137703515625, 'test/num_examples': 10000, 'score': 3781.8462586402893, 'total_duration': 4749.990668058395, 'accumulated_submission_time': 3781.8462586402893, 'accumulated_eval_time': 962.0914425849915, 'accumulated_logging_time': 0.18298816680908203, 'global_step': 9569, 'preemption_count': 0}), (10630, {'train/accuracy': 0.336328125, 'train/loss': 3.3631167602539063, 'validation/accuracy': 0.30742, 'validation/loss': 3.516368125, 'validation/num_examples': 50000, 'test/accuracy': 0.2369, 'test/loss': 3.9875234375, 'test/num_examples': 10000, 'score': 4201.509041547775, 'total_duration': 5262.149795770645, 'accumulated_submission_time': 4201.509041547775, 'accumulated_eval_time': 1053.929636001587, 'accumulated_logging_time': 0.20192193984985352, 'global_step': 10630, 'preemption_count': 0}), (11693, {'train/accuracy': 0.36359375, 'train/loss': 3.16656005859375, 'validation/accuracy': 0.32978, 'validation/loss': 3.3330153125, 'validation/num_examples': 50000, 'test/accuracy': 0.2595, 'test/loss': 3.809386328125, 'test/num_examples': 10000, 'score': 4621.015128612518, 'total_duration': 5774.456464767456, 'accumulated_submission_time': 4621.015128612518, 'accumulated_eval_time': 1146.0826177597046, 'accumulated_logging_time': 0.22032809257507324, 'global_step': 11693, 'preemption_count': 0}), (12755, {'train/accuracy': 0.39318359375, 'train/loss': 3.022621765136719, 'validation/accuracy': 0.36166, 'validation/loss': 3.196005, 'validation/num_examples': 50000, 'test/accuracy': 0.2757, 'test/loss': 3.739610546875, 'test/num_examples': 10000, 'score': 5040.703492879868, 'total_duration': 6286.846559762955, 'accumulated_submission_time': 5040.703492879868, 'accumulated_eval_time': 1238.1237270832062, 'accumulated_logging_time': 0.2383556365966797, 'global_step': 12755, 'preemption_count': 0}), (13818, {'train/accuracy': 0.41384765625, 'train/loss': 2.9016952514648438, 'validation/accuracy': 0.3759, 'validation/loss': 3.09055375, 'validation/num_examples': 50000, 'test/accuracy': 0.2921, 'test/loss': 3.60971015625, 'test/num_examples': 10000, 'score': 5460.2712507247925, 'total_duration': 6798.5139672756195, 'accumulated_submission_time': 5460.2712507247925, 'accumulated_eval_time': 1329.5722353458405, 'accumulated_logging_time': 0.25736451148986816, 'global_step': 13818, 'preemption_count': 0}), (14889, {'train/accuracy': 0.42787109375, 'train/loss': 2.8932342529296875, 'validation/accuracy': 0.3908, 'validation/loss': 3.077198125, 'validation/num_examples': 50000, 'test/accuracy': 0.3037, 'test/loss': 3.60474921875, 'test/num_examples': 10000, 'score': 5879.701997756958, 'total_duration': 7311.282973527908, 'accumulated_submission_time': 5879.701997756958, 'accumulated_eval_time': 1422.2345299720764, 'accumulated_logging_time': 0.2780754566192627, 'global_step': 14889, 'preemption_count': 0}), (15957, {'train/accuracy': 0.45201171875, 'train/loss': 2.725330810546875, 'validation/accuracy': 0.41268, 'validation/loss': 2.9218, 'validation/num_examples': 50000, 'test/accuracy': 0.3181, 'test/loss': 3.47887265625, 'test/num_examples': 10000, 'score': 6299.385862112045, 'total_duration': 7823.124364376068, 'accumulated_submission_time': 6299.385862112045, 'accumulated_eval_time': 1513.7289867401123, 'accumulated_logging_time': 0.2994375228881836, 'global_step': 15957, 'preemption_count': 0}), (17017, {'train/accuracy': 0.4660546875, 'train/loss': 2.62017822265625, 'validation/accuracy': 0.42384, 'validation/loss': 2.8271321875, 'validation/num_examples': 50000, 'test/accuracy': 0.3245, 'test/loss': 3.406220703125, 'test/num_examples': 10000, 'score': 6719.104863166809, 'total_duration': 8335.285031318665, 'accumulated_submission_time': 6719.104863166809, 'accumulated_eval_time': 1605.5176692008972, 'accumulated_logging_time': 0.31922245025634766, 'global_step': 17017, 'preemption_count': 0}), (18080, {'train/accuracy': 0.48859375, 'train/loss': 2.5167784118652343, 'validation/accuracy': 0.44384, 'validation/loss': 2.7273859375, 'validation/num_examples': 50000, 'test/accuracy': 0.3457, 'test/loss': 3.3038578125, 'test/num_examples': 10000, 'score': 7138.56667137146, 'total_duration': 8847.50152850151, 'accumulated_submission_time': 7138.56667137146, 'accumulated_eval_time': 1697.6066262722015, 'accumulated_logging_time': 0.3389284610748291, 'global_step': 18080, 'preemption_count': 0}), (19140, {'train/accuracy': 0.50171875, 'train/loss': 2.4302407836914064, 'validation/accuracy': 0.45448, 'validation/loss': 2.6533515625, 'validation/num_examples': 50000, 'test/accuracy': 0.3505, 'test/loss': 3.238496875, 'test/num_examples': 10000, 'score': 7557.983078241348, 'total_duration': 9359.538076877594, 'accumulated_submission_time': 7557.983078241348, 'accumulated_eval_time': 1789.5577504634857, 'accumulated_logging_time': 0.3572392463684082, 'global_step': 19140, 'preemption_count': 0}), (20200, {'train/accuracy': 0.52119140625, 'train/loss': 2.306282958984375, 'validation/accuracy': 0.47096, 'validation/loss': 2.53298421875, 'validation/num_examples': 50000, 'test/accuracy': 0.3654, 'test/loss': 3.13711640625, 'test/num_examples': 10000, 'score': 7977.5208649635315, 'total_duration': 9870.805797100067, 'accumulated_submission_time': 7977.5208649635315, 'accumulated_eval_time': 1880.6277930736542, 'accumulated_logging_time': 0.37806034088134766, 'global_step': 20200, 'preemption_count': 0}), (21268, {'train/accuracy': 0.52189453125, 'train/loss': 2.353961944580078, 'validation/accuracy': 0.4726, 'validation/loss': 2.58099875, 'validation/num_examples': 50000, 'test/accuracy': 0.3671, 'test/loss': 3.1641763671875, 'test/num_examples': 10000, 'score': 8398.447336435318, 'total_duration': 10390.26659154892, 'accumulated_submission_time': 8398.447336435318, 'accumulated_eval_time': 1978.4955728054047, 'accumulated_logging_time': 0.39896392822265625, 'global_step': 21268, 'preemption_count': 0}), (22342, {'train/accuracy': 0.53634765625, 'train/loss': 2.258028717041016, 'validation/accuracy': 0.4851, 'validation/loss': 2.4922884375, 'validation/num_examples': 50000, 'test/accuracy': 0.3798, 'test/loss': 3.0956224609375, 'test/num_examples': 10000, 'score': 8817.898244857788, 'total_duration': 10901.095012187958, 'accumulated_submission_time': 8817.898244857788, 'accumulated_eval_time': 2069.204158782959, 'accumulated_logging_time': 0.4212656021118164, 'global_step': 22342, 'preemption_count': 0}), (23407, {'train/accuracy': 0.5525, 'train/loss': 2.2130686950683596, 'validation/accuracy': 0.49992, 'validation/loss': 2.45232109375, 'validation/num_examples': 50000, 'test/accuracy': 0.3919, 'test/loss': 3.046474609375, 'test/num_examples': 10000, 'score': 9237.432081699371, 'total_duration': 11413.66327381134, 'accumulated_submission_time': 9237.432081699371, 'accumulated_eval_time': 2161.569289445877, 'accumulated_logging_time': 0.4417691230773926, 'global_step': 23407, 'preemption_count': 0}), (24466, {'train/accuracy': 0.5576171875, 'train/loss': 2.1414308166503906, 'validation/accuracy': 0.5017, 'validation/loss': 2.39850046875, 'validation/num_examples': 50000, 'test/accuracy': 0.3937, 'test/loss': 3.0115390625, 'test/num_examples': 10000, 'score': 9656.903904914856, 'total_duration': 11926.030137062073, 'accumulated_submission_time': 9656.903904914856, 'accumulated_eval_time': 2253.7928602695465, 'accumulated_logging_time': 0.46236324310302734, 'global_step': 24466, 'preemption_count': 0}), (25527, {'train/accuracy': 0.56931640625, 'train/loss': 2.1410311889648437, 'validation/accuracy': 0.51466, 'validation/loss': 2.39132984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4011, 'test/loss': 2.9879015625, 'test/num_examples': 10000, 'score': 10076.61198925972, 'total_duration': 12438.6609416008, 'accumulated_submission_time': 10076.61198925972, 'accumulated_eval_time': 2346.0431690216064, 'accumulated_logging_time': 0.4815998077392578, 'global_step': 25527, 'preemption_count': 0}), (26589, {'train/accuracy': 0.58044921875, 'train/loss': 2.0083384704589844, 'validation/accuracy': 0.52534, 'validation/loss': 2.27333015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4139, 'test/loss': 2.8756615234375, 'test/num_examples': 10000, 'score': 10496.263469934464, 'total_duration': 12952.51955485344, 'accumulated_submission_time': 10496.263469934464, 'accumulated_eval_time': 2439.5775997638702, 'accumulated_logging_time': 0.5005035400390625, 'global_step': 26589, 'preemption_count': 0}), (27649, {'train/accuracy': 0.58712890625, 'train/loss': 2.0247796630859374, 'validation/accuracy': 0.52964, 'validation/loss': 2.28833140625, 'validation/num_examples': 50000, 'test/accuracy': 0.4156, 'test/loss': 2.892295703125, 'test/num_examples': 10000, 'score': 10915.69979429245, 'total_duration': 13465.142020225525, 'accumulated_submission_time': 10915.69979429245, 'accumulated_eval_time': 2532.093765258789, 'accumulated_logging_time': 0.5219442844390869, 'global_step': 27649, 'preemption_count': 0}), (28000, {'train/accuracy': 0.5923828125, 'train/loss': 1.997209014892578, 'validation/accuracy': 0.53652, 'validation/loss': 2.25481859375, 'validation/num_examples': 50000, 'test/accuracy': 0.421, 'test/loss': 2.8524482421875, 'test/num_examples': 10000, 'score': 11052.285992860794, 'total_duration': 13693.702577114105, 'accumulated_submission_time': 11052.285992860794, 'accumulated_eval_time': 2623.824229478836, 'accumulated_logging_time': 0.5422286987304688, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0607 10:41:08.174610 139989027190592 submission_runner.py:584] Timing: 11052.285992860794
I0607 10:41:08.174660 139989027190592 submission_runner.py:586] Total number of evals: 28
I0607 10:41:08.174703 139989027190592 submission_runner.py:587] ====================
I0607 10:41:08.174847 139989027190592 submission_runner.py:655] Final imagenet_vit score: 11052.285992860794
