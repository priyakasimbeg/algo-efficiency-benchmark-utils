torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-07-2023-10-45-42.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 10:46:05.510501 140220877612864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 10:46:05.510541 140457614022464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 10:46:05.510563 140344816109376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 10:46:05.511435 140150134417216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 10:46:05.511515 139928469600064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 10:46:05.511473 139802379282240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 10:46:05.511592 140417923757888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 10:46:05.522644 139893975131968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 10:46:05.522942 139893975131968 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.531532 140220877612864 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.531557 140457614022464 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.531584 140344816109376 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.532398 139928469600064 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.532427 140150134417216 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.532476 139802379282240 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:05.532501 140417923757888 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 10:46:06.690503 139893975131968 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch because --overwrite was set.
I0607 10:46:06.696861 139893975131968 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch.
W0607 10:46:06.733679 139802379282240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.733675 140457614022464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.733766 139928469600064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.733956 140417923757888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.734017 139893975131968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.734545 140344816109376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.734605 140220877612864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 10:46:06.736479 140150134417216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 10:46:06.739438 139893975131968 submission_runner.py:541] Using RNG seed 315155151
I0607 10:46:06.740884 139893975131968 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 10:46:06.741020 139893975131968 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch/trial_1.
I0607 10:46:06.741394 139893975131968 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch/trial_1/hparams.json.
I0607 10:46:06.742505 139893975131968 submission_runner.py:255] Initializing dataset.
I0607 10:46:06.742629 139893975131968 submission_runner.py:262] Initializing model.
I0607 10:46:10.749173 139893975131968 submission_runner.py:272] Initializing optimizer.
I0607 10:46:10.750136 139893975131968 submission_runner.py:279] Initializing metrics bundle.
I0607 10:46:10.750269 139893975131968 submission_runner.py:297] Initializing checkpoint and logger.
I0607 10:46:10.753870 139893975131968 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 10:46:10.753980 139893975131968 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 10:46:11.251544 139893975131968 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0607 10:46:11.252515 139893975131968 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0607 10:46:11.301109 139893975131968 submission_runner.py:332] Starting training loop.
I0607 10:46:11.873203 139893975131968 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:46:11.880748 139893975131968 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 10:46:12.030055 139893975131968 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:46:16.551638 139856030177024 logging_writer.py:48] [0] global_step=0, grad_norm=2.422786, loss=0.790057
I0607 10:46:16.563116 139893975131968 submission.py:120] 0) loss = 0.790, grad_norm = 2.423
I0607 10:46:16.568345 139893975131968 spec.py:298] Evaluating on the training split.
I0607 10:46:16.574550 139893975131968 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:46:16.578609 139893975131968 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 10:46:16.634241 139893975131968 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:47:14.799802 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 10:47:14.803148 139893975131968 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:47:14.808390 139893975131968 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 10:47:14.864468 139893975131968 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:48:00.952732 139893975131968 spec.py:326] Evaluating on the test split.
I0607 10:48:00.956146 139893975131968 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:48:00.960751 139893975131968 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 10:48:01.018533 139893975131968 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 10:48:47.991884 139893975131968 submission_runner.py:419] Time since start: 156.69s, 	Step: 1, 	{'train/accuracy': 0.48630294684236786, 'train/loss': 0.7890582516451058, 'train/mean_average_precision': 0.022155409653072215, 'validation/accuracy': 0.4963380089582993, 'validation/loss': 0.7852067293601481, 'validation/mean_average_precision': 0.02740020127348663, 'validation/num_examples': 43793, 'test/accuracy': 0.49977781980364744, 'test/loss': 0.7828834061508745, 'test/mean_average_precision': 0.028979926353912756, 'test/num_examples': 43793, 'score': 5.267739772796631, 'total_duration': 156.69123435020447, 'accumulated_submission_time': 5.267739772796631, 'accumulated_eval_time': 151.4232199192047, 'accumulated_logging_time': 0}
I0607 10:48:48.007609 139841760724736 logging_writer.py:48] [1] accumulated_eval_time=151.423220, accumulated_logging_time=0, accumulated_submission_time=5.267740, global_step=1, preemption_count=0, score=5.267740, test/accuracy=0.499778, test/loss=0.782883, test/mean_average_precision=0.028980, test/num_examples=43793, total_duration=156.691234, train/accuracy=0.486303, train/loss=0.789058, train/mean_average_precision=0.022155, validation/accuracy=0.496338, validation/loss=0.785207, validation/mean_average_precision=0.027400, validation/num_examples=43793
I0607 10:48:48.278438 139893975131968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.284037 139802379282240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.284506 140150134417216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.284505 140344816109376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.284519 140457614022464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.284528 139928469600064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.284851 140220877612864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.285053 140417923757888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 10:48:48.319276 139841769117440 logging_writer.py:48] [1] global_step=1, grad_norm=2.414586, loss=0.789761
I0607 10:48:48.323382 139893975131968 submission.py:120] 1) loss = 0.790, grad_norm = 2.415
I0607 10:48:48.624027 139841760724736 logging_writer.py:48] [2] global_step=2, grad_norm=2.407658, loss=0.787274
I0607 10:48:48.628235 139893975131968 submission.py:120] 2) loss = 0.787, grad_norm = 2.408
I0607 10:48:48.929307 139841769117440 logging_writer.py:48] [3] global_step=3, grad_norm=2.404052, loss=0.787856
I0607 10:48:48.933521 139893975131968 submission.py:120] 3) loss = 0.788, grad_norm = 2.404
I0607 10:48:49.235797 139841760724736 logging_writer.py:48] [4] global_step=4, grad_norm=2.413196, loss=0.784924
I0607 10:48:49.239752 139893975131968 submission.py:120] 4) loss = 0.785, grad_norm = 2.413
I0607 10:48:49.541810 139841769117440 logging_writer.py:48] [5] global_step=5, grad_norm=2.382594, loss=0.782475
I0607 10:48:49.545840 139893975131968 submission.py:120] 5) loss = 0.782, grad_norm = 2.383
I0607 10:48:49.853560 139841760724736 logging_writer.py:48] [6] global_step=6, grad_norm=2.343468, loss=0.777931
I0607 10:48:49.858541 139893975131968 submission.py:120] 6) loss = 0.778, grad_norm = 2.343
I0607 10:48:50.152610 139841769117440 logging_writer.py:48] [7] global_step=7, grad_norm=2.332385, loss=0.775521
I0607 10:48:50.156802 139893975131968 submission.py:120] 7) loss = 0.776, grad_norm = 2.332
I0607 10:48:50.453010 139841760724736 logging_writer.py:48] [8] global_step=8, grad_norm=2.320637, loss=0.770476
I0607 10:48:50.457675 139893975131968 submission.py:120] 8) loss = 0.770, grad_norm = 2.321
I0607 10:48:50.751340 139841769117440 logging_writer.py:48] [9] global_step=9, grad_norm=2.292643, loss=0.765452
I0607 10:48:50.756258 139893975131968 submission.py:120] 9) loss = 0.765, grad_norm = 2.293
I0607 10:48:51.075184 139841760724736 logging_writer.py:48] [10] global_step=10, grad_norm=2.241570, loss=0.758098
I0607 10:48:51.079590 139893975131968 submission.py:120] 10) loss = 0.758, grad_norm = 2.242
I0607 10:48:51.369955 139841769117440 logging_writer.py:48] [11] global_step=11, grad_norm=2.233711, loss=0.753618
I0607 10:48:51.373985 139893975131968 submission.py:120] 11) loss = 0.754, grad_norm = 2.234
I0607 10:48:51.666110 139841760724736 logging_writer.py:48] [12] global_step=12, grad_norm=2.198281, loss=0.746288
I0607 10:48:51.670454 139893975131968 submission.py:120] 12) loss = 0.746, grad_norm = 2.198
I0607 10:48:51.970519 139841769117440 logging_writer.py:48] [13] global_step=13, grad_norm=2.174108, loss=0.737616
I0607 10:48:51.974871 139893975131968 submission.py:120] 13) loss = 0.738, grad_norm = 2.174
I0607 10:48:52.275600 139841760724736 logging_writer.py:48] [14] global_step=14, grad_norm=2.096914, loss=0.731074
I0607 10:48:52.279709 139893975131968 submission.py:120] 14) loss = 0.731, grad_norm = 2.097
I0607 10:48:52.580738 139841769117440 logging_writer.py:48] [15] global_step=15, grad_norm=2.034543, loss=0.722358
I0607 10:48:52.584904 139893975131968 submission.py:120] 15) loss = 0.722, grad_norm = 2.035
I0607 10:48:52.881958 139841760724736 logging_writer.py:48] [16] global_step=16, grad_norm=1.966990, loss=0.714141
I0607 10:48:52.886353 139893975131968 submission.py:120] 16) loss = 0.714, grad_norm = 1.967
I0607 10:48:53.183361 139841769117440 logging_writer.py:48] [17] global_step=17, grad_norm=1.861780, loss=0.704969
I0607 10:48:53.187895 139893975131968 submission.py:120] 17) loss = 0.705, grad_norm = 1.862
I0607 10:48:53.698366 139841760724736 logging_writer.py:48] [18] global_step=18, grad_norm=1.763080, loss=0.698004
I0607 10:48:53.702747 139893975131968 submission.py:120] 18) loss = 0.698, grad_norm = 1.763
I0607 10:48:54.001656 139841769117440 logging_writer.py:48] [19] global_step=19, grad_norm=1.668727, loss=0.690045
I0607 10:48:54.005919 139893975131968 submission.py:120] 19) loss = 0.690, grad_norm = 1.669
I0607 10:48:54.300002 139841760724736 logging_writer.py:48] [20] global_step=20, grad_norm=1.587862, loss=0.683495
I0607 10:48:54.304117 139893975131968 submission.py:120] 20) loss = 0.683, grad_norm = 1.588
I0607 10:48:54.602151 139841769117440 logging_writer.py:48] [21] global_step=21, grad_norm=1.512947, loss=0.674590
I0607 10:48:54.606643 139893975131968 submission.py:120] 21) loss = 0.675, grad_norm = 1.513
I0607 10:48:54.904026 139841760724736 logging_writer.py:48] [22] global_step=22, grad_norm=1.471898, loss=0.667552
I0607 10:48:54.908882 139893975131968 submission.py:120] 22) loss = 0.668, grad_norm = 1.472
I0607 10:48:55.201845 139841769117440 logging_writer.py:48] [23] global_step=23, grad_norm=1.441608, loss=0.660828
I0607 10:48:55.206278 139893975131968 submission.py:120] 23) loss = 0.661, grad_norm = 1.442
I0607 10:48:55.504461 139841760724736 logging_writer.py:48] [24] global_step=24, grad_norm=1.392970, loss=0.654630
I0607 10:48:55.508917 139893975131968 submission.py:120] 24) loss = 0.655, grad_norm = 1.393
I0607 10:48:55.816390 139841769117440 logging_writer.py:48] [25] global_step=25, grad_norm=1.327728, loss=0.645837
I0607 10:48:55.821164 139893975131968 submission.py:120] 25) loss = 0.646, grad_norm = 1.328
I0607 10:48:56.119241 139841760724736 logging_writer.py:48] [26] global_step=26, grad_norm=1.319449, loss=0.640897
I0607 10:48:56.123138 139893975131968 submission.py:120] 26) loss = 0.641, grad_norm = 1.319
I0607 10:48:56.424176 139841769117440 logging_writer.py:48] [27] global_step=27, grad_norm=1.309405, loss=0.633939
I0607 10:48:56.428063 139893975131968 submission.py:120] 27) loss = 0.634, grad_norm = 1.309
I0607 10:48:56.727185 139841760724736 logging_writer.py:48] [28] global_step=28, grad_norm=1.264062, loss=0.626501
I0607 10:48:56.731156 139893975131968 submission.py:120] 28) loss = 0.627, grad_norm = 1.264
I0607 10:48:57.034124 139841769117440 logging_writer.py:48] [29] global_step=29, grad_norm=1.255781, loss=0.618694
I0607 10:48:57.038565 139893975131968 submission.py:120] 29) loss = 0.619, grad_norm = 1.256
I0607 10:48:57.342436 139841760724736 logging_writer.py:48] [30] global_step=30, grad_norm=1.230928, loss=0.612571
I0607 10:48:57.346574 139893975131968 submission.py:120] 30) loss = 0.613, grad_norm = 1.231
I0607 10:48:57.651542 139841769117440 logging_writer.py:48] [31] global_step=31, grad_norm=1.194849, loss=0.605073
I0607 10:48:57.655962 139893975131968 submission.py:120] 31) loss = 0.605, grad_norm = 1.195
I0607 10:48:57.959945 139841760724736 logging_writer.py:48] [32] global_step=32, grad_norm=1.145273, loss=0.598855
I0607 10:48:57.964457 139893975131968 submission.py:120] 32) loss = 0.599, grad_norm = 1.145
I0607 10:48:58.263703 139841769117440 logging_writer.py:48] [33] global_step=33, grad_norm=1.096498, loss=0.589623
I0607 10:48:58.267583 139893975131968 submission.py:120] 33) loss = 0.590, grad_norm = 1.096
I0607 10:48:58.562830 139841760724736 logging_writer.py:48] [34] global_step=34, grad_norm=1.052008, loss=0.583409
I0607 10:48:58.567131 139893975131968 submission.py:120] 34) loss = 0.583, grad_norm = 1.052
I0607 10:48:58.857164 139841769117440 logging_writer.py:48] [35] global_step=35, grad_norm=1.023587, loss=0.576382
I0607 10:48:58.861230 139893975131968 submission.py:120] 35) loss = 0.576, grad_norm = 1.024
I0607 10:48:59.153091 139841760724736 logging_writer.py:48] [36] global_step=36, grad_norm=0.998704, loss=0.569606
I0607 10:48:59.157012 139893975131968 submission.py:120] 36) loss = 0.570, grad_norm = 0.999
I0607 10:48:59.451103 139841769117440 logging_writer.py:48] [37] global_step=37, grad_norm=0.966741, loss=0.564107
I0607 10:48:59.455123 139893975131968 submission.py:120] 37) loss = 0.564, grad_norm = 0.967
I0607 10:48:59.754718 139841760724736 logging_writer.py:48] [38] global_step=38, grad_norm=0.929841, loss=0.557096
I0607 10:48:59.758737 139893975131968 submission.py:120] 38) loss = 0.557, grad_norm = 0.930
I0607 10:49:00.061173 139841769117440 logging_writer.py:48] [39] global_step=39, grad_norm=0.934932, loss=0.550021
I0607 10:49:00.065595 139893975131968 submission.py:120] 39) loss = 0.550, grad_norm = 0.935
I0607 10:49:00.367697 139841760724736 logging_writer.py:48] [40] global_step=40, grad_norm=0.891652, loss=0.544602
I0607 10:49:00.371963 139893975131968 submission.py:120] 40) loss = 0.545, grad_norm = 0.892
I0607 10:49:00.673598 139841769117440 logging_writer.py:48] [41] global_step=41, grad_norm=0.865334, loss=0.537489
I0607 10:49:00.677924 139893975131968 submission.py:120] 41) loss = 0.537, grad_norm = 0.865
I0607 10:49:00.977255 139841760724736 logging_writer.py:48] [42] global_step=42, grad_norm=0.838262, loss=0.532213
I0607 10:49:00.981498 139893975131968 submission.py:120] 42) loss = 0.532, grad_norm = 0.838
I0607 10:49:01.310945 139841769117440 logging_writer.py:48] [43] global_step=43, grad_norm=0.820790, loss=0.525435
I0607 10:49:01.315584 139893975131968 submission.py:120] 43) loss = 0.525, grad_norm = 0.821
I0607 10:49:01.699296 139841760724736 logging_writer.py:48] [44] global_step=44, grad_norm=0.820185, loss=0.518698
I0607 10:49:01.703344 139893975131968 submission.py:120] 44) loss = 0.519, grad_norm = 0.820
I0607 10:49:02.013252 139841769117440 logging_writer.py:48] [45] global_step=45, grad_norm=0.773989, loss=0.512997
I0607 10:49:02.017127 139893975131968 submission.py:120] 45) loss = 0.513, grad_norm = 0.774
I0607 10:49:02.311639 139841760724736 logging_writer.py:48] [46] global_step=46, grad_norm=0.756899, loss=0.508842
I0607 10:49:02.315543 139893975131968 submission.py:120] 46) loss = 0.509, grad_norm = 0.757
I0607 10:49:02.612215 139841769117440 logging_writer.py:48] [47] global_step=47, grad_norm=0.752000, loss=0.503361
I0607 10:49:02.616037 139893975131968 submission.py:120] 47) loss = 0.503, grad_norm = 0.752
I0607 10:49:02.912425 139841760724736 logging_writer.py:48] [48] global_step=48, grad_norm=0.727687, loss=0.498150
I0607 10:49:02.916422 139893975131968 submission.py:120] 48) loss = 0.498, grad_norm = 0.728
I0607 10:49:03.206757 139841769117440 logging_writer.py:48] [49] global_step=49, grad_norm=0.694994, loss=0.493356
I0607 10:49:03.211335 139893975131968 submission.py:120] 49) loss = 0.493, grad_norm = 0.695
I0607 10:49:03.508136 139841760724736 logging_writer.py:48] [50] global_step=50, grad_norm=0.701783, loss=0.487282
I0607 10:49:03.512908 139893975131968 submission.py:120] 50) loss = 0.487, grad_norm = 0.702
I0607 10:49:03.814093 139841769117440 logging_writer.py:48] [51] global_step=51, grad_norm=0.697896, loss=0.482347
I0607 10:49:03.818533 139893975131968 submission.py:120] 51) loss = 0.482, grad_norm = 0.698
I0607 10:49:04.118094 139841760724736 logging_writer.py:48] [52] global_step=52, grad_norm=0.681878, loss=0.476953
I0607 10:49:04.122169 139893975131968 submission.py:120] 52) loss = 0.477, grad_norm = 0.682
I0607 10:49:04.420338 139841769117440 logging_writer.py:48] [53] global_step=53, grad_norm=0.657757, loss=0.474626
I0607 10:49:04.424466 139893975131968 submission.py:120] 53) loss = 0.475, grad_norm = 0.658
I0607 10:49:04.722336 139841760724736 logging_writer.py:48] [54] global_step=54, grad_norm=0.645797, loss=0.468885
I0607 10:49:04.726390 139893975131968 submission.py:120] 54) loss = 0.469, grad_norm = 0.646
I0607 10:49:05.024378 139841769117440 logging_writer.py:48] [55] global_step=55, grad_norm=0.648103, loss=0.465552
I0607 10:49:05.028094 139893975131968 submission.py:120] 55) loss = 0.466, grad_norm = 0.648
I0607 10:49:05.326576 139841760724736 logging_writer.py:48] [56] global_step=56, grad_norm=0.628269, loss=0.459413
I0607 10:49:05.331267 139893975131968 submission.py:120] 56) loss = 0.459, grad_norm = 0.628
I0607 10:49:05.632587 139841769117440 logging_writer.py:48] [57] global_step=57, grad_norm=0.604176, loss=0.455185
I0607 10:49:05.636943 139893975131968 submission.py:120] 57) loss = 0.455, grad_norm = 0.604
I0607 10:49:05.939006 139841760724736 logging_writer.py:48] [58] global_step=58, grad_norm=0.585816, loss=0.450455
I0607 10:49:05.943342 139893975131968 submission.py:120] 58) loss = 0.450, grad_norm = 0.586
I0607 10:49:06.241243 139841769117440 logging_writer.py:48] [59] global_step=59, grad_norm=0.577994, loss=0.446078
I0607 10:49:06.245338 139893975131968 submission.py:120] 59) loss = 0.446, grad_norm = 0.578
I0607 10:49:06.542042 139841760724736 logging_writer.py:48] [60] global_step=60, grad_norm=0.570960, loss=0.441387
I0607 10:49:06.546080 139893975131968 submission.py:120] 60) loss = 0.441, grad_norm = 0.571
I0607 10:49:06.852927 139841769117440 logging_writer.py:48] [61] global_step=61, grad_norm=0.559897, loss=0.439638
I0607 10:49:06.857064 139893975131968 submission.py:120] 61) loss = 0.440, grad_norm = 0.560
I0607 10:49:07.162542 139841760724736 logging_writer.py:48] [62] global_step=62, grad_norm=0.536849, loss=0.434184
I0607 10:49:07.167038 139893975131968 submission.py:120] 62) loss = 0.434, grad_norm = 0.537
I0607 10:49:07.463661 139841769117440 logging_writer.py:48] [63] global_step=63, grad_norm=0.525072, loss=0.429878
I0607 10:49:07.467950 139893975131968 submission.py:120] 63) loss = 0.430, grad_norm = 0.525
I0607 10:49:07.771613 139841760724736 logging_writer.py:48] [64] global_step=64, grad_norm=0.516421, loss=0.427850
I0607 10:49:07.776178 139893975131968 submission.py:120] 64) loss = 0.428, grad_norm = 0.516
I0607 10:49:08.086187 139841769117440 logging_writer.py:48] [65] global_step=65, grad_norm=0.504149, loss=0.423394
I0607 10:49:08.090394 139893975131968 submission.py:120] 65) loss = 0.423, grad_norm = 0.504
I0607 10:49:08.411745 139841760724736 logging_writer.py:48] [66] global_step=66, grad_norm=0.501232, loss=0.420571
I0607 10:49:08.416023 139893975131968 submission.py:120] 66) loss = 0.421, grad_norm = 0.501
I0607 10:49:08.734975 139841769117440 logging_writer.py:48] [67] global_step=67, grad_norm=0.493264, loss=0.416406
I0607 10:49:08.739415 139893975131968 submission.py:120] 67) loss = 0.416, grad_norm = 0.493
I0607 10:49:09.048082 139841760724736 logging_writer.py:48] [68] global_step=68, grad_norm=0.489543, loss=0.413915
I0607 10:49:09.052906 139893975131968 submission.py:120] 68) loss = 0.414, grad_norm = 0.490
I0607 10:49:09.360396 139841769117440 logging_writer.py:48] [69] global_step=69, grad_norm=0.478183, loss=0.413349
I0607 10:49:09.364609 139893975131968 submission.py:120] 69) loss = 0.413, grad_norm = 0.478
I0607 10:49:09.667814 139841760724736 logging_writer.py:48] [70] global_step=70, grad_norm=0.473237, loss=0.409365
I0607 10:49:09.672137 139893975131968 submission.py:120] 70) loss = 0.409, grad_norm = 0.473
I0607 10:49:09.972108 139841769117440 logging_writer.py:48] [71] global_step=71, grad_norm=0.480473, loss=0.406494
I0607 10:49:09.976118 139893975131968 submission.py:120] 71) loss = 0.406, grad_norm = 0.480
I0607 10:49:10.272881 139841760724736 logging_writer.py:48] [72] global_step=72, grad_norm=0.468622, loss=0.405329
I0607 10:49:10.276915 139893975131968 submission.py:120] 72) loss = 0.405, grad_norm = 0.469
I0607 10:49:10.575772 139841769117440 logging_writer.py:48] [73] global_step=73, grad_norm=0.465084, loss=0.401446
I0607 10:49:10.579902 139893975131968 submission.py:120] 73) loss = 0.401, grad_norm = 0.465
I0607 10:49:10.881150 139841760724736 logging_writer.py:48] [74] global_step=74, grad_norm=0.460223, loss=0.398948
I0607 10:49:10.885316 139893975131968 submission.py:120] 74) loss = 0.399, grad_norm = 0.460
I0607 10:49:11.184752 139841769117440 logging_writer.py:48] [75] global_step=75, grad_norm=0.458695, loss=0.395417
I0607 10:49:11.188389 139893975131968 submission.py:120] 75) loss = 0.395, grad_norm = 0.459
I0607 10:49:11.486778 139841760724736 logging_writer.py:48] [76] global_step=76, grad_norm=0.453970, loss=0.397040
I0607 10:49:11.491262 139893975131968 submission.py:120] 76) loss = 0.397, grad_norm = 0.454
I0607 10:49:11.793378 139841769117440 logging_writer.py:48] [77] global_step=77, grad_norm=0.449925, loss=0.394706
I0607 10:49:11.797633 139893975131968 submission.py:120] 77) loss = 0.395, grad_norm = 0.450
I0607 10:49:12.096515 139841760724736 logging_writer.py:48] [78] global_step=78, grad_norm=0.447024, loss=0.392661
I0607 10:49:12.100850 139893975131968 submission.py:120] 78) loss = 0.393, grad_norm = 0.447
I0607 10:49:12.399161 139841769117440 logging_writer.py:48] [79] global_step=79, grad_norm=0.456176, loss=0.387512
I0607 10:49:12.403407 139893975131968 submission.py:120] 79) loss = 0.388, grad_norm = 0.456
I0607 10:49:12.699032 139841760724736 logging_writer.py:48] [80] global_step=80, grad_norm=0.441072, loss=0.387700
I0607 10:49:12.703240 139893975131968 submission.py:120] 80) loss = 0.388, grad_norm = 0.441
I0607 10:49:13.028719 139841769117440 logging_writer.py:48] [81] global_step=81, grad_norm=0.440277, loss=0.384587
I0607 10:49:13.032827 139893975131968 submission.py:120] 81) loss = 0.385, grad_norm = 0.440
I0607 10:49:13.333276 139841760724736 logging_writer.py:48] [82] global_step=82, grad_norm=0.439222, loss=0.384371
I0607 10:49:13.337403 139893975131968 submission.py:120] 82) loss = 0.384, grad_norm = 0.439
I0607 10:49:13.642484 139841769117440 logging_writer.py:48] [83] global_step=83, grad_norm=0.439907, loss=0.381885
I0607 10:49:13.646897 139893975131968 submission.py:120] 83) loss = 0.382, grad_norm = 0.440
I0607 10:49:13.952075 139841760724736 logging_writer.py:48] [84] global_step=84, grad_norm=0.432610, loss=0.382070
I0607 10:49:13.956215 139893975131968 submission.py:120] 84) loss = 0.382, grad_norm = 0.433
I0607 10:49:14.256472 139841769117440 logging_writer.py:48] [85] global_step=85, grad_norm=0.434041, loss=0.380655
I0607 10:49:14.260556 139893975131968 submission.py:120] 85) loss = 0.381, grad_norm = 0.434
I0607 10:49:14.556715 139841760724736 logging_writer.py:48] [86] global_step=86, grad_norm=0.432579, loss=0.377237
I0607 10:49:14.560886 139893975131968 submission.py:120] 86) loss = 0.377, grad_norm = 0.433
I0607 10:49:14.860342 139841769117440 logging_writer.py:48] [87] global_step=87, grad_norm=0.431764, loss=0.376906
I0607 10:49:14.864272 139893975131968 submission.py:120] 87) loss = 0.377, grad_norm = 0.432
I0607 10:49:15.163684 139841760724736 logging_writer.py:48] [88] global_step=88, grad_norm=0.430652, loss=0.374972
I0607 10:49:15.167658 139893975131968 submission.py:120] 88) loss = 0.375, grad_norm = 0.431
I0607 10:49:15.463557 139841769117440 logging_writer.py:48] [89] global_step=89, grad_norm=0.429081, loss=0.374159
I0607 10:49:15.467538 139893975131968 submission.py:120] 89) loss = 0.374, grad_norm = 0.429
I0607 10:49:15.764159 139841760724736 logging_writer.py:48] [90] global_step=90, grad_norm=0.432384, loss=0.371808
I0607 10:49:15.768138 139893975131968 submission.py:120] 90) loss = 0.372, grad_norm = 0.432
I0607 10:49:16.069443 139841769117440 logging_writer.py:48] [91] global_step=91, grad_norm=0.421948, loss=0.369703
I0607 10:49:16.073399 139893975131968 submission.py:120] 91) loss = 0.370, grad_norm = 0.422
I0607 10:49:16.373739 139841760724736 logging_writer.py:48] [92] global_step=92, grad_norm=0.421265, loss=0.368945
I0607 10:49:16.378193 139893975131968 submission.py:120] 92) loss = 0.369, grad_norm = 0.421
I0607 10:49:16.679620 139841769117440 logging_writer.py:48] [93] global_step=93, grad_norm=0.426506, loss=0.365719
I0607 10:49:16.684373 139893975131968 submission.py:120] 93) loss = 0.366, grad_norm = 0.427
I0607 10:49:16.981422 139841760724736 logging_writer.py:48] [94] global_step=94, grad_norm=0.423660, loss=0.364057
I0607 10:49:16.985579 139893975131968 submission.py:120] 94) loss = 0.364, grad_norm = 0.424
I0607 10:49:17.288319 139841769117440 logging_writer.py:48] [95] global_step=95, grad_norm=0.424824, loss=0.363630
I0607 10:49:17.292544 139893975131968 submission.py:120] 95) loss = 0.364, grad_norm = 0.425
I0607 10:49:17.593384 139841760724736 logging_writer.py:48] [96] global_step=96, grad_norm=0.422190, loss=0.361457
I0607 10:49:17.597647 139893975131968 submission.py:120] 96) loss = 0.361, grad_norm = 0.422
I0607 10:49:17.902299 139841769117440 logging_writer.py:48] [97] global_step=97, grad_norm=0.417969, loss=0.361115
I0607 10:49:17.906739 139893975131968 submission.py:120] 97) loss = 0.361, grad_norm = 0.418
I0607 10:49:18.208791 139841760724736 logging_writer.py:48] [98] global_step=98, grad_norm=0.415713, loss=0.357295
I0607 10:49:18.213121 139893975131968 submission.py:120] 98) loss = 0.357, grad_norm = 0.416
I0607 10:49:18.512815 139841769117440 logging_writer.py:48] [99] global_step=99, grad_norm=0.416487, loss=0.357221
I0607 10:49:18.517171 139893975131968 submission.py:120] 99) loss = 0.357, grad_norm = 0.416
I0607 10:49:18.818201 139841760724736 logging_writer.py:48] [100] global_step=100, grad_norm=0.413971, loss=0.356121
I0607 10:49:18.822300 139893975131968 submission.py:120] 100) loss = 0.356, grad_norm = 0.414
I0607 10:51:17.425739 139841769117440 logging_writer.py:48] [500] global_step=500, grad_norm=0.046774, loss=0.070895
I0607 10:51:17.430991 139893975131968 submission.py:120] 500) loss = 0.071, grad_norm = 0.047
I0607 10:52:48.266853 139893975131968 spec.py:298] Evaluating on the training split.
I0607 10:53:46.727403 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 10:53:49.954157 139893975131968 spec.py:326] Evaluating on the test split.
I0607 10:53:53.127806 139893975131968 submission_runner.py:419] Time since start: 461.83s, 	Step: 806, 	{'train/accuracy': 0.9867192734897874, 'train/loss': 0.05308328094329657, 'train/mean_average_precision': 0.04951251638365205, 'validation/accuracy': 0.984169530578295, 'validation/loss': 0.06228685576196287, 'validation/mean_average_precision': 0.04807080751566898, 'validation/num_examples': 43793, 'test/accuracy': 0.9831804326427566, 'test/loss': 0.06555142386969247, 'test/mean_average_precision': 0.05074568539607096, 'test/num_examples': 43793, 'score': 245.30491065979004, 'total_duration': 461.8272271156311, 'accumulated_submission_time': 245.30491065979004, 'accumulated_eval_time': 216.2839274406433, 'accumulated_logging_time': 0.025660991668701172}
I0607 10:53:53.137915 139841760724736 logging_writer.py:48] [806] accumulated_eval_time=216.283927, accumulated_logging_time=0.025661, accumulated_submission_time=245.304911, global_step=806, preemption_count=0, score=245.304911, test/accuracy=0.983180, test/loss=0.065551, test/mean_average_precision=0.050746, test/num_examples=43793, total_duration=461.827227, train/accuracy=0.986719, train/loss=0.053083, train/mean_average_precision=0.049513, validation/accuracy=0.984170, validation/loss=0.062287, validation/mean_average_precision=0.048071, validation/num_examples=43793
I0607 10:54:51.318959 139841769117440 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.117859, loss=0.052975
I0607 10:54:51.323551 139893975131968 submission.py:120] 1000) loss = 0.053, grad_norm = 0.118
I0607 10:57:19.178637 139841760724736 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.011598, loss=0.046916
I0607 10:57:19.184400 139893975131968 submission.py:120] 1500) loss = 0.047, grad_norm = 0.012
I0607 10:57:53.185046 139893975131968 spec.py:298] Evaluating on the training split.
I0607 10:58:52.011177 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 10:58:55.248436 139893975131968 spec.py:326] Evaluating on the test split.
I0607 10:58:58.478681 139893975131968 submission_runner.py:419] Time since start: 767.18s, 	Step: 1616, 	{'train/accuracy': 0.9870751470096564, 'train/loss': 0.04848041981336165, 'train/mean_average_precision': 0.08313269357825044, 'validation/accuracy': 0.9844216197521002, 'validation/loss': 0.05778544912669978, 'validation/mean_average_precision': 0.08719600315716167, 'validation/num_examples': 43793, 'test/accuracy': 0.9834323070644037, 'test/loss': 0.061161917135000056, 'test/mean_average_precision': 0.08401649356245705, 'test/num_examples': 43793, 'score': 485.1325948238373, 'total_duration': 767.1780886650085, 'accumulated_submission_time': 485.1325948238373, 'accumulated_eval_time': 281.57729744911194, 'accumulated_logging_time': 0.046861886978149414}
I0607 10:58:58.488746 139841769117440 logging_writer.py:48] [1616] accumulated_eval_time=281.577297, accumulated_logging_time=0.046862, accumulated_submission_time=485.132595, global_step=1616, preemption_count=0, score=485.132595, test/accuracy=0.983432, test/loss=0.061162, test/mean_average_precision=0.084016, test/num_examples=43793, total_duration=767.178089, train/accuracy=0.987075, train/loss=0.048480, train/mean_average_precision=0.083133, validation/accuracy=0.984422, validation/loss=0.057785, validation/mean_average_precision=0.087196, validation/num_examples=43793
I0607 11:00:53.045071 139841760724736 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.026313, loss=0.049237
I0607 11:00:53.050810 139893975131968 submission.py:120] 2000) loss = 0.049, grad_norm = 0.026
I0607 11:02:58.547898 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:03:57.781031 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:04:01.054032 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:04:04.317624 139893975131968 submission_runner.py:419] Time since start: 1073.02s, 	Step: 2421, 	{'train/accuracy': 0.9876401803737933, 'train/loss': 0.045168281874094496, 'train/mean_average_precision': 0.11594505132735314, 'validation/accuracy': 0.9848141644076067, 'validation/loss': 0.05459519923366515, 'validation/mean_average_precision': 0.11683352920239723, 'validation/num_examples': 43793, 'test/accuracy': 0.9838139094490395, 'test/loss': 0.05781813808572912, 'test/mean_average_precision': 0.1141335402328601, 'test/num_examples': 43793, 'score': 724.9708936214447, 'total_duration': 1073.0170452594757, 'accumulated_submission_time': 724.9708936214447, 'accumulated_eval_time': 347.34680128097534, 'accumulated_logging_time': 0.06747651100158691}
I0607 11:04:04.327791 139841769117440 logging_writer.py:48] [2421] accumulated_eval_time=347.346801, accumulated_logging_time=0.067477, accumulated_submission_time=724.970894, global_step=2421, preemption_count=0, score=724.970894, test/accuracy=0.983814, test/loss=0.057818, test/mean_average_precision=0.114134, test/num_examples=43793, total_duration=1073.017045, train/accuracy=0.987640, train/loss=0.045168, train/mean_average_precision=0.115945, validation/accuracy=0.984814, validation/loss=0.054595, validation/mean_average_precision=0.116834, validation/num_examples=43793
I0607 11:04:28.025907 139841760724736 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.016439, loss=0.045490
I0607 11:04:28.030956 139893975131968 submission.py:120] 2500) loss = 0.045, grad_norm = 0.016
I0607 11:06:56.313585 139841769117440 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.012349, loss=0.047328
I0607 11:06:56.319630 139893975131968 submission.py:120] 3000) loss = 0.047, grad_norm = 0.012
I0607 11:08:04.508086 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:09:05.139532 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:09:08.363578 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:09:11.537604 139893975131968 submission_runner.py:419] Time since start: 1380.24s, 	Step: 3230, 	{'train/accuracy': 0.9877289185267306, 'train/loss': 0.043963524474397886, 'train/mean_average_precision': 0.15114338271175232, 'validation/accuracy': 0.984984253560303, 'validation/loss': 0.05278647970864824, 'validation/mean_average_precision': 0.13599124740379717, 'validation/num_examples': 43793, 'test/accuracy': 0.983999235110452, 'test/loss': 0.05566665589110264, 'test/mean_average_precision': 0.13530707026025351, 'test/num_examples': 43793, 'score': 964.9294726848602, 'total_duration': 1380.2370221614838, 'accumulated_submission_time': 964.9294726848602, 'accumulated_eval_time': 414.37606620788574, 'accumulated_logging_time': 0.08963966369628906}
I0607 11:09:11.552701 139841760724736 logging_writer.py:48] [3230] accumulated_eval_time=414.376066, accumulated_logging_time=0.089640, accumulated_submission_time=964.929473, global_step=3230, preemption_count=0, score=964.929473, test/accuracy=0.983999, test/loss=0.055667, test/mean_average_precision=0.135307, test/num_examples=43793, total_duration=1380.237022, train/accuracy=0.987729, train/loss=0.043964, train/mean_average_precision=0.151143, validation/accuracy=0.984984, validation/loss=0.052786, validation/mean_average_precision=0.135991, validation/num_examples=43793
I0607 11:10:32.132115 139841769117440 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011565, loss=0.047681
I0607 11:10:32.140884 139893975131968 submission.py:120] 3500) loss = 0.048, grad_norm = 0.012
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0607 11:13:00.194896 139841760724736 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.016972, loss=0.043000
I0607 11:13:00.200786 139893975131968 submission.py:120] 4000) loss = 0.043, grad_norm = 0.017
I0607 11:13:11.569002 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:14:11.212206 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:14:14.423678 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:14:17.568693 139893975131968 submission_runner.py:419] Time since start: 1686.27s, 	Step: 4038, 	{'train/accuracy': 0.9880568255097664, 'train/loss': 0.042323980437690235, 'train/mean_average_precision': 0.17078083092168295, 'validation/accuracy': 0.9851856001467881, 'validation/loss': 0.051598050256676305, 'validation/mean_average_precision': 0.1541669269160101, 'validation/num_examples': 43793, 'test/accuracy': 0.9842418432490284, 'test/loss': 0.0543140154848014, 'test/mean_average_precision': 0.15405868377087178, 'test/num_examples': 43793, 'score': 1204.7252173423767, 'total_duration': 1686.2681159973145, 'accumulated_submission_time': 1204.7252173423767, 'accumulated_eval_time': 480.3755261898041, 'accumulated_logging_time': 0.11551594734191895}
I0607 11:14:17.578935 139841769117440 logging_writer.py:48] [4038] accumulated_eval_time=480.375526, accumulated_logging_time=0.115516, accumulated_submission_time=1204.725217, global_step=4038, preemption_count=0, score=1204.725217, test/accuracy=0.984242, test/loss=0.054314, test/mean_average_precision=0.154059, test/num_examples=43793, total_duration=1686.268116, train/accuracy=0.988057, train/loss=0.042324, train/mean_average_precision=0.170781, validation/accuracy=0.985186, validation/loss=0.051598, validation/mean_average_precision=0.154167, validation/num_examples=43793
I0607 11:16:34.663257 139841760724736 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013990, loss=0.044910
I0607 11:16:34.668386 139893975131968 submission.py:120] 4500) loss = 0.045, grad_norm = 0.014
I0607 11:18:17.668159 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:19:18.536774 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:19:21.749100 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:19:24.928861 139893975131968 submission_runner.py:419] Time since start: 1993.63s, 	Step: 4847, 	{'train/accuracy': 0.9883467293371991, 'train/loss': 0.04064191207265899, 'train/mean_average_precision': 0.1928252778195712, 'validation/accuracy': 0.9854864022044204, 'validation/loss': 0.05013781813572546, 'validation/mean_average_precision': 0.17350387091107397, 'validation/num_examples': 43793, 'test/accuracy': 0.9845417338647687, 'test/loss': 0.05280153827669879, 'test/mean_average_precision': 0.174168320777957, 'test/num_examples': 43793, 'score': 1444.5916619300842, 'total_duration': 1993.6283056735992, 'accumulated_submission_time': 1444.5916619300842, 'accumulated_eval_time': 547.6360187530518, 'accumulated_logging_time': 0.13864636421203613}
I0607 11:19:24.939421 139841769117440 logging_writer.py:48] [4847] accumulated_eval_time=547.636019, accumulated_logging_time=0.138646, accumulated_submission_time=1444.591662, global_step=4847, preemption_count=0, score=1444.591662, test/accuracy=0.984542, test/loss=0.052802, test/mean_average_precision=0.174168, test/num_examples=43793, total_duration=1993.628306, train/accuracy=0.988347, train/loss=0.040642, train/mean_average_precision=0.192825, validation/accuracy=0.985486, validation/loss=0.050138, validation/mean_average_precision=0.173504, validation/num_examples=43793
I0607 11:20:10.744316 139841760724736 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.013221, loss=0.044347
I0607 11:20:10.749383 139893975131968 submission.py:120] 5000) loss = 0.044, grad_norm = 0.013
I0607 11:22:40.105783 139841769117440 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.010602, loss=0.038818
I0607 11:22:40.111236 139893975131968 submission.py:120] 5500) loss = 0.039, grad_norm = 0.011
I0607 11:23:25.118048 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:24:25.768548 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:24:28.987936 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:24:32.157448 139893975131968 submission_runner.py:419] Time since start: 2300.86s, 	Step: 5651, 	{'train/accuracy': 0.9885155806619214, 'train/loss': 0.03938126268065978, 'train/mean_average_precision': 0.23110692952281192, 'validation/accuracy': 0.9856455309582555, 'validation/loss': 0.04852048062160887, 'validation/mean_average_precision': 0.19245739984321628, 'validation/num_examples': 43793, 'test/accuracy': 0.9847653882425188, 'test/loss': 0.051187480756667826, 'test/mean_average_precision': 0.19464023304905445, 'test/num_examples': 43793, 'score': 1684.547529220581, 'total_duration': 2300.856873512268, 'accumulated_submission_time': 1684.547529220581, 'accumulated_eval_time': 614.6752550601959, 'accumulated_logging_time': 0.16307902336120605}
I0607 11:24:32.167800 139841760724736 logging_writer.py:48] [5651] accumulated_eval_time=614.675255, accumulated_logging_time=0.163079, accumulated_submission_time=1684.547529, global_step=5651, preemption_count=0, score=1684.547529, test/accuracy=0.984765, test/loss=0.051187, test/mean_average_precision=0.194640, test/num_examples=43793, total_duration=2300.856874, train/accuracy=0.988516, train/loss=0.039381, train/mean_average_precision=0.231107, validation/accuracy=0.985646, validation/loss=0.048520, validation/mean_average_precision=0.192457, validation/num_examples=43793
I0607 11:26:16.857438 139841769117440 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.009671, loss=0.038572
I0607 11:26:16.863051 139893975131968 submission.py:120] 6000) loss = 0.039, grad_norm = 0.010
I0607 11:28:32.288486 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:29:32.158241 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:29:35.407046 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:29:38.579114 139893975131968 submission_runner.py:419] Time since start: 2607.28s, 	Step: 6453, 	{'train/accuracy': 0.989167771097521, 'train/loss': 0.03720977698810369, 'train/mean_average_precision': 0.2653584975850052, 'validation/accuracy': 0.9859601349996387, 'validation/loss': 0.047248830155020634, 'validation/mean_average_precision': 0.20958068201370725, 'validation/num_examples': 43793, 'test/accuracy': 0.985181949786012, 'test/loss': 0.049834906162036126, 'test/mean_average_precision': 0.2098387459043158, 'test/num_examples': 43793, 'score': 1924.4476079940796, 'total_duration': 2607.278535604477, 'accumulated_submission_time': 1924.4476079940796, 'accumulated_eval_time': 680.9656548500061, 'accumulated_logging_time': 0.18550801277160645}
I0607 11:29:38.589822 139841760724736 logging_writer.py:48] [6453] accumulated_eval_time=680.965655, accumulated_logging_time=0.185508, accumulated_submission_time=1924.447608, global_step=6453, preemption_count=0, score=1924.447608, test/accuracy=0.985182, test/loss=0.049835, test/mean_average_precision=0.209839, test/num_examples=43793, total_duration=2607.278536, train/accuracy=0.989168, train/loss=0.037210, train/mean_average_precision=0.265358, validation/accuracy=0.985960, validation/loss=0.047249, validation/mean_average_precision=0.209581, validation/num_examples=43793
I0607 11:29:52.619026 139841769117440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.011324, loss=0.038394
I0607 11:29:52.625008 139893975131968 submission.py:120] 6500) loss = 0.038, grad_norm = 0.011
I0607 11:32:19.594125 139841760724736 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.010839, loss=0.042661
I0607 11:32:19.599642 139893975131968 submission.py:120] 7000) loss = 0.043, grad_norm = 0.011
I0607 11:33:38.807239 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:34:40.088432 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:34:43.285018 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:34:46.458561 139893975131968 submission_runner.py:419] Time since start: 2915.16s, 	Step: 7271, 	{'train/accuracy': 0.98896432149081, 'train/loss': 0.0374562425451536, 'train/mean_average_precision': 0.2705501007608124, 'validation/accuracy': 0.985903709242539, 'validation/loss': 0.04751141035875415, 'validation/mean_average_precision': 0.22001600765047283, 'validation/num_examples': 43793, 'test/accuracy': 0.9850400071771575, 'test/loss': 0.05020351403252213, 'test/mean_average_precision': 0.21986439394602794, 'test/num_examples': 43793, 'score': 2164.441212415695, 'total_duration': 2915.1579587459564, 'accumulated_submission_time': 2164.441212415695, 'accumulated_eval_time': 748.6167373657227, 'accumulated_logging_time': 0.2083594799041748}
I0607 11:34:46.469150 139841769117440 logging_writer.py:48] [7271] accumulated_eval_time=748.616737, accumulated_logging_time=0.208359, accumulated_submission_time=2164.441212, global_step=7271, preemption_count=0, score=2164.441212, test/accuracy=0.985040, test/loss=0.050204, test/mean_average_precision=0.219864, test/num_examples=43793, total_duration=2915.157959, train/accuracy=0.988964, train/loss=0.037456, train/mean_average_precision=0.270550, validation/accuracy=0.985904, validation/loss=0.047511, validation/mean_average_precision=0.220016, validation/num_examples=43793
I0607 11:35:56.869456 139841760724736 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.011442, loss=0.041110
I0607 11:35:56.877840 139893975131968 submission.py:120] 7500) loss = 0.041, grad_norm = 0.011
I0607 11:38:25.701041 139841769117440 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.012663, loss=0.034994
I0607 11:38:25.707273 139893975131968 submission.py:120] 8000) loss = 0.035, grad_norm = 0.013
I0607 11:38:46.476941 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:39:47.520555 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:39:50.791571 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:39:54.019395 139893975131968 submission_runner.py:419] Time since start: 3222.72s, 	Step: 8071, 	{'train/accuracy': 0.9895591859408438, 'train/loss': 0.035413785073734376, 'train/mean_average_precision': 0.30105829338508816, 'validation/accuracy': 0.9862621548793666, 'validation/loss': 0.04635086078710278, 'validation/mean_average_precision': 0.22602280541673067, 'validation/num_examples': 43793, 'test/accuracy': 0.9853946531019514, 'test/loss': 0.049214265921896184, 'test/mean_average_precision': 0.2289259317680632, 'test/num_examples': 43793, 'score': 2404.2303414344788, 'total_duration': 3222.7188119888306, 'accumulated_submission_time': 2404.2303414344788, 'accumulated_eval_time': 816.1589467525482, 'accumulated_logging_time': 0.2302393913269043}
I0607 11:39:54.029921 139841760724736 logging_writer.py:48] [8071] accumulated_eval_time=816.158947, accumulated_logging_time=0.230239, accumulated_submission_time=2404.230341, global_step=8071, preemption_count=0, score=2404.230341, test/accuracy=0.985395, test/loss=0.049214, test/mean_average_precision=0.228926, test/num_examples=43793, total_duration=3222.718812, train/accuracy=0.989559, train/loss=0.035414, train/mean_average_precision=0.301058, validation/accuracy=0.986262, validation/loss=0.046351, validation/mean_average_precision=0.226023, validation/num_examples=43793
I0607 11:42:04.773320 139841769117440 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.008295, loss=0.038540
I0607 11:42:04.779948 139893975131968 submission.py:120] 8500) loss = 0.039, grad_norm = 0.008
I0607 11:43:54.249778 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:44:55.500813 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:44:58.714562 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:45:02.089348 139893975131968 submission_runner.py:419] Time since start: 3530.79s, 	Step: 8861, 	{'train/accuracy': 0.9894245864085549, 'train/loss': 0.035548450004628834, 'train/mean_average_precision': 0.3139895779484536, 'validation/accuracy': 0.9860693330475511, 'validation/loss': 0.04709886678102016, 'validation/mean_average_precision': 0.2260729836569421, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625748305007, 'test/loss': 0.049946937367086754, 'test/mean_average_precision': 0.22991836163138146, 'test/num_examples': 43793, 'score': 2644.231785297394, 'total_duration': 3530.7887859344482, 'accumulated_submission_time': 2644.231785297394, 'accumulated_eval_time': 883.9982924461365, 'accumulated_logging_time': 0.2534806728363037}
I0607 11:45:02.100728 139841760724736 logging_writer.py:48] [8861] accumulated_eval_time=883.998292, accumulated_logging_time=0.253481, accumulated_submission_time=2644.231785, global_step=8861, preemption_count=0, score=2644.231785, test/accuracy=0.985163, test/loss=0.049947, test/mean_average_precision=0.229918, test/num_examples=43793, total_duration=3530.788786, train/accuracy=0.989425, train/loss=0.035548, train/mean_average_precision=0.313990, validation/accuracy=0.986069, validation/loss=0.047099, validation/mean_average_precision=0.226073, validation/num_examples=43793
I0607 11:45:43.890646 139841769117440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.009666, loss=0.036648
I0607 11:45:43.896036 139893975131968 submission.py:120] 9000) loss = 0.037, grad_norm = 0.010
I0607 11:48:11.305438 139841760724736 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.006827, loss=0.037030
I0607 11:48:11.314240 139893975131968 submission.py:120] 9500) loss = 0.037, grad_norm = 0.007
I0607 11:49:02.259186 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:50:03.370129 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:50:06.676986 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:50:09.907502 139893975131968 submission_runner.py:419] Time since start: 3838.61s, 	Step: 9673, 	{'train/accuracy': 0.990005980991814, 'train/loss': 0.03345137277627963, 'train/mean_average_precision': 0.3414510731723641, 'validation/accuracy': 0.9865036895950092, 'validation/loss': 0.045389900839850714, 'validation/mean_average_precision': 0.23993791356082395, 'validation/num_examples': 43793, 'test/accuracy': 0.9857332936287143, 'test/loss': 0.04817658371307544, 'test/mean_average_precision': 0.24410928129072348, 'test/num_examples': 43793, 'score': 2884.1731748580933, 'total_duration': 3838.6069383621216, 'accumulated_submission_time': 2884.1731748580933, 'accumulated_eval_time': 951.6464085578918, 'accumulated_logging_time': 0.27706122398376465}
I0607 11:50:09.918179 139841769117440 logging_writer.py:48] [9673] accumulated_eval_time=951.646409, accumulated_logging_time=0.277061, accumulated_submission_time=2884.173175, global_step=9673, preemption_count=0, score=2884.173175, test/accuracy=0.985733, test/loss=0.048177, test/mean_average_precision=0.244109, test/num_examples=43793, total_duration=3838.606938, train/accuracy=0.990006, train/loss=0.033451, train/mean_average_precision=0.341451, validation/accuracy=0.986504, validation/loss=0.045390, validation/mean_average_precision=0.239938, validation/num_examples=43793
I0607 11:51:47.184930 139841760724736 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.008097, loss=0.033663
I0607 11:51:47.190687 139893975131968 submission.py:120] 10000) loss = 0.034, grad_norm = 0.008
I0607 11:54:10.180955 139893975131968 spec.py:298] Evaluating on the training split.
I0607 11:55:11.651196 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 11:55:14.967561 139893975131968 spec.py:326] Evaluating on the test split.
I0607 11:55:18.247300 139893975131968 submission_runner.py:419] Time since start: 4146.95s, 	Step: 10484, 	{'train/accuracy': 0.9903371669990859, 'train/loss': 0.03243829479458223, 'train/mean_average_precision': 0.37422292257237116, 'validation/accuracy': 0.9865455014869607, 'validation/loss': 0.045292532157607285, 'validation/mean_average_precision': 0.24429848929602252, 'validation/num_examples': 43793, 'test/accuracy': 0.9856772747356055, 'test/loss': 0.0482601053028832, 'test/mean_average_precision': 0.24087408118014206, 'test/num_examples': 43793, 'score': 3124.218169927597, 'total_duration': 4146.946712970734, 'accumulated_submission_time': 3124.218169927597, 'accumulated_eval_time': 1019.7125253677368, 'accumulated_logging_time': 0.2985234260559082}
I0607 11:55:18.262146 139841769117440 logging_writer.py:48] [10484] accumulated_eval_time=1019.712525, accumulated_logging_time=0.298523, accumulated_submission_time=3124.218170, global_step=10484, preemption_count=0, score=3124.218170, test/accuracy=0.985677, test/loss=0.048260, test/mean_average_precision=0.240874, test/num_examples=43793, total_duration=4146.946713, train/accuracy=0.990337, train/loss=0.032438, train/mean_average_precision=0.374223, validation/accuracy=0.986546, validation/loss=0.045293, validation/mean_average_precision=0.244298, validation/num_examples=43793
I0607 11:55:23.382775 139841760724736 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.009646, loss=0.038997
I0607 11:55:23.387688 139893975131968 submission.py:120] 10500) loss = 0.039, grad_norm = 0.010
I0607 11:57:50.392768 139841769117440 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.009999, loss=0.032542
I0607 11:57:50.398749 139893975131968 submission.py:120] 11000) loss = 0.033, grad_norm = 0.010
I0607 11:59:18.476900 139893975131968 spec.py:298] Evaluating on the training split.
I0607 12:00:19.895175 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 12:00:23.183769 139893975131968 spec.py:326] Evaluating on the test split.
I0607 12:00:26.452883 139893975131968 submission_runner.py:419] Time since start: 4455.15s, 	Step: 11298, 	{'train/accuracy': 0.9903636312550484, 'train/loss': 0.032101003562454156, 'train/mean_average_precision': 0.38245637127947474, 'validation/accuracy': 0.9864557885925792, 'validation/loss': 0.04553436181453868, 'validation/mean_average_precision': 0.24842945562351285, 'validation/num_examples': 43793, 'test/accuracy': 0.9855572342503724, 'test/loss': 0.04862425443275816, 'test/mean_average_precision': 0.2429876699737781, 'test/num_examples': 43793, 'score': 3364.21489405632, 'total_duration': 4455.152314186096, 'accumulated_submission_time': 3364.21489405632, 'accumulated_eval_time': 1087.688274383545, 'accumulated_logging_time': 0.32463550567626953}
I0607 12:00:26.463499 139841760724736 logging_writer.py:48] [11298] accumulated_eval_time=1087.688274, accumulated_logging_time=0.324636, accumulated_submission_time=3364.214894, global_step=11298, preemption_count=0, score=3364.214894, test/accuracy=0.985557, test/loss=0.048624, test/mean_average_precision=0.242988, test/num_examples=43793, total_duration=4455.152314, train/accuracy=0.990364, train/loss=0.032101, train/mean_average_precision=0.382456, validation/accuracy=0.986456, validation/loss=0.045534, validation/mean_average_precision=0.248429, validation/num_examples=43793
I0607 12:01:26.392799 139841769117440 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.012663, loss=0.034054
I0607 12:01:26.398231 139893975131968 submission.py:120] 11500) loss = 0.034, grad_norm = 0.013
I0607 12:03:52.985387 139893975131968 spec.py:298] Evaluating on the training split.
I0607 12:04:54.344640 139893975131968 spec.py:310] Evaluating on the validation split.
I0607 12:04:57.657496 139893975131968 spec.py:326] Evaluating on the test split.
I0607 12:05:00.951540 139893975131968 submission_runner.py:419] Time since start: 4729.65s, 	Step: 12000, 	{'train/accuracy': 0.9908996184421652, 'train/loss': 0.030561583615497028, 'train/mean_average_precision': 0.40960240944467374, 'validation/accuracy': 0.9866141054650173, 'validation/loss': 0.04491797574930564, 'validation/mean_average_precision': 0.251598391519426, 'validation/num_examples': 43793, 'test/accuracy': 0.9856743263728104, 'test/loss': 0.04797091237402593, 'test/mean_average_precision': 0.24164383748905413, 'test/num_examples': 43793, 'score': 3570.5472073554993, 'total_duration': 4729.650901556015, 'accumulated_submission_time': 3570.5472073554993, 'accumulated_eval_time': 1155.654138803482, 'accumulated_logging_time': 0.3475522994995117}
I0607 12:05:00.962642 139841760724736 logging_writer.py:48] [12000] accumulated_eval_time=1155.654139, accumulated_logging_time=0.347552, accumulated_submission_time=3570.547207, global_step=12000, preemption_count=0, score=3570.547207, test/accuracy=0.985674, test/loss=0.047971, test/mean_average_precision=0.241644, test/num_examples=43793, total_duration=4729.650902, train/accuracy=0.990900, train/loss=0.030562, train/mean_average_precision=0.409602, validation/accuracy=0.986614, validation/loss=0.044918, validation/mean_average_precision=0.251598, validation/num_examples=43793
I0607 12:05:00.984857 139841769117440 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3570.547207
I0607 12:05:01.084865 139893975131968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/adamw/ogbg_pytorch/trial_1/checkpoint_12000.
I0607 12:05:01.270631 139893975131968 submission_runner.py:581] Tuning trial 1/1
I0607 12:05:01.270870 139893975131968 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 12:05:01.272754 139893975131968 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.48630294684236786, 'train/loss': 0.7890582516451058, 'train/mean_average_precision': 0.022155409653072215, 'validation/accuracy': 0.4963380089582993, 'validation/loss': 0.7852067293601481, 'validation/mean_average_precision': 0.02740020127348663, 'validation/num_examples': 43793, 'test/accuracy': 0.49977781980364744, 'test/loss': 0.7828834061508745, 'test/mean_average_precision': 0.028979926353912756, 'test/num_examples': 43793, 'score': 5.267739772796631, 'total_duration': 156.69123435020447, 'accumulated_submission_time': 5.267739772796631, 'accumulated_eval_time': 151.4232199192047, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (806, {'train/accuracy': 0.9867192734897874, 'train/loss': 0.05308328094329657, 'train/mean_average_precision': 0.04951251638365205, 'validation/accuracy': 0.984169530578295, 'validation/loss': 0.06228685576196287, 'validation/mean_average_precision': 0.04807080751566898, 'validation/num_examples': 43793, 'test/accuracy': 0.9831804326427566, 'test/loss': 0.06555142386969247, 'test/mean_average_precision': 0.05074568539607096, 'test/num_examples': 43793, 'score': 245.30491065979004, 'total_duration': 461.8272271156311, 'accumulated_submission_time': 245.30491065979004, 'accumulated_eval_time': 216.2839274406433, 'accumulated_logging_time': 0.025660991668701172, 'global_step': 806, 'preemption_count': 0}), (1616, {'train/accuracy': 0.9870751470096564, 'train/loss': 0.04848041981336165, 'train/mean_average_precision': 0.08313269357825044, 'validation/accuracy': 0.9844216197521002, 'validation/loss': 0.05778544912669978, 'validation/mean_average_precision': 0.08719600315716167, 'validation/num_examples': 43793, 'test/accuracy': 0.9834323070644037, 'test/loss': 0.061161917135000056, 'test/mean_average_precision': 0.08401649356245705, 'test/num_examples': 43793, 'score': 485.1325948238373, 'total_duration': 767.1780886650085, 'accumulated_submission_time': 485.1325948238373, 'accumulated_eval_time': 281.57729744911194, 'accumulated_logging_time': 0.046861886978149414, 'global_step': 1616, 'preemption_count': 0}), (2421, {'train/accuracy': 0.9876401803737933, 'train/loss': 0.045168281874094496, 'train/mean_average_precision': 0.11594505132735314, 'validation/accuracy': 0.9848141644076067, 'validation/loss': 0.05459519923366515, 'validation/mean_average_precision': 0.11683352920239723, 'validation/num_examples': 43793, 'test/accuracy': 0.9838139094490395, 'test/loss': 0.05781813808572912, 'test/mean_average_precision': 0.1141335402328601, 'test/num_examples': 43793, 'score': 724.9708936214447, 'total_duration': 1073.0170452594757, 'accumulated_submission_time': 724.9708936214447, 'accumulated_eval_time': 347.34680128097534, 'accumulated_logging_time': 0.06747651100158691, 'global_step': 2421, 'preemption_count': 0}), (3230, {'train/accuracy': 0.9877289185267306, 'train/loss': 0.043963524474397886, 'train/mean_average_precision': 0.15114338271175232, 'validation/accuracy': 0.984984253560303, 'validation/loss': 0.05278647970864824, 'validation/mean_average_precision': 0.13599124740379717, 'validation/num_examples': 43793, 'test/accuracy': 0.983999235110452, 'test/loss': 0.05566665589110264, 'test/mean_average_precision': 0.13530707026025351, 'test/num_examples': 43793, 'score': 964.9294726848602, 'total_duration': 1380.2370221614838, 'accumulated_submission_time': 964.9294726848602, 'accumulated_eval_time': 414.37606620788574, 'accumulated_logging_time': 0.08963966369628906, 'global_step': 3230, 'preemption_count': 0}), (4038, {'train/accuracy': 0.9880568255097664, 'train/loss': 0.042323980437690235, 'train/mean_average_precision': 0.17078083092168295, 'validation/accuracy': 0.9851856001467881, 'validation/loss': 0.051598050256676305, 'validation/mean_average_precision': 0.1541669269160101, 'validation/num_examples': 43793, 'test/accuracy': 0.9842418432490284, 'test/loss': 0.0543140154848014, 'test/mean_average_precision': 0.15405868377087178, 'test/num_examples': 43793, 'score': 1204.7252173423767, 'total_duration': 1686.2681159973145, 'accumulated_submission_time': 1204.7252173423767, 'accumulated_eval_time': 480.3755261898041, 'accumulated_logging_time': 0.11551594734191895, 'global_step': 4038, 'preemption_count': 0}), (4847, {'train/accuracy': 0.9883467293371991, 'train/loss': 0.04064191207265899, 'train/mean_average_precision': 0.1928252778195712, 'validation/accuracy': 0.9854864022044204, 'validation/loss': 0.05013781813572546, 'validation/mean_average_precision': 0.17350387091107397, 'validation/num_examples': 43793, 'test/accuracy': 0.9845417338647687, 'test/loss': 0.05280153827669879, 'test/mean_average_precision': 0.174168320777957, 'test/num_examples': 43793, 'score': 1444.5916619300842, 'total_duration': 1993.6283056735992, 'accumulated_submission_time': 1444.5916619300842, 'accumulated_eval_time': 547.6360187530518, 'accumulated_logging_time': 0.13864636421203613, 'global_step': 4847, 'preemption_count': 0}), (5651, {'train/accuracy': 0.9885155806619214, 'train/loss': 0.03938126268065978, 'train/mean_average_precision': 0.23110692952281192, 'validation/accuracy': 0.9856455309582555, 'validation/loss': 0.04852048062160887, 'validation/mean_average_precision': 0.19245739984321628, 'validation/num_examples': 43793, 'test/accuracy': 0.9847653882425188, 'test/loss': 0.051187480756667826, 'test/mean_average_precision': 0.19464023304905445, 'test/num_examples': 43793, 'score': 1684.547529220581, 'total_duration': 2300.856873512268, 'accumulated_submission_time': 1684.547529220581, 'accumulated_eval_time': 614.6752550601959, 'accumulated_logging_time': 0.16307902336120605, 'global_step': 5651, 'preemption_count': 0}), (6453, {'train/accuracy': 0.989167771097521, 'train/loss': 0.03720977698810369, 'train/mean_average_precision': 0.2653584975850052, 'validation/accuracy': 0.9859601349996387, 'validation/loss': 0.047248830155020634, 'validation/mean_average_precision': 0.20958068201370725, 'validation/num_examples': 43793, 'test/accuracy': 0.985181949786012, 'test/loss': 0.049834906162036126, 'test/mean_average_precision': 0.2098387459043158, 'test/num_examples': 43793, 'score': 1924.4476079940796, 'total_duration': 2607.278535604477, 'accumulated_submission_time': 1924.4476079940796, 'accumulated_eval_time': 680.9656548500061, 'accumulated_logging_time': 0.18550801277160645, 'global_step': 6453, 'preemption_count': 0}), (7271, {'train/accuracy': 0.98896432149081, 'train/loss': 0.0374562425451536, 'train/mean_average_precision': 0.2705501007608124, 'validation/accuracy': 0.985903709242539, 'validation/loss': 0.04751141035875415, 'validation/mean_average_precision': 0.22001600765047283, 'validation/num_examples': 43793, 'test/accuracy': 0.9850400071771575, 'test/loss': 0.05020351403252213, 'test/mean_average_precision': 0.21986439394602794, 'test/num_examples': 43793, 'score': 2164.441212415695, 'total_duration': 2915.1579587459564, 'accumulated_submission_time': 2164.441212415695, 'accumulated_eval_time': 748.6167373657227, 'accumulated_logging_time': 0.2083594799041748, 'global_step': 7271, 'preemption_count': 0}), (8071, {'train/accuracy': 0.9895591859408438, 'train/loss': 0.035413785073734376, 'train/mean_average_precision': 0.30105829338508816, 'validation/accuracy': 0.9862621548793666, 'validation/loss': 0.04635086078710278, 'validation/mean_average_precision': 0.22602280541673067, 'validation/num_examples': 43793, 'test/accuracy': 0.9853946531019514, 'test/loss': 0.049214265921896184, 'test/mean_average_precision': 0.2289259317680632, 'test/num_examples': 43793, 'score': 2404.2303414344788, 'total_duration': 3222.7188119888306, 'accumulated_submission_time': 2404.2303414344788, 'accumulated_eval_time': 816.1589467525482, 'accumulated_logging_time': 0.2302393913269043, 'global_step': 8071, 'preemption_count': 0}), (8861, {'train/accuracy': 0.9894245864085549, 'train/loss': 0.035548450004628834, 'train/mean_average_precision': 0.3139895779484536, 'validation/accuracy': 0.9860693330475511, 'validation/loss': 0.04709886678102016, 'validation/mean_average_precision': 0.2260729836569421, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625748305007, 'test/loss': 0.049946937367086754, 'test/mean_average_precision': 0.22991836163138146, 'test/num_examples': 43793, 'score': 2644.231785297394, 'total_duration': 3530.7887859344482, 'accumulated_submission_time': 2644.231785297394, 'accumulated_eval_time': 883.9982924461365, 'accumulated_logging_time': 0.2534806728363037, 'global_step': 8861, 'preemption_count': 0}), (9673, {'train/accuracy': 0.990005980991814, 'train/loss': 0.03345137277627963, 'train/mean_average_precision': 0.3414510731723641, 'validation/accuracy': 0.9865036895950092, 'validation/loss': 0.045389900839850714, 'validation/mean_average_precision': 0.23993791356082395, 'validation/num_examples': 43793, 'test/accuracy': 0.9857332936287143, 'test/loss': 0.04817658371307544, 'test/mean_average_precision': 0.24410928129072348, 'test/num_examples': 43793, 'score': 2884.1731748580933, 'total_duration': 3838.6069383621216, 'accumulated_submission_time': 2884.1731748580933, 'accumulated_eval_time': 951.6464085578918, 'accumulated_logging_time': 0.27706122398376465, 'global_step': 9673, 'preemption_count': 0}), (10484, {'train/accuracy': 0.9903371669990859, 'train/loss': 0.03243829479458223, 'train/mean_average_precision': 0.37422292257237116, 'validation/accuracy': 0.9865455014869607, 'validation/loss': 0.045292532157607285, 'validation/mean_average_precision': 0.24429848929602252, 'validation/num_examples': 43793, 'test/accuracy': 0.9856772747356055, 'test/loss': 0.0482601053028832, 'test/mean_average_precision': 0.24087408118014206, 'test/num_examples': 43793, 'score': 3124.218169927597, 'total_duration': 4146.946712970734, 'accumulated_submission_time': 3124.218169927597, 'accumulated_eval_time': 1019.7125253677368, 'accumulated_logging_time': 0.2985234260559082, 'global_step': 10484, 'preemption_count': 0}), (11298, {'train/accuracy': 0.9903636312550484, 'train/loss': 0.032101003562454156, 'train/mean_average_precision': 0.38245637127947474, 'validation/accuracy': 0.9864557885925792, 'validation/loss': 0.04553436181453868, 'validation/mean_average_precision': 0.24842945562351285, 'validation/num_examples': 43793, 'test/accuracy': 0.9855572342503724, 'test/loss': 0.04862425443275816, 'test/mean_average_precision': 0.2429876699737781, 'test/num_examples': 43793, 'score': 3364.21489405632, 'total_duration': 4455.152314186096, 'accumulated_submission_time': 3364.21489405632, 'accumulated_eval_time': 1087.688274383545, 'accumulated_logging_time': 0.32463550567626953, 'global_step': 11298, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9908996184421652, 'train/loss': 0.030561583615497028, 'train/mean_average_precision': 0.40960240944467374, 'validation/accuracy': 0.9866141054650173, 'validation/loss': 0.04491797574930564, 'validation/mean_average_precision': 0.251598391519426, 'validation/num_examples': 43793, 'test/accuracy': 0.9856743263728104, 'test/loss': 0.04797091237402593, 'test/mean_average_precision': 0.24164383748905413, 'test/num_examples': 43793, 'score': 3570.5472073554993, 'total_duration': 4729.650901556015, 'accumulated_submission_time': 3570.5472073554993, 'accumulated_eval_time': 1155.654138803482, 'accumulated_logging_time': 0.3475522994995117, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0607 12:05:01.272882 139893975131968 submission_runner.py:584] Timing: 3570.5472073554993
I0607 12:05:01.272934 139893975131968 submission_runner.py:586] Total number of evals: 16
I0607 12:05:01.272979 139893975131968 submission_runner.py:587] ====================
I0607 12:05:01.273100 139893975131968 submission_runner.py:655] Final ogbg score: 3570.5472073554993
