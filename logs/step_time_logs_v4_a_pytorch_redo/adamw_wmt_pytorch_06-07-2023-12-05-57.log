torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_pytorch_06-07-2023-12-05-57.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 12:06:20.702284 140548505618240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 12:06:20.702330 140631325992768 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 12:06:20.702352 139729412405056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 12:06:20.703090 140104514905920 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 12:06:20.703486 139885366220608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 12:06:20.704032 139726182946624 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 12:06:21.690742 139923020547904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 12:06:21.699369 140561621198656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 12:06:21.699746 140561621198656 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.701378 139923020547904 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.708164 140548505618240 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.708190 140631325992768 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.708271 139729412405056 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.708291 139885366220608 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.708323 140104514905920 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:21.708362 139726182946624 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 12:06:26.305169 140561621198656 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch because --overwrite was set.
I0607 12:06:26.314131 140561621198656 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch.
W0607 12:06:26.345027 139885366220608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.345161 140548505618240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.345785 140631325992768 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.346529 139729412405056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.347242 140104514905920 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.347284 139923020547904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.347404 140561621198656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 12:06:26.348327 139726182946624 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 12:06:26.352324 140561621198656 submission_runner.py:541] Using RNG seed 1689525285
I0607 12:06:26.353531 140561621198656 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 12:06:26.353640 140561621198656 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch/trial_1.
I0607 12:06:26.353886 140561621198656 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch/trial_1/hparams.json.
I0607 12:06:26.354888 140561621198656 submission_runner.py:255] Initializing dataset.
I0607 12:06:26.355003 140561621198656 submission_runner.py:262] Initializing model.
I0607 12:06:29.956619 140561621198656 submission_runner.py:272] Initializing optimizer.
I0607 12:06:29.957947 140561621198656 submission_runner.py:279] Initializing metrics bundle.
I0607 12:06:29.958102 140561621198656 submission_runner.py:297] Initializing checkpoint and logger.
I0607 12:06:29.963124 140561621198656 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 12:06:29.963232 140561621198656 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 12:06:30.552705 140561621198656 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0607 12:06:30.553481 140561621198656 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch/trial_1/flags_0.json.
I0607 12:06:30.605602 140561621198656 submission_runner.py:332] Starting training loop.
I0607 12:06:30.620852 140561621198656 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:06:30.624286 140561621198656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:06:30.624402 140561621198656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:06:30.694460 140561621198656 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:06:35.051072 140521213253376 logging_writer.py:48] [0] global_step=0, grad_norm=5.448702, loss=11.052062
I0607 12:06:35.058249 140561621198656 submission.py:120] 0) loss = 11.052, grad_norm = 5.449
I0607 12:06:35.059542 140561621198656 spec.py:298] Evaluating on the training split.
I0607 12:06:35.062195 140561621198656 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:06:35.064920 140561621198656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:06:35.065030 140561621198656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:06:35.094870 140561621198656 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0607 12:06:39.209060 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 12:11:13.505914 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 12:11:13.510958 140561621198656 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:11:13.514553 140561621198656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:11:13.514675 140561621198656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:11:13.544228 140561621198656 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:11:17.368884 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 12:15:46.015104 140561621198656 spec.py:326] Evaluating on the test split.
I0607 12:15:46.017985 140561621198656 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:15:46.021136 140561621198656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0607 12:15:46.021264 140561621198656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0607 12:15:46.050211 140561621198656 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0607 12:15:49.942080 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 12:20:25.402369 140561621198656 submission_runner.py:419] Time since start: 834.80s, 	Step: 1, 	{'train/accuracy': 0.0005973441161604558, 'train/loss': 11.055712390295456, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.065812110203222, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.057252629132531, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.454050779342651, 'total_duration': 834.7971563339233, 'accumulated_submission_time': 4.454050779342651, 'accumulated_eval_time': 830.3427360057831, 'accumulated_logging_time': 0}
I0607 12:20:25.419781 140503663007488 logging_writer.py:48] [1] accumulated_eval_time=830.342736, accumulated_logging_time=0, accumulated_submission_time=4.454051, global_step=1, preemption_count=0, score=4.454051, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.057253, test/num_examples=3003, total_duration=834.797156, train/accuracy=0.000597, train/bleu=0.000000, train/loss=11.055712, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.065812, validation/num_examples=3000
I0607 12:20:25.438352 140561621198656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438353 139729412405056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438353 140548505618240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438379 139726182946624 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438385 140631325992768 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438383 140104514905920 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438660 139885366220608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.438801 139923020547904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 12:20:25.899293 140503654614784 logging_writer.py:48] [1] global_step=1, grad_norm=5.284430, loss=11.056710
I0607 12:20:25.902611 140561621198656 submission.py:120] 1) loss = 11.057, grad_norm = 5.284
I0607 12:20:26.347761 140503663007488 logging_writer.py:48] [2] global_step=2, grad_norm=5.395920, loss=11.043570
I0607 12:20:26.351086 140561621198656 submission.py:120] 2) loss = 11.044, grad_norm = 5.396
I0607 12:20:26.795187 140503654614784 logging_writer.py:48] [3] global_step=3, grad_norm=5.287488, loss=11.036984
I0607 12:20:26.799026 140561621198656 submission.py:120] 3) loss = 11.037, grad_norm = 5.287
I0607 12:20:27.245102 140503663007488 logging_writer.py:48] [4] global_step=4, grad_norm=5.201109, loss=11.024194
I0607 12:20:27.248730 140561621198656 submission.py:120] 4) loss = 11.024, grad_norm = 5.201
I0607 12:20:27.698815 140503654614784 logging_writer.py:48] [5] global_step=5, grad_norm=5.107277, loss=10.973540
I0607 12:20:27.702550 140561621198656 submission.py:120] 5) loss = 10.974, grad_norm = 5.107
I0607 12:20:28.151710 140503663007488 logging_writer.py:48] [6] global_step=6, grad_norm=5.140074, loss=10.950068
I0607 12:20:28.155422 140561621198656 submission.py:120] 6) loss = 10.950, grad_norm = 5.140
I0607 12:20:28.603936 140503654614784 logging_writer.py:48] [7] global_step=7, grad_norm=4.949592, loss=10.906345
I0607 12:20:28.607739 140561621198656 submission.py:120] 7) loss = 10.906, grad_norm = 4.950
I0607 12:20:29.057260 140503663007488 logging_writer.py:48] [8] global_step=8, grad_norm=4.843841, loss=10.851315
I0607 12:20:29.061072 140561621198656 submission.py:120] 8) loss = 10.851, grad_norm = 4.844
I0607 12:20:29.507013 140503654614784 logging_writer.py:48] [9] global_step=9, grad_norm=4.759897, loss=10.792275
I0607 12:20:29.510826 140561621198656 submission.py:120] 9) loss = 10.792, grad_norm = 4.760
I0607 12:20:29.957899 140503663007488 logging_writer.py:48] [10] global_step=10, grad_norm=4.456666, loss=10.737864
I0607 12:20:29.961212 140561621198656 submission.py:120] 10) loss = 10.738, grad_norm = 4.457
I0607 12:20:30.415093 140503654614784 logging_writer.py:48] [11] global_step=11, grad_norm=4.385418, loss=10.671443
I0607 12:20:30.418365 140561621198656 submission.py:120] 11) loss = 10.671, grad_norm = 4.385
I0607 12:20:30.872367 140503663007488 logging_writer.py:48] [12] global_step=12, grad_norm=4.155177, loss=10.606329
I0607 12:20:30.876034 140561621198656 submission.py:120] 12) loss = 10.606, grad_norm = 4.155
I0607 12:20:31.321007 140503654614784 logging_writer.py:48] [13] global_step=13, grad_norm=4.008033, loss=10.538427
I0607 12:20:31.324466 140561621198656 submission.py:120] 13) loss = 10.538, grad_norm = 4.008
I0607 12:20:31.770569 140503663007488 logging_writer.py:48] [14] global_step=14, grad_norm=3.765923, loss=10.473873
I0607 12:20:31.774216 140561621198656 submission.py:120] 14) loss = 10.474, grad_norm = 3.766
I0607 12:20:32.224408 140503654614784 logging_writer.py:48] [15] global_step=15, grad_norm=3.584527, loss=10.399561
I0607 12:20:32.227972 140561621198656 submission.py:120] 15) loss = 10.400, grad_norm = 3.585
I0607 12:20:32.673557 140503663007488 logging_writer.py:48] [16] global_step=16, grad_norm=3.409606, loss=10.323581
I0607 12:20:32.677454 140561621198656 submission.py:120] 16) loss = 10.324, grad_norm = 3.410
I0607 12:20:33.121707 140503654614784 logging_writer.py:48] [17] global_step=17, grad_norm=3.195013, loss=10.263866
I0607 12:20:33.125295 140561621198656 submission.py:120] 17) loss = 10.264, grad_norm = 3.195
I0607 12:20:33.573719 140503663007488 logging_writer.py:48] [18] global_step=18, grad_norm=3.015460, loss=10.192350
I0607 12:20:33.577908 140561621198656 submission.py:120] 18) loss = 10.192, grad_norm = 3.015
I0607 12:20:34.022896 140503654614784 logging_writer.py:48] [19] global_step=19, grad_norm=2.834444, loss=10.109533
I0607 12:20:34.025862 140561621198656 submission.py:120] 19) loss = 10.110, grad_norm = 2.834
I0607 12:20:34.471895 140503663007488 logging_writer.py:48] [20] global_step=20, grad_norm=2.675164, loss=10.055546
I0607 12:20:34.475024 140561621198656 submission.py:120] 20) loss = 10.056, grad_norm = 2.675
I0607 12:20:34.925966 140503654614784 logging_writer.py:48] [21] global_step=21, grad_norm=2.499334, loss=9.988078
I0607 12:20:34.929666 140561621198656 submission.py:120] 21) loss = 9.988, grad_norm = 2.499
I0607 12:20:35.379907 140503663007488 logging_writer.py:48] [22] global_step=22, grad_norm=2.348228, loss=9.926880
I0607 12:20:35.383692 140561621198656 submission.py:120] 22) loss = 9.927, grad_norm = 2.348
I0607 12:20:35.835894 140503654614784 logging_writer.py:48] [23] global_step=23, grad_norm=2.200253, loss=9.855515
I0607 12:20:35.839550 140561621198656 submission.py:120] 23) loss = 9.856, grad_norm = 2.200
I0607 12:20:36.294417 140503663007488 logging_writer.py:48] [24] global_step=24, grad_norm=2.046810, loss=9.812168
I0607 12:20:36.297969 140561621198656 submission.py:120] 24) loss = 9.812, grad_norm = 2.047
I0607 12:20:36.748557 140503654614784 logging_writer.py:48] [25] global_step=25, grad_norm=1.936387, loss=9.740996
I0607 12:20:36.752406 140561621198656 submission.py:120] 25) loss = 9.741, grad_norm = 1.936
I0607 12:20:37.196884 140503663007488 logging_writer.py:48] [26] global_step=26, grad_norm=1.802404, loss=9.700131
I0607 12:20:37.200048 140561621198656 submission.py:120] 26) loss = 9.700, grad_norm = 1.802
I0607 12:20:37.645743 140503654614784 logging_writer.py:48] [27] global_step=27, grad_norm=1.700531, loss=9.632905
I0607 12:20:37.648887 140561621198656 submission.py:120] 27) loss = 9.633, grad_norm = 1.701
I0607 12:20:38.105870 140503663007488 logging_writer.py:48] [28] global_step=28, grad_norm=1.590146, loss=9.598878
I0607 12:20:38.110187 140561621198656 submission.py:120] 28) loss = 9.599, grad_norm = 1.590
I0607 12:20:38.566380 140503654614784 logging_writer.py:48] [29] global_step=29, grad_norm=1.493397, loss=9.534762
I0607 12:20:38.570314 140561621198656 submission.py:120] 29) loss = 9.535, grad_norm = 1.493
I0607 12:20:39.015786 140503663007488 logging_writer.py:48] [30] global_step=30, grad_norm=1.408508, loss=9.502754
I0607 12:20:39.019095 140561621198656 submission.py:120] 30) loss = 9.503, grad_norm = 1.409
I0607 12:20:39.475031 140503654614784 logging_writer.py:48] [31] global_step=31, grad_norm=1.327709, loss=9.459126
I0607 12:20:39.478407 140561621198656 submission.py:120] 31) loss = 9.459, grad_norm = 1.328
I0607 12:20:39.923351 140503663007488 logging_writer.py:48] [32] global_step=32, grad_norm=1.230829, loss=9.413859
I0607 12:20:39.926521 140561621198656 submission.py:120] 32) loss = 9.414, grad_norm = 1.231
I0607 12:20:40.372232 140503654614784 logging_writer.py:48] [33] global_step=33, grad_norm=1.176442, loss=9.376773
I0607 12:20:40.375555 140561621198656 submission.py:120] 33) loss = 9.377, grad_norm = 1.176
I0607 12:20:40.822135 140503663007488 logging_writer.py:48] [34] global_step=34, grad_norm=1.095451, loss=9.354861
I0607 12:20:40.825395 140561621198656 submission.py:120] 34) loss = 9.355, grad_norm = 1.095
I0607 12:20:41.269404 140503654614784 logging_writer.py:48] [35] global_step=35, grad_norm=1.036991, loss=9.313162
I0607 12:20:41.272810 140561621198656 submission.py:120] 35) loss = 9.313, grad_norm = 1.037
I0607 12:20:41.715420 140503663007488 logging_writer.py:48] [36] global_step=36, grad_norm=0.962725, loss=9.318069
I0607 12:20:41.719329 140561621198656 submission.py:120] 36) loss = 9.318, grad_norm = 0.963
I0607 12:20:42.166826 140503654614784 logging_writer.py:48] [37] global_step=37, grad_norm=0.916052, loss=9.280277
I0607 12:20:42.170804 140561621198656 submission.py:120] 37) loss = 9.280, grad_norm = 0.916
I0607 12:20:42.618407 140503663007488 logging_writer.py:48] [38] global_step=38, grad_norm=0.881901, loss=9.244380
I0607 12:20:42.622398 140561621198656 submission.py:120] 38) loss = 9.244, grad_norm = 0.882
I0607 12:20:43.067202 140503654614784 logging_writer.py:48] [39] global_step=39, grad_norm=0.827735, loss=9.224948
I0607 12:20:43.070972 140561621198656 submission.py:120] 39) loss = 9.225, grad_norm = 0.828
I0607 12:20:43.517194 140503663007488 logging_writer.py:48] [40] global_step=40, grad_norm=0.803436, loss=9.177056
I0607 12:20:43.520630 140561621198656 submission.py:120] 40) loss = 9.177, grad_norm = 0.803
I0607 12:20:43.974598 140503654614784 logging_writer.py:48] [41] global_step=41, grad_norm=0.750660, loss=9.162671
I0607 12:20:43.977886 140561621198656 submission.py:120] 41) loss = 9.163, grad_norm = 0.751
I0607 12:20:44.425683 140503663007488 logging_writer.py:48] [42] global_step=42, grad_norm=0.721494, loss=9.130443
I0607 12:20:44.429401 140561621198656 submission.py:120] 42) loss = 9.130, grad_norm = 0.721
I0607 12:20:44.878136 140503654614784 logging_writer.py:48] [43] global_step=43, grad_norm=0.685746, loss=9.117893
I0607 12:20:44.881606 140561621198656 submission.py:120] 43) loss = 9.118, grad_norm = 0.686
I0607 12:20:45.328015 140503663007488 logging_writer.py:48] [44] global_step=44, grad_norm=0.661795, loss=9.088317
I0607 12:20:45.332073 140561621198656 submission.py:120] 44) loss = 9.088, grad_norm = 0.662
I0607 12:20:45.777627 140503654614784 logging_writer.py:48] [45] global_step=45, grad_norm=0.621836, loss=9.074976
I0607 12:20:45.781149 140561621198656 submission.py:120] 45) loss = 9.075, grad_norm = 0.622
I0607 12:20:46.229019 140503663007488 logging_writer.py:48] [46] global_step=46, grad_norm=0.607785, loss=9.015488
I0607 12:20:46.232512 140561621198656 submission.py:120] 46) loss = 9.015, grad_norm = 0.608
I0607 12:20:46.678492 140503654614784 logging_writer.py:48] [47] global_step=47, grad_norm=0.580974, loss=9.025690
I0607 12:20:46.682030 140561621198656 submission.py:120] 47) loss = 9.026, grad_norm = 0.581
I0607 12:20:47.126049 140503663007488 logging_writer.py:48] [48] global_step=48, grad_norm=0.549529, loss=9.004058
I0607 12:20:47.129417 140561621198656 submission.py:120] 48) loss = 9.004, grad_norm = 0.550
I0607 12:20:47.577498 140503654614784 logging_writer.py:48] [49] global_step=49, grad_norm=0.524360, loss=9.000930
I0607 12:20:47.580706 140561621198656 submission.py:120] 49) loss = 9.001, grad_norm = 0.524
I0607 12:20:48.027416 140503663007488 logging_writer.py:48] [50] global_step=50, grad_norm=0.505493, loss=8.978837
I0607 12:20:48.030609 140561621198656 submission.py:120] 50) loss = 8.979, grad_norm = 0.505
I0607 12:20:48.474458 140503654614784 logging_writer.py:48] [51] global_step=51, grad_norm=0.495614, loss=8.924561
I0607 12:20:48.477746 140561621198656 submission.py:120] 51) loss = 8.925, grad_norm = 0.496
I0607 12:20:48.926216 140503663007488 logging_writer.py:48] [52] global_step=52, grad_norm=0.475002, loss=8.958867
I0607 12:20:48.929505 140561621198656 submission.py:120] 52) loss = 8.959, grad_norm = 0.475
I0607 12:20:49.376283 140503654614784 logging_writer.py:48] [53] global_step=53, grad_norm=0.451789, loss=8.955370
I0607 12:20:49.379956 140561621198656 submission.py:120] 53) loss = 8.955, grad_norm = 0.452
I0607 12:20:49.824185 140503663007488 logging_writer.py:48] [54] global_step=54, grad_norm=0.428615, loss=8.919895
I0607 12:20:49.827594 140561621198656 submission.py:120] 54) loss = 8.920, grad_norm = 0.429
I0607 12:20:50.276049 140503654614784 logging_writer.py:48] [55] global_step=55, grad_norm=0.418927, loss=8.937631
I0607 12:20:50.279920 140561621198656 submission.py:120] 55) loss = 8.938, grad_norm = 0.419
I0607 12:20:50.725236 140503663007488 logging_writer.py:48] [56] global_step=56, grad_norm=0.415326, loss=8.885986
I0607 12:20:50.729169 140561621198656 submission.py:120] 56) loss = 8.886, grad_norm = 0.415
I0607 12:20:51.172027 140503654614784 logging_writer.py:48] [57] global_step=57, grad_norm=0.399419, loss=8.887716
I0607 12:20:51.175851 140561621198656 submission.py:120] 57) loss = 8.888, grad_norm = 0.399
I0607 12:20:51.624407 140503663007488 logging_writer.py:48] [58] global_step=58, grad_norm=0.393175, loss=8.864772
I0607 12:20:51.628323 140561621198656 submission.py:120] 58) loss = 8.865, grad_norm = 0.393
I0607 12:20:52.075964 140503654614784 logging_writer.py:48] [59] global_step=59, grad_norm=0.362148, loss=8.897079
I0607 12:20:52.079822 140561621198656 submission.py:120] 59) loss = 8.897, grad_norm = 0.362
I0607 12:20:52.524735 140503663007488 logging_writer.py:48] [60] global_step=60, grad_norm=0.349641, loss=8.849870
I0607 12:20:52.528260 140561621198656 submission.py:120] 60) loss = 8.850, grad_norm = 0.350
I0607 12:20:52.975564 140503654614784 logging_writer.py:48] [61] global_step=61, grad_norm=0.342843, loss=8.833385
I0607 12:20:52.979315 140561621198656 submission.py:120] 61) loss = 8.833, grad_norm = 0.343
I0607 12:20:53.425408 140503663007488 logging_writer.py:48] [62] global_step=62, grad_norm=0.337625, loss=8.845616
I0607 12:20:53.428905 140561621198656 submission.py:120] 62) loss = 8.846, grad_norm = 0.338
I0607 12:20:53.872610 140503654614784 logging_writer.py:48] [63] global_step=63, grad_norm=0.343857, loss=8.795146
I0607 12:20:53.876304 140561621198656 submission.py:120] 63) loss = 8.795, grad_norm = 0.344
I0607 12:20:54.322436 140503663007488 logging_writer.py:48] [64] global_step=64, grad_norm=0.318579, loss=8.834457
I0607 12:20:54.326151 140561621198656 submission.py:120] 64) loss = 8.834, grad_norm = 0.319
I0607 12:20:54.781012 140503654614784 logging_writer.py:48] [65] global_step=65, grad_norm=0.313358, loss=8.818465
I0607 12:20:54.784926 140561621198656 submission.py:120] 65) loss = 8.818, grad_norm = 0.313
I0607 12:20:55.227953 140503663007488 logging_writer.py:48] [66] global_step=66, grad_norm=0.306831, loss=8.807644
I0607 12:20:55.231593 140561621198656 submission.py:120] 66) loss = 8.808, grad_norm = 0.307
I0607 12:20:55.680258 140503654614784 logging_writer.py:48] [67] global_step=67, grad_norm=0.303918, loss=8.776675
I0607 12:20:55.683624 140561621198656 submission.py:120] 67) loss = 8.777, grad_norm = 0.304
I0607 12:20:56.134715 140503663007488 logging_writer.py:48] [68] global_step=68, grad_norm=0.305818, loss=8.762436
I0607 12:20:56.137876 140561621198656 submission.py:120] 68) loss = 8.762, grad_norm = 0.306
I0607 12:20:56.583379 140503654614784 logging_writer.py:48] [69] global_step=69, grad_norm=0.289499, loss=8.782626
I0607 12:20:56.586602 140561621198656 submission.py:120] 69) loss = 8.783, grad_norm = 0.289
I0607 12:20:57.031024 140503663007488 logging_writer.py:48] [70] global_step=70, grad_norm=0.285975, loss=8.762579
I0607 12:20:57.034293 140561621198656 submission.py:120] 70) loss = 8.763, grad_norm = 0.286
I0607 12:20:57.483893 140503654614784 logging_writer.py:48] [71] global_step=71, grad_norm=0.300152, loss=8.733888
I0607 12:20:57.487804 140561621198656 submission.py:120] 71) loss = 8.734, grad_norm = 0.300
I0607 12:20:57.945062 140503663007488 logging_writer.py:48] [72] global_step=72, grad_norm=0.279594, loss=8.745505
I0607 12:20:57.948723 140561621198656 submission.py:120] 72) loss = 8.746, grad_norm = 0.280
I0607 12:20:58.404700 140503654614784 logging_writer.py:48] [73] global_step=73, grad_norm=0.279508, loss=8.730676
I0607 12:20:58.408391 140561621198656 submission.py:120] 73) loss = 8.731, grad_norm = 0.280
I0607 12:20:58.855689 140503663007488 logging_writer.py:48] [74] global_step=74, grad_norm=0.281867, loss=8.701746
I0607 12:20:58.859389 140561621198656 submission.py:120] 74) loss = 8.702, grad_norm = 0.282
I0607 12:20:59.309780 140503654614784 logging_writer.py:48] [75] global_step=75, grad_norm=0.268308, loss=8.740105
I0607 12:20:59.313510 140561621198656 submission.py:120] 75) loss = 8.740, grad_norm = 0.268
I0607 12:20:59.757062 140503663007488 logging_writer.py:48] [76] global_step=76, grad_norm=0.282313, loss=8.733057
I0607 12:20:59.760521 140561621198656 submission.py:120] 76) loss = 8.733, grad_norm = 0.282
I0607 12:21:00.205288 140503654614784 logging_writer.py:48] [77] global_step=77, grad_norm=0.258777, loss=8.680798
I0607 12:21:00.208349 140561621198656 submission.py:120] 77) loss = 8.681, grad_norm = 0.259
I0607 12:21:00.657387 140503663007488 logging_writer.py:48] [78] global_step=78, grad_norm=0.272168, loss=8.693146
I0607 12:21:00.661352 140561621198656 submission.py:120] 78) loss = 8.693, grad_norm = 0.272
I0607 12:21:01.106194 140503654614784 logging_writer.py:48] [79] global_step=79, grad_norm=0.267949, loss=8.699587
I0607 12:21:01.109805 140561621198656 submission.py:120] 79) loss = 8.700, grad_norm = 0.268
I0607 12:21:01.580485 140503663007488 logging_writer.py:48] [80] global_step=80, grad_norm=0.259103, loss=8.667814
I0607 12:21:01.585015 140561621198656 submission.py:120] 80) loss = 8.668, grad_norm = 0.259
I0607 12:21:02.040491 140503654614784 logging_writer.py:48] [81] global_step=81, grad_norm=0.256864, loss=8.674027
I0607 12:21:02.044305 140561621198656 submission.py:120] 81) loss = 8.674, grad_norm = 0.257
I0607 12:21:02.490799 140503663007488 logging_writer.py:48] [82] global_step=82, grad_norm=0.261302, loss=8.656802
I0607 12:21:02.493956 140561621198656 submission.py:120] 82) loss = 8.657, grad_norm = 0.261
I0607 12:21:02.938060 140503654614784 logging_writer.py:48] [83] global_step=83, grad_norm=0.239659, loss=8.643579
I0607 12:21:02.941362 140561621198656 submission.py:120] 83) loss = 8.644, grad_norm = 0.240
I0607 12:21:03.386719 140503663007488 logging_writer.py:48] [84] global_step=84, grad_norm=0.282419, loss=8.708556
I0607 12:21:03.389983 140561621198656 submission.py:120] 84) loss = 8.709, grad_norm = 0.282
I0607 12:21:03.836415 140503654614784 logging_writer.py:48] [85] global_step=85, grad_norm=0.236112, loss=8.641598
I0607 12:21:03.840244 140561621198656 submission.py:120] 85) loss = 8.642, grad_norm = 0.236
I0607 12:21:04.285191 140503663007488 logging_writer.py:48] [86] global_step=86, grad_norm=0.245003, loss=8.643363
I0607 12:21:04.288425 140561621198656 submission.py:120] 86) loss = 8.643, grad_norm = 0.245
I0607 12:21:04.735709 140503654614784 logging_writer.py:48] [87] global_step=87, grad_norm=0.255380, loss=8.615686
I0607 12:21:04.738830 140561621198656 submission.py:120] 87) loss = 8.616, grad_norm = 0.255
I0607 12:21:05.185090 140503663007488 logging_writer.py:48] [88] global_step=88, grad_norm=0.254421, loss=8.627332
I0607 12:21:05.188367 140561621198656 submission.py:120] 88) loss = 8.627, grad_norm = 0.254
I0607 12:21:05.633295 140503654614784 logging_writer.py:48] [89] global_step=89, grad_norm=0.231783, loss=8.595630
I0607 12:21:05.636662 140561621198656 submission.py:120] 89) loss = 8.596, grad_norm = 0.232
I0607 12:21:06.081730 140503663007488 logging_writer.py:48] [90] global_step=90, grad_norm=0.229498, loss=8.628972
I0607 12:21:06.084911 140561621198656 submission.py:120] 90) loss = 8.629, grad_norm = 0.229
I0607 12:21:06.530661 140503654614784 logging_writer.py:48] [91] global_step=91, grad_norm=0.228537, loss=8.648744
I0607 12:21:06.533794 140561621198656 submission.py:120] 91) loss = 8.649, grad_norm = 0.229
I0607 12:21:06.976703 140503663007488 logging_writer.py:48] [92] global_step=92, grad_norm=0.256051, loss=8.591284
I0607 12:21:06.980250 140561621198656 submission.py:120] 92) loss = 8.591, grad_norm = 0.256
I0607 12:21:07.423274 140503654614784 logging_writer.py:48] [93] global_step=93, grad_norm=0.213711, loss=8.602744
I0607 12:21:07.426444 140561621198656 submission.py:120] 93) loss = 8.603, grad_norm = 0.214
I0607 12:21:07.874612 140503663007488 logging_writer.py:48] [94] global_step=94, grad_norm=0.215175, loss=8.569762
I0607 12:21:07.877661 140561621198656 submission.py:120] 94) loss = 8.570, grad_norm = 0.215
I0607 12:21:08.322224 140503654614784 logging_writer.py:48] [95] global_step=95, grad_norm=0.250123, loss=8.615094
I0607 12:21:08.325493 140561621198656 submission.py:120] 95) loss = 8.615, grad_norm = 0.250
I0607 12:21:08.771978 140503663007488 logging_writer.py:48] [96] global_step=96, grad_norm=0.225454, loss=8.587255
I0607 12:21:08.775957 140561621198656 submission.py:120] 96) loss = 8.587, grad_norm = 0.225
I0607 12:21:09.226572 140503654614784 logging_writer.py:48] [97] global_step=97, grad_norm=0.267797, loss=8.591465
I0607 12:21:09.230432 140561621198656 submission.py:120] 97) loss = 8.591, grad_norm = 0.268
I0607 12:21:09.687067 140503663007488 logging_writer.py:48] [98] global_step=98, grad_norm=0.240292, loss=8.585111
I0607 12:21:09.690785 140561621198656 submission.py:120] 98) loss = 8.585, grad_norm = 0.240
I0607 12:21:10.135992 140503654614784 logging_writer.py:48] [99] global_step=99, grad_norm=0.280305, loss=8.574660
I0607 12:21:10.139623 140561621198656 submission.py:120] 99) loss = 8.575, grad_norm = 0.280
I0607 12:21:10.588752 140503663007488 logging_writer.py:48] [100] global_step=100, grad_norm=0.231469, loss=8.555340
I0607 12:21:10.592638 140561621198656 submission.py:120] 100) loss = 8.555, grad_norm = 0.231
I0607 12:24:06.087251 140503654614784 logging_writer.py:48] [500] global_step=500, grad_norm=0.898937, loss=6.955727
I0607 12:24:06.090799 140561621198656 submission.py:120] 500) loss = 6.956, grad_norm = 0.899
I0607 12:27:45.426189 140503663007488 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.774396, loss=5.768102
I0607 12:27:45.429923 140561621198656 submission.py:120] 1000) loss = 5.768, grad_norm = 0.774
I0607 12:31:24.348981 140503654614784 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.405633, loss=6.645352
I0607 12:31:24.352766 140561621198656 submission.py:120] 1500) loss = 6.645, grad_norm = 0.406
I0607 12:34:25.674086 140561621198656 spec.py:298] Evaluating on the training split.
I0607 12:34:29.548095 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 12:39:01.554935 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 12:39:05.282860 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 12:43:31.587887 140561621198656 spec.py:326] Evaluating on the test split.
I0607 12:43:35.387824 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 12:48:07.125847 140561621198656 submission_runner.py:419] Time since start: 2496.52s, 	Step: 1915, 	{'train/accuracy': 0.2417568559112171, 'train/loss': 5.301772591259287, 'train/bleu': 0.6629717710167636, 'validation/accuracy': 0.21666191367744975, 'validation/loss': 5.562506974495046, 'validation/bleu': 0.1469157551224518, 'validation/num_examples': 3000, 'test/accuracy': 0.20431119632792982, 'test/loss': 5.812350023240951, 'test/bleu': 0.1183117424881427, 'test/num_examples': 3003, 'score': 844.0216586589813, 'total_duration': 2496.5206553936005, 'accumulated_submission_time': 844.0216586589813, 'accumulated_eval_time': 1651.7944433689117, 'accumulated_logging_time': 0.027712106704711914}
I0607 12:48:07.136544 140503663007488 logging_writer.py:48] [1915] accumulated_eval_time=1651.794443, accumulated_logging_time=0.027712, accumulated_submission_time=844.021659, global_step=1915, preemption_count=0, score=844.021659, test/accuracy=0.204311, test/bleu=0.118312, test/loss=5.812350, test/num_examples=3003, total_duration=2496.520655, train/accuracy=0.241757, train/bleu=0.662972, train/loss=5.301773, validation/accuracy=0.216662, validation/bleu=0.146916, validation/loss=5.562507, validation/num_examples=3000
I0607 12:48:44.834797 140503654614784 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.530702, loss=5.983200
I0607 12:48:44.838157 140561621198656 submission.py:120] 2000) loss = 5.983, grad_norm = 0.531
I0607 12:52:23.992966 140503663007488 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.426164, loss=4.980941
I0607 12:52:23.998512 140561621198656 submission.py:120] 2500) loss = 4.981, grad_norm = 0.426
I0607 12:56:03.250629 140503654614784 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.452792, loss=4.341915
I0607 12:56:03.254758 140561621198656 submission.py:120] 3000) loss = 4.342, grad_norm = 0.453
I0607 12:59:42.804408 140503663007488 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.347528, loss=4.018265
I0607 12:59:42.808332 140561621198656 submission.py:120] 3500) loss = 4.018, grad_norm = 0.348
I0607 13:02:07.561283 140561621198656 spec.py:298] Evaluating on the training split.
I0607 13:02:11.426961 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:05:05.293599 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 13:05:09.009324 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:07:45.136443 140561621198656 spec.py:326] Evaluating on the test split.
I0607 13:07:48.955781 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:10:10.362629 140561621198656 submission_runner.py:419] Time since start: 3819.76s, 	Step: 3831, 	{'train/accuracy': 0.5220287296364657, 'train/loss': 2.86148820032788, 'train/bleu': 22.53136627953202, 'validation/accuracy': 0.52254776754163, 'validation/loss': 2.866422920980521, 'validation/bleu': 18.62650785760944, 'validation/num_examples': 3000, 'test/accuracy': 0.517808378362675, 'test/loss': 2.9405841467666027, 'test/bleu': 16.80986994417857, 'test/num_examples': 3003, 'score': 1683.761564731598, 'total_duration': 3819.757385492325, 'accumulated_submission_time': 1683.761564731598, 'accumulated_eval_time': 2134.595678806305, 'accumulated_logging_time': 0.04730701446533203}
I0607 13:10:10.373111 140503654614784 logging_writer.py:48] [3831] accumulated_eval_time=2134.595679, accumulated_logging_time=0.047307, accumulated_submission_time=1683.761565, global_step=3831, preemption_count=0, score=1683.761565, test/accuracy=0.517808, test/bleu=16.809870, test/loss=2.940584, test/num_examples=3003, total_duration=3819.757385, train/accuracy=0.522029, train/bleu=22.531366, train/loss=2.861488, validation/accuracy=0.522548, validation/bleu=18.626508, validation/loss=2.866423, validation/num_examples=3000
I0607 13:11:24.971776 140503663007488 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.386965, loss=3.980866
I0607 13:11:24.975599 140561621198656 submission.py:120] 4000) loss = 3.981, grad_norm = 0.387
I0607 13:15:04.261683 140503654614784 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.327012, loss=3.709066
I0607 13:15:04.265768 140561621198656 submission.py:120] 4500) loss = 3.709, grad_norm = 0.327
I0607 13:18:43.409856 140503663007488 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.256518, loss=3.603800
I0607 13:18:43.413518 140561621198656 submission.py:120] 5000) loss = 3.604, grad_norm = 0.257
I0607 13:22:22.691764 140503654614784 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.213942, loss=3.593862
I0607 13:22:22.696103 140561621198656 submission.py:120] 5500) loss = 3.594, grad_norm = 0.214
I0607 13:24:10.619835 140561621198656 spec.py:298] Evaluating on the training split.
I0607 13:24:14.499960 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:26:57.769412 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 13:27:01.496116 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:29:48.305175 140561621198656 spec.py:326] Evaluating on the test split.
I0607 13:29:52.114278 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:32:27.684621 140561621198656 submission_runner.py:419] Time since start: 5157.08s, 	Step: 5747, 	{'train/accuracy': 0.571314084216793, 'train/loss': 2.421344781100451, 'train/bleu': 26.412074911849324, 'validation/accuracy': 0.5780213512541692, 'validation/loss': 2.351900278669824, 'validation/bleu': 22.606445330584112, 'validation/num_examples': 3000, 'test/accuracy': 0.5792690721050491, 'test/loss': 2.357288216838069, 'test/bleu': 21.133634933919062, 'test/num_examples': 3003, 'score': 2523.3306078910828, 'total_duration': 5157.079364776611, 'accumulated_submission_time': 2523.3306078910828, 'accumulated_eval_time': 2631.660338163376, 'accumulated_logging_time': 0.06803607940673828}
I0607 13:32:27.695484 140503663007488 logging_writer.py:48] [5747] accumulated_eval_time=2631.660338, accumulated_logging_time=0.068036, accumulated_submission_time=2523.330608, global_step=5747, preemption_count=0, score=2523.330608, test/accuracy=0.579269, test/bleu=21.133635, test/loss=2.357288, test/num_examples=3003, total_duration=5157.079365, train/accuracy=0.571314, train/bleu=26.412075, train/loss=2.421345, validation/accuracy=0.578021, validation/bleu=22.606445, validation/loss=2.351900, validation/num_examples=3000
I0607 13:34:19.069609 140503654614784 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.214904, loss=3.451845
I0607 13:34:19.073747 140561621198656 submission.py:120] 6000) loss = 3.452, grad_norm = 0.215
I0607 13:37:58.160008 140503663007488 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.186436, loss=3.409746
I0607 13:37:58.163612 140561621198656 submission.py:120] 6500) loss = 3.410, grad_norm = 0.186
I0607 13:41:37.258856 140503654614784 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.152696, loss=3.355818
I0607 13:41:37.262385 140561621198656 submission.py:120] 7000) loss = 3.356, grad_norm = 0.153
I0607 13:45:16.125705 140503663007488 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.182030, loss=3.270528
I0607 13:45:16.129246 140561621198656 submission.py:120] 7500) loss = 3.271, grad_norm = 0.182
I0607 13:46:27.978322 140561621198656 spec.py:298] Evaluating on the training split.
I0607 13:46:31.856060 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:48:53.553575 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 13:48:57.304712 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:51:08.871993 140561621198656 spec.py:326] Evaluating on the test split.
I0607 13:51:12.684403 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 13:53:18.451613 140561621198656 submission_runner.py:419] Time since start: 6407.85s, 	Step: 7665, 	{'train/accuracy': 0.5981907875998314, 'train/loss': 2.19889409328609, 'train/bleu': 28.586829970144343, 'validation/accuracy': 0.6070228515455481, 'validation/loss': 2.1126375137940014, 'validation/bleu': 24.726943761585797, 'validation/num_examples': 3000, 'test/accuracy': 0.6109581081866249, 'test/loss': 2.0957815859043634, 'test/bleu': 23.22789494533924, 'test/num_examples': 3003, 'score': 3362.938707113266, 'total_duration': 6407.846405744553, 'accumulated_submission_time': 3362.938707113266, 'accumulated_eval_time': 3042.133537054062, 'accumulated_logging_time': 0.08791947364807129}
I0607 13:53:18.463007 140503654614784 logging_writer.py:48] [7665] accumulated_eval_time=3042.133537, accumulated_logging_time=0.087919, accumulated_submission_time=3362.938707, global_step=7665, preemption_count=0, score=3362.938707, test/accuracy=0.610958, test/bleu=23.227895, test/loss=2.095782, test/num_examples=3003, total_duration=6407.846406, train/accuracy=0.598191, train/bleu=28.586830, train/loss=2.198894, validation/accuracy=0.607023, validation/bleu=24.726944, validation/loss=2.112638, validation/num_examples=3000
I0607 13:55:45.558628 140503663007488 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.175499, loss=3.389295
I0607 13:55:45.563059 140561621198656 submission.py:120] 8000) loss = 3.389, grad_norm = 0.175
I0607 13:59:24.839768 140503654614784 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.137478, loss=3.217919
I0607 13:59:24.843331 140561621198656 submission.py:120] 8500) loss = 3.218, grad_norm = 0.137
I0607 14:03:03.810791 140503663007488 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.146704, loss=3.226573
I0607 14:03:03.814820 140561621198656 submission.py:120] 9000) loss = 3.227, grad_norm = 0.147
I0607 14:06:43.013556 140503654614784 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.159986, loss=3.154466
I0607 14:06:43.018182 140561621198656 submission.py:120] 9500) loss = 3.154, grad_norm = 0.160
I0607 14:07:18.558991 140561621198656 spec.py:298] Evaluating on the training split.
I0607 14:07:22.447978 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:09:54.480373 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 14:09:58.194421 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:12:05.140071 140561621198656 spec.py:326] Evaluating on the test split.
I0607 14:12:08.934888 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:14:09.263977 140561621198656 submission_runner.py:419] Time since start: 7658.66s, 	Step: 9582, 	{'train/accuracy': 0.6096842925219942, 'train/loss': 2.084381729976173, 'train/bleu': 28.88054123043119, 'validation/accuracy': 0.6240592181126087, 'validation/loss': 1.9804840067079144, 'validation/bleu': 25.826328956003128, 'validation/num_examples': 3000, 'test/accuracy': 0.6289814653419324, 'test/loss': 1.9439008846086805, 'test/bleu': 24.57179525126144, 'test/num_examples': 3003, 'score': 4202.363080501556, 'total_duration': 7658.658774852753, 'accumulated_submission_time': 4202.363080501556, 'accumulated_eval_time': 3452.8384211063385, 'accumulated_logging_time': 0.10976028442382812}
I0607 14:14:09.275681 140503663007488 logging_writer.py:48] [9582] accumulated_eval_time=3452.838421, accumulated_logging_time=0.109760, accumulated_submission_time=4202.363081, global_step=9582, preemption_count=0, score=4202.363081, test/accuracy=0.628981, test/bleu=24.571795, test/loss=1.943901, test/num_examples=3003, total_duration=7658.658775, train/accuracy=0.609684, train/bleu=28.880541, train/loss=2.084382, validation/accuracy=0.624059, validation/bleu=25.826329, validation/loss=1.980484, validation/num_examples=3000
I0607 14:17:12.629684 140503654614784 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.140962, loss=3.203665
I0607 14:17:12.634304 140561621198656 submission.py:120] 10000) loss = 3.204, grad_norm = 0.141
I0607 14:20:51.572630 140503663007488 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.163832, loss=3.212304
I0607 14:20:51.577080 140561621198656 submission.py:120] 10500) loss = 3.212, grad_norm = 0.164
I0607 14:24:30.834169 140503654614784 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.132906, loss=3.174307
I0607 14:24:30.838265 140561621198656 submission.py:120] 11000) loss = 3.174, grad_norm = 0.133
I0607 14:28:09.363170 140561621198656 spec.py:298] Evaluating on the training split.
I0607 14:28:13.238138 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:30:35.619581 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 14:30:39.337790 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:32:52.792713 140561621198656 spec.py:326] Evaluating on the test split.
I0607 14:32:56.583944 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:34:54.653815 140561621198656 submission_runner.py:419] Time since start: 8904.05s, 	Step: 11500, 	{'train/accuracy': 0.6162588014036375, 'train/loss': 2.02686882898328, 'train/bleu': 29.253734736042293, 'validation/accuracy': 0.6334825358644034, 'validation/loss': 1.8982180940099938, 'validation/bleu': 26.138923634903065, 'validation/num_examples': 3000, 'test/accuracy': 0.6391145197838591, 'test/loss': 1.8602719554354774, 'test/bleu': 24.833097606322177, 'test/num_examples': 3003, 'score': 5041.781784296036, 'total_duration': 8904.048609495163, 'accumulated_submission_time': 5041.781784296036, 'accumulated_eval_time': 3858.1289937496185, 'accumulated_logging_time': 0.13077878952026367}
I0607 14:34:54.664446 140503663007488 logging_writer.py:48] [11500] accumulated_eval_time=3858.128994, accumulated_logging_time=0.130779, accumulated_submission_time=5041.781784, global_step=11500, preemption_count=0, score=5041.781784, test/accuracy=0.639115, test/bleu=24.833098, test/loss=1.860272, test/num_examples=3003, total_duration=8904.048609, train/accuracy=0.616259, train/bleu=29.253735, train/loss=2.026869, validation/accuracy=0.633483, validation/bleu=26.138924, validation/loss=1.898218, validation/num_examples=3000
I0607 14:34:55.115383 140503654614784 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.158550, loss=3.202446
I0607 14:34:55.118590 140561621198656 submission.py:120] 11500) loss = 3.202, grad_norm = 0.159
I0607 14:38:33.854694 140503663007488 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.139059, loss=3.129714
I0607 14:38:33.858300 140561621198656 submission.py:120] 12000) loss = 3.130, grad_norm = 0.139
I0607 14:42:12.981569 140503654614784 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.179388, loss=3.115622
I0607 14:42:12.985612 140561621198656 submission.py:120] 12500) loss = 3.116, grad_norm = 0.179
I0607 14:45:51.947990 140503663007488 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.148859, loss=3.050245
I0607 14:45:51.952310 140561621198656 submission.py:120] 13000) loss = 3.050, grad_norm = 0.149
I0607 14:48:54.942578 140561621198656 spec.py:298] Evaluating on the training split.
I0607 14:48:58.815207 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:51:25.284538 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 14:51:28.999672 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:53:37.026779 140561621198656 spec.py:326] Evaluating on the test split.
I0607 14:53:40.815634 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 14:55:40.573819 140561621198656 submission_runner.py:419] Time since start: 10149.97s, 	Step: 13419, 	{'train/accuracy': 0.6302520049739325, 'train/loss': 1.9163187195546276, 'train/bleu': 30.042672265713534, 'validation/accuracy': 0.6423850913193885, 'validation/loss': 1.836201077791968, 'validation/bleu': 27.052647319881935, 'validation/num_examples': 3000, 'test/accuracy': 0.6495380861077218, 'test/loss': 1.792840334669688, 'test/bleu': 25.9850878890134, 'test/num_examples': 3003, 'score': 5881.405564546585, 'total_duration': 10149.968618154526, 'accumulated_submission_time': 5881.405564546585, 'accumulated_eval_time': 4263.760216474533, 'accumulated_logging_time': 0.15152645111083984}
I0607 14:55:40.584401 140503654614784 logging_writer.py:48] [13419] accumulated_eval_time=4263.760216, accumulated_logging_time=0.151526, accumulated_submission_time=5881.405565, global_step=13419, preemption_count=0, score=5881.405565, test/accuracy=0.649538, test/bleu=25.985088, test/loss=1.792840, test/num_examples=3003, total_duration=10149.968618, train/accuracy=0.630252, train/bleu=30.042672, train/loss=1.916319, validation/accuracy=0.642385, validation/bleu=27.052647, validation/loss=1.836201, validation/num_examples=3000
I0607 14:56:16.427659 140503663007488 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.136877, loss=3.047455
I0607 14:56:16.430989 140561621198656 submission.py:120] 13500) loss = 3.047, grad_norm = 0.137
I0607 14:59:55.468910 140503654614784 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.134550, loss=3.121523
I0607 14:59:55.473162 140561621198656 submission.py:120] 14000) loss = 3.122, grad_norm = 0.135
I0607 15:03:34.160126 140503663007488 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.152707, loss=3.039050
I0607 15:03:34.163528 140561621198656 submission.py:120] 14500) loss = 3.039, grad_norm = 0.153
I0607 15:07:13.070760 140503654614784 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.265082, loss=3.016597
I0607 15:07:13.074304 140561621198656 submission.py:120] 15000) loss = 3.017, grad_norm = 0.265
I0607 15:09:40.876628 140561621198656 spec.py:298] Evaluating on the training split.
I0607 15:09:44.743300 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:11:52.964593 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 15:11:56.695427 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:14:03.418355 140561621198656 spec.py:326] Evaluating on the test split.
I0607 15:14:07.216153 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:16:00.761021 140561621198656 submission_runner.py:419] Time since start: 11370.16s, 	Step: 15339, 	{'train/accuracy': 0.6319449217586547, 'train/loss': 1.9071249169473274, 'train/bleu': 30.984707600447813, 'validation/accuracy': 0.6491302029733047, 'validation/loss': 1.7864866833641244, 'validation/bleu': 27.451076374555814, 'validation/num_examples': 3000, 'test/accuracy': 0.6564057869966882, 'test/loss': 1.7325035587705537, 'test/bleu': 26.437617202555415, 'test/num_examples': 3003, 'score': 6721.052958250046, 'total_duration': 11370.155807733536, 'accumulated_submission_time': 6721.052958250046, 'accumulated_eval_time': 4643.644496202469, 'accumulated_logging_time': 0.17097020149230957}
I0607 15:16:00.772803 140503663007488 logging_writer.py:48] [15339] accumulated_eval_time=4643.644496, accumulated_logging_time=0.170970, accumulated_submission_time=6721.052958, global_step=15339, preemption_count=0, score=6721.052958, test/accuracy=0.656406, test/bleu=26.437617, test/loss=1.732504, test/num_examples=3003, total_duration=11370.155808, train/accuracy=0.631945, train/bleu=30.984708, train/loss=1.907125, validation/accuracy=0.649130, validation/bleu=27.451076, validation/loss=1.786487, validation/num_examples=3000
I0607 15:17:11.626389 140503654614784 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.192684, loss=3.042660
I0607 15:17:11.629688 140561621198656 submission.py:120] 15500) loss = 3.043, grad_norm = 0.193
I0607 15:20:50.352680 140503663007488 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.135768, loss=2.981990
I0607 15:20:50.356049 140561621198656 submission.py:120] 16000) loss = 2.982, grad_norm = 0.136
I0607 15:24:29.035771 140503654614784 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.233190, loss=3.048163
I0607 15:24:29.039109 140561621198656 submission.py:120] 16500) loss = 3.048, grad_norm = 0.233
I0607 15:28:07.663209 140503663007488 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.253494, loss=3.011268
I0607 15:28:07.667596 140561621198656 submission.py:120] 17000) loss = 3.011, grad_norm = 0.253
I0607 15:30:01.055460 140561621198656 spec.py:298] Evaluating on the training split.
I0607 15:30:04.931935 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:32:35.406148 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 15:32:39.133612 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:34:44.277335 140561621198656 spec.py:326] Evaluating on the test split.
I0607 15:34:48.073753 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:37:04.348656 140561621198656 submission_runner.py:419] Time since start: 12633.74s, 	Step: 17260, 	{'train/accuracy': 0.6344378304602645, 'train/loss': 1.8869933420500293, 'train/bleu': 30.987247224950245, 'validation/accuracy': 0.6557017271949511, 'validation/loss': 1.7483482070898066, 'validation/bleu': 28.197192164340997, 'validation/num_examples': 3000, 'test/accuracy': 0.6624484341409563, 'test/loss': 1.6995170239962814, 'test/bleu': 27.218521152476374, 'test/num_examples': 3003, 'score': 7560.697140693665, 'total_duration': 12633.743463039398, 'accumulated_submission_time': 7560.697140693665, 'accumulated_eval_time': 5066.937623977661, 'accumulated_logging_time': 0.19302678108215332}
I0607 15:37:04.359477 140503654614784 logging_writer.py:48] [17260] accumulated_eval_time=5066.937624, accumulated_logging_time=0.193027, accumulated_submission_time=7560.697141, global_step=17260, preemption_count=0, score=7560.697141, test/accuracy=0.662448, test/bleu=27.218521, test/loss=1.699517, test/num_examples=3003, total_duration=12633.743463, train/accuracy=0.634438, train/bleu=30.987247, train/loss=1.886993, validation/accuracy=0.655702, validation/bleu=28.197192, validation/loss=1.748348, validation/num_examples=3000
I0607 15:38:49.704158 140503663007488 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.172360, loss=2.957410
I0607 15:38:49.708542 140561621198656 submission.py:120] 17500) loss = 2.957, grad_norm = 0.172
I0607 15:42:28.492519 140503654614784 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.145633, loss=3.016431
I0607 15:42:28.496285 140561621198656 submission.py:120] 18000) loss = 3.016, grad_norm = 0.146
I0607 15:46:07.214479 140503663007488 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.162563, loss=2.991047
I0607 15:46:07.218163 140561621198656 submission.py:120] 18500) loss = 2.991, grad_norm = 0.163
I0607 15:49:45.995683 140503654614784 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.124799, loss=2.955368
I0607 15:49:45.999274 140561621198656 submission.py:120] 19000) loss = 2.955, grad_norm = 0.125
I0607 15:51:04.743473 140561621198656 spec.py:298] Evaluating on the training split.
I0607 15:51:08.601387 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:54:01.261173 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 15:54:04.978169 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:56:20.871016 140561621198656 spec.py:326] Evaluating on the test split.
I0607 15:56:24.666587 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 15:58:28.146566 140561621198656 submission_runner.py:419] Time since start: 13917.54s, 	Step: 19181, 	{'train/accuracy': 0.6598800322989964, 'train/loss': 1.7104009257123083, 'train/bleu': 32.984759999629766, 'validation/accuracy': 0.661231726822978, 'validation/loss': 1.7035634245080655, 'validation/bleu': 28.517258398172963, 'validation/num_examples': 3000, 'test/accuracy': 0.6691069664749288, 'test/loss': 1.6442444149090698, 'test/bleu': 27.542278099456265, 'test/num_examples': 3003, 'score': 8400.43985247612, 'total_duration': 13917.541372060776, 'accumulated_submission_time': 8400.43985247612, 'accumulated_eval_time': 5510.340662479401, 'accumulated_logging_time': 0.21300554275512695}
I0607 15:58:28.157441 140503663007488 logging_writer.py:48] [19181] accumulated_eval_time=5510.340662, accumulated_logging_time=0.213006, accumulated_submission_time=8400.439852, global_step=19181, preemption_count=0, score=8400.439852, test/accuracy=0.669107, test/bleu=27.542278, test/loss=1.644244, test/num_examples=3003, total_duration=13917.541372, train/accuracy=0.659880, train/bleu=32.984760, train/loss=1.710401, validation/accuracy=0.661232, validation/bleu=28.517258, validation/loss=1.703563, validation/num_examples=3000
I0607 16:00:48.143386 140503654614784 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.148423, loss=2.938671
I0607 16:00:48.147160 140561621198656 submission.py:120] 19500) loss = 2.939, grad_norm = 0.148
I0607 16:04:26.511966 140561621198656 spec.py:298] Evaluating on the training split.
I0607 16:04:30.371503 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 16:07:22.924092 140561621198656 spec.py:310] Evaluating on the validation split.
I0607 16:07:26.632987 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 16:09:34.692031 140561621198656 spec.py:326] Evaluating on the test split.
I0607 16:09:38.501394 140561621198656 workload.py:130] Translating evaluation dataset.
I0607 16:11:31.673440 140561621198656 submission_runner.py:419] Time since start: 14701.07s, 	Step: 20000, 	{'train/accuracy': 0.6499437725196796, 'train/loss': 1.7783297928189474, 'train/bleu': 31.91507162451714, 'validation/accuracy': 0.6614425115621629, 'validation/loss': 1.7019575470235955, 'validation/bleu': 28.281407205148067, 'validation/num_examples': 3000, 'test/accuracy': 0.66887455696938, 'test/loss': 1.6384804776015338, 'test/bleu': 27.665663287185215, 'test/num_examples': 3003, 'score': 8758.510364055634, 'total_duration': 14701.06825876236, 'accumulated_submission_time': 8758.510364055634, 'accumulated_eval_time': 5935.502071380615, 'accumulated_logging_time': 0.23418474197387695}
I0607 16:11:31.685036 140503663007488 logging_writer.py:48] [20000] accumulated_eval_time=5935.502071, accumulated_logging_time=0.234185, accumulated_submission_time=8758.510364, global_step=20000, preemption_count=0, score=8758.510364, test/accuracy=0.668875, test/bleu=27.665663, test/loss=1.638480, test/num_examples=3003, total_duration=14701.068259, train/accuracy=0.649944, train/bleu=31.915072, train/loss=1.778330, validation/accuracy=0.661443, validation/bleu=28.281407, validation/loss=1.701958, validation/num_examples=3000
I0607 16:11:31.702982 140503654614784 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=8758.510364
I0607 16:11:34.035423 140561621198656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/adamw/wmt_pytorch/trial_1/checkpoint_20000.
I0607 16:11:34.057533 140561621198656 submission_runner.py:581] Tuning trial 1/1
I0607 16:11:34.057710 140561621198656 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 16:11:34.058584 140561621198656 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005973441161604558, 'train/loss': 11.055712390295456, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.065812110203222, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.057252629132531, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.454050779342651, 'total_duration': 834.7971563339233, 'accumulated_submission_time': 4.454050779342651, 'accumulated_eval_time': 830.3427360057831, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1915, {'train/accuracy': 0.2417568559112171, 'train/loss': 5.301772591259287, 'train/bleu': 0.6629717710167636, 'validation/accuracy': 0.21666191367744975, 'validation/loss': 5.562506974495046, 'validation/bleu': 0.1469157551224518, 'validation/num_examples': 3000, 'test/accuracy': 0.20431119632792982, 'test/loss': 5.812350023240951, 'test/bleu': 0.1183117424881427, 'test/num_examples': 3003, 'score': 844.0216586589813, 'total_duration': 2496.5206553936005, 'accumulated_submission_time': 844.0216586589813, 'accumulated_eval_time': 1651.7944433689117, 'accumulated_logging_time': 0.027712106704711914, 'global_step': 1915, 'preemption_count': 0}), (3831, {'train/accuracy': 0.5220287296364657, 'train/loss': 2.86148820032788, 'train/bleu': 22.53136627953202, 'validation/accuracy': 0.52254776754163, 'validation/loss': 2.866422920980521, 'validation/bleu': 18.62650785760944, 'validation/num_examples': 3000, 'test/accuracy': 0.517808378362675, 'test/loss': 2.9405841467666027, 'test/bleu': 16.80986994417857, 'test/num_examples': 3003, 'score': 1683.761564731598, 'total_duration': 3819.757385492325, 'accumulated_submission_time': 1683.761564731598, 'accumulated_eval_time': 2134.595678806305, 'accumulated_logging_time': 0.04730701446533203, 'global_step': 3831, 'preemption_count': 0}), (5747, {'train/accuracy': 0.571314084216793, 'train/loss': 2.421344781100451, 'train/bleu': 26.412074911849324, 'validation/accuracy': 0.5780213512541692, 'validation/loss': 2.351900278669824, 'validation/bleu': 22.606445330584112, 'validation/num_examples': 3000, 'test/accuracy': 0.5792690721050491, 'test/loss': 2.357288216838069, 'test/bleu': 21.133634933919062, 'test/num_examples': 3003, 'score': 2523.3306078910828, 'total_duration': 5157.079364776611, 'accumulated_submission_time': 2523.3306078910828, 'accumulated_eval_time': 2631.660338163376, 'accumulated_logging_time': 0.06803607940673828, 'global_step': 5747, 'preemption_count': 0}), (7665, {'train/accuracy': 0.5981907875998314, 'train/loss': 2.19889409328609, 'train/bleu': 28.586829970144343, 'validation/accuracy': 0.6070228515455481, 'validation/loss': 2.1126375137940014, 'validation/bleu': 24.726943761585797, 'validation/num_examples': 3000, 'test/accuracy': 0.6109581081866249, 'test/loss': 2.0957815859043634, 'test/bleu': 23.22789494533924, 'test/num_examples': 3003, 'score': 3362.938707113266, 'total_duration': 6407.846405744553, 'accumulated_submission_time': 3362.938707113266, 'accumulated_eval_time': 3042.133537054062, 'accumulated_logging_time': 0.08791947364807129, 'global_step': 7665, 'preemption_count': 0}), (9582, {'train/accuracy': 0.6096842925219942, 'train/loss': 2.084381729976173, 'train/bleu': 28.88054123043119, 'validation/accuracy': 0.6240592181126087, 'validation/loss': 1.9804840067079144, 'validation/bleu': 25.826328956003128, 'validation/num_examples': 3000, 'test/accuracy': 0.6289814653419324, 'test/loss': 1.9439008846086805, 'test/bleu': 24.57179525126144, 'test/num_examples': 3003, 'score': 4202.363080501556, 'total_duration': 7658.658774852753, 'accumulated_submission_time': 4202.363080501556, 'accumulated_eval_time': 3452.8384211063385, 'accumulated_logging_time': 0.10976028442382812, 'global_step': 9582, 'preemption_count': 0}), (11500, {'train/accuracy': 0.6162588014036375, 'train/loss': 2.02686882898328, 'train/bleu': 29.253734736042293, 'validation/accuracy': 0.6334825358644034, 'validation/loss': 1.8982180940099938, 'validation/bleu': 26.138923634903065, 'validation/num_examples': 3000, 'test/accuracy': 0.6391145197838591, 'test/loss': 1.8602719554354774, 'test/bleu': 24.833097606322177, 'test/num_examples': 3003, 'score': 5041.781784296036, 'total_duration': 8904.048609495163, 'accumulated_submission_time': 5041.781784296036, 'accumulated_eval_time': 3858.1289937496185, 'accumulated_logging_time': 0.13077878952026367, 'global_step': 11500, 'preemption_count': 0}), (13419, {'train/accuracy': 0.6302520049739325, 'train/loss': 1.9163187195546276, 'train/bleu': 30.042672265713534, 'validation/accuracy': 0.6423850913193885, 'validation/loss': 1.836201077791968, 'validation/bleu': 27.052647319881935, 'validation/num_examples': 3000, 'test/accuracy': 0.6495380861077218, 'test/loss': 1.792840334669688, 'test/bleu': 25.9850878890134, 'test/num_examples': 3003, 'score': 5881.405564546585, 'total_duration': 10149.968618154526, 'accumulated_submission_time': 5881.405564546585, 'accumulated_eval_time': 4263.760216474533, 'accumulated_logging_time': 0.15152645111083984, 'global_step': 13419, 'preemption_count': 0}), (15339, {'train/accuracy': 0.6319449217586547, 'train/loss': 1.9071249169473274, 'train/bleu': 30.984707600447813, 'validation/accuracy': 0.6491302029733047, 'validation/loss': 1.7864866833641244, 'validation/bleu': 27.451076374555814, 'validation/num_examples': 3000, 'test/accuracy': 0.6564057869966882, 'test/loss': 1.7325035587705537, 'test/bleu': 26.437617202555415, 'test/num_examples': 3003, 'score': 6721.052958250046, 'total_duration': 11370.155807733536, 'accumulated_submission_time': 6721.052958250046, 'accumulated_eval_time': 4643.644496202469, 'accumulated_logging_time': 0.17097020149230957, 'global_step': 15339, 'preemption_count': 0}), (17260, {'train/accuracy': 0.6344378304602645, 'train/loss': 1.8869933420500293, 'train/bleu': 30.987247224950245, 'validation/accuracy': 0.6557017271949511, 'validation/loss': 1.7483482070898066, 'validation/bleu': 28.197192164340997, 'validation/num_examples': 3000, 'test/accuracy': 0.6624484341409563, 'test/loss': 1.6995170239962814, 'test/bleu': 27.218521152476374, 'test/num_examples': 3003, 'score': 7560.697140693665, 'total_duration': 12633.743463039398, 'accumulated_submission_time': 7560.697140693665, 'accumulated_eval_time': 5066.937623977661, 'accumulated_logging_time': 0.19302678108215332, 'global_step': 17260, 'preemption_count': 0}), (19181, {'train/accuracy': 0.6598800322989964, 'train/loss': 1.7104009257123083, 'train/bleu': 32.984759999629766, 'validation/accuracy': 0.661231726822978, 'validation/loss': 1.7035634245080655, 'validation/bleu': 28.517258398172963, 'validation/num_examples': 3000, 'test/accuracy': 0.6691069664749288, 'test/loss': 1.6442444149090698, 'test/bleu': 27.542278099456265, 'test/num_examples': 3003, 'score': 8400.43985247612, 'total_duration': 13917.541372060776, 'accumulated_submission_time': 8400.43985247612, 'accumulated_eval_time': 5510.340662479401, 'accumulated_logging_time': 0.21300554275512695, 'global_step': 19181, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6499437725196796, 'train/loss': 1.7783297928189474, 'train/bleu': 31.91507162451714, 'validation/accuracy': 0.6614425115621629, 'validation/loss': 1.7019575470235955, 'validation/bleu': 28.281407205148067, 'validation/num_examples': 3000, 'test/accuracy': 0.66887455696938, 'test/loss': 1.6384804776015338, 'test/bleu': 27.665663287185215, 'test/num_examples': 3003, 'score': 8758.510364055634, 'total_duration': 14701.06825876236, 'accumulated_submission_time': 8758.510364055634, 'accumulated_eval_time': 5935.502071380615, 'accumulated_logging_time': 0.23418474197387695, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0607 16:11:34.058708 140561621198656 submission_runner.py:584] Timing: 8758.510364055634
I0607 16:11:34.058788 140561621198656 submission_runner.py:586] Total number of evals: 12
I0607 16:11:34.058864 140561621198656 submission_runner.py:587] ====================
I0607 16:11:34.058978 140561621198656 submission_runner.py:655] Final wmt score: 8758.510364055634
