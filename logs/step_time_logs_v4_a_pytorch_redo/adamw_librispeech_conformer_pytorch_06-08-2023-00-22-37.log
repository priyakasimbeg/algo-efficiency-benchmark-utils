torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_06-08-2023-00-22-37.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 00:23:01.078452 140429038036800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 00:23:01.078471 139960420198208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 00:23:01.078486 139892763408192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 00:23:01.078515 140446466635584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 00:23:01.079010 140694347081536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 00:23:01.079450 140308546836288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 00:23:01.080016 139797578352448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 00:23:01.080130 140421284235072 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 00:23:01.080356 139797578352448 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.080480 140421284235072 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.089151 140429038036800 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.089197 139960420198208 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.089228 139892763408192 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.089268 140446466635584 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.089670 140694347081536 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.090049 140308546836288 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 00:23:01.455067 140308546836288 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch because --overwrite was set.
I0608 00:23:01.479715 140308546836288 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch.
W0608 00:23:01.482774 139960420198208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.483437 139892763408192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.483490 140694347081536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.483922 140429038036800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.485603 140446466635584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.485685 139797578352448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.486585 140421284235072 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 00:23:01.510554 140308546836288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 00:23:01.515343 140308546836288 submission_runner.py:541] Using RNG seed 3359524372
I0608 00:23:01.516996 140308546836288 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 00:23:01.517117 140308546836288 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch/trial_1.
I0608 00:23:01.517391 140308546836288 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch/trial_1/hparams.json.
I0608 00:23:01.518340 140308546836288 submission_runner.py:255] Initializing dataset.
I0608 00:23:01.518465 140308546836288 input_pipeline.py:20] Loading split = train-clean-100
I0608 00:23:01.775280 140308546836288 input_pipeline.py:20] Loading split = train-clean-360
I0608 00:23:02.170864 140308546836288 input_pipeline.py:20] Loading split = train-other-500
I0608 00:23:02.672296 140308546836288 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0608 00:23:09.389497 140308546836288 submission_runner.py:272] Initializing optimizer.
I0608 00:23:09.390887 140308546836288 submission_runner.py:279] Initializing metrics bundle.
I0608 00:23:09.391024 140308546836288 submission_runner.py:297] Initializing checkpoint and logger.
I0608 00:23:09.392188 140308546836288 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0608 00:23:09.392294 140308546836288 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0608 00:23:10.004111 140308546836288 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0608 00:23:10.005047 140308546836288 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0608 00:23:10.012330 140308546836288 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0608 00:23:18.041925 140281877882624 logging_writer.py:48] [0] global_step=0, grad_norm=123.814796, loss=31.303211
I0608 00:23:18.061370 140308546836288 submission.py:120] 0) loss = 31.303, grad_norm = 123.815
I0608 00:23:18.067565 140308546836288 spec.py:298] Evaluating on the training split.
I0608 00:23:18.068633 140308546836288 input_pipeline.py:20] Loading split = train-clean-100
I0608 00:23:18.103966 140308546836288 input_pipeline.py:20] Loading split = train-clean-360
I0608 00:23:18.536058 140308546836288 input_pipeline.py:20] Loading split = train-other-500
I0608 00:23:35.236306 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 00:23:35.237750 140308546836288 input_pipeline.py:20] Loading split = dev-clean
I0608 00:23:35.241668 140308546836288 input_pipeline.py:20] Loading split = dev-other
I0608 00:23:46.077423 140308546836288 spec.py:326] Evaluating on the test split.
I0608 00:23:46.078922 140308546836288 input_pipeline.py:20] Loading split = test-clean
I0608 00:23:51.856248 140308546836288 submission_runner.py:419] Time since start: 41.84s, 	Step: 1, 	{'train/ctc_loss': 31.10721778348339, 'train/wer': 2.0333790382457986, 'validation/ctc_loss': 30.116382233273058, 'validation/wer': 1.776072997634336, 'validation/num_examples': 5348, 'test/ctc_loss': 30.21974643525389, 'test/wer': 1.8231470761481121, 'test/num_examples': 2472, 'score': 8.055304288864136, 'total_duration': 41.8441047668457, 'accumulated_submission_time': 8.055304288864136, 'accumulated_eval_time': 33.78838229179382, 'accumulated_logging_time': 0}
I0608 00:23:51.878253 140280225322752 logging_writer.py:48] [1] accumulated_eval_time=33.788382, accumulated_logging_time=0, accumulated_submission_time=8.055304, global_step=1, preemption_count=0, score=8.055304, test/ctc_loss=30.219746, test/num_examples=2472, test/wer=1.823147, total_duration=41.844105, train/ctc_loss=31.107218, train/wer=2.033379, validation/ctc_loss=30.116382, validation/num_examples=5348, validation/wer=1.776073
I0608 00:23:51.922902 140308546836288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.923202 139960420198208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.923572 140446466635584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.923590 140421284235072 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.923666 140694347081536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.923678 140429038036800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.924064 139892763408192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:51.924150 139797578352448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 00:23:53.039159 140279302584064 logging_writer.py:48] [1] global_step=1, grad_norm=120.693214, loss=30.759367
I0608 00:23:53.043436 140308546836288 submission.py:120] 1) loss = 30.759, grad_norm = 120.693
I0608 00:23:53.931343 140280225322752 logging_writer.py:48] [2] global_step=2, grad_norm=126.144379, loss=31.202223
I0608 00:23:53.934970 140308546836288 submission.py:120] 2) loss = 31.202, grad_norm = 126.144
I0608 00:23:54.883532 140279302584064 logging_writer.py:48] [3] global_step=3, grad_norm=129.404770, loss=31.023628
I0608 00:23:54.887042 140308546836288 submission.py:120] 3) loss = 31.024, grad_norm = 129.405
I0608 00:23:55.683568 140280225322752 logging_writer.py:48] [4] global_step=4, grad_norm=135.085693, loss=30.384022
I0608 00:23:55.687716 140308546836288 submission.py:120] 4) loss = 30.384, grad_norm = 135.086
I0608 00:23:56.486189 140279302584064 logging_writer.py:48] [5] global_step=5, grad_norm=143.704941, loss=30.279995
I0608 00:23:56.489746 140308546836288 submission.py:120] 5) loss = 30.280, grad_norm = 143.705
I0608 00:23:57.289040 140280225322752 logging_writer.py:48] [6] global_step=6, grad_norm=157.737564, loss=29.948427
I0608 00:23:57.292261 140308546836288 submission.py:120] 6) loss = 29.948, grad_norm = 157.738
I0608 00:23:58.088608 140279302584064 logging_writer.py:48] [7] global_step=7, grad_norm=162.313431, loss=28.551630
I0608 00:23:58.092761 140308546836288 submission.py:120] 7) loss = 28.552, grad_norm = 162.313
I0608 00:23:58.887890 140280225322752 logging_writer.py:48] [8] global_step=8, grad_norm=171.912842, loss=28.171272
I0608 00:23:58.891940 140308546836288 submission.py:120] 8) loss = 28.171, grad_norm = 171.913
I0608 00:23:59.692806 140279302584064 logging_writer.py:48] [9] global_step=9, grad_norm=173.084274, loss=27.046043
I0608 00:23:59.696519 140308546836288 submission.py:120] 9) loss = 27.046, grad_norm = 173.084
I0608 00:24:00.495293 140280225322752 logging_writer.py:48] [10] global_step=10, grad_norm=176.169159, loss=26.002993
I0608 00:24:00.498522 140308546836288 submission.py:120] 10) loss = 26.003, grad_norm = 176.169
I0608 00:24:01.294573 140279302584064 logging_writer.py:48] [11] global_step=11, grad_norm=179.565353, loss=24.974546
I0608 00:24:01.297802 140308546836288 submission.py:120] 11) loss = 24.975, grad_norm = 179.565
I0608 00:24:02.097408 140280225322752 logging_writer.py:48] [12] global_step=12, grad_norm=180.840576, loss=23.827068
I0608 00:24:02.100987 140308546836288 submission.py:120] 12) loss = 23.827, grad_norm = 180.841
I0608 00:24:02.897305 140279302584064 logging_writer.py:48] [13] global_step=13, grad_norm=177.841125, loss=22.092381
I0608 00:24:02.900891 140308546836288 submission.py:120] 13) loss = 22.092, grad_norm = 177.841
I0608 00:24:03.700219 140280225322752 logging_writer.py:48] [14] global_step=14, grad_norm=174.264450, loss=20.593576
I0608 00:24:03.703920 140308546836288 submission.py:120] 14) loss = 20.594, grad_norm = 174.264
I0608 00:24:04.502885 140279302584064 logging_writer.py:48] [15] global_step=15, grad_norm=165.056290, loss=18.562494
I0608 00:24:04.506161 140308546836288 submission.py:120] 15) loss = 18.562, grad_norm = 165.056
I0608 00:24:05.300904 140280225322752 logging_writer.py:48] [16] global_step=16, grad_norm=163.120667, loss=17.219866
I0608 00:24:05.304110 140308546836288 submission.py:120] 16) loss = 17.220, grad_norm = 163.121
I0608 00:24:06.101761 140279302584064 logging_writer.py:48] [17] global_step=17, grad_norm=153.800903, loss=15.507166
I0608 00:24:06.105340 140308546836288 submission.py:120] 17) loss = 15.507, grad_norm = 153.801
I0608 00:24:06.903459 140280225322752 logging_writer.py:48] [18] global_step=18, grad_norm=142.163925, loss=13.819786
I0608 00:24:06.907690 140308546836288 submission.py:120] 18) loss = 13.820, grad_norm = 142.164
I0608 00:24:07.704821 140279302584064 logging_writer.py:48] [19] global_step=19, grad_norm=125.956200, loss=12.228593
I0608 00:24:07.708553 140308546836288 submission.py:120] 19) loss = 12.229, grad_norm = 125.956
I0608 00:24:08.505163 140280225322752 logging_writer.py:48] [20] global_step=20, grad_norm=106.767532, loss=10.727826
I0608 00:24:08.508332 140308546836288 submission.py:120] 20) loss = 10.728, grad_norm = 106.768
I0608 00:24:09.304816 140279302584064 logging_writer.py:48] [21] global_step=21, grad_norm=89.366043, loss=9.710717
I0608 00:24:09.308296 140308546836288 submission.py:120] 21) loss = 9.711, grad_norm = 89.366
I0608 00:24:10.105410 140280225322752 logging_writer.py:48] [22] global_step=22, grad_norm=72.395370, loss=8.899095
I0608 00:24:10.108699 140308546836288 submission.py:120] 22) loss = 8.899, grad_norm = 72.395
I0608 00:24:10.911987 140279302584064 logging_writer.py:48] [23] global_step=23, grad_norm=51.442513, loss=8.142234
I0608 00:24:10.916004 140308546836288 submission.py:120] 23) loss = 8.142, grad_norm = 51.443
I0608 00:24:11.716053 140280225322752 logging_writer.py:48] [24] global_step=24, grad_norm=36.659760, loss=7.716797
I0608 00:24:11.720228 140308546836288 submission.py:120] 24) loss = 7.717, grad_norm = 36.660
I0608 00:24:12.518890 140279302584064 logging_writer.py:48] [25] global_step=25, grad_norm=24.590715, loss=7.450567
I0608 00:24:12.522432 140308546836288 submission.py:120] 25) loss = 7.451, grad_norm = 24.591
I0608 00:24:13.319863 140280225322752 logging_writer.py:48] [26] global_step=26, grad_norm=14.365850, loss=7.294501
I0608 00:24:13.324123 140308546836288 submission.py:120] 26) loss = 7.295, grad_norm = 14.366
I0608 00:24:14.121551 140279302584064 logging_writer.py:48] [27] global_step=27, grad_norm=7.255325, loss=7.202885
I0608 00:24:14.125527 140308546836288 submission.py:120] 27) loss = 7.203, grad_norm = 7.255
I0608 00:24:14.922649 140280225322752 logging_writer.py:48] [28] global_step=28, grad_norm=3.623084, loss=7.180604
I0608 00:24:14.926244 140308546836288 submission.py:120] 28) loss = 7.181, grad_norm = 3.623
I0608 00:24:15.724406 140279302584064 logging_writer.py:48] [29] global_step=29, grad_norm=4.723695, loss=7.181612
I0608 00:24:15.727675 140308546836288 submission.py:120] 29) loss = 7.182, grad_norm = 4.724
I0608 00:24:16.528218 140280225322752 logging_writer.py:48] [30] global_step=30, grad_norm=6.906291, loss=7.176054
I0608 00:24:16.532323 140308546836288 submission.py:120] 30) loss = 7.176, grad_norm = 6.906
I0608 00:24:17.331935 140279302584064 logging_writer.py:48] [31] global_step=31, grad_norm=8.551516, loss=7.195302
I0608 00:24:17.335429 140308546836288 submission.py:120] 31) loss = 7.195, grad_norm = 8.552
I0608 00:24:18.135658 140280225322752 logging_writer.py:48] [32] global_step=32, grad_norm=9.776723, loss=7.208033
I0608 00:24:18.139098 140308546836288 submission.py:120] 32) loss = 7.208, grad_norm = 9.777
I0608 00:24:18.938250 140279302584064 logging_writer.py:48] [33] global_step=33, grad_norm=10.176151, loss=7.207910
I0608 00:24:18.941701 140308546836288 submission.py:120] 33) loss = 7.208, grad_norm = 10.176
I0608 00:24:19.740011 140280225322752 logging_writer.py:48] [34] global_step=34, grad_norm=10.411962, loss=7.199088
I0608 00:24:19.743614 140308546836288 submission.py:120] 34) loss = 7.199, grad_norm = 10.412
I0608 00:24:20.543027 140279302584064 logging_writer.py:48] [35] global_step=35, grad_norm=10.366631, loss=7.177831
I0608 00:24:20.546491 140308546836288 submission.py:120] 35) loss = 7.178, grad_norm = 10.367
I0608 00:24:21.343627 140280225322752 logging_writer.py:48] [36] global_step=36, grad_norm=9.784446, loss=7.175389
I0608 00:24:21.347013 140308546836288 submission.py:120] 36) loss = 7.175, grad_norm = 9.784
I0608 00:24:22.144893 140279302584064 logging_writer.py:48] [37] global_step=37, grad_norm=9.703899, loss=7.173049
I0608 00:24:22.148198 140308546836288 submission.py:120] 37) loss = 7.173, grad_norm = 9.704
I0608 00:24:22.945304 140280225322752 logging_writer.py:48] [38] global_step=38, grad_norm=8.253181, loss=7.136319
I0608 00:24:22.948609 140308546836288 submission.py:120] 38) loss = 7.136, grad_norm = 8.253
I0608 00:24:23.747422 140279302584064 logging_writer.py:48] [39] global_step=39, grad_norm=7.672194, loss=7.114277
I0608 00:24:23.750910 140308546836288 submission.py:120] 39) loss = 7.114, grad_norm = 7.672
I0608 00:24:24.552830 140280225322752 logging_writer.py:48] [40] global_step=40, grad_norm=6.426234, loss=7.106079
I0608 00:24:24.556482 140308546836288 submission.py:120] 40) loss = 7.106, grad_norm = 6.426
I0608 00:24:25.359277 140279302584064 logging_writer.py:48] [41] global_step=41, grad_norm=5.798059, loss=7.069407
I0608 00:24:25.362756 140308546836288 submission.py:120] 41) loss = 7.069, grad_norm = 5.798
I0608 00:24:26.163822 140280225322752 logging_writer.py:48] [42] global_step=42, grad_norm=4.778921, loss=7.053757
I0608 00:24:26.167258 140308546836288 submission.py:120] 42) loss = 7.054, grad_norm = 4.779
I0608 00:24:26.965024 140279302584064 logging_writer.py:48] [43] global_step=43, grad_norm=3.971581, loss=7.038639
I0608 00:24:26.968740 140308546836288 submission.py:120] 43) loss = 7.039, grad_norm = 3.972
I0608 00:24:27.767742 140280225322752 logging_writer.py:48] [44] global_step=44, grad_norm=3.380147, loss=7.008602
I0608 00:24:27.771208 140308546836288 submission.py:120] 44) loss = 7.009, grad_norm = 3.380
I0608 00:24:28.570863 140279302584064 logging_writer.py:48] [45] global_step=45, grad_norm=3.082539, loss=7.006601
I0608 00:24:28.574115 140308546836288 submission.py:120] 45) loss = 7.007, grad_norm = 3.083
I0608 00:24:29.389982 140280225322752 logging_writer.py:48] [46] global_step=46, grad_norm=3.068725, loss=6.986050
I0608 00:24:29.393309 140308546836288 submission.py:120] 46) loss = 6.986, grad_norm = 3.069
I0608 00:24:30.193969 140279302584064 logging_writer.py:48] [47] global_step=47, grad_norm=3.342831, loss=6.971363
I0608 00:24:30.197413 140308546836288 submission.py:120] 47) loss = 6.971, grad_norm = 3.343
I0608 00:24:30.995338 140280225322752 logging_writer.py:48] [48] global_step=48, grad_norm=4.158786, loss=6.973186
I0608 00:24:30.998846 140308546836288 submission.py:120] 48) loss = 6.973, grad_norm = 4.159
I0608 00:24:31.796464 140279302584064 logging_writer.py:48] [49] global_step=49, grad_norm=4.511534, loss=6.958822
I0608 00:24:31.799884 140308546836288 submission.py:120] 49) loss = 6.959, grad_norm = 4.512
I0608 00:24:32.601117 140280225322752 logging_writer.py:48] [50] global_step=50, grad_norm=3.916364, loss=6.953391
I0608 00:24:32.604411 140308546836288 submission.py:120] 50) loss = 6.953, grad_norm = 3.916
I0608 00:24:33.405440 140279302584064 logging_writer.py:48] [51] global_step=51, grad_norm=3.693756, loss=6.936014
I0608 00:24:33.408731 140308546836288 submission.py:120] 51) loss = 6.936, grad_norm = 3.694
I0608 00:24:34.206976 140280225322752 logging_writer.py:48] [52] global_step=52, grad_norm=3.416774, loss=6.916137
I0608 00:24:34.210356 140308546836288 submission.py:120] 52) loss = 6.916, grad_norm = 3.417
I0608 00:24:35.009504 140279302584064 logging_writer.py:48] [53] global_step=53, grad_norm=3.454079, loss=6.901086
I0608 00:24:35.012934 140308546836288 submission.py:120] 53) loss = 6.901, grad_norm = 3.454
I0608 00:24:35.808973 140280225322752 logging_writer.py:48] [54] global_step=54, grad_norm=3.875880, loss=6.897110
I0608 00:24:35.812497 140308546836288 submission.py:120] 54) loss = 6.897, grad_norm = 3.876
I0608 00:24:36.612186 140279302584064 logging_writer.py:48] [55] global_step=55, grad_norm=3.880411, loss=6.887381
I0608 00:24:36.615555 140308546836288 submission.py:120] 55) loss = 6.887, grad_norm = 3.880
I0608 00:24:37.413839 140280225322752 logging_writer.py:48] [56] global_step=56, grad_norm=3.113825, loss=6.846251
I0608 00:24:37.417174 140308546836288 submission.py:120] 56) loss = 6.846, grad_norm = 3.114
I0608 00:24:38.215524 140279302584064 logging_writer.py:48] [57] global_step=57, grad_norm=3.186555, loss=6.852205
I0608 00:24:38.218872 140308546836288 submission.py:120] 57) loss = 6.852, grad_norm = 3.187
I0608 00:24:39.019189 140280225322752 logging_writer.py:48] [58] global_step=58, grad_norm=2.948334, loss=6.827744
I0608 00:24:39.022564 140308546836288 submission.py:120] 58) loss = 6.828, grad_norm = 2.948
I0608 00:24:39.823127 140279302584064 logging_writer.py:48] [59] global_step=59, grad_norm=2.850375, loss=6.829557
I0608 00:24:39.826335 140308546836288 submission.py:120] 59) loss = 6.830, grad_norm = 2.850
I0608 00:24:40.627424 140280225322752 logging_writer.py:48] [60] global_step=60, grad_norm=2.789907, loss=6.814352
I0608 00:24:40.630887 140308546836288 submission.py:120] 60) loss = 6.814, grad_norm = 2.790
I0608 00:24:41.430488 140279302584064 logging_writer.py:48] [61] global_step=61, grad_norm=2.696157, loss=6.802176
I0608 00:24:41.433836 140308546836288 submission.py:120] 61) loss = 6.802, grad_norm = 2.696
I0608 00:24:42.233122 140280225322752 logging_writer.py:48] [62] global_step=62, grad_norm=2.743304, loss=6.787197
I0608 00:24:42.236202 140308546836288 submission.py:120] 62) loss = 6.787, grad_norm = 2.743
I0608 00:24:43.034821 140279302584064 logging_writer.py:48] [63] global_step=63, grad_norm=3.005539, loss=6.779723
I0608 00:24:43.038316 140308546836288 submission.py:120] 63) loss = 6.780, grad_norm = 3.006
I0608 00:24:43.840637 140280225322752 logging_writer.py:48] [64] global_step=64, grad_norm=2.743079, loss=6.742044
I0608 00:24:43.844250 140308546836288 submission.py:120] 64) loss = 6.742, grad_norm = 2.743
I0608 00:24:44.652111 140279302584064 logging_writer.py:48] [65] global_step=65, grad_norm=2.580278, loss=6.753531
I0608 00:24:44.656258 140308546836288 submission.py:120] 65) loss = 6.754, grad_norm = 2.580
I0608 00:24:45.455567 140280225322752 logging_writer.py:48] [66] global_step=66, grad_norm=2.598697, loss=6.719586
I0608 00:24:45.459030 140308546836288 submission.py:120] 66) loss = 6.720, grad_norm = 2.599
I0608 00:24:46.256534 140279302584064 logging_writer.py:48] [67] global_step=67, grad_norm=2.587311, loss=6.700431
I0608 00:24:46.259749 140308546836288 submission.py:120] 67) loss = 6.700, grad_norm = 2.587
I0608 00:24:47.060238 140280225322752 logging_writer.py:48] [68] global_step=68, grad_norm=2.510199, loss=6.690789
I0608 00:24:47.063565 140308546836288 submission.py:120] 68) loss = 6.691, grad_norm = 2.510
I0608 00:24:47.862916 140279302584064 logging_writer.py:48] [69] global_step=69, grad_norm=3.109895, loss=6.694640
I0608 00:24:47.866293 140308546836288 submission.py:120] 69) loss = 6.695, grad_norm = 3.110
I0608 00:24:48.666193 140280225322752 logging_writer.py:48] [70] global_step=70, grad_norm=2.864977, loss=6.664526
I0608 00:24:48.669582 140308546836288 submission.py:120] 70) loss = 6.665, grad_norm = 2.865
I0608 00:24:49.470637 140279302584064 logging_writer.py:48] [71] global_step=71, grad_norm=3.043872, loss=6.664624
I0608 00:24:49.473902 140308546836288 submission.py:120] 71) loss = 6.665, grad_norm = 3.044
I0608 00:24:50.274019 140280225322752 logging_writer.py:48] [72] global_step=72, grad_norm=2.579070, loss=6.627245
I0608 00:24:50.277363 140308546836288 submission.py:120] 72) loss = 6.627, grad_norm = 2.579
I0608 00:24:51.080436 140279302584064 logging_writer.py:48] [73] global_step=73, grad_norm=2.660147, loss=6.613022
I0608 00:24:51.083778 140308546836288 submission.py:120] 73) loss = 6.613, grad_norm = 2.660
I0608 00:24:51.886924 140280225322752 logging_writer.py:48] [74] global_step=74, grad_norm=2.376556, loss=6.621367
I0608 00:24:51.890481 140308546836288 submission.py:120] 74) loss = 6.621, grad_norm = 2.377
I0608 00:24:52.692337 140279302584064 logging_writer.py:48] [75] global_step=75, grad_norm=2.587914, loss=6.601896
I0608 00:24:52.695665 140308546836288 submission.py:120] 75) loss = 6.602, grad_norm = 2.588
I0608 00:24:53.497573 140280225322752 logging_writer.py:48] [76] global_step=76, grad_norm=2.237436, loss=6.575966
I0608 00:24:53.500749 140308546836288 submission.py:120] 76) loss = 6.576, grad_norm = 2.237
I0608 00:24:54.301242 140279302584064 logging_writer.py:48] [77] global_step=77, grad_norm=2.157191, loss=6.580112
I0608 00:24:54.304746 140308546836288 submission.py:120] 77) loss = 6.580, grad_norm = 2.157
I0608 00:24:55.108308 140280225322752 logging_writer.py:48] [78] global_step=78, grad_norm=2.053875, loss=6.555601
I0608 00:24:55.111680 140308546836288 submission.py:120] 78) loss = 6.556, grad_norm = 2.054
I0608 00:24:55.915323 140279302584064 logging_writer.py:48] [79] global_step=79, grad_norm=2.174257, loss=6.552357
I0608 00:24:55.918630 140308546836288 submission.py:120] 79) loss = 6.552, grad_norm = 2.174
I0608 00:24:56.718760 140280225322752 logging_writer.py:48] [80] global_step=80, grad_norm=2.374527, loss=6.535806
I0608 00:24:56.721856 140308546836288 submission.py:120] 80) loss = 6.536, grad_norm = 2.375
I0608 00:24:57.520843 140279302584064 logging_writer.py:48] [81] global_step=81, grad_norm=2.178762, loss=6.514068
I0608 00:24:57.524317 140308546836288 submission.py:120] 81) loss = 6.514, grad_norm = 2.179
I0608 00:24:58.324023 140280225322752 logging_writer.py:48] [82] global_step=82, grad_norm=2.192080, loss=6.508499
I0608 00:24:58.327231 140308546836288 submission.py:120] 82) loss = 6.508, grad_norm = 2.192
I0608 00:24:59.127566 140279302584064 logging_writer.py:48] [83] global_step=83, grad_norm=2.295067, loss=6.496908
I0608 00:24:59.130973 140308546836288 submission.py:120] 83) loss = 6.497, grad_norm = 2.295
I0608 00:24:59.941347 140280225322752 logging_writer.py:48] [84] global_step=84, grad_norm=1.954563, loss=6.495783
I0608 00:24:59.945427 140308546836288 submission.py:120] 84) loss = 6.496, grad_norm = 1.955
I0608 00:25:00.747457 140279302584064 logging_writer.py:48] [85] global_step=85, grad_norm=2.002101, loss=6.463451
I0608 00:25:00.751039 140308546836288 submission.py:120] 85) loss = 6.463, grad_norm = 2.002
I0608 00:25:01.550072 140280225322752 logging_writer.py:48] [86] global_step=86, grad_norm=1.753247, loss=6.457137
I0608 00:25:01.553832 140308546836288 submission.py:120] 86) loss = 6.457, grad_norm = 1.753
I0608 00:25:02.356023 140279302584064 logging_writer.py:48] [87] global_step=87, grad_norm=1.751532, loss=6.438825
I0608 00:25:02.359432 140308546836288 submission.py:120] 87) loss = 6.439, grad_norm = 1.752
I0608 00:25:03.156594 140280225322752 logging_writer.py:48] [88] global_step=88, grad_norm=1.970890, loss=6.440230
I0608 00:25:03.159957 140308546836288 submission.py:120] 88) loss = 6.440, grad_norm = 1.971
I0608 00:25:03.958901 140279302584064 logging_writer.py:48] [89] global_step=89, grad_norm=2.067254, loss=6.444066
I0608 00:25:03.962968 140308546836288 submission.py:120] 89) loss = 6.444, grad_norm = 2.067
I0608 00:25:04.762735 140280225322752 logging_writer.py:48] [90] global_step=90, grad_norm=1.643618, loss=6.397181
I0608 00:25:04.766500 140308546836288 submission.py:120] 90) loss = 6.397, grad_norm = 1.644
I0608 00:25:05.565572 140279302584064 logging_writer.py:48] [91] global_step=91, grad_norm=2.046229, loss=6.405133
I0608 00:25:05.569186 140308546836288 submission.py:120] 91) loss = 6.405, grad_norm = 2.046
I0608 00:25:06.373575 140280225322752 logging_writer.py:48] [92] global_step=92, grad_norm=1.683124, loss=6.383733
I0608 00:25:06.376782 140308546836288 submission.py:120] 92) loss = 6.384, grad_norm = 1.683
I0608 00:25:07.179277 140279302584064 logging_writer.py:48] [93] global_step=93, grad_norm=1.679592, loss=6.371369
I0608 00:25:07.182688 140308546836288 submission.py:120] 93) loss = 6.371, grad_norm = 1.680
I0608 00:25:07.983787 140280225322752 logging_writer.py:48] [94] global_step=94, grad_norm=1.530233, loss=6.372680
I0608 00:25:07.987024 140308546836288 submission.py:120] 94) loss = 6.373, grad_norm = 1.530
I0608 00:25:08.789351 140279302584064 logging_writer.py:48] [95] global_step=95, grad_norm=1.529471, loss=6.354815
I0608 00:25:08.792775 140308546836288 submission.py:120] 95) loss = 6.355, grad_norm = 1.529
I0608 00:25:09.596038 140280225322752 logging_writer.py:48] [96] global_step=96, grad_norm=1.647198, loss=6.353282
I0608 00:25:09.599349 140308546836288 submission.py:120] 96) loss = 6.353, grad_norm = 1.647
I0608 00:25:10.398981 140279302584064 logging_writer.py:48] [97] global_step=97, grad_norm=1.483534, loss=6.331281
I0608 00:25:10.402224 140308546836288 submission.py:120] 97) loss = 6.331, grad_norm = 1.484
I0608 00:25:11.203001 140280225322752 logging_writer.py:48] [98] global_step=98, grad_norm=1.638277, loss=6.326247
I0608 00:25:11.206235 140308546836288 submission.py:120] 98) loss = 6.326, grad_norm = 1.638
I0608 00:25:12.005967 140279302584064 logging_writer.py:48] [99] global_step=99, grad_norm=1.756919, loss=6.313716
I0608 00:25:12.009310 140308546836288 submission.py:120] 99) loss = 6.314, grad_norm = 1.757
I0608 00:25:12.810528 140280225322752 logging_writer.py:48] [100] global_step=100, grad_norm=1.388345, loss=6.316407
I0608 00:25:12.813757 140308546836288 submission.py:120] 100) loss = 6.316, grad_norm = 1.388
I0608 00:30:29.878038 140279302584064 logging_writer.py:48] [500] global_step=500, grad_norm=1.032799, loss=5.791842
I0608 00:30:29.882152 140308546836288 submission.py:120] 500) loss = 5.792, grad_norm = 1.033
I0608 00:37:06.018771 140280225322752 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.940742, loss=5.468038
I0608 00:37:06.022960 140308546836288 submission.py:120] 1000) loss = 5.468, grad_norm = 2.941
I0608 00:43:43.766416 140280225322752 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.574160, loss=3.603207
I0608 00:43:43.774793 140308546836288 submission.py:120] 1500) loss = 3.603, grad_norm = 1.574
I0608 00:50:19.553327 140279302584064 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.120711, loss=2.971216
I0608 00:50:19.581925 140308546836288 submission.py:120] 2000) loss = 2.971, grad_norm = 1.121
I0608 00:56:56.697604 140280225322752 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.188133, loss=2.695151
I0608 00:56:56.705988 140308546836288 submission.py:120] 2500) loss = 2.695, grad_norm = 1.188
I0608 01:03:31.698286 140279302584064 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.985915, loss=2.446180
I0608 01:03:31.705729 140308546836288 submission.py:120] 3000) loss = 2.446, grad_norm = 0.986
I0608 01:03:52.228321 140308546836288 spec.py:298] Evaluating on the training split.
I0608 01:04:04.417295 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 01:04:14.776296 140308546836288 spec.py:326] Evaluating on the test split.
I0608 01:04:20.332646 140308546836288 submission_runner.py:419] Time since start: 2470.32s, 	Step: 3027, 	{'train/ctc_loss': 2.9382558993935914, 'train/wer': 0.6047193483527263, 'validation/ctc_loss': 3.1451656133840666, 'validation/wer': 0.6087770965094386, 'validation/num_examples': 5348, 'test/ctc_loss': 2.8129121233627976, 'test/wer': 0.5608636483659334, 'test/num_examples': 2472, 'score': 2407.2553544044495, 'total_duration': 2470.3205037117004, 'accumulated_submission_time': 2407.2553544044495, 'accumulated_eval_time': 61.89239740371704, 'accumulated_logging_time': 0.031235218048095703}
I0608 01:04:20.350106 140280225322752 logging_writer.py:48] [3027] accumulated_eval_time=61.892397, accumulated_logging_time=0.031235, accumulated_submission_time=2407.255354, global_step=3027, preemption_count=0, score=2407.255354, test/ctc_loss=2.812912, test/num_examples=2472, test/wer=0.560864, total_duration=2470.320504, train/ctc_loss=2.938256, train/wer=0.604719, validation/ctc_loss=3.145166, validation/num_examples=5348, validation/wer=0.608777
I0608 01:10:36.416602 140280225322752 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.863357, loss=2.357931
I0608 01:10:36.424137 140308546836288 submission.py:120] 3500) loss = 2.358, grad_norm = 0.863
I0608 01:17:11.303512 140279302584064 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.909287, loss=2.156399
I0608 01:17:11.308982 140308546836288 submission.py:120] 4000) loss = 2.156, grad_norm = 0.909
I0608 01:23:47.292147 140280225322752 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.991709, loss=2.013824
I0608 01:23:47.300933 140308546836288 submission.py:120] 4500) loss = 2.014, grad_norm = 0.992
I0608 01:30:21.780301 140279302584064 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.721484, loss=1.977786
I0608 01:30:21.785187 140308546836288 submission.py:120] 5000) loss = 1.978, grad_norm = 0.721
I0608 01:36:57.501917 140280225322752 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.739838, loss=1.907286
I0608 01:36:57.510159 140308546836288 submission.py:120] 5500) loss = 1.907, grad_norm = 0.740
I0608 01:43:32.018786 140279302584064 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.675122, loss=1.845308
I0608 01:43:32.023905 140308546836288 submission.py:120] 6000) loss = 1.845, grad_norm = 0.675
I0608 01:44:20.898785 140308546836288 spec.py:298] Evaluating on the training split.
I0608 01:44:33.600485 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 01:44:43.848340 140308546836288 spec.py:326] Evaluating on the test split.
I0608 01:44:49.499572 140308546836288 submission_runner.py:419] Time since start: 4899.49s, 	Step: 6063, 	{'train/ctc_loss': 0.6913709755747369, 'train/wer': 0.2302497872824726, 'validation/ctc_loss': 0.9201543898809523, 'validation/wer': 0.2712789069666393, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6192742273695077, 'test/wer': 0.20350171632847885, 'test/num_examples': 2472, 'score': 4806.622150421143, 'total_duration': 4899.487455368042, 'accumulated_submission_time': 4806.622150421143, 'accumulated_eval_time': 90.49292755126953, 'accumulated_logging_time': 0.05910682678222656}
I0608 01:44:49.517495 140280225322752 logging_writer.py:48] [6063] accumulated_eval_time=90.492928, accumulated_logging_time=0.059107, accumulated_submission_time=4806.622150, global_step=6063, preemption_count=0, score=4806.622150, test/ctc_loss=0.619274, test/num_examples=2472, test/wer=0.203502, total_duration=4899.487455, train/ctc_loss=0.691371, train/wer=0.230250, validation/ctc_loss=0.920154, validation/num_examples=5348, validation/wer=0.271279
I0608 01:50:36.596690 140280225322752 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.615751, loss=1.741804
I0608 01:50:36.604454 140308546836288 submission.py:120] 6500) loss = 1.742, grad_norm = 0.616
I0608 01:57:10.723206 140279302584064 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.614547, loss=1.765372
I0608 01:57:10.728331 140308546836288 submission.py:120] 7000) loss = 1.765, grad_norm = 0.615
I0608 02:03:46.348881 140280225322752 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.496803, loss=1.726357
I0608 02:03:46.356621 140308546836288 submission.py:120] 7500) loss = 1.726, grad_norm = 0.497
I0608 02:10:20.302181 140279302584064 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.455941, loss=1.592314
I0608 02:10:20.307435 140308546836288 submission.py:120] 8000) loss = 1.592, grad_norm = 0.456
I0608 02:16:55.949928 140280225322752 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.466405, loss=1.589723
I0608 02:16:55.958098 140308546836288 submission.py:120] 8500) loss = 1.590, grad_norm = 0.466
I0608 02:23:30.009755 140279302584064 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.465779, loss=1.637146
I0608 02:23:30.016601 140308546836288 submission.py:120] 9000) loss = 1.637, grad_norm = 0.466
I0608 02:24:49.643275 140308546836288 spec.py:298] Evaluating on the training split.
I0608 02:25:01.994580 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 02:25:12.354841 140308546836288 spec.py:326] Evaluating on the test split.
I0608 02:25:18.028451 140308546836288 submission_runner.py:419] Time since start: 7328.02s, 	Step: 9102, 	{'train/ctc_loss': 0.48756207665411055, 'train/wer': 0.17023905656389385, 'validation/ctc_loss': 0.7108609761151296, 'validation/wer': 0.21431950948679573, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45821842336602364, 'test/wer': 0.15422582414234354, 'test/num_examples': 2472, 'score': 7205.584575176239, 'total_duration': 7328.0163197517395, 'accumulated_submission_time': 7205.584575176239, 'accumulated_eval_time': 118.8778190612793, 'accumulated_logging_time': 0.08745884895324707}
I0608 02:25:18.046320 140280225322752 logging_writer.py:48] [9102] accumulated_eval_time=118.877819, accumulated_logging_time=0.087459, accumulated_submission_time=7205.584575, global_step=9102, preemption_count=0, score=7205.584575, test/ctc_loss=0.458218, test/num_examples=2472, test/wer=0.154226, total_duration=7328.016320, train/ctc_loss=0.487562, train/wer=0.170239, validation/ctc_loss=0.710861, validation/num_examples=5348, validation/wer=0.214320
I0608 02:30:34.279305 140280225322752 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.462605, loss=1.562806
I0608 02:30:34.286868 140308546836288 submission.py:120] 9500) loss = 1.563, grad_norm = 0.463
I0608 02:37:08.399874 140279302584064 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.455676, loss=1.591976
I0608 02:37:08.404649 140308546836288 submission.py:120] 10000) loss = 1.592, grad_norm = 0.456
I0608 02:43:44.276256 140280225322752 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.473354, loss=1.561418
I0608 02:43:44.283640 140308546836288 submission.py:120] 10500) loss = 1.561, grad_norm = 0.473
I0608 02:50:18.545940 140279302584064 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.532523, loss=1.511941
I0608 02:50:18.575716 140308546836288 submission.py:120] 11000) loss = 1.512, grad_norm = 0.533
I0608 02:56:54.088878 140280225322752 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.429411, loss=1.450917
I0608 02:56:54.097002 140308546836288 submission.py:120] 11500) loss = 1.451, grad_norm = 0.429
I0608 03:03:28.029738 140279302584064 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.492657, loss=1.500877
I0608 03:03:28.061070 140308546836288 submission.py:120] 12000) loss = 1.501, grad_norm = 0.493
I0608 03:05:18.241050 140308546836288 spec.py:298] Evaluating on the training split.
I0608 03:05:30.856915 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 03:05:41.123491 140308546836288 spec.py:326] Evaluating on the test split.
I0608 03:05:46.770523 140308546836288 submission_runner.py:419] Time since start: 9756.76s, 	Step: 12141, 	{'train/ctc_loss': 0.38330401935778113, 'train/wer': 0.1376404342147336, 'validation/ctc_loss': 0.6114731279510749, 'validation/wer': 0.18593154057838074, 'validation/num_examples': 5348, 'test/ctc_loss': 0.38315072746628814, 'test/wer': 0.13186277496800927, 'test/num_examples': 2472, 'score': 9604.606491804123, 'total_duration': 9756.758280038834, 'accumulated_submission_time': 9604.606491804123, 'accumulated_eval_time': 147.40692615509033, 'accumulated_logging_time': 0.11506080627441406}
I0608 03:05:46.787451 140280225322752 logging_writer.py:48] [12141] accumulated_eval_time=147.406926, accumulated_logging_time=0.115061, accumulated_submission_time=9604.606492, global_step=12141, preemption_count=0, score=9604.606492, test/ctc_loss=0.383151, test/num_examples=2472, test/wer=0.131863, total_duration=9756.758280, train/ctc_loss=0.383304, train/wer=0.137640, validation/ctc_loss=0.611473, validation/num_examples=5348, validation/wer=0.185932
I0608 03:10:31.905139 140280225322752 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.398634, loss=1.430629
I0608 03:10:31.912882 140308546836288 submission.py:120] 12500) loss = 1.431, grad_norm = 0.399
I0608 03:17:05.650309 140279302584064 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.410981, loss=1.476182
I0608 03:17:05.655384 140308546836288 submission.py:120] 13000) loss = 1.476, grad_norm = 0.411
I0608 03:23:41.167413 140280225322752 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.293545, loss=1.356177
I0608 03:23:41.175082 140308546836288 submission.py:120] 13500) loss = 1.356, grad_norm = 0.294
I0608 03:30:15.027778 140279302584064 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.324607, loss=1.396026
I0608 03:30:15.032840 140308546836288 submission.py:120] 14000) loss = 1.396, grad_norm = 0.325
I0608 03:36:50.418104 140280225322752 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.348571, loss=1.399243
I0608 03:36:50.428540 140308546836288 submission.py:120] 14500) loss = 1.399, grad_norm = 0.349
I0608 03:43:24.340608 140279302584064 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.349044, loss=1.461890
I0608 03:43:24.369884 140308546836288 submission.py:120] 15000) loss = 1.462, grad_norm = 0.349
I0608 03:45:46.941815 140308546836288 spec.py:298] Evaluating on the training split.
I0608 03:45:59.189082 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 03:46:09.673907 140308546836288 spec.py:326] Evaluating on the test split.
I0608 03:46:15.233295 140308546836288 submission_runner.py:419] Time since start: 12185.22s, 	Step: 15182, 	{'train/ctc_loss': 0.3218464426276583, 'train/wer': 0.11856902074063637, 'validation/ctc_loss': 0.5493334769627788, 'validation/wer': 0.16690967025539516, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3341001334763533, 'test/wer': 0.11567444600166554, 'test/num_examples': 2472, 'score': 12003.591171503067, 'total_duration': 12185.22114944458, 'accumulated_submission_time': 12003.591171503067, 'accumulated_eval_time': 175.69811058044434, 'accumulated_logging_time': 0.14176225662231445}
I0608 03:46:15.252899 140280225322752 logging_writer.py:48] [15182] accumulated_eval_time=175.698111, accumulated_logging_time=0.141762, accumulated_submission_time=12003.591172, global_step=15182, preemption_count=0, score=12003.591172, test/ctc_loss=0.334100, test/num_examples=2472, test/wer=0.115674, total_duration=12185.221149, train/ctc_loss=0.321846, train/wer=0.118569, validation/ctc_loss=0.549333, validation/num_examples=5348, validation/wer=0.166910
I0608 03:50:27.828660 140280225322752 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.335648, loss=1.353862
I0608 03:50:27.836721 140308546836288 submission.py:120] 15500) loss = 1.354, grad_norm = 0.336
I0608 03:57:01.636018 140279302584064 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.340999, loss=1.382828
I0608 03:57:01.641992 140308546836288 submission.py:120] 16000) loss = 1.383, grad_norm = 0.341
I0608 04:03:36.893180 140280225322752 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.352427, loss=1.387913
I0608 04:03:36.901305 140308546836288 submission.py:120] 16500) loss = 1.388, grad_norm = 0.352
I0608 04:10:10.705118 140279302584064 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.396656, loss=1.349229
I0608 04:10:10.709794 140308546836288 submission.py:120] 17000) loss = 1.349, grad_norm = 0.397
I0608 04:16:44.525549 140280225322752 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.333592, loss=1.327834
I0608 04:16:44.530160 140308546836288 submission.py:120] 17500) loss = 1.328, grad_norm = 0.334
I0608 04:23:20.294501 140280225322752 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.327223, loss=1.363323
I0608 04:23:20.327689 140308546836288 submission.py:120] 18000) loss = 1.363, grad_norm = 0.327
I0608 04:26:15.332035 140308546836288 spec.py:298] Evaluating on the training split.
I0608 04:26:27.547150 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 04:26:37.771090 140308546836288 spec.py:326] Evaluating on the test split.
I0608 04:26:43.364681 140308546836288 submission_runner.py:419] Time since start: 14613.35s, 	Step: 18223, 	{'train/ctc_loss': 0.2792272829006267, 'train/wer': 0.10455947148501223, 'validation/ctc_loss': 0.5206168590359403, 'validation/wer': 0.15693525805049968, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3082662843167301, 'test/wer': 0.10523429407104991, 'test/num_examples': 2472, 'score': 14402.497485399246, 'total_duration': 14613.352481126785, 'accumulated_submission_time': 14402.497485399246, 'accumulated_eval_time': 203.73038291931152, 'accumulated_logging_time': 0.17261767387390137}
I0608 04:26:43.382527 140280225322752 logging_writer.py:48] [18223] accumulated_eval_time=203.730383, accumulated_logging_time=0.172618, accumulated_submission_time=14402.497485, global_step=18223, preemption_count=0, score=14402.497485, test/ctc_loss=0.308266, test/num_examples=2472, test/wer=0.105234, total_duration=14613.352481, train/ctc_loss=0.279227, train/wer=0.104559, validation/ctc_loss=0.520617, validation/num_examples=5348, validation/wer=0.156935
I0608 04:30:22.398341 140279302584064 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.437372, loss=1.341558
I0608 04:30:22.403372 140308546836288 submission.py:120] 18500) loss = 1.342, grad_norm = 0.437
I0608 04:36:58.050178 140280225322752 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.491342, loss=1.336098
I0608 04:36:58.058598 140308546836288 submission.py:120] 19000) loss = 1.336, grad_norm = 0.491
I0608 04:43:31.872406 140279302584064 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.366785, loss=1.292827
I0608 04:43:31.877185 140308546836288 submission.py:120] 19500) loss = 1.293, grad_norm = 0.367
I0608 04:50:06.593303 140308546836288 spec.py:298] Evaluating on the training split.
I0608 04:50:18.971997 140308546836288 spec.py:310] Evaluating on the validation split.
I0608 04:50:29.645186 140308546836288 spec.py:326] Evaluating on the test split.
I0608 04:50:35.158839 140308546836288 submission_runner.py:419] Time since start: 16045.15s, 	Step: 20000, 	{'train/ctc_loss': 0.2678112489653891, 'train/wer': 0.10063571378246991, 'validation/ctc_loss': 0.5233546481784961, 'validation/wer': 0.15591174624631873, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30339674132202077, 'test/wer': 0.10212662238742307, 'test/num_examples': 2472, 'score': 15805.020161867142, 'total_duration': 16045.146612644196, 'accumulated_submission_time': 15805.020161867142, 'accumulated_eval_time': 232.29568767547607, 'accumulated_logging_time': 0.20009183883666992}
I0608 04:50:35.181641 140280225322752 logging_writer.py:48] [20000] accumulated_eval_time=232.295688, accumulated_logging_time=0.200092, accumulated_submission_time=15805.020162, global_step=20000, preemption_count=0, score=15805.020162, test/ctc_loss=0.303397, test/num_examples=2472, test/wer=0.102127, total_duration=16045.146613, train/ctc_loss=0.267811, train/wer=0.100636, validation/ctc_loss=0.523355, validation/num_examples=5348, validation/wer=0.155912
I0608 04:50:35.205466 140279302584064 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15805.020162
I0608 04:50:35.920230 140308546836288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0608 04:50:36.035980 140308546836288 submission_runner.py:581] Tuning trial 1/1
I0608 04:50:36.036209 140308546836288 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0608 04:50:36.036768 140308546836288 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.10721778348339, 'train/wer': 2.0333790382457986, 'validation/ctc_loss': 30.116382233273058, 'validation/wer': 1.776072997634336, 'validation/num_examples': 5348, 'test/ctc_loss': 30.21974643525389, 'test/wer': 1.8231470761481121, 'test/num_examples': 2472, 'score': 8.055304288864136, 'total_duration': 41.8441047668457, 'accumulated_submission_time': 8.055304288864136, 'accumulated_eval_time': 33.78838229179382, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3027, {'train/ctc_loss': 2.9382558993935914, 'train/wer': 0.6047193483527263, 'validation/ctc_loss': 3.1451656133840666, 'validation/wer': 0.6087770965094386, 'validation/num_examples': 5348, 'test/ctc_loss': 2.8129121233627976, 'test/wer': 0.5608636483659334, 'test/num_examples': 2472, 'score': 2407.2553544044495, 'total_duration': 2470.3205037117004, 'accumulated_submission_time': 2407.2553544044495, 'accumulated_eval_time': 61.89239740371704, 'accumulated_logging_time': 0.031235218048095703, 'global_step': 3027, 'preemption_count': 0}), (6063, {'train/ctc_loss': 0.6913709755747369, 'train/wer': 0.2302497872824726, 'validation/ctc_loss': 0.9201543898809523, 'validation/wer': 0.2712789069666393, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6192742273695077, 'test/wer': 0.20350171632847885, 'test/num_examples': 2472, 'score': 4806.622150421143, 'total_duration': 4899.487455368042, 'accumulated_submission_time': 4806.622150421143, 'accumulated_eval_time': 90.49292755126953, 'accumulated_logging_time': 0.05910682678222656, 'global_step': 6063, 'preemption_count': 0}), (9102, {'train/ctc_loss': 0.48756207665411055, 'train/wer': 0.17023905656389385, 'validation/ctc_loss': 0.7108609761151296, 'validation/wer': 0.21431950948679573, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45821842336602364, 'test/wer': 0.15422582414234354, 'test/num_examples': 2472, 'score': 7205.584575176239, 'total_duration': 7328.0163197517395, 'accumulated_submission_time': 7205.584575176239, 'accumulated_eval_time': 118.8778190612793, 'accumulated_logging_time': 0.08745884895324707, 'global_step': 9102, 'preemption_count': 0}), (12141, {'train/ctc_loss': 0.38330401935778113, 'train/wer': 0.1376404342147336, 'validation/ctc_loss': 0.6114731279510749, 'validation/wer': 0.18593154057838074, 'validation/num_examples': 5348, 'test/ctc_loss': 0.38315072746628814, 'test/wer': 0.13186277496800927, 'test/num_examples': 2472, 'score': 9604.606491804123, 'total_duration': 9756.758280038834, 'accumulated_submission_time': 9604.606491804123, 'accumulated_eval_time': 147.40692615509033, 'accumulated_logging_time': 0.11506080627441406, 'global_step': 12141, 'preemption_count': 0}), (15182, {'train/ctc_loss': 0.3218464426276583, 'train/wer': 0.11856902074063637, 'validation/ctc_loss': 0.5493334769627788, 'validation/wer': 0.16690967025539516, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3341001334763533, 'test/wer': 0.11567444600166554, 'test/num_examples': 2472, 'score': 12003.591171503067, 'total_duration': 12185.22114944458, 'accumulated_submission_time': 12003.591171503067, 'accumulated_eval_time': 175.69811058044434, 'accumulated_logging_time': 0.14176225662231445, 'global_step': 15182, 'preemption_count': 0}), (18223, {'train/ctc_loss': 0.2792272829006267, 'train/wer': 0.10455947148501223, 'validation/ctc_loss': 0.5206168590359403, 'validation/wer': 0.15693525805049968, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3082662843167301, 'test/wer': 0.10523429407104991, 'test/num_examples': 2472, 'score': 14402.497485399246, 'total_duration': 14613.352481126785, 'accumulated_submission_time': 14402.497485399246, 'accumulated_eval_time': 203.73038291931152, 'accumulated_logging_time': 0.17261767387390137, 'global_step': 18223, 'preemption_count': 0}), (20000, {'train/ctc_loss': 0.2678112489653891, 'train/wer': 0.10063571378246991, 'validation/ctc_loss': 0.5233546481784961, 'validation/wer': 0.15591174624631873, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30339674132202077, 'test/wer': 0.10212662238742307, 'test/num_examples': 2472, 'score': 15805.020161867142, 'total_duration': 16045.146612644196, 'accumulated_submission_time': 15805.020161867142, 'accumulated_eval_time': 232.29568767547607, 'accumulated_logging_time': 0.20009183883666992, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0608 04:50:36.036863 140308546836288 submission_runner.py:584] Timing: 15805.020161867142
I0608 04:50:36.036913 140308546836288 submission_runner.py:586] Total number of evals: 8
I0608 04:50:36.036982 140308546836288 submission_runner.py:587] ====================
I0608 04:50:36.037158 140308546836288 submission_runner.py:655] Final librispeech_conformer score: 15805.020161867142
