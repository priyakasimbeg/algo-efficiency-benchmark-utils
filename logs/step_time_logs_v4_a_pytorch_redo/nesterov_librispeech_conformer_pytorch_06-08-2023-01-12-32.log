torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_06-08-2023-01-12-32.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 01:12:55.131993 139719573530432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 01:12:55.132018 140614900242240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 01:12:55.132894 140162146604864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 01:12:55.132954 139742109992768 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 01:12:55.133034 139991005116224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 01:12:55.133072 139750907844416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 01:12:55.133111 140108195632960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 01:12:55.143447 140162146604864 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.143471 139742109992768 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.143425 139623933597504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 01:12:55.143566 139991005116224 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.143712 139623933597504 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.143722 139750907844416 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.143786 140108195632960 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.152839 139719573530432 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.152894 140614900242240 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 01:12:55.502683 139623933597504 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch because --overwrite was set.
I0608 01:12:55.519392 139623933597504 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch.
W0608 01:12:55.819628 139991005116224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.819813 139750907844416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.820357 140108195632960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.820616 139719573530432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.820954 140162146604864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.822206 139742109992768 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.822193 140614900242240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 01:12:55.842129 139623933597504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 01:12:55.847305 139623933597504 submission_runner.py:541] Using RNG seed 1404161554
I0608 01:12:55.848805 139623933597504 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 01:12:55.848917 139623933597504 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch/trial_1.
I0608 01:12:55.849136 139623933597504 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch/trial_1/hparams.json.
I0608 01:12:55.850146 139623933597504 submission_runner.py:255] Initializing dataset.
I0608 01:12:55.850285 139623933597504 input_pipeline.py:20] Loading split = train-clean-100
I0608 01:12:55.884732 139623933597504 input_pipeline.py:20] Loading split = train-clean-360
I0608 01:12:56.239118 139623933597504 input_pipeline.py:20] Loading split = train-other-500
I0608 01:12:56.682229 139623933597504 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0608 01:13:03.482691 139623933597504 submission_runner.py:272] Initializing optimizer.
I0608 01:13:03.954576 139623933597504 submission_runner.py:279] Initializing metrics bundle.
I0608 01:13:03.954784 139623933597504 submission_runner.py:297] Initializing checkpoint and logger.
I0608 01:13:03.956108 139623933597504 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0608 01:13:03.956230 139623933597504 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0608 01:13:04.508563 139623933597504 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0608 01:13:04.509586 139623933597504 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0608 01:13:04.517392 139623933597504 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0608 01:13:12.357804 139597418952448 logging_writer.py:48] [0] global_step=0, grad_norm=91.281403, loss=31.413752
I0608 01:13:12.380454 139623933597504 submission.py:139] 0) loss = 31.414, grad_norm = 91.281
I0608 01:13:12.381753 139623933597504 spec.py:298] Evaluating on the training split.
I0608 01:13:12.382838 139623933597504 input_pipeline.py:20] Loading split = train-clean-100
I0608 01:13:12.417290 139623933597504 input_pipeline.py:20] Loading split = train-clean-360
I0608 01:13:12.846445 139623933597504 input_pipeline.py:20] Loading split = train-other-500
I0608 01:13:29.742272 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 01:13:29.743722 139623933597504 input_pipeline.py:20] Loading split = dev-clean
I0608 01:13:29.747538 139623933597504 input_pipeline.py:20] Loading split = dev-other
I0608 01:13:40.666171 139623933597504 spec.py:326] Evaluating on the test split.
I0608 01:13:40.667556 139623933597504 input_pipeline.py:20] Loading split = test-clean
I0608 01:13:46.523962 139623933597504 submission_runner.py:419] Time since start: 42.01s, 	Step: 1, 	{'train/ctc_loss': 30.599367896473527, 'train/wer': 2.3741242029567995, 'validation/ctc_loss': 29.31221745027125, 'validation/wer': 2.0297880558103607, 'validation/num_examples': 5348, 'test/ctc_loss': 29.40055164849345, 'test/wer': 2.100197022322426, 'test/num_examples': 2472, 'score': 7.864490032196045, 'total_duration': 42.00683093070984, 'accumulated_submission_time': 7.864490032196045, 'accumulated_eval_time': 34.14194917678833, 'accumulated_logging_time': 0}
I0608 01:13:46.546461 139596216469248 logging_writer.py:48] [1] accumulated_eval_time=34.141949, accumulated_logging_time=0, accumulated_submission_time=7.864490, global_step=1, preemption_count=0, score=7.864490, test/ctc_loss=29.400552, test/num_examples=2472, test/wer=2.100197, total_duration=42.006831, train/ctc_loss=30.599368, train/wer=2.374124, validation/ctc_loss=29.312217, validation/num_examples=5348, validation/wer=2.029788
I0608 01:13:46.591049 139623933597504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.590989 139750907844416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.591029 139742109992768 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.591038 139991005116224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.591049 140614900242240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.591037 139719573530432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.591138 140108195632960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:46.591201 140162146604864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 01:13:47.580261 139595798804224 logging_writer.py:48] [1] global_step=1, grad_norm=89.725822, loss=30.793571
I0608 01:13:47.584056 139623933597504 submission.py:139] 1) loss = 30.794, grad_norm = 89.726
I0608 01:13:48.417887 139596216469248 logging_writer.py:48] [2] global_step=2, grad_norm=139.183136, loss=29.344130
I0608 01:13:48.421277 139623933597504 submission.py:139] 2) loss = 29.344, grad_norm = 139.183
I0608 01:13:49.364588 139595798804224 logging_writer.py:48] [3] global_step=3, grad_norm=171.345856, loss=20.523285
I0608 01:13:49.367783 139623933597504 submission.py:139] 3) loss = 20.523, grad_norm = 171.346
I0608 01:13:50.154222 139596216469248 logging_writer.py:48] [4] global_step=4, grad_norm=35.775932, loss=7.615373
I0608 01:13:50.157513 139623933597504 submission.py:139] 4) loss = 7.615, grad_norm = 35.776
I0608 01:13:50.942188 139595798804224 logging_writer.py:48] [5] global_step=5, grad_norm=25.054403, loss=8.205541
I0608 01:13:50.945369 139623933597504 submission.py:139] 5) loss = 8.206, grad_norm = 25.054
I0608 01:13:51.734399 139596216469248 logging_writer.py:48] [6] global_step=6, grad_norm=27.352493, loss=9.817323
I0608 01:13:51.737845 139623933597504 submission.py:139] 6) loss = 9.817, grad_norm = 27.352
I0608 01:13:52.526185 139595798804224 logging_writer.py:48] [7] global_step=7, grad_norm=28.250360, loss=10.681515
I0608 01:13:52.529394 139623933597504 submission.py:139] 7) loss = 10.682, grad_norm = 28.250
I0608 01:13:53.314987 139596216469248 logging_writer.py:48] [8] global_step=8, grad_norm=29.763035, loss=10.693362
I0608 01:13:53.318813 139623933597504 submission.py:139] 8) loss = 10.693, grad_norm = 29.763
I0608 01:13:54.106347 139595798804224 logging_writer.py:48] [9] global_step=9, grad_norm=31.104408, loss=9.860203
I0608 01:13:54.109937 139623933597504 submission.py:139] 9) loss = 9.860, grad_norm = 31.104
I0608 01:13:54.895864 139596216469248 logging_writer.py:48] [10] global_step=10, grad_norm=29.128948, loss=8.162624
I0608 01:13:54.899172 139623933597504 submission.py:139] 10) loss = 8.163, grad_norm = 29.129
I0608 01:13:55.685232 139595798804224 logging_writer.py:48] [11] global_step=11, grad_norm=20.256510, loss=7.146936
I0608 01:13:55.688654 139623933597504 submission.py:139] 11) loss = 7.147, grad_norm = 20.257
I0608 01:13:56.478274 139596216469248 logging_writer.py:48] [12] global_step=12, grad_norm=74.958191, loss=8.559579
I0608 01:13:56.481831 139623933597504 submission.py:139] 12) loss = 8.560, grad_norm = 74.958
I0608 01:13:57.269818 139595798804224 logging_writer.py:48] [13] global_step=13, grad_norm=18.841341, loss=7.195275
I0608 01:13:57.273272 139623933597504 submission.py:139] 13) loss = 7.195, grad_norm = 18.841
I0608 01:13:58.062984 139596216469248 logging_writer.py:48] [14] global_step=14, grad_norm=5.505079, loss=6.931697
I0608 01:13:58.066756 139623933597504 submission.py:139] 14) loss = 6.932, grad_norm = 5.505
I0608 01:13:58.853262 139595798804224 logging_writer.py:48] [15] global_step=15, grad_norm=6.277226, loss=6.888096
I0608 01:13:58.856607 139623933597504 submission.py:139] 15) loss = 6.888, grad_norm = 6.277
I0608 01:13:59.645471 139596216469248 logging_writer.py:48] [16] global_step=16, grad_norm=2.264064, loss=6.862882
I0608 01:13:59.649470 139623933597504 submission.py:139] 16) loss = 6.863, grad_norm = 2.264
I0608 01:14:00.436363 139595798804224 logging_writer.py:48] [17] global_step=17, grad_norm=2.420970, loss=6.816713
I0608 01:14:00.439951 139623933597504 submission.py:139] 17) loss = 6.817, grad_norm = 2.421
I0608 01:14:01.224978 139596216469248 logging_writer.py:48] [18] global_step=18, grad_norm=2.286654, loss=6.779323
I0608 01:14:01.228539 139623933597504 submission.py:139] 18) loss = 6.779, grad_norm = 2.287
I0608 01:14:02.026283 139595798804224 logging_writer.py:48] [19] global_step=19, grad_norm=2.432925, loss=6.719301
I0608 01:14:02.029600 139623933597504 submission.py:139] 19) loss = 6.719, grad_norm = 2.433
I0608 01:14:02.817775 139596216469248 logging_writer.py:48] [20] global_step=20, grad_norm=2.021801, loss=6.663076
I0608 01:14:02.821104 139623933597504 submission.py:139] 20) loss = 6.663, grad_norm = 2.022
I0608 01:14:03.609721 139595798804224 logging_writer.py:48] [21] global_step=21, grad_norm=1.935949, loss=6.639878
I0608 01:14:03.613177 139623933597504 submission.py:139] 21) loss = 6.640, grad_norm = 1.936
I0608 01:14:04.397919 139596216469248 logging_writer.py:48] [22] global_step=22, grad_norm=1.825638, loss=6.604572
I0608 01:14:04.402036 139623933597504 submission.py:139] 22) loss = 6.605, grad_norm = 1.826
I0608 01:14:05.195106 139595798804224 logging_writer.py:48] [23] global_step=23, grad_norm=2.975044, loss=6.566537
I0608 01:14:05.198826 139623933597504 submission.py:139] 23) loss = 6.567, grad_norm = 2.975
I0608 01:14:05.989544 139596216469248 logging_writer.py:48] [24] global_step=24, grad_norm=6.264536, loss=6.533442
I0608 01:14:05.992981 139623933597504 submission.py:139] 24) loss = 6.533, grad_norm = 6.265
I0608 01:14:06.780549 139595798804224 logging_writer.py:48] [25] global_step=25, grad_norm=12.262179, loss=6.583016
I0608 01:14:06.784044 139623933597504 submission.py:139] 25) loss = 6.583, grad_norm = 12.262
I0608 01:14:07.571364 139596216469248 logging_writer.py:48] [26] global_step=26, grad_norm=33.243694, loss=6.774101
I0608 01:14:07.574752 139623933597504 submission.py:139] 26) loss = 6.774, grad_norm = 33.244
I0608 01:14:08.364067 139595798804224 logging_writer.py:48] [27] global_step=27, grad_norm=34.569279, loss=8.335501
I0608 01:14:08.368183 139623933597504 submission.py:139] 27) loss = 8.336, grad_norm = 34.569
I0608 01:14:09.155401 139596216469248 logging_writer.py:48] [28] global_step=28, grad_norm=2.446210, loss=6.402148
I0608 01:14:09.158809 139623933597504 submission.py:139] 28) loss = 6.402, grad_norm = 2.446
I0608 01:14:09.945873 139595798804224 logging_writer.py:48] [29] global_step=29, grad_norm=16.578129, loss=6.480002
I0608 01:14:09.949176 139623933597504 submission.py:139] 29) loss = 6.480, grad_norm = 16.578
I0608 01:14:10.737410 139596216469248 logging_writer.py:48] [30] global_step=30, grad_norm=26.053013, loss=6.905721
I0608 01:14:10.741348 139623933597504 submission.py:139] 30) loss = 6.906, grad_norm = 26.053
I0608 01:14:11.529250 139595798804224 logging_writer.py:48] [31] global_step=31, grad_norm=88.168625, loss=8.279354
I0608 01:14:11.533703 139623933597504 submission.py:139] 31) loss = 8.279, grad_norm = 88.169
I0608 01:14:12.321476 139596216469248 logging_writer.py:48] [32] global_step=32, grad_norm=32.307034, loss=14.280110
I0608 01:14:12.324962 139623933597504 submission.py:139] 32) loss = 14.280, grad_norm = 32.307
I0608 01:14:13.111150 139595798804224 logging_writer.py:48] [33] global_step=33, grad_norm=31.544857, loss=13.890619
I0608 01:14:13.114670 139623933597504 submission.py:139] 33) loss = 13.891, grad_norm = 31.545
I0608 01:14:13.903779 139596216469248 logging_writer.py:48] [34] global_step=34, grad_norm=31.192551, loss=11.335572
I0608 01:14:13.906766 139623933597504 submission.py:139] 34) loss = 11.336, grad_norm = 31.193
I0608 01:14:14.695601 139595798804224 logging_writer.py:48] [35] global_step=35, grad_norm=24.094048, loss=7.027970
I0608 01:14:14.699757 139623933597504 submission.py:139] 35) loss = 7.028, grad_norm = 24.094
I0608 01:14:15.489161 139596216469248 logging_writer.py:48] [36] global_step=36, grad_norm=124.638405, loss=14.909625
I0608 01:14:15.493012 139623933597504 submission.py:139] 36) loss = 14.910, grad_norm = 124.638
I0608 01:14:16.283964 139595798804224 logging_writer.py:48] [37] global_step=37, grad_norm=28.370831, loss=14.520336
I0608 01:14:16.287304 139623933597504 submission.py:139] 37) loss = 14.520, grad_norm = 28.371
I0608 01:14:17.074523 139596216469248 logging_writer.py:48] [38] global_step=38, grad_norm=27.619539, loss=14.532608
I0608 01:14:17.077580 139623933597504 submission.py:139] 38) loss = 14.533, grad_norm = 27.620
I0608 01:14:17.867413 139595798804224 logging_writer.py:48] [39] global_step=39, grad_norm=27.117516, loss=12.630922
I0608 01:14:17.870977 139623933597504 submission.py:139] 39) loss = 12.631, grad_norm = 27.118
I0608 01:14:18.657608 139596216469248 logging_writer.py:48] [40] global_step=40, grad_norm=26.093424, loss=8.988586
I0608 01:14:18.661737 139623933597504 submission.py:139] 40) loss = 8.989, grad_norm = 26.093
I0608 01:14:19.449092 139595798804224 logging_writer.py:48] [41] global_step=41, grad_norm=40.795967, loss=7.191298
I0608 01:14:19.452361 139623933597504 submission.py:139] 41) loss = 7.191, grad_norm = 40.796
I0608 01:14:20.242685 139596216469248 logging_writer.py:48] [42] global_step=42, grad_norm=16.017433, loss=6.570211
I0608 01:14:20.245996 139623933597504 submission.py:139] 42) loss = 6.570, grad_norm = 16.017
I0608 01:14:21.033424 139595798804224 logging_writer.py:48] [43] global_step=43, grad_norm=48.508488, loss=7.556723
I0608 01:14:21.036608 139623933597504 submission.py:139] 43) loss = 7.557, grad_norm = 48.508
I0608 01:14:21.824905 139596216469248 logging_writer.py:48] [44] global_step=44, grad_norm=24.736183, loss=8.794919
I0608 01:14:21.828261 139623933597504 submission.py:139] 44) loss = 8.795, grad_norm = 24.736
I0608 01:14:22.639800 139595798804224 logging_writer.py:48] [45] global_step=45, grad_norm=20.663904, loss=7.095652
I0608 01:14:22.643866 139623933597504 submission.py:139] 45) loss = 7.096, grad_norm = 20.664
I0608 01:14:23.431754 139596216469248 logging_writer.py:48] [46] global_step=46, grad_norm=31.099733, loss=6.778925
I0608 01:14:23.435262 139623933597504 submission.py:139] 46) loss = 6.779, grad_norm = 31.100
I0608 01:14:24.226001 139595798804224 logging_writer.py:48] [47] global_step=47, grad_norm=20.913721, loss=7.204899
I0608 01:14:24.229239 139623933597504 submission.py:139] 47) loss = 7.205, grad_norm = 20.914
I0608 01:14:25.014618 139596216469248 logging_writer.py:48] [48] global_step=48, grad_norm=8.963513, loss=6.212441
I0608 01:14:25.018528 139623933597504 submission.py:139] 48) loss = 6.212, grad_norm = 8.964
I0608 01:14:25.809486 139595798804224 logging_writer.py:48] [49] global_step=49, grad_norm=4.567854, loss=6.137165
I0608 01:14:25.813300 139623933597504 submission.py:139] 49) loss = 6.137, grad_norm = 4.568
I0608 01:14:26.600817 139596216469248 logging_writer.py:48] [50] global_step=50, grad_norm=11.561298, loss=6.224451
I0608 01:14:26.604286 139623933597504 submission.py:139] 50) loss = 6.224, grad_norm = 11.561
I0608 01:14:27.390373 139595798804224 logging_writer.py:48] [51] global_step=51, grad_norm=13.454236, loss=6.376024
I0608 01:14:27.393671 139623933597504 submission.py:139] 51) loss = 6.376, grad_norm = 13.454
I0608 01:14:28.180470 139596216469248 logging_writer.py:48] [52] global_step=52, grad_norm=18.140676, loss=6.354630
I0608 01:14:28.183659 139623933597504 submission.py:139] 52) loss = 6.355, grad_norm = 18.141
I0608 01:14:28.974179 139595798804224 logging_writer.py:48] [53] global_step=53, grad_norm=18.449745, loss=6.819307
I0608 01:14:28.978032 139623933597504 submission.py:139] 53) loss = 6.819, grad_norm = 18.450
I0608 01:14:29.768075 139596216469248 logging_writer.py:48] [54] global_step=54, grad_norm=15.148624, loss=6.270901
I0608 01:14:29.772208 139623933597504 submission.py:139] 54) loss = 6.271, grad_norm = 15.149
I0608 01:14:30.561064 139595798804224 logging_writer.py:48] [55] global_step=55, grad_norm=16.109991, loss=6.556862
I0608 01:14:30.565229 139623933597504 submission.py:139] 55) loss = 6.557, grad_norm = 16.110
I0608 01:14:31.356350 139596216469248 logging_writer.py:48] [56] global_step=56, grad_norm=22.166300, loss=6.410263
I0608 01:14:31.359633 139623933597504 submission.py:139] 56) loss = 6.410, grad_norm = 22.166
I0608 01:14:32.148930 139595798804224 logging_writer.py:48] [57] global_step=57, grad_norm=20.634022, loss=7.246409
I0608 01:14:32.152160 139623933597504 submission.py:139] 57) loss = 7.246, grad_norm = 20.634
I0608 01:14:32.952799 139596216469248 logging_writer.py:48] [58] global_step=58, grad_norm=7.317102, loss=6.086794
I0608 01:14:32.956207 139623933597504 submission.py:139] 58) loss = 6.087, grad_norm = 7.317
I0608 01:14:33.746876 139595798804224 logging_writer.py:48] [59] global_step=59, grad_norm=5.120129, loss=6.070512
I0608 01:14:33.750589 139623933597504 submission.py:139] 59) loss = 6.071, grad_norm = 5.120
I0608 01:14:34.540424 139596216469248 logging_writer.py:48] [60] global_step=60, grad_norm=14.923009, loss=6.219962
I0608 01:14:34.544144 139623933597504 submission.py:139] 60) loss = 6.220, grad_norm = 14.923
I0608 01:14:35.334496 139595798804224 logging_writer.py:48] [61] global_step=61, grad_norm=18.242439, loss=6.781334
I0608 01:14:35.337826 139623933597504 submission.py:139] 61) loss = 6.781, grad_norm = 18.242
I0608 01:14:36.125468 139596216469248 logging_writer.py:48] [62] global_step=62, grad_norm=22.963667, loss=6.438721
I0608 01:14:36.128906 139623933597504 submission.py:139] 62) loss = 6.439, grad_norm = 22.964
I0608 01:14:36.918356 139595798804224 logging_writer.py:48] [63] global_step=63, grad_norm=21.448177, loss=7.584050
I0608 01:14:36.922513 139623933597504 submission.py:139] 63) loss = 7.584, grad_norm = 21.448
I0608 01:14:37.715134 139596216469248 logging_writer.py:48] [64] global_step=64, grad_norm=4.703860, loss=6.022413
I0608 01:14:37.718859 139623933597504 submission.py:139] 64) loss = 6.022, grad_norm = 4.704
I0608 01:14:38.510793 139595798804224 logging_writer.py:48] [65] global_step=65, grad_norm=0.657930, loss=6.038043
I0608 01:14:38.514783 139623933597504 submission.py:139] 65) loss = 6.038, grad_norm = 0.658
I0608 01:14:39.303765 139596216469248 logging_writer.py:48] [66] global_step=66, grad_norm=3.489646, loss=5.996667
I0608 01:14:39.307382 139623933597504 submission.py:139] 66) loss = 5.997, grad_norm = 3.490
I0608 01:14:40.098912 139595798804224 logging_writer.py:48] [67] global_step=67, grad_norm=5.694579, loss=6.018694
I0608 01:14:40.102367 139623933597504 submission.py:139] 67) loss = 6.019, grad_norm = 5.695
I0608 01:14:40.890593 139596216469248 logging_writer.py:48] [68] global_step=68, grad_norm=15.408697, loss=6.195066
I0608 01:14:40.894522 139623933597504 submission.py:139] 68) loss = 6.195, grad_norm = 15.409
I0608 01:14:41.683796 139595798804224 logging_writer.py:48] [69] global_step=69, grad_norm=19.639519, loss=7.073745
I0608 01:14:41.687144 139623933597504 submission.py:139] 69) loss = 7.074, grad_norm = 19.640
I0608 01:14:42.478195 139596216469248 logging_writer.py:48] [70] global_step=70, grad_norm=23.518967, loss=6.428343
I0608 01:14:42.481786 139623933597504 submission.py:139] 70) loss = 6.428, grad_norm = 23.519
I0608 01:14:43.281484 139595798804224 logging_writer.py:48] [71] global_step=71, grad_norm=21.974955, loss=8.003284
I0608 01:14:43.284987 139623933597504 submission.py:139] 71) loss = 8.003, grad_norm = 21.975
I0608 01:14:44.074835 139596216469248 logging_writer.py:48] [72] global_step=72, grad_norm=2.184801, loss=5.964370
I0608 01:14:44.077838 139623933597504 submission.py:139] 72) loss = 5.964, grad_norm = 2.185
I0608 01:14:44.866832 139595798804224 logging_writer.py:48] [73] global_step=73, grad_norm=9.169232, loss=6.046672
I0608 01:14:44.870718 139623933597504 submission.py:139] 73) loss = 6.047, grad_norm = 9.169
I0608 01:14:45.661332 139596216469248 logging_writer.py:48] [74] global_step=74, grad_norm=13.929645, loss=6.330027
I0608 01:14:45.665157 139623933597504 submission.py:139] 74) loss = 6.330, grad_norm = 13.930
I0608 01:14:46.454653 139595798804224 logging_writer.py:48] [75] global_step=75, grad_norm=43.500195, loss=7.343445
I0608 01:14:46.457987 139623933597504 submission.py:139] 75) loss = 7.343, grad_norm = 43.500
I0608 01:14:47.247834 139596216469248 logging_writer.py:48] [76] global_step=76, grad_norm=23.063652, loss=11.893378
I0608 01:14:47.251204 139623933597504 submission.py:139] 76) loss = 11.893, grad_norm = 23.064
I0608 01:14:48.041160 139595798804224 logging_writer.py:48] [77] global_step=77, grad_norm=22.882946, loss=10.162663
I0608 01:14:48.044533 139623933597504 submission.py:139] 77) loss = 10.163, grad_norm = 22.883
I0608 01:14:48.837352 139596216469248 logging_writer.py:48] [78] global_step=78, grad_norm=13.480834, loss=6.317952
I0608 01:14:48.840966 139623933597504 submission.py:139] 78) loss = 6.318, grad_norm = 13.481
I0608 01:14:49.631484 139595798804224 logging_writer.py:48] [79] global_step=79, grad_norm=87.358337, loss=13.213960
I0608 01:14:49.634936 139623933597504 submission.py:139] 79) loss = 13.214, grad_norm = 87.358
I0608 01:14:50.426230 139596216469248 logging_writer.py:48] [80] global_step=80, grad_norm=22.828642, loss=18.528849
I0608 01:14:50.430186 139623933597504 submission.py:139] 80) loss = 18.529, grad_norm = 22.829
I0608 01:14:51.221875 139595798804224 logging_writer.py:48] [81] global_step=81, grad_norm=22.708839, loss=19.598124
I0608 01:14:51.225476 139623933597504 submission.py:139] 81) loss = 19.598, grad_norm = 22.709
I0608 01:14:52.016421 139596216469248 logging_writer.py:48] [82] global_step=82, grad_norm=22.565798, loss=17.859629
I0608 01:14:52.019787 139623933597504 submission.py:139] 82) loss = 17.860, grad_norm = 22.566
I0608 01:14:52.807483 139595798804224 logging_writer.py:48] [83] global_step=83, grad_norm=22.431995, loss=13.525265
I0608 01:14:52.811382 139623933597504 submission.py:139] 83) loss = 13.525, grad_norm = 22.432
I0608 01:14:53.627186 139596216469248 logging_writer.py:48] [84] global_step=84, grad_norm=18.372810, loss=6.958127
I0608 01:14:53.631136 139623933597504 submission.py:139] 84) loss = 6.958, grad_norm = 18.373
I0608 01:14:54.420592 139595798804224 logging_writer.py:48] [85] global_step=85, grad_norm=74.060440, loss=19.180307
I0608 01:14:54.424095 139623933597504 submission.py:139] 85) loss = 19.180, grad_norm = 74.060
I0608 01:14:55.215749 139596216469248 logging_writer.py:48] [86] global_step=86, grad_norm=37.780972, loss=30.325123
I0608 01:14:55.219798 139623933597504 submission.py:139] 86) loss = 30.325, grad_norm = 37.781
I0608 01:14:56.005869 139595798804224 logging_writer.py:48] [87] global_step=87, grad_norm=43.069427, loss=28.745741
I0608 01:14:56.009420 139623933597504 submission.py:139] 87) loss = 28.746, grad_norm = 43.069
I0608 01:14:56.799943 139596216469248 logging_writer.py:48] [88] global_step=88, grad_norm=26.260260, loss=19.409681
I0608 01:14:56.803365 139623933597504 submission.py:139] 88) loss = 19.410, grad_norm = 26.260
I0608 01:14:57.594802 139595798804224 logging_writer.py:48] [89] global_step=89, grad_norm=21.435900, loss=16.750757
I0608 01:14:57.598256 139623933597504 submission.py:139] 89) loss = 16.751, grad_norm = 21.436
I0608 01:14:58.390073 139596216469248 logging_writer.py:48] [90] global_step=90, grad_norm=40.148743, loss=17.905617
I0608 01:14:58.393368 139623933597504 submission.py:139] 90) loss = 17.906, grad_norm = 40.149
I0608 01:14:59.182163 139595798804224 logging_writer.py:48] [91] global_step=91, grad_norm=10.868793, loss=12.059105
I0608 01:14:59.185485 139623933597504 submission.py:139] 91) loss = 12.059, grad_norm = 10.869
I0608 01:14:59.973852 139596216469248 logging_writer.py:48] [92] global_step=92, grad_norm=59.306000, loss=11.387796
I0608 01:14:59.977129 139623933597504 submission.py:139] 92) loss = 11.388, grad_norm = 59.306
I0608 01:15:00.767115 139595798804224 logging_writer.py:48] [93] global_step=93, grad_norm=21.260695, loss=19.704914
I0608 01:15:00.770537 139623933597504 submission.py:139] 93) loss = 19.705, grad_norm = 21.261
I0608 01:15:01.562481 139596216469248 logging_writer.py:48] [94] global_step=94, grad_norm=20.983011, loss=20.579739
I0608 01:15:01.566297 139623933597504 submission.py:139] 94) loss = 20.580, grad_norm = 20.983
I0608 01:15:02.354972 139595798804224 logging_writer.py:48] [95] global_step=95, grad_norm=20.710325, loss=18.765041
I0608 01:15:02.358346 139623933597504 submission.py:139] 95) loss = 18.765, grad_norm = 20.710
I0608 01:15:03.151150 139596216469248 logging_writer.py:48] [96] global_step=96, grad_norm=20.383718, loss=14.418522
I0608 01:15:03.154429 139623933597504 submission.py:139] 96) loss = 14.419, grad_norm = 20.384
I0608 01:15:03.956279 139595798804224 logging_writer.py:48] [97] global_step=97, grad_norm=17.828842, loss=10.124034
I0608 01:15:03.959595 139623933597504 submission.py:139] 97) loss = 10.124, grad_norm = 17.829
I0608 01:15:04.750722 139596216469248 logging_writer.py:48] [98] global_step=98, grad_norm=5.976420, loss=10.037992
I0608 01:15:04.754018 139623933597504 submission.py:139] 98) loss = 10.038, grad_norm = 5.976
I0608 01:15:05.544470 139595798804224 logging_writer.py:48] [99] global_step=99, grad_norm=10.203552, loss=10.195940
I0608 01:15:05.547970 139623933597504 submission.py:139] 99) loss = 10.196, grad_norm = 10.204
I0608 01:15:06.342185 139596216469248 logging_writer.py:48] [100] global_step=100, grad_norm=12.137553, loss=10.434476
I0608 01:15:06.345350 139623933597504 submission.py:139] 100) loss = 10.434, grad_norm = 12.138
I0608 01:20:13.677357 139595798804224 logging_writer.py:48] [500] global_step=500, grad_norm=nan, loss=nan
I0608 01:20:13.681424 139623933597504 submission.py:139] 500) loss = nan, grad_norm = nan
I0608 01:26:28.828517 139596216469248 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0608 01:26:28.833141 139623933597504 submission.py:139] 1000) loss = nan, grad_norm = nan
I0608 01:32:44.750295 139596216469248 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0608 01:32:44.757781 139623933597504 submission.py:139] 1500) loss = nan, grad_norm = nan
I0608 01:39:00.179799 139595798804224 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0608 01:39:00.184477 139623933597504 submission.py:139] 2000) loss = nan, grad_norm = nan
I0608 01:45:16.124851 139595798804224 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0608 01:45:16.133201 139623933597504 submission.py:139] 2500) loss = nan, grad_norm = nan
I0608 01:51:31.408357 139595790411520 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0608 01:51:31.413393 139623933597504 submission.py:139] 3000) loss = nan, grad_norm = nan
I0608 01:53:46.973315 139623933597504 spec.py:298] Evaluating on the training split.
I0608 01:53:56.845638 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 01:54:06.226603 139623933597504 spec.py:326] Evaluating on the test split.
I0608 01:54:11.307070 139623933597504 submission_runner.py:419] Time since start: 2466.79s, 	Step: 3180, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2405.988163948059, 'total_duration': 2466.789939403534, 'accumulated_submission_time': 2405.988163948059, 'accumulated_eval_time': 58.475602865219116, 'accumulated_logging_time': 0.03152632713317871}
I0608 01:54:11.327082 139595798804224 logging_writer.py:48] [3180] accumulated_eval_time=58.475603, accumulated_logging_time=0.031526, accumulated_submission_time=2405.988164, global_step=3180, preemption_count=0, score=2405.988164, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2466.789939, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 01:58:11.611244 139595790411520 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0608 01:58:11.616135 139623933597504 submission.py:139] 3500) loss = nan, grad_norm = nan
I0608 02:04:27.079916 139595798804224 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0608 02:04:27.087296 139623933597504 submission.py:139] 4000) loss = nan, grad_norm = nan
I0608 02:10:43.104252 139595790411520 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0608 02:10:43.111563 139623933597504 submission.py:139] 4500) loss = nan, grad_norm = nan
I0608 02:16:58.770443 139595782018816 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0608 02:16:58.775903 139623933597504 submission.py:139] 5000) loss = nan, grad_norm = nan
I0608 02:23:14.794081 139595790411520 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0608 02:23:14.801148 139623933597504 submission.py:139] 5500) loss = nan, grad_norm = nan
I0608 02:29:30.301124 139595782018816 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0608 02:29:30.305732 139623933597504 submission.py:139] 6000) loss = nan, grad_norm = nan
I0608 02:34:11.375260 139623933597504 spec.py:298] Evaluating on the training split.
I0608 02:34:21.174754 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 02:34:30.544823 139623933597504 spec.py:326] Evaluating on the test split.
I0608 02:34:35.963942 139623933597504 submission_runner.py:419] Time since start: 4891.45s, 	Step: 6374, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4803.946727991104, 'total_duration': 4891.446758508682, 'accumulated_submission_time': 4803.946727991104, 'accumulated_eval_time': 83.06413769721985, 'accumulated_logging_time': 0.06089615821838379}
I0608 02:34:35.983375 139595782018816 logging_writer.py:48] [6374] accumulated_eval_time=83.064138, accumulated_logging_time=0.060896, accumulated_submission_time=4803.946728, global_step=6374, preemption_count=0, score=4803.946728, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4891.446759, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 02:36:10.997046 139595664586496 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0608 02:36:11.001313 139623933597504 submission.py:139] 6500) loss = nan, grad_norm = nan
I0608 02:42:26.598527 139595782018816 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0608 02:42:26.604918 139623933597504 submission.py:139] 7000) loss = nan, grad_norm = nan
I0608 02:48:42.857837 139595782018816 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0608 02:48:42.864523 139623933597504 submission.py:139] 7500) loss = nan, grad_norm = nan
I0608 02:54:58.038424 139595664586496 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0608 02:54:58.043825 139623933597504 submission.py:139] 8000) loss = nan, grad_norm = nan
I0608 03:01:14.218493 139595664586496 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0608 03:01:14.225725 139623933597504 submission.py:139] 8500) loss = nan, grad_norm = nan
I0608 03:07:29.416319 139595656193792 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0608 03:07:29.420645 139623933597504 submission.py:139] 9000) loss = nan, grad_norm = nan
I0608 03:13:45.878105 139595664586496 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0608 03:13:45.884626 139623933597504 submission.py:139] 9500) loss = nan, grad_norm = nan
I0608 03:14:35.995426 139623933597504 spec.py:298] Evaluating on the training split.
I0608 03:14:45.780708 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 03:14:55.152260 139623933597504 spec.py:326] Evaluating on the test split.
I0608 03:15:00.404028 139623933597504 submission_runner.py:419] Time since start: 7315.89s, 	Step: 9568, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7201.791194200516, 'total_duration': 7315.8868589401245, 'accumulated_submission_time': 7201.791194200516, 'accumulated_eval_time': 107.47243237495422, 'accumulated_logging_time': 0.08954024314880371}
I0608 03:15:00.424183 139595664586496 logging_writer.py:48] [9568] accumulated_eval_time=107.472432, accumulated_logging_time=0.089540, accumulated_submission_time=7201.791194, global_step=9568, preemption_count=0, score=7201.791194, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7315.886859, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 03:20:25.660163 139595656193792 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0608 03:20:25.664424 139623933597504 submission.py:139] 10000) loss = nan, grad_norm = nan
I0608 03:26:41.944042 139595664586496 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0608 03:26:41.950435 139623933597504 submission.py:139] 10500) loss = nan, grad_norm = nan
I0608 03:32:56.873997 139595656193792 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0608 03:32:56.878444 139623933597504 submission.py:139] 11000) loss = nan, grad_norm = nan
I0608 03:39:13.267500 139595664586496 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0608 03:39:13.274867 139623933597504 submission.py:139] 11500) loss = nan, grad_norm = nan
I0608 03:45:28.281369 139595656193792 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0608 03:45:28.286205 139623933597504 submission.py:139] 12000) loss = nan, grad_norm = nan
I0608 03:51:44.955950 139595664586496 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0608 03:51:44.963050 139623933597504 submission.py:139] 12500) loss = nan, grad_norm = nan
I0608 03:55:00.404444 139623933597504 spec.py:298] Evaluating on the training split.
I0608 03:55:10.392750 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 03:55:19.808503 139623933597504 spec.py:326] Evaluating on the test split.
I0608 03:55:25.063359 139623933597504 submission_runner.py:419] Time since start: 9740.55s, 	Step: 12762, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9599.499625205994, 'total_duration': 9740.546185016632, 'accumulated_submission_time': 9599.499625205994, 'accumulated_eval_time': 132.1310977935791, 'accumulated_logging_time': 0.1192777156829834}
I0608 03:55:25.085420 139595664586496 logging_writer.py:48] [12762] accumulated_eval_time=132.131098, accumulated_logging_time=0.119278, accumulated_submission_time=9599.499625, global_step=12762, preemption_count=0, score=9599.499625, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9740.546185, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 03:58:24.604464 139595656193792 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0608 03:58:24.608389 139623933597504 submission.py:139] 13000) loss = nan, grad_norm = nan
I0608 04:04:41.417936 139595664586496 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0608 04:04:41.426186 139623933597504 submission.py:139] 13500) loss = nan, grad_norm = nan
I0608 04:10:56.146200 139595656193792 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0608 04:10:56.151292 139623933597504 submission.py:139] 14000) loss = nan, grad_norm = nan
I0608 04:17:13.079390 139595664586496 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0608 04:17:13.089823 139623933597504 submission.py:139] 14500) loss = nan, grad_norm = nan
I0608 04:23:27.881825 139595656193792 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0608 04:23:27.920619 139623933597504 submission.py:139] 15000) loss = nan, grad_norm = nan
I0608 04:29:44.851191 139595664586496 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0608 04:29:44.858442 139623933597504 submission.py:139] 15500) loss = nan, grad_norm = nan
I0608 04:35:25.692635 139623933597504 spec.py:298] Evaluating on the training split.
I0608 04:35:35.621408 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 04:35:45.020146 139623933597504 spec.py:326] Evaluating on the test split.
I0608 04:35:50.102640 139623933597504 submission_runner.py:419] Time since start: 12165.59s, 	Step: 15956, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11997.894587516785, 'total_duration': 12165.585510730743, 'accumulated_submission_time': 11997.894587516785, 'accumulated_eval_time': 156.54112148284912, 'accumulated_logging_time': 0.1520092487335205}
I0608 04:35:50.124913 139595664586496 logging_writer.py:48] [15956] accumulated_eval_time=156.541121, accumulated_logging_time=0.152009, accumulated_submission_time=11997.894588, global_step=15956, preemption_count=0, score=11997.894588, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12165.585511, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 04:36:23.983397 139595656193792 logging_writer.py:48] [16000] global_step=16000, grad_norm=nan, loss=nan
I0608 04:36:23.987317 139623933597504 submission.py:139] 16000) loss = nan, grad_norm = nan
I0608 04:42:41.052933 139595664586496 logging_writer.py:48] [16500] global_step=16500, grad_norm=nan, loss=nan
I0608 04:42:41.060671 139623933597504 submission.py:139] 16500) loss = nan, grad_norm = nan
I0608 04:48:55.667715 139595656193792 logging_writer.py:48] [17000] global_step=17000, grad_norm=nan, loss=nan
I0608 04:48:55.695007 139623933597504 submission.py:139] 17000) loss = nan, grad_norm = nan
I0608 04:55:11.341405 139595664586496 logging_writer.py:48] [17500] global_step=17500, grad_norm=nan, loss=nan
I0608 04:55:11.347122 139623933597504 submission.py:139] 17500) loss = nan, grad_norm = nan
I0608 05:01:27.451093 139595664586496 logging_writer.py:48] [18000] global_step=18000, grad_norm=nan, loss=nan
I0608 05:01:27.458330 139623933597504 submission.py:139] 18000) loss = nan, grad_norm = nan
I0608 05:07:43.055254 139595656193792 logging_writer.py:48] [18500] global_step=18500, grad_norm=nan, loss=nan
I0608 05:07:43.083295 139623933597504 submission.py:139] 18500) loss = nan, grad_norm = nan
I0608 05:13:59.157575 139595664586496 logging_writer.py:48] [19000] global_step=19000, grad_norm=nan, loss=nan
I0608 05:13:59.164742 139623933597504 submission.py:139] 19000) loss = nan, grad_norm = nan
I0608 05:15:50.368569 139623933597504 spec.py:298] Evaluating on the training split.
I0608 05:16:00.420233 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 05:16:10.013616 139623933597504 spec.py:326] Evaluating on the test split.
I0608 05:16:15.129701 139623933597504 submission_runner.py:419] Time since start: 14590.61s, 	Step: 19149, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14395.922531366348, 'total_duration': 14590.612525463104, 'accumulated_submission_time': 14395.922531366348, 'accumulated_eval_time': 181.30218648910522, 'accumulated_logging_time': 0.18513059616088867}
I0608 05:16:15.150976 139595664586496 logging_writer.py:48] [19149] accumulated_eval_time=181.302186, accumulated_logging_time=0.185131, accumulated_submission_time=14395.922531, global_step=19149, preemption_count=0, score=14395.922531, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=14590.612525, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 05:20:39.730330 139595656193792 logging_writer.py:48] [19500] global_step=19500, grad_norm=nan, loss=nan
I0608 05:20:39.735008 139623933597504 submission.py:139] 19500) loss = nan, grad_norm = nan
I0608 05:26:54.906043 139623933597504 spec.py:298] Evaluating on the training split.
I0608 05:27:04.489347 139623933597504 spec.py:310] Evaluating on the validation split.
I0608 05:27:13.899712 139623933597504 spec.py:326] Evaluating on the test split.
I0608 05:27:19.427611 139623933597504 submission_runner.py:419] Time since start: 15254.91s, 	Step: 20000, 	{'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15035.009685516357, 'total_duration': 15254.910471439362, 'accumulated_submission_time': 15035.009685516357, 'accumulated_eval_time': 205.8240451812744, 'accumulated_logging_time': 0.21541762351989746}
I0608 05:27:19.446441 139595664586496 logging_writer.py:48] [20000] accumulated_eval_time=205.824045, accumulated_logging_time=0.215418, accumulated_submission_time=15035.009686, global_step=20000, preemption_count=0, score=15035.009686, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=15254.910471, train/ctc_loss=nan, train/wer=0.941603, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0608 05:27:19.469511 139595656193792 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15035.009686
I0608 05:27:19.938141 139623933597504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0608 05:27:20.085425 139623933597504 submission_runner.py:581] Tuning trial 1/1
I0608 05:27:20.085693 139623933597504 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0608 05:27:20.086199 139623933597504 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 30.599367896473527, 'train/wer': 2.3741242029567995, 'validation/ctc_loss': 29.31221745027125, 'validation/wer': 2.0297880558103607, 'validation/num_examples': 5348, 'test/ctc_loss': 29.40055164849345, 'test/wer': 2.100197022322426, 'test/num_examples': 2472, 'score': 7.864490032196045, 'total_duration': 42.00683093070984, 'accumulated_submission_time': 7.864490032196045, 'accumulated_eval_time': 34.14194917678833, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3180, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2405.988163948059, 'total_duration': 2466.789939403534, 'accumulated_submission_time': 2405.988163948059, 'accumulated_eval_time': 58.475602865219116, 'accumulated_logging_time': 0.03152632713317871, 'global_step': 3180, 'preemption_count': 0}), (6374, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4803.946727991104, 'total_duration': 4891.446758508682, 'accumulated_submission_time': 4803.946727991104, 'accumulated_eval_time': 83.06413769721985, 'accumulated_logging_time': 0.06089615821838379, 'global_step': 6374, 'preemption_count': 0}), (9568, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7201.791194200516, 'total_duration': 7315.8868589401245, 'accumulated_submission_time': 7201.791194200516, 'accumulated_eval_time': 107.47243237495422, 'accumulated_logging_time': 0.08954024314880371, 'global_step': 9568, 'preemption_count': 0}), (12762, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9599.499625205994, 'total_duration': 9740.546185016632, 'accumulated_submission_time': 9599.499625205994, 'accumulated_eval_time': 132.1310977935791, 'accumulated_logging_time': 0.1192777156829834, 'global_step': 12762, 'preemption_count': 0}), (15956, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11997.894587516785, 'total_duration': 12165.585510730743, 'accumulated_submission_time': 11997.894587516785, 'accumulated_eval_time': 156.54112148284912, 'accumulated_logging_time': 0.1520092487335205, 'global_step': 15956, 'preemption_count': 0}), (19149, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14395.922531366348, 'total_duration': 14590.612525463104, 'accumulated_submission_time': 14395.922531366348, 'accumulated_eval_time': 181.30218648910522, 'accumulated_logging_time': 0.18513059616088867, 'global_step': 19149, 'preemption_count': 0}), (20000, {'train/ctc_loss': nan, 'train/wer': 0.9416026678543108, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15035.009685516357, 'total_duration': 15254.910471439362, 'accumulated_submission_time': 15035.009685516357, 'accumulated_eval_time': 205.8240451812744, 'accumulated_logging_time': 0.21541762351989746, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0608 05:27:20.086305 139623933597504 submission_runner.py:584] Timing: 15035.009685516357
I0608 05:27:20.086362 139623933597504 submission_runner.py:586] Total number of evals: 8
I0608 05:27:20.086418 139623933597504 submission_runner.py:587] ====================
I0608 05:27:20.086589 139623933597504 submission_runner.py:655] Final librispeech_conformer score: 15035.009685516357
