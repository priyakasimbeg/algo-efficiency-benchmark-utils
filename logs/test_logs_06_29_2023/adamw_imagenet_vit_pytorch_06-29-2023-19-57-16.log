torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-29-2023-19-57-16.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-29 19:57:20.885792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:57:20.885790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0629 19:57:33.782714 140104357345088 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0629 19:57:33.783937 140413069969216 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0629 19:57:33.783958 139827112675136 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0629 19:57:33.784098 139893769275200 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0629 19:57:33.784388 140219273279296 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0629 19:57:33.784994 140163763431232 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0629 19:57:34.770253 140471977011008 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0629 19:57:34.772010 140105298036544 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0629 19:57:34.772329 140105298036544 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.780914 140471977011008 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.781011 140104357345088 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.781041 140413069969216 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.781070 140219273279296 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.781144 139827112675136 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.781125 140163763431232 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:57:34.781184 139893769275200 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0629 19:57:35.582497 140105298036544 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/imagenet_vit_pytorch because --overwrite was set.
I0629 19:57:35.598512 140105298036544 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/imagenet_vit_pytorch.
W0629 19:57:35.609358 140219273279296 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.609946 140471977011008 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.610867 139827112675136 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.611180 140413069969216 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.611304 140163763431232 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.612498 140104357345088 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.613225 139893769275200 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:57:35.622260 140105298036544 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0629 19:57:35.626711 140105298036544 submission_runner.py:547] Using RNG seed 1985456224
I0629 19:57:35.628044 140105298036544 submission_runner.py:556] --- Tuning run 1/1 ---
I0629 19:57:35.628158 140105298036544 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1.
I0629 19:57:35.628385 140105298036544 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0629 19:57:35.629167 140105298036544 submission_runner.py:249] Initializing dataset.
I0629 19:57:42.090431 140105298036544 submission_runner.py:256] Initializing model.
I0629 19:57:46.201678 140105298036544 submission_runner.py:268] Initializing optimizer.
I0629 19:57:46.203299 140105298036544 submission_runner.py:275] Initializing metrics bundle.
I0629 19:57:46.203431 140105298036544 submission_runner.py:292] Initializing checkpoint and logger.
I0629 19:57:46.802820 140105298036544 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0629 19:57:46.803730 140105298036544 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0629 19:57:46.845608 140105298036544 submission_runner.py:328] Starting training loop.
I0629 19:57:53.380944 140077405550336 logging_writer.py:48] [0] global_step=0, grad_norm=0.331221, loss=6.907756
I0629 19:57:53.394788 140105298036544 submission.py:119] 0) loss = 6.908, grad_norm = 0.331
I0629 19:57:53.729619 140105298036544 spec.py:298] Evaluating on the training split.
I0629 19:58:55.816931 140105298036544 spec.py:310] Evaluating on the validation split.
I0629 19:59:44.685670 140105298036544 spec.py:326] Evaluating on the test split.
I0629 19:59:44.699116 140105298036544 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0629 19:59:44.705627 140105298036544 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0629 19:59:44.764841 140105298036544 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0629 19:59:58.333083 140105298036544 submission_runner.py:424] Time since start: 131.49s, 	Step: 1, 	{'train/accuracy': 0.0020703125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00214, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0019, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.882441997528076, 'total_duration': 131.48786425590515, 'accumulated_submission_time': 6.882441997528076, 'accumulated_eval_time': 124.60341262817383, 'accumulated_logging_time': 0}
I0629 19:59:58.341197 140072473065216 logging_writer.py:48] [1] accumulated_eval_time=124.603413, accumulated_logging_time=0, accumulated_submission_time=6.882442, global_step=1, preemption_count=0, score=6.882442, test/accuracy=0.001900, test/loss=6.907755, test/num_examples=10000, total_duration=131.487864, train/accuracy=0.002070, train/loss=6.907756, validation/accuracy=0.002140, validation/loss=6.907756, validation/num_examples=50000
I0629 19:59:58.358442 140105298036544 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359629 140163763431232 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359635 139893769275200 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359645 140104357345088 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359658 140413069969216 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359704 140471977011008 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359667 140219273279296 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:58.359721 139827112675136 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:59:59.036875 140072464672512 logging_writer.py:48] [1] global_step=1, grad_norm=0.351437, loss=6.907756
I0629 19:59:59.040543 140105298036544 submission.py:119] 1) loss = 6.908, grad_norm = 0.351
I0629 19:59:59.418275 140072473065216 logging_writer.py:48] [2] global_step=2, grad_norm=0.348751, loss=6.907754
I0629 19:59:59.422434 140105298036544 submission.py:119] 2) loss = 6.908, grad_norm = 0.349
I0629 19:59:59.808531 140072464672512 logging_writer.py:48] [3] global_step=3, grad_norm=0.339663, loss=6.907751
I0629 19:59:59.813247 140105298036544 submission.py:119] 3) loss = 6.908, grad_norm = 0.340
I0629 20:00:00.199326 140072473065216 logging_writer.py:48] [4] global_step=4, grad_norm=0.341432, loss=6.907751
I0629 20:00:00.203701 140105298036544 submission.py:119] 4) loss = 6.908, grad_norm = 0.341
I0629 20:00:00.587601 140072464672512 logging_writer.py:48] [5] global_step=5, grad_norm=0.339044, loss=6.907753
I0629 20:00:00.592343 140105298036544 submission.py:119] 5) loss = 6.908, grad_norm = 0.339
I0629 20:00:00.981416 140072473065216 logging_writer.py:48] [6] global_step=6, grad_norm=0.349210, loss=6.907745
I0629 20:00:00.986280 140105298036544 submission.py:119] 6) loss = 6.908, grad_norm = 0.349
I0629 20:00:01.374972 140072464672512 logging_writer.py:48] [7] global_step=7, grad_norm=0.356637, loss=6.907743
I0629 20:00:01.379911 140105298036544 submission.py:119] 7) loss = 6.908, grad_norm = 0.357
I0629 20:00:01.772592 140072473065216 logging_writer.py:48] [8] global_step=8, grad_norm=0.350345, loss=6.907737
I0629 20:00:01.778659 140105298036544 submission.py:119] 8) loss = 6.908, grad_norm = 0.350
I0629 20:00:02.176803 140072464672512 logging_writer.py:48] [9] global_step=9, grad_norm=0.347566, loss=6.907738
I0629 20:00:02.182659 140105298036544 submission.py:119] 9) loss = 6.908, grad_norm = 0.348
I0629 20:00:02.185513 140105298036544 spec.py:298] Evaluating on the training split.
I0629 20:00:56.966258 140105298036544 spec.py:310] Evaluating on the validation split.
I0629 20:01:44.092555 140105298036544 spec.py:326] Evaluating on the test split.
I0629 20:01:45.548666 140105298036544 submission_runner.py:424] Time since start: 238.70s, 	Step: 10, 	{'train/accuracy': 0.0033984375, 'train/loss': 6.90768310546875, 'validation/accuracy': 0.00358, 'validation/loss': 6.907685, 'validation/num_examples': 50000, 'test/accuracy': 0.0035, 'test/loss': 6.907690625, 'test/num_examples': 10000, 'score': 10.71035385131836, 'total_duration': 238.7034559249878, 'accumulated_submission_time': 10.71035385131836, 'accumulated_eval_time': 227.96662783622742, 'accumulated_logging_time': 0.017787456512451172}
I0629 20:01:45.556781 140065586013952 logging_writer.py:48] [10] accumulated_eval_time=227.966628, accumulated_logging_time=0.017787, accumulated_submission_time=10.710354, global_step=10, preemption_count=0, score=10.710354, test/accuracy=0.003500, test/loss=6.907691, test/num_examples=10000, total_duration=238.703456, train/accuracy=0.003398, train/loss=6.907683, validation/accuracy=0.003580, validation/loss=6.907685, validation/num_examples=50000
I0629 20:01:45.572788 140065594406656 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=10.710354
I0629 20:01:46.124669 140105298036544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/checkpoint_10.
I0629 20:01:46.386555 140105298036544 submission_runner.py:587] Tuning trial 1/1
I0629 20:01:46.386765 140105298036544 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0629 20:01:46.387059 140105298036544 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0020703125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00214, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0019, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.882441997528076, 'total_duration': 131.48786425590515, 'accumulated_submission_time': 6.882441997528076, 'accumulated_eval_time': 124.60341262817383, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.0033984375, 'train/loss': 6.90768310546875, 'validation/accuracy': 0.00358, 'validation/loss': 6.907685, 'validation/num_examples': 50000, 'test/accuracy': 0.0035, 'test/loss': 6.907690625, 'test/num_examples': 10000, 'score': 10.71035385131836, 'total_duration': 238.7034559249878, 'accumulated_submission_time': 10.71035385131836, 'accumulated_eval_time': 227.96662783622742, 'accumulated_logging_time': 0.017787456512451172, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0629 20:01:46.387230 140105298036544 submission_runner.py:590] Timing: 10.71035385131836
I0629 20:01:46.387309 140105298036544 submission_runner.py:591] ====================
I0629 20:01:46.387399 140105298036544 submission_runner.py:659] Final imagenet_vit score: 10.71035385131836
