torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_prelaunch/adamw --overwrite=true --save_checkpoints=false --max_global_steps=6000 --torch_compile=true 2>&1 | tee -a /logs/ogbg_pytorch_09-26-2023-05-38-30.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-09-26 05:38:41.028685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 05:38:41.028697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0926 05:38:55.531404 139907112462144 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0926 05:38:55.531433 140696494057280 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0926 05:38:55.532390 140146213455680 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0926 05:38:55.532423 140403271239488 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0926 05:38:55.532486 140564145178432 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0926 05:38:55.532977 140019811829568 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0926 05:38:55.532973 140452195379008 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0926 05:38:55.543120 140146213455680 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.543234 140403271239488 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.543249 140564145178432 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.542992 140042832865088 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0926 05:38:55.543526 140042832865088 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.543815 140019811829568 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.543864 140452195379008 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.552645 139907112462144 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:55.552698 140696494057280 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 05:38:57.227491 140042832865088 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_prelaunch/adamw/ogbg_pytorch.
W0926 05:38:57.272077 140019811829568 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.272078 140146213455680 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.272124 140042832865088 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.273482 140403271239488 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.273664 139907112462144 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.273767 140452195379008 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.273942 140696494057280 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 05:38:57.274419 140564145178432 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0926 05:38:57.278700 140042832865088 submission_runner.py:507] Using RNG seed 1438381070
I0926 05:38:57.281663 140042832865088 submission_runner.py:516] --- Tuning run 1/1 ---
I0926 05:38:57.281787 140042832865088 submission_runner.py:521] Creating tuning directory at /experiment_runs/timing_prelaunch/adamw/ogbg_pytorch/trial_1.
I0926 05:38:57.283571 140042832865088 logger_utils.py:92] Saving hparams to /experiment_runs/timing_prelaunch/adamw/ogbg_pytorch/trial_1/hparams.json.
I0926 05:38:57.285609 140042832865088 submission_runner.py:191] Initializing dataset.
I0926 05:38:57.285725 140042832865088 submission_runner.py:198] Initializing model.
W0926 05:39:01.787977 140564145178432 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0926 05:39:01.787993 140019811829568 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0926 05:39:01.787993 139907112462144 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0926 05:39:01.788138 140403271239488 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0926 05:39:01.788176 140146213455680 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0926 05:39:01.788203 140696494057280 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0926 05:39:01.788232 140042832865088 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0926 05:39:01.788486 140042832865088 submission_runner.py:232] Initializing optimizer.
W0926 05:39:01.788475 140452195379008 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0926 05:39:01.789327 140042832865088 submission_runner.py:239] Initializing metrics bundle.
I0926 05:39:01.789421 140042832865088 submission_runner.py:257] Initializing checkpoint and logger.
I0926 05:39:01.790042 140042832865088 submission_runner.py:277] Saving meta data to /experiment_runs/timing_prelaunch/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0926 05:39:01.790300 140042832865088 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0926 05:39:01.790372 140042832865088 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0926 05:39:02.328904 140042832865088 submission_runner.py:280] Saving flags to /experiment_runs/timing_prelaunch/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0926 05:39:02.427438 140042832865088 submission_runner.py:290] Starting training loop.
I0926 05:39:03.027739 140042832865088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:39:03.034450 140042832865088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0926 05:39:03.462217 140042832865088 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0926 05:39:03.525690 140042832865088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:39:08.095695 140004475000576 logging_writer.py:48] [0] global_step=0, grad_norm=2.376252, loss=0.799873
I0926 05:39:08.109131 140042832865088 submission.py:120] 0) loss = 0.800, grad_norm = 2.376
I0926 05:39:08.423431 140042832865088 spec.py:321] Evaluating on the training split.
I0926 05:39:08.428066 140042832865088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:39:08.433012 140042832865088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0926 05:39:08.496649 140042832865088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:40:04.262276 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 05:40:04.265600 140042832865088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:40:04.270344 140042832865088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0926 05:40:04.333104 140042832865088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:40:48.566212 140042832865088 spec.py:349] Evaluating on the test split.
I0926 05:40:48.569993 140042832865088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:40:48.574523 140042832865088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0926 05:40:48.644248 140042832865088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 05:41:35.456422 140042832865088 submission_runner.py:381] Time since start: 153.03s, 	Step: 1, 	{'train/accuracy': 0.4533885444602805, 'train/loss': 0.7990552913917301, 'train/mean_average_precision': 0.021168002470919888, 'validation/accuracy': 0.45115356168309506, 'validation/loss': 0.7958022382758237, 'validation/mean_average_precision': 0.026436775838472752, 'validation/num_examples': 43793, 'test/accuracy': 0.4500920099789445, 'test/loss': 0.7961657805432485, 'test/mean_average_precision': 0.028030669958094934, 'test/num_examples': 43793, 'score': 5.6920037269592285, 'total_duration': 153.02934503555298, 'accumulated_submission_time': 5.6920037269592285, 'accumulated_eval_time': 147.03274297714233, 'accumulated_logging_time': 0}
I0926 05:41:35.473328 139990306195200 logging_writer.py:48] [1] accumulated_eval_time=147.032743, accumulated_logging_time=0, accumulated_submission_time=5.692004, global_step=1, preemption_count=0, score=5.692004, test/accuracy=0.450092, test/loss=0.796166, test/mean_average_precision=0.028031, test/num_examples=43793, total_duration=153.029345, train/accuracy=0.453389, train/loss=0.799055, train/mean_average_precision=0.021168, validation/accuracy=0.451154, validation/loss=0.795802, validation/mean_average_precision=0.026437, validation/num_examples=43793
I0926 05:41:36.201986 140042832865088 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.207526 139907112462144 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.207557 140403271239488 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.207795 140146213455680 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.208128 140019811829568 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.208148 140696494057280 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.208158 140564145178432 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.208182 140452195379008 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 05:41:36.266072 139990314587904 logging_writer.py:48] [1] global_step=1, grad_norm=2.382911, loss=0.800947
I0926 05:41:36.270303 140042832865088 submission.py:120] 1) loss = 0.801, grad_norm = 2.383
I0926 05:41:36.604593 139990306195200 logging_writer.py:48] [2] global_step=2, grad_norm=2.389253, loss=0.798201
I0926 05:41:36.608835 140042832865088 submission.py:120] 2) loss = 0.798, grad_norm = 2.389
I0926 05:41:36.938363 139990314587904 logging_writer.py:48] [3] global_step=3, grad_norm=2.389880, loss=0.797702
I0926 05:41:36.942849 140042832865088 submission.py:120] 3) loss = 0.798, grad_norm = 2.390
I0926 05:41:37.269347 139990306195200 logging_writer.py:48] [4] global_step=4, grad_norm=2.371250, loss=0.794705
I0926 05:41:37.273723 140042832865088 submission.py:120] 4) loss = 0.795, grad_norm = 2.371
I0926 05:41:37.601458 139990314587904 logging_writer.py:48] [5] global_step=5, grad_norm=2.368283, loss=0.792447
I0926 05:41:37.606124 140042832865088 submission.py:120] 5) loss = 0.792, grad_norm = 2.368
I0926 05:41:37.931040 139990306195200 logging_writer.py:48] [6] global_step=6, grad_norm=2.370236, loss=0.789512
I0926 05:41:37.935627 140042832865088 submission.py:120] 6) loss = 0.790, grad_norm = 2.370
I0926 05:41:38.269239 139990314587904 logging_writer.py:48] [7] global_step=7, grad_norm=2.354193, loss=0.784086
I0926 05:41:38.273620 140042832865088 submission.py:120] 7) loss = 0.784, grad_norm = 2.354
I0926 05:41:38.607659 139990306195200 logging_writer.py:48] [8] global_step=8, grad_norm=2.338408, loss=0.779901
I0926 05:41:38.611800 140042832865088 submission.py:120] 8) loss = 0.780, grad_norm = 2.338
I0926 05:41:38.943756 139990314587904 logging_writer.py:48] [9] global_step=9, grad_norm=2.362698, loss=0.775527
I0926 05:41:38.947925 140042832865088 submission.py:120] 9) loss = 0.776, grad_norm = 2.363
I0926 05:41:39.269178 139990306195200 logging_writer.py:48] [10] global_step=10, grad_norm=2.366179, loss=0.767454
I0926 05:41:39.273608 140042832865088 submission.py:120] 10) loss = 0.767, grad_norm = 2.366
I0926 05:44:20.069598 139990314587904 logging_writer.py:48] [500] global_step=500, grad_norm=0.066867, loss=0.064833
I0926 05:44:20.074502 140042832865088 submission.py:120] 500) loss = 0.065, grad_norm = 0.067
I0926 05:45:35.980718 140042832865088 spec.py:321] Evaluating on the training split.
I0926 05:46:35.532099 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 05:46:38.823300 140042832865088 spec.py:349] Evaluating on the test split.
I0926 05:46:42.085077 140042832865088 submission_runner.py:381] Time since start: 459.66s, 	Step: 730, 	{'train/accuracy': 0.9868569212537354, 'train/loss': 0.05438703650297449, 'train/mean_average_precision': 0.04676140114407111, 'validation/accuracy': 0.9841228473979607, 'validation/loss': 0.06409339969854844, 'validation/mean_average_precision': 0.04789730251573847, 'validation/num_examples': 43793, 'test/accuracy': 0.9831505278201196, 'test/loss': 0.06722436309045704, 'test/mean_average_precision': 0.05015518652452082, 'test/num_examples': 43793, 'score': 245.2103831768036, 'total_duration': 459.65817618370056, 'accumulated_submission_time': 245.2103831768036, 'accumulated_eval_time': 213.13722014427185, 'accumulated_logging_time': 0.027504682540893555}
I0926 05:46:42.099789 139990306195200 logging_writer.py:48] [730] accumulated_eval_time=213.137220, accumulated_logging_time=0.027505, accumulated_submission_time=245.210383, global_step=730, preemption_count=0, score=245.210383, test/accuracy=0.983151, test/loss=0.067224, test/mean_average_precision=0.050155, test/num_examples=43793, total_duration=459.658176, train/accuracy=0.986857, train/loss=0.054387, train/mean_average_precision=0.046761, validation/accuracy=0.984123, validation/loss=0.064093, validation/mean_average_precision=0.047897, validation/num_examples=43793
I0926 05:48:11.080911 139990314587904 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.026733, loss=0.052501
I0926 05:48:11.087420 140042832865088 submission.py:120] 1000) loss = 0.053, grad_norm = 0.027
I0926 05:50:42.870509 140042832865088 spec.py:321] Evaluating on the training split.
I0926 05:51:43.783870 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 05:51:47.167346 140042832865088 spec.py:349] Evaluating on the test split.
I0926 05:51:50.444041 140042832865088 submission_runner.py:381] Time since start: 768.02s, 	Step: 1466, 	{'train/accuracy': 0.9868212236595906, 'train/loss': 0.05146824505434848, 'train/mean_average_precision': 0.06345075633013715, 'validation/accuracy': 0.984169530578295, 'validation/loss': 0.06065887499015594, 'validation/mean_average_precision': 0.06050511923268515, 'validation/num_examples': 43793, 'test/accuracy': 0.9831450522892142, 'test/loss': 0.06387834644442189, 'test/mean_average_precision': 0.06362262156064462, 'test/num_examples': 43793, 'score': 484.8383755683899, 'total_duration': 768.0170669555664, 'accumulated_submission_time': 484.8383755683899, 'accumulated_eval_time': 280.7107136249542, 'accumulated_logging_time': 0.05521798133850098}
I0926 05:51:50.459165 139990306195200 logging_writer.py:48] [1466] accumulated_eval_time=280.710714, accumulated_logging_time=0.055218, accumulated_submission_time=484.838376, global_step=1466, preemption_count=0, score=484.838376, test/accuracy=0.983145, test/loss=0.063878, test/mean_average_precision=0.063623, test/num_examples=43793, total_duration=768.017067, train/accuracy=0.986821, train/loss=0.051468, train/mean_average_precision=0.063451, validation/accuracy=0.984170, validation/loss=0.060659, validation/mean_average_precision=0.060505, validation/num_examples=43793
I0926 05:52:02.748825 139990314587904 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.016989, loss=0.049535
I0926 05:52:02.754166 140042832865088 submission.py:120] 1500) loss = 0.050, grad_norm = 0.017
I0926 05:54:48.172540 139990306195200 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.022510, loss=0.047879
I0926 05:54:48.178533 140042832865088 submission.py:120] 2000) loss = 0.048, grad_norm = 0.023
I0926 05:55:51.032716 140042832865088 spec.py:321] Evaluating on the training split.
I0926 05:56:55.124713 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 05:56:58.489603 140042832865088 spec.py:349] Evaluating on the test split.
I0926 05:57:01.769811 140042832865088 submission_runner.py:381] Time since start: 1079.34s, 	Step: 2190, 	{'train/accuracy': 0.9876020553501498, 'train/loss': 0.04533968796173177, 'train/mean_average_precision': 0.11887440440677867, 'validation/accuracy': 0.9848299960948504, 'validation/loss': 0.05453485361169499, 'validation/mean_average_precision': 0.11224466811363258, 'validation/num_examples': 43793, 'test/accuracy': 0.9838412871035663, 'test/loss': 0.057663711003163594, 'test/mean_average_precision': 0.11285995988340203, 'test/num_examples': 43793, 'score': 724.2041809558868, 'total_duration': 1079.342854976654, 'accumulated_submission_time': 724.2041809558868, 'accumulated_eval_time': 351.44779324531555, 'accumulated_logging_time': 0.08339715003967285}
I0926 05:57:01.784808 139990314587904 logging_writer.py:48] [2190] accumulated_eval_time=351.447793, accumulated_logging_time=0.083397, accumulated_submission_time=724.204181, global_step=2190, preemption_count=0, score=724.204181, test/accuracy=0.983841, test/loss=0.057664, test/mean_average_precision=0.112860, test/num_examples=43793, total_duration=1079.342855, train/accuracy=0.987602, train/loss=0.045340, train/mean_average_precision=0.118874, validation/accuracy=0.984830, validation/loss=0.054535, validation/mean_average_precision=0.112245, validation/num_examples=43793
I0926 05:58:45.996168 139990306195200 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.019917, loss=0.045572
I0926 05:58:46.002086 140042832865088 submission.py:120] 2500) loss = 0.046, grad_norm = 0.020
I0926 06:01:02.465605 140042832865088 spec.py:321] Evaluating on the training split.
I0926 06:02:06.599323 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 06:02:09.980344 140042832865088 spec.py:349] Evaluating on the test split.
I0926 06:02:13.286631 140042832865088 submission_runner.py:381] Time since start: 1390.86s, 	Step: 2910, 	{'train/accuracy': 0.9876433579067745, 'train/loss': 0.04391902493622039, 'train/mean_average_precision': 0.14820091202551894, 'validation/accuracy': 0.9848251248064678, 'validation/loss': 0.052577781789825015, 'validation/mean_average_precision': 0.137099882894617, 'validation/num_examples': 43793, 'test/accuracy': 0.9838947788285649, 'test/loss': 0.055368042021751336, 'test/mean_average_precision': 0.13516735123534035, 'test/num_examples': 43793, 'score': 963.4344320297241, 'total_duration': 1390.859735250473, 'accumulated_submission_time': 963.4344320297241, 'accumulated_eval_time': 422.2689301967621, 'accumulated_logging_time': 0.11114764213562012}
I0926 06:02:13.301481 139990314587904 logging_writer.py:48] [2910] accumulated_eval_time=422.268930, accumulated_logging_time=0.111148, accumulated_submission_time=963.434432, global_step=2910, preemption_count=0, score=963.434432, test/accuracy=0.983895, test/loss=0.055368, test/mean_average_precision=0.135167, test/num_examples=43793, total_duration=1390.859735, train/accuracy=0.987643, train/loss=0.043919, train/mean_average_precision=0.148201, validation/accuracy=0.984825, validation/loss=0.052578, validation/mean_average_precision=0.137100, validation/num_examples=43793
I0926 06:02:43.665366 139990306195200 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.020903, loss=0.042927
I0926 06:02:43.670716 140042832865088 submission.py:120] 3000) loss = 0.043, grad_norm = 0.021
I0926 06:05:27.100567 139990314587904 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.023811, loss=0.052842
I0926 06:05:27.106869 140042832865088 submission.py:120] 3500) loss = 0.053, grad_norm = 0.024
I0926 06:06:14.111518 140042832865088 spec.py:321] Evaluating on the training split.
I0926 06:07:18.798865 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 06:07:22.158200 140042832865088 spec.py:349] Evaluating on the test split.
I0926 06:07:25.469089 140042832865088 submission_runner.py:381] Time since start: 1703.04s, 	Step: 3644, 	{'train/accuracy': 0.9880930297770747, 'train/loss': 0.04223708263279011, 'train/mean_average_precision': 0.1718429352894457, 'validation/accuracy': 0.9852058971817161, 'validation/loss': 0.05188818368532451, 'validation/mean_average_precision': 0.14986977956018988, 'validation/num_examples': 43793, 'test/accuracy': 0.9843294517435144, 'test/loss': 0.054515688764926615, 'test/mean_average_precision': 0.15305523049357161, 'test/num_examples': 43793, 'score': 1202.9169085025787, 'total_duration': 1703.042144060135, 'accumulated_submission_time': 1202.9169085025787, 'accumulated_eval_time': 493.6266059875488, 'accumulated_logging_time': 0.13926100730895996}
I0926 06:07:25.484498 139990306195200 logging_writer.py:48] [3644] accumulated_eval_time=493.626606, accumulated_logging_time=0.139261, accumulated_submission_time=1202.916909, global_step=3644, preemption_count=0, score=1202.916909, test/accuracy=0.984329, test/loss=0.054516, test/mean_average_precision=0.153055, test/num_examples=43793, total_duration=1703.042144, train/accuracy=0.988093, train/loss=0.042237, train/mean_average_precision=0.171843, validation/accuracy=0.985206, validation/loss=0.051888, validation/mean_average_precision=0.149870, validation/num_examples=43793
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0926 06:09:23.844947 139990314587904 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.037646, loss=0.046202
I0926 06:09:23.851193 140042832865088 submission.py:120] 4000) loss = 0.046, grad_norm = 0.038
I0926 06:11:26.316526 140042832865088 spec.py:321] Evaluating on the training split.
I0926 06:12:34.834908 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 06:12:38.253219 140042832865088 spec.py:349] Evaluating on the test split.
I0926 06:12:41.824660 140042832865088 submission_runner.py:381] Time since start: 2019.40s, 	Step: 4371, 	{'train/accuracy': 0.9883630009269173, 'train/loss': 0.04005145450175484, 'train/mean_average_precision': 0.19318019630699623, 'validation/accuracy': 0.9854145507007754, 'validation/loss': 0.05017255777956933, 'validation/mean_average_precision': 0.1687422490395117, 'validation/num_examples': 43793, 'test/accuracy': 0.9845699539086656, 'test/loss': 0.05287635956379394, 'test/mean_average_precision': 0.1710085439042452, 'test/num_examples': 43793, 'score': 1442.4222264289856, 'total_duration': 2019.397616147995, 'accumulated_submission_time': 1442.4222264289856, 'accumulated_eval_time': 569.1345179080963, 'accumulated_logging_time': 0.16689014434814453}
I0926 06:12:41.840276 139995725588224 logging_writer.py:48] [4371] accumulated_eval_time=569.134518, accumulated_logging_time=0.166890, accumulated_submission_time=1442.422226, global_step=4371, preemption_count=0, score=1442.422226, test/accuracy=0.984570, test/loss=0.052876, test/mean_average_precision=0.171009, test/num_examples=43793, total_duration=2019.397616, train/accuracy=0.988363, train/loss=0.040051, train/mean_average_precision=0.193180, validation/accuracy=0.985415, validation/loss=0.050173, validation/mean_average_precision=0.168742, validation/num_examples=43793
I0926 06:13:25.547834 139995733980928 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016158, loss=0.043498
I0926 06:13:25.555001 140042832865088 submission.py:120] 4500) loss = 0.043, grad_norm = 0.016
I0926 06:16:11.340630 139995725588224 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014805, loss=0.038551
I0926 06:16:11.348021 140042832865088 submission.py:120] 5000) loss = 0.039, grad_norm = 0.015
I0926 06:16:42.613135 140042832865088 spec.py:321] Evaluating on the training split.
I0926 06:17:49.477456 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 06:17:53.000072 140042832865088 spec.py:349] Evaluating on the test split.
I0926 06:17:56.424563 140042832865088 submission_runner.py:381] Time since start: 2334.00s, 	Step: 5095, 	{'train/accuracy': 0.987994322372303, 'train/loss': 0.04144732059170334, 'train/mean_average_precision': 0.2310087726931116, 'validation/accuracy': 0.9852696298713899, 'validation/loss': 0.051560107487007865, 'validation/mean_average_precision': 0.18726969916662597, 'validation/num_examples': 43793, 'test/accuracy': 0.9843901037781585, 'test/loss': 0.05469891503408097, 'test/mean_average_precision': 0.1872355117387969, 'test/num_examples': 43793, 'score': 1681.8490889072418, 'total_duration': 2333.9974682331085, 'accumulated_submission_time': 1681.8490889072418, 'accumulated_eval_time': 642.9456629753113, 'accumulated_logging_time': 0.1965329647064209}
I0926 06:17:56.441780 139995733980928 logging_writer.py:48] [5095] accumulated_eval_time=642.945663, accumulated_logging_time=0.196533, accumulated_submission_time=1681.849089, global_step=5095, preemption_count=0, score=1681.849089, test/accuracy=0.984390, test/loss=0.054699, test/mean_average_precision=0.187236, test/num_examples=43793, total_duration=2333.997468, train/accuracy=0.987994, train/loss=0.041447, train/mean_average_precision=0.231009, validation/accuracy=0.985270, validation/loss=0.051560, validation/mean_average_precision=0.187270, validation/num_examples=43793
I0926 06:20:10.867253 139995725588224 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.010758, loss=0.042299
I0926 06:20:10.874393 140042832865088 submission.py:120] 5500) loss = 0.042, grad_norm = 0.011
I0926 06:21:57.344646 140042832865088 spec.py:321] Evaluating on the training split.
I0926 06:23:05.461281 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 06:23:08.887379 140042832865088 spec.py:349] Evaluating on the test split.
I0926 06:23:12.256723 140042832865088 submission_runner.py:381] Time since start: 2649.83s, 	Step: 5828, 	{'train/accuracy': 0.9889876971100313, 'train/loss': 0.03768486535817277, 'train/mean_average_precision': 0.2585499968062631, 'validation/accuracy': 0.9858728577494486, 'validation/loss': 0.047799986624253984, 'validation/mean_average_precision': 0.2048389021168746, 'validation/num_examples': 43793, 'test/accuracy': 0.9849515562933014, 'test/loss': 0.05060124356677768, 'test/mean_average_precision': 0.20186875179451852, 'test/num_examples': 43793, 'score': 1921.2193565368652, 'total_duration': 2649.8297724723816, 'accumulated_submission_time': 1921.2193565368652, 'accumulated_eval_time': 717.8578276634216, 'accumulated_logging_time': 0.22710180282592773}
I0926 06:23:12.272469 139995733980928 logging_writer.py:48] [5828] accumulated_eval_time=717.857828, accumulated_logging_time=0.227102, accumulated_submission_time=1921.219357, global_step=5828, preemption_count=0, score=1921.219357, test/accuracy=0.984952, test/loss=0.050601, test/mean_average_precision=0.201869, test/num_examples=43793, total_duration=2649.829772, train/accuracy=0.988988, train/loss=0.037685, train/mean_average_precision=0.258550, validation/accuracy=0.985873, validation/loss=0.047800, validation/mean_average_precision=0.204839, validation/num_examples=43793
I0926 06:24:09.238718 140042832865088 spec.py:321] Evaluating on the training split.
I0926 06:25:16.246007 140042832865088 spec.py:333] Evaluating on the validation split.
I0926 06:25:19.724059 140042832865088 spec.py:349] Evaluating on the test split.
I0926 06:25:23.069862 140042832865088 submission_runner.py:381] Time since start: 2780.64s, 	Step: 6000, 	{'train/accuracy': 0.9884746422814832, 'train/loss': 0.03888852932643394, 'train/mean_average_precision': 0.23767422712327751, 'validation/accuracy': 0.9855935705488399, 'validation/loss': 0.048829842319439605, 'validation/mean_average_precision': 0.19759428614990313, 'validation/num_examples': 43793, 'test/accuracy': 0.9847064209866149, 'test/loss': 0.05146736791545275, 'test/mean_average_precision': 0.1999208677453865, 'test/num_examples': 43793, 'score': 1976.8015427589417, 'total_duration': 2780.6429376602173, 'accumulated_submission_time': 1976.8015427589417, 'accumulated_eval_time': 791.6889886856079, 'accumulated_logging_time': 0.2568328380584717}
I0926 06:25:23.087547 139995725588224 logging_writer.py:48] [6000] accumulated_eval_time=791.688989, accumulated_logging_time=0.256833, accumulated_submission_time=1976.801543, global_step=6000, preemption_count=0, score=1976.801543, test/accuracy=0.984706, test/loss=0.051467, test/mean_average_precision=0.199921, test/num_examples=43793, total_duration=2780.642938, train/accuracy=0.988475, train/loss=0.038889, train/mean_average_precision=0.237674, validation/accuracy=0.985594, validation/loss=0.048830, validation/mean_average_precision=0.197594, validation/num_examples=43793
I0926 06:25:23.826510 139995733980928 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1976.801543
I0926 06:25:23.940313 140042832865088 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_prelaunch/adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0926 06:25:24.129611 140042832865088 submission_runner.py:549] Tuning trial 1/1
I0926 06:25:24.129868 140042832865088 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0926 06:25:24.131076 140042832865088 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4533885444602805, 'train/loss': 0.7990552913917301, 'train/mean_average_precision': 0.021168002470919888, 'validation/accuracy': 0.45115356168309506, 'validation/loss': 0.7958022382758237, 'validation/mean_average_precision': 0.026436775838472752, 'validation/num_examples': 43793, 'test/accuracy': 0.4500920099789445, 'test/loss': 0.7961657805432485, 'test/mean_average_precision': 0.028030669958094934, 'test/num_examples': 43793, 'score': 5.6920037269592285, 'total_duration': 153.02934503555298, 'accumulated_submission_time': 5.6920037269592285, 'accumulated_eval_time': 147.03274297714233, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (730, {'train/accuracy': 0.9868569212537354, 'train/loss': 0.05438703650297449, 'train/mean_average_precision': 0.04676140114407111, 'validation/accuracy': 0.9841228473979607, 'validation/loss': 0.06409339969854844, 'validation/mean_average_precision': 0.04789730251573847, 'validation/num_examples': 43793, 'test/accuracy': 0.9831505278201196, 'test/loss': 0.06722436309045704, 'test/mean_average_precision': 0.05015518652452082, 'test/num_examples': 43793, 'score': 245.2103831768036, 'total_duration': 459.65817618370056, 'accumulated_submission_time': 245.2103831768036, 'accumulated_eval_time': 213.13722014427185, 'accumulated_logging_time': 0.027504682540893555, 'global_step': 730, 'preemption_count': 0}), (1466, {'train/accuracy': 0.9868212236595906, 'train/loss': 0.05146824505434848, 'train/mean_average_precision': 0.06345075633013715, 'validation/accuracy': 0.984169530578295, 'validation/loss': 0.06065887499015594, 'validation/mean_average_precision': 0.06050511923268515, 'validation/num_examples': 43793, 'test/accuracy': 0.9831450522892142, 'test/loss': 0.06387834644442189, 'test/mean_average_precision': 0.06362262156064462, 'test/num_examples': 43793, 'score': 484.8383755683899, 'total_duration': 768.0170669555664, 'accumulated_submission_time': 484.8383755683899, 'accumulated_eval_time': 280.7107136249542, 'accumulated_logging_time': 0.05521798133850098, 'global_step': 1466, 'preemption_count': 0}), (2190, {'train/accuracy': 0.9876020553501498, 'train/loss': 0.04533968796173177, 'train/mean_average_precision': 0.11887440440677867, 'validation/accuracy': 0.9848299960948504, 'validation/loss': 0.05453485361169499, 'validation/mean_average_precision': 0.11224466811363258, 'validation/num_examples': 43793, 'test/accuracy': 0.9838412871035663, 'test/loss': 0.057663711003163594, 'test/mean_average_precision': 0.11285995988340203, 'test/num_examples': 43793, 'score': 724.2041809558868, 'total_duration': 1079.342854976654, 'accumulated_submission_time': 724.2041809558868, 'accumulated_eval_time': 351.44779324531555, 'accumulated_logging_time': 0.08339715003967285, 'global_step': 2190, 'preemption_count': 0}), (2910, {'train/accuracy': 0.9876433579067745, 'train/loss': 0.04391902493622039, 'train/mean_average_precision': 0.14820091202551894, 'validation/accuracy': 0.9848251248064678, 'validation/loss': 0.052577781789825015, 'validation/mean_average_precision': 0.137099882894617, 'validation/num_examples': 43793, 'test/accuracy': 0.9838947788285649, 'test/loss': 0.055368042021751336, 'test/mean_average_precision': 0.13516735123534035, 'test/num_examples': 43793, 'score': 963.4344320297241, 'total_duration': 1390.859735250473, 'accumulated_submission_time': 963.4344320297241, 'accumulated_eval_time': 422.2689301967621, 'accumulated_logging_time': 0.11114764213562012, 'global_step': 2910, 'preemption_count': 0}), (3644, {'train/accuracy': 0.9880930297770747, 'train/loss': 0.04223708263279011, 'train/mean_average_precision': 0.1718429352894457, 'validation/accuracy': 0.9852058971817161, 'validation/loss': 0.05188818368532451, 'validation/mean_average_precision': 0.14986977956018988, 'validation/num_examples': 43793, 'test/accuracy': 0.9843294517435144, 'test/loss': 0.054515688764926615, 'test/mean_average_precision': 0.15305523049357161, 'test/num_examples': 43793, 'score': 1202.9169085025787, 'total_duration': 1703.042144060135, 'accumulated_submission_time': 1202.9169085025787, 'accumulated_eval_time': 493.6266059875488, 'accumulated_logging_time': 0.13926100730895996, 'global_step': 3644, 'preemption_count': 0}), (4371, {'train/accuracy': 0.9883630009269173, 'train/loss': 0.04005145450175484, 'train/mean_average_precision': 0.19318019630699623, 'validation/accuracy': 0.9854145507007754, 'validation/loss': 0.05017255777956933, 'validation/mean_average_precision': 0.1687422490395117, 'validation/num_examples': 43793, 'test/accuracy': 0.9845699539086656, 'test/loss': 0.05287635956379394, 'test/mean_average_precision': 0.1710085439042452, 'test/num_examples': 43793, 'score': 1442.4222264289856, 'total_duration': 2019.397616147995, 'accumulated_submission_time': 1442.4222264289856, 'accumulated_eval_time': 569.1345179080963, 'accumulated_logging_time': 0.16689014434814453, 'global_step': 4371, 'preemption_count': 0}), (5095, {'train/accuracy': 0.987994322372303, 'train/loss': 0.04144732059170334, 'train/mean_average_precision': 0.2310087726931116, 'validation/accuracy': 0.9852696298713899, 'validation/loss': 0.051560107487007865, 'validation/mean_average_precision': 0.18726969916662597, 'validation/num_examples': 43793, 'test/accuracy': 0.9843901037781585, 'test/loss': 0.05469891503408097, 'test/mean_average_precision': 0.1872355117387969, 'test/num_examples': 43793, 'score': 1681.8490889072418, 'total_duration': 2333.9974682331085, 'accumulated_submission_time': 1681.8490889072418, 'accumulated_eval_time': 642.9456629753113, 'accumulated_logging_time': 0.1965329647064209, 'global_step': 5095, 'preemption_count': 0}), (5828, {'train/accuracy': 0.9889876971100313, 'train/loss': 0.03768486535817277, 'train/mean_average_precision': 0.2585499968062631, 'validation/accuracy': 0.9858728577494486, 'validation/loss': 0.047799986624253984, 'validation/mean_average_precision': 0.2048389021168746, 'validation/num_examples': 43793, 'test/accuracy': 0.9849515562933014, 'test/loss': 0.05060124356677768, 'test/mean_average_precision': 0.20186875179451852, 'test/num_examples': 43793, 'score': 1921.2193565368652, 'total_duration': 2649.8297724723816, 'accumulated_submission_time': 1921.2193565368652, 'accumulated_eval_time': 717.8578276634216, 'accumulated_logging_time': 0.22710180282592773, 'global_step': 5828, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9884746422814832, 'train/loss': 0.03888852932643394, 'train/mean_average_precision': 0.23767422712327751, 'validation/accuracy': 0.9855935705488399, 'validation/loss': 0.048829842319439605, 'validation/mean_average_precision': 0.19759428614990313, 'validation/num_examples': 43793, 'test/accuracy': 0.9847064209866149, 'test/loss': 0.05146736791545275, 'test/mean_average_precision': 0.1999208677453865, 'test/num_examples': 43793, 'score': 1976.8015427589417, 'total_duration': 2780.6429376602173, 'accumulated_submission_time': 1976.8015427589417, 'accumulated_eval_time': 791.6889886856079, 'accumulated_logging_time': 0.2568328380584717, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0926 06:25:24.131260 140042832865088 submission_runner.py:552] Timing: 1976.8015427589417
I0926 06:25:24.131354 140042832865088 submission_runner.py:554] Total number of evals: 10
I0926 06:25:24.131433 140042832865088 submission_runner.py:555] ====================
I0926 06:25:24.131600 140042832865088 submission_runner.py:625] Final ogbg score: 1976.8015427589417
