torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_prelaunch/adamw --overwrite=true --save_checkpoints=false --max_global_steps=2400 --torch_compile=true 2>&1 | tee -a /logs/criteo1tb_pytorch_10-03-2023-23-40-47.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-10-03 23:40:57.123463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-03 23:40:57.123472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1003 23:41:11.617786 140271068911424 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I1003 23:41:11.617803 139727095072576 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I1003 23:41:11.617824 139879683770176 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I1003 23:41:11.617784 140620982802240 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I1003 23:41:11.618864 140634149807936 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I1003 23:41:11.619497 139945715451712 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I1003 23:41:12.590565 139664581920576 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I1003 23:41:12.595905 140664004405056 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I1003 23:41:12.596428 140664004405056 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.600531 139945715451712 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.601558 139664581920576 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.605217 140271068911424 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.605259 139727095072576 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.605298 139879683770176 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.605231 140620982802240 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.605320 140634149807936 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1003 23:41:12.623972 140664004405056 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch because --overwrite was set.
W1003 23:41:12.655241 140634149807936 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1003 23:41:12.655463 139727095072576 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1003 23:41:12.656063 140620982802240 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1003 23:41:12.656237 139879683770176 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1003 23:41:12.656346 140271068911424 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1003 23:41:12.656641 139945715451712 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1003 23:41:12.656817 139664581920576 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1003 23:41:12.659729 140664004405056 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch.
W1003 23:41:12.691072 140664004405056 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1003 23:41:12.696391 140664004405056 submission_runner.py:507] Using RNG seed 167978596
I1003 23:41:12.697851 140664004405056 submission_runner.py:516] --- Tuning run 1/1 ---
I1003 23:41:12.698010 140664004405056 submission_runner.py:521] Creating tuning directory at /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch/trial_1.
I1003 23:41:12.698212 140664004405056 logger_utils.py:92] Saving hparams to /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch/trial_1/hparams.json.
I1003 23:41:12.699148 140664004405056 submission_runner.py:191] Initializing dataset.
I1003 23:41:12.699289 140664004405056 submission_runner.py:198] Initializing model.
W1003 23:41:23.240651 139945715451712 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240651 140620982802240 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240651 139664581920576 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240652 140271068911424 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240671 139879683770176 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240756 140664004405056 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240768 139727095072576 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W1003 23:41:23.240768 140634149807936 submission_runner.py:215] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I1003 23:41:23.240977 140664004405056 submission_runner.py:232] Initializing optimizer.
I1003 23:41:23.241571 140664004405056 submission_runner.py:239] Initializing metrics bundle.
I1003 23:41:23.241670 140664004405056 submission_runner.py:257] Initializing checkpoint and logger.
I1003 23:41:23.242413 140664004405056 submission_runner.py:277] Saving meta data to /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I1003 23:41:23.242672 140664004405056 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1003 23:41:23.242742 140664004405056 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1003 23:41:23.673855 140664004405056 submission_runner.py:280] Saving flags to /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch/trial_1/flags_0.json.
I1003 23:41:23.765795 140664004405056 submission_runner.py:290] Starting training loop.
I1003 23:41:28.899204 140622228862720 logging_writer.py:48] [0] global_step=0, grad_norm=3.753580, loss=0.386781
I1003 23:41:28.909319 140664004405056 submission.py:120] 0) loss = 0.387, grad_norm = 3.754
I1003 23:41:29.243344 140664004405056 spec.py:321] Evaluating on the training split.
I1003 23:45:14.789888 140664004405056 spec.py:333] Evaluating on the validation split.
I1003 23:49:01.784834 140664004405056 spec.py:349] Evaluating on the test split.
I1003 23:53:01.187733 140664004405056 submission_runner.py:381] Time since start: 697.42s, 	Step: 1, 	{'train/loss': 0.38689082556013066, 'validation/loss': 0.3863260224719101, 'validation/num_examples': 89000000, 'test/loss': 0.3877247241005304, 'test/num_examples': 89274637, 'score': 5.144791126251221, 'total_duration': 697.4223875999451, 'accumulated_submission_time': 5.144791126251221, 'accumulated_eval_time': 691.9444887638092, 'accumulated_logging_time': 0}
I1003 23:53:01.208444 140599655040768 logging_writer.py:48] [1] accumulated_eval_time=691.944489, accumulated_logging_time=0, accumulated_submission_time=5.144791, global_step=1, preemption_count=0, score=5.144791, test/loss=0.387725, test/num_examples=89274637, total_duration=697.422388, train/loss=0.386891, validation/loss=0.386326, validation/num_examples=89000000
I1003 23:53:01.554145 140664004405056 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554226 139879683770176 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554180 140271068911424 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554224 140634149807936 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554276 140620982802240 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554273 139727095072576 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554359 139945715451712 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.554429 139664581920576 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1003 23:53:01.812044 140599646648064 logging_writer.py:48] [1] global_step=1, grad_norm=3.748255, loss=0.386848
I1003 23:53:01.816538 140664004405056 submission.py:120] 1) loss = 0.387, grad_norm = 3.748
I1003 23:53:02.035944 140599655040768 logging_writer.py:48] [2] global_step=2, grad_norm=3.684927, loss=0.381834
I1003 23:53:02.039933 140664004405056 submission.py:120] 2) loss = 0.382, grad_norm = 3.685
I1003 23:53:02.255900 140599646648064 logging_writer.py:48] [3] global_step=3, grad_norm=3.562805, loss=0.371122
I1003 23:53:02.259725 140664004405056 submission.py:120] 3) loss = 0.371, grad_norm = 3.563
I1003 23:53:02.475178 140599655040768 logging_writer.py:48] [4] global_step=4, grad_norm=3.377764, loss=0.356190
I1003 23:53:02.479376 140664004405056 submission.py:120] 4) loss = 0.356, grad_norm = 3.378
I1003 23:53:02.695534 140599646648064 logging_writer.py:48] [5] global_step=5, grad_norm=3.143657, loss=0.337421
I1003 23:53:02.699848 140664004405056 submission.py:120] 5) loss = 0.337, grad_norm = 3.144
I1003 23:53:02.916271 140599655040768 logging_writer.py:48] [6] global_step=6, grad_norm=2.875888, loss=0.316190
I1003 23:53:02.920268 140664004405056 submission.py:120] 6) loss = 0.316, grad_norm = 2.876
I1003 23:53:03.136363 140599646648064 logging_writer.py:48] [7] global_step=7, grad_norm=2.599283, loss=0.293636
I1003 23:53:03.141230 140664004405056 submission.py:120] 7) loss = 0.294, grad_norm = 2.599
I1003 23:53:03.356845 140599655040768 logging_writer.py:48] [8] global_step=8, grad_norm=2.319414, loss=0.270664
I1003 23:53:03.360477 140664004405056 submission.py:120] 8) loss = 0.271, grad_norm = 2.319
I1003 23:53:03.576967 140599646648064 logging_writer.py:48] [9] global_step=9, grad_norm=2.023341, loss=0.247608
I1003 23:53:03.580343 140664004405056 submission.py:120] 9) loss = 0.248, grad_norm = 2.023
I1003 23:53:03.795846 140599655040768 logging_writer.py:48] [10] global_step=10, grad_norm=1.746500, loss=0.224902
I1003 23:53:03.799008 140664004405056 submission.py:120] 10) loss = 0.225, grad_norm = 1.746
I1003 23:58:29.175410 140599646648064 logging_writer.py:48] [500] global_step=500, grad_norm=0.022934, loss=0.124994
I1003 23:58:29.178999 140664004405056 submission.py:120] 500) loss = 0.125, grad_norm = 0.023
I1004 00:04:20.410304 140599655040768 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.014274, loss=0.129136
I1004 00:04:20.414245 140664004405056 submission.py:120] 1000) loss = 0.129, grad_norm = 0.014
I1004 00:10:11.479241 140599646648064 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.033752, loss=0.120623
I1004 00:10:11.482980 140664004405056 submission.py:120] 1500) loss = 0.121, grad_norm = 0.034
I1004 00:13:01.793718 140664004405056 spec.py:321] Evaluating on the training split.
I1004 00:16:50.176520 140664004405056 spec.py:333] Evaluating on the validation split.
I1004 00:19:38.628759 140664004405056 spec.py:349] Evaluating on the test split.
I1004 00:22:32.707031 140664004405056 submission_runner.py:381] Time since start: 2468.94s, 	Step: 1748, 	{'train/loss': 0.12377626421173399, 'validation/loss': 0.1253312584269663, 'validation/num_examples': 89000000, 'test/loss': 0.12788337632781413, 'test/num_examples': 89274637, 'score': 1204.1456191539764, 'total_duration': 2468.9417316913605, 'accumulated_submission_time': 1204.1456191539764, 'accumulated_eval_time': 1262.8579723834991, 'accumulated_logging_time': 0.02820277214050293}
I1004 00:22:32.720155 140599655040768 logging_writer.py:48] [1748] accumulated_eval_time=1262.857972, accumulated_logging_time=0.028203, accumulated_submission_time=1204.145619, global_step=1748, preemption_count=0, score=1204.145619, test/loss=0.127883, test/num_examples=89274637, total_duration=2468.941732, train/loss=0.123776, validation/loss=0.125331, validation/num_examples=89000000
I1004 00:25:15.717820 140599646648064 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.023879, loss=0.128273
I1004 00:25:15.721352 140664004405056 submission.py:120] 2000) loss = 0.128, grad_norm = 0.024
I1004 00:29:56.225865 140664004405056 spec.py:321] Evaluating on the training split.
I1004 00:33:35.694841 140664004405056 spec.py:333] Evaluating on the validation split.
I1004 00:36:24.888959 140664004405056 spec.py:349] Evaluating on the test split.
I1004 00:39:19.281724 140664004405056 submission_runner.py:381] Time since start: 3475.52s, 	Step: 2400, 	{'train/loss': 0.12362897786578142, 'validation/loss': 0.12500773033707865, 'validation/num_examples': 89000000, 'test/loss': 0.12776105715221223, 'test/num_examples': 89274637, 'score': 1646.8132498264313, 'total_duration': 3475.51642370224, 'accumulated_submission_time': 1646.8132498264313, 'accumulated_eval_time': 1825.9138717651367, 'accumulated_logging_time': 0.04899001121520996}
I1004 00:39:19.296116 140599655040768 logging_writer.py:48] [2400] accumulated_eval_time=1825.913872, accumulated_logging_time=0.048990, accumulated_submission_time=1646.813250, global_step=2400, preemption_count=0, score=1646.813250, test/loss=0.127761, test/num_examples=89274637, total_duration=3475.516424, train/loss=0.123629, validation/loss=0.125008, validation/num_examples=89000000
I1004 00:39:19.566777 140599646648064 logging_writer.py:48] [2400] global_step=2400, preemption_count=0, score=1646.813250
I1004 00:39:29.787518 140664004405056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_prelaunch/adamw/criteo1tb_pytorch/trial_1/checkpoint_2400.
I1004 00:39:29.862784 140664004405056 submission_runner.py:549] Tuning trial 1/1
I1004 00:39:29.862988 140664004405056 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I1004 00:39:29.863748 140664004405056 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.38689082556013066, 'validation/loss': 0.3863260224719101, 'validation/num_examples': 89000000, 'test/loss': 0.3877247241005304, 'test/num_examples': 89274637, 'score': 5.144791126251221, 'total_duration': 697.4223875999451, 'accumulated_submission_time': 5.144791126251221, 'accumulated_eval_time': 691.9444887638092, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1748, {'train/loss': 0.12377626421173399, 'validation/loss': 0.1253312584269663, 'validation/num_examples': 89000000, 'test/loss': 0.12788337632781413, 'test/num_examples': 89274637, 'score': 1204.1456191539764, 'total_duration': 2468.9417316913605, 'accumulated_submission_time': 1204.1456191539764, 'accumulated_eval_time': 1262.8579723834991, 'accumulated_logging_time': 0.02820277214050293, 'global_step': 1748, 'preemption_count': 0}), (2400, {'train/loss': 0.12362897786578142, 'validation/loss': 0.12500773033707865, 'validation/num_examples': 89000000, 'test/loss': 0.12776105715221223, 'test/num_examples': 89274637, 'score': 1646.8132498264313, 'total_duration': 3475.51642370224, 'accumulated_submission_time': 1646.8132498264313, 'accumulated_eval_time': 1825.9138717651367, 'accumulated_logging_time': 0.04899001121520996, 'global_step': 2400, 'preemption_count': 0})], 'global_step': 2400}
I1004 00:39:29.863835 140664004405056 submission_runner.py:552] Timing: 1646.8132498264313
I1004 00:39:29.863889 140664004405056 submission_runner.py:554] Total number of evals: 3
I1004 00:39:29.863937 140664004405056 submission_runner.py:555] ====================
I1004 00:39:29.864013 140664004405056 submission_runner.py:625] Final criteo1tb score: 1646.8132498264313
