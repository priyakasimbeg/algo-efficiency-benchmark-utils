torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_prelaunch/adamw --overwrite=true --save_checkpoints=false --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=true 2>&1 | tee -a /logs/imagenet_resnet_pytorch_09-26-2023-00-59-53.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-09-26 01:00:03.006472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-26 01:00:03.006491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0926 01:00:17.155423 139726288025408 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0926 01:00:17.155388 140422881253184 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0926 01:00:17.155519 140625127495488 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0926 01:00:17.156426 139645281040192 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0926 01:00:17.156524 139776650663744 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0926 01:00:17.156594 139779978749760 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0926 01:00:17.157244 140553053583168 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0926 01:00:17.167343 139779978749760 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.167473 139776650663744 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.167242 140656325334848 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0926 01:00:17.167823 140656325334848 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.167998 140553053583168 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.176705 140422881253184 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.176731 139726288025408 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.176775 140625127495488 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0926 01:00:17.177351 139645281040192 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0926 01:00:18.366503 140656325334848 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_prelaunch/adamw/imagenet_resnet_pytorch.
W0926 01:00:18.407427 140422881253184 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.407427 139726288025408 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.407834 139779978749760 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.407956 139776650663744 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.408089 140553053583168 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.408529 139645281040192 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.409058 140625127495488 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0926 01:00:18.410609 140656325334848 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0926 01:00:18.415578 140656325334848 submission_runner.py:507] Using RNG seed 2393347833
I0926 01:00:18.417003 140656325334848 submission_runner.py:516] --- Tuning run 1/1 ---
I0926 01:00:18.417126 140656325334848 submission_runner.py:521] Creating tuning directory at /experiment_runs/timing_prelaunch/adamw/imagenet_resnet_pytorch/trial_1.
I0926 01:00:18.417636 140656325334848 logger_utils.py:92] Saving hparams to /experiment_runs/timing_prelaunch/adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0926 01:00:18.418467 140656325334848 submission_runner.py:191] Initializing dataset.
I0926 01:00:24.790323 140656325334848 submission_runner.py:198] Initializing model.
I0926 01:00:29.564636 140656325334848 submission_runner.py:229] Performing `torch.compile`.
I0926 01:00:30.158106 140656325334848 submission_runner.py:232] Initializing optimizer.
I0926 01:00:30.159630 140656325334848 submission_runner.py:239] Initializing metrics bundle.
I0926 01:00:30.159765 140656325334848 submission_runner.py:257] Initializing checkpoint and logger.
I0926 01:00:30.160360 140656325334848 submission_runner.py:277] Saving meta data to /experiment_runs/timing_prelaunch/adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0926 01:00:30.662246 140656325334848 submission_runner.py:280] Saving flags to /experiment_runs/timing_prelaunch/adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0926 01:00:30.752344 140656325334848 submission_runner.py:290] Starting training loop.
[2023-09-26 01:00:32,892] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:32,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:32,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:33,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:33,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:33,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:33,199] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:33,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:00:34,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:34,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:34,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:34,945] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:34,946] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:34,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:35,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,004] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,005] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:35,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:35,039] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:35,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:35,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,065] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,066] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:35,084] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,087] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,088] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:35,120] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,124] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,125] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:35,128] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,131] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,132] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:35,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:35,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:00:35,238] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,240] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:00:35,242] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,243] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:35,243] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:00:35,244] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:00:45,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:45,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:45,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:45,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:45,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:45,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:46,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:46,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-26 01:00:54,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:54,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:54,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:54,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:54,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:54,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:55,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:55,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-26 01:00:59,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,453] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:00:59,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-26 01:01:01,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:01,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-26 01:01:02,566] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,570] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:02,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-26 01:01:03,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:03,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-26 01:01:04,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,831] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:04,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-26 01:01:06,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,390] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,466] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,470] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,503] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-26 01:01:06,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,599] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,623] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,627] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:06,665] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-26 01:01:06,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-26 01:01:06,676] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:12,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-26 01:01:12,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:12,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:12,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:12,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:13,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:13,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:13,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:13,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:13,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-26 01:01:13,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-26 01:01:13,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,466] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:14,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-26 01:01:15,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:15,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-26 01:01:16,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:16,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-26 01:01:17,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:17,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:18,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:18,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:18,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:18,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:18,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:18,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-26 01:01:19,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:19,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:20,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:20,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:20,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:20,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:20,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:20,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-26 01:01:21,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:21,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-26 01:01:26,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:26,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:27,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:27,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:27,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:27,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:27,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-26 01:01:27,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0926 01:01:30.833301 140627319473920 logging_writer.py:48] [0] global_step=0, grad_norm=0.583602, loss=6.916740
I0926 01:01:30.861659 140656325334848 submission.py:120] 0) loss = 6.917, grad_norm = 0.584
I0926 01:01:31.487780 140656325334848 spec.py:321] Evaluating on the training split.
[2023-09-26 01:01:41,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,336] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,451] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,483] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:41,534] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:01:42,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,622] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,625] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,626] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,701] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,705] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,705] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,718] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,807] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,827] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,830] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,837] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,840] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,840] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,858] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,862] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,862] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,884] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,887] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,888] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:42,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:01:42,919] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:01:42,922] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:01:42,922] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:01:45,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:45,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-26 01:01:48,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:48,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:48,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:48,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:48,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:48,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:49,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:49,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-26 01:01:50,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:50,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:50,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:50,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:50,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:51,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:51,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:51,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-26 01:01:51,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-26 01:01:52,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:52,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:52,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:52,937] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:52,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:52,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:53,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:53,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-26 01:01:53,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-26 01:01:53,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:53,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-26 01:01:54,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:54,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:54,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:54,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:54,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:54,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:54,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:54,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:54,785] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:54,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:54,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:54,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:54,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:54,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:54,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:54,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:54,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:55,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:55,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:55,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:55,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:55,034] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:01:55,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-26 01:01:55,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-26 01:01:55,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-26 01:01:55,221] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0926 01:02:46.391247 140656325334848 spec.py:333] Evaluating on the validation split.
[2023-09-26 01:03:37,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:37,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:37,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:38,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:38,509] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:38,512] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:38,512] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:38,649] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:39,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:39,122] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:39,125] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:39,126] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:39,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:39,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:39,214] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:39,214] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:39,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:39,992] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:39,995] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:39,996] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:40,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:40,891] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:41,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:41,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:41,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:41,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:42,264] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:42,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:42,285] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:42,289] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:42,289] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:42,304] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:42,308] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:42,308] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:42,335] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:42,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:42,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:42,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:42,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:42,738] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:43,785] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:03:43,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:03:43,809] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:03:43,809] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:03:43,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:44,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:44,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:45,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:45,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:45,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:45,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:46,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:46,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-26 01:03:46,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:46,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:47,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:47,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:48,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:48,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:48,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:48,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:48,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:48,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:48,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:48,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:48,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:49,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:49,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:49,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:49,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:49,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-26 01:03:49,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:49,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:49,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:49,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:50,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:50,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:50,046] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:50,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:50,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:50,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:50,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:50,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:50,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:50,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:50,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:50,669] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:50,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:50,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:50,685] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:50,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:51,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:51,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:51,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:51,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:51,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-26 01:03:51,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:51,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:51,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:52,305] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:52,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:52,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:52,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:52,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-26 01:03:53,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:53,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:53,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:53,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-26 01:03:53,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:53,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:54,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:54,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-26 01:03:54,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:54,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:54,166] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:54,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:54,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:54,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:54,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:54,335] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:54,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:54,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:54,412] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:03:54,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-26 01:03:55,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-26 01:03:55,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-26 01:03:55,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-26 01:03:55,386] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0926 01:03:56.607692 140656325334848 spec.py:349] Evaluating on the test split.
I0926 01:03:56.623528 140656325334848 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0926 01:03:56.630349 140656325334848 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0926 01:03:56.701200 140656325334848 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-09-26 01:03:58,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:58,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:58,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:58,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:58,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:59,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:59,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:03:59,827] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-26 01:04:01,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:01,653] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:01,658] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:01,659] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:01,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:01,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:01,851] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:01,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:02,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:02,069] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:02,072] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:02,072] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:02,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:02,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:02,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:02,102] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:02,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:02,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:02,172] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:02,173] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:02,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:02,221] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:02,227] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:02,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:02,231] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:02,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:02,244] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:02,244] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:02,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-26 01:04:02,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-26 01:04:02,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-26 01:04:02,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-26 01:04:04,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:04,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:04,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:04,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:04,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:04,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:04,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:05,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-26 01:04:10,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:11,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-26 01:04:13,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:13,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-26 01:04:17,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:17,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:17,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:17,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:17,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:17,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:18,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:18,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-26 01:04:18,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:18,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:19,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-26 01:04:19,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:19,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:19,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:19,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:19,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:19,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:20,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-26 01:04:20,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:20,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-26 01:04:22,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:22,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:22,537] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:22,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:22,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:22,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:22,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:22,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:22,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:22,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:22,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:22,805] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:22,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:22,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:22,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:22,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:22,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:23,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:23,005] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:23,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-26 01:04:23,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:23,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:23,098] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-26 01:04:23,147] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-26 01:04:23,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-26 01:04:23,156] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0926 01:04:35.804642 140656325334848 submission_runner.py:381] Time since start: 245.05s, 	Step: 1, 	{'train/accuracy': 0.0005978954081632653, 'train/loss': 6.921145069355867, 'validation/accuracy': 0.0006, 'validation/loss': 6.921351875, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.92348359375, 'test/num_examples': 10000, 'score': 60.11047315597534, 'total_duration': 245.052729845047, 'accumulated_submission_time': 60.11047315597534, 'accumulated_eval_time': 184.3169949054718, 'accumulated_logging_time': 0}
I0926 01:04:35.826982 140609562072832 logging_writer.py:48] [1] accumulated_eval_time=184.316995, accumulated_logging_time=0, accumulated_submission_time=60.110473, global_step=1, preemption_count=0, score=60.110473, test/accuracy=0.001100, test/loss=6.923484, test/num_examples=10000, total_duration=245.052730, train/accuracy=0.000598, train/loss=6.921145, validation/accuracy=0.000600, validation/loss=6.921352, validation/num_examples=50000
I0926 01:04:36.439733 140656325334848 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439738 140553053583168 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439789 139776650663744 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439871 140625127495488 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439890 140422881253184 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439901 139645281040192 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439820 139726288025408 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0926 01:04:36.439946 139779978749760 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I0926 01:04:36.914306 140609553680128 logging_writer.py:48] [1] global_step=1, grad_norm=0.621649, loss=6.928989
I0926 01:04:36.918012 140656325334848 submission.py:120] 1) loss = 6.929, grad_norm = 0.622
I0926 01:04:37.269343 140609562072832 logging_writer.py:48] [2] global_step=2, grad_norm=0.609702, loss=6.930316
I0926 01:04:37.273375 140656325334848 submission.py:120] 2) loss = 6.930, grad_norm = 0.610
I0926 01:04:37.628615 140609553680128 logging_writer.py:48] [3] global_step=3, grad_norm=0.611323, loss=6.923920
I0926 01:04:37.632763 140656325334848 submission.py:120] 3) loss = 6.924, grad_norm = 0.611
I0926 01:04:37.986224 140609562072832 logging_writer.py:48] [4] global_step=4, grad_norm=0.598515, loss=6.918643
I0926 01:04:37.991495 140656325334848 submission.py:120] 4) loss = 6.919, grad_norm = 0.599
I0926 01:04:38.358059 140609553680128 logging_writer.py:48] [5] global_step=5, grad_norm=0.601077, loss=6.930491
I0926 01:04:38.362222 140656325334848 submission.py:120] 5) loss = 6.930, grad_norm = 0.601
I0926 01:04:38.728054 140609562072832 logging_writer.py:48] [6] global_step=6, grad_norm=0.611720, loss=6.928116
I0926 01:04:38.733111 140656325334848 submission.py:120] 6) loss = 6.928, grad_norm = 0.612
I0926 01:04:39.098062 140609553680128 logging_writer.py:48] [7] global_step=7, grad_norm=0.613567, loss=6.921317
I0926 01:04:39.102421 140656325334848 submission.py:120] 7) loss = 6.921, grad_norm = 0.614
I0926 01:04:39.460294 140609562072832 logging_writer.py:48] [8] global_step=8, grad_norm=0.611739, loss=6.933051
I0926 01:04:39.465012 140656325334848 submission.py:120] 8) loss = 6.933, grad_norm = 0.612
I0926 01:04:39.818852 140609553680128 logging_writer.py:48] [9] global_step=9, grad_norm=0.619224, loss=6.920156
I0926 01:04:39.823334 140656325334848 submission.py:120] 9) loss = 6.920, grad_norm = 0.619
I0926 01:04:40.180045 140609562072832 logging_writer.py:48] [10] global_step=10, grad_norm=0.613112, loss=6.929019
I0926 01:04:40.185113 140656325334848 submission.py:120] 10) loss = 6.929, grad_norm = 0.613
I0926 01:07:33.932086 140609553680128 logging_writer.py:48] [500] global_step=500, grad_norm=0.991456, loss=6.294028
I0926 01:07:34.027660 140656325334848 submission.py:120] 500) loss = 6.294, grad_norm = 0.991
I0926 01:10:32.355982 140609562072832 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.370049, loss=5.676949
I0926 01:10:32.368346 140656325334848 submission.py:120] 1000) loss = 5.677, grad_norm = 2.370
I0926 01:13:06.803108 140656325334848 spec.py:321] Evaluating on the training split.
I0926 01:13:49.060638 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 01:14:45.120824 140656325334848 spec.py:349] Evaluating on the test split.
I0926 01:14:46.267384 140656325334848 submission_runner.py:381] Time since start: 855.52s, 	Step: 1432, 	{'train/accuracy': 0.11957908163265306, 'train/loss': 4.724138843769929, 'validation/accuracy': 0.10642, 'validation/loss': 4.82047375, 'validation/num_examples': 50000, 'test/accuracy': 0.0747, 'test/loss': 5.26302890625, 'test/num_examples': 10000, 'score': 568.9091465473175, 'total_duration': 855.5154278278351, 'accumulated_submission_time': 568.9091465473175, 'accumulated_eval_time': 283.78160214424133, 'accumulated_logging_time': 0.03265833854675293}
I0926 01:14:46.283796 140609570465536 logging_writer.py:48] [1432] accumulated_eval_time=283.781602, accumulated_logging_time=0.032658, accumulated_submission_time=568.909147, global_step=1432, preemption_count=0, score=568.909147, test/accuracy=0.074700, test/loss=5.263029, test/num_examples=10000, total_duration=855.515428, train/accuracy=0.119579, train/loss=4.724139, validation/accuracy=0.106420, validation/loss=4.820474, validation/num_examples=50000
I0926 01:15:10.915605 140609578858240 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.287086, loss=5.376842
I0926 01:15:10.920060 140656325334848 submission.py:120] 1500) loss = 5.377, grad_norm = 6.287
I0926 01:18:05.294021 140609570465536 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.301754, loss=4.928020
I0926 01:18:05.299320 140656325334848 submission.py:120] 2000) loss = 4.928, grad_norm = 3.302
I0926 01:20:59.530533 140609578858240 logging_writer.py:48] [2500] global_step=2500, grad_norm=6.805259, loss=4.687251
I0926 01:20:59.534931 140656325334848 submission.py:120] 2500) loss = 4.687, grad_norm = 6.805
I0926 01:23:17.122637 140656325334848 spec.py:321] Evaluating on the training split.
I0926 01:24:01.350840 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 01:24:58.226110 140656325334848 spec.py:349] Evaluating on the test split.
I0926 01:24:59.351810 140656325334848 submission_runner.py:381] Time since start: 1468.60s, 	Step: 2888, 	{'train/accuracy': 0.26656170280612246, 'train/loss': 3.6110814931441326, 'validation/accuracy': 0.24044, 'validation/loss': 3.756244375, 'validation/num_examples': 50000, 'test/accuracy': 0.171, 'test/loss': 4.38323828125, 'test/num_examples': 10000, 'score': 1077.5918776988983, 'total_duration': 1468.5999245643616, 'accumulated_submission_time': 1077.5918776988983, 'accumulated_eval_time': 386.010867357254, 'accumulated_logging_time': 0.05695152282714844}
I0926 01:24:59.373479 140609570465536 logging_writer.py:48] [2888] accumulated_eval_time=386.010867, accumulated_logging_time=0.056952, accumulated_submission_time=1077.591878, global_step=2888, preemption_count=0, score=1077.591878, test/accuracy=0.171000, test/loss=4.383238, test/num_examples=10000, total_duration=1468.599925, train/accuracy=0.266562, train/loss=3.611081, validation/accuracy=0.240440, validation/loss=3.756244, validation/num_examples=50000
I0926 01:25:39.577705 140609578858240 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.770436, loss=4.371596
I0926 01:25:39.582050 140656325334848 submission.py:120] 3000) loss = 4.372, grad_norm = 3.770
I0926 01:28:33.822469 140609570465536 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.513794, loss=4.011235
I0926 01:28:33.827440 140656325334848 submission.py:120] 3500) loss = 4.011, grad_norm = 4.514
I0926 01:31:29.913427 140609578858240 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.433290, loss=4.005162
I0926 01:31:29.918740 140656325334848 submission.py:120] 4000) loss = 4.005, grad_norm = 3.433
I0926 01:33:30.288809 140656325334848 spec.py:321] Evaluating on the training split.
I0926 01:34:12.740745 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 01:35:08.393829 140656325334848 spec.py:349] Evaluating on the test split.
I0926 01:35:09.511303 140656325334848 submission_runner.py:381] Time since start: 2078.76s, 	Step: 4344, 	{'train/accuracy': 0.3878547512755102, 'train/loss': 2.8768307432836417, 'validation/accuracy': 0.35422, 'validation/loss': 3.06729875, 'validation/num_examples': 50000, 'test/accuracy': 0.2712, 'test/loss': 3.6964484375, 'test/num_examples': 10000, 'score': 1586.322928905487, 'total_duration': 2078.759357213974, 'accumulated_submission_time': 1586.322928905487, 'accumulated_eval_time': 485.2335298061371, 'accumulated_logging_time': 0.0879521369934082}
I0926 01:35:09.528641 140609570465536 logging_writer.py:48] [4344] accumulated_eval_time=485.233530, accumulated_logging_time=0.087952, accumulated_submission_time=1586.322929, global_step=4344, preemption_count=0, score=1586.322929, test/accuracy=0.271200, test/loss=3.696448, test/num_examples=10000, total_duration=2078.759357, train/accuracy=0.387855, train/loss=2.876831, validation/accuracy=0.354220, validation/loss=3.067299, validation/num_examples=50000
I0926 01:36:04.961202 140609578858240 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.029381, loss=3.656997
I0926 01:36:04.965783 140656325334848 submission.py:120] 4500) loss = 3.657, grad_norm = 2.029
I0926 01:38:59.120741 140609570465536 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.267097, loss=3.594692
I0926 01:38:59.127935 140656325334848 submission.py:120] 5000) loss = 3.595, grad_norm = 2.267
I0926 01:41:55.109941 140609578858240 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.491596, loss=3.627318
I0926 01:41:55.114763 140656325334848 submission.py:120] 5500) loss = 3.627, grad_norm = 1.492
I0926 01:43:40.477003 140656325334848 spec.py:321] Evaluating on the training split.
I0926 01:44:23.720917 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 01:45:18.980211 140656325334848 spec.py:349] Evaluating on the test split.
I0926 01:45:20.095795 140656325334848 submission_runner.py:381] Time since start: 2689.34s, 	Step: 5801, 	{'train/accuracy': 0.43943319515306123, 'train/loss': 2.591961607641103, 'validation/accuracy': 0.40314, 'validation/loss': 2.79827, 'validation/num_examples': 50000, 'test/accuracy': 0.2946, 'test/loss': 3.5387671875, 'test/num_examples': 10000, 'score': 2095.106781721115, 'total_duration': 2689.34393286705, 'accumulated_submission_time': 2095.106781721115, 'accumulated_eval_time': 584.8527553081512, 'accumulated_logging_time': 0.1150045394897461}
I0926 01:45:20.115018 140609570465536 logging_writer.py:48] [5801] accumulated_eval_time=584.852755, accumulated_logging_time=0.115005, accumulated_submission_time=2095.106782, global_step=5801, preemption_count=0, score=2095.106782, test/accuracy=0.294600, test/loss=3.538767, test/num_examples=10000, total_duration=2689.343933, train/accuracy=0.439433, train/loss=2.591962, validation/accuracy=0.403140, validation/loss=2.798270, validation/num_examples=50000
I0926 01:46:30.243692 140609578858240 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.874474, loss=3.467958
I0926 01:46:30.248487 140656325334848 submission.py:120] 6000) loss = 3.468, grad_norm = 1.874
I0926 01:49:25.503477 140609570465536 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.495271, loss=3.297179
I0926 01:49:25.512886 140656325334848 submission.py:120] 6500) loss = 3.297, grad_norm = 1.495
I0926 01:52:19.583772 140609578858240 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.230847, loss=3.251072
I0926 01:52:19.588773 140656325334848 submission.py:120] 7000) loss = 3.251, grad_norm = 1.231
I0926 01:53:51.217801 140656325334848 spec.py:321] Evaluating on the training split.
I0926 01:54:35.518379 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 01:55:31.758171 140656325334848 spec.py:349] Evaluating on the test split.
I0926 01:55:32.880932 140656325334848 submission_runner.py:381] Time since start: 3302.13s, 	Step: 7262, 	{'train/accuracy': 0.5139309630102041, 'train/loss': 2.151299301458865, 'validation/accuracy': 0.47062, 'validation/loss': 2.38710265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3488, 'test/loss': 3.14955625, 'test/num_examples': 10000, 'score': 2603.988212585449, 'total_duration': 3302.1272473335266, 'accumulated_submission_time': 2603.988212585449, 'accumulated_eval_time': 686.5146219730377, 'accumulated_logging_time': 0.14304327964782715}
I0926 01:55:32.899855 140609570465536 logging_writer.py:48] [7262] accumulated_eval_time=686.514622, accumulated_logging_time=0.143043, accumulated_submission_time=2603.988213, global_step=7262, preemption_count=0, score=2603.988213, test/accuracy=0.348800, test/loss=3.149556, test/num_examples=10000, total_duration=3302.127247, train/accuracy=0.513931, train/loss=2.151299, validation/accuracy=0.470620, validation/loss=2.387103, validation/num_examples=50000
I0926 01:56:56.829804 140609578858240 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.016283, loss=3.227401
I0926 01:56:56.833562 140656325334848 submission.py:120] 7500) loss = 3.227, grad_norm = 1.016
I0926 01:59:52.224617 140609570465536 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.637196, loss=3.170175
I0926 01:59:52.228890 140656325334848 submission.py:120] 8000) loss = 3.170, grad_norm = 1.637
I0926 02:02:46.350337 140609578858240 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.100038, loss=3.177662
I0926 02:02:46.355547 140656325334848 submission.py:120] 8500) loss = 3.178, grad_norm = 1.100
I0926 02:04:04.060455 140656325334848 spec.py:321] Evaluating on the training split.
I0926 02:04:46.215432 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 02:05:41.457881 140656325334848 spec.py:349] Evaluating on the test split.
I0926 02:05:42.572628 140656325334848 submission_runner.py:381] Time since start: 3911.82s, 	Step: 8722, 	{'train/accuracy': 0.5694156568877551, 'train/loss': 1.9325605119977678, 'validation/accuracy': 0.51614, 'validation/loss': 2.18937625, 'validation/num_examples': 50000, 'test/accuracy': 0.384, 'test/loss': 2.92783046875, 'test/num_examples': 10000, 'score': 3112.8735070228577, 'total_duration': 3911.820773601532, 'accumulated_submission_time': 3112.8735070228577, 'accumulated_eval_time': 785.0275065898895, 'accumulated_logging_time': 0.17378950119018555}
I0926 02:05:42.593397 140609570465536 logging_writer.py:48] [8722] accumulated_eval_time=785.027507, accumulated_logging_time=0.173790, accumulated_submission_time=3112.873507, global_step=8722, preemption_count=0, score=3112.873507, test/accuracy=0.384000, test/loss=2.927830, test/num_examples=10000, total_duration=3911.820774, train/accuracy=0.569416, train/loss=1.932561, validation/accuracy=0.516140, validation/loss=2.189376, validation/num_examples=50000
I0926 02:07:21.979362 140609578858240 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.964904, loss=3.111997
I0926 02:07:21.984985 140656325334848 submission.py:120] 9000) loss = 3.112, grad_norm = 0.965
I0926 02:10:16.063192 140609570465536 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.809239, loss=2.984039
I0926 02:10:16.068879 140656325334848 submission.py:120] 9500) loss = 2.984, grad_norm = 0.809
I0926 02:13:10.291759 140609578858240 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.744044, loss=2.946587
I0926 02:13:10.296504 140656325334848 submission.py:120] 10000) loss = 2.947, grad_norm = 0.744
I0926 02:14:13.612318 140656325334848 spec.py:321] Evaluating on the training split.
I0926 02:14:55.308635 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 02:15:52.671010 140656325334848 spec.py:349] Evaluating on the test split.
I0926 02:15:53.785908 140656325334848 submission_runner.py:381] Time since start: 4523.03s, 	Step: 10176, 	{'train/accuracy': 0.6085578762755102, 'train/loss': 1.740108567841199, 'validation/accuracy': 0.5497, 'validation/loss': 2.02119453125, 'validation/num_examples': 50000, 'test/accuracy': 0.4202, 'test/loss': 2.7638474609375, 'test/num_examples': 10000, 'score': 3621.637321472168, 'total_duration': 4523.03390955925, 'accumulated_submission_time': 3621.637321472168, 'accumulated_eval_time': 885.2012858390808, 'accumulated_logging_time': 0.20300078392028809}
I0926 02:15:53.804231 140609570465536 logging_writer.py:48] [10176] accumulated_eval_time=885.201286, accumulated_logging_time=0.203001, accumulated_submission_time=3621.637321, global_step=10176, preemption_count=0, score=3621.637321, test/accuracy=0.420200, test/loss=2.763847, test/num_examples=10000, total_duration=4523.033910, train/accuracy=0.608558, train/loss=1.740109, validation/accuracy=0.549700, validation/loss=2.021195, validation/num_examples=50000
I0926 02:17:47.598302 140609578858240 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.853177, loss=2.914402
I0926 02:17:47.603159 140656325334848 submission.py:120] 10500) loss = 2.914, grad_norm = 0.853
I0926 02:20:41.597758 140609570465536 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.727402, loss=2.838809
I0926 02:20:41.603333 140656325334848 submission.py:120] 11000) loss = 2.839, grad_norm = 0.727
I0926 02:23:36.992828 140609578858240 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.727543, loss=2.737667
I0926 02:23:36.997710 140656325334848 submission.py:120] 11500) loss = 2.738, grad_norm = 0.728
I0926 02:24:24.680000 140656325334848 spec.py:321] Evaluating on the training split.
I0926 02:25:06.980485 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 02:26:01.232969 140656325334848 spec.py:349] Evaluating on the test split.
I0926 02:26:02.357772 140656325334848 submission_runner.py:381] Time since start: 5131.61s, 	Step: 11636, 	{'train/accuracy': 0.6472616390306123, 'train/loss': 1.559710911342076, 'validation/accuracy': 0.5856, 'validation/loss': 1.8601384375, 'validation/num_examples': 50000, 'test/accuracy': 0.4477, 'test/loss': 2.5905474609375, 'test/num_examples': 10000, 'score': 4130.339219093323, 'total_duration': 5131.60591173172, 'accumulated_submission_time': 4130.339219093323, 'accumulated_eval_time': 982.879442691803, 'accumulated_logging_time': 0.23070478439331055}
I0926 02:26:02.374161 140609570465536 logging_writer.py:48] [11636] accumulated_eval_time=982.879443, accumulated_logging_time=0.230705, accumulated_submission_time=4130.339219, global_step=11636, preemption_count=0, score=4130.339219, test/accuracy=0.447700, test/loss=2.590547, test/num_examples=10000, total_duration=5131.605912, train/accuracy=0.647262, train/loss=1.559711, validation/accuracy=0.585600, validation/loss=1.860138, validation/num_examples=50000
I0926 02:28:10.142566 140609578858240 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.785666, loss=2.701231
I0926 02:28:10.146707 140656325334848 submission.py:120] 12000) loss = 2.701, grad_norm = 0.786
I0926 02:31:04.139760 140609570465536 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.759362, loss=2.796217
I0926 02:31:04.145963 140656325334848 submission.py:120] 12500) loss = 2.796, grad_norm = 0.759
I0926 02:33:59.336160 140609578858240 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.734640, loss=2.694377
I0926 02:33:59.342609 140656325334848 submission.py:120] 13000) loss = 2.694, grad_norm = 0.735
I0926 02:34:33.219949 140656325334848 spec.py:321] Evaluating on the training split.
I0926 02:35:16.172639 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 02:36:12.660295 140656325334848 spec.py:349] Evaluating on the test split.
I0926 02:36:13.773392 140656325334848 submission_runner.py:381] Time since start: 5743.02s, 	Step: 13096, 	{'train/accuracy': 0.6509885204081632, 'train/loss': 1.5418056565888074, 'validation/accuracy': 0.58236, 'validation/loss': 1.8723846875, 'validation/num_examples': 50000, 'test/accuracy': 0.4564, 'test/loss': 2.599479296875, 'test/num_examples': 10000, 'score': 4638.941096544266, 'total_duration': 5743.021557331085, 'accumulated_submission_time': 4638.941096544266, 'accumulated_eval_time': 1083.4334161281586, 'accumulated_logging_time': 0.25530242919921875}
I0926 02:36:13.790132 140609570465536 logging_writer.py:48] [13096] accumulated_eval_time=1083.433416, accumulated_logging_time=0.255302, accumulated_submission_time=4638.941097, global_step=13096, preemption_count=0, score=4638.941097, test/accuracy=0.456400, test/loss=2.599479, test/num_examples=10000, total_duration=5743.021557, train/accuracy=0.650989, train/loss=1.541806, validation/accuracy=0.582360, validation/loss=1.872385, validation/num_examples=50000
I0926 02:38:35.493090 140609578858240 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.534214, loss=2.669798
I0926 02:38:35.499459 140656325334848 submission.py:120] 13500) loss = 2.670, grad_norm = 0.534
I0926 02:41:33.663434 140656325334848 spec.py:321] Evaluating on the training split.
I0926 02:42:15.855856 140656325334848 spec.py:333] Evaluating on the validation split.
I0926 02:43:12.420339 140656325334848 spec.py:349] Evaluating on the test split.
I0926 02:43:13.534625 140656325334848 submission_runner.py:381] Time since start: 6162.78s, 	Step: 14000, 	{'train/accuracy': 0.6852080676020408, 'train/loss': 1.3743490102339764, 'validation/accuracy': 0.61722, 'validation/loss': 1.7093584375, 'validation/num_examples': 50000, 'test/accuracy': 0.4687, 'test/loss': 2.4813970703125, 'test/num_examples': 10000, 'score': 4956.926801919937, 'total_duration': 6162.7811398506165, 'accumulated_submission_time': 4956.926801919937, 'accumulated_eval_time': 1183.3034601211548, 'accumulated_logging_time': 0.28114795684814453}
I0926 02:43:13.554067 140609570465536 logging_writer.py:48] [14000] accumulated_eval_time=1183.303460, accumulated_logging_time=0.281148, accumulated_submission_time=4956.926802, global_step=14000, preemption_count=0, score=4956.926802, test/accuracy=0.468700, test/loss=2.481397, test/num_examples=10000, total_duration=6162.781140, train/accuracy=0.685208, train/loss=1.374349, validation/accuracy=0.617220, validation/loss=1.709358, validation/num_examples=50000
I0926 02:43:14.113271 140609578858240 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4956.926802
I0926 02:43:14.936234 140656325334848 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_prelaunch/adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0926 02:43:15.670724 140656325334848 submission_runner.py:549] Tuning trial 1/1
I0926 02:43:15.670911 140656325334848 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0926 02:43:15.671768 140656325334848 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005978954081632653, 'train/loss': 6.921145069355867, 'validation/accuracy': 0.0006, 'validation/loss': 6.921351875, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.92348359375, 'test/num_examples': 10000, 'score': 60.11047315597534, 'total_duration': 245.052729845047, 'accumulated_submission_time': 60.11047315597534, 'accumulated_eval_time': 184.3169949054718, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1432, {'train/accuracy': 0.11957908163265306, 'train/loss': 4.724138843769929, 'validation/accuracy': 0.10642, 'validation/loss': 4.82047375, 'validation/num_examples': 50000, 'test/accuracy': 0.0747, 'test/loss': 5.26302890625, 'test/num_examples': 10000, 'score': 568.9091465473175, 'total_duration': 855.5154278278351, 'accumulated_submission_time': 568.9091465473175, 'accumulated_eval_time': 283.78160214424133, 'accumulated_logging_time': 0.03265833854675293, 'global_step': 1432, 'preemption_count': 0}), (2888, {'train/accuracy': 0.26656170280612246, 'train/loss': 3.6110814931441326, 'validation/accuracy': 0.24044, 'validation/loss': 3.756244375, 'validation/num_examples': 50000, 'test/accuracy': 0.171, 'test/loss': 4.38323828125, 'test/num_examples': 10000, 'score': 1077.5918776988983, 'total_duration': 1468.5999245643616, 'accumulated_submission_time': 1077.5918776988983, 'accumulated_eval_time': 386.010867357254, 'accumulated_logging_time': 0.05695152282714844, 'global_step': 2888, 'preemption_count': 0}), (4344, {'train/accuracy': 0.3878547512755102, 'train/loss': 2.8768307432836417, 'validation/accuracy': 0.35422, 'validation/loss': 3.06729875, 'validation/num_examples': 50000, 'test/accuracy': 0.2712, 'test/loss': 3.6964484375, 'test/num_examples': 10000, 'score': 1586.322928905487, 'total_duration': 2078.759357213974, 'accumulated_submission_time': 1586.322928905487, 'accumulated_eval_time': 485.2335298061371, 'accumulated_logging_time': 0.0879521369934082, 'global_step': 4344, 'preemption_count': 0}), (5801, {'train/accuracy': 0.43943319515306123, 'train/loss': 2.591961607641103, 'validation/accuracy': 0.40314, 'validation/loss': 2.79827, 'validation/num_examples': 50000, 'test/accuracy': 0.2946, 'test/loss': 3.5387671875, 'test/num_examples': 10000, 'score': 2095.106781721115, 'total_duration': 2689.34393286705, 'accumulated_submission_time': 2095.106781721115, 'accumulated_eval_time': 584.8527553081512, 'accumulated_logging_time': 0.1150045394897461, 'global_step': 5801, 'preemption_count': 0}), (7262, {'train/accuracy': 0.5139309630102041, 'train/loss': 2.151299301458865, 'validation/accuracy': 0.47062, 'validation/loss': 2.38710265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3488, 'test/loss': 3.14955625, 'test/num_examples': 10000, 'score': 2603.988212585449, 'total_duration': 3302.1272473335266, 'accumulated_submission_time': 2603.988212585449, 'accumulated_eval_time': 686.5146219730377, 'accumulated_logging_time': 0.14304327964782715, 'global_step': 7262, 'preemption_count': 0}), (8722, {'train/accuracy': 0.5694156568877551, 'train/loss': 1.9325605119977678, 'validation/accuracy': 0.51614, 'validation/loss': 2.18937625, 'validation/num_examples': 50000, 'test/accuracy': 0.384, 'test/loss': 2.92783046875, 'test/num_examples': 10000, 'score': 3112.8735070228577, 'total_duration': 3911.820773601532, 'accumulated_submission_time': 3112.8735070228577, 'accumulated_eval_time': 785.0275065898895, 'accumulated_logging_time': 0.17378950119018555, 'global_step': 8722, 'preemption_count': 0}), (10176, {'train/accuracy': 0.6085578762755102, 'train/loss': 1.740108567841199, 'validation/accuracy': 0.5497, 'validation/loss': 2.02119453125, 'validation/num_examples': 50000, 'test/accuracy': 0.4202, 'test/loss': 2.7638474609375, 'test/num_examples': 10000, 'score': 3621.637321472168, 'total_duration': 4523.03390955925, 'accumulated_submission_time': 3621.637321472168, 'accumulated_eval_time': 885.2012858390808, 'accumulated_logging_time': 0.20300078392028809, 'global_step': 10176, 'preemption_count': 0}), (11636, {'train/accuracy': 0.6472616390306123, 'train/loss': 1.559710911342076, 'validation/accuracy': 0.5856, 'validation/loss': 1.8601384375, 'validation/num_examples': 50000, 'test/accuracy': 0.4477, 'test/loss': 2.5905474609375, 'test/num_examples': 10000, 'score': 4130.339219093323, 'total_duration': 5131.60591173172, 'accumulated_submission_time': 4130.339219093323, 'accumulated_eval_time': 982.879442691803, 'accumulated_logging_time': 0.23070478439331055, 'global_step': 11636, 'preemption_count': 0}), (13096, {'train/accuracy': 0.6509885204081632, 'train/loss': 1.5418056565888074, 'validation/accuracy': 0.58236, 'validation/loss': 1.8723846875, 'validation/num_examples': 50000, 'test/accuracy': 0.4564, 'test/loss': 2.599479296875, 'test/num_examples': 10000, 'score': 4638.941096544266, 'total_duration': 5743.021557331085, 'accumulated_submission_time': 4638.941096544266, 'accumulated_eval_time': 1083.4334161281586, 'accumulated_logging_time': 0.25530242919921875, 'global_step': 13096, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6852080676020408, 'train/loss': 1.3743490102339764, 'validation/accuracy': 0.61722, 'validation/loss': 1.7093584375, 'validation/num_examples': 50000, 'test/accuracy': 0.4687, 'test/loss': 2.4813970703125, 'test/num_examples': 10000, 'score': 4956.926801919937, 'total_duration': 6162.7811398506165, 'accumulated_submission_time': 4956.926801919937, 'accumulated_eval_time': 1183.3034601211548, 'accumulated_logging_time': 0.28114795684814453, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0926 02:43:15.671884 140656325334848 submission_runner.py:552] Timing: 4956.926801919937
I0926 02:43:15.671942 140656325334848 submission_runner.py:554] Total number of evals: 11
I0926 02:43:15.671998 140656325334848 submission_runner.py:555] ====================
I0926 02:43:15.672112 140656325334848 submission_runner.py:625] Final imagenet_resnet score: 4956.926801919937
