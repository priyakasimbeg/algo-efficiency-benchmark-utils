I0418 18:35:29.841449 139859899844416 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax.
I0418 18:35:29.914992 139859899844416 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 18:35:30.775305 139859899844416 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0418 18:35:30.776004 139859899844416 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 18:35:30.779648 139859899844416 submission_runner.py:528] Using RNG seed 2662507266
I0418 18:35:33.390963 139859899844416 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 18:35:33.391160 139859899844416 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1.
I0418 18:35:33.391428 139859899844416 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0418 18:35:33.513451 139859899844416 submission_runner.py:232] Initializing dataset.
I0418 18:35:33.513637 139859899844416 submission_runner.py:239] Initializing model.
I0418 18:35:50.045719 139859899844416 submission_runner.py:249] Initializing optimizer.
I0418 18:35:50.692381 139859899844416 submission_runner.py:256] Initializing metrics bundle.
I0418 18:35:50.692571 139859899844416 submission_runner.py:273] Initializing checkpoint and logger.
I0418 18:35:50.693503 139859899844416 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0418 18:35:50.693791 139859899844416 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 18:35:50.693871 139859899844416 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 18:35:51.618992 139859899844416 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0418 18:35:51.620072 139859899844416 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0418 18:35:51.626820 139859899844416 submission_runner.py:309] Starting training loop.
I0418 18:35:51.819741 139859899844416 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:35:51.851727 139859899844416 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:35:52.167640 139859899844416 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0418 18:36:48.506685 139684137826048 logging_writer.py:48] [0] global_step=0, grad_norm=24.189912796020508, loss=32.879852294921875
I0418 18:36:48.528160 139859899844416 spec.py:298] Evaluating on the training split.
I0418 18:36:48.650732 139859899844416 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:36:48.676678 139859899844416 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:36:48.955002 139859899844416 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0418 18:38:11.755710 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 18:38:12.101868 139859899844416 input_pipeline.py:20] Loading split = dev-clean
I0418 18:38:12.106757 139859899844416 input_pipeline.py:20] Loading split = dev-other
I0418 18:39:02.967302 139859899844416 spec.py:326] Evaluating on the test split.
I0418 18:39:03.059503 139859899844416 input_pipeline.py:20] Loading split = test-clean
I0418 18:39:36.606534 139859899844416 submission_runner.py:406] Time since start: 224.98s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.87637, dtype=float32), 'train/wer': 3.749614767923703, 'validation/ctc_loss': DeviceArray(31.066803, dtype=float32), 'validation/wer': 3.4339260388426323, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.069, dtype=float32), 'test/wer': 3.6274450063981476, 'test/num_examples': 2472, 'score': 56.90117430686951, 'total_duration': 224.9784083366394, 'accumulated_submission_time': 56.90117430686951, 'accumulated_eval_time': 168.07708072662354, 'accumulated_logging_time': 0}
I0418 18:39:36.626235 139681027061504 logging_writer.py:48] [1] accumulated_eval_time=168.077081, accumulated_logging_time=0, accumulated_submission_time=56.901174, global_step=1, preemption_count=0, score=56.901174, test/ctc_loss=31.069000244140625, test/num_examples=2472, test/wer=3.627445, total_duration=224.978408, train/ctc_loss=31.87636947631836, train/wer=3.749615, validation/ctc_loss=31.066802978515625, validation/num_examples=5348, validation/wer=3.433926
I0418 18:39:36.771160 139859899844416 checkpoints.py:356] Saving checkpoint at step: 1
I0418 18:39:37.169079 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0418 18:39:37.170013 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0418 18:41:48.766349 139684945667840 logging_writer.py:48] [100] global_step=100, grad_norm=4.7164998054504395, loss=8.001718521118164
I0418 18:43:43.710569 139684954060544 logging_writer.py:48] [200] global_step=200, grad_norm=1.9250009059906006, loss=6.280178546905518
I0418 18:45:39.075868 139684945667840 logging_writer.py:48] [300] global_step=300, grad_norm=0.755936861038208, loss=5.888542652130127
I0418 18:47:34.508151 139684954060544 logging_writer.py:48] [400] global_step=400, grad_norm=0.7119215130805969, loss=5.860447883605957
I0418 18:49:29.865366 139684945667840 logging_writer.py:48] [500] global_step=500, grad_norm=0.47953400015830994, loss=5.819458961486816
I0418 18:51:25.098084 139684954060544 logging_writer.py:48] [600] global_step=600, grad_norm=1.3524136543273926, loss=5.755324840545654
I0418 18:53:20.477453 139684945667840 logging_writer.py:48] [700] global_step=700, grad_norm=1.5332177877426147, loss=5.653448581695557
I0418 18:55:16.364000 139684954060544 logging_writer.py:48] [800] global_step=800, grad_norm=0.7193799018859863, loss=5.509446144104004
I0418 18:57:11.632770 139684945667840 logging_writer.py:48] [900] global_step=900, grad_norm=1.315184473991394, loss=5.381185531616211
I0418 18:59:07.245852 139684954060544 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9180108904838562, loss=5.0491719245910645
I0418 19:01:05.348097 139685760489216 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4562649726867676, loss=4.652151584625244
I0418 19:03:00.226222 139685752096512 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.910855770111084, loss=4.309822082519531
I0418 19:04:55.176593 139685760489216 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.5418202877044678, loss=4.0838518142700195
I0418 19:06:50.455977 139685752096512 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.7664623260498047, loss=3.808699369430542
I0418 19:08:45.516545 139685760489216 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.268143892288208, loss=3.662172317504883
I0418 19:10:40.565688 139685752096512 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.5359020233154297, loss=3.477419853210449
I0418 19:12:35.762137 139685760489216 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.148023843765259, loss=3.4256248474121094
I0418 19:14:30.743700 139685752096512 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.064065456390381, loss=3.3290648460388184
I0418 19:16:25.439244 139685760489216 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.3469181060791016, loss=3.2347335815429688
I0418 19:18:20.370380 139685752096512 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.610161304473877, loss=3.1391470432281494
I0418 19:19:37.439900 139859899844416 spec.py:298] Evaluating on the training split.
I0418 19:20:11.160854 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 19:20:47.264641 139859899844416 spec.py:326] Evaluating on the test split.
I0418 19:21:05.649715 139859899844416 submission_runner.py:406] Time since start: 2714.02s, 	Step: 2066, 	{'train/ctc_loss': DeviceArray(6.112874, dtype=float32), 'train/wer': 0.893730304216185, 'validation/ctc_loss': DeviceArray(6.0239143, dtype=float32), 'validation/wer': 0.8606740055379213, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9477425, dtype=float32), 'test/wer': 0.8582048625921638, 'test/num_examples': 2472, 'score': 2457.141233444214, 'total_duration': 2714.0196969509125, 'accumulated_submission_time': 2457.141233444214, 'accumulated_eval_time': 256.2839529514313, 'accumulated_logging_time': 0.5689632892608643}
I0418 19:21:05.669414 139685176809216 logging_writer.py:48] [2066] accumulated_eval_time=256.283953, accumulated_logging_time=0.568963, accumulated_submission_time=2457.141233, global_step=2066, preemption_count=0, score=2457.141233, test/ctc_loss=5.947742462158203, test/num_examples=2472, test/wer=0.858205, total_duration=2714.019697, train/ctc_loss=6.1128740310668945, train/wer=0.893730, validation/ctc_loss=6.023914337158203, validation/num_examples=5348, validation/wer=0.860674
I0418 19:21:05.827590 139859899844416 checkpoints.py:356] Saving checkpoint at step: 2066
I0418 19:21:06.435112 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_2066
I0418 19:21:06.450779 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_2066.
I0418 19:21:46.190996 139685168416512 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.3746960163116455, loss=3.0537240505218506
I0418 19:23:39.950770 139685126452992 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.5031301975250244, loss=3.015505075454712
I0418 19:25:33.845493 139685168416512 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.636990785598755, loss=2.8854775428771973
I0418 19:27:28.273412 139685126452992 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.611076831817627, loss=2.8964638710021973
I0418 19:29:22.603266 139685168416512 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.63098406791687, loss=2.8894217014312744
I0418 19:31:17.838293 139685126452992 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.623354434967041, loss=2.7360594272613525
I0418 19:33:12.303451 139685168416512 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.856843948364258, loss=2.7453458309173584
I0418 19:35:07.439508 139685126452992 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.669395685195923, loss=2.7258334159851074
I0418 19:37:02.512910 139685168416512 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.71347713470459, loss=2.583120107650757
I0418 19:38:57.248997 139685126452992 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.7846503257751465, loss=2.6284778118133545
I0418 19:40:54.793414 139684521449216 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.9868597984313965, loss=2.5546560287475586
I0418 19:42:48.629333 139684513056512 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.082430839538574, loss=2.4911599159240723
I0418 19:44:42.676872 139684521449216 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.0703604221343994, loss=2.4671237468719482
I0418 19:46:37.168836 139684513056512 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.29480242729187, loss=2.4472315311431885
I0418 19:48:31.612689 139684521449216 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.522602081298828, loss=2.489522933959961
I0418 19:50:26.009378 139684513056512 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.9004178047180176, loss=2.4566783905029297
I0418 19:52:20.376816 139684521449216 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.6960718631744385, loss=2.3884482383728027
I0418 19:54:14.644428 139684513056512 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.820991277694702, loss=2.4129631519317627
I0418 19:56:09.212515 139684521449216 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.6790781021118164, loss=2.338930606842041
I0418 19:58:03.181055 139684513056512 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.2178733348846436, loss=2.3120856285095215
I0418 19:59:57.152060 139684521449216 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.972849130630493, loss=2.3024137020111084
I0418 20:01:07.410752 139859899844416 spec.py:298] Evaluating on the training split.
I0418 20:01:44.441185 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 20:02:21.565164 139859899844416 spec.py:326] Evaluating on the test split.
I0418 20:02:41.057489 139859899844416 submission_runner.py:406] Time since start: 5209.43s, 	Step: 4160, 	{'train/ctc_loss': DeviceArray(1.4487426, dtype=float32), 'train/wer': 0.40490637402445195, 'validation/ctc_loss': DeviceArray(1.8827316, dtype=float32), 'validation/wer': 0.46255149591409467, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.4370253, dtype=float32), 'test/wer': 0.39365872483903075, 'test/num_examples': 2472, 'score': 4858.070352554321, 'total_duration': 5209.427096128464, 'accumulated_submission_time': 4858.070352554321, 'accumulated_eval_time': 349.92733335494995, 'accumulated_logging_time': 1.3758723735809326}
I0418 20:02:41.077068 139684521449216 logging_writer.py:48] [4160] accumulated_eval_time=349.927333, accumulated_logging_time=1.375872, accumulated_submission_time=4858.070353, global_step=4160, preemption_count=0, score=4858.070353, test/ctc_loss=1.4370253086090088, test/num_examples=2472, test/wer=0.393659, total_duration=5209.427096, train/ctc_loss=1.4487426280975342, train/wer=0.404906, validation/ctc_loss=1.882731556892395, validation/num_examples=5348, validation/wer=0.462551
I0418 20:02:41.240458 139859899844416 checkpoints.py:356] Saving checkpoint at step: 4160
I0418 20:02:41.842800 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_4160
I0418 20:02:41.858132 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_4160.
I0418 20:03:28.646457 139684513056512 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.955855369567871, loss=2.2840576171875
I0418 20:05:22.754165 139684462700288 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.2621471881866455, loss=2.2526140213012695
I0418 20:07:16.764619 139684513056512 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.603579521179199, loss=2.149831533432007
I0418 20:09:12.714219 139684462700288 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.6073286533355713, loss=2.205158233642578
I0418 20:11:07.312201 139684513056512 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.6789627075195312, loss=2.18061900138855
I0418 20:13:01.386454 139684462700288 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.3650572299957275, loss=2.151097536087036
I0418 20:14:57.460373 139684513056512 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.631831407546997, loss=2.112994909286499
I0418 20:16:52.124038 139684462700288 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.001474618911743, loss=2.1314144134521484
I0418 20:18:46.547287 139684513056512 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.7095696926116943, loss=2.190080165863037
I0418 20:20:40.775430 139684462700288 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.78133487701416, loss=2.035325050354004
I0418 20:22:38.601073 139684521449216 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.864201545715332, loss=2.070815086364746
I0418 20:24:32.532432 139684513056512 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.7055470943450928, loss=2.114328145980835
I0418 20:26:26.651805 139684521449216 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.8281304836273193, loss=2.1244587898254395
I0418 20:28:21.064065 139684513056512 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.340822696685791, loss=2.0751476287841797
I0418 20:30:15.338285 139684521449216 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.502676486968994, loss=2.115267276763916
I0418 20:32:09.962609 139684513056512 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.589698314666748, loss=2.0999977588653564
I0418 20:34:04.546393 139684521449216 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.661001443862915, loss=2.0107152462005615
I0418 20:35:59.112475 139684513056512 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.196052074432373, loss=2.005082130432129
I0418 20:37:53.295013 139684521449216 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.3719589710235596, loss=2.057067632675171
I0418 20:39:47.774498 139684513056512 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.6178715229034424, loss=2.0267157554626465
I0418 20:41:45.110854 139685176809216 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.801654815673828, loss=1.9825725555419922
I0418 20:42:42.009240 139859899844416 spec.py:298] Evaluating on the training split.
I0418 20:43:20.832229 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 20:43:58.974353 139859899844416 spec.py:326] Evaluating on the test split.
I0418 20:44:18.557713 139859899844416 submission_runner.py:406] Time since start: 7706.93s, 	Step: 6251, 	{'train/ctc_loss': DeviceArray(0.71834177, dtype=float32), 'train/wer': 0.24431899046444328, 'validation/ctc_loss': DeviceArray(1.1505024, dtype=float32), 'validation/wer': 0.31651053073353336, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.789191, dtype=float32), 'test/wer': 0.25029959579956534, 'test/num_examples': 2472, 'score': 7258.190971851349, 'total_duration': 7706.9273335933685, 'accumulated_submission_time': 7258.190971851349, 'accumulated_eval_time': 446.472309589386, 'accumulated_logging_time': 2.1825690269470215}
I0418 20:44:18.576729 139685176809216 logging_writer.py:48] [6251] accumulated_eval_time=446.472310, accumulated_logging_time=2.182569, accumulated_submission_time=7258.190972, global_step=6251, preemption_count=0, score=7258.190972, test/ctc_loss=0.7891910076141357, test/num_examples=2472, test/wer=0.250300, total_duration=7706.927334, train/ctc_loss=0.7183417677879333, train/wer=0.244319, validation/ctc_loss=1.1505024433135986, validation/num_examples=5348, validation/wer=0.316511
I0418 20:44:18.740126 139859899844416 checkpoints.py:356] Saving checkpoint at step: 6251
I0418 20:44:19.403069 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_6251
I0418 20:44:19.418634 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_6251.
I0418 20:45:16.441781 139685168416512 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.6736574172973633, loss=1.8890008926391602
I0418 20:47:10.696547 139682570565376 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.1319501399993896, loss=2.0029850006103516
I0418 20:49:05.043574 139685168416512 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.3967125415802, loss=1.9403142929077148
I0418 20:50:59.402296 139682570565376 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.729785442352295, loss=1.9622665643692017
I0418 20:52:53.810589 139685168416512 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.724121570587158, loss=2.04691219329834
I0418 20:54:48.101268 139682570565376 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.9827609062194824, loss=1.9294480085372925
I0418 20:56:43.071012 139685168416512 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.132054567337036, loss=1.9623844623565674
I0418 20:58:37.966548 139682570565376 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.386291027069092, loss=2.012754440307617
I0418 21:00:32.724592 139685168416512 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.3128905296325684, loss=1.9092051982879639
I0418 21:02:27.297763 139682570565376 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.0291683673858643, loss=1.9325634241104126
I0418 21:04:25.038618 139685176809216 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.6524697542190552, loss=1.8880119323730469
I0418 21:06:18.977467 139685168416512 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.487380266189575, loss=1.8940092325210571
I0418 21:08:13.356732 139685176809216 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2306675910949707, loss=1.9049838781356812
I0418 21:10:07.599735 139685168416512 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.8062427043914795, loss=1.9225151538848877
I0418 21:12:01.535729 139685176809216 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.483619451522827, loss=1.8791195154190063
I0418 21:13:55.751955 139685168416512 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.8216724395751953, loss=1.8060222864151
I0418 21:15:50.096301 139685176809216 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.3901257514953613, loss=1.8519315719604492
I0418 21:17:44.097313 139685168416512 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.6242642402648926, loss=1.8926624059677124
I0418 21:19:38.423007 139685176809216 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.3453733921051025, loss=1.8761168718338013
I0418 21:21:32.702465 139685168416512 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.849461555480957, loss=1.8560363054275513
I0418 21:23:30.274112 139685176809216 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.534494161605835, loss=1.8635270595550537
I0418 21:24:20.255832 139859899844416 spec.py:298] Evaluating on the training split.
I0418 21:24:59.401467 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 21:25:37.443495 139859899844416 spec.py:326] Evaluating on the test split.
I0418 21:25:56.944487 139859899844416 submission_runner.py:406] Time since start: 10205.31s, 	Step: 8345, 	{'train/ctc_loss': DeviceArray(0.5618466, dtype=float32), 'train/wer': 0.19125239556388693, 'validation/ctc_loss': DeviceArray(0.94501865, dtype=float32), 'validation/wer': 0.2684830533820876, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6029804, dtype=float32), 'test/wer': 0.1947068023480186, 'test/num_examples': 2472, 'score': 9658.996666431427, 'total_duration': 10205.31457400322, 'accumulated_submission_time': 9658.996666431427, 'accumulated_eval_time': 543.15793633461, 'accumulated_logging_time': 3.0501930713653564}
I0418 21:25:56.964426 139684521441024 logging_writer.py:48] [8345] accumulated_eval_time=543.157936, accumulated_logging_time=3.050193, accumulated_submission_time=9658.996666, global_step=8345, preemption_count=0, score=9658.996666, test/ctc_loss=0.602980375289917, test/num_examples=2472, test/wer=0.194707, total_duration=10205.314574, train/ctc_loss=0.5618466138839722, train/wer=0.191252, validation/ctc_loss=0.9450186491012573, validation/num_examples=5348, validation/wer=0.268483
I0418 21:25:57.128761 139859899844416 checkpoints.py:356] Saving checkpoint at step: 8345
I0418 21:25:57.797119 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8345
I0418 21:25:57.812385 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8345.
I0418 21:27:01.405429 139684513048320 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.848407506942749, loss=1.781915545463562
I0418 21:28:55.278876 139682562172672 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.9655145406723022, loss=1.8130970001220703
I0418 21:30:49.519217 139684513048320 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.380526304244995, loss=1.866850733757019
I0418 21:32:43.126437 139682562172672 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.7220964431762695, loss=1.8034827709197998
I0418 21:34:37.165845 139684513048320 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.341853618621826, loss=1.7513673305511475
I0418 21:36:31.276572 139682562172672 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.2690515518188477, loss=1.7920283079147339
I0418 21:38:25.526781 139684513048320 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.1268019676208496, loss=1.824741005897522
I0418 21:40:20.022038 139682562172672 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.567034959793091, loss=1.8718470335006714
I0418 21:42:14.371022 139684513048320 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.9518847465515137, loss=1.8069988489151
I0418 21:44:12.008403 139684521441024 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.0055031776428223, loss=1.7706369161605835
I0418 21:46:05.947191 139684513048320 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.2934770584106445, loss=1.8414897918701172
I0418 21:47:59.522506 139684521441024 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.24385404586792, loss=1.849432349205017
I0418 21:49:53.247207 139684513048320 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.471445083618164, loss=1.7124801874160767
I0418 21:51:48.278910 139684521441024 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.055558204650879, loss=1.8110413551330566
I0418 21:53:42.963821 139684513048320 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.4216160774230957, loss=1.811751127243042
I0418 21:55:37.393079 139684521441024 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.3899989128112793, loss=1.7990342378616333
I0418 21:57:31.895143 139684513048320 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.183527708053589, loss=1.7225919961929321
I0418 21:59:26.281309 139684521441024 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.5949132442474365, loss=1.7482504844665527
I0418 22:01:20.704021 139684513048320 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.5783069133758545, loss=1.8093197345733643
I0418 22:03:18.193947 139684521441024 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.9801136255264282, loss=1.7180445194244385
I0418 22:05:12.444427 139684513048320 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.1680686473846436, loss=1.7379803657531738
I0418 22:05:57.896151 139859899844416 spec.py:298] Evaluating on the training split.
I0418 22:06:36.397434 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 22:07:14.151395 139859899844416 spec.py:326] Evaluating on the test split.
I0418 22:07:33.860551 139859899844416 submission_runner.py:406] Time since start: 12702.23s, 	Step: 10441, 	{'train/ctc_loss': DeviceArray(0.4782733, dtype=float32), 'train/wer': 0.16294657418078237, 'validation/ctc_loss': DeviceArray(0.84119993, dtype=float32), 'validation/wer': 0.23877702630994993, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.52547765, dtype=float32), 'test/wer': 0.16748928564174437, 'test/num_examples': 2472, 'score': 12059.048158884048, 'total_duration': 12702.23068356514, 'accumulated_submission_time': 12059.048158884048, 'accumulated_eval_time': 639.1193578243256, 'accumulated_logging_time': 3.9247941970825195}
I0418 22:07:33.880752 139684521441024 logging_writer.py:48] [10441] accumulated_eval_time=639.119358, accumulated_logging_time=3.924794, accumulated_submission_time=12059.048159, global_step=10441, preemption_count=0, score=12059.048159, test/ctc_loss=0.5254776477813721, test/num_examples=2472, test/wer=0.167489, total_duration=12702.230684, train/ctc_loss=0.47827330231666565, train/wer=0.162947, validation/ctc_loss=0.8411999344825745, validation/num_examples=5348, validation/wer=0.238777
I0418 22:07:34.048875 139859899844416 checkpoints.py:356] Saving checkpoint at step: 10441
I0418 22:07:34.704647 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_10441
I0418 22:07:34.720240 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_10441.
I0418 22:08:43.263991 139684513048320 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.1123335361480713, loss=1.8301589488983154
I0418 22:10:37.687526 139682553779968 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.919265031814575, loss=1.715185284614563
I0418 22:12:32.212307 139684513048320 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.195420503616333, loss=1.734266996383667
I0418 22:14:26.400965 139682553779968 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.845529794692993, loss=1.706291675567627
I0418 22:16:20.841679 139684513048320 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.5470638275146484, loss=1.7394706010818481
I0418 22:18:15.534785 139682553779968 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.0856757164001465, loss=1.7307926416397095
I0418 22:20:09.574916 139684513048320 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.284887790679932, loss=1.788122534751892
I0418 22:22:04.126926 139682553779968 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.2534892559051514, loss=1.7693250179290771
I0418 22:23:58.367816 139684513048320 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.620070457458496, loss=1.741557002067566
I0418 22:25:55.611614 139684521441024 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.4021098613739014, loss=1.7034002542495728
I0418 22:27:49.271550 139684513048320 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.389547348022461, loss=1.7146968841552734
I0418 22:29:43.148947 139684521441024 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.197911739349365, loss=1.7062004804611206
I0418 22:31:37.445763 139684513048320 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.9283597469329834, loss=1.7516093254089355
I0418 22:33:31.631020 139684521441024 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.2454705238342285, loss=1.7336018085479736
I0418 22:35:25.921348 139684513048320 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.74467933177948, loss=1.701645851135254
I0418 22:37:20.091133 139684521441024 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.440044403076172, loss=1.7089840173721313
I0418 22:39:14.361726 139684513048320 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.798146963119507, loss=1.7046672105789185
I0418 22:41:08.916620 139684521441024 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.8073744773864746, loss=1.7476177215576172
I0418 22:43:03.199612 139684513048320 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.263396739959717, loss=1.7503870725631714
I0418 22:45:00.761938 139684521441024 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.4774367809295654, loss=1.6900615692138672
I0418 22:46:54.888725 139684513048320 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.6466641426086426, loss=1.6825082302093506
I0418 22:47:35.627947 139859899844416 spec.py:298] Evaluating on the training split.
I0418 22:48:14.359963 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 22:48:51.851804 139859899844416 spec.py:326] Evaluating on the test split.
I0418 22:49:11.635729 139859899844416 submission_runner.py:406] Time since start: 15200.01s, 	Step: 12537, 	{'train/ctc_loss': DeviceArray(0.43767014, dtype=float32), 'train/wer': 0.14767592597532156, 'validation/ctc_loss': DeviceArray(0.7503223, dtype=float32), 'validation/wer': 0.21440631361614682, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46131444, dtype=float32), 'test/wer': 0.1507728556049804, 'test/num_examples': 2472, 'score': 14459.928000926971, 'total_duration': 15200.005754947662, 'accumulated_submission_time': 14459.928000926971, 'accumulated_eval_time': 735.1240258216858, 'accumulated_logging_time': 4.78726053237915}
I0418 22:49:11.655779 139684746729216 logging_writer.py:48] [12537] accumulated_eval_time=735.124026, accumulated_logging_time=4.787261, accumulated_submission_time=14459.928001, global_step=12537, preemption_count=0, score=14459.928001, test/ctc_loss=0.46131443977355957, test/num_examples=2472, test/wer=0.150773, total_duration=15200.005755, train/ctc_loss=0.43767014145851135, train/wer=0.147676, validation/ctc_loss=0.7503222823143005, validation/num_examples=5348, validation/wer=0.214406
I0418 22:49:11.814716 139859899844416 checkpoints.py:356] Saving checkpoint at step: 12537
I0418 22:49:12.517726 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_12537
I0418 22:49:12.533180 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_12537.
I0418 22:50:25.278117 139684738336512 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.0696349143981934, loss=1.6738897562026978
I0418 22:52:19.275700 139684654409472 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.0259454250335693, loss=1.6127222776412964
I0418 22:54:13.362926 139684738336512 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.6900484561920166, loss=1.7084434032440186
I0418 22:56:07.745588 139684654409472 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.209987163543701, loss=1.639804482460022
I0418 22:58:01.830458 139684738336512 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.297614812850952, loss=1.6479378938674927
I0418 22:59:56.098446 139684654409472 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.3579187393188477, loss=1.6658780574798584
I0418 23:01:50.455425 139684738336512 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.187403678894043, loss=1.6983460187911987
I0418 23:03:44.642229 139684654409472 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.070180892944336, loss=1.6026403903961182
I0418 23:05:42.368933 139684746729216 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.0432515144348145, loss=1.642095685005188
I0418 23:07:36.455490 139684738336512 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.916515350341797, loss=1.6525522470474243
I0418 23:09:30.161521 139684746729216 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.724738597869873, loss=1.5751047134399414
I0418 23:11:24.248124 139684738336512 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.692403793334961, loss=1.6504634618759155
I0418 23:13:18.380570 139684746729216 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.6439385414123535, loss=1.683270812034607
I0418 23:15:12.537992 139684738336512 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.5308022499084473, loss=1.6200178861618042
I0418 23:17:06.958167 139684746729216 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.5022246837615967, loss=1.690119981765747
I0418 23:19:01.809701 139684738336512 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.1279942989349365, loss=1.696576476097107
I0418 23:20:56.755177 139684746729216 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.6460587978363037, loss=1.6782585382461548
I0418 23:22:51.599034 139684738336512 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.566161632537842, loss=1.7522451877593994
I0418 23:24:45.616704 139684746729216 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.190586090087891, loss=1.6853456497192383
I0418 23:26:42.998574 139684746729216 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.924202919006348, loss=1.7024438381195068
I0418 23:28:36.946566 139684738336512 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.736394166946411, loss=1.6183170080184937
I0418 23:29:13.204213 139859899844416 spec.py:298] Evaluating on the training split.
I0418 23:29:52.037251 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 23:30:29.383565 139859899844416 spec.py:326] Evaluating on the test split.
I0418 23:30:48.780013 139859899844416 submission_runner.py:406] Time since start: 17697.15s, 	Step: 14633, 	{'train/ctc_loss': DeviceArray(0.3912211, dtype=float32), 'train/wer': 0.1333141896922128, 'validation/ctc_loss': DeviceArray(0.72585994, dtype=float32), 'validation/wer': 0.20535653986049068, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4312457, dtype=float32), 'test/wer': 0.14000771840026, 'test/num_examples': 2472, 'score': 16860.567298412323, 'total_duration': 17697.149845838547, 'accumulated_submission_time': 16860.567298412323, 'accumulated_eval_time': 830.696536064148, 'accumulated_logging_time': 5.690735578536987}
I0418 23:30:48.800334 139685760489216 logging_writer.py:48] [14633] accumulated_eval_time=830.696536, accumulated_logging_time=5.690736, accumulated_submission_time=16860.567298, global_step=14633, preemption_count=0, score=16860.567298, test/ctc_loss=0.43124571442604065, test/num_examples=2472, test/wer=0.140008, total_duration=17697.149846, train/ctc_loss=0.3912211060523987, train/wer=0.133314, validation/ctc_loss=0.7258599400520325, validation/num_examples=5348, validation/wer=0.205357
I0418 23:30:48.950487 139859899844416 checkpoints.py:356] Saving checkpoint at step: 14633
I0418 23:30:49.585539 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_14633
I0418 23:30:49.600856 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_14633.
I0418 23:32:06.792454 139685752096512 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.5082621574401855, loss=1.5981225967407227
I0418 23:34:01.574136 139685659776768 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.861163377761841, loss=1.7004294395446777
I0418 23:35:55.887959 139685752096512 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.0933570861816406, loss=1.6189934015274048
I0418 23:37:50.409995 139685659776768 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.5055205821990967, loss=1.684183120727539
I0418 23:39:44.926214 139685752096512 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.155302047729492, loss=1.5934666395187378
I0418 23:41:39.528277 139685659776768 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.6312437057495117, loss=1.633873701095581
I0418 23:43:34.225823 139685752096512 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.1162264347076416, loss=1.64926278591156
I0418 23:45:28.800115 139685659776768 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.433925628662109, loss=1.6616942882537842
I0418 23:47:26.351134 139684890089216 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.6573774814605713, loss=1.6270968914031982
I0418 23:49:20.275809 139684881696512 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.7811758518218994, loss=1.5877799987792969
I0418 23:51:14.247791 139684890089216 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.644078493118286, loss=1.7097508907318115
I0418 23:53:08.479536 139684881696512 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.6110334396362305, loss=1.6015269756317139
I0418 23:55:02.816453 139684890089216 logging_writer.py:48] [15900] global_step=15900, grad_norm=4.311148166656494, loss=1.6937198638916016
I0418 23:56:55.843744 139859899844416 spec.py:298] Evaluating on the training split.
I0418 23:57:35.282527 139859899844416 spec.py:310] Evaluating on the validation split.
I0418 23:58:13.680483 139859899844416 spec.py:326] Evaluating on the test split.
I0418 23:58:33.114562 139859899844416 submission_runner.py:406] Time since start: 19361.49s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.34792152, dtype=float32), 'train/wer': 0.12396578174159495, 'validation/ctc_loss': DeviceArray(0.7226281, dtype=float32), 'validation/wer': 0.20654323727194668, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4334989, dtype=float32), 'test/wer': 0.1407795584262588, 'test/num_examples': 2472, 'score': 18426.790853977203, 'total_duration': 19361.485968351364, 'accumulated_submission_time': 18426.790853977203, 'accumulated_eval_time': 927.9656536579132, 'accumulated_logging_time': 6.513792037963867}
I0418 23:58:33.134962 139684598249216 logging_writer.py:48] [16000] accumulated_eval_time=927.965654, accumulated_logging_time=6.513792, accumulated_submission_time=18426.790854, global_step=16000, preemption_count=0, score=18426.790854, test/ctc_loss=0.43349888920783997, test/num_examples=2472, test/wer=0.140780, total_duration=19361.485968, train/ctc_loss=0.3479215204715729, train/wer=0.123966, validation/ctc_loss=0.722628116607666, validation/num_examples=5348, validation/wer=0.206543
I0418 23:58:33.282578 139859899844416 checkpoints.py:356] Saving checkpoint at step: 16000
I0418 23:58:33.961269 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0418 23:58:33.976708 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0418 23:58:33.989574 139684589856512 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18426.790854
I0418 23:58:34.074520 139859899844416 checkpoints.py:356] Saving checkpoint at step: 16000
I0418 23:58:35.036510 139859899844416 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0418 23:58:35.051889 139859899844416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0418 23:58:36.483832 139859899844416 submission_runner.py:567] Tuning trial 1/1
I0418 23:58:36.484116 139859899844416 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 23:58:36.490052 139859899844416 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.87637, dtype=float32), 'train/wer': 3.749614767923703, 'validation/ctc_loss': DeviceArray(31.066803, dtype=float32), 'validation/wer': 3.4339260388426323, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.069, dtype=float32), 'test/wer': 3.6274450063981476, 'test/num_examples': 2472, 'score': 56.90117430686951, 'total_duration': 224.9784083366394, 'accumulated_submission_time': 56.90117430686951, 'accumulated_eval_time': 168.07708072662354, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2066, {'train/ctc_loss': DeviceArray(6.112874, dtype=float32), 'train/wer': 0.893730304216185, 'validation/ctc_loss': DeviceArray(6.0239143, dtype=float32), 'validation/wer': 0.8606740055379213, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9477425, dtype=float32), 'test/wer': 0.8582048625921638, 'test/num_examples': 2472, 'score': 2457.141233444214, 'total_duration': 2714.0196969509125, 'accumulated_submission_time': 2457.141233444214, 'accumulated_eval_time': 256.2839529514313, 'accumulated_logging_time': 0.5689632892608643, 'global_step': 2066, 'preemption_count': 0}), (4160, {'train/ctc_loss': DeviceArray(1.4487426, dtype=float32), 'train/wer': 0.40490637402445195, 'validation/ctc_loss': DeviceArray(1.8827316, dtype=float32), 'validation/wer': 0.46255149591409467, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.4370253, dtype=float32), 'test/wer': 0.39365872483903075, 'test/num_examples': 2472, 'score': 4858.070352554321, 'total_duration': 5209.427096128464, 'accumulated_submission_time': 4858.070352554321, 'accumulated_eval_time': 349.92733335494995, 'accumulated_logging_time': 1.3758723735809326, 'global_step': 4160, 'preemption_count': 0}), (6251, {'train/ctc_loss': DeviceArray(0.71834177, dtype=float32), 'train/wer': 0.24431899046444328, 'validation/ctc_loss': DeviceArray(1.1505024, dtype=float32), 'validation/wer': 0.31651053073353336, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.789191, dtype=float32), 'test/wer': 0.25029959579956534, 'test/num_examples': 2472, 'score': 7258.190971851349, 'total_duration': 7706.9273335933685, 'accumulated_submission_time': 7258.190971851349, 'accumulated_eval_time': 446.472309589386, 'accumulated_logging_time': 2.1825690269470215, 'global_step': 6251, 'preemption_count': 0}), (8345, {'train/ctc_loss': DeviceArray(0.5618466, dtype=float32), 'train/wer': 0.19125239556388693, 'validation/ctc_loss': DeviceArray(0.94501865, dtype=float32), 'validation/wer': 0.2684830533820876, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6029804, dtype=float32), 'test/wer': 0.1947068023480186, 'test/num_examples': 2472, 'score': 9658.996666431427, 'total_duration': 10205.31457400322, 'accumulated_submission_time': 9658.996666431427, 'accumulated_eval_time': 543.15793633461, 'accumulated_logging_time': 3.0501930713653564, 'global_step': 8345, 'preemption_count': 0}), (10441, {'train/ctc_loss': DeviceArray(0.4782733, dtype=float32), 'train/wer': 0.16294657418078237, 'validation/ctc_loss': DeviceArray(0.84119993, dtype=float32), 'validation/wer': 0.23877702630994993, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.52547765, dtype=float32), 'test/wer': 0.16748928564174437, 'test/num_examples': 2472, 'score': 12059.048158884048, 'total_duration': 12702.23068356514, 'accumulated_submission_time': 12059.048158884048, 'accumulated_eval_time': 639.1193578243256, 'accumulated_logging_time': 3.9247941970825195, 'global_step': 10441, 'preemption_count': 0}), (12537, {'train/ctc_loss': DeviceArray(0.43767014, dtype=float32), 'train/wer': 0.14767592597532156, 'validation/ctc_loss': DeviceArray(0.7503223, dtype=float32), 'validation/wer': 0.21440631361614682, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46131444, dtype=float32), 'test/wer': 0.1507728556049804, 'test/num_examples': 2472, 'score': 14459.928000926971, 'total_duration': 15200.005754947662, 'accumulated_submission_time': 14459.928000926971, 'accumulated_eval_time': 735.1240258216858, 'accumulated_logging_time': 4.78726053237915, 'global_step': 12537, 'preemption_count': 0}), (14633, {'train/ctc_loss': DeviceArray(0.3912211, dtype=float32), 'train/wer': 0.1333141896922128, 'validation/ctc_loss': DeviceArray(0.72585994, dtype=float32), 'validation/wer': 0.20535653986049068, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4312457, dtype=float32), 'test/wer': 0.14000771840026, 'test/num_examples': 2472, 'score': 16860.567298412323, 'total_duration': 17697.149845838547, 'accumulated_submission_time': 16860.567298412323, 'accumulated_eval_time': 830.696536064148, 'accumulated_logging_time': 5.690735578536987, 'global_step': 14633, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.34792152, dtype=float32), 'train/wer': 0.12396578174159495, 'validation/ctc_loss': DeviceArray(0.7226281, dtype=float32), 'validation/wer': 0.20654323727194668, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4334989, dtype=float32), 'test/wer': 0.1407795584262588, 'test/num_examples': 2472, 'score': 18426.790853977203, 'total_duration': 19361.485968351364, 'accumulated_submission_time': 18426.790853977203, 'accumulated_eval_time': 927.9656536579132, 'accumulated_logging_time': 6.513792037963867, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0418 23:58:36.490231 139859899844416 submission_runner.py:570] Timing: 18426.790853977203
I0418 23:58:36.490291 139859899844416 submission_runner.py:571] ====================
I0418 23:58:36.490812 139859899844416 submission_runner.py:631] Final librispeech_deepspeech score: 18426.790853977203
