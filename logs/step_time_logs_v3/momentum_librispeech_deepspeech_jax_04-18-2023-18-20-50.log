I0418 18:21:11.261231 140254504810304 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax.
I0418 18:21:11.329538 140254504810304 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 18:21:12.193005 140254504810304 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0418 18:21:12.193721 140254504810304 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 18:21:12.197772 140254504810304 submission_runner.py:528] Using RNG seed 2307044562
I0418 18:21:14.829851 140254504810304 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 18:21:14.830090 140254504810304 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1.
I0418 18:21:14.830327 140254504810304 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/hparams.json.
I0418 18:21:14.952536 140254504810304 submission_runner.py:232] Initializing dataset.
I0418 18:21:14.952718 140254504810304 submission_runner.py:239] Initializing model.
I0418 18:21:31.962701 140254504810304 submission_runner.py:249] Initializing optimizer.
I0418 18:21:32.505724 140254504810304 submission_runner.py:256] Initializing metrics bundle.
I0418 18:21:32.505921 140254504810304 submission_runner.py:273] Initializing checkpoint and logger.
I0418 18:21:32.506731 140254504810304 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0418 18:21:32.506990 140254504810304 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 18:21:32.507055 140254504810304 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 18:21:33.365481 140254504810304 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0418 18:21:33.366404 140254504810304 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0418 18:21:33.372724 140254504810304 submission_runner.py:309] Starting training loop.
I0418 18:21:33.571626 140254504810304 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:21:33.604973 140254504810304 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:21:33.917514 140254504810304 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0418 18:22:28.533526 140079569819392 logging_writer.py:48] [0] global_step=0, grad_norm=21.682126998901367, loss=33.25893783569336
I0418 18:22:28.554884 140254504810304 spec.py:298] Evaluating on the training split.
I0418 18:22:28.678147 140254504810304 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:22:28.706457 140254504810304 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:22:28.997794 140254504810304 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0418 18:24:08.371167 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 18:24:08.461234 140254504810304 input_pipeline.py:20] Loading split = dev-clean
I0418 18:24:08.465898 140254504810304 input_pipeline.py:20] Loading split = dev-other
I0418 18:25:06.622904 140254504810304 spec.py:326] Evaluating on the test split.
I0418 18:25:06.716047 140254504810304 input_pipeline.py:20] Loading split = test-clean
I0418 18:25:43.858969 140254504810304 submission_runner.py:406] Time since start: 250.48s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.60102, dtype=float32), 'train/wer': 5.107547425192095, 'validation/ctc_loss': DeviceArray(31.428885, dtype=float32), 'validation/wer': 4.688805487751932, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.706348, dtype=float32), 'test/wer': 5.009627688745354, 'test/num_examples': 2472, 'score': 55.1819953918457, 'total_duration': 250.4847912788391, 'accumulated_submission_time': 55.1819953918457, 'accumulated_eval_time': 195.3026406764984, 'accumulated_logging_time': 0}
I0418 18:25:43.884203 140076835141376 logging_writer.py:48] [1] accumulated_eval_time=195.302641, accumulated_logging_time=0, accumulated_submission_time=55.181995, global_step=1, preemption_count=0, score=55.181995, test/ctc_loss=31.706348419189453, test/num_examples=2472, test/wer=5.009628, total_duration=250.484791, train/ctc_loss=32.60102081298828, train/wer=5.107547, validation/ctc_loss=31.428884506225586, validation/num_examples=5348, validation/wer=4.688805
I0418 18:25:43.978645 140254504810304 checkpoints.py:356] Saving checkpoint at step: 1
I0418 18:25:44.268636 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0418 18:25:44.269288 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0418 18:27:57.461513 140080276649728 logging_writer.py:48] [100] global_step=100, grad_norm=5.499612331390381, loss=6.101100921630859
I0418 18:29:55.626305 140080285042432 logging_writer.py:48] [200] global_step=200, grad_norm=13.522064208984375, loss=6.508603572845459
I0418 18:31:53.186252 140080276649728 logging_writer.py:48] [300] global_step=300, grad_norm=4.220880508422852, loss=5.946041107177734
I0418 18:33:47.375727 140080285042432 logging_writer.py:48] [400] global_step=400, grad_norm=3.0296013355255127, loss=5.905999183654785
I0418 18:35:42.798632 140080276649728 logging_writer.py:48] [500] global_step=500, grad_norm=0.980897068977356, loss=5.811351299285889
I0418 18:37:37.508534 140080285042432 logging_writer.py:48] [600] global_step=600, grad_norm=1.0517221689224243, loss=5.704236030578613
I0418 18:39:34.576602 140080276649728 logging_writer.py:48] [700] global_step=700, grad_norm=2.1240291595458984, loss=5.490991592407227
I0418 18:41:26.924525 140080285042432 logging_writer.py:48] [800] global_step=800, grad_norm=1.3349589109420776, loss=5.093650817871094
I0418 18:43:23.694434 140080276649728 logging_writer.py:48] [900] global_step=900, grad_norm=1.5181587934494019, loss=4.7898454666137695
I0418 18:45:20.979210 140080285042432 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.878957211971283, loss=4.416965961456299
I0418 18:47:18.686165 140078466832128 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.3349082469940186, loss=4.200125217437744
I0418 18:49:15.279353 140078458439424 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3320776224136353, loss=3.9610915184020996
I0418 18:51:11.262569 140078466832128 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.983223557472229, loss=3.7940611839294434
I0418 18:53:03.205906 140078458439424 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0558456182479858, loss=3.592050075531006
I0418 18:54:56.826956 140078466832128 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3211473226547241, loss=3.492894172668457
I0418 18:56:52.457291 140078458439424 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.332802653312683, loss=3.3586502075195312
I0418 18:58:48.707527 140078466832128 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0711052417755127, loss=3.31706166267395
I0418 19:00:44.375690 140078458439424 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.7904781103134155, loss=3.2619762420654297
I0418 19:02:39.481962 140078466832128 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8820509314537048, loss=3.2215802669525146
I0418 19:04:34.543699 140078458439424 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0756406784057617, loss=3.164771556854248
I0418 19:05:46.330970 140254504810304 spec.py:298] Evaluating on the training split.
I0418 19:06:16.126007 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 19:06:51.534907 140254504810304 spec.py:326] Evaluating on the test split.
I0418 19:07:10.314393 140254504810304 submission_runner.py:406] Time since start: 2736.94s, 	Step: 2061, 	{'train/ctc_loss': DeviceArray(5.6104712, dtype=float32), 'train/wer': 0.9244763238656258, 'validation/ctc_loss': DeviceArray(5.7855654, dtype=float32), 'validation/wer': 0.8862893033217879, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.54653, dtype=float32), 'test/wer': 0.8858489224707006, 'test/num_examples': 2472, 'score': 2457.214750289917, 'total_duration': 2736.938204050064, 'accumulated_submission_time': 2457.214750289917, 'accumulated_eval_time': 279.28281140327454, 'accumulated_logging_time': 0.4118919372558594}
I0418 19:07:10.333051 140078466832128 logging_writer.py:48] [2061] accumulated_eval_time=279.282811, accumulated_logging_time=0.411892, accumulated_submission_time=2457.214750, global_step=2061, preemption_count=0, score=2457.214750, test/ctc_loss=5.546529769897461, test/num_examples=2472, test/wer=0.885849, total_duration=2736.938204, train/ctc_loss=5.610471248626709, train/wer=0.924476, validation/ctc_loss=5.785565376281738, validation/num_examples=5348, validation/wer=0.886289
I0418 19:07:10.433430 140254504810304 checkpoints.py:356] Saving checkpoint at step: 2061
I0418 19:07:10.874058 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_2061
I0418 19:07:10.884486 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_2061.
I0418 19:07:56.409927 140078458439424 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8238738775253296, loss=3.0440456867218018
I0418 19:09:49.735448 140077430728448 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.5998127460479736, loss=3.003617763519287
I0418 19:11:43.983492 140078458439424 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0534977912902832, loss=3.0028440952301025
I0418 19:13:36.725451 140077430728448 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8187167644500732, loss=2.945695400238037
I0418 19:15:33.126664 140078458439424 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9921396374702454, loss=2.8979005813598633
I0418 19:17:29.295126 140077430728448 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.1280423402786255, loss=2.8541619777679443
I0418 19:19:24.508605 140078458439424 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0054384469985962, loss=2.8418595790863037
I0418 19:21:17.542748 140077430728448 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8244714736938477, loss=2.8112902641296387
I0418 19:23:11.454773 140078458439424 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0639443397521973, loss=2.783594846725464
I0418 19:25:08.905150 140077430728448 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9597880244255066, loss=2.7560746669769287
I0418 19:27:09.064990 140078466832128 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0217112302780151, loss=2.6326966285705566
I0418 19:29:04.693231 140078458439424 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.010850429534912, loss=2.7256598472595215
I0418 19:30:59.987570 140078466832128 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8545222878456116, loss=2.688690423965454
I0418 19:32:55.083109 140078458439424 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7908665537834167, loss=2.7037353515625
I0418 19:34:48.593219 140078466832128 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8175933957099915, loss=2.6905481815338135
I0418 19:36:42.254298 140078458439424 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9254017472267151, loss=2.6860740184783936
I0418 19:38:35.641924 140078466832128 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8817073106765747, loss=2.631958246231079
I0418 19:40:32.106820 140078458439424 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7454524040222168, loss=2.555079936981201
I0418 19:42:26.864484 140078466832128 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9345076680183411, loss=2.5763635635375977
I0418 19:44:21.197571 140078458439424 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8263164162635803, loss=2.645573616027832
I0418 19:46:15.955644 140078466832128 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.058283567428589, loss=2.8522074222564697
I0418 19:47:10.914624 140254504810304 spec.py:298] Evaluating on the training split.
I0418 19:47:49.038279 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 19:48:27.137969 140254504810304 spec.py:326] Evaluating on the test split.
I0418 19:48:46.765157 140254504810304 submission_runner.py:406] Time since start: 5233.39s, 	Step: 4146, 	{'train/ctc_loss': DeviceArray(1.3405861, dtype=float32), 'train/wer': 0.4007511386942972, 'validation/ctc_loss': DeviceArray(1.7707318, dtype=float32), 'validation/wer': 0.4575538596609712, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3610432, dtype=float32), 'test/wer': 0.3926228342778218, 'test/num_examples': 2472, 'score': 4857.214957237244, 'total_duration': 5233.389305114746, 'accumulated_submission_time': 4857.214957237244, 'accumulated_eval_time': 375.13050150871277, 'accumulated_logging_time': 0.9845161437988281}
I0418 19:48:46.784463 140079997638400 logging_writer.py:48] [4146] accumulated_eval_time=375.130502, accumulated_logging_time=0.984516, accumulated_submission_time=4857.214957, global_step=4146, preemption_count=0, score=4857.214957, test/ctc_loss=1.3610432147979736, test/num_examples=2472, test/wer=0.392623, total_duration=5233.389305, train/ctc_loss=1.3405860662460327, train/wer=0.400751, validation/ctc_loss=1.770731806755066, validation/num_examples=5348, validation/wer=0.457554
I0418 19:48:46.886369 140254504810304 checkpoints.py:356] Saving checkpoint at step: 4146
I0418 19:48:47.361449 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_4146
I0418 19:48:47.371653 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_4146.
I0418 19:49:48.744371 140079989245696 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8411045074462891, loss=2.5304975509643555
I0418 19:51:40.422008 140079938889472 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9065390229225159, loss=2.5606234073638916
I0418 19:53:32.942844 140079989245696 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7978706955909729, loss=2.527033567428589
I0418 19:55:25.521409 140079938889472 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6961750388145447, loss=2.4300851821899414
I0418 19:57:18.445186 140079989245696 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9634064435958862, loss=2.5040245056152344
I0418 19:59:13.726018 140079938889472 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8372055888175964, loss=2.533806800842285
I0418 20:01:05.625118 140079989245696 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9742624163627625, loss=2.5575621128082275
I0418 20:03:01.721243 140079938889472 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9704087972640991, loss=2.502253532409668
I0418 20:04:58.507386 140079989245696 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6655580997467041, loss=2.420168399810791
I0418 20:06:54.588681 140079938889472 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6984484791755676, loss=2.334900140762329
I0418 20:08:52.442361 140079997638400 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7577242255210876, loss=2.3459959030151367
I0418 20:10:44.517666 140079989245696 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9036566019058228, loss=2.4261064529418945
I0418 20:12:37.790686 140079997638400 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8205443620681763, loss=2.4272165298461914
I0418 20:14:31.185134 140079989245696 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7242361903190613, loss=2.336902141571045
I0418 20:16:26.626263 140079997638400 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6929891109466553, loss=2.371798276901245
I0418 20:18:21.394595 140079989245696 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8677980899810791, loss=2.4681689739227295
I0418 20:20:15.459189 140079997638400 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5988045930862427, loss=2.341938018798828
I0418 20:22:07.256603 140079989245696 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6548519730567932, loss=2.391761064529419
I0418 20:24:02.803653 140079997638400 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9876050353050232, loss=2.446816921234131
I0418 20:25:58.398346 140079989245696 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6547881364822388, loss=2.374091625213623
I0418 20:27:54.556294 140079997638400 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6964467763900757, loss=2.4372739791870117
I0418 20:28:48.486119 140254504810304 spec.py:298] Evaluating on the training split.
I0418 20:29:27.467578 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 20:30:07.359692 140254504810304 spec.py:326] Evaluating on the test split.
I0418 20:30:28.237984 140254504810304 submission_runner.py:406] Time since start: 7734.86s, 	Step: 6249, 	{'train/ctc_loss': DeviceArray(1.0248456, dtype=float32), 'train/wer': 0.3281728523912688, 'validation/ctc_loss': DeviceArray(1.5237039, dtype=float32), 'validation/wer': 0.40150893882237165, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.1006055, dtype=float32), 'test/wer': 0.32986005321633866, 'test/num_examples': 2472, 'score': 7258.297897815704, 'total_duration': 7734.862092018127, 'accumulated_submission_time': 7258.297897815704, 'accumulated_eval_time': 474.8792450428009, 'accumulated_logging_time': 1.594550371170044}
I0418 20:30:28.258604 140079997638400 logging_writer.py:48] [6249] accumulated_eval_time=474.879245, accumulated_logging_time=1.594550, accumulated_submission_time=7258.297898, global_step=6249, preemption_count=0, score=7258.297898, test/ctc_loss=1.1006054878234863, test/num_examples=2472, test/wer=0.329860, total_duration=7734.862092, train/ctc_loss=1.0248456001281738, train/wer=0.328173, validation/ctc_loss=1.523703932762146, validation/num_examples=5348, validation/wer=0.401509
I0418 20:30:28.354591 140254504810304 checkpoints.py:356] Saving checkpoint at step: 6249
I0418 20:30:28.806798 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_6249
I0418 20:30:28.817051 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_6249.
I0418 20:31:28.339771 140079989245696 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8671732544898987, loss=2.3966972827911377
I0418 20:33:23.432254 140079930496768 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6552098989486694, loss=2.3073105812072754
I0418 20:35:14.857664 140079989245696 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6339486837387085, loss=2.3173561096191406
I0418 20:37:08.148140 140079930496768 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6794739365577698, loss=2.3901591300964355
I0418 20:39:03.048646 140079989245696 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8094523549079895, loss=2.354729175567627
I0418 20:40:59.649094 140079930496768 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6908784508705139, loss=2.371704578399658
I0418 20:42:56.587221 140079989245696 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8974135518074036, loss=2.3643343448638916
I0418 20:44:49.642056 140079930496768 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6645615696907043, loss=2.2150495052337646
I0418 20:46:41.025082 140079989245696 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8263270258903503, loss=2.275757312774658
I0418 20:48:33.821996 140079930496768 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.0327552556991577, loss=2.5842974185943604
I0418 20:50:33.290990 140079997638400 logging_writer.py:48] [7300] global_step=7300, grad_norm=36.10451126098633, loss=11.328838348388672
I0418 20:52:28.417022 140079989245696 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1831.829833984375
I0418 20:54:26.405563 140079997638400 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1875.170166015625
I0418 20:56:25.179253 140079989245696 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1858.561279296875
I0418 20:58:18.619162 140079997638400 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1758.2296142578125
I0418 21:00:09.958932 140079989245696 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1838.0113525390625
I0418 21:02:01.239269 140079997638400 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1824.5186767578125
I0418 21:03:54.915703 140079989245696 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1783.83544921875
I0418 21:05:49.640971 140079997638400 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1802.6773681640625
I0418 21:07:40.939815 140079989245696 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1876.6822509765625
I0418 21:09:35.567982 140079997638400 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1804.8380126953125
I0418 21:10:29.792690 140254504810304 spec.py:298] Evaluating on the training split.
I0418 21:10:59.136037 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 21:11:35.584514 140254504810304 spec.py:326] Evaluating on the test split.
I0418 21:11:54.241143 140254504810304 submission_runner.py:406] Time since start: 10220.87s, 	Step: 8350, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9659.2427880764, 'total_duration': 10220.865172624588, 'accumulated_submission_time': 9659.2427880764, 'accumulated_eval_time': 559.3245084285736, 'accumulated_logging_time': 2.1767375469207764}
I0418 21:11:54.260331 140080427718400 logging_writer.py:48] [8350] accumulated_eval_time=559.324508, accumulated_logging_time=2.176738, accumulated_submission_time=9659.242788, global_step=8350, preemption_count=0, score=9659.242788, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10220.865173, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 21:11:54.358541 140254504810304 checkpoints.py:356] Saving checkpoint at step: 8350
I0418 21:11:54.767669 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8350
I0418 21:11:54.778047 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8350.
I0418 21:12:52.623336 140080419325696 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1809.8143310546875
I0418 21:14:46.047089 140080352184064 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1875.307373046875
I0418 21:16:37.689156 140080419325696 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1857.0782470703125
I0418 21:18:30.616827 140080352184064 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1905.315673828125
I0418 21:20:27.025398 140080419325696 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1800.268798828125
I0418 21:22:23.456111 140080352184064 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1830.38916015625
I0418 21:24:20.055990 140080419325696 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1794.715576171875
I0418 21:26:16.650101 140080352184064 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1772.349853515625
I0418 21:28:13.095929 140080419325696 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1766.1138916015625
I0418 21:30:12.381222 140078466832128 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1835.2442626953125
I0418 21:32:07.554822 140078458439424 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1765.870361328125
I0418 21:34:05.175210 140078466832128 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1822.829833984375
I0418 21:36:01.704770 140078458439424 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1850.9017333984375
I0418 21:37:55.981685 140078466832128 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1845.2982177734375
I0418 21:39:49.165432 140078458439424 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1820.4964599609375
I0418 21:41:43.854324 140078466832128 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1805.474365234375
I0418 21:43:37.476314 140078458439424 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1861.399169921875
I0418 21:45:29.371917 140078466832128 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1786.4495849609375
I0418 21:47:21.166907 140078458439424 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1795.470703125
I0418 21:49:16.341296 140080427718400 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1836.692626953125
I0418 21:51:08.364664 140080419325696 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1888.1708984375
I0418 21:51:55.262288 140254504810304 spec.py:298] Evaluating on the training split.
I0418 21:52:24.384395 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 21:53:00.205832 140254504810304 spec.py:326] Evaluating on the test split.
I0418 21:53:19.047850 140254504810304 submission_runner.py:406] Time since start: 12705.67s, 	Step: 10442, 	{'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12059.69641494751, 'total_duration': 12705.671605110168, 'accumulated_submission_time': 12059.69641494751, 'accumulated_eval_time': 643.1065981388092, 'accumulated_logging_time': 2.7165756225585938}
I0418 21:53:19.071180 140080427718400 logging_writer.py:48] [10442] accumulated_eval_time=643.106598, accumulated_logging_time=2.716576, accumulated_submission_time=12059.696415, global_step=10442, preemption_count=0, score=12059.696415, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=12705.671605, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 21:53:19.166007 140254504810304 checkpoints.py:356] Saving checkpoint at step: 10442
I0418 21:53:19.663543 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_10442
I0418 21:53:19.673454 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_10442.
I0418 21:54:27.451065 140080419325696 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1887.474853515625
I0418 21:56:22.242468 140077430728448 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1858.0216064453125
I0418 21:58:16.585661 140080419325696 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1771.3687744140625
I0418 22:00:09.348965 140077430728448 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1773.3321533203125
I0418 22:02:03.113549 140080419325696 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1819.3321533203125
I0418 22:03:59.454865 140077430728448 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1902.4837646484375
I0418 22:05:54.954752 140080419325696 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1800.268798828125
I0418 22:07:50.914159 140077430728448 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1880.1279296875
I0418 22:09:47.487773 140080419325696 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1838.8035888671875
I0418 22:11:46.666885 140080427718400 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1828.689208984375
I0418 22:13:41.057763 140080419325696 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1826.0804443359375
I0418 22:15:32.625939 140080427718400 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1870.784912109375
I0418 22:17:27.318475 140080419325696 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1871.1953125
I0418 22:19:20.708151 140080427718400 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1828.036376953125
I0418 22:21:15.513340 140080419325696 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1859.9114990234375
I0418 22:23:10.304454 140080427718400 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1862.6180419921875
I0418 22:25:03.736285 140080419325696 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1870.375
I0418 22:26:57.065345 140080427718400 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1909.43701171875
I0418 22:28:48.884829 140080419325696 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0, loss=1837.7474365234375
I0418 22:30:47.001061 140080100038400 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1824.77880859375
I0418 22:32:41.585117 140080091645696 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1839.067626953125
I0418 22:33:20.394940 140254504810304 spec.py:298] Evaluating on the training split.
I0418 22:33:50.140750 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 22:34:26.515446 140254504810304 spec.py:326] Evaluating on the test split.
I0418 22:34:45.444817 140254504810304 submission_runner.py:406] Time since start: 15192.07s, 	Step: 12535, 	{'train/ctc_loss': DeviceArray(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14460.385578155518, 'total_duration': 15192.068930387497, 'accumulated_submission_time': 14460.385578155518, 'accumulated_eval_time': 728.1533768177032, 'accumulated_logging_time': 3.3453054428100586}
I0418 22:34:45.465455 140079884998400 logging_writer.py:48] [12535] accumulated_eval_time=728.153377, accumulated_logging_time=3.345305, accumulated_submission_time=14460.385578, global_step=12535, preemption_count=0, score=14460.385578, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=15192.068930, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 22:34:45.563525 140254504810304 checkpoints.py:356] Saving checkpoint at step: 12535
I0418 22:34:45.966320 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_12535
I0418 22:34:45.976663 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_12535.
I0418 22:36:01.690809 140079876605696 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1822.829833984375
I0418 22:37:56.629499 140079792678656 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1837.219970703125
I0418 22:39:51.331111 140079876605696 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1783.462646484375
I0418 22:41:48.076120 140079792678656 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1805.2197265625
I0418 22:43:45.007627 140079876605696 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1844.3675537109375
I0418 22:45:40.386236 140079792678656 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1852.375
I0418 22:47:34.275947 140079876605696 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1766.1138916015625
I0418 22:49:26.640335 140079792678656 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1841.051513671875
I0418 22:51:21.783699 140079884998400 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1845.1651611328125
I0418 22:53:16.546670 140079876605696 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1851.5711669921875
I0418 22:55:10.386917 140079884998400 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1849.1636962890625
I0418 22:57:01.853510 140079876605696 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1835.375732421875
I0418 22:58:57.230929 140079884998400 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1857.6171875
I0418 23:00:52.579724 140079876605696 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1832.7479248046875
I0418 23:02:48.181596 140079884998400 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1812.8902587890625
I0418 23:04:42.387237 140079876605696 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1866.96484375
I0418 23:06:33.900446 140079884998400 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1846.629150390625
I0418 23:08:27.646840 140079876605696 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1778.7535400390625
I0418 23:10:21.746332 140079884998400 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1822.829833984375
I0418 23:12:20.496804 140079884998400 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1808.4078369140625
I0418 23:14:14.807115 140079876605696 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1833.0103759765625
I0418 23:14:46.930426 140254504810304 spec.py:298] Evaluating on the training split.
I0418 23:15:16.704642 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 23:15:53.058411 140254504810304 spec.py:326] Evaluating on the test split.
I0418 23:16:12.117931 140254504810304 submission_runner.py:406] Time since start: 17678.74s, 	Step: 14630, 	{'train/ctc_loss': DeviceArray(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16861.308940172195, 'total_duration': 17678.74213719368, 'accumulated_submission_time': 16861.308940172195, 'accumulated_eval_time': 813.3378653526306, 'accumulated_logging_time': 3.879986047744751}
I0418 23:16:12.138710 140080100038400 logging_writer.py:48] [14630] accumulated_eval_time=813.337865, accumulated_logging_time=3.879986, accumulated_submission_time=16861.308940, global_step=14630, preemption_count=0, score=16861.308940, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=17678.742137, train/ctc_loss=1746.1109619140625, train/wer=0.942824, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 23:16:12.237434 140254504810304 checkpoints.py:356] Saving checkpoint at step: 14630
I0418 23:16:12.678040 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_14630
I0418 23:16:12.688590 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_14630.
I0418 23:17:33.076221 140080091645696 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1811.22265625
I0418 23:19:27.005914 140079999325952 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1770.7562255859375
I0418 23:21:20.972195 140080091645696 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1900.788818359375
I0418 23:23:16.697679 140079999325952 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1850.5003662109375
I0418 23:25:10.700729 140080091645696 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1844.899169921875
I0418 23:27:04.983600 140079999325952 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1798.7508544921875
I0418 23:28:58.757978 140080091645696 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1785.2037353515625
I0418 23:30:50.644723 140079999325952 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1839.9930419921875
I0418 23:32:45.632210 140080100038400 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1827.6448974609375
I0418 23:34:38.855123 140080091645696 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1795.5966796875
I0418 23:36:33.644376 140080100038400 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1752.8121337890625
I0418 23:38:24.997952 140080091645696 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1870.2384033203125
I0418 23:40:16.309609 140080100038400 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1790.9488525390625
I0418 23:42:06.319318 140254504810304 spec.py:298] Evaluating on the training split.
I0418 23:42:35.855077 140254504810304 spec.py:310] Evaluating on the validation split.
I0418 23:43:12.104085 140254504810304 spec.py:326] Evaluating on the test split.
I0418 23:43:30.301911 140254504810304 submission_runner.py:406] Time since start: 19316.93s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18414.918209791183, 'total_duration': 19316.92738175392, 'accumulated_submission_time': 18414.918209791183, 'accumulated_eval_time': 897.3187267780304, 'accumulated_logging_time': 4.453773260116577}
I0418 23:43:30.323493 140079808198400 logging_writer.py:48] [16000] accumulated_eval_time=897.318727, accumulated_logging_time=4.453773, accumulated_submission_time=18414.918210, global_step=16000, preemption_count=0, score=18414.918210, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=19316.927382, train/ctc_loss=1733.7392578125, train/wer=0.944086, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 23:43:30.416654 140254504810304 checkpoints.py:356] Saving checkpoint at step: 16000
I0418 23:43:30.784998 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0418 23:43:30.793312 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0418 23:43:30.804851 140079799805696 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18414.918210
I0418 23:43:30.871945 140254504810304 checkpoints.py:356] Saving checkpoint at step: 16000
I0418 23:43:31.356015 140254504810304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0418 23:43:31.365522 140254504810304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0418 23:43:32.663006 140254504810304 submission_runner.py:567] Tuning trial 1/1
I0418 23:43:32.663275 140254504810304 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0418 23:43:32.668204 140254504810304 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.60102, dtype=float32), 'train/wer': 5.107547425192095, 'validation/ctc_loss': DeviceArray(31.428885, dtype=float32), 'validation/wer': 4.688805487751932, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.706348, dtype=float32), 'test/wer': 5.009627688745354, 'test/num_examples': 2472, 'score': 55.1819953918457, 'total_duration': 250.4847912788391, 'accumulated_submission_time': 55.1819953918457, 'accumulated_eval_time': 195.3026406764984, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2061, {'train/ctc_loss': DeviceArray(5.6104712, dtype=float32), 'train/wer': 0.9244763238656258, 'validation/ctc_loss': DeviceArray(5.7855654, dtype=float32), 'validation/wer': 0.8862893033217879, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.54653, dtype=float32), 'test/wer': 0.8858489224707006, 'test/num_examples': 2472, 'score': 2457.214750289917, 'total_duration': 2736.938204050064, 'accumulated_submission_time': 2457.214750289917, 'accumulated_eval_time': 279.28281140327454, 'accumulated_logging_time': 0.4118919372558594, 'global_step': 2061, 'preemption_count': 0}), (4146, {'train/ctc_loss': DeviceArray(1.3405861, dtype=float32), 'train/wer': 0.4007511386942972, 'validation/ctc_loss': DeviceArray(1.7707318, dtype=float32), 'validation/wer': 0.4575538596609712, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3610432, dtype=float32), 'test/wer': 0.3926228342778218, 'test/num_examples': 2472, 'score': 4857.214957237244, 'total_duration': 5233.389305114746, 'accumulated_submission_time': 4857.214957237244, 'accumulated_eval_time': 375.13050150871277, 'accumulated_logging_time': 0.9845161437988281, 'global_step': 4146, 'preemption_count': 0}), (6249, {'train/ctc_loss': DeviceArray(1.0248456, dtype=float32), 'train/wer': 0.3281728523912688, 'validation/ctc_loss': DeviceArray(1.5237039, dtype=float32), 'validation/wer': 0.40150893882237165, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.1006055, dtype=float32), 'test/wer': 0.32986005321633866, 'test/num_examples': 2472, 'score': 7258.297897815704, 'total_duration': 7734.862092018127, 'accumulated_submission_time': 7258.297897815704, 'accumulated_eval_time': 474.8792450428009, 'accumulated_logging_time': 1.594550371170044, 'global_step': 6249, 'preemption_count': 0}), (8350, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9659.2427880764, 'total_duration': 10220.865172624588, 'accumulated_submission_time': 9659.2427880764, 'accumulated_eval_time': 559.3245084285736, 'accumulated_logging_time': 2.1767375469207764, 'global_step': 8350, 'preemption_count': 0}), (10442, {'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12059.69641494751, 'total_duration': 12705.671605110168, 'accumulated_submission_time': 12059.69641494751, 'accumulated_eval_time': 643.1065981388092, 'accumulated_logging_time': 2.7165756225585938, 'global_step': 10442, 'preemption_count': 0}), (12535, {'train/ctc_loss': DeviceArray(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14460.385578155518, 'total_duration': 15192.068930387497, 'accumulated_submission_time': 14460.385578155518, 'accumulated_eval_time': 728.1533768177032, 'accumulated_logging_time': 3.3453054428100586, 'global_step': 12535, 'preemption_count': 0}), (14630, {'train/ctc_loss': DeviceArray(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16861.308940172195, 'total_duration': 17678.74213719368, 'accumulated_submission_time': 16861.308940172195, 'accumulated_eval_time': 813.3378653526306, 'accumulated_logging_time': 3.879986047744751, 'global_step': 14630, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18414.918209791183, 'total_duration': 19316.92738175392, 'accumulated_submission_time': 18414.918209791183, 'accumulated_eval_time': 897.3187267780304, 'accumulated_logging_time': 4.453773260116577, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0418 23:43:32.668381 140254504810304 submission_runner.py:570] Timing: 18414.918209791183
I0418 23:43:32.668436 140254504810304 submission_runner.py:571] ====================
I0418 23:43:32.668898 140254504810304 submission_runner.py:631] Final librispeech_deepspeech score: 18414.918209791183
