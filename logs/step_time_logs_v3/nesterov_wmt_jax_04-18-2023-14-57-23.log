I0418 14:57:45.570739 140391635986240 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nesterov/wmt_jax.
I0418 14:57:45.638872 140391635986240 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 14:57:46.512742 140391635986240 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0418 14:57:46.513461 140391635986240 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 14:57:46.517872 140391635986240 submission_runner.py:528] Using RNG seed 198445102
I0418 14:57:49.161521 140391635986240 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 14:57:49.161728 140391635986240 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1.
I0418 14:57:49.161896 140391635986240 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/hparams.json.
I0418 14:57:49.290311 140391635986240 submission_runner.py:232] Initializing dataset.
I0418 14:57:49.298780 140391635986240 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:57:49.302058 140391635986240 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:57:49.302167 140391635986240 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:57:49.416235 140391635986240 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:57:51.319489 140391635986240 submission_runner.py:239] Initializing model.
I0418 14:58:02.910295 140391635986240 submission_runner.py:249] Initializing optimizer.
I0418 14:58:03.447333 140391635986240 submission_runner.py:256] Initializing metrics bundle.
I0418 14:58:03.447505 140391635986240 submission_runner.py:273] Initializing checkpoint and logger.
I0418 14:58:03.448494 140391635986240 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1 with prefix checkpoint_
I0418 14:58:03.448732 140391635986240 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 14:58:03.448800 140391635986240 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 14:58:04.184322 140391635986240 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/meta_data_0.json.
I0418 14:58:04.185259 140391635986240 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/flags_0.json.
I0418 14:58:04.189531 140391635986240 submission_runner.py:309] Starting training loop.
I0418 14:58:34.354260 140215800813312 logging_writer.py:48] [0] global_step=0, grad_norm=5.411429405212402, loss=11.032635688781738
I0418 14:58:34.366468 140391635986240 spec.py:298] Evaluating on the training split.
I0418 14:58:34.368938 140391635986240 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:58:34.371345 140391635986240 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:58:34.371452 140391635986240 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:58:34.400843 140391635986240 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:58:42.654376 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 15:03:47.877789 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 15:03:47.881469 140391635986240 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:03:47.884689 140391635986240 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 15:03:47.884809 140391635986240 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 15:03:47.914880 140391635986240 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:03:55.505404 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 15:08:53.682962 140391635986240 spec.py:326] Evaluating on the test split.
I0418 15:08:53.685363 140391635986240 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:08:53.688024 140391635986240 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 15:08:53.688129 140391635986240 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 15:08:53.716481 140391635986240 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:09:00.456282 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 15:13:53.001492 140391635986240 submission_runner.py:406] Time since start: 948.81s, 	Step: 1, 	{'train/accuracy': 0.0006259104120545089, 'train/loss': 11.027392387390137, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.020657539367676, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.023274421691895, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.17678666114807, 'total_duration': 948.8118412494659, 'accumulated_submission_time': 30.17678666114807, 'accumulated_eval_time': 918.6349136829376, 'accumulated_logging_time': 0}
I0418 15:13:53.019431 140204341475072 logging_writer.py:48] [1] accumulated_eval_time=918.634914, accumulated_logging_time=0, accumulated_submission_time=30.176787, global_step=1, preemption_count=0, score=30.176787, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.023274, test/num_examples=3003, total_duration=948.811841, train/accuracy=0.000626, train/bleu=0.000000, train/loss=11.027392, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.020658, validation/num_examples=3000
I0418 15:13:53.630660 140391635986240 checkpoints.py:356] Saving checkpoint at step: 1
I0418 15:13:55.885824 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_1
I0418 15:13:55.888305 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_1.
I0418 15:14:31.756995 140204349867776 logging_writer.py:48] [100] global_step=100, grad_norm=0.16171568632125854, loss=9.051372528076172
I0418 15:15:07.547836 140204458972928 logging_writer.py:48] [200] global_step=200, grad_norm=0.14590808749198914, loss=8.900025367736816
I0418 15:15:43.432176 140204349867776 logging_writer.py:48] [300] global_step=300, grad_norm=0.15250317752361298, loss=8.758898735046387
I0418 15:16:19.312704 140204458972928 logging_writer.py:48] [400] global_step=400, grad_norm=0.24271686375141144, loss=8.632705688476562
I0418 15:16:55.254425 140204349867776 logging_writer.py:48] [500] global_step=500, grad_norm=0.41409382224082947, loss=8.43088150024414
I0418 15:17:31.190781 140204458972928 logging_writer.py:48] [600] global_step=600, grad_norm=0.3385995626449585, loss=8.270401000976562
I0418 15:18:07.167359 140204349867776 logging_writer.py:48] [700] global_step=700, grad_norm=0.3723722994327545, loss=8.131101608276367
I0418 15:18:43.144091 140204458972928 logging_writer.py:48] [800] global_step=800, grad_norm=0.7167753577232361, loss=7.996710300445557
I0418 15:19:19.098717 140204349867776 logging_writer.py:48] [900] global_step=900, grad_norm=1.0772874355316162, loss=7.9219136238098145
I0418 15:19:55.041715 140204458972928 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8810592889785767, loss=7.851383209228516
I0418 15:20:30.991325 140204349867776 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6701461672782898, loss=7.800267696380615
I0418 15:21:06.914107 140204458972928 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8487520217895508, loss=7.6417365074157715
I0418 15:21:42.890953 140204349867776 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5081775188446045, loss=7.613876819610596
I0418 15:22:18.886708 140204458972928 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7891637682914734, loss=7.514859199523926
I0418 15:22:54.877580 140204349867776 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9229329228401184, loss=7.466406345367432
I0418 15:23:30.864028 140204458972928 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6969620585441589, loss=7.2782440185546875
I0418 15:24:06.819482 140204349867776 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.676624059677124, loss=7.247797966003418
I0418 15:24:42.786180 140204458972928 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6159168481826782, loss=7.265555381774902
I0418 15:25:18.748290 140204349867776 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8833279609680176, loss=7.126541614532471
I0418 15:25:54.685629 140204458972928 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8488844037055969, loss=7.095513343811035
I0418 15:26:30.669835 140204349867776 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7302435040473938, loss=6.90828275680542
I0418 15:27:06.622736 140204458972928 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8889927268028259, loss=6.925293922424316
I0418 15:27:42.622670 140204349867776 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6483785510063171, loss=6.893261432647705
I0418 15:27:55.972770 140391635986240 spec.py:298] Evaluating on the training split.
I0418 15:27:58.977995 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 15:32:50.663165 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 15:32:53.323007 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 15:37:37.234564 140391635986240 spec.py:326] Evaluating on the test split.
I0418 15:37:39.946578 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 15:42:32.056659 140391635986240 submission_runner.py:406] Time since start: 2667.87s, 	Step: 2339, 	{'train/accuracy': 0.3056677281856537, 'train/loss': 5.54689359664917, 'train/bleu': 6.81536990875158, 'validation/accuracy': 0.2773679196834564, 'validation/loss': 5.824658393859863, 'validation/bleu': 3.600315981757768, 'validation/num_examples': 3000, 'test/accuracy': 0.2596362829208374, 'test/loss': 6.099384307861328, 'test/bleu': 2.6952968050477195, 'test/num_examples': 3003, 'score': 870.2246091365814, 'total_duration': 2667.867000102997, 'accumulated_submission_time': 870.2246091365814, 'accumulated_eval_time': 1794.7187087535858, 'accumulated_logging_time': 2.900588274002075}
I0418 15:42:32.064877 140204458972928 logging_writer.py:48] [2339] accumulated_eval_time=1794.718709, accumulated_logging_time=2.900588, accumulated_submission_time=870.224609, global_step=2339, preemption_count=0, score=870.224609, test/accuracy=0.259636, test/bleu=2.695297, test/loss=6.099384, test/num_examples=3003, total_duration=2667.867000, train/accuracy=0.305668, train/bleu=6.815370, train/loss=5.546894, validation/accuracy=0.277368, validation/bleu=3.600316, validation/loss=5.824658, validation/num_examples=3000
I0418 15:42:32.688517 140391635986240 checkpoints.py:356] Saving checkpoint at step: 2339
I0418 15:42:34.934715 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_2339
I0418 15:42:34.937295 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_2339.
I0418 15:42:57.265061 140204349867776 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0233819484710693, loss=6.821776390075684
I0418 15:43:33.215064 140204433794816 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.661080002784729, loss=6.796117782592773
I0418 15:44:09.129145 140204349867776 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.731243908405304, loss=6.626904010772705
I0418 15:44:45.081847 140204433794816 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7310746312141418, loss=6.595637321472168
I0418 15:45:21.041987 140204349867776 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6564762592315674, loss=6.534435272216797
I0418 15:45:57.025467 140204433794816 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7505786418914795, loss=6.585175514221191
I0418 15:46:33.051590 140204349867776 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7948269248008728, loss=6.508907318115234
I0418 15:47:09.015478 140204433794816 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6134257316589355, loss=6.4073076248168945
I0418 15:47:44.940675 140204349867776 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7879224419593811, loss=6.326529502868652
I0418 15:48:20.923431 140204433794816 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.96576327085495, loss=6.291797161102295
I0418 15:48:56.897912 140204349867776 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9184221029281616, loss=6.278934955596924
I0418 15:49:32.860966 140204433794816 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.615958034992218, loss=6.144710540771484
I0418 15:50:08.853357 140204349867776 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7308306097984314, loss=6.135348320007324
I0418 15:50:44.828810 140204433794816 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5860676169395447, loss=6.047828197479248
I0418 15:51:20.803512 140204349867776 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7076672911643982, loss=6.029812335968018
I0418 15:51:56.736976 140204433794816 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6309439539909363, loss=5.940340042114258
I0418 15:52:32.714859 140204349867776 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.686919629573822, loss=6.033895969390869
I0418 15:53:08.663839 140204433794816 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6808475255966187, loss=5.817818641662598
I0418 15:53:44.627934 140204349867776 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6957013607025146, loss=5.782431125640869
I0418 15:54:20.619813 140204433794816 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.607894778251648, loss=5.73343563079834
I0418 15:54:56.580897 140204349867776 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7080599665641785, loss=5.702009677886963
I0418 15:55:32.560369 140204433794816 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6260749697685242, loss=5.636283874511719
I0418 15:56:08.530117 140204349867776 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5550373196601868, loss=5.589591026306152
I0418 15:56:35.204148 140391635986240 spec.py:298] Evaluating on the training split.
I0418 15:56:38.210762 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:00:09.425108 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 16:00:12.082679 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:03:09.274323 140391635986240 spec.py:326] Evaluating on the test split.
I0418 16:03:11.983678 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:06:32.372072 140391635986240 submission_runner.py:406] Time since start: 4108.18s, 	Step: 4676, 	{'train/accuracy': 0.46202072501182556, 'train/loss': 3.8852646350860596, 'train/bleu': 17.674319813713883, 'validation/accuracy': 0.45202165842056274, 'validation/loss': 3.981436252593994, 'validation/bleu': 13.551092794062708, 'validation/num_examples': 3000, 'test/accuracy': 0.4417058825492859, 'test/loss': 4.143922328948975, 'test/bleu': 11.648672749768199, 'test/num_examples': 3003, 'score': 1710.4665358066559, 'total_duration': 4108.182411670685, 'accumulated_submission_time': 1710.4665358066559, 'accumulated_eval_time': 2391.8865365982056, 'accumulated_logging_time': 5.782594680786133}
I0418 16:06:32.380886 140204433794816 logging_writer.py:48] [4676] accumulated_eval_time=2391.886537, accumulated_logging_time=5.782595, accumulated_submission_time=1710.466536, global_step=4676, preemption_count=0, score=1710.466536, test/accuracy=0.441706, test/bleu=11.648673, test/loss=4.143922, test/num_examples=3003, total_duration=4108.182412, train/accuracy=0.462021, train/bleu=17.674320, train/loss=3.885265, validation/accuracy=0.452022, validation/bleu=13.551093, validation/loss=3.981436, validation/num_examples=3000
I0418 16:06:32.995829 140391635986240 checkpoints.py:356] Saving checkpoint at step: 4676
I0418 16:06:35.197136 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_4676
I0418 16:06:35.199626 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_4676.
I0418 16:06:44.181791 140204349867776 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6434645652770996, loss=5.537167549133301
I0418 16:07:20.078320 140204417009408 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6081569194793701, loss=5.465623378753662
I0418 16:07:55.959668 140204349867776 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6624525189399719, loss=5.386921405792236
I0418 16:08:31.915059 140204417009408 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6301246285438538, loss=5.429040431976318
I0418 16:09:07.837867 140204349867776 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6411908268928528, loss=5.4732584953308105
I0418 16:09:43.771866 140204417009408 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6792719960212708, loss=5.366281032562256
I0418 16:10:19.755941 140204349867776 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6171795725822449, loss=5.418234825134277
I0418 16:10:55.736100 140204417009408 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5442293286323547, loss=5.310639381408691
I0418 16:11:31.729342 140204349867776 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5370312333106995, loss=5.343532562255859
I0418 16:12:07.718309 140204417009408 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.563944935798645, loss=5.33277702331543
I0418 16:12:43.680003 140204349867776 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5026417374610901, loss=5.350603103637695
I0418 16:13:19.620701 140204417009408 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5853395462036133, loss=5.225824356079102
I0418 16:13:55.579849 140204349867776 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5218181610107422, loss=5.169783592224121
I0418 16:14:31.522118 140204417009408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5241685509681702, loss=5.205543041229248
I0418 16:15:07.501026 140204349867776 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5341161489486694, loss=5.1528167724609375
I0418 16:15:43.475646 140204417009408 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.44461530447006226, loss=5.177755355834961
I0418 16:16:19.462780 140204349867776 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.45742809772491455, loss=5.0789794921875
I0418 16:16:55.381036 140204417009408 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4307989180088043, loss=5.174951076507568
I0418 16:17:31.347840 140204349867776 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.47689083218574524, loss=5.056367874145508
I0418 16:18:07.316788 140204417009408 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5056867599487305, loss=5.1286420822143555
I0418 16:18:43.268740 140204349867776 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4442167282104492, loss=5.04601526260376
I0418 16:19:19.215574 140204417009408 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4811137020587921, loss=5.099982261657715
I0418 16:19:55.170140 140204349867776 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4285505712032318, loss=4.998254299163818
I0418 16:20:31.102289 140204417009408 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4777756631374359, loss=5.117048263549805
I0418 16:20:35.490426 140391635986240 spec.py:298] Evaluating on the training split.
I0418 16:20:38.498808 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:23:18.464083 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 16:23:21.127869 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:25:56.682843 140391635986240 spec.py:326] Evaluating on the test split.
I0418 16:25:59.389642 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:28:25.705611 140391635986240 submission_runner.py:406] Time since start: 5421.52s, 	Step: 7014, 	{'train/accuracy': 0.5417113304138184, 'train/loss': 3.1251208782196045, 'train/bleu': 24.153051353132373, 'validation/accuracy': 0.5348600745201111, 'validation/loss': 3.1582558155059814, 'validation/bleu': 19.76593604261199, 'validation/num_examples': 3000, 'test/accuracy': 0.5304398536682129, 'test/loss': 3.2196590900421143, 'test/bleu': 18.41959580226212, 'test/num_examples': 3003, 'score': 2550.732313156128, 'total_duration': 5421.515946626663, 'accumulated_submission_time': 2550.732313156128, 'accumulated_eval_time': 2862.101647377014, 'accumulated_logging_time': 8.612815618515015}
I0418 16:28:25.714051 140204349867776 logging_writer.py:48] [7014] accumulated_eval_time=2862.101647, accumulated_logging_time=8.612816, accumulated_submission_time=2550.732313, global_step=7014, preemption_count=0, score=2550.732313, test/accuracy=0.530440, test/bleu=18.419596, test/loss=3.219659, test/num_examples=3003, total_duration=5421.515947, train/accuracy=0.541711, train/bleu=24.153051, train/loss=3.125121, validation/accuracy=0.534860, validation/bleu=19.765936, validation/loss=3.158256, validation/num_examples=3000
I0418 16:28:26.323230 140391635986240 checkpoints.py:356] Saving checkpoint at step: 7014
I0418 16:28:28.536122 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_7014
I0418 16:28:28.538608 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_7014.
I0418 16:28:59.761495 140204417009408 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.44525977969169617, loss=5.031785488128662
I0418 16:29:35.614661 140204408616704 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4116722643375397, loss=5.025485515594482
I0418 16:30:11.513099 140204417009408 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4276396930217743, loss=4.923040866851807
I0418 16:30:47.386204 140204408616704 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4763326048851013, loss=4.950160980224609
I0418 16:31:23.301257 140204417009408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4225831627845764, loss=4.838860034942627
I0418 16:31:59.228209 140204408616704 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.43244484066963196, loss=4.950689792633057
I0418 16:32:35.213159 140204417009408 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.41444310545921326, loss=5.0477375984191895
I0418 16:33:11.166920 140204408616704 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.43558469414711, loss=4.951329231262207
I0418 16:33:47.129624 140204417009408 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.41815441846847534, loss=4.941680431365967
I0418 16:34:23.082247 140204408616704 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.39503952860832214, loss=4.853407382965088
I0418 16:34:59.015928 140204417009408 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.4005174934864044, loss=4.847030162811279
I0418 16:35:34.984706 140204408616704 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.43588170409202576, loss=4.827534198760986
I0418 16:36:10.895906 140204417009408 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3970888555049896, loss=4.81801176071167
I0418 16:36:46.835800 140204408616704 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.39842402935028076, loss=4.866229057312012
I0418 16:37:22.785189 140204417009408 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.41219717264175415, loss=4.8783745765686035
I0418 16:37:58.748584 140204408616704 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4037971794605255, loss=4.840004920959473
I0418 16:38:34.744925 140204417009408 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.381464421749115, loss=4.808221817016602
I0418 16:39:10.691730 140204408616704 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3843698501586914, loss=4.816816806793213
I0418 16:39:46.619375 140204417009408 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3808017671108246, loss=4.854159355163574
I0418 16:40:22.545599 140204408616704 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3846319317817688, loss=4.868741035461426
I0418 16:40:58.474361 140204417009408 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.365113228559494, loss=4.778751850128174
I0418 16:41:34.400449 140204408616704 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3959115147590637, loss=4.81769323348999
I0418 16:42:10.365407 140204417009408 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.38011446595191956, loss=4.781897068023682
I0418 16:42:28.727252 140391635986240 spec.py:298] Evaluating on the training split.
I0418 16:42:31.740195 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:45:19.394754 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 16:45:22.053104 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:47:49.386773 140391635986240 spec.py:326] Evaluating on the test split.
I0418 16:47:52.085386 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 16:50:15.787730 140391635986240 submission_runner.py:406] Time since start: 6731.60s, 	Step: 9353, 	{'train/accuracy': 0.5666933059692383, 'train/loss': 2.8394055366516113, 'train/bleu': 26.34558191575142, 'validation/accuracy': 0.5698627233505249, 'validation/loss': 2.782454013824463, 'validation/bleu': 22.114418340987562, 'validation/num_examples': 3000, 'test/accuracy': 0.5715298652648926, 'test/loss': 2.797715425491333, 'test/bleu': 20.672278143065704, 'test/num_examples': 3003, 'score': 3390.8962829113007, 'total_duration': 6731.598087310791, 'accumulated_submission_time': 3390.8962829113007, 'accumulated_eval_time': 3329.1620557308197, 'accumulated_logging_time': 11.448454856872559}
I0418 16:50:15.796084 140204408616704 logging_writer.py:48] [9353] accumulated_eval_time=3329.162056, accumulated_logging_time=11.448455, accumulated_submission_time=3390.896283, global_step=9353, preemption_count=0, score=3390.896283, test/accuracy=0.571530, test/bleu=20.672278, test/loss=2.797715, test/num_examples=3003, total_duration=6731.598087, train/accuracy=0.566693, train/bleu=26.345582, train/loss=2.839406, validation/accuracy=0.569863, validation/bleu=22.114418, validation/loss=2.782454, validation/num_examples=3000
I0418 16:50:16.419050 140391635986240 checkpoints.py:356] Saving checkpoint at step: 9353
I0418 16:50:18.628729 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_9353
I0418 16:50:18.631220 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_9353.
I0418 16:50:35.831409 140204417009408 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3732775151729584, loss=4.741273880004883
I0418 16:51:11.705852 140204129679104 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3912532925605774, loss=4.762304306030273
I0418 16:51:47.595108 140204417009408 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.36763817071914673, loss=4.732246398925781
I0418 16:52:23.488245 140204129679104 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.39125296473503113, loss=4.687520980834961
I0418 16:52:59.383994 140204417009408 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3794672191143036, loss=4.730987071990967
I0418 16:53:35.280877 140204129679104 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3538655638694763, loss=4.63063383102417
I0418 16:54:11.235756 140204417009408 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.38026493787765503, loss=4.767915725708008
I0418 16:54:47.181925 140204129679104 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.36000195145606995, loss=4.666308403015137
I0418 16:55:23.131568 140204417009408 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.40592244267463684, loss=4.787909507751465
I0418 16:55:59.118543 140204129679104 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.36282575130462646, loss=4.706428527832031
I0418 16:56:35.064733 140204417009408 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.38250356912612915, loss=4.643959999084473
I0418 16:57:11.035774 140204129679104 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3609895408153534, loss=4.719517707824707
I0418 16:57:46.986600 140204417009408 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3720605969429016, loss=4.632694244384766
I0418 16:58:22.907904 140204129679104 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3545989990234375, loss=4.794410228729248
I0418 16:58:58.873147 140204417009408 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.37038007378578186, loss=4.7570295333862305
I0418 16:59:34.782638 140204129679104 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.364367812871933, loss=4.6574931144714355
I0418 17:00:10.683273 140204417009408 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3443828523159027, loss=4.809200286865234
I0418 17:00:46.663929 140204129679104 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3555907607078552, loss=4.601620197296143
I0418 17:01:22.649826 140204417009408 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.36580824851989746, loss=4.7086262702941895
I0418 17:01:58.602248 140204129679104 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3472425043582916, loss=4.654109954833984
I0418 17:02:34.570455 140204417009408 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.34746280312538147, loss=4.617311000823975
I0418 17:03:10.493571 140204129679104 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3478792607784271, loss=4.687125205993652
I0418 17:03:46.446846 140204417009408 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3447485864162445, loss=4.533195495605469
I0418 17:04:18.870678 140391635986240 spec.py:298] Evaluating on the training split.
I0418 17:04:21.891860 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:06:50.181797 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 17:06:52.835115 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:09:17.811541 140391635986240 spec.py:326] Evaluating on the test split.
I0418 17:09:20.511519 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:11:39.680849 140391635986240 submission_runner.py:406] Time since start: 8015.49s, 	Step: 11692, 	{'train/accuracy': 0.5826308727264404, 'train/loss': 2.682703971862793, 'train/bleu': 26.98227893706596, 'validation/accuracy': 0.5903336405754089, 'validation/loss': 2.576795816421509, 'validation/bleu': 23.46139071801215, 'validation/num_examples': 3000, 'test/accuracy': 0.5926210284233093, 'test/loss': 2.578812599182129, 'test/bleu': 22.244177843138303, 'test/num_examples': 3003, 'score': 4231.109885454178, 'total_duration': 8015.491140365601, 'accumulated_submission_time': 4231.109885454178, 'accumulated_eval_time': 3769.9720816612244, 'accumulated_logging_time': 14.294504642486572}
I0418 17:11:39.690179 140204129679104 logging_writer.py:48] [11692] accumulated_eval_time=3769.972082, accumulated_logging_time=14.294505, accumulated_submission_time=4231.109885, global_step=11692, preemption_count=0, score=4231.109885, test/accuracy=0.592621, test/bleu=22.244178, test/loss=2.578813, test/num_examples=3003, total_duration=8015.491140, train/accuracy=0.582631, train/bleu=26.982279, train/loss=2.682704, validation/accuracy=0.590334, validation/bleu=23.461391, validation/loss=2.576796, validation/num_examples=3000
I0418 17:11:40.305422 140391635986240 checkpoints.py:356] Saving checkpoint at step: 11692
I0418 17:11:42.510012 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_11692
I0418 17:11:42.512565 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_11692.
I0418 17:11:45.762589 140204417009408 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3411145508289337, loss=4.576258659362793
I0418 17:12:21.591495 140204121286400 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.343514084815979, loss=4.596157073974609
I0418 17:12:57.482945 140204417009408 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.34389153122901917, loss=4.710216045379639
I0418 17:13:33.400559 140204121286400 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3629920780658722, loss=4.621981143951416
I0418 17:14:09.308411 140204417009408 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.36151209473609924, loss=4.493901252746582
I0418 17:14:45.218540 140204121286400 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3530566692352295, loss=4.652201175689697
I0418 17:15:21.137964 140204417009408 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.341278612613678, loss=4.591313362121582
I0418 17:15:57.031951 140204121286400 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.33406805992126465, loss=4.533641338348389
I0418 17:16:33.004561 140204417009408 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.357170432806015, loss=4.635144233703613
I0418 17:17:08.900059 140204121286400 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.34178316593170166, loss=4.553705215454102
I0418 17:17:44.799304 140204417009408 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.33439698815345764, loss=4.600352764129639
I0418 17:18:20.724214 140204121286400 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3433305025100708, loss=4.603250503540039
I0418 17:18:56.642531 140204417009408 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3299873471260071, loss=4.498505115509033
I0418 17:19:32.580548 140204121286400 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.33819207549095154, loss=4.569402694702148
I0418 17:20:08.537364 140204417009408 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.33213138580322266, loss=4.533229827880859
I0418 17:20:44.480337 140204121286400 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3350045382976532, loss=4.569720268249512
I0418 17:21:20.427760 140204417009408 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.35252949595451355, loss=4.556334495544434
I0418 17:21:56.372885 140204121286400 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3440322279930115, loss=4.577450752258301
I0418 17:22:32.368675 140204417009408 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.33528995513916016, loss=4.557184219360352
I0418 17:23:08.327143 140204121286400 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.34402692317962646, loss=4.53203821182251
I0418 17:23:44.272808 140204417009408 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.337098091840744, loss=4.557277202606201
I0418 17:24:20.203782 140204121286400 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.34514299035072327, loss=4.603253364562988
I0418 17:24:56.151993 140204417009408 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.343528151512146, loss=4.6032891273498535
I0418 17:25:32.125539 140204121286400 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.336758553981781, loss=4.503599643707275
I0418 17:25:42.606849 140391635986240 spec.py:298] Evaluating on the training split.
I0418 17:25:45.618875 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:28:17.085212 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 17:28:19.756767 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:30:44.625355 140391635986240 spec.py:326] Evaluating on the test split.
I0418 17:30:47.342011 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:33:10.506424 140391635986240 submission_runner.py:406] Time since start: 9306.32s, 	Step: 14031, 	{'train/accuracy': 0.5978480577468872, 'train/loss': 2.53454327583313, 'train/bleu': 28.084821965666613, 'validation/accuracy': 0.6034519076347351, 'validation/loss': 2.4549405574798584, 'validation/bleu': 24.299863370180123, 'validation/num_examples': 3000, 'test/accuracy': 0.6077741384506226, 'test/loss': 2.4446675777435303, 'test/bleu': 23.090845019792773, 'test/num_examples': 3003, 'score': 5071.177137374878, 'total_duration': 9306.316750764847, 'accumulated_submission_time': 5071.177137374878, 'accumulated_eval_time': 4217.871542453766, 'accumulated_logging_time': 17.129082918167114}
I0418 17:33:10.516298 140204417009408 logging_writer.py:48] [14031] accumulated_eval_time=4217.871542, accumulated_logging_time=17.129083, accumulated_submission_time=5071.177137, global_step=14031, preemption_count=0, score=5071.177137, test/accuracy=0.607774, test/bleu=23.090845, test/loss=2.444668, test/num_examples=3003, total_duration=9306.316751, train/accuracy=0.597848, train/bleu=28.084822, train/loss=2.534543, validation/accuracy=0.603452, validation/bleu=24.299863, validation/loss=2.454941, validation/num_examples=3000
I0418 17:33:11.436196 140391635986240 checkpoints.py:356] Saving checkpoint at step: 14031
I0418 17:33:14.670289 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_14031
I0418 17:33:14.673728 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_14031.
I0418 17:33:39.800937 140204121286400 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.32679253816604614, loss=4.469944953918457
I0418 17:34:15.673193 140204112893696 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.331627756357193, loss=4.56266975402832
I0418 17:34:51.575189 140204121286400 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.34193506836891174, loss=4.5289692878723145
I0418 17:35:27.468345 140204112893696 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.33737897872924805, loss=4.50044059753418
I0418 17:36:03.406468 140204121286400 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.3306661546230316, loss=4.460656642913818
I0418 17:36:39.314028 140204112893696 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3424137532711029, loss=4.51152229309082
I0418 17:37:15.213443 140204121286400 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3381100296974182, loss=4.563356876373291
I0418 17:37:51.162587 140204112893696 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3252745270729065, loss=4.51719331741333
I0418 17:38:27.107622 140204121286400 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3273235261440277, loss=4.452425479888916
I0418 17:39:03.033854 140204112893696 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.34325528144836426, loss=4.439014911651611
I0418 17:39:38.979113 140204121286400 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3266408443450928, loss=4.451967239379883
I0418 17:40:14.854412 140204112893696 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3377281427383423, loss=4.559504985809326
I0418 17:40:50.773537 140204121286400 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3382136821746826, loss=4.5482282638549805
I0418 17:41:26.701365 140204112893696 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.314014196395874, loss=4.500598907470703
I0418 17:42:02.602875 140204121286400 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3428792357444763, loss=4.535586357116699
I0418 17:42:38.522599 140204112893696 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.31931084394454956, loss=4.440834999084473
I0418 17:43:14.484210 140204121286400 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3333224356174469, loss=4.448631763458252
I0418 17:43:50.459406 140204112893696 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3240926265716553, loss=4.50324821472168
I0418 17:44:26.411147 140204121286400 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3210195302963257, loss=4.46584939956665
I0418 17:45:02.339300 140204112893696 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.31472164392471313, loss=4.437235355377197
I0418 17:45:38.249850 140204121286400 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.32845351099967957, loss=4.509859085083008
I0418 17:46:14.210210 140204112893696 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.33119022846221924, loss=4.464354991912842
I0418 17:46:50.184376 140204121286400 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3201678395271301, loss=4.488123416900635
I0418 17:47:14.696063 140391635986240 spec.py:298] Evaluating on the training split.
I0418 17:47:17.701222 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:49:59.898384 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 17:50:02.551772 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:52:27.861100 140391635986240 spec.py:326] Evaluating on the test split.
I0418 17:52:30.564325 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 17:54:54.985398 140391635986240 submission_runner.py:406] Time since start: 10610.80s, 	Step: 16370, 	{'train/accuracy': 0.6017758250236511, 'train/loss': 2.4928479194641113, 'train/bleu': 29.02024066617451, 'validation/accuracy': 0.6122304797172546, 'validation/loss': 2.3794639110565186, 'validation/bleu': 25.106822043407924, 'validation/num_examples': 3000, 'test/accuracy': 0.6168497204780579, 'test/loss': 2.360391855239868, 'test/bleu': 23.890500441896847, 'test/num_examples': 3003, 'score': 5911.170006752014, 'total_duration': 10610.795687198639, 'accumulated_submission_time': 5911.170006752014, 'accumulated_eval_time': 4678.160729169846, 'accumulated_logging_time': 21.30055069923401}
I0418 17:54:54.994850 140204112893696 logging_writer.py:48] [16370] accumulated_eval_time=4678.160729, accumulated_logging_time=21.300551, accumulated_submission_time=5911.170007, global_step=16370, preemption_count=0, score=5911.170007, test/accuracy=0.616850, test/bleu=23.890500, test/loss=2.360392, test/num_examples=3003, total_duration=10610.795687, train/accuracy=0.601776, train/bleu=29.020241, train/loss=2.492848, validation/accuracy=0.612230, validation/bleu=25.106822, validation/loss=2.379464, validation/num_examples=3000
I0418 17:54:55.625946 140391635986240 checkpoints.py:356] Saving checkpoint at step: 16370
I0418 17:54:57.847347 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_16370
I0418 17:54:57.849986 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_16370.
I0418 17:55:08.974228 140204121286400 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.320060670375824, loss=4.446690082550049
I0418 17:55:44.810603 140204104500992 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3294419050216675, loss=4.4443559646606445
I0418 17:56:20.733965 140204121286400 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.32908204197883606, loss=4.4138689041137695
I0418 17:56:56.674418 140204104500992 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.32589128613471985, loss=4.42072057723999
I0418 17:57:32.622746 140204121286400 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3281666338443756, loss=4.475287914276123
I0418 17:58:08.535734 140204104500992 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.32601335644721985, loss=4.471996307373047
I0418 17:58:44.468530 140204121286400 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3234635889530182, loss=4.505693435668945
I0418 17:59:20.453268 140204104500992 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3189392387866974, loss=4.498907566070557
I0418 17:59:56.380932 140204121286400 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.31996849179267883, loss=4.402895927429199
I0418 18:00:32.358400 140204104500992 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3269481658935547, loss=4.497256278991699
I0418 18:01:08.283308 140204121286400 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.336717426776886, loss=4.509335517883301
I0418 18:01:44.193292 140204104500992 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.32097384333610535, loss=4.407607078552246
I0418 18:02:20.124601 140204121286400 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3170791566371918, loss=4.500908374786377
I0418 18:02:56.027488 140204104500992 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3220844566822052, loss=4.391856670379639
I0418 18:03:32.027547 140204121286400 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3167274296283722, loss=4.37062406539917
I0418 18:04:07.977057 140204104500992 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.32644298672676086, loss=4.393936634063721
I0418 18:04:43.914624 140204121286400 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3216177523136139, loss=4.400847911834717
I0418 18:05:19.844055 140204104500992 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.32788023352622986, loss=4.410263538360596
I0418 18:05:55.765281 140204121286400 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.31954458355903625, loss=4.4507737159729
I0418 18:06:31.691654 140204104500992 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3226671814918518, loss=4.461254596710205
I0418 18:07:07.610600 140204121286400 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3215295076370239, loss=4.451455593109131
I0418 18:07:43.547554 140204104500992 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.31096455454826355, loss=4.4405951499938965
I0418 18:08:19.458792 140204121286400 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3298652768135071, loss=4.434292316436768
I0418 18:08:55.404331 140204104500992 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.318051815032959, loss=4.394106864929199
I0418 18:08:57.975597 140391635986240 spec.py:298] Evaluating on the training split.
I0418 18:09:00.980700 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 18:11:33.425692 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 18:11:36.080795 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 18:13:59.375621 140391635986240 spec.py:326] Evaluating on the test split.
I0418 18:14:02.077294 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 18:16:21.901277 140391635986240 submission_runner.py:406] Time since start: 11897.71s, 	Step: 18709, 	{'train/accuracy': 0.6066215634346008, 'train/loss': 2.456886053085327, 'train/bleu': 29.458484187128306, 'validation/accuracy': 0.6179588437080383, 'validation/loss': 2.3170437812805176, 'validation/bleu': 25.728684676524146, 'validation/num_examples': 3000, 'test/accuracy': 0.6256347894668579, 'test/loss': 2.2894904613494873, 'test/bleu': 24.369917892841436, 'test/num_examples': 3003, 'score': 6751.269082069397, 'total_duration': 11897.71165561676, 'accumulated_submission_time': 6751.269082069397, 'accumulated_eval_time': 5122.086347103119, 'accumulated_logging_time': 24.16819667816162}
I0418 18:16:21.910331 140204121286400 logging_writer.py:48] [18709] accumulated_eval_time=5122.086347, accumulated_logging_time=24.168197, accumulated_submission_time=6751.269082, global_step=18709, preemption_count=0, score=6751.269082, test/accuracy=0.625635, test/bleu=24.369918, test/loss=2.289490, test/num_examples=3003, total_duration=11897.711656, train/accuracy=0.606622, train/bleu=29.458484, train/loss=2.456886, validation/accuracy=0.617959, validation/bleu=25.728685, validation/loss=2.317044, validation/num_examples=3000
I0418 18:16:22.527923 140391635986240 checkpoints.py:356] Saving checkpoint at step: 18709
I0418 18:16:24.735536 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_18709
I0418 18:16:24.738021 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_18709.
I0418 18:16:57.760621 140204104500992 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3271554410457611, loss=4.390105724334717
I0418 18:17:33.622922 140204096108288 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3168846070766449, loss=4.375735759735107
I0418 18:18:09.544023 140204104500992 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3364412486553192, loss=4.344376087188721
I0418 18:18:45.452066 140204096108288 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.33288145065307617, loss=4.426502227783203
I0418 18:19:21.380418 140204104500992 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3264332711696625, loss=4.482362747192383
I0418 18:19:57.292619 140204096108288 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.309062659740448, loss=4.376783847808838
I0418 18:20:33.230883 140204104500992 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.314523309469223, loss=4.415356636047363
I0418 18:21:09.172221 140204096108288 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3292706310749054, loss=4.366458415985107
I0418 18:21:45.100785 140204104500992 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2989983558654785, loss=4.302778244018555
I0418 18:22:21.022825 140204096108288 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.32163792848587036, loss=4.381416320800781
I0418 18:22:56.963976 140204104500992 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3273555338382721, loss=4.49289083480835
I0418 18:23:32.868538 140204096108288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.32711753249168396, loss=4.356392860412598
I0418 18:24:08.131228 140391635986240 spec.py:298] Evaluating on the training split.
I0418 18:24:11.130010 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 18:26:45.012841 140391635986240 spec.py:310] Evaluating on the validation split.
I0418 18:26:47.687649 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 18:29:10.067441 140391635986240 spec.py:326] Evaluating on the test split.
I0418 18:29:12.772727 140391635986240 workload.py:179] Translating evaluation dataset.
I0418 18:31:34.564797 140391635986240 submission_runner.py:406] Time since start: 12810.38s, 	Step: 20000, 	{'train/accuracy': 0.6209314465522766, 'train/loss': 2.333723306655884, 'train/bleu': 30.402126439217927, 'validation/accuracy': 0.6225837469100952, 'validation/loss': 2.2762746810913086, 'validation/bleu': 26.11916726004152, 'validation/num_examples': 3000, 'test/accuracy': 0.6283655762672424, 'test/loss': 2.2507011890411377, 'test/bleu': 24.734926246902845, 'test/num_examples': 3003, 'score': 7214.646990537643, 'total_duration': 12810.375185966492, 'accumulated_submission_time': 7214.646990537643, 'accumulated_eval_time': 5568.519872665405, 'accumulated_logging_time': 27.008020877838135}
I0418 18:31:34.573371 140204104500992 logging_writer.py:48] [20000] accumulated_eval_time=5568.519873, accumulated_logging_time=27.008021, accumulated_submission_time=7214.646991, global_step=20000, preemption_count=0, score=7214.646991, test/accuracy=0.628366, test/bleu=24.734926, test/loss=2.250701, test/num_examples=3003, total_duration=12810.375186, train/accuracy=0.620931, train/bleu=30.402126, train/loss=2.333723, validation/accuracy=0.622584, validation/bleu=26.119167, validation/loss=2.276275, validation/num_examples=3000
I0418 18:31:35.188867 140391635986240 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:31:37.382626 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_20000
I0418 18:31:37.385111 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_20000.
I0418 18:31:37.395335 140204096108288 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7214.646991
I0418 18:31:37.759511 140391635986240 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:31:41.111857 140391635986240 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_20000
I0418 18:31:41.114192 140391635986240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/wmt_jax/trial_1/checkpoint_20000.
I0418 18:31:41.145803 140391635986240 submission_runner.py:567] Tuning trial 1/1
I0418 18:31:41.145964 140391635986240 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0418 18:31:41.146785 140391635986240 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006259104120545089, 'train/loss': 11.027392387390137, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.020657539367676, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.023274421691895, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.17678666114807, 'total_duration': 948.8118412494659, 'accumulated_submission_time': 30.17678666114807, 'accumulated_eval_time': 918.6349136829376, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2339, {'train/accuracy': 0.3056677281856537, 'train/loss': 5.54689359664917, 'train/bleu': 6.81536990875158, 'validation/accuracy': 0.2773679196834564, 'validation/loss': 5.824658393859863, 'validation/bleu': 3.600315981757768, 'validation/num_examples': 3000, 'test/accuracy': 0.2596362829208374, 'test/loss': 6.099384307861328, 'test/bleu': 2.6952968050477195, 'test/num_examples': 3003, 'score': 870.2246091365814, 'total_duration': 2667.867000102997, 'accumulated_submission_time': 870.2246091365814, 'accumulated_eval_time': 1794.7187087535858, 'accumulated_logging_time': 2.900588274002075, 'global_step': 2339, 'preemption_count': 0}), (4676, {'train/accuracy': 0.46202072501182556, 'train/loss': 3.8852646350860596, 'train/bleu': 17.674319813713883, 'validation/accuracy': 0.45202165842056274, 'validation/loss': 3.981436252593994, 'validation/bleu': 13.551092794062708, 'validation/num_examples': 3000, 'test/accuracy': 0.4417058825492859, 'test/loss': 4.143922328948975, 'test/bleu': 11.648672749768199, 'test/num_examples': 3003, 'score': 1710.4665358066559, 'total_duration': 4108.182411670685, 'accumulated_submission_time': 1710.4665358066559, 'accumulated_eval_time': 2391.8865365982056, 'accumulated_logging_time': 5.782594680786133, 'global_step': 4676, 'preemption_count': 0}), (7014, {'train/accuracy': 0.5417113304138184, 'train/loss': 3.1251208782196045, 'train/bleu': 24.153051353132373, 'validation/accuracy': 0.5348600745201111, 'validation/loss': 3.1582558155059814, 'validation/bleu': 19.76593604261199, 'validation/num_examples': 3000, 'test/accuracy': 0.5304398536682129, 'test/loss': 3.2196590900421143, 'test/bleu': 18.41959580226212, 'test/num_examples': 3003, 'score': 2550.732313156128, 'total_duration': 5421.515946626663, 'accumulated_submission_time': 2550.732313156128, 'accumulated_eval_time': 2862.101647377014, 'accumulated_logging_time': 8.612815618515015, 'global_step': 7014, 'preemption_count': 0}), (9353, {'train/accuracy': 0.5666933059692383, 'train/loss': 2.8394055366516113, 'train/bleu': 26.34558191575142, 'validation/accuracy': 0.5698627233505249, 'validation/loss': 2.782454013824463, 'validation/bleu': 22.114418340987562, 'validation/num_examples': 3000, 'test/accuracy': 0.5715298652648926, 'test/loss': 2.797715425491333, 'test/bleu': 20.672278143065704, 'test/num_examples': 3003, 'score': 3390.8962829113007, 'total_duration': 6731.598087310791, 'accumulated_submission_time': 3390.8962829113007, 'accumulated_eval_time': 3329.1620557308197, 'accumulated_logging_time': 11.448454856872559, 'global_step': 9353, 'preemption_count': 0}), (11692, {'train/accuracy': 0.5826308727264404, 'train/loss': 2.682703971862793, 'train/bleu': 26.98227893706596, 'validation/accuracy': 0.5903336405754089, 'validation/loss': 2.576795816421509, 'validation/bleu': 23.46139071801215, 'validation/num_examples': 3000, 'test/accuracy': 0.5926210284233093, 'test/loss': 2.578812599182129, 'test/bleu': 22.244177843138303, 'test/num_examples': 3003, 'score': 4231.109885454178, 'total_duration': 8015.491140365601, 'accumulated_submission_time': 4231.109885454178, 'accumulated_eval_time': 3769.9720816612244, 'accumulated_logging_time': 14.294504642486572, 'global_step': 11692, 'preemption_count': 0}), (14031, {'train/accuracy': 0.5978480577468872, 'train/loss': 2.53454327583313, 'train/bleu': 28.084821965666613, 'validation/accuracy': 0.6034519076347351, 'validation/loss': 2.4549405574798584, 'validation/bleu': 24.299863370180123, 'validation/num_examples': 3000, 'test/accuracy': 0.6077741384506226, 'test/loss': 2.4446675777435303, 'test/bleu': 23.090845019792773, 'test/num_examples': 3003, 'score': 5071.177137374878, 'total_duration': 9306.316750764847, 'accumulated_submission_time': 5071.177137374878, 'accumulated_eval_time': 4217.871542453766, 'accumulated_logging_time': 17.129082918167114, 'global_step': 14031, 'preemption_count': 0}), (16370, {'train/accuracy': 0.6017758250236511, 'train/loss': 2.4928479194641113, 'train/bleu': 29.02024066617451, 'validation/accuracy': 0.6122304797172546, 'validation/loss': 2.3794639110565186, 'validation/bleu': 25.106822043407924, 'validation/num_examples': 3000, 'test/accuracy': 0.6168497204780579, 'test/loss': 2.360391855239868, 'test/bleu': 23.890500441896847, 'test/num_examples': 3003, 'score': 5911.170006752014, 'total_duration': 10610.795687198639, 'accumulated_submission_time': 5911.170006752014, 'accumulated_eval_time': 4678.160729169846, 'accumulated_logging_time': 21.30055069923401, 'global_step': 16370, 'preemption_count': 0}), (18709, {'train/accuracy': 0.6066215634346008, 'train/loss': 2.456886053085327, 'train/bleu': 29.458484187128306, 'validation/accuracy': 0.6179588437080383, 'validation/loss': 2.3170437812805176, 'validation/bleu': 25.728684676524146, 'validation/num_examples': 3000, 'test/accuracy': 0.6256347894668579, 'test/loss': 2.2894904613494873, 'test/bleu': 24.369917892841436, 'test/num_examples': 3003, 'score': 6751.269082069397, 'total_duration': 11897.71165561676, 'accumulated_submission_time': 6751.269082069397, 'accumulated_eval_time': 5122.086347103119, 'accumulated_logging_time': 24.16819667816162, 'global_step': 18709, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6209314465522766, 'train/loss': 2.333723306655884, 'train/bleu': 30.402126439217927, 'validation/accuracy': 0.6225837469100952, 'validation/loss': 2.2762746810913086, 'validation/bleu': 26.11916726004152, 'validation/num_examples': 3000, 'test/accuracy': 0.6283655762672424, 'test/loss': 2.2507011890411377, 'test/bleu': 24.734926246902845, 'test/num_examples': 3003, 'score': 7214.646990537643, 'total_duration': 12810.375185966492, 'accumulated_submission_time': 7214.646990537643, 'accumulated_eval_time': 5568.519872665405, 'accumulated_logging_time': 27.008020877838135, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0418 18:31:41.146956 140391635986240 submission_runner.py:570] Timing: 7214.646990537643
I0418 18:31:41.147026 140391635986240 submission_runner.py:571] ====================
I0418 18:31:41.147133 140391635986240 submission_runner.py:631] Final wmt score: 7214.646990537643
