I0418 13:41:41.381299 139727044499264 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_adamw/ogbg_jax.
I0418 13:41:41.444460 139727044499264 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 13:41:42.334403 139727044499264 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0418 13:41:42.335077 139727044499264 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 13:41:42.338944 139727044499264 submission_runner.py:528] Using RNG seed 1885608297
I0418 13:41:44.968753 139727044499264 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 13:41:44.968940 139727044499264 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1.
I0418 13:41:44.969195 139727044499264 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/hparams.json.
I0418 13:41:45.092891 139727044499264 submission_runner.py:232] Initializing dataset.
I0418 13:41:45.328499 139727044499264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:41:45.333023 139727044499264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:41:45.557063 139727044499264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:41:45.610503 139727044499264 submission_runner.py:239] Initializing model.
I0418 13:41:52.686024 139727044499264 submission_runner.py:249] Initializing optimizer.
I0418 13:41:53.071145 139727044499264 submission_runner.py:256] Initializing metrics bundle.
I0418 13:41:53.071322 139727044499264 submission_runner.py:273] Initializing checkpoint and logger.
I0418 13:41:53.072273 139727044499264 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1 with prefix checkpoint_
I0418 13:41:53.072507 139727044499264 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 13:41:53.072567 139727044499264 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 13:41:53.903399 139727044499264 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/meta_data_0.json.
I0418 13:41:53.904301 139727044499264 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/flags_0.json.
I0418 13:41:53.910857 139727044499264 submission_runner.py:309] Starting training loop.
I0418 13:42:13.877485 139551095908096 logging_writer.py:48] [0] global_step=0, grad_norm=3.0103087425231934, loss=0.7869975566864014
I0418 13:42:13.892779 139727044499264 spec.py:298] Evaluating on the training split.
I0418 13:42:13.901923 139727044499264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:42:13.906372 139727044499264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:42:13.967033 139727044499264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0418 13:42:30.442951 139727044499264 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0418 13:43:44.235556 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 13:43:44.238663 139727044499264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:43:44.242565 139727044499264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:43:44.300329 139727044499264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:44:47.230942 139727044499264 spec.py:326] Evaluating on the test split.
I0418 13:44:47.233588 139727044499264 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:44:47.237524 139727044499264 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:44:47.290133 139727044499264 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:45:49.443381 139727044499264 submission_runner.py:406] Time since start: 235.53s, 	Step: 1, 	{'train/accuracy': 0.44985297322273254, 'train/loss': 0.7875021696090698, 'train/mean_average_precision': 0.025462158852563352, 'validation/accuracy': 0.4534994661808014, 'validation/loss': 0.7844457626342773, 'validation/mean_average_precision': 0.029282101697097506, 'validation/num_examples': 43793, 'test/accuracy': 0.4528583884239197, 'test/loss': 0.7841711044311523, 'test/mean_average_precision': 0.029529472461958025, 'test/num_examples': 43793, 'score': 19.98175048828125, 'total_duration': 235.53248405456543, 'accumulated_submission_time': 19.98175048828125, 'accumulated_eval_time': 215.55058646202087, 'accumulated_logging_time': 0}
I0418 13:45:49.460976 139541347895040 logging_writer.py:48] [1] accumulated_eval_time=215.550586, accumulated_logging_time=0, accumulated_submission_time=19.981750, global_step=1, preemption_count=0, score=19.981750, test/accuracy=0.452858, test/loss=0.784171, test/mean_average_precision=0.029529, test/num_examples=43793, total_duration=235.532484, train/accuracy=0.449853, train/loss=0.787502, train/mean_average_precision=0.025462, validation/accuracy=0.453499, validation/loss=0.784446, validation/mean_average_precision=0.029282, validation/num_examples=43793
I0418 13:45:49.495727 139727044499264 checkpoints.py:356] Saving checkpoint at step: 1
I0418 13:45:49.603752 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_1
I0418 13:45:49.604171 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_1.
I0418 13:46:13.358906 139541356287744 logging_writer.py:48] [100] global_step=100, grad_norm=0.402414470911026, loss=0.34487730264663696
I0418 13:46:36.680797 139542765721344 logging_writer.py:48] [200] global_step=200, grad_norm=0.2541838586330414, loss=0.20666512846946716
I0418 13:46:59.968287 139541356287744 logging_writer.py:48] [300] global_step=300, grad_norm=0.1290367990732193, loss=0.11397570371627808
I0418 13:47:23.411973 139542765721344 logging_writer.py:48] [400] global_step=400, grad_norm=0.06533011049032211, loss=0.07207376509904861
I0418 13:47:46.503966 139541356287744 logging_writer.py:48] [500] global_step=500, grad_norm=0.03514442965388298, loss=0.0654921680688858
I0418 13:48:09.901665 139542765721344 logging_writer.py:48] [600] global_step=600, grad_norm=0.06900204718112946, loss=0.055859364569187164
I0418 13:48:33.520832 139541356287744 logging_writer.py:48] [700] global_step=700, grad_norm=0.06800030171871185, loss=0.05853772535920143
I0418 13:48:57.159234 139542765721344 logging_writer.py:48] [800] global_step=800, grad_norm=0.02506103366613388, loss=0.05570552870631218
I0418 13:49:20.545973 139541356287744 logging_writer.py:48] [900] global_step=900, grad_norm=0.05804961547255516, loss=0.05507490038871765
I0418 13:49:43.934734 139542765721344 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.09898387640714645, loss=0.0497513972222805
I0418 13:49:49.605225 139727044499264 spec.py:298] Evaluating on the training split.
I0418 13:51:01.823060 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 13:51:04.408153 139727044499264 spec.py:326] Evaluating on the test split.
I0418 13:51:06.905962 139727044499264 submission_runner.py:406] Time since start: 553.00s, 	Step: 1025, 	{'train/accuracy': 0.9867925643920898, 'train/loss': 0.05000653862953186, 'train/mean_average_precision': 0.06915501916559448, 'validation/accuracy': 0.9841703176498413, 'validation/loss': 0.05972651392221451, 'validation/mean_average_precision': 0.07063657075720024, 'validation/num_examples': 43793, 'test/accuracy': 0.9831997752189636, 'test/loss': 0.06312652677297592, 'test/mean_average_precision': 0.0696901416303955, 'test/num_examples': 43793, 'score': 259.97359323501587, 'total_duration': 552.9950459003448, 'accumulated_submission_time': 259.97359323501587, 'accumulated_eval_time': 292.85129594802856, 'accumulated_logging_time': 0.1611769199371338}
I0418 13:51:06.914053 139541356287744 logging_writer.py:48] [1025] accumulated_eval_time=292.851296, accumulated_logging_time=0.161177, accumulated_submission_time=259.973593, global_step=1025, preemption_count=0, score=259.973593, test/accuracy=0.983200, test/loss=0.063127, test/mean_average_precision=0.069690, test/num_examples=43793, total_duration=552.995046, train/accuracy=0.986793, train/loss=0.050007, train/mean_average_precision=0.069155, validation/accuracy=0.984170, validation/loss=0.059727, validation/mean_average_precision=0.070637, validation/num_examples=43793
I0418 13:51:06.948642 139727044499264 checkpoints.py:356] Saving checkpoint at step: 1025
I0418 13:51:07.057280 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_1025
I0418 13:51:07.057711 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_1025.
I0418 13:51:24.774391 139542765721344 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.03280642628669739, loss=0.049054212868213654
I0418 13:51:48.104401 139542631798528 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.025926977396011353, loss=0.050617020577192307
I0418 13:52:11.616729 139542765721344 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04862949997186661, loss=0.051150932908058167
I0418 13:52:35.469502 139542631798528 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.012827076949179173, loss=0.05159638077020645
I0418 13:52:58.955192 139542765721344 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.032884906977415085, loss=0.054991282522678375
I0418 13:53:22.600260 139542631798528 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.056967079639434814, loss=0.051014650613069534
I0418 13:53:46.333916 139542765721344 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.025168266147375107, loss=0.05082140862941742
I0418 13:54:10.133627 139542631798528 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03649663180112839, loss=0.0459878034889698
I0418 13:54:33.908418 139542765721344 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03361900895833969, loss=0.04631999507546425
I0418 13:54:57.641433 139542631798528 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.026624154299497604, loss=0.04634249582886696
I0418 13:55:07.065986 139727044499264 spec.py:298] Evaluating on the training split.
I0418 13:56:20.983791 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 13:56:23.518968 139727044499264 spec.py:326] Evaluating on the test split.
I0418 13:56:25.994012 139727044499264 submission_runner.py:406] Time since start: 872.08s, 	Step: 2041, 	{'train/accuracy': 0.9872336983680725, 'train/loss': 0.0475408136844635, 'train/mean_average_precision': 0.10896417252690277, 'validation/accuracy': 0.9845815300941467, 'validation/loss': 0.057930443435907364, 'validation/mean_average_precision': 0.10832898726624798, 'validation/num_examples': 43793, 'test/accuracy': 0.9835880994796753, 'test/loss': 0.06144624575972557, 'test/mean_average_precision': 0.10712030059053332, 'test/num_examples': 43793, 'score': 499.9730613231659, 'total_duration': 872.083099603653, 'accumulated_submission_time': 499.9730613231659, 'accumulated_eval_time': 371.7792899608612, 'accumulated_logging_time': 0.3133881092071533}
I0418 13:56:26.002926 139542765721344 logging_writer.py:48] [2041] accumulated_eval_time=371.779290, accumulated_logging_time=0.313388, accumulated_submission_time=499.973061, global_step=2041, preemption_count=0, score=499.973061, test/accuracy=0.983588, test/loss=0.061446, test/mean_average_precision=0.107120, test/num_examples=43793, total_duration=872.083100, train/accuracy=0.987234, train/loss=0.047541, train/mean_average_precision=0.108964, validation/accuracy=0.984582, validation/loss=0.057930, validation/mean_average_precision=0.108329, validation/num_examples=43793
I0418 13:56:26.035409 139727044499264 checkpoints.py:356] Saving checkpoint at step: 2041
I0418 13:56:26.141511 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_2041
I0418 13:56:26.141719 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_2041.
I0418 13:56:40.039839 139542631798528 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.020331822335720062, loss=0.05400465056300163
I0418 13:57:03.201277 139542430471936 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.027712523937225342, loss=0.0473974272608757
I0418 13:57:26.415674 139542631798528 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.01861649379134178, loss=0.0513351671397686
I0418 13:57:49.429062 139542430471936 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.03059503436088562, loss=0.04973750188946724
I0418 13:58:12.541888 139542631798528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.06648535281419754, loss=0.05040894076228142
I0418 13:58:35.420592 139542430471936 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.019243799149990082, loss=0.05081074312329292
I0418 13:58:58.365125 139542631798528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.037148214876651764, loss=0.05068330466747284
I0418 13:59:21.718850 139542430471936 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.02852378599345684, loss=0.04451695829629898
I0418 13:59:44.732915 139542631798528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.023083096370100975, loss=0.04791952297091484
I0418 14:00:07.732811 139542430471936 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.021295098587870598, loss=0.045039352029561996
I0418 14:00:26.172127 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:01:36.789138 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:01:39.362384 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:01:41.845616 139727044499264 submission_runner.py:406] Time since start: 1187.93s, 	Step: 3082, 	{'train/accuracy': 0.9877272248268127, 'train/loss': 0.04349270462989807, 'train/mean_average_precision': 0.15567129245983768, 'validation/accuracy': 0.9850134253501892, 'validation/loss': 0.052849311381578445, 'validation/mean_average_precision': 0.1475472884229207, 'validation/num_examples': 43793, 'test/accuracy': 0.9839684367179871, 'test/loss': 0.055928148329257965, 'test/mean_average_precision': 0.138594094206493, 'test/num_examples': 43793, 'score': 739.9945759773254, 'total_duration': 1187.9346897602081, 'accumulated_submission_time': 739.9945759773254, 'accumulated_eval_time': 447.4527335166931, 'accumulated_logging_time': 0.46144962310791016}
I0418 14:01:41.853375 139542631798528 logging_writer.py:48] [3082] accumulated_eval_time=447.452734, accumulated_logging_time=0.461450, accumulated_submission_time=739.994576, global_step=3082, preemption_count=0, score=739.994576, test/accuracy=0.983968, test/loss=0.055928, test/mean_average_precision=0.138594, test/num_examples=43793, total_duration=1187.934690, train/accuracy=0.987727, train/loss=0.043493, train/mean_average_precision=0.155671, validation/accuracy=0.985013, validation/loss=0.052849, validation/mean_average_precision=0.147547, validation/num_examples=43793
I0418 14:01:41.886585 139727044499264 checkpoints.py:356] Saving checkpoint at step: 3082
I0418 14:01:41.979420 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_3082
I0418 14:01:41.980020 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_3082.
I0418 14:01:46.485143 139542430471936 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.018222525715827942, loss=0.04433629661798477
I0418 14:02:09.846985 139542422079232 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.012635835446417332, loss=0.04868745803833008
I0418 14:02:32.688533 139542430471936 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.01631699688732624, loss=0.045447949320077896
I0418 14:02:55.679234 139542422079232 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0104352543130517, loss=0.044095031917095184
I0418 14:03:19.098089 139542430471936 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.025723332539200783, loss=0.046852584928274155
I0418 14:03:42.269921 139542422079232 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.013907085172832012, loss=0.041122596710920334
I0418 14:04:05.735589 139542430471936 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.01656610704958439, loss=0.04043366387486458
I0418 14:04:29.069087 139542422079232 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.020308561623096466, loss=0.04310067743062973
I0418 14:04:52.360061 139542430471936 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.01622822694480419, loss=0.04325166344642639
I0418 14:05:15.548274 139542422079232 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.028380392119288445, loss=0.04279429838061333
I0418 14:05:38.564077 139542430471936 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.027503764256834984, loss=0.04335283860564232
I0418 14:05:42.053580 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:06:54.875749 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:06:57.411840 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:06:59.878082 139727044499264 submission_runner.py:406] Time since start: 1505.97s, 	Step: 4116, 	{'train/accuracy': 0.9880774617195129, 'train/loss': 0.04142257943749428, 'train/mean_average_precision': 0.1835592278158294, 'validation/accuracy': 0.9852330684661865, 'validation/loss': 0.051216255873441696, 'validation/mean_average_precision': 0.1619396106349861, 'validation/num_examples': 43793, 'test/accuracy': 0.9842881560325623, 'test/loss': 0.054057713598012924, 'test/mean_average_precision': 0.1622006880494167, 'test/num_examples': 43793, 'score': 980.0591495037079, 'total_duration': 1505.967169046402, 'accumulated_submission_time': 980.0591495037079, 'accumulated_eval_time': 525.2771983146667, 'accumulated_logging_time': 0.5962679386138916}
I0418 14:06:59.885881 139542422079232 logging_writer.py:48] [4116] accumulated_eval_time=525.277198, accumulated_logging_time=0.596268, accumulated_submission_time=980.059150, global_step=4116, preemption_count=0, score=980.059150, test/accuracy=0.984288, test/loss=0.054058, test/mean_average_precision=0.162201, test/num_examples=43793, total_duration=1505.967169, train/accuracy=0.988077, train/loss=0.041423, train/mean_average_precision=0.183559, validation/accuracy=0.985233, validation/loss=0.051216, validation/mean_average_precision=0.161940, validation/num_examples=43793
I0418 14:06:59.919266 139727044499264 checkpoints.py:356] Saving checkpoint at step: 4116
I0418 14:07:00.010675 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_4116
I0418 14:07:00.011329 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_4116.
I0418 14:07:19.891557 139542430471936 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.019192896783351898, loss=0.04129663482308388
I0418 14:07:43.038434 139542413686528 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.022728564217686653, loss=0.04787122830748558
I0418 14:08:06.029777 139542430471936 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.013316546566784382, loss=0.04059084132313728
I0418 14:08:28.973167 139542413686528 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.01752118393778801, loss=0.04293369501829147
I0418 14:08:52.047180 139542430471936 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.02353675290942192, loss=0.04661276191473007
I0418 14:09:15.201777 139542413686528 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.012675879523158073, loss=0.04498245567083359
I0418 14:09:38.225213 139542430471936 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01382889412343502, loss=0.03623760864138603
I0418 14:10:01.172551 139542413686528 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.011298899538815022, loss=0.04041567072272301
I0418 14:10:24.192163 139542430471936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014062412083148956, loss=0.04498298838734627
I0418 14:10:47.097010 139542413686528 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.02212037332355976, loss=0.04218750819563866
I0418 14:11:00.019054 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:12:11.405319 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:12:13.915217 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:12:16.382822 139727044499264 submission_runner.py:406] Time since start: 1822.47s, 	Step: 5157, 	{'train/accuracy': 0.9886689186096191, 'train/loss': 0.03919142484664917, 'train/mean_average_precision': 0.22250609317772063, 'validation/accuracy': 0.9856324791908264, 'validation/loss': 0.04878205060958862, 'validation/mean_average_precision': 0.18289274583365314, 'validation/num_examples': 43793, 'test/accuracy': 0.9847270250320435, 'test/loss': 0.05137151479721069, 'test/mean_average_precision': 0.18851524231168648, 'test/num_examples': 43793, 'score': 1220.057788848877, 'total_duration': 1822.4718911647797, 'accumulated_submission_time': 1220.057788848877, 'accumulated_eval_time': 601.6409287452698, 'accumulated_logging_time': 0.7299759387969971}
I0418 14:12:16.390691 139542430471936 logging_writer.py:48] [5157] accumulated_eval_time=601.640929, accumulated_logging_time=0.729976, accumulated_submission_time=1220.057789, global_step=5157, preemption_count=0, score=1220.057789, test/accuracy=0.984727, test/loss=0.051372, test/mean_average_precision=0.188515, test/num_examples=43793, total_duration=1822.471891, train/accuracy=0.988669, train/loss=0.039191, train/mean_average_precision=0.222506, validation/accuracy=0.985632, validation/loss=0.048782, validation/mean_average_precision=0.182893, validation/num_examples=43793
I0418 14:12:16.422846 139727044499264 checkpoints.py:356] Saving checkpoint at step: 5157
I0418 14:12:16.510463 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_5157
I0418 14:12:16.511085 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_5157.
I0418 14:12:26.639692 139542413686528 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.013605132699012756, loss=0.044175829738378525
I0418 14:12:49.683456 139542405293824 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.012089118361473083, loss=0.040112391114234924
I0418 14:13:12.557452 139542413686528 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.01636374369263649, loss=0.044182855635881424
I0418 14:13:35.437997 139542405293824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013268068432807922, loss=0.03695332631468773
I0418 14:13:58.405262 139542413686528 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.009097328409552574, loss=0.04051761329174042
I0418 14:14:21.571866 139542405293824 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.00913558155298233, loss=0.04045865684747696
I0418 14:14:44.681138 139542413686528 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01051359437406063, loss=0.038704533129930496
I0418 14:15:07.942619 139542405293824 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.008703268133103848, loss=0.03899439424276352
I0418 14:15:30.920866 139542413686528 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.013918416574597359, loss=0.04120875522494316
I0418 14:15:53.846487 139542405293824 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.011177276261150837, loss=0.04023873060941696
I0418 14:16:16.664498 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:17:28.340041 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:17:30.874083 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:17:33.343244 139727044499264 submission_runner.py:406] Time since start: 2139.43s, 	Step: 6200, 	{'train/accuracy': 0.9885526299476624, 'train/loss': 0.039289288222789764, 'train/mean_average_precision': 0.2362580476786148, 'validation/accuracy': 0.9856662154197693, 'validation/loss': 0.04935925826430321, 'validation/mean_average_precision': 0.19681640403550515, 'validation/num_examples': 43793, 'test/accuracy': 0.9847620129585266, 'test/loss': 0.05222162976861, 'test/mean_average_precision': 0.1994952891673684, 'test/num_examples': 43793, 'score': 1460.2022490501404, 'total_duration': 2139.4323081970215, 'accumulated_submission_time': 1460.2022490501404, 'accumulated_eval_time': 678.3196170330048, 'accumulated_logging_time': 0.8586809635162354}
I0418 14:17:33.351216 139542413686528 logging_writer.py:48] [6200] accumulated_eval_time=678.319617, accumulated_logging_time=0.858681, accumulated_submission_time=1460.202249, global_step=6200, preemption_count=0, score=1460.202249, test/accuracy=0.984762, test/loss=0.052222, test/mean_average_precision=0.199495, test/num_examples=43793, total_duration=2139.432308, train/accuracy=0.988553, train/loss=0.039289, train/mean_average_precision=0.236258, validation/accuracy=0.985666, validation/loss=0.049359, validation/mean_average_precision=0.196816, validation/num_examples=43793
I0418 14:17:33.384460 139727044499264 checkpoints.py:356] Saving checkpoint at step: 6200
I0418 14:17:33.472087 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_6200
I0418 14:17:33.472525 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_6200.
I0418 14:17:33.712564 139542405293824 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.020792942494153976, loss=0.04158966615796089
I0418 14:17:56.836952 139542396901120 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.014779319986701012, loss=0.04032769426703453
I0418 14:18:19.890451 139542405293824 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.009343027137219906, loss=0.040463294833898544
I0418 14:18:42.781839 139542396901120 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.015240455977618694, loss=0.04081673547625542
I0418 14:19:05.737531 139542405293824 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.007761592045426369, loss=0.03822541981935501
I0418 14:19:28.579228 139542396901120 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.00910645816475153, loss=0.03722584620118141
I0418 14:19:51.627174 139542405293824 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.009510636329650879, loss=0.03961246833205223
I0418 14:20:14.776132 139542396901120 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.008000255562365055, loss=0.03974764421582222
I0418 14:20:37.521309 139542405293824 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.009642413817346096, loss=0.04048924893140793
I0418 14:21:00.553733 139542396901120 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.011056868359446526, loss=0.04054185375571251
I0418 14:21:23.767645 139542405293824 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.01183324959129095, loss=0.03903733938932419
I0418 14:21:33.625641 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:22:43.804100 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:22:46.322302 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:22:48.797261 139727044499264 submission_runner.py:406] Time since start: 2454.89s, 	Step: 7244, 	{'train/accuracy': 0.9894978404045105, 'train/loss': 0.035622939467430115, 'train/mean_average_precision': 0.3084368291622885, 'validation/accuracy': 0.9860851168632507, 'validation/loss': 0.04658699035644531, 'validation/mean_average_precision': 0.21529827811748514, 'validation/num_examples': 43793, 'test/accuracy': 0.9851962327957153, 'test/loss': 0.04914597421884537, 'test/mean_average_precision': 0.22051086212467824, 'test/num_examples': 43793, 'score': 1700.3463351726532, 'total_duration': 2454.886340856552, 'accumulated_submission_time': 1700.3463351726532, 'accumulated_eval_time': 753.4912016391754, 'accumulated_logging_time': 0.9883911609649658}
I0418 14:22:48.806071 139542396901120 logging_writer.py:48] [7244] accumulated_eval_time=753.491202, accumulated_logging_time=0.988391, accumulated_submission_time=1700.346335, global_step=7244, preemption_count=0, score=1700.346335, test/accuracy=0.985196, test/loss=0.049146, test/mean_average_precision=0.220511, test/num_examples=43793, total_duration=2454.886341, train/accuracy=0.989498, train/loss=0.035623, train/mean_average_precision=0.308437, validation/accuracy=0.986085, validation/loss=0.046587, validation/mean_average_precision=0.215298, validation/num_examples=43793
I0418 14:22:48.837399 139727044499264 checkpoints.py:356] Saving checkpoint at step: 7244
I0418 14:22:48.931572 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_7244
I0418 14:22:48.932006 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_7244.
I0418 14:23:02.061168 139542405293824 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.010941501706838608, loss=0.03971324488520622
I0418 14:23:24.948615 139542388508416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.009802659042179585, loss=0.03868976607918739
I0418 14:23:47.915502 139542405293824 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.008144642226397991, loss=0.037793565541505814
I0418 14:24:10.982816 139542388508416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.010293803177773952, loss=0.03788089007139206
I0418 14:24:33.748636 139542405293824 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.009303472004830837, loss=0.03957493603229523
I0418 14:24:56.564630 139542388508416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.015236704610288143, loss=0.04493711516261101
I0418 14:25:19.685418 139542405293824 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.011096617206931114, loss=0.03691170737147331
I0418 14:25:42.429616 139542388508416 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.01100881490856409, loss=0.038654062896966934
I0418 14:26:05.339374 139542405293824 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.008203927427530289, loss=0.03886900097131729
I0418 14:26:28.247408 139542388508416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.013231598772108555, loss=0.03774160146713257
I0418 14:26:48.937645 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:28:00.965196 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:28:03.555102 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:28:06.009063 139727044499264 submission_runner.py:406] Time since start: 2772.10s, 	Step: 8291, 	{'train/accuracy': 0.9899130463600159, 'train/loss': 0.03412054106593132, 'train/mean_average_precision': 0.3215796232930254, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.04614945501089096, 'validation/mean_average_precision': 0.23095489021040128, 'validation/num_examples': 43793, 'test/accuracy': 0.9854569435119629, 'test/loss': 0.048762936145067215, 'test/mean_average_precision': 0.23857616107339905, 'test/num_examples': 43793, 'score': 1940.342838525772, 'total_duration': 2772.098137140274, 'accumulated_submission_time': 1940.342838525772, 'accumulated_eval_time': 830.5625710487366, 'accumulated_logging_time': 1.1235713958740234}
I0418 14:28:06.017101 139542405293824 logging_writer.py:48] [8291] accumulated_eval_time=830.562571, accumulated_logging_time=1.123571, accumulated_submission_time=1940.342839, global_step=8291, preemption_count=0, score=1940.342839, test/accuracy=0.985457, test/loss=0.048763, test/mean_average_precision=0.238576, test/num_examples=43793, total_duration=2772.098137, train/accuracy=0.989913, train/loss=0.034121, train/mean_average_precision=0.321580, validation/accuracy=0.986360, validation/loss=0.046149, validation/mean_average_precision=0.230955, validation/num_examples=43793
I0418 14:28:06.050977 139727044499264 checkpoints.py:356] Saving checkpoint at step: 8291
I0418 14:28:06.142838 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_8291
I0418 14:28:06.143290 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_8291.
I0418 14:28:08.447333 139542388508416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.011734973639249802, loss=0.037247542291879654
I0418 14:28:33.065238 139542380115712 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.009601236321032047, loss=0.03849685937166214
I0418 14:28:58.119222 139542388508416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.007964099757373333, loss=0.039003320038318634
I0418 14:29:21.776145 139542380115712 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0086572440341115, loss=0.0355958491563797
I0418 14:29:44.703077 139542388508416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.012722590938210487, loss=0.04086429998278618
I0418 14:30:07.854799 139542380115712 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.009621976874768734, loss=0.03811565414071083
I0418 14:30:31.176817 139542388508416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01530449464917183, loss=0.04356251284480095
I0418 14:30:54.274885 139542380115712 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.009250165894627571, loss=0.03591572493314743
I0418 14:31:17.287237 139542388508416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.00847704615443945, loss=0.038044873625040054
I0418 14:31:40.283195 139542380115712 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01037391647696495, loss=0.033453553915023804
I0418 14:32:04.184046 139542388508416 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.011376393027603626, loss=0.036565933376550674
I0418 14:32:06.279577 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:33:23.800745 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:33:26.330988 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:33:28.792823 139727044499264 submission_runner.py:406] Time since start: 3094.88s, 	Step: 9310, 	{'train/accuracy': 0.9898830056190491, 'train/loss': 0.033903248608112335, 'train/mean_average_precision': 0.33786820172275855, 'validation/accuracy': 0.9864029884338379, 'validation/loss': 0.04559865593910217, 'validation/mean_average_precision': 0.23393462240928675, 'validation/num_examples': 43793, 'test/accuracy': 0.985511302947998, 'test/loss': 0.048293337225914, 'test/mean_average_precision': 0.23677903655076998, 'test/num_examples': 43793, 'score': 2180.469968557358, 'total_duration': 3094.8818910121918, 'accumulated_submission_time': 2180.469968557358, 'accumulated_eval_time': 913.0757610797882, 'accumulated_logging_time': 1.258284330368042}
I0418 14:33:28.800914 139542380115712 logging_writer.py:48] [9310] accumulated_eval_time=913.075761, accumulated_logging_time=1.258284, accumulated_submission_time=2180.469969, global_step=9310, preemption_count=0, score=2180.469969, test/accuracy=0.985511, test/loss=0.048293, test/mean_average_precision=0.236779, test/num_examples=43793, total_duration=3094.881891, train/accuracy=0.989883, train/loss=0.033903, train/mean_average_precision=0.337868, validation/accuracy=0.986403, validation/loss=0.045599, validation/mean_average_precision=0.233935, validation/num_examples=43793
I0418 14:33:28.833240 139727044499264 checkpoints.py:356] Saving checkpoint at step: 9310
I0418 14:33:28.923228 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_9310
I0418 14:33:28.923639 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_9310.
I0418 14:33:50.068602 139542388508416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.008816869929432869, loss=0.03383250534534454
I0418 14:34:13.180152 139542371723008 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.010926415212452412, loss=0.035409871488809586
I0418 14:34:36.290976 139542388508416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.013069435954093933, loss=0.03610321879386902
I0418 14:34:59.485934 139542371723008 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.00967329554259777, loss=0.03383895382285118
I0418 14:35:22.314729 139542388508416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.010634515434503555, loss=0.032167479395866394
I0418 14:35:45.291737 139542371723008 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.009432113729417324, loss=0.037448618561029434
I0418 14:36:08.312086 139542388508416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.010001196525990963, loss=0.04041074216365814
I0418 14:36:31.157155 139542371723008 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.008953114040195942, loss=0.03622645139694214
I0418 14:36:53.919384 139542388508416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.00857545342296362, loss=0.031456612050533295
I0418 14:37:16.889911 139542371723008 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.009034286253154278, loss=0.03207287937402725
I0418 14:37:29.069478 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:38:39.578056 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:38:42.351422 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:38:44.792915 139727044499264 submission_runner.py:406] Time since start: 3410.88s, 	Step: 10354, 	{'train/accuracy': 0.9904689192771912, 'train/loss': 0.03184741362929344, 'train/mean_average_precision': 0.3960898823917566, 'validation/accuracy': 0.9865178465843201, 'validation/loss': 0.045510780066251755, 'validation/mean_average_precision': 0.24733611962795127, 'validation/num_examples': 43793, 'test/accuracy': 0.9856574535369873, 'test/loss': 0.04820645600557327, 'test/mean_average_precision': 0.24584901950343474, 'test/num_examples': 43793, 'score': 2420.606655359268, 'total_duration': 3410.8819744586945, 'accumulated_submission_time': 2420.606655359268, 'accumulated_eval_time': 988.7991330623627, 'accumulated_logging_time': 1.3895342350006104}
I0418 14:38:44.801094 139542388508416 logging_writer.py:48] [10354] accumulated_eval_time=988.799133, accumulated_logging_time=1.389534, accumulated_submission_time=2420.606655, global_step=10354, preemption_count=0, score=2420.606655, test/accuracy=0.985657, test/loss=0.048206, test/mean_average_precision=0.245849, test/num_examples=43793, total_duration=3410.881974, train/accuracy=0.990469, train/loss=0.031847, train/mean_average_precision=0.396090, validation/accuracy=0.986518, validation/loss=0.045511, validation/mean_average_precision=0.247336, validation/num_examples=43793
I0418 14:38:44.832880 139727044499264 checkpoints.py:356] Saving checkpoint at step: 10354
I0418 14:38:44.939842 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_10354
I0418 14:38:44.940117 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_10354.
I0418 14:38:55.893511 139542371723008 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.009190691635012627, loss=0.03409525752067566
I0418 14:39:18.878928 139542363330304 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.008468005806207657, loss=0.03356783837080002
I0418 14:39:41.607008 139542371723008 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.012262854725122452, loss=0.04171338304877281
I0418 14:40:04.599794 139542363330304 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.012739257887005806, loss=0.03791933134198189
I0418 14:40:27.209101 139542371723008 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.010887856595218182, loss=0.03648819401860237
I0418 14:40:50.665317 139542363330304 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.011778431013226509, loss=0.03880154341459274
I0418 14:41:13.538316 139542371723008 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.008348789066076279, loss=0.03335658088326454
I0418 14:41:36.571462 139542363330304 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.00968860276043415, loss=0.038128066807985306
I0418 14:41:59.384493 139542371723008 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.008550665341317654, loss=0.034351594746112823
I0418 14:42:22.226184 139542363330304 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.011083994060754776, loss=0.03658111393451691
I0418 14:42:45.170106 139542371723008 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.011820468120276928, loss=0.03602711856365204
I0418 14:42:45.174771 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:43:56.367154 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:43:58.904749 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:44:01.411686 139727044499264 submission_runner.py:406] Time since start: 3727.50s, 	Step: 11401, 	{'train/accuracy': 0.9907508492469788, 'train/loss': 0.03084455244243145, 'train/mean_average_precision': 0.4144457430475402, 'validation/accuracy': 0.9865536093711853, 'validation/loss': 0.0448969304561615, 'validation/mean_average_precision': 0.24546651062514882, 'validation/num_examples': 43793, 'test/accuracy': 0.9856646060943604, 'test/loss': 0.04768535867333412, 'test/mean_average_precision': 0.2413343470278305, 'test/num_examples': 43793, 'score': 2660.832388162613, 'total_duration': 3727.5007724761963, 'accumulated_submission_time': 2660.832388162613, 'accumulated_eval_time': 1065.0359899997711, 'accumulated_logging_time': 1.5371263027191162}
I0418 14:44:01.419869 139542363330304 logging_writer.py:48] [11401] accumulated_eval_time=1065.035990, accumulated_logging_time=1.537126, accumulated_submission_time=2660.832388, global_step=11401, preemption_count=0, score=2660.832388, test/accuracy=0.985665, test/loss=0.047685, test/mean_average_precision=0.241334, test/num_examples=43793, total_duration=3727.500772, train/accuracy=0.990751, train/loss=0.030845, train/mean_average_precision=0.414446, validation/accuracy=0.986554, validation/loss=0.044897, validation/mean_average_precision=0.245467, validation/num_examples=43793
I0418 14:44:01.451696 139727044499264 checkpoints.py:356] Saving checkpoint at step: 11401
I0418 14:44:01.558997 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_11401
I0418 14:44:01.559589 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_11401.
I0418 14:44:24.309524 139542371723008 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.009942343458533287, loss=0.03380339965224266
I0418 14:44:47.283635 139542145267456 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.012043238617479801, loss=0.03539035841822624
I0418 14:45:10.281339 139542371723008 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.009741779416799545, loss=0.03433647379279137
I0418 14:45:33.098896 139542145267456 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.011281351558864117, loss=0.03143567219376564
I0418 14:45:56.037181 139542371723008 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.010949238203465939, loss=0.03760785982012749
I0418 14:46:18.759198 139727044499264 spec.py:298] Evaluating on the training split.
I0418 14:47:30.888946 139727044499264 spec.py:310] Evaluating on the validation split.
I0418 14:47:33.425122 139727044499264 spec.py:326] Evaluating on the test split.
I0418 14:47:35.883296 139727044499264 submission_runner.py:406] Time since start: 3941.97s, 	Step: 12000, 	{'train/accuracy': 0.9910141825675964, 'train/loss': 0.029737913981080055, 'train/mean_average_precision': 0.4303647005116712, 'validation/accuracy': 0.986682653427124, 'validation/loss': 0.04495403170585632, 'validation/mean_average_precision': 0.25115181717451773, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.047789137810468674, 'test/mean_average_precision': 0.24962656470796227, 'test/num_examples': 43793, 'score': 2798.0263595581055, 'total_duration': 3941.9723846912384, 'accumulated_submission_time': 2798.0263595581055, 'accumulated_eval_time': 1142.1600670814514, 'accumulated_logging_time': 1.685539722442627}
I0418 14:47:35.892307 139542145267456 logging_writer.py:48] [12000] accumulated_eval_time=1142.160067, accumulated_logging_time=1.685540, accumulated_submission_time=2798.026360, global_step=12000, preemption_count=0, score=2798.026360, test/accuracy=0.985814, test/loss=0.047789, test/mean_average_precision=0.249627, test/num_examples=43793, total_duration=3941.972385, train/accuracy=0.991014, train/loss=0.029738, train/mean_average_precision=0.430365, validation/accuracy=0.986683, validation/loss=0.044954, validation/mean_average_precision=0.251152, validation/num_examples=43793
I0418 14:47:35.925094 139727044499264 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:47:36.019651 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_12000
I0418 14:47:36.020137 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:47:36.027664 139542371723008 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2798.026360
I0418 14:47:36.053432 139727044499264 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:47:36.207698 139727044499264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_12000
I0418 14:47:36.208130 139727044499264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:47:36.347597 139727044499264 submission_runner.py:567] Tuning trial 1/1
I0418 14:47:36.347813 139727044499264 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 14:47:36.349153 139727044499264 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.44985297322273254, 'train/loss': 0.7875021696090698, 'train/mean_average_precision': 0.025462158852563352, 'validation/accuracy': 0.4534994661808014, 'validation/loss': 0.7844457626342773, 'validation/mean_average_precision': 0.029282101697097506, 'validation/num_examples': 43793, 'test/accuracy': 0.4528583884239197, 'test/loss': 0.7841711044311523, 'test/mean_average_precision': 0.029529472461958025, 'test/num_examples': 43793, 'score': 19.98175048828125, 'total_duration': 235.53248405456543, 'accumulated_submission_time': 19.98175048828125, 'accumulated_eval_time': 215.55058646202087, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1025, {'train/accuracy': 0.9867925643920898, 'train/loss': 0.05000653862953186, 'train/mean_average_precision': 0.06915501916559448, 'validation/accuracy': 0.9841703176498413, 'validation/loss': 0.05972651392221451, 'validation/mean_average_precision': 0.07063657075720024, 'validation/num_examples': 43793, 'test/accuracy': 0.9831997752189636, 'test/loss': 0.06312652677297592, 'test/mean_average_precision': 0.0696901416303955, 'test/num_examples': 43793, 'score': 259.97359323501587, 'total_duration': 552.9950459003448, 'accumulated_submission_time': 259.97359323501587, 'accumulated_eval_time': 292.85129594802856, 'accumulated_logging_time': 0.1611769199371338, 'global_step': 1025, 'preemption_count': 0}), (2041, {'train/accuracy': 0.9872336983680725, 'train/loss': 0.0475408136844635, 'train/mean_average_precision': 0.10896417252690277, 'validation/accuracy': 0.9845815300941467, 'validation/loss': 0.057930443435907364, 'validation/mean_average_precision': 0.10832898726624798, 'validation/num_examples': 43793, 'test/accuracy': 0.9835880994796753, 'test/loss': 0.06144624575972557, 'test/mean_average_precision': 0.10712030059053332, 'test/num_examples': 43793, 'score': 499.9730613231659, 'total_duration': 872.083099603653, 'accumulated_submission_time': 499.9730613231659, 'accumulated_eval_time': 371.7792899608612, 'accumulated_logging_time': 0.3133881092071533, 'global_step': 2041, 'preemption_count': 0}), (3082, {'train/accuracy': 0.9877272248268127, 'train/loss': 0.04349270462989807, 'train/mean_average_precision': 0.15567129245983768, 'validation/accuracy': 0.9850134253501892, 'validation/loss': 0.052849311381578445, 'validation/mean_average_precision': 0.1475472884229207, 'validation/num_examples': 43793, 'test/accuracy': 0.9839684367179871, 'test/loss': 0.055928148329257965, 'test/mean_average_precision': 0.138594094206493, 'test/num_examples': 43793, 'score': 739.9945759773254, 'total_duration': 1187.9346897602081, 'accumulated_submission_time': 739.9945759773254, 'accumulated_eval_time': 447.4527335166931, 'accumulated_logging_time': 0.46144962310791016, 'global_step': 3082, 'preemption_count': 0}), (4116, {'train/accuracy': 0.9880774617195129, 'train/loss': 0.04142257943749428, 'train/mean_average_precision': 0.1835592278158294, 'validation/accuracy': 0.9852330684661865, 'validation/loss': 0.051216255873441696, 'validation/mean_average_precision': 0.1619396106349861, 'validation/num_examples': 43793, 'test/accuracy': 0.9842881560325623, 'test/loss': 0.054057713598012924, 'test/mean_average_precision': 0.1622006880494167, 'test/num_examples': 43793, 'score': 980.0591495037079, 'total_duration': 1505.967169046402, 'accumulated_submission_time': 980.0591495037079, 'accumulated_eval_time': 525.2771983146667, 'accumulated_logging_time': 0.5962679386138916, 'global_step': 4116, 'preemption_count': 0}), (5157, {'train/accuracy': 0.9886689186096191, 'train/loss': 0.03919142484664917, 'train/mean_average_precision': 0.22250609317772063, 'validation/accuracy': 0.9856324791908264, 'validation/loss': 0.04878205060958862, 'validation/mean_average_precision': 0.18289274583365314, 'validation/num_examples': 43793, 'test/accuracy': 0.9847270250320435, 'test/loss': 0.05137151479721069, 'test/mean_average_precision': 0.18851524231168648, 'test/num_examples': 43793, 'score': 1220.057788848877, 'total_duration': 1822.4718911647797, 'accumulated_submission_time': 1220.057788848877, 'accumulated_eval_time': 601.6409287452698, 'accumulated_logging_time': 0.7299759387969971, 'global_step': 5157, 'preemption_count': 0}), (6200, {'train/accuracy': 0.9885526299476624, 'train/loss': 0.039289288222789764, 'train/mean_average_precision': 0.2362580476786148, 'validation/accuracy': 0.9856662154197693, 'validation/loss': 0.04935925826430321, 'validation/mean_average_precision': 0.19681640403550515, 'validation/num_examples': 43793, 'test/accuracy': 0.9847620129585266, 'test/loss': 0.05222162976861, 'test/mean_average_precision': 0.1994952891673684, 'test/num_examples': 43793, 'score': 1460.2022490501404, 'total_duration': 2139.4323081970215, 'accumulated_submission_time': 1460.2022490501404, 'accumulated_eval_time': 678.3196170330048, 'accumulated_logging_time': 0.8586809635162354, 'global_step': 6200, 'preemption_count': 0}), (7244, {'train/accuracy': 0.9894978404045105, 'train/loss': 0.035622939467430115, 'train/mean_average_precision': 0.3084368291622885, 'validation/accuracy': 0.9860851168632507, 'validation/loss': 0.04658699035644531, 'validation/mean_average_precision': 0.21529827811748514, 'validation/num_examples': 43793, 'test/accuracy': 0.9851962327957153, 'test/loss': 0.04914597421884537, 'test/mean_average_precision': 0.22051086212467824, 'test/num_examples': 43793, 'score': 1700.3463351726532, 'total_duration': 2454.886340856552, 'accumulated_submission_time': 1700.3463351726532, 'accumulated_eval_time': 753.4912016391754, 'accumulated_logging_time': 0.9883911609649658, 'global_step': 7244, 'preemption_count': 0}), (8291, {'train/accuracy': 0.9899130463600159, 'train/loss': 0.03412054106593132, 'train/mean_average_precision': 0.3215796232930254, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.04614945501089096, 'validation/mean_average_precision': 0.23095489021040128, 'validation/num_examples': 43793, 'test/accuracy': 0.9854569435119629, 'test/loss': 0.048762936145067215, 'test/mean_average_precision': 0.23857616107339905, 'test/num_examples': 43793, 'score': 1940.342838525772, 'total_duration': 2772.098137140274, 'accumulated_submission_time': 1940.342838525772, 'accumulated_eval_time': 830.5625710487366, 'accumulated_logging_time': 1.1235713958740234, 'global_step': 8291, 'preemption_count': 0}), (9310, {'train/accuracy': 0.9898830056190491, 'train/loss': 0.033903248608112335, 'train/mean_average_precision': 0.33786820172275855, 'validation/accuracy': 0.9864029884338379, 'validation/loss': 0.04559865593910217, 'validation/mean_average_precision': 0.23393462240928675, 'validation/num_examples': 43793, 'test/accuracy': 0.985511302947998, 'test/loss': 0.048293337225914, 'test/mean_average_precision': 0.23677903655076998, 'test/num_examples': 43793, 'score': 2180.469968557358, 'total_duration': 3094.8818910121918, 'accumulated_submission_time': 2180.469968557358, 'accumulated_eval_time': 913.0757610797882, 'accumulated_logging_time': 1.258284330368042, 'global_step': 9310, 'preemption_count': 0}), (10354, {'train/accuracy': 0.9904689192771912, 'train/loss': 0.03184741362929344, 'train/mean_average_precision': 0.3960898823917566, 'validation/accuracy': 0.9865178465843201, 'validation/loss': 0.045510780066251755, 'validation/mean_average_precision': 0.24733611962795127, 'validation/num_examples': 43793, 'test/accuracy': 0.9856574535369873, 'test/loss': 0.04820645600557327, 'test/mean_average_precision': 0.24584901950343474, 'test/num_examples': 43793, 'score': 2420.606655359268, 'total_duration': 3410.8819744586945, 'accumulated_submission_time': 2420.606655359268, 'accumulated_eval_time': 988.7991330623627, 'accumulated_logging_time': 1.3895342350006104, 'global_step': 10354, 'preemption_count': 0}), (11401, {'train/accuracy': 0.9907508492469788, 'train/loss': 0.03084455244243145, 'train/mean_average_precision': 0.4144457430475402, 'validation/accuracy': 0.9865536093711853, 'validation/loss': 0.0448969304561615, 'validation/mean_average_precision': 0.24546651062514882, 'validation/num_examples': 43793, 'test/accuracy': 0.9856646060943604, 'test/loss': 0.04768535867333412, 'test/mean_average_precision': 0.2413343470278305, 'test/num_examples': 43793, 'score': 2660.832388162613, 'total_duration': 3727.5007724761963, 'accumulated_submission_time': 2660.832388162613, 'accumulated_eval_time': 1065.0359899997711, 'accumulated_logging_time': 1.5371263027191162, 'global_step': 11401, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9910141825675964, 'train/loss': 0.029737913981080055, 'train/mean_average_precision': 0.4303647005116712, 'validation/accuracy': 0.986682653427124, 'validation/loss': 0.04495403170585632, 'validation/mean_average_precision': 0.25115181717451773, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.047789137810468674, 'test/mean_average_precision': 0.24962656470796227, 'test/num_examples': 43793, 'score': 2798.0263595581055, 'total_duration': 3941.9723846912384, 'accumulated_submission_time': 2798.0263595581055, 'accumulated_eval_time': 1142.1600670814514, 'accumulated_logging_time': 1.685539722442627, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0418 14:47:36.349295 139727044499264 submission_runner.py:570] Timing: 2798.0263595581055
I0418 14:47:36.349338 139727044499264 submission_runner.py:571] ====================
I0418 14:47:36.349449 139727044499264 submission_runner.py:631] Final ogbg score: 2798.0263595581055
