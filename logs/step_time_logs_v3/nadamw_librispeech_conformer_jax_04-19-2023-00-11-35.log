I0419 00:11:56.622971 139696509540160 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax.
I0419 00:11:56.689250 139696509540160 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0419 00:11:57.481420 139696509540160 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0419 00:11:57.482161 139696509540160 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0419 00:11:57.487017 139696509540160 submission_runner.py:528] Using RNG seed 3454460965
I0419 00:12:00.125458 139696509540160 submission_runner.py:537] --- Tuning run 1/1 ---
I0419 00:12:00.125656 139696509540160 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1.
I0419 00:12:00.125823 139696509540160 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/hparams.json.
I0419 00:12:00.246318 139696509540160 submission_runner.py:232] Initializing dataset.
I0419 00:12:00.246499 139696509540160 submission_runner.py:239] Initializing model.
I0419 00:12:05.767273 139696509540160 submission_runner.py:249] Initializing optimizer.
I0419 00:12:06.553466 139696509540160 submission_runner.py:256] Initializing metrics bundle.
I0419 00:12:06.553639 139696509540160 submission_runner.py:273] Initializing checkpoint and logger.
I0419 00:12:06.554693 139696509540160 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0419 00:12:06.555021 139696509540160 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0419 00:12:06.555092 139696509540160 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0419 00:12:07.184819 139696509540160 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0419 00:12:07.185727 139696509540160 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0419 00:12:07.192565 139696509540160 submission_runner.py:309] Starting training loop.
I0419 00:12:07.384548 139696509540160 input_pipeline.py:20] Loading split = train-clean-100
I0419 00:12:07.414833 139696509540160 input_pipeline.py:20] Loading split = train-clean-360
I0419 00:12:07.707690 139696509540160 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0419 00:13:12.143993 139521173747456 logging_writer.py:48] [0] global_step=0, grad_norm=37.973777770996094, loss=31.39882469177246
I0419 00:13:12.169599 139696509540160 spec.py:298] Evaluating on the training split.
I0419 00:13:12.269644 139696509540160 input_pipeline.py:20] Loading split = train-clean-100
I0419 00:13:12.296692 139696509540160 input_pipeline.py:20] Loading split = train-clean-360
I0419 00:13:12.546151 139696509540160 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0419 00:13:55.983096 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 00:13:56.043523 139696509540160 input_pipeline.py:20] Loading split = dev-clean
I0419 00:13:56.047570 139696509540160 input_pipeline.py:20] Loading split = dev-other
I0419 00:14:35.370636 139696509540160 spec.py:326] Evaluating on the test split.
I0419 00:14:35.430748 139696509540160 input_pipeline.py:20] Loading split = test-clean
I0419 00:15:04.376651 139696509540160 submission_runner.py:406] Time since start: 177.18s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.08853, dtype=float32), 'train/wer': 0.9593996626616413, 'validation/ctc_loss': DeviceArray(31.043484, dtype=float32), 'validation/wer': 1.0086542079518375, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.10625, dtype=float32), 'test/wer': 1.0176507626998152, 'test/num_examples': 2472, 'score': 64.97686910629272, 'total_duration': 177.18286800384521, 'accumulated_submission_time': 64.97686910629272, 'accumulated_eval_time': 112.20585536956787, 'accumulated_logging_time': 0}
I0419 00:15:04.396133 139518883653376 logging_writer.py:48] [1] accumulated_eval_time=112.205855, accumulated_logging_time=0, accumulated_submission_time=64.976869, global_step=1, preemption_count=0, score=64.976869, test/ctc_loss=31.106250762939453, test/num_examples=2472, test/wer=1.017651, total_duration=177.182868, train/ctc_loss=32.088531494140625, train/wer=0.959400, validation/ctc_loss=31.04348373413086, validation/num_examples=5348, validation/wer=1.008654
I0419 00:15:04.656875 139696509540160 checkpoints.py:356] Saving checkpoint at step: 1
I0419 00:15:05.459072 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_1
I0419 00:15:05.460216 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_1.
I0419 00:16:38.490088 139523868665600 logging_writer.py:48] [100] global_step=100, grad_norm=2.1569204330444336, loss=6.625351428985596
I0419 00:17:54.869497 139523877058304 logging_writer.py:48] [200] global_step=200, grad_norm=1.0828988552093506, loss=5.876855850219727
I0419 00:19:11.124417 139523868665600 logging_writer.py:48] [300] global_step=300, grad_norm=0.4529620409011841, loss=5.818265914916992
I0419 00:20:27.707597 139523877058304 logging_writer.py:48] [400] global_step=400, grad_norm=0.7012441158294678, loss=5.830113410949707
I0419 00:21:43.843272 139523868665600 logging_writer.py:48] [500] global_step=500, grad_norm=0.3302166759967804, loss=5.800699710845947
I0419 00:23:00.035039 139523877058304 logging_writer.py:48] [600] global_step=600, grad_norm=0.33243390917778015, loss=5.788180828094482
I0419 00:24:26.207777 139523868665600 logging_writer.py:48] [700] global_step=700, grad_norm=1.2448034286499023, loss=5.80222225189209
I0419 00:25:53.188773 139523877058304 logging_writer.py:48] [800] global_step=800, grad_norm=1.161746859550476, loss=5.798365116119385
I0419 00:27:15.221323 139523868665600 logging_writer.py:48] [900] global_step=900, grad_norm=1.2390061616897583, loss=5.765397548675537
I0419 00:28:34.449286 139523877058304 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3925306797027588, loss=5.78583288192749
I0419 00:29:53.166713 139520917997312 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8588511943817139, loss=5.668877124786377
I0419 00:31:09.284478 139520234223360 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3718997836112976, loss=5.521620273590088
I0419 00:32:25.706256 139520917997312 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7527672052383423, loss=5.403834342956543
I0419 00:33:41.744980 139520234223360 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2332569360733032, loss=5.046266555786133
I0419 00:34:57.773847 139520917997312 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.7028985023498535, loss=4.351902008056641
I0419 00:36:14.145617 139520234223360 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1914294958114624, loss=3.9009952545166016
I0419 00:37:30.206387 139520917997312 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.2836230993270874, loss=3.6137659549713135
I0419 00:38:46.203972 139520234223360 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9853770136833191, loss=3.4180054664611816
I0419 00:40:04.964094 139520917997312 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9839137196540833, loss=3.275925874710083
I0419 00:41:24.286789 139520234223360 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3967410326004028, loss=3.1006529331207275
I0419 00:42:45.631097 139520917997312 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.971588671207428, loss=3.031548023223877
I0419 00:44:01.563880 139520234223360 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9766471982002258, loss=2.89886736869812
I0419 00:45:17.390146 139520917997312 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0121705532073975, loss=2.8386683464050293
I0419 00:46:33.237457 139520234223360 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2604197263717651, loss=2.7786896228790283
I0419 00:47:49.442561 139520917997312 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.232359766960144, loss=2.7771880626678467
I0419 00:49:05.504884 139520234223360 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9923256039619446, loss=2.6985108852386475
I0419 00:50:21.441384 139520917997312 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.133484959602356, loss=2.629572868347168
I0419 00:51:37.155334 139520234223360 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0338232517242432, loss=2.5984363555908203
I0419 00:52:52.833283 139520917997312 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2079764604568481, loss=2.531363010406494
I0419 00:54:13.983399 139520234223360 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9745010733604431, loss=2.5322725772857666
I0419 00:55:05.814135 139696509540160 spec.py:298] Evaluating on the training split.
I0419 00:55:37.370560 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 00:56:14.255110 139696509540160 spec.py:326] Evaluating on the test split.
I0419 00:56:33.118762 139696509540160 submission_runner.py:406] Time since start: 2665.92s, 	Step: 3070, 	{'train/ctc_loss': DeviceArray(2.9769435, dtype=float32), 'train/wer': 0.6461967827645156, 'validation/ctc_loss': DeviceArray(3.4007664, dtype=float32), 'validation/wer': 0.7056315063338768, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3.0779033, dtype=float32), 'test/wer': 0.6434302195681758, 'test/num_examples': 2472, 'score': 2465.296181201935, 'total_duration': 2665.9228432178497, 'accumulated_submission_time': 2465.296181201935, 'accumulated_eval_time': 199.50718212127686, 'accumulated_logging_time': 1.0857183933258057}
I0419 00:56:33.137831 139520917997312 logging_writer.py:48] [3070] accumulated_eval_time=199.507182, accumulated_logging_time=1.085718, accumulated_submission_time=2465.296181, global_step=3070, preemption_count=0, score=2465.296181, test/ctc_loss=3.0779032707214355, test/num_examples=2472, test/wer=0.643430, total_duration=2665.922843, train/ctc_loss=2.9769434928894043, train/wer=0.646197, validation/ctc_loss=3.400766372680664, validation/num_examples=5348, validation/wer=0.705632
I0419 00:56:33.426492 139696509540160 checkpoints.py:356] Saving checkpoint at step: 3070
I0419 00:56:34.760695 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_3070
I0419 00:56:34.786610 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_3070.
I0419 00:57:01.556074 139524028126976 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9707139134407043, loss=2.458475351333618
I0419 00:58:17.347284 139524019734272 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9130240082740784, loss=2.409451961517334
I0419 00:59:33.098960 139524028126976 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9870918989181519, loss=2.4189186096191406
I0419 01:00:48.686312 139524019734272 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9038316607475281, loss=2.29758358001709
I0419 01:02:04.415237 139524028126976 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8388963341712952, loss=2.325914144515991
I0419 01:03:20.102172 139524019734272 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.0258564949035645, loss=2.261842727661133
I0419 01:04:35.921039 139524028126976 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1111154556274414, loss=2.2737481594085693
I0419 01:05:51.594424 139524019734272 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8278489112854004, loss=2.2306861877441406
I0419 01:07:11.894165 139524028126976 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0159937143325806, loss=2.226715087890625
I0419 01:08:29.684478 139524019734272 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7995513677597046, loss=2.1513731479644775
I0419 01:09:51.003913 139524028126976 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8977063298225403, loss=2.1391618251800537
I0419 01:11:10.120833 139520917997312 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9324766397476196, loss=2.090426206588745
I0419 01:12:25.658818 139520234223360 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1419099569320679, loss=2.0817840099334717
I0419 01:13:41.392865 139520917997312 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8405938744544983, loss=2.067580223083496
I0419 01:14:56.885938 139520234223360 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8081366419792175, loss=2.0772225856781006
I0419 01:16:12.420113 139520917997312 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7360115647315979, loss=2.0834624767303467
I0419 01:17:27.833935 139520234223360 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7117148041725159, loss=1.9701026678085327
I0419 01:18:43.250081 139520917997312 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7420144081115723, loss=2.0145201683044434
I0419 01:19:58.660235 139520234223360 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7393523454666138, loss=1.9676491022109985
I0419 01:21:15.909389 139520917997312 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7557058930397034, loss=1.938897728919983
I0419 01:22:38.095103 139520234223360 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7847967743873596, loss=1.868645191192627
I0419 01:23:59.264462 139520917997312 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6690725684165955, loss=1.8768244981765747
I0419 01:25:14.700850 139520234223360 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8815140724182129, loss=1.9373754262924194
I0419 01:26:29.966236 139520917997312 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5899317264556885, loss=1.873842477798462
I0419 01:27:45.304969 139520234223360 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7187932133674622, loss=1.8797719478607178
I0419 01:29:00.527843 139520917997312 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6093405485153198, loss=1.9214270114898682
I0419 01:30:15.898448 139520234223360 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7269805073738098, loss=1.8577759265899658
I0419 01:31:36.241181 139520917997312 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6420001983642578, loss=1.8064260482788086
I0419 01:32:54.862068 139520234223360 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7655741572380066, loss=1.8942664861679077
I0419 01:34:18.309410 139520917997312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5946208834648132, loss=1.814392328262329
I0419 01:35:41.714525 139520234223360 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6011043190956116, loss=1.9092007875442505
I0419 01:36:34.874572 139696509540160 spec.py:298] Evaluating on the training split.
I0419 01:37:10.599039 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 01:37:47.966914 139696509540160 spec.py:326] Evaluating on the test split.
I0419 01:38:08.085125 139696509540160 submission_runner.py:406] Time since start: 5160.89s, 	Step: 6163, 	{'train/ctc_loss': DeviceArray(0.582594, dtype=float32), 'train/wer': 0.20277548411155208, 'validation/ctc_loss': DeviceArray(0.9223782, dtype=float32), 'validation/wer': 0.27077926463352275, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6258962, dtype=float32), 'test/wer': 0.20662969959173724, 'test/num_examples': 2472, 'score': 4865.337178945541, 'total_duration': 5160.889156103134, 'accumulated_submission_time': 4865.337178945541, 'accumulated_eval_time': 292.71436500549316, 'accumulated_logging_time': 2.766684055328369}
I0419 01:38:08.106134 139523444446976 logging_writer.py:48] [6163] accumulated_eval_time=292.714365, accumulated_logging_time=2.766684, accumulated_submission_time=4865.337179, global_step=6163, preemption_count=0, score=4865.337179, test/ctc_loss=0.6258962154388428, test/num_examples=2472, test/wer=0.206630, total_duration=5160.889156, train/ctc_loss=0.5825939774513245, train/wer=0.202775, validation/ctc_loss=0.9223781824111938, validation/num_examples=5348, validation/wer=0.270779
I0419 01:38:08.367810 139696509540160 checkpoints.py:356] Saving checkpoint at step: 6163
I0419 01:38:09.627262 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_6163
I0419 01:38:09.654136 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_6163.
I0419 01:38:41.968104 139523444446976 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5307385325431824, loss=1.8555630445480347
I0419 01:39:57.146866 139523436054272 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7614471316337585, loss=1.7968833446502686
I0419 01:41:12.333742 139523444446976 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5882118940353394, loss=1.6946451663970947
I0419 01:42:27.510179 139523436054272 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6681496500968933, loss=1.7431358098983765
I0419 01:43:42.921943 139523444446976 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5658886432647705, loss=1.7770055532455444
I0419 01:44:58.053546 139523436054272 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5455162525177002, loss=1.810300588607788
I0419 01:46:15.666939 139523444446976 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5902205109596252, loss=1.722494125366211
I0419 01:47:37.979629 139523436054272 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.601673424243927, loss=1.8051663637161255
I0419 01:49:01.098995 139523444446976 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5513381361961365, loss=1.7784005403518677
I0419 01:50:23.970751 139523436054272 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.489195853471756, loss=1.7492916584014893
I0419 01:51:48.561682 139523444446976 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6542119383811951, loss=1.7239052057266235
I0419 01:53:07.003600 139523444446976 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5697749257087708, loss=1.7524704933166504
I0419 01:54:22.253714 139523436054272 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5587664842605591, loss=1.6511757373809814
I0419 01:55:37.422294 139523444446976 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5827192664146423, loss=1.6886626482009888
I0419 01:56:52.531525 139523436054272 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6097920536994934, loss=1.7192381620407104
I0419 01:58:07.974480 139523444446976 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5542076826095581, loss=1.6867769956588745
I0419 01:59:23.101296 139523436054272 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5639808773994446, loss=1.6895915269851685
I0419 02:00:40.547078 139523444446976 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4943845272064209, loss=1.7072644233703613
I0419 02:02:00.056152 139523436054272 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.48110485076904297, loss=1.5959011316299438
I0419 02:03:18.981802 139523444446976 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5290679335594177, loss=1.6435290575027466
I0419 02:04:39.254443 139523436054272 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5448028445243835, loss=1.6054638624191284
I0419 02:05:58.172997 139523444446976 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4767433702945709, loss=1.6659849882125854
I0419 02:07:13.252705 139523436054272 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5510179996490479, loss=1.6948575973510742
I0419 02:08:28.360418 139523444446976 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4923733174800873, loss=1.6705293655395508
I0419 02:09:43.718646 139523436054272 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6198030114173889, loss=1.6432639360427856
I0419 02:10:58.833606 139523444446976 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5680234432220459, loss=1.631203055381775
I0419 02:12:14.119120 139523436054272 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5303292274475098, loss=1.6277788877487183
I0419 02:13:29.320223 139523444446976 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5019924640655518, loss=1.5916495323181152
I0419 02:14:44.673782 139523436054272 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5018529295921326, loss=1.6424551010131836
I0419 02:16:00.258676 139523444446976 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5110999345779419, loss=1.5661529302597046
I0419 02:17:19.729360 139523436054272 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4567112326622009, loss=1.6420702934265137
I0419 02:18:09.925052 139696509540160 spec.py:298] Evaluating on the training split.
I0419 02:18:46.041484 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 02:19:23.390094 139696509540160 spec.py:326] Evaluating on the test split.
I0419 02:19:42.665830 139696509540160 submission_runner.py:406] Time since start: 7655.47s, 	Step: 9266, 	{'train/ctc_loss': DeviceArray(0.37213886, dtype=float32), 'train/wer': 0.14008149237524445, 'validation/ctc_loss': DeviceArray(0.70191854, dtype=float32), 'validation/wer': 0.21428089031249697, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45475775, dtype=float32), 'test/wer': 0.1572116263481811, 'test/num_examples': 2472, 'score': 7265.567469358444, 'total_duration': 7655.469788074493, 'accumulated_submission_time': 7265.567469358444, 'accumulated_eval_time': 385.451691865921, 'accumulated_logging_time': 4.3431243896484375}
I0419 02:19:42.685914 139523444446976 logging_writer.py:48] [9266] accumulated_eval_time=385.451692, accumulated_logging_time=4.343124, accumulated_submission_time=7265.567469, global_step=9266, preemption_count=0, score=7265.567469, test/ctc_loss=0.4547577500343323, test/num_examples=2472, test/wer=0.157212, total_duration=7655.469788, train/ctc_loss=0.3721388578414917, train/wer=0.140081, validation/ctc_loss=0.7019185423851013, validation/num_examples=5348, validation/wer=0.214281
I0419 02:19:42.942716 139696509540160 checkpoints.py:356] Saving checkpoint at step: 9266
I0419 02:19:44.212148 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_9266
I0419 02:19:44.239174 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_9266.
I0419 02:20:13.580011 139523444446976 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4739837348461151, loss=1.5802720785140991
I0419 02:21:28.673916 139523436054272 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.515373170375824, loss=1.5888444185256958
I0419 02:22:43.862204 139523444446976 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5750129222869873, loss=1.5527514219284058
I0419 02:23:59.116233 139523436054272 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.48485875129699707, loss=1.6608625650405884
I0419 02:25:14.178091 139523444446976 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4535360336303711, loss=1.6527140140533447
I0419 02:26:29.336543 139523436054272 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6288924217224121, loss=1.5949311256408691
I0419 02:27:44.486283 139523444446976 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5625949501991272, loss=1.5087634325027466
I0419 02:29:05.966446 139523436054272 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4331252872943878, loss=1.567309856414795
I0419 02:30:29.261960 139523444446976 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.37151262164115906, loss=1.5301399230957031
I0419 02:31:48.441221 139523436054272 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4109664261341095, loss=1.5422701835632324
I0419 02:33:08.449638 139523444446976 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.40566346049308777, loss=1.4793140888214111
I0419 02:34:23.666733 139523436054272 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4930572211742401, loss=1.4860613346099854
I0419 02:35:38.908914 139523444446976 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.604469895362854, loss=1.5198183059692383
I0419 02:36:54.243198 139523436054272 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4649503827095032, loss=1.5492231845855713
I0419 02:38:09.756231 139523444446976 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5195326209068298, loss=1.545303463935852
I0419 02:39:25.076489 139523436054272 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5035277605056763, loss=1.5006815195083618
I0419 02:40:40.195361 139523444446976 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.435106486082077, loss=1.5252255201339722
I0419 02:41:55.367465 139523436054272 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.46754148602485657, loss=1.5058196783065796
I0419 02:43:15.586677 139523444446976 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4135274291038513, loss=1.4932827949523926
I0419 02:44:36.029820 139523436054272 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.43820810317993164, loss=1.5602991580963135
I0419 02:45:53.248303 139523444446976 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4366183578968048, loss=1.4695055484771729
I0419 02:47:13.476274 139523444446976 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.38333141803741455, loss=1.478999137878418
I0419 02:48:29.029981 139523436054272 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5619359016418457, loss=1.4713090658187866
I0419 02:49:44.504865 139523444446976 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4681316018104553, loss=1.4805604219436646
I0419 02:50:59.688763 139523436054272 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.36505523324012756, loss=1.469260811805725
I0419 02:52:14.768954 139523444446976 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.3780711591243744, loss=1.5219885110855103
I0419 02:53:29.756458 139523436054272 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4086292088031769, loss=1.459699034690857
I0419 02:54:44.919528 139523444446976 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3797512948513031, loss=1.4836505651474
I0419 02:56:00.362469 139523436054272 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3741535246372223, loss=1.4214386940002441
I0419 02:57:20.446372 139523444446976 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.32488805055618286, loss=1.4930288791656494
I0419 02:58:42.544430 139523436054272 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5762672424316406, loss=1.4680324792861938
I0419 02:59:44.700897 139696509540160 spec.py:298] Evaluating on the training split.
I0419 03:00:22.392740 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 03:01:01.159289 139696509540160 spec.py:326] Evaluating on the test split.
I0419 03:01:21.053495 139696509540160 submission_runner.py:406] Time since start: 10153.86s, 	Step: 12379, 	{'train/ctc_loss': DeviceArray(0.311867, dtype=float32), 'train/wer': 0.11537978196441474, 'validation/ctc_loss': DeviceArray(0.624469, dtype=float32), 'validation/wer': 0.18695790600970583, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3799516, dtype=float32), 'test/wer': 0.1304409643938009, 'test/num_examples': 2472, 'score': 9665.98833489418, 'total_duration': 10153.857450962067, 'accumulated_submission_time': 9665.98833489418, 'accumulated_eval_time': 481.80106949806213, 'accumulated_logging_time': 5.924015760421753}
I0419 03:01:21.072522 139523598046976 logging_writer.py:48] [12379] accumulated_eval_time=481.801069, accumulated_logging_time=5.924016, accumulated_submission_time=9665.988335, global_step=12379, preemption_count=0, score=9665.988335, test/ctc_loss=0.3799515962600708, test/num_examples=2472, test/wer=0.130441, total_duration=10153.857451, train/ctc_loss=0.31186699867248535, train/wer=0.115380, validation/ctc_loss=0.624468982219696, validation/num_examples=5348, validation/wer=0.186958
I0419 03:01:21.350423 139696509540160 checkpoints.py:356] Saving checkpoint at step: 12379
I0419 03:01:22.671448 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_12379
I0419 03:01:22.698711 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_12379.
I0419 03:01:39.230872 139523589654272 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4030887186527252, loss=1.4756190776824951
I0419 03:02:54.153979 139523522512640 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.42483651638031006, loss=1.4927647113800049
I0419 03:04:09.267617 139523589654272 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4249151349067688, loss=1.4639555215835571
I0419 03:05:24.345109 139523522512640 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4582446217536926, loss=1.49589204788208
I0419 03:06:39.411038 139523589654272 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4782296121120453, loss=1.4057557582855225
I0419 03:07:54.420911 139523522512640 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.38614946603775024, loss=1.4724675416946411
I0419 03:09:09.529292 139523589654272 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.42785176634788513, loss=1.4183300733566284
I0419 03:10:27.040889 139523522512640 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4534361660480499, loss=1.4492539167404175
I0419 03:11:45.653942 139523589654272 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.36941787600517273, loss=1.3604480028152466
I0419 03:13:05.069687 139523522512640 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3812040388584137, loss=1.4512372016906738
I0419 03:14:31.174052 139523598046976 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.4743385314941406, loss=1.4124622344970703
I0419 03:15:46.256524 139523589654272 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3983837366104126, loss=1.4386827945709229
I0419 03:17:01.590027 139523598046976 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3953014016151428, loss=1.4042730331420898
I0419 03:18:16.932780 139523589654272 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.45556989312171936, loss=1.4478988647460938
I0419 03:19:32.569441 139523598046976 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.382352739572525, loss=1.4044275283813477
I0419 03:20:47.575196 139523589654272 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.35330671072006226, loss=1.4239028692245483
I0419 03:22:02.573758 139523598046976 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.43950939178466797, loss=1.4823617935180664
I0419 03:23:17.644533 139523589654272 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.40721753239631653, loss=1.4631706476211548
I0419 03:24:34.790488 139523598046976 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.4717632830142975, loss=1.41242516040802
I0419 03:25:50.902886 139523589654272 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.39660128951072693, loss=1.4611598253250122
I0419 03:27:10.898129 139523598046976 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3271588087081909, loss=1.391750454902649
I0419 03:28:28.946096 139523598046976 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.46950259804725647, loss=1.407915472984314
I0419 03:29:44.329074 139523589654272 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.358626127243042, loss=1.3708117008209229
I0419 03:30:59.314234 139523598046976 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3723621964454651, loss=1.3989653587341309
I0419 03:32:14.527915 139523589654272 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.46330466866493225, loss=1.391737937927246
I0419 03:33:29.573881 139523598046976 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.4013696610927582, loss=1.3923600912094116
I0419 03:34:44.783697 139523589654272 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4060117304325104, loss=1.3473143577575684
I0419 03:36:01.404706 139523598046976 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.4310173988342285, loss=1.4312219619750977
I0419 03:37:26.339008 139523589654272 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.44925758242607117, loss=1.4257532358169556
I0419 03:38:47.962535 139523598046976 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.35596585273742676, loss=1.3816123008728027
I0419 03:40:13.332233 139523589654272 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.36905863881111145, loss=1.3558125495910645
I0419 03:41:23.298318 139696509540160 spec.py:298] Evaluating on the training split.
I0419 03:41:59.175974 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 03:42:36.260725 139696509540160 spec.py:326] Evaluating on the test split.
I0419 03:42:54.990226 139696509540160 submission_runner.py:406] Time since start: 12647.79s, 	Step: 15487, 	{'train/ctc_loss': DeviceArray(0.2651507, dtype=float32), 'train/wer': 0.1020352693035291, 'validation/ctc_loss': DeviceArray(0.5658664, dtype=float32), 'validation/wer': 0.17104844233904815, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3447695, dtype=float32), 'test/wer': 0.11697438709808462, 'test/num_examples': 2472, 'score': 12066.548863172531, 'total_duration': 12647.794288873672, 'accumulated_submission_time': 12066.548863172531, 'accumulated_eval_time': 573.4898188114166, 'accumulated_logging_time': 7.574470281600952}
I0419 03:42:55.010886 139524028126976 logging_writer.py:48] [15487] accumulated_eval_time=573.489819, accumulated_logging_time=7.574470, accumulated_submission_time=12066.548863, global_step=15487, preemption_count=0, score=12066.548863, test/ctc_loss=0.34476950764656067, test/num_examples=2472, test/wer=0.116974, total_duration=12647.794289, train/ctc_loss=0.26515069603919983, train/wer=0.102035, validation/ctc_loss=0.5658664107322693, validation/num_examples=5348, validation/wer=0.171048
I0419 03:42:55.268615 139696509540160 checkpoints.py:356] Saving checkpoint at step: 15487
I0419 03:42:56.574843 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_15487
I0419 03:42:56.601916 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_15487.
I0419 03:43:07.253139 139524019734272 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.40344688296318054, loss=1.3907532691955566
I0419 03:44:22.587070 139523944199936 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.38816219568252563, loss=1.409646987915039
I0419 03:45:37.673971 139524019734272 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3911057710647583, loss=1.4271131753921509
I0419 03:46:52.787892 139523944199936 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.4576496481895447, loss=1.3962262868881226
I0419 03:48:07.878675 139524019734272 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.34583553671836853, loss=1.3202149868011475
I0419 03:49:22.937896 139523944199936 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.43152347207069397, loss=1.3792539834976196
I0419 03:50:37.939942 139524019734272 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.40960440039634705, loss=1.3885713815689087
I0419 03:51:52.993324 139523944199936 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.4920549988746643, loss=1.372521996498108
I0419 03:53:12.885838 139524019734272 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.40788155794143677, loss=1.3704874515533447
I0419 03:54:37.384273 139523944199936 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.36428752541542053, loss=1.3228483200073242
I0419 03:55:59.701544 139520917997312 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.35263893008232117, loss=1.392889380455017
I0419 03:57:14.761924 139520234223360 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.38007161021232605, loss=1.3655450344085693
I0419 03:58:29.736856 139520917997312 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.35627585649490356, loss=1.360537052154541
I0419 03:59:44.723899 139520234223360 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3412726819515228, loss=1.3366659879684448
I0419 04:00:59.851185 139520917997312 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.34448784589767456, loss=1.3895410299301147
I0419 04:02:14.883397 139520234223360 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3690917193889618, loss=1.2972491979599
I0419 04:03:31.102409 139520917997312 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3102414011955261, loss=1.3189780712127686
I0419 04:04:50.989098 139520234223360 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.351482629776001, loss=1.3144371509552002
I0419 04:06:15.190963 139520917997312 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3175884187221527, loss=1.3049389123916626
I0419 04:07:36.735743 139520234223360 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.387336790561676, loss=1.3479657173156738
I0419 04:08:56.795299 139520917997312 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5430264472961426, loss=1.3802944421768188
I0419 04:10:15.362875 139520917997312 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.40588685870170593, loss=1.3154857158660889
I0419 04:11:30.526626 139520234223360 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3501969873905182, loss=1.319809913635254
I0419 04:12:45.637106 139520917997312 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.40977564454078674, loss=1.3509602546691895
I0419 04:14:00.977770 139520234223360 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3708592355251312, loss=1.3335537910461426
I0419 04:15:16.146749 139520917997312 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3407065272331238, loss=1.346131443977356
I0419 04:16:31.107774 139520234223360 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4940609633922577, loss=1.4106733798980713
I0419 04:17:55.098311 139520917997312 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.32794344425201416, loss=1.2826722860336304
I0419 04:19:15.873076 139520234223360 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5425870418548584, loss=1.3471243381500244
I0419 04:20:37.956733 139520917997312 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3399018943309784, loss=1.34095299243927
I0419 04:22:00.310429 139520234223360 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.33719712495803833, loss=1.261569857597351
I0419 04:22:57.152151 139696509540160 spec.py:298] Evaluating on the training split.
I0419 04:23:34.277424 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 04:24:10.997237 139696509540160 spec.py:326] Evaluating on the test split.
I0419 04:24:30.950107 139696509540160 submission_runner.py:406] Time since start: 15143.75s, 	Step: 18571, 	{'train/ctc_loss': DeviceArray(0.25421897, dtype=float32), 'train/wer': 0.09252017860667588, 'validation/ctc_loss': DeviceArray(0.52003217, dtype=float32), 'validation/wer': 0.15722293509826432, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3107976, dtype=float32), 'test/wer': 0.10442183088578799, 'test/num_examples': 2472, 'score': 14467.058519124985, 'total_duration': 15143.754440069199, 'accumulated_submission_time': 14467.058519124985, 'accumulated_eval_time': 667.2849473953247, 'accumulated_logging_time': 9.193625211715698}
I0419 04:24:30.971110 139520917997312 logging_writer.py:48] [18571] accumulated_eval_time=667.284947, accumulated_logging_time=9.193625, accumulated_submission_time=14467.058519, global_step=18571, preemption_count=0, score=14467.058519, test/ctc_loss=0.3107976019382477, test/num_examples=2472, test/wer=0.104422, total_duration=15143.754440, train/ctc_loss=0.2542189657688141, train/wer=0.092520, validation/ctc_loss=0.5200321674346924, validation/num_examples=5348, validation/wer=0.157223
I0419 04:24:31.245336 139696509540160 checkpoints.py:356] Saving checkpoint at step: 18571
I0419 04:24:32.577059 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_18571
I0419 04:24:32.604243 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_18571.
I0419 04:24:55.182026 139520234223360 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5081750750541687, loss=1.3635791540145874
I0419 04:26:10.342407 139518892046080 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.311075359582901, loss=1.2649853229522705
I0419 04:27:25.339971 139520234223360 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.32192516326904297, loss=1.3204854726791382
I0419 04:28:40.489360 139518892046080 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3357662260532379, loss=1.2944438457489014
I0419 04:29:55.884147 139520234223360 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5707693696022034, loss=1.2764874696731567
I0419 04:31:10.900251 139518892046080 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3591531217098236, loss=1.379827857017517
I0419 04:32:25.875279 139520234223360 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.38581469655036926, loss=1.3555327653884888
I0419 04:33:44.192554 139518892046080 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3265154957771301, loss=1.3076523542404175
I0419 04:35:07.994274 139520234223360 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.36203181743621826, loss=1.3214341402053833
I0419 04:36:29.639736 139518892046080 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.34529027342796326, loss=1.3652081489562988
I0419 04:37:50.094887 139520917997312 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3673020005226135, loss=1.2993459701538086
I0419 04:39:05.727867 139520234223360 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4882144033908844, loss=1.3065792322158813
I0419 04:40:21.144418 139520917997312 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3877709209918976, loss=1.3071867227554321
I0419 04:41:36.475056 139520234223360 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.4050047695636749, loss=1.3211965560913086
I0419 04:42:50.346068 139696509540160 spec.py:298] Evaluating on the training split.
I0419 04:43:26.686135 139696509540160 spec.py:310] Evaluating on the validation split.
I0419 04:44:04.992798 139696509540160 spec.py:326] Evaluating on the test split.
I0419 04:44:24.658589 139696509540160 submission_runner.py:406] Time since start: 16337.46s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.2232742, dtype=float32), 'train/wer': 0.08647076340586646, 'validation/ctc_loss': DeviceArray(0.5178104, dtype=float32), 'validation/wer': 0.156190604829762, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30310953, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 15564.779766321182, 'total_duration': 16337.462383031845, 'accumulated_submission_time': 15564.779766321182, 'accumulated_eval_time': 761.5938663482666, 'accumulated_logging_time': 10.852925777435303}
I0419 04:44:24.679635 139520917997312 logging_writer.py:48] [20000] accumulated_eval_time=761.593866, accumulated_logging_time=10.852926, accumulated_submission_time=15564.779766, global_step=20000, preemption_count=0, score=15564.779766, test/ctc_loss=0.3031095266342163, test/num_examples=2472, test/wer=0.100502, total_duration=16337.462383, train/ctc_loss=0.22327420115470886, train/wer=0.086471, validation/ctc_loss=0.5178104043006897, validation/num_examples=5348, validation/wer=0.156191
I0419 04:44:24.933706 139696509540160 checkpoints.py:356] Saving checkpoint at step: 20000
I0419 04:44:26.266047 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0419 04:44:26.292996 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0419 04:44:26.305232 139520234223360 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15564.779766
I0419 04:44:26.442403 139696509540160 checkpoints.py:356] Saving checkpoint at step: 20000
I0419 04:44:28.199833 139696509540160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0419 04:44:28.228405 139696509540160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0419 04:44:29.440880 139696509540160 submission_runner.py:567] Tuning trial 1/1
I0419 04:44:29.441097 139696509540160 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0419 04:44:29.447048 139696509540160 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.08853, dtype=float32), 'train/wer': 0.9593996626616413, 'validation/ctc_loss': DeviceArray(31.043484, dtype=float32), 'validation/wer': 1.0086542079518375, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.10625, dtype=float32), 'test/wer': 1.0176507626998152, 'test/num_examples': 2472, 'score': 64.97686910629272, 'total_duration': 177.18286800384521, 'accumulated_submission_time': 64.97686910629272, 'accumulated_eval_time': 112.20585536956787, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3070, {'train/ctc_loss': DeviceArray(2.9769435, dtype=float32), 'train/wer': 0.6461967827645156, 'validation/ctc_loss': DeviceArray(3.4007664, dtype=float32), 'validation/wer': 0.7056315063338768, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3.0779033, dtype=float32), 'test/wer': 0.6434302195681758, 'test/num_examples': 2472, 'score': 2465.296181201935, 'total_duration': 2665.9228432178497, 'accumulated_submission_time': 2465.296181201935, 'accumulated_eval_time': 199.50718212127686, 'accumulated_logging_time': 1.0857183933258057, 'global_step': 3070, 'preemption_count': 0}), (6163, {'train/ctc_loss': DeviceArray(0.582594, dtype=float32), 'train/wer': 0.20277548411155208, 'validation/ctc_loss': DeviceArray(0.9223782, dtype=float32), 'validation/wer': 0.27077926463352275, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6258962, dtype=float32), 'test/wer': 0.20662969959173724, 'test/num_examples': 2472, 'score': 4865.337178945541, 'total_duration': 5160.889156103134, 'accumulated_submission_time': 4865.337178945541, 'accumulated_eval_time': 292.71436500549316, 'accumulated_logging_time': 2.766684055328369, 'global_step': 6163, 'preemption_count': 0}), (9266, {'train/ctc_loss': DeviceArray(0.37213886, dtype=float32), 'train/wer': 0.14008149237524445, 'validation/ctc_loss': DeviceArray(0.70191854, dtype=float32), 'validation/wer': 0.21428089031249697, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45475775, dtype=float32), 'test/wer': 0.1572116263481811, 'test/num_examples': 2472, 'score': 7265.567469358444, 'total_duration': 7655.469788074493, 'accumulated_submission_time': 7265.567469358444, 'accumulated_eval_time': 385.451691865921, 'accumulated_logging_time': 4.3431243896484375, 'global_step': 9266, 'preemption_count': 0}), (12379, {'train/ctc_loss': DeviceArray(0.311867, dtype=float32), 'train/wer': 0.11537978196441474, 'validation/ctc_loss': DeviceArray(0.624469, dtype=float32), 'validation/wer': 0.18695790600970583, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3799516, dtype=float32), 'test/wer': 0.1304409643938009, 'test/num_examples': 2472, 'score': 9665.98833489418, 'total_duration': 10153.857450962067, 'accumulated_submission_time': 9665.98833489418, 'accumulated_eval_time': 481.80106949806213, 'accumulated_logging_time': 5.924015760421753, 'global_step': 12379, 'preemption_count': 0}), (15487, {'train/ctc_loss': DeviceArray(0.2651507, dtype=float32), 'train/wer': 0.1020352693035291, 'validation/ctc_loss': DeviceArray(0.5658664, dtype=float32), 'validation/wer': 0.17104844233904815, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3447695, dtype=float32), 'test/wer': 0.11697438709808462, 'test/num_examples': 2472, 'score': 12066.548863172531, 'total_duration': 12647.794288873672, 'accumulated_submission_time': 12066.548863172531, 'accumulated_eval_time': 573.4898188114166, 'accumulated_logging_time': 7.574470281600952, 'global_step': 15487, 'preemption_count': 0}), (18571, {'train/ctc_loss': DeviceArray(0.25421897, dtype=float32), 'train/wer': 0.09252017860667588, 'validation/ctc_loss': DeviceArray(0.52003217, dtype=float32), 'validation/wer': 0.15722293509826432, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3107976, dtype=float32), 'test/wer': 0.10442183088578799, 'test/num_examples': 2472, 'score': 14467.058519124985, 'total_duration': 15143.754440069199, 'accumulated_submission_time': 14467.058519124985, 'accumulated_eval_time': 667.2849473953247, 'accumulated_logging_time': 9.193625211715698, 'global_step': 18571, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.2232742, dtype=float32), 'train/wer': 0.08647076340586646, 'validation/ctc_loss': DeviceArray(0.5178104, dtype=float32), 'validation/wer': 0.156190604829762, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30310953, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 15564.779766321182, 'total_duration': 16337.462383031845, 'accumulated_submission_time': 15564.779766321182, 'accumulated_eval_time': 761.5938663482666, 'accumulated_logging_time': 10.852925777435303, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0419 04:44:29.447229 139696509540160 submission_runner.py:570] Timing: 15564.779766321182
I0419 04:44:29.447278 139696509540160 submission_runner.py:571] ====================
I0419 04:44:29.448249 139696509540160 submission_runner.py:631] Final librispeech_conformer score: 15564.779766321182
