python3 submission_runner.py --framework=jax --workload=wmt --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_jax_04-28-2023-19-11-06.log
I0428 19:11:28.778665 140643883775808 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_sam/wmt_jax.
I0428 19:11:28.846043 140643883775808 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 19:11:29.669808 140643883775808 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0428 19:11:29.670572 140643883775808 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 19:11:29.676797 140643883775808 submission_runner.py:538] Using RNG seed 2012507770
I0428 19:11:32.420210 140643883775808 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 19:11:32.420418 140643883775808 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1.
I0428 19:11:32.420628 140643883775808 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1/hparams.json.
I0428 19:11:32.571792 140643883775808 submission_runner.py:241] Initializing dataset.
I0428 19:11:32.582815 140643883775808 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 19:11:32.586576 140643883775808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:11:32.586688 140643883775808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:11:32.710088 140643883775808 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 19:11:35.047561 140643883775808 submission_runner.py:248] Initializing model.
I0428 19:11:48.234065 140643883775808 submission_runner.py:258] Initializing optimizer.
I0428 19:11:49.195360 140643883775808 submission_runner.py:265] Initializing metrics bundle.
I0428 19:11:49.195584 140643883775808 submission_runner.py:282] Initializing checkpoint and logger.
I0428 19:11:49.196749 140643883775808 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1 with prefix checkpoint_
I0428 19:11:49.197052 140643883775808 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 19:11:49.197154 140643883775808 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 19:11:50.105537 140643883775808 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1/meta_data_0.json.
I0428 19:11:50.106555 140643883775808 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1/flags_0.json.
I0428 19:11:50.110594 140643883775808 submission_runner.py:318] Starting training loop.
I0428 19:12:44.120364 140467802990336 logging_writer.py:48] [0] global_step=0, grad_norm=5.991818904876709, loss=11.09646224975586
I0428 19:12:44.137403 140643883775808 spec.py:298] Evaluating on the training split.
I0428 19:12:44.140568 140643883775808 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 19:12:44.143563 140643883775808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:12:44.143674 140643883775808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:12:44.177644 140643883775808 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 19:12:52.986340 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 19:18:03.564112 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 19:18:03.568352 140643883775808 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 19:18:03.572234 140643883775808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:18:03.572346 140643883775808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:18:03.605154 140643883775808 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 19:18:11.737049 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 19:23:15.245916 140643883775808 spec.py:326] Evaluating on the test split.
I0428 19:23:15.248771 140643883775808 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 19:23:15.252628 140643883775808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:23:15.252756 140643883775808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:23:15.283888 140643883775808 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 19:23:23.083088 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 19:28:19.018317 140643883775808 submission_runner.py:415] Time since start: 988.91s, 	Step: 1, 	{'train/accuracy': 0.0006262025563046336, 'train/loss': 11.08775520324707, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.09957218170166, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.077507972717285, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 54.026357889175415, 'total_duration': 988.907639503479, 'accumulated_submission_time': 54.026357889175415, 'accumulated_eval_time': 934.8810908794403, 'accumulated_logging_time': 0}
I0428 19:28:19.037211 140456636708608 logging_writer.py:48] [1] accumulated_eval_time=934.881091, accumulated_logging_time=0, accumulated_submission_time=54.026358, global_step=1, preemption_count=0, score=54.026358, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.077508, test/num_examples=3003, total_duration=988.907640, train/accuracy=0.000626, train/bleu=0.000000, train/loss=11.087755, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.099572, validation/num_examples=3000
I0428 19:29:30.892086 140456645101312 logging_writer.py:48] [100] global_step=100, grad_norm=0.23828448355197906, loss=8.655561447143555
I0428 19:30:42.789269 140456636708608 logging_writer.py:48] [200] global_step=200, grad_norm=0.403374582529068, loss=8.232047080993652
I0428 19:31:54.696662 140456645101312 logging_writer.py:48] [300] global_step=300, grad_norm=0.6082350611686707, loss=7.765832901000977
I0428 19:33:06.628855 140456636708608 logging_writer.py:48] [400] global_step=400, grad_norm=0.5949633717536926, loss=7.4715447425842285
I0428 19:34:18.533895 140456645101312 logging_writer.py:48] [500] global_step=500, grad_norm=0.5557855367660522, loss=7.1852240562438965
I0428 19:35:30.467727 140456636708608 logging_writer.py:48] [600] global_step=600, grad_norm=0.6886052489280701, loss=6.9649434089660645
I0428 19:36:42.399499 140456645101312 logging_writer.py:48] [700] global_step=700, grad_norm=0.5589137077331543, loss=6.660555362701416
I0428 19:37:54.375858 140456636708608 logging_writer.py:48] [800] global_step=800, grad_norm=0.7250758409500122, loss=6.467532157897949
I0428 19:39:06.347311 140456645101312 logging_writer.py:48] [900] global_step=900, grad_norm=0.732731819152832, loss=6.302325248718262
I0428 19:40:18.288432 140456636708608 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5163791179656982, loss=6.179074287414551
I0428 19:41:30.270490 140456645101312 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6552238464355469, loss=5.92880392074585
I0428 19:42:19.620030 140643883775808 spec.py:298] Evaluating on the training split.
I0428 19:42:22.677603 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 19:47:10.881742 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 19:47:13.577891 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 19:51:58.712052 140643883775808 spec.py:326] Evaluating on the test split.
I0428 19:52:01.460253 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 19:56:55.971295 140643883775808 submission_runner.py:415] Time since start: 2705.86s, 	Step: 1170, 	{'train/accuracy': 0.30098676681518555, 'train/loss': 5.1518683433532715, 'train/bleu': 6.874567742672105, 'validation/accuracy': 0.28159600496292114, 'validation/loss': 5.376704216003418, 'validation/bleu': 3.749163831479815, 'validation/num_examples': 3000, 'test/accuracy': 0.26305270195007324, 'test/loss': 5.645264625549316, 'test/bleu': 2.5738034559259413, 'test/num_examples': 3003, 'score': 894.5841176509857, 'total_duration': 2705.8606135845184, 'accumulated_submission_time': 894.5841176509857, 'accumulated_eval_time': 1811.232308626175, 'accumulated_logging_time': 0.02819991111755371}
I0428 19:56:55.983256 140456636708608 logging_writer.py:48] [1170] accumulated_eval_time=1811.232309, accumulated_logging_time=0.028200, accumulated_submission_time=894.584118, global_step=1170, preemption_count=0, score=894.584118, test/accuracy=0.263053, test/bleu=2.573803, test/loss=5.645265, test/num_examples=3003, total_duration=2705.860614, train/accuracy=0.300987, train/bleu=6.874568, train/loss=5.151868, validation/accuracy=0.281596, validation/bleu=3.749164, validation/loss=5.376704, validation/num_examples=3000
I0428 19:57:18.308887 140456645101312 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6729415655136108, loss=5.7627716064453125
I0428 19:58:30.278253 140456636708608 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7836354970932007, loss=5.725119113922119
I0428 19:59:42.306202 140456645101312 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6063518524169922, loss=5.592446327209473
I0428 20:00:54.289089 140456636708608 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5748797059059143, loss=5.448666095733643
I0428 20:02:06.294701 140456645101312 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.690757691860199, loss=5.3133344650268555
I0428 20:03:18.256542 140456636708608 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7231789231300354, loss=5.151509761810303
I0428 20:04:30.228946 140456645101312 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0862534046173096, loss=5.118865013122559
I0428 20:05:41.945772 140456636708608 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03323257341980934, loss=7.509579658508301
I0428 20:06:53.608637 140456645101312 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.04894642159342766, loss=7.403444290161133
I0428 20:08:05.431516 140456636708608 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.2723565995693207, loss=6.857320785522461
I0428 20:09:17.269778 140456645101312 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6745577454566956, loss=6.575895309448242
I0428 20:10:29.144950 140456636708608 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8648410439491272, loss=6.2976508140563965
I0428 20:10:56.152344 140643883775808 spec.py:298] Evaluating on the training split.
I0428 20:10:59.189161 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 20:15:39.323010 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 20:15:42.008793 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 20:19:57.779482 140643883775808 spec.py:326] Evaluating on the test split.
I0428 20:20:00.519510 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 20:24:39.018468 140643883775808 submission_runner.py:415] Time since start: 4368.91s, 	Step: 2339, 	{'train/accuracy': 0.20432625710964203, 'train/loss': 5.637054443359375, 'train/bleu': 0.5850271742606513, 'validation/accuracy': 0.18432505428791046, 'validation/loss': 5.840880393981934, 'validation/bleu': 0.3074781629464187, 'validation/num_examples': 3000, 'test/accuracy': 0.17370286583900452, 'test/loss': 6.085827350616455, 'test/bleu': 0.2084107339569262, 'test/num_examples': 3003, 'score': 1734.7267639636993, 'total_duration': 4368.907752037048, 'accumulated_submission_time': 1734.7267639636993, 'accumulated_eval_time': 2634.098353624344, 'accumulated_logging_time': 0.050847530364990234}
I0428 20:24:39.029242 140456645101312 logging_writer.py:48] [2339] accumulated_eval_time=2634.098354, accumulated_logging_time=0.050848, accumulated_submission_time=1734.726764, global_step=2339, preemption_count=0, score=1734.726764, test/accuracy=0.173703, test/bleu=0.208411, test/loss=6.085827, test/num_examples=3003, total_duration=4368.907752, train/accuracy=0.204326, train/bleu=0.585027, train/loss=5.637054, validation/accuracy=0.184325, validation/bleu=0.307478, validation/loss=5.840880, validation/num_examples=3000
I0428 20:25:23.571649 140456636708608 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1462562084197998, loss=6.174614429473877
I0428 20:26:35.423380 140456645101312 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.47166693210601807, loss=5.9564900398254395
I0428 20:27:47.286440 140456636708608 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.39848124980926514, loss=5.790520668029785
I0428 20:28:59.085131 140456645101312 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4947890341281891, loss=5.766923427581787
I0428 20:30:10.944792 140456636708608 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.34505730867385864, loss=5.6861138343811035
I0428 20:31:22.810892 140456645101312 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.36062827706336975, loss=5.657931804656982
I0428 20:32:34.605086 140456636708608 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.414919912815094, loss=6.644465446472168
I0428 20:33:46.468601 140456645101312 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.35634738206863403, loss=5.835621356964111
I0428 20:34:58.338668 140456636708608 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3809033930301666, loss=5.609719276428223
I0428 20:36:10.236478 140456645101312 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.38871803879737854, loss=5.568090915679932
I0428 20:37:22.164920 140456636708608 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.34150391817092896, loss=5.458846092224121
I0428 20:38:34.020449 140456645101312 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.101633220911026, loss=7.147574424743652
I0428 20:38:39.492179 140643883775808 spec.py:298] Evaluating on the training split.
I0428 20:38:42.513959 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 20:43:36.228172 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 20:43:38.900017 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 20:48:26.154383 140643883775808 spec.py:326] Evaluating on the test split.
I0428 20:48:28.909110 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 20:53:22.499083 140643883775808 submission_runner.py:415] Time since start: 6092.39s, 	Step: 3509, 	{'train/accuracy': 0.13001438975334167, 'train/loss': 6.4877824783325195, 'train/bleu': 0.04383961369026429, 'validation/accuracy': 0.12996739149093628, 'validation/loss': 6.525054454803467, 'validation/bleu': 0.024670444530500397, 'validation/num_examples': 3000, 'test/accuracy': 0.12137586623430252, 'test/loss': 6.740372657775879, 'test/bleu': 0.029134862193480035, 'test/num_examples': 3003, 'score': 2575.1640317440033, 'total_duration': 6092.388398170471, 'accumulated_submission_time': 2575.1640317440033, 'accumulated_eval_time': 3517.105197429657, 'accumulated_logging_time': 0.07258272171020508}
I0428 20:53:22.508751 140456636708608 logging_writer.py:48] [3509] accumulated_eval_time=3517.105197, accumulated_logging_time=0.072583, accumulated_submission_time=2575.164032, global_step=3509, preemption_count=0, score=2575.164032, test/accuracy=0.121376, test/bleu=0.029135, test/loss=6.740373, test/num_examples=3003, total_duration=6092.388398, train/accuracy=0.130014, train/bleu=0.043840, train/loss=6.487782, validation/accuracy=0.129967, validation/bleu=0.024670, validation/loss=6.525054, validation/num_examples=3000
I0428 20:54:28.638627 140456645101312 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.2016931027173996, loss=6.32011079788208
I0428 20:55:40.502311 140456636708608 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.18076090514659882, loss=5.87795352935791
I0428 20:56:52.387782 140456645101312 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.31650131940841675, loss=5.598959445953369
I0428 20:58:04.275462 140456636708608 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.285991907119751, loss=5.468756675720215
I0428 20:59:16.168831 140456645101312 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3033123016357422, loss=5.345986366271973
I0428 21:00:27.926364 140456636708608 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.060563743114471436, loss=7.226431369781494
I0428 21:01:39.770123 140456645101312 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.2485610395669937, loss=6.138521671295166
I0428 21:02:51.674408 140456636708608 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.24887120723724365, loss=5.390463352203369
I0428 21:04:03.582160 140456645101312 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.25054478645324707, loss=5.350231170654297
I0428 21:05:15.514559 140456636708608 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.32115116715431213, loss=5.2072882652282715
I0428 21:06:27.438007 140456645101312 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.22895048558712006, loss=5.010416507720947
I0428 21:07:22.529667 140643883775808 spec.py:298] Evaluating on the training split.
I0428 21:07:25.584248 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 21:12:09.386049 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 21:12:12.079403 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 21:16:57.977640 140643883775808 spec.py:326] Evaluating on the test split.
I0428 21:17:00.721619 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 21:21:45.384877 140643883775808 submission_runner.py:415] Time since start: 7795.27s, 	Step: 4678, 	{'train/accuracy': 0.32383227348327637, 'train/loss': 4.419957160949707, 'train/bleu': 4.921298744158865, 'validation/accuracy': 0.2954457998275757, 'validation/loss': 4.704185485839844, 'validation/bleu': 2.393651476391127, 'validation/num_examples': 3000, 'test/accuracy': 0.2755911946296692, 'test/loss': 4.970096588134766, 'test/bleu': 1.815797744167973, 'test/num_examples': 3003, 'score': 3415.1582798957825, 'total_duration': 7795.274191379547, 'accumulated_submission_time': 3415.1582798957825, 'accumulated_eval_time': 4379.960402965546, 'accumulated_logging_time': 0.09151172637939453}
I0428 21:21:45.395184 140456636708608 logging_writer.py:48] [4678] accumulated_eval_time=4379.960403, accumulated_logging_time=0.091512, accumulated_submission_time=3415.158280, global_step=4678, preemption_count=0, score=3415.158280, test/accuracy=0.275591, test/bleu=1.815798, test/loss=4.970097, test/num_examples=3003, total_duration=7795.274191, train/accuracy=0.323832, train/bleu=4.921299, train/loss=4.419957, validation/accuracy=0.295446, validation/bleu=2.393651, validation/loss=4.704185, validation/num_examples=3000
I0428 21:22:01.960264 140456645101312 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.22718214988708496, loss=5.0057692527771
I0428 21:23:13.883426 140456636708608 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.30387213826179504, loss=4.765517711639404
I0428 21:24:25.773294 140456645101312 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.2898431122303009, loss=4.535305023193359
I0428 21:25:37.667122 140456636708608 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.3895909786224365, loss=4.3663482666015625
I0428 21:26:49.575652 140456645101312 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.35398855805397034, loss=4.239352703094482
I0428 21:28:01.450474 140456636708608 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3620513081550598, loss=4.197362422943115
I0428 21:29:13.370221 140456645101312 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.31785258650779724, loss=4.071368217468262
I0428 21:30:25.300307 140456636708608 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.2986927628517151, loss=4.013220310211182
I0428 21:31:37.192119 140456645101312 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.3197977542877197, loss=4.010316371917725
I0428 21:32:49.046071 140456636708608 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.31759047508239746, loss=3.9276628494262695
I0428 21:34:00.885816 140456645101312 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.3157581686973572, loss=3.8526458740234375
I0428 21:35:12.776838 140456636708608 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2954975962638855, loss=3.789952039718628
I0428 21:35:45.529896 140643883775808 spec.py:298] Evaluating on the training split.
I0428 21:35:48.579412 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 21:38:48.692108 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 21:38:51.382257 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 21:41:43.442738 140643883775808 spec.py:326] Evaluating on the test split.
I0428 21:41:46.197321 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 21:44:44.726428 140643883775808 submission_runner.py:415] Time since start: 9174.62s, 	Step: 5847, 	{'train/accuracy': 0.530608057975769, 'train/loss': 2.774466037750244, 'train/bleu': 22.131336541307874, 'validation/accuracy': 0.5292060971260071, 'validation/loss': 2.7967660427093506, 'validation/bleu': 18.32744920109158, 'validation/num_examples': 3000, 'test/accuracy': 0.5263261795043945, 'test/loss': 2.8446686267852783, 'test/bleu': 17.05274472855493, 'test/num_examples': 3003, 'score': 4255.268531084061, 'total_duration': 9174.61575293541, 'accumulated_submission_time': 4255.268531084061, 'accumulated_eval_time': 4919.156898021698, 'accumulated_logging_time': 0.1113901138305664}
I0428 21:44:44.736962 140456645101312 logging_writer.py:48] [5847] accumulated_eval_time=4919.156898, accumulated_logging_time=0.111390, accumulated_submission_time=4255.268531, global_step=5847, preemption_count=0, score=4255.268531, test/accuracy=0.526326, test/bleu=17.052745, test/loss=2.844669, test/num_examples=3003, total_duration=9174.615753, train/accuracy=0.530608, train/bleu=22.131337, train/loss=2.774466, validation/accuracy=0.529206, validation/bleu=18.327449, validation/loss=2.796766, validation/num_examples=3000
I0428 21:45:23.535409 140456636708608 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.34528106451034546, loss=3.7649121284484863
I0428 21:46:35.401616 140456645101312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.28034406900405884, loss=3.79298734664917
I0428 21:47:47.318181 140456636708608 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.25813594460487366, loss=3.7638354301452637
I0428 21:48:59.163878 140456645101312 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.24375104904174805, loss=3.720238447189331
I0428 21:50:11.059745 140456636708608 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.26501962542533875, loss=3.605152130126953
I0428 21:51:22.948409 140456645101312 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.31642523407936096, loss=3.6619133949279785
I0428 21:52:34.741255 140456636708608 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.2887133061885834, loss=3.5791091918945312
I0428 21:53:46.574404 140456645101312 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.23505520820617676, loss=3.6330618858337402
I0428 21:54:58.456269 140456636708608 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.22780971229076385, loss=3.5883047580718994
I0428 21:56:10.371948 140456645101312 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.22933444380760193, loss=3.6239430904388428
I0428 21:57:22.258969 140456636708608 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.19303558766841888, loss=3.493333101272583
I0428 21:58:34.117713 140456645101312 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.19949030876159668, loss=3.592419147491455
I0428 21:58:45.325096 140643883775808 spec.py:298] Evaluating on the training split.
I0428 21:58:48.358324 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:01:32.737991 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 22:01:35.431884 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:04:19.608305 140643883775808 spec.py:326] Evaluating on the test split.
I0428 22:04:22.353954 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:06:51.128437 140643883775808 submission_runner.py:415] Time since start: 10501.02s, 	Step: 7017, 	{'train/accuracy': 0.5730332136154175, 'train/loss': 2.4161376953125, 'train/bleu': 26.034562007744945, 'validation/accuracy': 0.5734584927558899, 'validation/loss': 2.40120267868042, 'validation/bleu': 22.261013445326185, 'validation/num_examples': 3000, 'test/accuracy': 0.5743071436882019, 'test/loss': 2.4103305339813232, 'test/bleu': 20.82175819518803, 'test/num_examples': 3003, 'score': 5095.830968379974, 'total_duration': 10501.017756462097, 'accumulated_submission_time': 5095.830968379974, 'accumulated_eval_time': 5404.960187911987, 'accumulated_logging_time': 0.13261103630065918}
I0428 22:06:51.138758 140456636708608 logging_writer.py:48] [7017] accumulated_eval_time=5404.960188, accumulated_logging_time=0.132611, accumulated_submission_time=5095.830968, global_step=7017, preemption_count=0, score=5095.830968, test/accuracy=0.574307, test/bleu=20.821758, test/loss=2.410331, test/num_examples=3003, total_duration=10501.017756, train/accuracy=0.573033, train/bleu=26.034562, train/loss=2.416138, validation/accuracy=0.573458, validation/bleu=22.261013, validation/loss=2.401203, validation/num_examples=3000
I0428 22:07:51.463030 140456645101312 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.22665046155452728, loss=3.5196094512939453
I0428 22:09:03.348711 140456636708608 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.17706423997879028, loss=3.4651265144348145
I0428 22:10:15.166255 140456645101312 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.18231499195098877, loss=3.4732611179351807
I0428 22:11:27.078706 140456636708608 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.18523196876049042, loss=3.445793867111206
I0428 22:12:38.972691 140456645101312 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.20680776238441467, loss=3.4202494621276855
I0428 22:13:50.840116 140456636708608 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.18562103807926178, loss=3.4208905696868896
I0428 22:15:02.718211 140456645101312 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.19128631055355072, loss=3.4593708515167236
I0428 22:16:14.547198 140456636708608 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1889413446187973, loss=3.3693506717681885
I0428 22:17:26.422810 140456645101312 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.18928836286067963, loss=3.3963685035705566
I0428 22:18:38.271988 140456636708608 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.177582249045372, loss=3.4695992469787598
I0428 22:19:50.120980 140456645101312 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.16830100119113922, loss=3.358161449432373
I0428 22:20:51.566808 140643883775808 spec.py:298] Evaluating on the training split.
I0428 22:20:54.604654 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:24:12.578583 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 22:24:15.264143 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:26:48.925587 140643883775808 spec.py:326] Evaluating on the test split.
I0428 22:26:51.660523 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:29:22.956284 140643883775808 submission_runner.py:415] Time since start: 11852.85s, 	Step: 8187, 	{'train/accuracy': 0.5871766209602356, 'train/loss': 2.2699596881866455, 'train/bleu': 27.318422657173365, 'validation/accuracy': 0.5980086922645569, 'validation/loss': 2.205597162246704, 'validation/bleu': 23.711984487251975, 'validation/num_examples': 3000, 'test/accuracy': 0.602765679359436, 'test/loss': 2.187260627746582, 'test/bleu': 22.865459199393385, 'test/num_examples': 3003, 'score': 5936.234283208847, 'total_duration': 11852.845603704453, 'accumulated_submission_time': 5936.234283208847, 'accumulated_eval_time': 5916.349623203278, 'accumulated_logging_time': 0.15257620811462402}
I0428 22:29:22.966547 140456636708608 logging_writer.py:48] [8187] accumulated_eval_time=5916.349623, accumulated_logging_time=0.152576, accumulated_submission_time=5936.234283, global_step=8187, preemption_count=0, score=5936.234283, test/accuracy=0.602766, test/bleu=22.865459, test/loss=2.187261, test/num_examples=3003, total_duration=11852.845604, train/accuracy=0.587177, train/bleu=27.318423, train/loss=2.269960, validation/accuracy=0.598009, validation/bleu=23.711984, validation/loss=2.205597, validation/num_examples=3000
I0428 22:29:33.028379 140456645101312 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17390543222427368, loss=3.353856086730957
I0428 22:30:44.853456 140456636708608 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.17293250560760498, loss=3.2437679767608643
I0428 22:31:56.692291 140456645101312 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.16311901807785034, loss=3.320067882537842
I0428 22:33:08.567505 140456636708608 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.15866604447364807, loss=3.3515894412994385
I0428 22:34:20.430517 140456645101312 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16273821890354156, loss=3.387605905532837
I0428 22:35:32.309655 140456636708608 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.16130253672599792, loss=3.366976261138916
I0428 22:36:44.233041 140456645101312 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1629602462053299, loss=3.314542293548584
I0428 22:37:56.107714 140456636708608 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.16542452573776245, loss=3.345137119293213
I0428 22:39:07.977650 140456645101312 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1533174067735672, loss=3.3230385780334473
I0428 22:40:19.839361 140456636708608 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.15595752000808716, loss=3.3297457695007324
I0428 22:41:31.697380 140456645101312 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1546655148267746, loss=3.438070297241211
I0428 22:42:43.517879 140456636708608 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.1658833622932434, loss=3.300391912460327
I0428 22:43:23.495236 140643883775808 spec.py:298] Evaluating on the training split.
I0428 22:43:26.535001 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:45:59.345266 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 22:46:02.043690 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:48:22.955465 140643883775808 spec.py:326] Evaluating on the test split.
I0428 22:48:25.698639 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 22:50:33.545207 140643883775808 submission_runner.py:415] Time since start: 13123.43s, 	Step: 9357, 	{'train/accuracy': 0.5973216891288757, 'train/loss': 2.182178497314453, 'train/bleu': 28.24414170223544, 'validation/accuracy': 0.6108789443969727, 'validation/loss': 2.0823862552642822, 'validation/bleu': 24.578429557889933, 'validation/num_examples': 3000, 'test/accuracy': 0.6132009029388428, 'test/loss': 2.0585591793060303, 'test/bleu': 23.21798424184976, 'test/num_examples': 3003, 'score': 6776.737730264664, 'total_duration': 13123.434525728226, 'accumulated_submission_time': 6776.737730264664, 'accumulated_eval_time': 6346.399564981461, 'accumulated_logging_time': 0.1721796989440918}
I0428 22:50:33.556335 140456645101312 logging_writer.py:48] [9357] accumulated_eval_time=6346.399565, accumulated_logging_time=0.172180, accumulated_submission_time=6776.737730, global_step=9357, preemption_count=0, score=6776.737730, test/accuracy=0.613201, test/bleu=23.217984, test/loss=2.058559, test/num_examples=3003, total_duration=13123.434526, train/accuracy=0.597322, train/bleu=28.244142, train/loss=2.182178, validation/accuracy=0.610879, validation/bleu=24.578430, validation/loss=2.082386, validation/num_examples=3000
I0428 22:51:05.171507 140456636708608 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.14930987358093262, loss=3.24334979057312
I0428 22:52:16.979599 140456645101312 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.20319634675979614, loss=3.3399500846862793
I0428 22:53:28.791205 140456636708608 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.14563491940498352, loss=3.199702739715576
I0428 22:54:40.634160 140456645101312 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15157656371593475, loss=3.2409539222717285
I0428 22:55:52.431189 140456636708608 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.14159715175628662, loss=3.2649552822113037
I0428 22:57:04.261908 140456645101312 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.14532062411308289, loss=3.2057044506073
I0428 22:58:16.120239 140456636708608 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1662919521331787, loss=3.221676826477051
I0428 22:59:27.998050 140456645101312 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.14779573678970337, loss=3.148442029953003
I0428 23:00:39.807862 140456636708608 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.133684441447258, loss=3.1711080074310303
I0428 23:01:51.652812 140456645101312 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.14492589235305786, loss=3.1779491901397705
I0428 23:03:03.486494 140456636708608 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.1727125644683838, loss=3.0910606384277344
I0428 23:04:15.298248 140456645101312 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.13670900464057922, loss=3.2128865718841553
I0428 23:04:33.656766 140643883775808 spec.py:298] Evaluating on the training split.
I0428 23:04:36.687938 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:07:15.061805 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 23:07:17.744472 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:09:46.917124 140643883775808 spec.py:326] Evaluating on the test split.
I0428 23:09:49.678744 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:12:06.693205 140643883775808 submission_runner.py:415] Time since start: 14416.58s, 	Step: 10527, 	{'train/accuracy': 0.6041759848594666, 'train/loss': 2.123868703842163, 'train/bleu': 28.978027669332036, 'validation/accuracy': 0.6187648177146912, 'validation/loss': 2.0142781734466553, 'validation/bleu': 25.675784148893516, 'validation/num_examples': 3000, 'test/accuracy': 0.6254488825798035, 'test/loss': 1.9811090230941772, 'test/bleu': 24.350858838808197, 'test/num_examples': 3003, 'score': 7616.8122589588165, 'total_duration': 14416.58245396614, 'accumulated_submission_time': 7616.8122589588165, 'accumulated_eval_time': 6799.435875415802, 'accumulated_logging_time': 0.19383597373962402}
I0428 23:12:06.705752 140456636708608 logging_writer.py:48] [10527] accumulated_eval_time=6799.435875, accumulated_logging_time=0.193836, accumulated_submission_time=7616.812259, global_step=10527, preemption_count=0, score=7616.812259, test/accuracy=0.625449, test/bleu=24.350859, test/loss=1.981109, test/num_examples=3003, total_duration=14416.582454, train/accuracy=0.604176, train/bleu=28.978028, train/loss=2.123869, validation/accuracy=0.618765, validation/bleu=25.675784, validation/loss=2.014278, validation/num_examples=3000
I0428 23:12:59.802939 140456645101312 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.1516224443912506, loss=3.2162296772003174
I0428 23:14:11.641649 140456636708608 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.16692282259464264, loss=3.217123508453369
I0428 23:15:23.493465 140456645101312 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.13377068936824799, loss=3.2456743717193604
I0428 23:16:35.293790 140456636708608 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.15002422034740448, loss=3.256739377975464
I0428 23:17:47.142372 140456645101312 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.14008590579032898, loss=3.320253849029541
I0428 23:18:58.987857 140456636708608 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.13335378468036652, loss=3.157642126083374
I0428 23:20:10.809392 140456645101312 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.14332009851932526, loss=3.1965696811676025
I0428 23:21:22.663949 140456636708608 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.13480380177497864, loss=3.233452796936035
I0428 23:22:34.496280 140456645101312 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.13779786229133606, loss=3.1558516025543213
I0428 23:23:46.268274 140456636708608 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.15336205065250397, loss=3.239576816558838
I0428 23:24:58.089381 140456645101312 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.13463053107261658, loss=3.1946563720703125
I0428 23:26:06.746675 140643883775808 spec.py:298] Evaluating on the training split.
I0428 23:26:09.789149 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:28:50.417121 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 23:28:53.104872 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:31:27.622663 140643883775808 spec.py:326] Evaluating on the test split.
I0428 23:31:30.371165 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:33:54.835227 140643883775808 submission_runner.py:415] Time since start: 15724.72s, 	Step: 11697, 	{'train/accuracy': 0.6104806661605835, 'train/loss': 2.0786304473876953, 'train/bleu': 29.605930549852186, 'validation/accuracy': 0.630109965801239, 'validation/loss': 1.9410308599472046, 'validation/bleu': 26.083416460719096, 'validation/num_examples': 3000, 'test/accuracy': 0.6354192495346069, 'test/loss': 1.8938559293746948, 'test/bleu': 25.060220426713055, 'test/num_examples': 3003, 'score': 8456.82792520523, 'total_duration': 15724.724514722824, 'accumulated_submission_time': 8456.82792520523, 'accumulated_eval_time': 7267.524353027344, 'accumulated_logging_time': 0.21636080741882324}
I0428 23:33:54.845823 140456636708608 logging_writer.py:48] [11697] accumulated_eval_time=7267.524353, accumulated_logging_time=0.216361, accumulated_submission_time=8456.827925, global_step=11697, preemption_count=0, score=8456.827925, test/accuracy=0.635419, test/bleu=25.060220, test/loss=1.893856, test/num_examples=3003, total_duration=15724.724515, train/accuracy=0.610481, train/bleu=29.605931, train/loss=2.078630, validation/accuracy=0.630110, validation/bleu=26.083416, validation/loss=1.941031, validation/num_examples=3000
I0428 23:33:57.750345 140456645101312 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.15759553015232086, loss=3.1491692066192627
I0428 23:35:09.523679 140456636708608 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.14045368134975433, loss=3.2166242599487305
I0428 23:36:21.348276 140456645101312 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.13347281515598297, loss=3.212888479232788
I0428 23:37:33.179901 140456636708608 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.14594976603984833, loss=3.233126163482666
I0428 23:38:44.964859 140456645101312 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.1308646947145462, loss=3.1512489318847656
I0428 23:39:56.775909 140456636708608 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1413082480430603, loss=3.075259208679199
I0428 23:41:08.614538 140456645101312 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.14591197669506073, loss=3.221202850341797
I0428 23:42:20.433512 140456636708608 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.15766838192939758, loss=3.181046962738037
I0428 23:43:32.255616 140456645101312 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1566694676876068, loss=3.123626470565796
I0428 23:44:44.110061 140456636708608 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.1270279735326767, loss=3.1524579524993896
I0428 23:45:55.913501 140456645101312 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1218017041683197, loss=3.1015727519989014
I0428 23:47:07.774283 140456636708608 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1430715173482895, loss=3.214726448059082
I0428 23:47:55.565735 140643883775808 spec.py:298] Evaluating on the training split.
I0428 23:47:58.614006 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:50:42.427484 140643883775808 spec.py:310] Evaluating on the validation split.
I0428 23:50:45.119068 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:53:09.179260 140643883775808 spec.py:326] Evaluating on the test split.
I0428 23:53:11.924135 140643883775808 workload.py:179] Translating evaluation dataset.
I0428 23:55:26.550238 140643883775808 submission_runner.py:415] Time since start: 17016.44s, 	Step: 12868, 	{'train/accuracy': 0.6306341290473938, 'train/loss': 1.9240976572036743, 'train/bleu': 30.710316298939198, 'validation/accuracy': 0.6346976161003113, 'validation/loss': 1.885799765586853, 'validation/bleu': 26.13946000269349, 'validation/num_examples': 3000, 'test/accuracy': 0.6405438780784607, 'test/loss': 1.8463101387023926, 'test/bleu': 25.452994887740306, 'test/num_examples': 3003, 'score': 9297.521749258041, 'total_duration': 17016.439516305923, 'accumulated_submission_time': 9297.521749258041, 'accumulated_eval_time': 7718.508774280548, 'accumulated_logging_time': 0.23734831809997559}
I0428 23:55:26.561200 140456645101312 logging_writer.py:48] [12868] accumulated_eval_time=7718.508774, accumulated_logging_time=0.237348, accumulated_submission_time=9297.521749, global_step=12868, preemption_count=0, score=9297.521749, test/accuracy=0.640544, test/bleu=25.452995, test/loss=1.846310, test/num_examples=3003, total_duration=17016.439516, train/accuracy=0.630634, train/bleu=30.710316, train/loss=1.924098, validation/accuracy=0.634698, validation/bleu=26.139460, validation/loss=1.885800, validation/num_examples=3000
I0428 23:55:50.246902 140456636708608 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.14162009954452515, loss=3.0638928413391113
I0428 23:57:02.050032 140456645101312 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.15777407586574554, loss=3.1633694171905518
I0428 23:58:13.860091 140456636708608 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1343851238489151, loss=3.124708890914917
I0428 23:59:25.635736 140456645101312 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.1477666050195694, loss=3.132161855697632
I0429 00:00:37.448494 140456636708608 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.12887945771217346, loss=3.0956008434295654
I0429 00:01:49.242750 140456645101312 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.14862015843391418, loss=3.0941073894500732
I0429 00:03:01.099863 140456636708608 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.14658869802951813, loss=3.1553452014923096
I0429 00:04:12.867942 140456645101312 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.1357249766588211, loss=3.024482488632202
I0429 00:05:24.640154 140456636708608 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.12974905967712402, loss=3.042203664779663
I0429 00:06:36.394957 140456645101312 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.14610129594802856, loss=3.051983594894409
I0429 00:07:48.202600 140456636708608 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.134670689702034, loss=3.1087422370910645
I0429 00:09:00.004741 140456645101312 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1361788660287857, loss=3.117021083831787
I0429 00:09:26.980158 140643883775808 spec.py:298] Evaluating on the training split.
I0429 00:09:30.011628 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:12:19.154555 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 00:12:21.846915 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:14:59.948158 140643883775808 spec.py:326] Evaluating on the test split.
I0429 00:15:02.698397 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:17:32.182275 140643883775808 submission_runner.py:415] Time since start: 18342.07s, 	Step: 14039, 	{'train/accuracy': 0.6242278218269348, 'train/loss': 1.9575997591018677, 'train/bleu': 30.079716259351304, 'validation/accuracy': 0.6397441029548645, 'validation/loss': 1.8536895513534546, 'validation/bleu': 26.987362958607704, 'validation/num_examples': 3000, 'test/accuracy': 0.6463192105293274, 'test/loss': 1.8076554536819458, 'test/bleu': 25.938665171086434, 'test/num_examples': 3003, 'score': 10137.91382932663, 'total_duration': 18342.0715610981, 'accumulated_submission_time': 10137.91382932663, 'accumulated_eval_time': 8203.710798740387, 'accumulated_logging_time': 0.2599155902862549}
I0429 00:17:32.194408 140456636708608 logging_writer.py:48] [14039] accumulated_eval_time=8203.710799, accumulated_logging_time=0.259916, accumulated_submission_time=10137.913829, global_step=14039, preemption_count=0, score=10137.913829, test/accuracy=0.646319, test/bleu=25.938665, test/loss=1.807655, test/num_examples=3003, total_duration=18342.071561, train/accuracy=0.624228, train/bleu=30.079716, train/loss=1.957600, validation/accuracy=0.639744, validation/bleu=26.987363, validation/loss=1.853690, validation/num_examples=3000
I0429 00:18:16.723780 140456645101312 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.13587230443954468, loss=3.117586612701416
I0429 00:19:28.446931 140456636708608 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.12440437078475952, loss=3.0215744972229004
I0429 00:20:40.265952 140456645101312 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.13427191972732544, loss=3.0904746055603027
I0429 00:21:52.145278 140456636708608 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.1477639377117157, loss=3.088900327682495
I0429 00:23:03.938392 140456645101312 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.13070723414421082, loss=3.1333987712860107
I0429 00:24:15.745250 140456636708608 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.15101470053195953, loss=3.1149773597717285
I0429 00:25:27.539899 140456645101312 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.14805983006954193, loss=3.0808253288269043
I0429 00:26:39.347637 140456636708608 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.1413397490978241, loss=3.046546697616577
I0429 00:27:51.172870 140456645101312 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.15352386236190796, loss=3.121675491333008
I0429 00:29:02.991909 140456636708608 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.13448598980903625, loss=3.117245674133301
I0429 00:30:14.760060 140456645101312 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1463841050863266, loss=3.0464389324188232
I0429 00:31:26.543570 140456636708608 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.13905301690101624, loss=3.0739691257476807
I0429 00:31:32.717349 140643883775808 spec.py:298] Evaluating on the training split.
I0429 00:31:35.761572 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:34:46.285096 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 00:34:48.983762 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:37:24.453006 140643883775808 spec.py:326] Evaluating on the test split.
I0429 00:37:27.191367 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:39:46.774111 140643883775808 submission_runner.py:415] Time since start: 19676.66s, 	Step: 15210, 	{'train/accuracy': 0.624638557434082, 'train/loss': 1.9574640989303589, 'train/bleu': 30.246807577490635, 'validation/accuracy': 0.6439225673675537, 'validation/loss': 1.8132266998291016, 'validation/bleu': 27.11621464670067, 'validation/num_examples': 3000, 'test/accuracy': 0.6505142450332642, 'test/loss': 1.7696057558059692, 'test/bleu': 26.130882741019448, 'test/num_examples': 3003, 'score': 10978.411244869232, 'total_duration': 19676.663434028625, 'accumulated_submission_time': 10978.411244869232, 'accumulated_eval_time': 8697.76753950119, 'accumulated_logging_time': 0.2823498249053955}
I0429 00:39:46.785860 140456645101312 logging_writer.py:48] [15210] accumulated_eval_time=8697.767540, accumulated_logging_time=0.282350, accumulated_submission_time=10978.411245, global_step=15210, preemption_count=0, score=10978.411245, test/accuracy=0.650514, test/bleu=26.130883, test/loss=1.769606, test/num_examples=3003, total_duration=19676.663434, train/accuracy=0.624639, train/bleu=30.246808, train/loss=1.957464, validation/accuracy=0.643923, validation/bleu=27.116215, validation/loss=1.813227, validation/num_examples=3000
I0429 00:40:52.170869 140456636708608 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.12943843007087708, loss=3.1961381435394287
I0429 00:42:03.979607 140456645101312 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1452135294675827, loss=3.0562803745269775
I0429 00:43:15.795544 140456636708608 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.13853424787521362, loss=3.135673999786377
I0429 00:44:27.616929 140456645101312 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.14222341775894165, loss=3.1020243167877197
I0429 00:45:39.451525 140456636708608 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1343982219696045, loss=3.0582380294799805
I0429 00:46:51.322445 140456645101312 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.1312471330165863, loss=2.9962050914764404
I0429 00:48:03.203157 140456636708608 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.15418292582035065, loss=2.983247995376587
I0429 00:49:15.042088 140456645101312 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.14373791217803955, loss=3.0748183727264404
I0429 00:50:26.908210 140456636708608 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.14324434101581573, loss=3.031799554824829
I0429 00:51:38.687760 140456645101312 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.1538085788488388, loss=3.028409481048584
I0429 00:52:50.526571 140456636708608 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1406668722629547, loss=3.0441231727600098
I0429 00:53:46.957500 140643883775808 spec.py:298] Evaluating on the training split.
I0429 00:53:50.004188 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:56:52.629306 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 00:56:55.318148 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 00:59:21.574479 140643883775808 spec.py:326] Evaluating on the test split.
I0429 00:59:24.318656 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:01:37.672330 140643883775808 submission_runner.py:415] Time since start: 20987.56s, 	Step: 16380, 	{'train/accuracy': 0.6308005452156067, 'train/loss': 1.9093120098114014, 'train/bleu': 30.529855121704703, 'validation/accuracy': 0.6481258869171143, 'validation/loss': 1.7881758213043213, 'validation/bleu': 27.39600455406402, 'validation/num_examples': 3000, 'test/accuracy': 0.6552437543869019, 'test/loss': 1.7372044324874878, 'test/bleu': 26.552643151908544, 'test/num_examples': 3003, 'score': 11818.558362483978, 'total_duration': 20987.56160712242, 'accumulated_submission_time': 11818.558362483978, 'accumulated_eval_time': 9168.48228597641, 'accumulated_logging_time': 0.3037679195404053}
I0429 01:01:37.684011 140456645101312 logging_writer.py:48] [16380] accumulated_eval_time=9168.482286, accumulated_logging_time=0.303768, accumulated_submission_time=11818.558362, global_step=16380, preemption_count=0, score=11818.558362, test/accuracy=0.655244, test/bleu=26.552643, test/loss=1.737204, test/num_examples=3003, total_duration=20987.561607, train/accuracy=0.630801, train/bleu=30.529855, train/loss=1.909312, validation/accuracy=0.648126, validation/bleu=27.396005, validation/loss=1.788176, validation/num_examples=3000
I0429 01:01:52.773353 140456636708608 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.12962323427200317, loss=2.9694736003875732
I0429 01:03:04.563917 140456645101312 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.14271879196166992, loss=3.051149606704712
I0429 01:04:16.353483 140456636708608 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.1342780888080597, loss=3.010043144226074
I0429 01:05:28.135389 140456645101312 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1482381969690323, loss=3.0299880504608154
I0429 01:06:39.940479 140456636708608 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1436433047056198, loss=3.0773472785949707
I0429 01:07:51.762621 140456645101312 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.1493496149778366, loss=3.034238815307617
I0429 01:09:03.592842 140456636708608 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.14742082357406616, loss=3.129526376724243
I0429 01:10:15.360108 140456645101312 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.14795023202896118, loss=3.1210947036743164
I0429 01:11:27.138321 140456636708608 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.1388401836156845, loss=3.061692953109741
I0429 01:12:38.947983 140456645101312 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.1402352899312973, loss=2.981473684310913
I0429 01:13:50.773843 140456636708608 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.1444753259420395, loss=2.971251964569092
I0429 01:15:02.611013 140456645101312 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.14977547526359558, loss=3.067471504211426
I0429 01:15:38.219164 140643883775808 spec.py:298] Evaluating on the training split.
I0429 01:15:41.262611 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:18:23.040332 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 01:18:25.737926 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:20:56.913422 140643883775808 spec.py:326] Evaluating on the test split.
I0429 01:20:59.659370 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:23:17.916715 140643883775808 submission_runner.py:415] Time since start: 22287.81s, 	Step: 17551, 	{'train/accuracy': 0.6301600933074951, 'train/loss': 1.9200448989868164, 'train/bleu': 31.061795285482606, 'validation/accuracy': 0.6505188941955566, 'validation/loss': 1.762885570526123, 'validation/bleu': 27.94500265665816, 'validation/num_examples': 3000, 'test/accuracy': 0.6603800058364868, 'test/loss': 1.703776478767395, 'test/bleu': 27.287384371374426, 'test/num_examples': 3003, 'score': 12659.06883263588, 'total_duration': 22287.806030750275, 'accumulated_submission_time': 12659.06883263588, 'accumulated_eval_time': 9628.179788351059, 'accumulated_logging_time': 0.32514238357543945}
I0429 01:23:17.928293 140456636708608 logging_writer.py:48] [17551] accumulated_eval_time=9628.179788, accumulated_logging_time=0.325142, accumulated_submission_time=12659.068833, global_step=17551, preemption_count=0, score=12659.068833, test/accuracy=0.660380, test/bleu=27.287384, test/loss=1.703776, test/num_examples=3003, total_duration=22287.806031, train/accuracy=0.630160, train/bleu=31.061795, train/loss=1.920045, validation/accuracy=0.650519, validation/bleu=27.945003, validation/loss=1.762886, validation/num_examples=3000
I0429 01:23:53.780119 140456645101312 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.15436434745788574, loss=3.105703830718994
I0429 01:25:05.581563 140456636708608 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.13922326266765594, loss=3.090833902359009
I0429 01:26:17.408155 140456645101312 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.14981551468372345, loss=3.020298957824707
I0429 01:27:29.254015 140456636708608 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.13471566140651703, loss=3.027724266052246
I0429 01:28:41.064514 140456645101312 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.13785529136657715, loss=3.025679588317871
I0429 01:29:52.845372 140456636708608 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.15708304941654205, loss=3.0330159664154053
I0429 01:31:04.637481 140456645101312 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1490006446838379, loss=3.0159215927124023
I0429 01:32:16.469238 140456636708608 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.14069180190563202, loss=2.968433141708374
I0429 01:33:28.266161 140456645101312 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.1592695713043213, loss=2.9835753440856934
I0429 01:34:40.078258 140456636708608 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.14739808440208435, loss=2.941793918609619
I0429 01:35:51.858623 140456645101312 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.15410403907299042, loss=3.0580010414123535
I0429 01:37:03.654978 140456636708608 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.14454029500484467, loss=2.9923408031463623
I0429 01:37:18.422146 140643883775808 spec.py:298] Evaluating on the training split.
I0429 01:37:21.458970 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:40:06.390613 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 01:40:09.083142 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:42:47.607114 140643883775808 spec.py:326] Evaluating on the test split.
I0429 01:42:50.348488 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 01:45:18.307710 140643883775808 submission_runner.py:415] Time since start: 23608.20s, 	Step: 18722, 	{'train/accuracy': 0.6324957013130188, 'train/loss': 1.8982248306274414, 'train/bleu': 30.788149248097092, 'validation/accuracy': 0.6532590985298157, 'validation/loss': 1.7472504377365112, 'validation/bleu': 27.77962535184389, 'validation/num_examples': 3000, 'test/accuracy': 0.6630991697311401, 'test/loss': 1.6940419673919678, 'test/bleu': 27.17777435520038, 'test/num_examples': 3003, 'score': 13499.536531209946, 'total_duration': 23608.196998119354, 'accumulated_submission_time': 13499.536531209946, 'accumulated_eval_time': 10108.065262794495, 'accumulated_logging_time': 0.3457338809967041}
I0429 01:45:18.319435 140456645101312 logging_writer.py:48] [18722] accumulated_eval_time=10108.065263, accumulated_logging_time=0.345734, accumulated_submission_time=13499.536531, global_step=18722, preemption_count=0, score=13499.536531, test/accuracy=0.663099, test/bleu=27.177774, test/loss=1.694042, test/num_examples=3003, total_duration=23608.196998, train/accuracy=0.632496, train/bleu=30.788149, train/loss=1.898225, validation/accuracy=0.653259, validation/bleu=27.779625, validation/loss=1.747250, validation/num_examples=3000
I0429 01:46:14.972682 140456636708608 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.15303567051887512, loss=3.0922088623046875
I0429 01:47:26.717704 140456645101312 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.1342536211013794, loss=3.0020227432250977
I0429 01:48:38.498362 140456636708608 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.14623087644577026, loss=2.8947317600250244
I0429 01:49:50.288012 140456645101312 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.1652945578098297, loss=2.9697422981262207
I0429 01:51:02.059525 140456636708608 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.14256109297275543, loss=2.924717664718628
I0429 01:52:13.834724 140456645101312 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.1534738540649414, loss=2.951233148574829
I0429 01:53:25.650797 140456636708608 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.14358210563659668, loss=2.999868869781494
I0429 01:54:37.379384 140456645101312 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.15304729342460632, loss=3.010619640350342
I0429 01:55:49.159885 140456636708608 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.15125881135463715, loss=3.066009283065796
I0429 01:57:00.974457 140456645101312 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.14877499639987946, loss=3.00116229057312
I0429 01:58:12.823800 140456636708608 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.1605466902256012, loss=3.0199623107910156
I0429 01:59:18.525171 140643883775808 spec.py:298] Evaluating on the training split.
I0429 01:59:21.577072 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 02:01:59.275079 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 02:02:01.958652 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 02:04:24.735571 140643883775808 spec.py:326] Evaluating on the test split.
I0429 02:04:27.480391 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 02:06:41.113199 140643883775808 submission_runner.py:415] Time since start: 24891.00s, 	Step: 19893, 	{'train/accuracy': 0.6428802013397217, 'train/loss': 1.8101420402526855, 'train/bleu': 31.652890632454614, 'validation/accuracy': 0.6564704775810242, 'validation/loss': 1.729822039604187, 'validation/bleu': 28.23211242602608, 'validation/num_examples': 3000, 'test/accuracy': 0.6648422479629517, 'test/loss': 1.6675126552581787, 'test/bleu': 27.482161826855798, 'test/num_examples': 3003, 'score': 14339.718233585358, 'total_duration': 24891.002490520477, 'accumulated_submission_time': 14339.718233585358, 'accumulated_eval_time': 10550.653219938278, 'accumulated_logging_time': 0.3667581081390381}
I0429 02:06:41.126799 140456645101312 logging_writer.py:48] [19893] accumulated_eval_time=10550.653220, accumulated_logging_time=0.366758, accumulated_submission_time=14339.718234, global_step=19893, preemption_count=0, score=14339.718234, test/accuracy=0.664842, test/bleu=27.482162, test/loss=1.667513, test/num_examples=3003, total_duration=24891.002491, train/accuracy=0.642880, train/bleu=31.652891, train/loss=1.810142, validation/accuracy=0.656470, validation/bleu=28.232112, validation/loss=1.729822, validation/num_examples=3000
I0429 02:06:46.881747 140456636708608 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.1457490175962448, loss=3.072923183441162
I0429 02:07:57.661989 140643883775808 spec.py:298] Evaluating on the training split.
I0429 02:08:00.711408 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 02:10:41.806109 140643883775808 spec.py:310] Evaluating on the validation split.
I0429 02:10:44.494386 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 02:13:09.761091 140643883775808 spec.py:326] Evaluating on the test split.
I0429 02:13:12.502337 140643883775808 workload.py:179] Translating evaluation dataset.
I0429 02:15:36.060401 140643883775808 submission_runner.py:415] Time since start: 25425.95s, 	Step: 20000, 	{'train/accuracy': 0.6396594047546387, 'train/loss': 1.8334894180297852, 'train/bleu': 31.42482306685711, 'validation/accuracy': 0.6545237898826599, 'validation/loss': 1.736132025718689, 'validation/bleu': 28.09382161119431, 'validation/num_examples': 3000, 'test/accuracy': 0.6620301008224487, 'test/loss': 1.6754614114761353, 'test/bleu': 27.09508705862738, 'test/num_examples': 3003, 'score': 14416.241226673126, 'total_duration': 25425.949693202972, 'accumulated_submission_time': 14416.241226673126, 'accumulated_eval_time': 11009.051575422287, 'accumulated_logging_time': 0.3904757499694824}
I0429 02:15:36.072861 140456645101312 logging_writer.py:48] [20000] accumulated_eval_time=11009.051575, accumulated_logging_time=0.390476, accumulated_submission_time=14416.241227, global_step=20000, preemption_count=0, score=14416.241227, test/accuracy=0.662030, test/bleu=27.095087, test/loss=1.675461, test/num_examples=3003, total_duration=25425.949693, train/accuracy=0.639659, train/bleu=31.424823, train/loss=1.833489, validation/accuracy=0.654524, validation/bleu=28.093822, validation/loss=1.736132, validation/num_examples=3000
I0429 02:15:36.090606 140456636708608 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=14416.241227
I0429 02:15:37.142819 140643883775808 checkpoints.py:356] Saving checkpoint at step: 20000
I0429 02:15:41.080111 140643883775808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1/checkpoint_20000
I0429 02:15:41.084553 140643883775808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_sam/wmt_jax/trial_1/checkpoint_20000.
I0429 02:15:41.154343 140643883775808 submission_runner.py:578] Tuning trial 1/1
I0429 02:15:41.154536 140643883775808 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0429 02:15:41.157081 140643883775808 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006262025563046336, 'train/loss': 11.08775520324707, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.09957218170166, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.077507972717285, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 54.026357889175415, 'total_duration': 988.907639503479, 'accumulated_submission_time': 54.026357889175415, 'accumulated_eval_time': 934.8810908794403, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1170, {'train/accuracy': 0.30098676681518555, 'train/loss': 5.1518683433532715, 'train/bleu': 6.874567742672105, 'validation/accuracy': 0.28159600496292114, 'validation/loss': 5.376704216003418, 'validation/bleu': 3.749163831479815, 'validation/num_examples': 3000, 'test/accuracy': 0.26305270195007324, 'test/loss': 5.645264625549316, 'test/bleu': 2.5738034559259413, 'test/num_examples': 3003, 'score': 894.5841176509857, 'total_duration': 2705.8606135845184, 'accumulated_submission_time': 894.5841176509857, 'accumulated_eval_time': 1811.232308626175, 'accumulated_logging_time': 0.02819991111755371, 'global_step': 1170, 'preemption_count': 0}), (2339, {'train/accuracy': 0.20432625710964203, 'train/loss': 5.637054443359375, 'train/bleu': 0.5850271742606513, 'validation/accuracy': 0.18432505428791046, 'validation/loss': 5.840880393981934, 'validation/bleu': 0.3074781629464187, 'validation/num_examples': 3000, 'test/accuracy': 0.17370286583900452, 'test/loss': 6.085827350616455, 'test/bleu': 0.2084107339569262, 'test/num_examples': 3003, 'score': 1734.7267639636993, 'total_duration': 4368.907752037048, 'accumulated_submission_time': 1734.7267639636993, 'accumulated_eval_time': 2634.098353624344, 'accumulated_logging_time': 0.050847530364990234, 'global_step': 2339, 'preemption_count': 0}), (3509, {'train/accuracy': 0.13001438975334167, 'train/loss': 6.4877824783325195, 'train/bleu': 0.04383961369026429, 'validation/accuracy': 0.12996739149093628, 'validation/loss': 6.525054454803467, 'validation/bleu': 0.024670444530500397, 'validation/num_examples': 3000, 'test/accuracy': 0.12137586623430252, 'test/loss': 6.740372657775879, 'test/bleu': 0.029134862193480035, 'test/num_examples': 3003, 'score': 2575.1640317440033, 'total_duration': 6092.388398170471, 'accumulated_submission_time': 2575.1640317440033, 'accumulated_eval_time': 3517.105197429657, 'accumulated_logging_time': 0.07258272171020508, 'global_step': 3509, 'preemption_count': 0}), (4678, {'train/accuracy': 0.32383227348327637, 'train/loss': 4.419957160949707, 'train/bleu': 4.921298744158865, 'validation/accuracy': 0.2954457998275757, 'validation/loss': 4.704185485839844, 'validation/bleu': 2.393651476391127, 'validation/num_examples': 3000, 'test/accuracy': 0.2755911946296692, 'test/loss': 4.970096588134766, 'test/bleu': 1.815797744167973, 'test/num_examples': 3003, 'score': 3415.1582798957825, 'total_duration': 7795.274191379547, 'accumulated_submission_time': 3415.1582798957825, 'accumulated_eval_time': 4379.960402965546, 'accumulated_logging_time': 0.09151172637939453, 'global_step': 4678, 'preemption_count': 0}), (5847, {'train/accuracy': 0.530608057975769, 'train/loss': 2.774466037750244, 'train/bleu': 22.131336541307874, 'validation/accuracy': 0.5292060971260071, 'validation/loss': 2.7967660427093506, 'validation/bleu': 18.32744920109158, 'validation/num_examples': 3000, 'test/accuracy': 0.5263261795043945, 'test/loss': 2.8446686267852783, 'test/bleu': 17.05274472855493, 'test/num_examples': 3003, 'score': 4255.268531084061, 'total_duration': 9174.61575293541, 'accumulated_submission_time': 4255.268531084061, 'accumulated_eval_time': 4919.156898021698, 'accumulated_logging_time': 0.1113901138305664, 'global_step': 5847, 'preemption_count': 0}), (7017, {'train/accuracy': 0.5730332136154175, 'train/loss': 2.4161376953125, 'train/bleu': 26.034562007744945, 'validation/accuracy': 0.5734584927558899, 'validation/loss': 2.40120267868042, 'validation/bleu': 22.261013445326185, 'validation/num_examples': 3000, 'test/accuracy': 0.5743071436882019, 'test/loss': 2.4103305339813232, 'test/bleu': 20.82175819518803, 'test/num_examples': 3003, 'score': 5095.830968379974, 'total_duration': 10501.017756462097, 'accumulated_submission_time': 5095.830968379974, 'accumulated_eval_time': 5404.960187911987, 'accumulated_logging_time': 0.13261103630065918, 'global_step': 7017, 'preemption_count': 0}), (8187, {'train/accuracy': 0.5871766209602356, 'train/loss': 2.2699596881866455, 'train/bleu': 27.318422657173365, 'validation/accuracy': 0.5980086922645569, 'validation/loss': 2.205597162246704, 'validation/bleu': 23.711984487251975, 'validation/num_examples': 3000, 'test/accuracy': 0.602765679359436, 'test/loss': 2.187260627746582, 'test/bleu': 22.865459199393385, 'test/num_examples': 3003, 'score': 5936.234283208847, 'total_duration': 11852.845603704453, 'accumulated_submission_time': 5936.234283208847, 'accumulated_eval_time': 5916.349623203278, 'accumulated_logging_time': 0.15257620811462402, 'global_step': 8187, 'preemption_count': 0}), (9357, {'train/accuracy': 0.5973216891288757, 'train/loss': 2.182178497314453, 'train/bleu': 28.24414170223544, 'validation/accuracy': 0.6108789443969727, 'validation/loss': 2.0823862552642822, 'validation/bleu': 24.578429557889933, 'validation/num_examples': 3000, 'test/accuracy': 0.6132009029388428, 'test/loss': 2.0585591793060303, 'test/bleu': 23.21798424184976, 'test/num_examples': 3003, 'score': 6776.737730264664, 'total_duration': 13123.434525728226, 'accumulated_submission_time': 6776.737730264664, 'accumulated_eval_time': 6346.399564981461, 'accumulated_logging_time': 0.1721796989440918, 'global_step': 9357, 'preemption_count': 0}), (10527, {'train/accuracy': 0.6041759848594666, 'train/loss': 2.123868703842163, 'train/bleu': 28.978027669332036, 'validation/accuracy': 0.6187648177146912, 'validation/loss': 2.0142781734466553, 'validation/bleu': 25.675784148893516, 'validation/num_examples': 3000, 'test/accuracy': 0.6254488825798035, 'test/loss': 1.9811090230941772, 'test/bleu': 24.350858838808197, 'test/num_examples': 3003, 'score': 7616.8122589588165, 'total_duration': 14416.58245396614, 'accumulated_submission_time': 7616.8122589588165, 'accumulated_eval_time': 6799.435875415802, 'accumulated_logging_time': 0.19383597373962402, 'global_step': 10527, 'preemption_count': 0}), (11697, {'train/accuracy': 0.6104806661605835, 'train/loss': 2.0786304473876953, 'train/bleu': 29.605930549852186, 'validation/accuracy': 0.630109965801239, 'validation/loss': 1.9410308599472046, 'validation/bleu': 26.083416460719096, 'validation/num_examples': 3000, 'test/accuracy': 0.6354192495346069, 'test/loss': 1.8938559293746948, 'test/bleu': 25.060220426713055, 'test/num_examples': 3003, 'score': 8456.82792520523, 'total_duration': 15724.724514722824, 'accumulated_submission_time': 8456.82792520523, 'accumulated_eval_time': 7267.524353027344, 'accumulated_logging_time': 0.21636080741882324, 'global_step': 11697, 'preemption_count': 0}), (12868, {'train/accuracy': 0.6306341290473938, 'train/loss': 1.9240976572036743, 'train/bleu': 30.710316298939198, 'validation/accuracy': 0.6346976161003113, 'validation/loss': 1.885799765586853, 'validation/bleu': 26.13946000269349, 'validation/num_examples': 3000, 'test/accuracy': 0.6405438780784607, 'test/loss': 1.8463101387023926, 'test/bleu': 25.452994887740306, 'test/num_examples': 3003, 'score': 9297.521749258041, 'total_duration': 17016.439516305923, 'accumulated_submission_time': 9297.521749258041, 'accumulated_eval_time': 7718.508774280548, 'accumulated_logging_time': 0.23734831809997559, 'global_step': 12868, 'preemption_count': 0}), (14039, {'train/accuracy': 0.6242278218269348, 'train/loss': 1.9575997591018677, 'train/bleu': 30.079716259351304, 'validation/accuracy': 0.6397441029548645, 'validation/loss': 1.8536895513534546, 'validation/bleu': 26.987362958607704, 'validation/num_examples': 3000, 'test/accuracy': 0.6463192105293274, 'test/loss': 1.8076554536819458, 'test/bleu': 25.938665171086434, 'test/num_examples': 3003, 'score': 10137.91382932663, 'total_duration': 18342.0715610981, 'accumulated_submission_time': 10137.91382932663, 'accumulated_eval_time': 8203.710798740387, 'accumulated_logging_time': 0.2599155902862549, 'global_step': 14039, 'preemption_count': 0}), (15210, {'train/accuracy': 0.624638557434082, 'train/loss': 1.9574640989303589, 'train/bleu': 30.246807577490635, 'validation/accuracy': 0.6439225673675537, 'validation/loss': 1.8132266998291016, 'validation/bleu': 27.11621464670067, 'validation/num_examples': 3000, 'test/accuracy': 0.6505142450332642, 'test/loss': 1.7696057558059692, 'test/bleu': 26.130882741019448, 'test/num_examples': 3003, 'score': 10978.411244869232, 'total_duration': 19676.663434028625, 'accumulated_submission_time': 10978.411244869232, 'accumulated_eval_time': 8697.76753950119, 'accumulated_logging_time': 0.2823498249053955, 'global_step': 15210, 'preemption_count': 0}), (16380, {'train/accuracy': 0.6308005452156067, 'train/loss': 1.9093120098114014, 'train/bleu': 30.529855121704703, 'validation/accuracy': 0.6481258869171143, 'validation/loss': 1.7881758213043213, 'validation/bleu': 27.39600455406402, 'validation/num_examples': 3000, 'test/accuracy': 0.6552437543869019, 'test/loss': 1.7372044324874878, 'test/bleu': 26.552643151908544, 'test/num_examples': 3003, 'score': 11818.558362483978, 'total_duration': 20987.56160712242, 'accumulated_submission_time': 11818.558362483978, 'accumulated_eval_time': 9168.48228597641, 'accumulated_logging_time': 0.3037679195404053, 'global_step': 16380, 'preemption_count': 0}), (17551, {'train/accuracy': 0.6301600933074951, 'train/loss': 1.9200448989868164, 'train/bleu': 31.061795285482606, 'validation/accuracy': 0.6505188941955566, 'validation/loss': 1.762885570526123, 'validation/bleu': 27.94500265665816, 'validation/num_examples': 3000, 'test/accuracy': 0.6603800058364868, 'test/loss': 1.703776478767395, 'test/bleu': 27.287384371374426, 'test/num_examples': 3003, 'score': 12659.06883263588, 'total_duration': 22287.806030750275, 'accumulated_submission_time': 12659.06883263588, 'accumulated_eval_time': 9628.179788351059, 'accumulated_logging_time': 0.32514238357543945, 'global_step': 17551, 'preemption_count': 0}), (18722, {'train/accuracy': 0.6324957013130188, 'train/loss': 1.8982248306274414, 'train/bleu': 30.788149248097092, 'validation/accuracy': 0.6532590985298157, 'validation/loss': 1.7472504377365112, 'validation/bleu': 27.77962535184389, 'validation/num_examples': 3000, 'test/accuracy': 0.6630991697311401, 'test/loss': 1.6940419673919678, 'test/bleu': 27.17777435520038, 'test/num_examples': 3003, 'score': 13499.536531209946, 'total_duration': 23608.196998119354, 'accumulated_submission_time': 13499.536531209946, 'accumulated_eval_time': 10108.065262794495, 'accumulated_logging_time': 0.3457338809967041, 'global_step': 18722, 'preemption_count': 0}), (19893, {'train/accuracy': 0.6428802013397217, 'train/loss': 1.8101420402526855, 'train/bleu': 31.652890632454614, 'validation/accuracy': 0.6564704775810242, 'validation/loss': 1.729822039604187, 'validation/bleu': 28.23211242602608, 'validation/num_examples': 3000, 'test/accuracy': 0.6648422479629517, 'test/loss': 1.6675126552581787, 'test/bleu': 27.482161826855798, 'test/num_examples': 3003, 'score': 14339.718233585358, 'total_duration': 24891.002490520477, 'accumulated_submission_time': 14339.718233585358, 'accumulated_eval_time': 10550.653219938278, 'accumulated_logging_time': 0.3667581081390381, 'global_step': 19893, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6396594047546387, 'train/loss': 1.8334894180297852, 'train/bleu': 31.42482306685711, 'validation/accuracy': 0.6545237898826599, 'validation/loss': 1.736132025718689, 'validation/bleu': 28.09382161119431, 'validation/num_examples': 3000, 'test/accuracy': 0.6620301008224487, 'test/loss': 1.6754614114761353, 'test/bleu': 27.09508705862738, 'test/num_examples': 3003, 'score': 14416.241226673126, 'total_duration': 25425.949693202972, 'accumulated_submission_time': 14416.241226673126, 'accumulated_eval_time': 11009.051575422287, 'accumulated_logging_time': 0.3904757499694824, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0429 02:15:41.157224 140643883775808 submission_runner.py:581] Timing: 14416.241226673126
I0429 02:15:41.157271 140643883775808 submission_runner.py:582] ====================
I0429 02:15:41.157390 140643883775808 submission_runner.py:645] Final wmt score: 14416.241226673126
