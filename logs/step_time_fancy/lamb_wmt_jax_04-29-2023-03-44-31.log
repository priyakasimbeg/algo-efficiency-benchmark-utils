python3 submission_runner.py --framework=jax --workload=wmt --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_jax_04-29-2023-03-44-31.log
I0429 03:44:52.602042 139723888199488 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_lamb/wmt_jax.
I0429 03:44:52.666462 139723888199488 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 03:44:53.524304 139723888199488 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0429 03:44:53.525344 139723888199488 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 03:44:53.529567 139723888199488 submission_runner.py:538] Using RNG seed 1145551288
I0429 03:44:56.163938 139723888199488 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 03:44:56.164138 139723888199488 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1.
I0429 03:44:56.164337 139723888199488 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1/hparams.json.
I0429 03:44:56.287017 139723888199488 submission_runner.py:241] Initializing dataset.
I0429 03:44:56.295971 139723888199488 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0429 03:44:56.299551 139723888199488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0429 03:44:56.299666 139723888199488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0429 03:44:56.413640 139723888199488 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0429 03:44:58.293307 139723888199488 submission_runner.py:248] Initializing model.
I0429 03:45:10.874114 139723888199488 submission_runner.py:258] Initializing optimizer.
I0429 03:45:11.785550 139723888199488 submission_runner.py:265] Initializing metrics bundle.
I0429 03:45:11.785744 139723888199488 submission_runner.py:282] Initializing checkpoint and logger.
I0429 03:45:11.786721 139723888199488 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1 with prefix checkpoint_
I0429 03:45:11.787005 139723888199488 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 03:45:11.787074 139723888199488 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 03:45:12.686096 139723888199488 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1/meta_data_0.json.
I0429 03:45:12.687128 139723888199488 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1/flags_0.json.
I0429 03:45:12.691576 139723888199488 submission_runner.py:318] Starting training loop.
I0429 03:46:06.292864 139547933398784 logging_writer.py:48] [0] global_step=0, grad_norm=5.308122158050537, loss=11.025724411010742
I0429 03:46:06.308210 139723888199488 spec.py:298] Evaluating on the training split.
I0429 03:46:06.310862 139723888199488 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0429 03:46:06.313261 139723888199488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0429 03:46:06.313367 139723888199488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0429 03:46:06.344349 139723888199488 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0429 03:46:14.488483 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 03:51:18.306643 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 03:51:18.310686 139723888199488 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0429 03:51:18.314644 139723888199488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0429 03:51:18.314754 139723888199488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0429 03:51:18.345609 139723888199488 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0429 03:51:26.082005 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 03:56:23.252886 139723888199488 spec.py:326] Evaluating on the test split.
I0429 03:56:23.255562 139723888199488 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0429 03:56:23.258648 139723888199488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0429 03:56:23.258750 139723888199488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0429 03:56:23.289587 139723888199488 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0429 03:56:30.266225 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:01:21.368107 139723888199488 submission_runner.py:415] Time since start: 968.68s, 	Step: 1, 	{'train/accuracy': 0.0005839783698320389, 'train/loss': 10.999003410339355, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.992518424987793, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.013197898864746, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 53.61647081375122, 'total_duration': 968.6764461994171, 'accumulated_submission_time': 53.61647081375122, 'accumulated_eval_time': 915.0598285198212, 'accumulated_logging_time': 0}
I0429 04:01:21.385905 139536716531456 logging_writer.py:48] [1] accumulated_eval_time=915.059829, accumulated_logging_time=0, accumulated_submission_time=53.616471, global_step=1, preemption_count=0, score=53.616471, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.013198, test/num_examples=3003, total_duration=968.676446, train/accuracy=0.000584, train/bleu=0.000000, train/loss=10.999003, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.992518, validation/num_examples=3000
I0429 04:01:59.251842 139536674567936 logging_writer.py:48] [100] global_step=100, grad_norm=2.921907663345337, loss=9.996770858764648
I0429 04:02:36.917519 139536716531456 logging_writer.py:48] [200] global_step=200, grad_norm=0.6128736734390259, loss=8.870271682739258
I0429 04:03:14.391178 139536674567936 logging_writer.py:48] [300] global_step=300, grad_norm=0.22738558053970337, loss=8.287138938903809
I0429 04:03:52.058472 139536716531456 logging_writer.py:48] [400] global_step=400, grad_norm=0.23651549220085144, loss=7.850520610809326
I0429 04:04:29.704345 139536674567936 logging_writer.py:48] [500] global_step=500, grad_norm=0.2073364108800888, loss=7.442154407501221
I0429 04:05:07.571468 139536716531456 logging_writer.py:48] [600] global_step=600, grad_norm=0.342598557472229, loss=7.1238813400268555
I0429 04:05:45.272428 139536674567936 logging_writer.py:48] [700] global_step=700, grad_norm=0.4063323438167572, loss=6.886476039886475
I0429 04:06:22.989377 139536716531456 logging_writer.py:48] [800] global_step=800, grad_norm=1.014461636543274, loss=6.69135046005249
I0429 04:07:00.599099 139536674567936 logging_writer.py:48] [900] global_step=900, grad_norm=0.8114915490150452, loss=6.508681297302246
I0429 04:07:38.621187 139536716531456 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1261597871780396, loss=6.369630336761475
I0429 04:08:16.311398 139536674567936 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8545570969581604, loss=6.220645427703857
I0429 04:08:54.012945 139536716531456 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0337265729904175, loss=6.0434441566467285
I0429 04:09:31.665266 139536674567936 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0484521389007568, loss=5.949606895446777
I0429 04:10:09.334938 139536716531456 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8780153393745422, loss=5.928617477416992
I0429 04:10:47.211696 139536674567936 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9019114971160889, loss=5.818053722381592
I0429 04:11:24.918031 139536716531456 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9502999782562256, loss=5.660966396331787
I0429 04:12:02.666582 139536674567936 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0484086275100708, loss=5.612846374511719
I0429 04:12:40.677616 139536716531456 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.098244309425354, loss=5.498322486877441
I0429 04:13:18.343207 139536674567936 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0386357307434082, loss=5.42534065246582
I0429 04:13:56.054302 139536716531456 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.099904179573059, loss=5.347111701965332
I0429 04:14:33.780405 139536674567936 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1617027521133423, loss=5.202151775360107
I0429 04:15:11.435591 139536716531456 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1392145156860352, loss=5.196777820587158
I0429 04:15:21.618880 139723888199488 spec.py:298] Evaluating on the training split.
I0429 04:15:24.404735 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:19:36.590206 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 04:19:39.254222 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:23:35.475264 139723888199488 spec.py:326] Evaluating on the test split.
I0429 04:23:38.179993 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:27:37.745265 139723888199488 submission_runner.py:415] Time since start: 2545.05s, 	Step: 2228, 	{'train/accuracy': 0.33459994196891785, 'train/loss': 4.2618560791015625, 'train/bleu': 8.65753714890141, 'validation/accuracy': 0.31049832701683044, 'validation/loss': 4.518955230712891, 'validation/bleu': 4.905476184455266, 'validation/num_examples': 3000, 'test/accuracy': 0.28867584466934204, 'test/loss': 4.806459426879883, 'test/bleu': 3.596795169746106, 'test/num_examples': 3003, 'score': 893.8091731071472, 'total_duration': 2545.0536069869995, 'accumulated_submission_time': 893.8091731071472, 'accumulated_eval_time': 1651.1861703395844, 'accumulated_logging_time': 0.02766704559326172}
I0429 04:27:37.755489 139536674567936 logging_writer.py:48] [2228] accumulated_eval_time=1651.186170, accumulated_logging_time=0.027667, accumulated_submission_time=893.809173, global_step=2228, preemption_count=0, score=893.809173, test/accuracy=0.288676, test/bleu=3.596795, test/loss=4.806459, test/num_examples=3003, total_duration=2545.053607, train/accuracy=0.334600, train/bleu=8.657537, train/loss=4.261856, validation/accuracy=0.310498, validation/bleu=4.905476, validation/loss=4.518955, validation/num_examples=3000
I0429 04:28:05.404662 139536716531456 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7866409420967102, loss=5.102284908294678
I0429 04:28:42.942995 139536674567936 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9764383435249329, loss=4.988961219787598
I0429 04:29:20.508226 139536716531456 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8189321756362915, loss=4.953779220581055
I0429 04:29:58.043745 139536674567936 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0013045072555542, loss=4.905015468597412
I0429 04:30:35.782480 139536716531456 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9991416931152344, loss=4.788585662841797
I0429 04:31:13.372951 139536674567936 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8497877717018127, loss=4.708007335662842
I0429 04:31:50.985502 139536716531456 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1519370079040527, loss=4.669838905334473
I0429 04:32:28.618130 139536674567936 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8311693072319031, loss=4.588502407073975
I0429 04:33:06.199202 139536716531456 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1681256294250488, loss=4.445189952850342
I0429 04:33:43.894565 139536674567936 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0570341348648071, loss=4.377685546875
I0429 04:34:21.477757 139536716531456 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8000550270080566, loss=4.381619453430176
I0429 04:34:58.967154 139536674567936 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9421592354774475, loss=4.33457088470459
I0429 04:35:36.517082 139536716531456 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8529146313667297, loss=4.205707550048828
I0429 04:36:14.347063 139536674567936 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8446519374847412, loss=4.259448051452637
I0429 04:36:51.904138 139536716531456 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9098424315452576, loss=4.147202014923096
I0429 04:37:29.431950 139536674567936 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.027200698852539, loss=4.126989364624023
I0429 04:38:07.009891 139536716531456 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8375312685966492, loss=4.074423789978027
I0429 04:38:44.554442 139536674567936 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8214258551597595, loss=4.079801559448242
I0429 04:39:22.257412 139536716531456 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9243242144584656, loss=3.957517147064209
I0429 04:39:59.729693 139536674567936 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8553561568260193, loss=3.985412120819092
I0429 04:40:37.216723 139536716531456 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7684297561645508, loss=3.9036688804626465
I0429 04:41:14.742055 139536674567936 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5988007187843323, loss=3.940725803375244
I0429 04:41:37.873930 139723888199488 spec.py:298] Evaluating on the training split.
I0429 04:41:40.652412 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:44:38.038228 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 04:44:40.691064 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:47:31.273476 139723888199488 spec.py:326] Evaluating on the test split.
I0429 04:47:33.978110 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 04:50:03.832974 139723888199488 submission_runner.py:415] Time since start: 3891.14s, 	Step: 4462, 	{'train/accuracy': 0.5154173970222473, 'train/loss': 2.7784924507141113, 'train/bleu': 22.40603292799442, 'validation/accuracy': 0.513484001159668, 'validation/loss': 2.779236316680908, 'validation/bleu': 18.37447807552748, 'validation/num_examples': 3000, 'test/accuracy': 0.5120097994804382, 'test/loss': 2.8204383850097656, 'test/bleu': 16.91225380539878, 'test/num_examples': 3003, 'score': 1733.8869132995605, 'total_duration': 3891.1413197517395, 'accumulated_submission_time': 1733.8869132995605, 'accumulated_eval_time': 2157.1451766490936, 'accumulated_logging_time': 0.04850292205810547}
I0429 04:50:03.841351 139536716531456 logging_writer.py:48] [4462] accumulated_eval_time=2157.145177, accumulated_logging_time=0.048503, accumulated_submission_time=1733.886913, global_step=4462, preemption_count=0, score=1733.886913, test/accuracy=0.512010, test/bleu=16.912254, test/loss=2.820438, test/num_examples=3003, total_duration=3891.141320, train/accuracy=0.515417, train/bleu=22.406033, train/loss=2.778492, validation/accuracy=0.513484, validation/bleu=18.374478, validation/loss=2.779236, validation/num_examples=3000
I0429 04:50:18.540753 139536674567936 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.607346773147583, loss=3.7912731170654297
I0429 04:50:56.147356 139536716531456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7615784406661987, loss=3.878124952316284
I0429 04:51:33.702423 139536674567936 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6842454671859741, loss=3.7322635650634766
I0429 04:52:11.224548 139536716531456 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6051385402679443, loss=3.767941951751709
I0429 04:52:48.763972 139536674567936 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6258501410484314, loss=3.711636543273926
I0429 04:53:26.548021 139536716531456 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5654920339584351, loss=3.691049098968506
I0429 04:54:04.104295 139536674567936 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.576518177986145, loss=3.6742491722106934
I0429 04:54:41.640254 139536716531456 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5943953990936279, loss=3.809443473815918
I0429 04:55:19.148010 139536674567936 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4952212870121002, loss=3.621983289718628
I0429 04:55:56.835140 139536716531456 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4979934096336365, loss=3.6612727642059326
I0429 04:56:34.332854 139536674567936 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.4744507074356079, loss=3.63653564453125
I0429 04:57:11.814330 139536716531456 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5422768592834473, loss=3.65958571434021
I0429 04:57:49.384614 139536674567936 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4650527536869049, loss=3.613586187362671
I0429 04:58:26.998709 139536716531456 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.503428041934967, loss=3.521841049194336
I0429 04:59:04.669489 139536674567936 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4325639009475708, loss=3.519144296646118
I0429 04:59:42.138640 139536716531456 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5009738802909851, loss=3.4758145809173584
I0429 05:00:19.633996 139536674567936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4437803030014038, loss=3.560533046722412
I0429 05:00:57.243782 139536716531456 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4464559257030487, loss=3.4931769371032715
I0429 05:01:34.976118 139536674567936 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.47056934237480164, loss=3.4661641120910645
I0429 05:02:12.519010 139536716531456 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4812621474266052, loss=3.4836862087249756
I0429 05:02:50.085093 139536674567936 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4360005557537079, loss=3.47588849067688
I0429 05:03:27.704069 139536716531456 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4186547100543976, loss=3.4722707271575928
I0429 05:04:04.163847 139723888199488 spec.py:298] Evaluating on the training split.
I0429 05:04:06.954165 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:06:43.740356 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 05:06:46.417482 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:09:10.580420 139723888199488 spec.py:326] Evaluating on the test split.
I0429 05:09:13.290955 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:11:35.812237 139723888199488 submission_runner.py:415] Time since start: 5183.12s, 	Step: 6698, 	{'train/accuracy': 0.5831562876701355, 'train/loss': 2.25498104095459, 'train/bleu': 27.08071988894994, 'validation/accuracy': 0.5791496634483337, 'validation/loss': 2.2739834785461426, 'validation/bleu': 22.668954467500736, 'validation/num_examples': 3000, 'test/accuracy': 0.5792807340621948, 'test/loss': 2.2721879482269287, 'test/bleu': 21.4245356294354, 'test/num_examples': 3003, 'score': 2574.1705930233, 'total_duration': 5183.120572566986, 'accumulated_submission_time': 2574.1705930233, 'accumulated_eval_time': 2608.7935285568237, 'accumulated_logging_time': 0.06556010246276855}
I0429 05:11:35.821168 139536674567936 logging_writer.py:48] [6698] accumulated_eval_time=2608.793529, accumulated_logging_time=0.065560, accumulated_submission_time=2574.170593, global_step=6698, preemption_count=0, score=2574.170593, test/accuracy=0.579281, test/bleu=21.424536, test/loss=2.272188, test/num_examples=3003, total_duration=5183.120573, train/accuracy=0.583156, train/bleu=27.080720, train/loss=2.254981, validation/accuracy=0.579150, validation/bleu=22.668954, validation/loss=2.273983, validation/num_examples=3000
I0429 05:11:36.959171 139536716531456 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.43299761414527893, loss=3.4098286628723145
I0429 05:12:14.671242 139536674567936 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4098648428916931, loss=3.447577476501465
I0429 05:12:52.125069 139536716531456 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.3873799443244934, loss=3.4148828983306885
I0429 05:13:29.560618 139536674567936 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.3752695918083191, loss=3.495260238647461
I0429 05:14:07.325291 139536716531456 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4055994749069214, loss=3.428013801574707
I0429 05:14:44.934482 139536674567936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3825863301753998, loss=3.394207715988159
I0429 05:15:22.464069 139536716531456 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.43016815185546875, loss=3.3344051837921143
I0429 05:15:59.927873 139536674567936 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.39664649963378906, loss=3.3779168128967285
I0429 05:16:37.591071 139536716531456 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3637531101703644, loss=3.372390031814575
I0429 05:17:15.296709 139536674567936 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.35743963718414307, loss=3.422687530517578
I0429 05:17:52.817948 139536716531456 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.33947673439979553, loss=3.314199447631836
I0429 05:18:30.318371 139536674567936 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.33542779088020325, loss=3.3406970500946045
I0429 05:19:07.846775 139536716531456 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.35643959045410156, loss=3.2919909954071045
I0429 05:19:45.597367 139536674567936 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3429511785507202, loss=3.3842170238494873
I0429 05:20:23.082522 139536716531456 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3270093500614166, loss=3.310218095779419
I0429 05:21:00.484981 139536674567936 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.32772934436798096, loss=3.2549846172332764
I0429 05:21:37.881261 139536716531456 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.35867956280708313, loss=3.3056600093841553
I0429 05:22:15.399959 139536674567936 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3393368422985077, loss=3.267364501953125
I0429 05:22:53.064654 139536716531456 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3127613663673401, loss=3.337656259536743
I0429 05:23:30.484480 139536674567936 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3103989064693451, loss=3.2911617755889893
I0429 05:24:07.909336 139536716531456 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.33966296911239624, loss=3.2594501972198486
I0429 05:24:45.341466 139536674567936 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3118816912174225, loss=3.2824172973632812
I0429 05:25:23.026759 139536716531456 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.29901769757270813, loss=3.274592161178589
I0429 05:25:36.133300 139723888199488 spec.py:298] Evaluating on the training split.
I0429 05:25:38.926216 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:28:48.388077 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 05:28:51.051711 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:31:13.997321 139723888199488 spec.py:326] Evaluating on the test split.
I0429 05:31:16.703933 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:33:29.796298 139723888199488 submission_runner.py:415] Time since start: 6497.10s, 	Step: 8936, 	{'train/accuracy': 0.6036889553070068, 'train/loss': 2.1165943145751953, 'train/bleu': 28.806562038594933, 'validation/accuracy': 0.6129000186920166, 'validation/loss': 2.0367703437805176, 'validation/bleu': 24.950720356056557, 'validation/num_examples': 3000, 'test/accuracy': 0.6173261404037476, 'test/loss': 2.00534725189209, 'test/bleu': 24.113168096459894, 'test/num_examples': 3003, 'score': 3414.4437255859375, 'total_duration': 6497.1046397686005, 'accumulated_submission_time': 3414.4437255859375, 'accumulated_eval_time': 3082.456480741501, 'accumulated_logging_time': 0.08328461647033691}
I0429 05:33:29.805544 139536674567936 logging_writer.py:48] [8936] accumulated_eval_time=3082.456481, accumulated_logging_time=0.083285, accumulated_submission_time=3414.443726, global_step=8936, preemption_count=0, score=3414.443726, test/accuracy=0.617326, test/bleu=24.113168, test/loss=2.005347, test/num_examples=3003, total_duration=6497.104640, train/accuracy=0.603689, train/bleu=28.806562, train/loss=2.116594, validation/accuracy=0.612900, validation/bleu=24.950720, validation/loss=2.036770, validation/num_examples=3000
I0429 05:33:54.186323 139536716531456 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.300172358751297, loss=3.296110153198242
I0429 05:34:31.614104 139536674567936 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3342585563659668, loss=3.326078414916992
I0429 05:35:09.138761 139536716531456 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3031613528728485, loss=3.256321430206299
I0429 05:35:46.586436 139536674567936 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3049338757991791, loss=3.205919027328491
I0429 05:36:24.277523 139536716531456 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.32259389758110046, loss=3.2027268409729004
I0429 05:37:01.783636 139536674567936 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.29557478427886963, loss=3.206613302230835
I0429 05:37:39.295269 139536716531456 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.29326796531677246, loss=3.1559414863586426
I0429 05:38:16.818222 139536674567936 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.31076863408088684, loss=3.1611294746398926
I0429 05:38:54.677602 139536716531456 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.29986295104026794, loss=3.2061455249786377
I0429 05:39:32.123180 139536674567936 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.29657691717147827, loss=3.2245230674743652
I0429 05:40:09.653060 139536716531456 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.29250720143318176, loss=3.20326566696167
I0429 05:40:47.184614 139536674567936 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2934902310371399, loss=3.188608407974243
I0429 05:41:24.696195 139536716531456 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.28623414039611816, loss=3.202085494995117
I0429 05:42:02.434668 139536674567936 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.28118252754211426, loss=3.1666903495788574
I0429 05:42:39.998446 139536716531456 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.32650747895240784, loss=3.1818065643310547
I0429 05:43:17.551808 139536674567936 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.29080119729042053, loss=3.2060608863830566
I0429 05:43:55.111322 139536716531456 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.27037036418914795, loss=3.1651549339294434
I0429 05:44:32.858738 139536674567936 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.2866125702857971, loss=3.235085964202881
I0429 05:45:10.323675 139536716531456 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2784416973590851, loss=3.1531729698181152
I0429 05:45:47.759174 139536674567936 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.27299779653549194, loss=3.159682512283325
I0429 05:46:25.207088 139536716531456 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.28911665081977844, loss=3.219231128692627
I0429 05:47:02.751972 139536674567936 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.28158071637153625, loss=3.116945266723633
I0429 05:47:29.914722 139723888199488 spec.py:298] Evaluating on the training split.
I0429 05:47:32.687492 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:50:04.775650 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 05:50:07.432402 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:52:29.925131 139723888199488 spec.py:326] Evaluating on the test split.
I0429 05:52:32.632793 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 05:54:43.673982 139723888199488 submission_runner.py:415] Time since start: 7770.98s, 	Step: 11173, 	{'train/accuracy': 0.616675853729248, 'train/loss': 2.014763355255127, 'train/bleu': 29.97519625327575, 'validation/accuracy': 0.6298868060112, 'validation/loss': 1.8995845317840576, 'validation/bleu': 26.017875681163808, 'validation/num_examples': 3000, 'test/accuracy': 0.6370693445205688, 'test/loss': 1.8521310091018677, 'test/bleu': 25.501189172347082, 'test/num_examples': 3003, 'score': 4254.513042926788, 'total_duration': 7770.982298135757, 'accumulated_submission_time': 4254.513042926788, 'accumulated_eval_time': 3516.215667963028, 'accumulated_logging_time': 0.10228109359741211}
I0429 05:54:43.683946 139536716531456 logging_writer.py:48] [11173] accumulated_eval_time=3516.215668, accumulated_logging_time=0.102281, accumulated_submission_time=4254.513043, global_step=11173, preemption_count=0, score=4254.513043, test/accuracy=0.637069, test/bleu=25.501189, test/loss=1.852131, test/num_examples=3003, total_duration=7770.982298, train/accuracy=0.616676, train/bleu=29.975196, train/loss=2.014763, validation/accuracy=0.629887, validation/bleu=26.017876, validation/loss=1.899585, validation/num_examples=3000
I0429 05:54:54.217245 139536674567936 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2724851071834564, loss=3.1390795707702637
I0429 05:55:31.610747 139536716531456 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.27807560563087463, loss=3.1440553665161133
I0429 05:56:09.012359 139536674567936 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.26144537329673767, loss=3.1034915447235107
I0429 05:56:46.487122 139536716531456 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2673967778682709, loss=3.168200969696045
I0429 05:57:24.282344 139536674567936 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.2829538583755493, loss=3.1851823329925537
I0429 05:58:01.778945 139536716531456 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.27683353424072266, loss=3.1369121074676514
I0429 05:58:39.224567 139536674567936 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2635416090488434, loss=3.093783140182495
I0429 05:59:16.661661 139536716531456 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2733091711997986, loss=3.167551040649414
I0429 05:59:54.098417 139536674567936 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2680935263633728, loss=3.1765613555908203
I0429 06:00:31.817767 139536716531456 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.25613898038864136, loss=3.091996669769287
I0429 06:01:09.267823 139536674567936 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.261364221572876, loss=3.109799861907959
I0429 06:01:46.691814 139536716531456 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2596970796585083, loss=3.078902244567871
I0429 06:02:24.311017 139536674567936 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.25541970133781433, loss=3.0971522331237793
I0429 06:03:01.743431 139536716531456 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2692539691925049, loss=3.143467903137207
I0429 06:03:39.221671 139536674567936 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.244405597448349, loss=3.091109275817871
I0429 06:04:16.731730 139536716531456 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.2450106143951416, loss=3.0748984813690186
I0429 06:04:54.217390 139536674567936 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.25317880511283875, loss=3.07492733001709
I0429 06:05:31.920651 139536716531456 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.25551095604896545, loss=2.9917750358581543
I0429 06:06:09.472851 139536674567936 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.25127407908439636, loss=3.0916104316711426
I0429 06:06:47.075396 139536716531456 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2592112720012665, loss=3.1252987384796143
I0429 06:07:24.571919 139536674567936 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.26241594552993774, loss=3.1128463745117188
I0429 06:08:02.318091 139536716531456 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.25436505675315857, loss=3.1752240657806396
I0429 06:08:39.772718 139536674567936 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.25897181034088135, loss=3.062093496322632
I0429 06:08:43.894183 139723888199488 spec.py:298] Evaluating on the training split.
I0429 06:08:46.676497 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:11:32.877755 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 06:11:35.539035 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:14:02.399310 139723888199488 spec.py:326] Evaluating on the test split.
I0429 06:14:05.124826 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:16:14.267289 139723888199488 submission_runner.py:415] Time since start: 9061.58s, 	Step: 13412, 	{'train/accuracy': 0.6381995677947998, 'train/loss': 1.8588451147079468, 'train/bleu': 31.41670233228391, 'validation/accuracy': 0.6415667533874512, 'validation/loss': 1.8262288570404053, 'validation/bleu': 27.137651813989375, 'validation/num_examples': 3000, 'test/accuracy': 0.6481901407241821, 'test/loss': 1.7765562534332275, 'test/bleu': 26.434506074613665, 'test/num_examples': 3003, 'score': 5094.683845281601, 'total_duration': 9061.5756149292, 'accumulated_submission_time': 5094.683845281601, 'accumulated_eval_time': 3966.5887093544006, 'accumulated_logging_time': 0.12103819847106934}
I0429 06:16:14.277041 139536716531456 logging_writer.py:48] [13412] accumulated_eval_time=3966.588709, accumulated_logging_time=0.121038, accumulated_submission_time=5094.683845, global_step=13412, preemption_count=0, score=5094.683845, test/accuracy=0.648190, test/bleu=26.434506, test/loss=1.776556, test/num_examples=3003, total_duration=9061.575615, train/accuracy=0.638200, train/bleu=31.416702, train/loss=1.858845, validation/accuracy=0.641567, validation/bleu=27.137652, validation/loss=1.826229, validation/num_examples=3000
I0429 06:16:47.532246 139536674567936 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.25358203053474426, loss=3.1369290351867676
I0429 06:17:24.973028 139536716531456 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2531660199165344, loss=3.1222331523895264
I0429 06:18:02.738335 139536674567936 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.25483110547065735, loss=3.1004717350006104
I0429 06:18:40.181263 139536716531456 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2425372302532196, loss=3.0699121952056885
I0429 06:19:17.707164 139536674567936 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2798520624637604, loss=3.0888450145721436
I0429 06:19:55.225566 139536716531456 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.24531583487987518, loss=3.076509952545166
I0429 06:20:32.695272 139536674567936 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.24705153703689575, loss=3.0266776084899902
I0429 06:21:10.359556 139536716531456 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.25074952840805054, loss=3.1106674671173096
I0429 06:21:47.865744 139536674567936 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.241984561085701, loss=2.9869937896728516
I0429 06:22:25.362893 139536716531456 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.231242835521698, loss=3.0396695137023926
I0429 06:23:02.826276 139536674567936 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2434791773557663, loss=3.107069969177246
I0429 06:23:40.483673 139536716531456 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.24741919338703156, loss=3.117318630218506
I0429 06:24:17.924738 139536674567936 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2427746057510376, loss=3.0969223976135254
I0429 06:24:55.436746 139536716531456 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2533392906188965, loss=3.06060791015625
I0429 06:25:32.872905 139536674567936 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.2467462122440338, loss=3.083279848098755
I0429 06:26:10.340381 139536716531456 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2443479299545288, loss=3.050961971282959
I0429 06:26:47.956245 139536674567936 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2575427293777466, loss=3.154531717300415
I0429 06:27:25.389132 139536716531456 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.23601219058036804, loss=3.0894460678100586
I0429 06:28:02.809783 139536674567936 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2437283992767334, loss=3.0444204807281494
I0429 06:28:40.307141 139536716531456 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2337435930967331, loss=3.040456533432007
I0429 06:29:17.993467 139536674567936 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.24506555497646332, loss=3.013364791870117
I0429 06:29:55.513161 139536716531456 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2601020932197571, loss=3.0930919647216797
I0429 06:30:14.309563 139723888199488 spec.py:298] Evaluating on the training split.
I0429 06:30:17.095811 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:33:22.561579 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 06:33:25.228641 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:36:27.055577 139723888199488 spec.py:326] Evaluating on the test split.
I0429 06:36:29.764491 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:38:52.452033 139723888199488 submission_runner.py:415] Time since start: 10419.76s, 	Step: 15651, 	{'train/accuracy': 0.6355159878730774, 'train/loss': 1.8743146657943726, 'train/bleu': 30.557655897445727, 'validation/accuracy': 0.647753894329071, 'validation/loss': 1.780929684638977, 'validation/bleu': 27.390614906740353, 'validation/num_examples': 3000, 'test/accuracy': 0.6559526324272156, 'test/loss': 1.7229359149932861, 'test/bleu': 26.549693374046118, 'test/num_examples': 3003, 'score': 5934.6777102947235, 'total_duration': 10419.760348320007, 'accumulated_submission_time': 5934.6777102947235, 'accumulated_eval_time': 4484.731123685837, 'accumulated_logging_time': 0.13928961753845215}
I0429 06:38:52.461991 139536674567936 logging_writer.py:48] [15651] accumulated_eval_time=4484.731124, accumulated_logging_time=0.139290, accumulated_submission_time=5934.677710, global_step=15651, preemption_count=0, score=5934.677710, test/accuracy=0.655953, test/bleu=26.549693, test/loss=1.722936, test/num_examples=3003, total_duration=10419.760348, train/accuracy=0.635516, train/bleu=30.557656, train/loss=1.874315, validation/accuracy=0.647754, validation/bleu=27.390615, validation/loss=1.780930, validation/num_examples=3000
I0429 06:39:11.217619 139536716531456 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.24477921426296234, loss=3.049355983734131
I0429 06:39:48.664867 139536674567936 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.24289187788963318, loss=2.967935800552368
I0429 06:40:26.377537 139536716531456 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.23723992705345154, loss=2.9273550510406494
I0429 06:41:03.864953 139536674567936 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.23189805448055267, loss=2.9651596546173096
I0429 06:41:41.301863 139536716531456 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.23587648570537567, loss=3.061354637145996
I0429 06:42:18.782721 139536674567936 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2361059933900833, loss=2.9930148124694824
I0429 06:42:56.198546 139536716531456 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.23377211391925812, loss=3.0595176219940186
I0429 06:43:33.871772 139536674567936 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.22872918844223022, loss=3.005732297897339
I0429 06:44:11.319575 139536716531456 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.233671635389328, loss=3.0389673709869385
I0429 06:44:48.774840 139536674567936 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.23844794929027557, loss=2.9541478157043457
I0429 06:45:26.151691 139536716531456 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.23012661933898926, loss=2.9753456115722656
I0429 06:46:03.819012 139536674567936 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.23618116974830627, loss=2.9546711444854736
I0429 06:46:41.214739 139536716531456 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.23026323318481445, loss=2.97086763381958
I0429 06:47:18.706110 139536674567936 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.24066637456417084, loss=3.0709874629974365
I0429 06:47:56.171235 139536716531456 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.23666217923164368, loss=3.047145128250122
I0429 06:48:33.639161 139536674567936 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.22837932407855988, loss=3.036410331726074
I0429 06:49:11.298578 139536716531456 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.23759008944034576, loss=2.9505341053009033
I0429 06:49:48.724771 139536674567936 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.22716379165649414, loss=2.961745500564575
I0429 06:50:26.205560 139536716531456 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.23701362311840057, loss=3.0227818489074707
I0429 06:51:03.695265 139536674567936 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.22908242046833038, loss=3.0176711082458496
I0429 06:51:41.419268 139536716531456 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.22279322147369385, loss=3.0169146060943604
I0429 06:52:18.915208 139536674567936 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.22618235647678375, loss=2.9677162170410156
I0429 06:52:52.577722 139723888199488 spec.py:298] Evaluating on the training split.
I0429 06:52:55.377256 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:55:35.852489 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 06:55:38.510433 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 06:58:02.995937 139723888199488 spec.py:326] Evaluating on the test split.
I0429 06:58:05.703919 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 07:00:19.786254 139723888199488 submission_runner.py:415] Time since start: 11707.09s, 	Step: 17891, 	{'train/accuracy': 0.6396002173423767, 'train/loss': 1.852996587753296, 'train/bleu': 31.195406893512605, 'validation/accuracy': 0.6517339944839478, 'validation/loss': 1.7530949115753174, 'validation/bleu': 27.73798861343846, 'validation/num_examples': 3000, 'test/accuracy': 0.6617628335952759, 'test/loss': 1.6927454471588135, 'test/bleu': 27.140436442868786, 'test/num_examples': 3003, 'score': 6774.75356388092, 'total_duration': 11707.094594478607, 'accumulated_submission_time': 6774.75356388092, 'accumulated_eval_time': 4931.939610958099, 'accumulated_logging_time': 0.15900969505310059}
I0429 07:00:19.796168 139536716531456 logging_writer.py:48] [17891] accumulated_eval_time=4931.939611, accumulated_logging_time=0.159010, accumulated_submission_time=6774.753564, global_step=17891, preemption_count=0, score=6774.753564, test/accuracy=0.661763, test/bleu=27.140436, test/loss=1.692745, test/num_examples=3003, total_duration=11707.094594, train/accuracy=0.639600, train/bleu=31.195407, train/loss=1.852997, validation/accuracy=0.651734, validation/bleu=27.737989, validation/loss=1.753095, validation/num_examples=3000
I0429 07:00:23.550686 139536674567936 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.21645674109458923, loss=2.9615676403045654
I0429 07:01:00.997215 139536716531456 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.22890888154506683, loss=3.0302300453186035
I0429 07:01:38.682058 139536674567936 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.24556531012058258, loss=2.995703935623169
I0429 07:02:16.248608 139536716531456 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.2328978329896927, loss=3.0036673545837402
I0429 07:02:53.710167 139536674567936 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.23129983246326447, loss=2.999143362045288
I0429 07:03:31.160412 139536716531456 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.22354310750961304, loss=2.977537155151367
I0429 07:04:08.777023 139536674567936 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.22186483442783356, loss=2.9548003673553467
I0429 07:04:46.582540 139536716531456 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.22022384405136108, loss=2.957148551940918
I0429 07:05:24.115184 139536674567936 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.22334353625774384, loss=2.9007201194763184
I0429 07:06:01.651806 139536716531456 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.22730621695518494, loss=2.9910221099853516
I0429 07:06:39.254428 139536674567936 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.22517001628875732, loss=2.9325084686279297
I0429 07:07:17.034732 139536716531456 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2196345329284668, loss=2.940272808074951
I0429 07:07:54.526995 139536674567936 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2226696014404297, loss=2.9058871269226074
I0429 07:08:32.096569 139536716531456 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.22167319059371948, loss=2.936281204223633
I0429 07:09:09.535099 139536674567936 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2157527059316635, loss=2.9173851013183594
I0429 07:09:46.937919 139536716531456 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.21139828860759735, loss=2.9332566261291504
I0429 07:10:24.560777 139536674567936 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2412157952785492, loss=2.9319069385528564
I0429 07:11:02.019862 139536716531456 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.21638455986976624, loss=2.9591383934020996
I0429 07:11:39.405294 139536674567936 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.23051856458187103, loss=2.9494035243988037
I0429 07:12:16.724688 139536716531456 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.21399708092212677, loss=2.9732136726379395
I0429 07:12:54.423712 139536674567936 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.2144303023815155, loss=2.911301851272583
I0429 07:13:31.472169 139723888199488 spec.py:298] Evaluating on the training split.
I0429 07:13:34.252747 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 07:17:34.679304 139723888199488 spec.py:310] Evaluating on the validation split.
I0429 07:17:37.333303 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 07:20:14.081766 139723888199488 spec.py:326] Evaluating on the test split.
I0429 07:20:16.777334 139723888199488 workload.py:179] Translating evaluation dataset.
I0429 07:22:40.194335 139723888199488 submission_runner.py:415] Time since start: 13047.50s, 	Step: 20000, 	{'train/accuracy': 0.6535845398902893, 'train/loss': 1.745597243309021, 'train/bleu': 32.194458606932685, 'validation/accuracy': 0.6564456820487976, 'validation/loss': 1.7223609685897827, 'validation/bleu': 28.30922073715908, 'validation/num_examples': 3000, 'test/accuracy': 0.6648190021514893, 'test/loss': 1.6608096361160278, 'test/bleu': 27.30953802694152, 'test/num_examples': 3003, 'score': 7566.391624689102, 'total_duration': 13047.502650737762, 'accumulated_submission_time': 7566.391624689102, 'accumulated_eval_time': 5480.6617114543915, 'accumulated_logging_time': 0.1774442195892334}
I0429 07:22:40.204967 139536716531456 logging_writer.py:48] [20000] accumulated_eval_time=5480.661711, accumulated_logging_time=0.177444, accumulated_submission_time=7566.391625, global_step=20000, preemption_count=0, score=7566.391625, test/accuracy=0.664819, test/bleu=27.309538, test/loss=1.660810, test/num_examples=3003, total_duration=13047.502651, train/accuracy=0.653585, train/bleu=32.194459, train/loss=1.745597, validation/accuracy=0.656446, validation/bleu=28.309221, validation/loss=1.722361, validation/num_examples=3000
I0429 07:22:40.222173 139536674567936 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7566.391625
I0429 07:22:41.313925 139723888199488 checkpoints.py:356] Saving checkpoint at step: 20000
I0429 07:22:45.288140 139723888199488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1/checkpoint_20000
I0429 07:22:45.292501 139723888199488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_lamb/wmt_jax/trial_1/checkpoint_20000.
I0429 07:22:45.366594 139723888199488 submission_runner.py:578] Tuning trial 1/1
I0429 07:22:45.366770 139723888199488 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 07:22:45.369209 139723888199488 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005839783698320389, 'train/loss': 10.999003410339355, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.992518424987793, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.013197898864746, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 53.61647081375122, 'total_duration': 968.6764461994171, 'accumulated_submission_time': 53.61647081375122, 'accumulated_eval_time': 915.0598285198212, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2228, {'train/accuracy': 0.33459994196891785, 'train/loss': 4.2618560791015625, 'train/bleu': 8.65753714890141, 'validation/accuracy': 0.31049832701683044, 'validation/loss': 4.518955230712891, 'validation/bleu': 4.905476184455266, 'validation/num_examples': 3000, 'test/accuracy': 0.28867584466934204, 'test/loss': 4.806459426879883, 'test/bleu': 3.596795169746106, 'test/num_examples': 3003, 'score': 893.8091731071472, 'total_duration': 2545.0536069869995, 'accumulated_submission_time': 893.8091731071472, 'accumulated_eval_time': 1651.1861703395844, 'accumulated_logging_time': 0.02766704559326172, 'global_step': 2228, 'preemption_count': 0}), (4462, {'train/accuracy': 0.5154173970222473, 'train/loss': 2.7784924507141113, 'train/bleu': 22.40603292799442, 'validation/accuracy': 0.513484001159668, 'validation/loss': 2.779236316680908, 'validation/bleu': 18.37447807552748, 'validation/num_examples': 3000, 'test/accuracy': 0.5120097994804382, 'test/loss': 2.8204383850097656, 'test/bleu': 16.91225380539878, 'test/num_examples': 3003, 'score': 1733.8869132995605, 'total_duration': 3891.1413197517395, 'accumulated_submission_time': 1733.8869132995605, 'accumulated_eval_time': 2157.1451766490936, 'accumulated_logging_time': 0.04850292205810547, 'global_step': 4462, 'preemption_count': 0}), (6698, {'train/accuracy': 0.5831562876701355, 'train/loss': 2.25498104095459, 'train/bleu': 27.08071988894994, 'validation/accuracy': 0.5791496634483337, 'validation/loss': 2.2739834785461426, 'validation/bleu': 22.668954467500736, 'validation/num_examples': 3000, 'test/accuracy': 0.5792807340621948, 'test/loss': 2.2721879482269287, 'test/bleu': 21.4245356294354, 'test/num_examples': 3003, 'score': 2574.1705930233, 'total_duration': 5183.120572566986, 'accumulated_submission_time': 2574.1705930233, 'accumulated_eval_time': 2608.7935285568237, 'accumulated_logging_time': 0.06556010246276855, 'global_step': 6698, 'preemption_count': 0}), (8936, {'train/accuracy': 0.6036889553070068, 'train/loss': 2.1165943145751953, 'train/bleu': 28.806562038594933, 'validation/accuracy': 0.6129000186920166, 'validation/loss': 2.0367703437805176, 'validation/bleu': 24.950720356056557, 'validation/num_examples': 3000, 'test/accuracy': 0.6173261404037476, 'test/loss': 2.00534725189209, 'test/bleu': 24.113168096459894, 'test/num_examples': 3003, 'score': 3414.4437255859375, 'total_duration': 6497.1046397686005, 'accumulated_submission_time': 3414.4437255859375, 'accumulated_eval_time': 3082.456480741501, 'accumulated_logging_time': 0.08328461647033691, 'global_step': 8936, 'preemption_count': 0}), (11173, {'train/accuracy': 0.616675853729248, 'train/loss': 2.014763355255127, 'train/bleu': 29.97519625327575, 'validation/accuracy': 0.6298868060112, 'validation/loss': 1.8995845317840576, 'validation/bleu': 26.017875681163808, 'validation/num_examples': 3000, 'test/accuracy': 0.6370693445205688, 'test/loss': 1.8521310091018677, 'test/bleu': 25.501189172347082, 'test/num_examples': 3003, 'score': 4254.513042926788, 'total_duration': 7770.982298135757, 'accumulated_submission_time': 4254.513042926788, 'accumulated_eval_time': 3516.215667963028, 'accumulated_logging_time': 0.10228109359741211, 'global_step': 11173, 'preemption_count': 0}), (13412, {'train/accuracy': 0.6381995677947998, 'train/loss': 1.8588451147079468, 'train/bleu': 31.41670233228391, 'validation/accuracy': 0.6415667533874512, 'validation/loss': 1.8262288570404053, 'validation/bleu': 27.137651813989375, 'validation/num_examples': 3000, 'test/accuracy': 0.6481901407241821, 'test/loss': 1.7765562534332275, 'test/bleu': 26.434506074613665, 'test/num_examples': 3003, 'score': 5094.683845281601, 'total_duration': 9061.5756149292, 'accumulated_submission_time': 5094.683845281601, 'accumulated_eval_time': 3966.5887093544006, 'accumulated_logging_time': 0.12103819847106934, 'global_step': 13412, 'preemption_count': 0}), (15651, {'train/accuracy': 0.6355159878730774, 'train/loss': 1.8743146657943726, 'train/bleu': 30.557655897445727, 'validation/accuracy': 0.647753894329071, 'validation/loss': 1.780929684638977, 'validation/bleu': 27.390614906740353, 'validation/num_examples': 3000, 'test/accuracy': 0.6559526324272156, 'test/loss': 1.7229359149932861, 'test/bleu': 26.549693374046118, 'test/num_examples': 3003, 'score': 5934.6777102947235, 'total_duration': 10419.760348320007, 'accumulated_submission_time': 5934.6777102947235, 'accumulated_eval_time': 4484.731123685837, 'accumulated_logging_time': 0.13928961753845215, 'global_step': 15651, 'preemption_count': 0}), (17891, {'train/accuracy': 0.6396002173423767, 'train/loss': 1.852996587753296, 'train/bleu': 31.195406893512605, 'validation/accuracy': 0.6517339944839478, 'validation/loss': 1.7530949115753174, 'validation/bleu': 27.73798861343846, 'validation/num_examples': 3000, 'test/accuracy': 0.6617628335952759, 'test/loss': 1.6927454471588135, 'test/bleu': 27.140436442868786, 'test/num_examples': 3003, 'score': 6774.75356388092, 'total_duration': 11707.094594478607, 'accumulated_submission_time': 6774.75356388092, 'accumulated_eval_time': 4931.939610958099, 'accumulated_logging_time': 0.15900969505310059, 'global_step': 17891, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6535845398902893, 'train/loss': 1.745597243309021, 'train/bleu': 32.194458606932685, 'validation/accuracy': 0.6564456820487976, 'validation/loss': 1.7223609685897827, 'validation/bleu': 28.30922073715908, 'validation/num_examples': 3000, 'test/accuracy': 0.6648190021514893, 'test/loss': 1.6608096361160278, 'test/bleu': 27.30953802694152, 'test/num_examples': 3003, 'score': 7566.391624689102, 'total_duration': 13047.502650737762, 'accumulated_submission_time': 7566.391624689102, 'accumulated_eval_time': 5480.6617114543915, 'accumulated_logging_time': 0.1774442195892334, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0429 07:22:45.369349 139723888199488 submission_runner.py:581] Timing: 7566.391624689102
I0429 07:22:45.369403 139723888199488 submission_runner.py:582] ====================
I0429 07:22:45.369513 139723888199488 submission_runner.py:645] Final wmt score: 7566.391624689102
