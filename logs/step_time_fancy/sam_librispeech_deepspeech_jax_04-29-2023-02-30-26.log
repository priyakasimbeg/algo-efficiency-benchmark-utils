python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_04-29-2023-02-30-26.log
I0429 02:30:47.869281 139853969647424 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax.
I0429 02:30:47.946303 139853969647424 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 02:30:48.811822 139853969647424 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0429 02:30:48.812442 139853969647424 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 02:30:48.818305 139853969647424 submission_runner.py:538] Using RNG seed 2399746712
I0429 02:30:51.609984 139853969647424 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 02:30:51.610185 139853969647424 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1.
I0429 02:30:51.610365 139853969647424 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1/hparams.json.
I0429 02:30:51.759435 139853969647424 submission_runner.py:241] Initializing dataset.
I0429 02:30:51.759642 139853969647424 submission_runner.py:248] Initializing model.
I0429 02:31:10.084887 139853969647424 submission_runner.py:258] Initializing optimizer.
I0429 02:31:10.795782 139853969647424 submission_runner.py:265] Initializing metrics bundle.
I0429 02:31:10.795988 139853969647424 submission_runner.py:282] Initializing checkpoint and logger.
I0429 02:31:10.796997 139853969647424 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0429 02:31:10.797288 139853969647424 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 02:31:10.797353 139853969647424 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 02:31:11.716789 139853969647424 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0429 02:31:11.717722 139853969647424 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0429 02:31:11.723997 139853969647424 submission_runner.py:318] Starting training loop.
I0429 02:31:11.937859 139853969647424 input_pipeline.py:20] Loading split = train-clean-100
I0429 02:31:11.976945 139853969647424 input_pipeline.py:20] Loading split = train-clean-360
I0429 02:31:12.322044 139853969647424 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0429 02:32:29.617745 139678871185152 logging_writer.py:48] [0] global_step=0, grad_norm=24.597780227661133, loss=33.244834899902344
I0429 02:32:29.644310 139853969647424 spec.py:298] Evaluating on the training split.
I0429 02:32:29.785248 139853969647424 input_pipeline.py:20] Loading split = train-clean-100
I0429 02:32:29.993233 139853969647424 input_pipeline.py:20] Loading split = train-clean-360
I0429 02:32:30.302936 139853969647424 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0429 02:34:15.993015 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 02:34:16.110906 139853969647424 input_pipeline.py:20] Loading split = dev-clean
I0429 02:34:16.117137 139853969647424 input_pipeline.py:20] Loading split = dev-other
I0429 02:35:18.541650 139853969647424 spec.py:326] Evaluating on the test split.
I0429 02:35:18.661675 139853969647424 input_pipeline.py:20] Loading split = test-clean
I0429 02:35:57.829197 139853969647424 submission_runner.py:415] Time since start: 286.10s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.216946, dtype=float32), 'train/wer': 3.1251743956021074, 'validation/ctc_loss': DeviceArray(30.119068, dtype=float32), 'validation/wer': 2.882989705641154, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.278898, dtype=float32), 'test/wer': 2.935124814656836, 'test/num_examples': 2472, 'score': 77.92011642456055, 'total_duration': 286.10368633270264, 'accumulated_submission_time': 77.92011642456055, 'accumulated_eval_time': 208.18341088294983, 'accumulated_logging_time': 0}
I0429 02:35:57.851058 139675658352384 logging_writer.py:48] [1] accumulated_eval_time=208.183411, accumulated_logging_time=0, accumulated_submission_time=77.920116, global_step=1, preemption_count=0, score=77.920116, test/ctc_loss=30.278898239135742, test/num_examples=2472, test/wer=2.935125, total_duration=286.103686, train/ctc_loss=31.21694564819336, train/wer=3.125174, validation/ctc_loss=30.119068145751953, validation/num_examples=5348, validation/wer=2.882990
I0429 02:40:27.517209 139679594768128 logging_writer.py:48] [100] global_step=100, grad_norm=6.401789665222168, loss=8.332221031188965
I0429 02:44:18.703964 139679603160832 logging_writer.py:48] [200] global_step=200, grad_norm=1.3770424127578735, loss=6.453773498535156
I0429 02:48:14.241146 139679594768128 logging_writer.py:48] [300] global_step=300, grad_norm=1.0227587223052979, loss=6.0455217361450195
I0429 02:52:09.988945 139679603160832 logging_writer.py:48] [400] global_step=400, grad_norm=0.48213672637939453, loss=5.919857025146484
I0429 02:56:05.555835 139679594768128 logging_writer.py:48] [500] global_step=500, grad_norm=1.4246610403060913, loss=5.85352897644043
I0429 03:00:00.974728 139679603160832 logging_writer.py:48] [600] global_step=600, grad_norm=0.40366801619529724, loss=5.806840896606445
I0429 03:03:56.771572 139679594768128 logging_writer.py:48] [700] global_step=700, grad_norm=0.5203934907913208, loss=5.748445510864258
I0429 03:07:52.008072 139679603160832 logging_writer.py:48] [800] global_step=800, grad_norm=0.5111376047134399, loss=5.673714637756348
I0429 03:11:47.220645 139679594768128 logging_writer.py:48] [900] global_step=900, grad_norm=0.7194073796272278, loss=5.560608863830566
I0429 03:15:42.276452 139679603160832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8628904223442078, loss=5.39795446395874
I0429 03:15:58.475658 139853969647424 spec.py:298] Evaluating on the training split.
I0429 03:16:30.006470 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 03:17:06.806574 139853969647424 spec.py:326] Evaluating on the test split.
I0429 03:17:25.923080 139853969647424 submission_runner.py:415] Time since start: 2774.20s, 	Step: 1008, 	{'train/ctc_loss': DeviceArray(5.9721546, dtype=float32), 'train/wer': 0.9379672921258059, 'validation/ctc_loss': DeviceArray(6.021142, dtype=float32), 'validation/wer': 0.892338565736283, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9237375, dtype=float32), 'test/wer': 0.8931814027176893, 'test/num_examples': 2472, 'score': 2478.5178701877594, 'total_duration': 2774.197058916092, 'accumulated_submission_time': 2478.5178701877594, 'accumulated_eval_time': 295.6288363933563, 'accumulated_logging_time': 0.03359484672546387}
I0429 03:17:25.942651 139679787800320 logging_writer.py:48] [1008] accumulated_eval_time=295.628836, accumulated_logging_time=0.033595, accumulated_submission_time=2478.517870, global_step=1008, preemption_count=0, score=2478.517870, test/ctc_loss=5.923737525939941, test/num_examples=2472, test/wer=0.893181, total_duration=2774.197059, train/ctc_loss=5.97215461730957, train/wer=0.937967, validation/ctc_loss=6.02114200592041, validation/num_examples=5348, validation/wer=0.892339
I0429 03:21:02.664424 139677654832896 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8789140582084656, loss=5.174179553985596
I0429 03:24:50.198152 139677424260864 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8120245337486267, loss=4.895873069763184
I0429 03:28:43.863856 139677654832896 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.658477783203125, loss=4.657825946807861
I0429 03:32:39.568159 139677424260864 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9847744703292847, loss=4.41581916809082
I0429 03:36:35.359747 139677654832896 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.2162717580795288, loss=4.2098612785339355
I0429 03:40:30.621542 139677424260864 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.4748620986938477, loss=3.9692206382751465
I0429 03:44:24.441663 139677654832896 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8970891237258911, loss=3.838374614715576
I0429 03:48:16.228826 139677424260864 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.2680152654647827, loss=3.724823474884033
I0429 03:52:06.997314 139677654832896 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.419455647468567, loss=3.5604896545410156
I0429 03:55:57.518854 139677424260864 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.592237114906311, loss=3.4799489974975586
I0429 03:57:26.980950 139853969647424 spec.py:298] Evaluating on the training split.
I0429 03:57:58.836069 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 03:58:35.694910 139853969647424 spec.py:326] Evaluating on the test split.
I0429 03:58:54.690039 139853969647424 submission_runner.py:415] Time since start: 5262.96s, 	Step: 2040, 	{'train/ctc_loss': DeviceArray(5.8819146, dtype=float32), 'train/wer': 0.9285992062435074, 'validation/ctc_loss': DeviceArray(5.8678126, dtype=float32), 'validation/wer': 0.8896660845738984, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.720955, dtype=float32), 'test/wer': 0.8895862531229054, 'test/num_examples': 2472, 'score': 4879.528731584549, 'total_duration': 5262.963617563248, 'accumulated_submission_time': 4879.528731584549, 'accumulated_eval_time': 383.3355960845947, 'accumulated_logging_time': 0.06422901153564453}
I0429 03:58:54.712285 139677654832896 logging_writer.py:48] [2040] accumulated_eval_time=383.335596, accumulated_logging_time=0.064229, accumulated_submission_time=4879.528732, global_step=2040, preemption_count=0, score=4879.528732, test/ctc_loss=5.720954895019531, test/num_examples=2472, test/wer=0.889586, total_duration=5262.963618, train/ctc_loss=5.8819146156311035, train/wer=0.928599, validation/ctc_loss=5.867812633514404, validation/num_examples=5348, validation/wer=0.889666
I0429 04:01:16.316509 139677654832896 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.5758466720581055, loss=3.4194469451904297
I0429 04:05:02.274428 139677424260864 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.535157322883606, loss=3.254080295562744
I0429 04:08:54.425570 139677654832896 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.412105679512024, loss=3.251246213912964
I0429 04:12:47.931590 139677424260864 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.543158769607544, loss=3.140639066696167
I0429 04:16:41.985622 139677654832896 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.4713374376296997, loss=3.1040000915527344
I0429 04:20:34.447749 139677424260864 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.4914941787719727, loss=2.980382204055786
I0429 04:24:29.191035 139677654832896 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.3328735828399658, loss=2.9479267597198486
I0429 04:28:22.884416 139677424260864 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.3831615447998047, loss=2.9084575176239014
I0429 04:32:16.476898 139677654832896 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.5619370937347412, loss=2.852574348449707
I0429 04:36:09.456904 139677424260864 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.3798327445983887, loss=2.771956443786621
I0429 04:38:56.638961 139853969647424 spec.py:298] Evaluating on the training split.
I0429 04:39:30.588320 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 04:40:07.739402 139853969647424 spec.py:326] Evaluating on the test split.
I0429 04:40:26.875814 139853969647424 submission_runner.py:415] Time since start: 7755.15s, 	Step: 3073, 	{'train/ctc_loss': DeviceArray(4.7688885, dtype=float32), 'train/wer': 0.888224720759471, 'validation/ctc_loss': DeviceArray(4.9171243, dtype=float32), 'validation/wer': 0.867794190006657, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.6572337, dtype=float32), 'test/wer': 0.8576158267828489, 'test/num_examples': 2472, 'score': 7281.427578926086, 'total_duration': 7755.149785041809, 'accumulated_submission_time': 7281.427578926086, 'accumulated_eval_time': 473.5705223083496, 'accumulated_logging_time': 0.09780502319335938}
I0429 04:40:26.895107 139679204120320 logging_writer.py:48] [3073] accumulated_eval_time=473.570522, accumulated_logging_time=0.097805, accumulated_submission_time=7281.427579, global_step=3073, preemption_count=0, score=7281.427579, test/ctc_loss=4.657233715057373, test/num_examples=2472, test/wer=0.857616, total_duration=7755.149785, train/ctc_loss=4.768888473510742, train/wer=0.888225, validation/ctc_loss=4.917124271392822, validation/num_examples=5348, validation/wer=0.867794
I0429 04:41:34.325573 139679204120320 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.7387052774429321, loss=2.7172229290008545
I0429 04:45:20.499332 139679195727616 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.6559373140335083, loss=2.736982583999634
I0429 04:49:08.619001 139679204120320 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.8712774515151978, loss=2.72896409034729
I0429 04:52:56.250670 139679195727616 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.5025553703308105, loss=2.6741533279418945
I0429 04:56:42.729759 139679204120320 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.3301992416381836, loss=2.564197063446045
I0429 05:00:29.585186 139679195727616 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.6545835733413696, loss=2.4558277130126953
I0429 05:04:23.250585 139679204120320 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.312723159790039, loss=2.517296552658081
I0429 05:08:17.172350 139679195727616 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.6054792404174805, loss=2.4398608207702637
I0429 05:12:11.008045 139679204120320 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.6354873180389404, loss=2.4360597133636475
I0429 05:16:04.654990 139679195727616 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.5014474391937256, loss=2.4550704956054688
I0429 05:19:55.780503 139679204120320 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.6704208850860596, loss=2.40798020362854
I0429 05:20:27.448037 139853969647424 spec.py:298] Evaluating on the training split.
I0429 05:21:13.830206 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 05:21:54.954415 139853969647424 spec.py:326] Evaluating on the test split.
I0429 05:22:16.363453 139853969647424 submission_runner.py:415] Time since start: 10264.64s, 	Step: 4115, 	{'train/ctc_loss': DeviceArray(2.3741717, dtype=float32), 'train/wer': 0.5461204955544617, 'validation/ctc_loss': DeviceArray(2.7713804, dtype=float32), 'validation/wer': 0.5942459647464037, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.2885396, dtype=float32), 'test/wer': 0.5288729104462454, 'test/num_examples': 2472, 'score': 9681.954368114471, 'total_duration': 10264.637373447418, 'accumulated_submission_time': 9681.954368114471, 'accumulated_eval_time': 582.4839155673981, 'accumulated_logging_time': 0.12741661071777344}
I0429 05:22:16.383164 139679787800320 logging_writer.py:48] [4115] accumulated_eval_time=582.483916, accumulated_logging_time=0.127417, accumulated_submission_time=9681.954368, global_step=4115, preemption_count=0, score=9681.954368, test/ctc_loss=2.2885396480560303, test/num_examples=2472, test/wer=0.528873, total_duration=10264.637373, train/ctc_loss=2.374171733856201, train/wer=0.546120, validation/ctc_loss=2.7713804244995117, validation/num_examples=5348, validation/wer=0.594246
I0429 05:25:38.573246 139677654832896 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.5348336696624756, loss=2.2939770221710205
I0429 05:29:29.743096 139677424260864 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.9387346506118774, loss=2.4283580780029297
I0429 05:33:20.967897 139677654832896 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.6485249996185303, loss=2.3774774074554443
I0429 05:37:13.993833 139677424260864 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.4201831817626953, loss=2.287349224090576
I0429 05:41:00.853744 139677654832896 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.4900873899459839, loss=2.20965313911438
I0429 05:44:48.992805 139677424260864 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.6406317949295044, loss=2.228978157043457
I0429 05:48:41.803703 139677654832896 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.3764901161193848, loss=2.267263650894165
I0429 05:52:33.044522 139677424260864 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.6152600049972534, loss=2.2827258110046387
I0429 05:56:20.889260 139677654832896 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.2033979892730713, loss=2.2469489574432373
I0429 06:00:08.958720 139677424260864 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.3014030456542969, loss=2.1262238025665283
I0429 06:02:17.364071 139853969647424 spec.py:298] Evaluating on the training split.
I0429 06:03:05.674863 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 06:03:48.372622 139853969647424 spec.py:326] Evaluating on the test split.
I0429 06:04:10.134464 139853969647424 submission_runner.py:415] Time since start: 12778.41s, 	Step: 5156, 	{'train/ctc_loss': DeviceArray(0.9845531, dtype=float32), 'train/wer': 0.3123844832459936, 'validation/ctc_loss': DeviceArray(1.3601289, dtype=float32), 'validation/wer': 0.3768680836283997, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.9794951, dtype=float32), 'test/wer': 0.30268315966932746, 'test/num_examples': 2472, 'score': 12082.905641317368, 'total_duration': 12778.406615972519, 'accumulated_submission_time': 12082.905641317368, 'accumulated_eval_time': 695.2507085800171, 'accumulated_logging_time': 0.16065645217895508}
I0429 06:04:10.156772 139677654832896 logging_writer.py:48] [5156] accumulated_eval_time=695.250709, accumulated_logging_time=0.160656, accumulated_submission_time=12082.905641, global_step=5156, preemption_count=0, score=12082.905641, test/ctc_loss=0.979495108127594, test/num_examples=2472, test/wer=0.302683, total_duration=12778.406616, train/ctc_loss=0.9845530986785889, train/wer=0.312384, validation/ctc_loss=1.3601288795471191, validation/num_examples=5348, validation/wer=0.376868
I0429 06:05:53.247923 139677424260864 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.3736616373062134, loss=2.1187827587127686
I0429 06:09:41.759865 139677654832896 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.4012343883514404, loss=2.1445021629333496
I0429 06:13:33.680762 139677424260864 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.5558725595474243, loss=2.192509412765503
I0429 06:17:26.802630 139677654832896 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.5836588144302368, loss=2.0460996627807617
I0429 06:21:15.073498 139677424260864 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.2319108247756958, loss=2.09405779838562
I0429 06:25:03.574208 139677654832896 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.3026150465011597, loss=2.0681354999542236
I0429 06:28:52.063872 139677424260864 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.660374641418457, loss=2.0114798545837402
I0429 06:32:40.464672 139677654832896 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.2316712141036987, loss=2.043949842453003
I0429 06:36:28.991946 139677424260864 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.6224164962768555, loss=1.9856818914413452
I0429 06:40:17.322128 139677654832896 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.4970921277999878, loss=1.9937334060668945
I0429 06:44:08.945979 139677654832896 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.3605903387069702, loss=2.0063083171844482
I0429 06:44:11.058898 139853969647424 spec.py:298] Evaluating on the training split.
I0429 06:44:58.990320 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 06:45:41.837956 139853969647424 spec.py:326] Evaluating on the test split.
I0429 06:46:03.957074 139853969647424 submission_runner.py:415] Time since start: 15292.23s, 	Step: 6202, 	{'train/ctc_loss': DeviceArray(0.7498109, dtype=float32), 'train/wer': 0.2557895129927288, 'validation/ctc_loss': DeviceArray(1.0954155, dtype=float32), 'validation/wer': 0.3174946212698627, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7536223, dtype=float32), 'test/wer': 0.2477809599252534, 'test/num_examples': 2472, 'score': 14483.768786668777, 'total_duration': 15292.230764389038, 'accumulated_submission_time': 14483.768786668777, 'accumulated_eval_time': 808.1466534137726, 'accumulated_logging_time': 0.20253324508666992}
I0429 06:46:03.978232 139679357720320 logging_writer.py:48] [6202] accumulated_eval_time=808.146653, accumulated_logging_time=0.202533, accumulated_submission_time=14483.768787, global_step=6202, preemption_count=0, score=14483.768787, test/ctc_loss=0.75362229347229, test/num_examples=2472, test/wer=0.247781, total_duration=15292.230764, train/ctc_loss=0.7498108744621277, train/wer=0.255790, validation/ctc_loss=1.095415472984314, validation/num_examples=5348, validation/wer=0.317495
I0429 06:49:46.695746 139679349327616 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.289306879043579, loss=2.023029327392578
I0429 06:53:31.641349 139679357720320 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.2119290828704834, loss=1.98390531539917
I0429 06:57:16.642251 139679349327616 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3369979858398438, loss=1.9825314283370972
I0429 07:01:03.722072 139679357720320 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.3801753520965576, loss=1.9652446508407593
I0429 07:04:51.441966 139679349327616 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.441825270652771, loss=2.016608238220215
I0429 07:08:38.352544 139679357720320 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.6987909078598022, loss=1.9852269887924194
I0429 07:12:24.267112 139679349327616 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.4642313718795776, loss=1.9908647537231445
I0429 07:16:11.590354 139679357720320 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.318493366241455, loss=1.9347021579742432
I0429 07:20:00.007647 139679349327616 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.361707329750061, loss=1.8945118188858032
I0429 07:23:52.227606 139679357720320 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.3562040328979492, loss=1.9233691692352295
I0429 07:26:04.109465 139853969647424 spec.py:298] Evaluating on the training split.
I0429 07:26:52.983316 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 07:27:36.593853 139853969647424 spec.py:326] Evaluating on the test split.
I0429 07:27:59.769933 139853969647424 submission_runner.py:415] Time since start: 17808.04s, 	Step: 7256, 	{'train/ctc_loss': DeviceArray(0.6382261, dtype=float32), 'train/wer': 0.22406568395975582, 'validation/ctc_loss': DeviceArray(1.025277, dtype=float32), 'validation/wer': 0.3014983260812936, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6822709, dtype=float32), 'test/wer': 0.2304958056588061, 'test/num_examples': 2472, 'score': 16883.8701171875, 'total_duration': 17808.043536663055, 'accumulated_submission_time': 16883.8701171875, 'accumulated_eval_time': 923.8049159049988, 'accumulated_logging_time': 0.23702549934387207}
I0429 07:27:59.793055 139679357720320 logging_writer.py:48] [7256] accumulated_eval_time=923.804916, accumulated_logging_time=0.237025, accumulated_submission_time=16883.870117, global_step=7256, preemption_count=0, score=16883.870117, test/ctc_loss=0.682270884513855, test/num_examples=2472, test/wer=0.230496, total_duration=17808.043537, train/ctc_loss=0.6382260918617249, train/wer=0.224066, validation/ctc_loss=1.025277018547058, validation/num_examples=5348, validation/wer=0.301498
I0429 07:29:42.108138 139679349327616 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.3740978240966797, loss=1.865809679031372
I0429 07:33:28.806399 139679357720320 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.4760112762451172, loss=1.8350780010223389
I0429 07:37:18.615935 139679349327616 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.2409220933914185, loss=1.8914868831634521
I0429 07:41:12.676182 139679357720320 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.4070945978164673, loss=1.9281560182571411
I0429 07:45:06.595102 139679349327616 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.1133785247802734, loss=1.8826112747192383
I0429 07:49:00.448822 139679357720320 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.5559265613555908, loss=1.9224340915679932
I0429 07:52:54.024271 139679349327616 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.3458356857299805, loss=1.8523571491241455
I0429 07:56:44.991097 139679357720320 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.313388705253601, loss=1.9357757568359375
I0429 08:00:31.075989 139679349327616 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.3438053131103516, loss=1.8675810098648071
I0429 08:04:18.752923 139679357720320 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2790803909301758, loss=1.8351956605911255
I0429 08:08:00.777210 139853969647424 spec.py:298] Evaluating on the training split.
I0429 08:08:51.149873 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 08:09:34.736065 139853969647424 spec.py:326] Evaluating on the test split.
I0429 08:09:57.058768 139853969647424 submission_runner.py:415] Time since start: 20325.33s, 	Step: 8297, 	{'train/ctc_loss': DeviceArray(0.52471966, dtype=float32), 'train/wer': 0.18591227020983384, 'validation/ctc_loss': DeviceArray(0.9234388, dtype=float32), 'validation/wer': 0.27049947418691933, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59733796, dtype=float32), 'test/wer': 0.1958036276481222, 'test/num_examples': 2472, 'score': 19284.824868917465, 'total_duration': 20325.332752227783, 'accumulated_submission_time': 19284.824868917465, 'accumulated_eval_time': 1040.0847752094269, 'accumulated_logging_time': 0.2724764347076416}
I0429 08:09:57.079972 139679787800320 logging_writer.py:48] [8297] accumulated_eval_time=1040.084775, accumulated_logging_time=0.272476, accumulated_submission_time=19284.824869, global_step=8297, preemption_count=0, score=19284.824869, test/ctc_loss=0.5973379611968994, test/num_examples=2472, test/wer=0.195804, total_duration=20325.332752, train/ctc_loss=0.5247196555137634, train/wer=0.185912, validation/ctc_loss=0.9234387874603271, validation/num_examples=5348, validation/wer=0.270499
I0429 08:10:06.482966 139679779407616 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.4250518083572388, loss=1.7954812049865723
I0429 08:13:58.343677 139679787800320 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.2905632257461548, loss=1.8232241868972778
I0429 08:17:45.011363 139679779407616 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0887949466705322, loss=1.7859221696853638
I0429 08:21:31.274477 139679787800320 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.7213788032531738, loss=1.8337901830673218
I0429 08:25:20.263210 139679779407616 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.410841464996338, loss=1.7413498163223267
I0429 08:29:11.145061 139679787800320 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.5317212343215942, loss=1.8730356693267822
I0429 08:32:58.927582 139679779407616 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0223356485366821, loss=1.7740094661712646
I0429 08:36:44.777813 139679787800320 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4977233409881592, loss=1.822634220123291
I0429 08:40:32.126297 139679779407616 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.449996829032898, loss=1.7700268030166626
I0429 08:44:19.473772 139679787800320 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.4079499244689941, loss=1.733924388885498
I0429 08:48:16.130065 139677654832896 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.1595327854156494, loss=1.7435986995697021
I0429 08:49:58.687728 139853969647424 spec.py:298] Evaluating on the training split.
I0429 08:50:47.597295 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 08:51:32.534342 139853969647424 spec.py:326] Evaluating on the test split.
I0429 08:51:54.785309 139853969647424 submission_runner.py:415] Time since start: 22843.06s, 	Step: 9345, 	{'train/ctc_loss': DeviceArray(0.47792998, dtype=float32), 'train/wer': 0.17167192462586917, 'validation/ctc_loss': DeviceArray(0.8800886, dtype=float32), 'validation/wer': 0.2592499686441741, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5656871, dtype=float32), 'test/wer': 0.188308654764081, 'test/num_examples': 2472, 'score': 21686.40473508835, 'total_duration': 22843.05931711197, 'accumulated_submission_time': 21686.40473508835, 'accumulated_eval_time': 1156.1804468631744, 'accumulated_logging_time': 0.3053867816925049}
I0429 08:51:54.806775 139677654832896 logging_writer.py:48] [9345] accumulated_eval_time=1156.180447, accumulated_logging_time=0.305387, accumulated_submission_time=21686.404735, global_step=9345, preemption_count=0, score=21686.404735, test/ctc_loss=0.5656871199607849, test/num_examples=2472, test/wer=0.188309, total_duration=22843.059317, train/ctc_loss=0.4779299795627594, train/wer=0.171672, validation/ctc_loss=0.8800886273384094, validation/num_examples=5348, validation/wer=0.259250
I0429 08:54:05.320961 139677424260864 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.4318122863769531, loss=1.6642448902130127
I0429 08:57:56.941372 139677654832896 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.2625792026519775, loss=1.7722373008728027
I0429 09:01:45.455292 139677424260864 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.3676223754882812, loss=1.7674493789672852
I0429 09:05:31.374586 139677654832896 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.6545357704162598, loss=1.753935694694519
I0429 09:09:16.739507 139677424260864 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.5041565895080566, loss=1.7536317110061646
I0429 09:13:03.516263 139677654832896 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.4118949174880981, loss=1.7571152448654175
I0429 09:16:49.903542 139677424260864 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.5263320207595825, loss=1.7896796464920044
I0429 09:20:37.421490 139677654832896 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.4105182886123657, loss=1.6688989400863647
I0429 09:24:24.983080 139677424260864 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.2132067680358887, loss=1.6956201791763306
I0429 09:28:15.264930 139677654832896 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.208724021911621, loss=1.721425175666809
I0429 09:31:56.625105 139853969647424 spec.py:298] Evaluating on the training split.
I0429 09:32:45.301289 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 09:33:29.396093 139853969647424 spec.py:326] Evaluating on the test split.
I0429 09:33:51.640801 139853969647424 submission_runner.py:415] Time since start: 25359.91s, 	Step: 10399, 	{'train/ctc_loss': DeviceArray(0.45261195, dtype=float32), 'train/wer': 0.16573441949513856, 'validation/ctc_loss': DeviceArray(0.8285316, dtype=float32), 'validation/wer': 0.24513502301035225, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5179954, dtype=float32), 'test/wer': 0.17384681006641886, 'test/num_examples': 2472, 'score': 24088.194754123688, 'total_duration': 25359.914532899857, 'accumulated_submission_time': 24088.194754123688, 'accumulated_eval_time': 1271.1939685344696, 'accumulated_logging_time': 0.33860015869140625}
I0429 09:33:51.662455 139677654832896 logging_writer.py:48] [10399] accumulated_eval_time=1271.193969, accumulated_logging_time=0.338600, accumulated_submission_time=24088.194754, global_step=10399, preemption_count=0, score=24088.194754, test/ctc_loss=0.5179954171180725, test/num_examples=2472, test/wer=0.173847, total_duration=25359.914533, train/ctc_loss=0.4526119530200958, train/wer=0.165734, validation/ctc_loss=0.8285316228866577, validation/num_examples=5348, validation/wer=0.245135
I0429 09:33:56.279635 139677424260864 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.2391737699508667, loss=1.6834933757781982
I0429 09:37:43.247370 139677654832896 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.2408344745635986, loss=1.7449121475219727
I0429 09:41:33.210516 139677424260864 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.7812395095825195, loss=1.810680627822876
I0429 09:45:27.411643 139677654832896 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.1168489456176758, loss=1.6960774660110474
I0429 09:49:21.794816 139677424260864 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.2866191864013672, loss=1.7305893898010254
I0429 09:53:16.574082 139677654832896 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5222569704055786, loss=1.6947230100631714
I0429 09:57:10.575227 139677424260864 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.3453195095062256, loss=1.7258796691894531
I0429 10:01:04.735173 139677654832896 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.4319965839385986, loss=1.7232130765914917
I0429 10:04:58.649121 139677424260864 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.2714579105377197, loss=1.6638877391815186
I0429 10:08:51.631796 139677654832896 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.403151273727417, loss=1.71132493019104
I0429 10:12:43.070210 139677654832896 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.2165627479553223, loss=1.6555774211883545
I0429 10:13:52.720901 139853969647424 spec.py:298] Evaluating on the training split.
I0429 10:14:41.829860 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 10:15:25.594735 139853969647424 spec.py:326] Evaluating on the test split.
I0429 10:15:47.862399 139853969647424 submission_runner.py:415] Time since start: 27876.14s, 	Step: 11432, 	{'train/ctc_loss': DeviceArray(0.43810198, dtype=float32), 'train/wer': 0.1530923609825749, 'validation/ctc_loss': DeviceArray(0.7952574, dtype=float32), 'validation/wer': 0.23512045461123598, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.48987395, dtype=float32), 'test/wer': 0.16253326021164666, 'test/num_examples': 2472, 'score': 26489.22334074974, 'total_duration': 27876.13638162613, 'accumulated_submission_time': 26489.22334074974, 'accumulated_eval_time': 1386.3335061073303, 'accumulated_logging_time': 0.3733692169189453}
I0429 10:15:47.883654 139677654832896 logging_writer.py:48] [11432] accumulated_eval_time=1386.333506, accumulated_logging_time=0.373369, accumulated_submission_time=26489.223341, global_step=11432, preemption_count=0, score=26489.223341, test/ctc_loss=0.4898739457130432, test/num_examples=2472, test/wer=0.162533, total_duration=27876.136382, train/ctc_loss=0.43810197710990906, train/wer=0.153092, validation/ctc_loss=0.7952573895454407, validation/num_examples=5348, validation/wer=0.235120
I0429 10:18:28.551992 139677424260864 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.4274260997772217, loss=1.694809079170227
I0429 10:22:19.280844 139677654832896 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.2475864887237549, loss=1.6918435096740723
I0429 10:26:07.218993 139677424260864 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.1346760988235474, loss=1.5594587326049805
I0429 10:29:55.343944 139677654832896 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.2015650272369385, loss=1.681490421295166
I0429 10:33:42.967777 139677424260864 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.2668638229370117, loss=1.8896148204803467
I0429 10:37:30.521595 139677654832896 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.099373698234558, loss=1.7416363954544067
I0429 10:41:18.135850 139677424260864 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.2194547653198242, loss=1.7494877576828003
I0429 10:45:05.317611 139677654832896 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9881885051727295, loss=1.7139720916748047
I0429 10:48:52.093544 139677424260864 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.1569575071334839, loss=1.6809873580932617
I0429 10:52:49.297720 139677654832896 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0287874937057495, loss=1.6394093036651611
I0429 10:55:48.904082 139853969647424 spec.py:298] Evaluating on the training split.
I0429 10:56:38.488913 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 10:57:22.142848 139853969647424 spec.py:326] Evaluating on the test split.
I0429 10:57:44.294258 139853969647424 submission_runner.py:415] Time since start: 30392.57s, 	Step: 12478, 	{'train/ctc_loss': DeviceArray(0.4510516, dtype=float32), 'train/wer': 0.15324346426781332, 'validation/ctc_loss': DeviceArray(0.78314805, dtype=float32), 'validation/wer': 0.22763364817798531, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.48461166, dtype=float32), 'test/wer': 0.16070521804480734, 'test/num_examples': 2472, 'score': 28890.21581840515, 'total_duration': 30392.567973136902, 'accumulated_submission_time': 28890.21581840515, 'accumulated_eval_time': 1501.7214703559875, 'accumulated_logging_time': 0.40641164779663086}
I0429 10:57:44.315633 139677654832896 logging_writer.py:48] [12478] accumulated_eval_time=1501.721470, accumulated_logging_time=0.406412, accumulated_submission_time=28890.215818, global_step=12478, preemption_count=0, score=28890.215818, test/ctc_loss=0.4846116602420807, test/num_examples=2472, test/wer=0.160705, total_duration=30392.567973, train/ctc_loss=0.45105159282684326, train/wer=0.153243, validation/ctc_loss=0.7831480503082275, validation/num_examples=5348, validation/wer=0.227634
I0429 10:58:37.990131 139677424260864 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.202263593673706, loss=1.643316626548767
I0429 11:02:26.385869 139677654832896 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9932658076286316, loss=1.6381934881210327
I0429 11:06:14.733140 139677424260864 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.6581135988235474, loss=1.6339420080184937
I0429 11:10:03.015036 139677654832896 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.3230082988739014, loss=1.694482684135437
I0429 11:13:54.242451 139677424260864 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.27584969997406, loss=1.6594038009643555
I0429 11:17:43.836014 139677654832896 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.3089661598205566, loss=1.6671569347381592
I0429 11:21:33.119856 139677424260864 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.3366669416427612, loss=1.6634024381637573
I0429 11:25:22.573738 139677654832896 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.127406120300293, loss=1.679952621459961
I0429 11:29:15.207711 139677424260864 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.2312144041061401, loss=1.6580586433410645
I0429 11:33:12.399191 139677654832896 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.3159687519073486, loss=1.6228711605072021
I0429 11:37:05.648710 139677424260864 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3717446327209473, loss=1.65287446975708
I0429 11:37:45.138464 139853969647424 spec.py:298] Evaluating on the training split.
I0429 11:38:35.619167 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 11:39:18.916823 139853969647424 spec.py:326] Evaluating on the test split.
I0429 11:39:41.121392 139853969647424 submission_runner.py:415] Time since start: 32909.40s, 	Step: 13518, 	{'train/ctc_loss': DeviceArray(0.41377082, dtype=float32), 'train/wer': 0.1446084988167507, 'validation/ctc_loss': DeviceArray(0.7469902, dtype=float32), 'validation/wer': 0.21898908817258247, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45953467, dtype=float32), 'test/wer': 0.15219466617918875, 'test/num_examples': 2472, 'score': 31291.008981466293, 'total_duration': 32909.39537501335, 'accumulated_submission_time': 31291.008981466293, 'accumulated_eval_time': 1617.7024574279785, 'accumulated_logging_time': 0.44039130210876465}
I0429 11:39:41.142975 139679204120320 logging_writer.py:48] [13518] accumulated_eval_time=1617.702457, accumulated_logging_time=0.440391, accumulated_submission_time=31291.008981, global_step=13518, preemption_count=0, score=31291.008981, test/ctc_loss=0.4595346748828888, test/num_examples=2472, test/wer=0.152195, total_duration=32909.395375, train/ctc_loss=0.4137708246707916, train/wer=0.144608, validation/ctc_loss=0.7469902038574219, validation/num_examples=5348, validation/wer=0.218989
I0429 11:42:49.756985 139679195727616 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.4077156782150269, loss=1.6248173713684082
I0429 11:46:36.304052 139679204120320 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.2564353942871094, loss=1.714716911315918
I0429 11:50:22.438169 139679195727616 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1613662242889404, loss=1.6908485889434814
I0429 11:54:08.427749 139679204120320 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3644179105758667, loss=1.5886881351470947
I0429 11:57:57.455985 139679195727616 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6002510786056519, loss=1.6424756050109863
I0429 12:01:46.929194 139679204120320 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.4606988430023193, loss=1.6648138761520386
I0429 12:05:36.189010 139679195727616 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.2738951444625854, loss=1.6123805046081543
I0429 12:09:24.164062 139679204120320 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.232757329940796, loss=1.6372278928756714
I0429 12:13:10.796611 139679195727616 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4032182693481445, loss=1.7266833782196045
I0429 12:17:00.489528 139679204120320 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4266607761383057, loss=1.6607704162597656
I0429 12:19:43.078457 139853969647424 spec.py:298] Evaluating on the training split.
I0429 12:20:32.715646 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 12:21:16.478939 139853969647424 spec.py:326] Evaluating on the test split.
I0429 12:21:38.849285 139853969647424 submission_runner.py:415] Time since start: 35427.12s, 	Step: 14573, 	{'train/ctc_loss': DeviceArray(0.3891327, dtype=float32), 'train/wer': 0.1367129766479456, 'validation/ctc_loss': DeviceArray(0.74100244, dtype=float32), 'validation/wer': 0.2170980906714006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45008618, dtype=float32), 'test/wer': 0.150610362967928, 'test/num_examples': 2472, 'score': 33692.91651511192, 'total_duration': 35427.1232612133, 'accumulated_submission_time': 33692.91651511192, 'accumulated_eval_time': 1733.4713237285614, 'accumulated_logging_time': 0.4732353687286377}
I0429 12:21:38.871772 139679357720320 logging_writer.py:48] [14573] accumulated_eval_time=1733.471324, accumulated_logging_time=0.473235, accumulated_submission_time=33692.916515, global_step=14573, preemption_count=0, score=33692.916515, test/ctc_loss=0.45008617639541626, test/num_examples=2472, test/wer=0.150610, total_duration=35427.123261, train/ctc_loss=0.38913270831108093, train/wer=0.136713, validation/ctc_loss=0.7410024404525757, validation/num_examples=5348, validation/wer=0.217098
I0429 12:22:44.246698 139679349327616 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.3081729412078857, loss=1.6607213020324707
I0429 12:26:37.630129 139679357720320 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.3849189281463623, loss=1.6508454084396362
I0429 12:30:30.814543 139679349327616 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.9213097095489502, loss=1.624243974685669
I0429 12:34:24.582096 139679357720320 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.760571002960205, loss=1.6058474779129028
I0429 12:38:11.158755 139679349327616 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.2651314735412598, loss=1.5923305749893188
I0429 12:41:58.140924 139679357720320 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5590518712997437, loss=1.7167391777038574
I0429 12:45:48.779199 139679349327616 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1431474685668945, loss=1.5839579105377197
I0429 12:49:40.940193 139679357720320 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.2373405694961548, loss=1.6399686336517334
I0429 12:53:28.044311 139679349327616 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.2050909996032715, loss=1.6082136631011963
I0429 12:57:21.406864 139679357720320 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.2541837692260742, loss=1.6636611223220825
I0429 13:01:10.851231 139679349327616 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.1570364236831665, loss=1.6549042463302612
I0429 13:01:40.468895 139853969647424 spec.py:298] Evaluating on the training split.
I0429 13:02:30.667649 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 13:03:14.658174 139853969647424 spec.py:326] Evaluating on the test split.
I0429 13:03:37.272972 139853969647424 submission_runner.py:415] Time since start: 37945.55s, 	Step: 15614, 	{'train/ctc_loss': DeviceArray(0.35884073, dtype=float32), 'train/wer': 0.12871488562560318, 'validation/ctc_loss': DeviceArray(0.718602, dtype=float32), 'validation/wer': 0.21240918870418432, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.43000782, dtype=float32), 'test/wer': 0.14409034590620112, 'test/num_examples': 2472, 'score': 36094.48369932175, 'total_duration': 37945.54689049721, 'accumulated_submission_time': 36094.48369932175, 'accumulated_eval_time': 1850.2733764648438, 'accumulated_logging_time': 0.5075218677520752}
I0429 13:03:37.296000 139679357720320 logging_writer.py:48] [15614] accumulated_eval_time=1850.273376, accumulated_logging_time=0.507522, accumulated_submission_time=36094.483699, global_step=15614, preemption_count=0, score=36094.483699, test/ctc_loss=0.43000781536102295, test/num_examples=2472, test/wer=0.144090, total_duration=37945.546890, train/ctc_loss=0.3588407337665558, train/wer=0.128715, validation/ctc_loss=0.7186020016670227, validation/num_examples=5348, validation/wer=0.212409
I0429 13:07:00.357152 139679349327616 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.148537278175354, loss=1.582894206047058
I0429 13:10:53.871734 139679357720320 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.3174419403076172, loss=1.6276010274887085
I0429 13:14:46.297015 139679349327616 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.3084980249404907, loss=1.6200321912765503
I0429 13:18:37.169190 139853969647424 spec.py:298] Evaluating on the training split.
I0429 13:19:27.671463 139853969647424 spec.py:310] Evaluating on the validation split.
I0429 13:20:11.786197 139853969647424 spec.py:326] Evaluating on the test split.
I0429 13:20:33.677592 139853969647424 submission_runner.py:415] Time since start: 38961.95s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.35746717, dtype=float32), 'train/wer': 0.12777306154176596, 'validation/ctc_loss': DeviceArray(0.7101746, dtype=float32), 'validation/wer': 0.2094086773630233, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4277598, dtype=float32), 'test/wer': 0.1419576300448886, 'test/num_examples': 2472, 'score': 36994.33605790138, 'total_duration': 38961.9516518116, 'accumulated_submission_time': 36994.33605790138, 'accumulated_eval_time': 1966.7799363136292, 'accumulated_logging_time': 0.5436201095581055}
I0429 13:20:33.699956 139679787800320 logging_writer.py:48] [16000] accumulated_eval_time=1966.779936, accumulated_logging_time=0.543620, accumulated_submission_time=36994.336058, global_step=16000, preemption_count=0, score=36994.336058, test/ctc_loss=0.4277597963809967, test/num_examples=2472, test/wer=0.141958, total_duration=38961.951652, train/ctc_loss=0.3574671745300293, train/wer=0.127773, validation/ctc_loss=0.7101746201515198, validation/num_examples=5348, validation/wer=0.209409
I0429 13:20:33.721262 139679779407616 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=36994.336058
I0429 13:20:33.887570 139853969647424 checkpoints.py:356] Saving checkpoint at step: 16000
I0429 13:20:34.589604 139853969647424 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0429 13:20:34.604165 139853969647424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0429 13:20:35.955432 139853969647424 submission_runner.py:578] Tuning trial 1/1
I0429 13:20:35.955825 139853969647424 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0429 13:20:35.967734 139853969647424 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.216946, dtype=float32), 'train/wer': 3.1251743956021074, 'validation/ctc_loss': DeviceArray(30.119068, dtype=float32), 'validation/wer': 2.882989705641154, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.278898, dtype=float32), 'test/wer': 2.935124814656836, 'test/num_examples': 2472, 'score': 77.92011642456055, 'total_duration': 286.10368633270264, 'accumulated_submission_time': 77.92011642456055, 'accumulated_eval_time': 208.18341088294983, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1008, {'train/ctc_loss': DeviceArray(5.9721546, dtype=float32), 'train/wer': 0.9379672921258059, 'validation/ctc_loss': DeviceArray(6.021142, dtype=float32), 'validation/wer': 0.892338565736283, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9237375, dtype=float32), 'test/wer': 0.8931814027176893, 'test/num_examples': 2472, 'score': 2478.5178701877594, 'total_duration': 2774.197058916092, 'accumulated_submission_time': 2478.5178701877594, 'accumulated_eval_time': 295.6288363933563, 'accumulated_logging_time': 0.03359484672546387, 'global_step': 1008, 'preemption_count': 0}), (2040, {'train/ctc_loss': DeviceArray(5.8819146, dtype=float32), 'train/wer': 0.9285992062435074, 'validation/ctc_loss': DeviceArray(5.8678126, dtype=float32), 'validation/wer': 0.8896660845738984, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.720955, dtype=float32), 'test/wer': 0.8895862531229054, 'test/num_examples': 2472, 'score': 4879.528731584549, 'total_duration': 5262.963617563248, 'accumulated_submission_time': 4879.528731584549, 'accumulated_eval_time': 383.3355960845947, 'accumulated_logging_time': 0.06422901153564453, 'global_step': 2040, 'preemption_count': 0}), (3073, {'train/ctc_loss': DeviceArray(4.7688885, dtype=float32), 'train/wer': 0.888224720759471, 'validation/ctc_loss': DeviceArray(4.9171243, dtype=float32), 'validation/wer': 0.867794190006657, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.6572337, dtype=float32), 'test/wer': 0.8576158267828489, 'test/num_examples': 2472, 'score': 7281.427578926086, 'total_duration': 7755.149785041809, 'accumulated_submission_time': 7281.427578926086, 'accumulated_eval_time': 473.5705223083496, 'accumulated_logging_time': 0.09780502319335938, 'global_step': 3073, 'preemption_count': 0}), (4115, {'train/ctc_loss': DeviceArray(2.3741717, dtype=float32), 'train/wer': 0.5461204955544617, 'validation/ctc_loss': DeviceArray(2.7713804, dtype=float32), 'validation/wer': 0.5942459647464037, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.2885396, dtype=float32), 'test/wer': 0.5288729104462454, 'test/num_examples': 2472, 'score': 9681.954368114471, 'total_duration': 10264.637373447418, 'accumulated_submission_time': 9681.954368114471, 'accumulated_eval_time': 582.4839155673981, 'accumulated_logging_time': 0.12741661071777344, 'global_step': 4115, 'preemption_count': 0}), (5156, {'train/ctc_loss': DeviceArray(0.9845531, dtype=float32), 'train/wer': 0.3123844832459936, 'validation/ctc_loss': DeviceArray(1.3601289, dtype=float32), 'validation/wer': 0.3768680836283997, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.9794951, dtype=float32), 'test/wer': 0.30268315966932746, 'test/num_examples': 2472, 'score': 12082.905641317368, 'total_duration': 12778.406615972519, 'accumulated_submission_time': 12082.905641317368, 'accumulated_eval_time': 695.2507085800171, 'accumulated_logging_time': 0.16065645217895508, 'global_step': 5156, 'preemption_count': 0}), (6202, {'train/ctc_loss': DeviceArray(0.7498109, dtype=float32), 'train/wer': 0.2557895129927288, 'validation/ctc_loss': DeviceArray(1.0954155, dtype=float32), 'validation/wer': 0.3174946212698627, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7536223, dtype=float32), 'test/wer': 0.2477809599252534, 'test/num_examples': 2472, 'score': 14483.768786668777, 'total_duration': 15292.230764389038, 'accumulated_submission_time': 14483.768786668777, 'accumulated_eval_time': 808.1466534137726, 'accumulated_logging_time': 0.20253324508666992, 'global_step': 6202, 'preemption_count': 0}), (7256, {'train/ctc_loss': DeviceArray(0.6382261, dtype=float32), 'train/wer': 0.22406568395975582, 'validation/ctc_loss': DeviceArray(1.025277, dtype=float32), 'validation/wer': 0.3014983260812936, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6822709, dtype=float32), 'test/wer': 0.2304958056588061, 'test/num_examples': 2472, 'score': 16883.8701171875, 'total_duration': 17808.043536663055, 'accumulated_submission_time': 16883.8701171875, 'accumulated_eval_time': 923.8049159049988, 'accumulated_logging_time': 0.23702549934387207, 'global_step': 7256, 'preemption_count': 0}), (8297, {'train/ctc_loss': DeviceArray(0.52471966, dtype=float32), 'train/wer': 0.18591227020983384, 'validation/ctc_loss': DeviceArray(0.9234388, dtype=float32), 'validation/wer': 0.27049947418691933, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59733796, dtype=float32), 'test/wer': 0.1958036276481222, 'test/num_examples': 2472, 'score': 19284.824868917465, 'total_duration': 20325.332752227783, 'accumulated_submission_time': 19284.824868917465, 'accumulated_eval_time': 1040.0847752094269, 'accumulated_logging_time': 0.2724764347076416, 'global_step': 8297, 'preemption_count': 0}), (9345, {'train/ctc_loss': DeviceArray(0.47792998, dtype=float32), 'train/wer': 0.17167192462586917, 'validation/ctc_loss': DeviceArray(0.8800886, dtype=float32), 'validation/wer': 0.2592499686441741, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5656871, dtype=float32), 'test/wer': 0.188308654764081, 'test/num_examples': 2472, 'score': 21686.40473508835, 'total_duration': 22843.05931711197, 'accumulated_submission_time': 21686.40473508835, 'accumulated_eval_time': 1156.1804468631744, 'accumulated_logging_time': 0.3053867816925049, 'global_step': 9345, 'preemption_count': 0}), (10399, {'train/ctc_loss': DeviceArray(0.45261195, dtype=float32), 'train/wer': 0.16573441949513856, 'validation/ctc_loss': DeviceArray(0.8285316, dtype=float32), 'validation/wer': 0.24513502301035225, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5179954, dtype=float32), 'test/wer': 0.17384681006641886, 'test/num_examples': 2472, 'score': 24088.194754123688, 'total_duration': 25359.914532899857, 'accumulated_submission_time': 24088.194754123688, 'accumulated_eval_time': 1271.1939685344696, 'accumulated_logging_time': 0.33860015869140625, 'global_step': 10399, 'preemption_count': 0}), (11432, {'train/ctc_loss': DeviceArray(0.43810198, dtype=float32), 'train/wer': 0.1530923609825749, 'validation/ctc_loss': DeviceArray(0.7952574, dtype=float32), 'validation/wer': 0.23512045461123598, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.48987395, dtype=float32), 'test/wer': 0.16253326021164666, 'test/num_examples': 2472, 'score': 26489.22334074974, 'total_duration': 27876.13638162613, 'accumulated_submission_time': 26489.22334074974, 'accumulated_eval_time': 1386.3335061073303, 'accumulated_logging_time': 0.3733692169189453, 'global_step': 11432, 'preemption_count': 0}), (12478, {'train/ctc_loss': DeviceArray(0.4510516, dtype=float32), 'train/wer': 0.15324346426781332, 'validation/ctc_loss': DeviceArray(0.78314805, dtype=float32), 'validation/wer': 0.22763364817798531, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.48461166, dtype=float32), 'test/wer': 0.16070521804480734, 'test/num_examples': 2472, 'score': 28890.21581840515, 'total_duration': 30392.567973136902, 'accumulated_submission_time': 28890.21581840515, 'accumulated_eval_time': 1501.7214703559875, 'accumulated_logging_time': 0.40641164779663086, 'global_step': 12478, 'preemption_count': 0}), (13518, {'train/ctc_loss': DeviceArray(0.41377082, dtype=float32), 'train/wer': 0.1446084988167507, 'validation/ctc_loss': DeviceArray(0.7469902, dtype=float32), 'validation/wer': 0.21898908817258247, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45953467, dtype=float32), 'test/wer': 0.15219466617918875, 'test/num_examples': 2472, 'score': 31291.008981466293, 'total_duration': 32909.39537501335, 'accumulated_submission_time': 31291.008981466293, 'accumulated_eval_time': 1617.7024574279785, 'accumulated_logging_time': 0.44039130210876465, 'global_step': 13518, 'preemption_count': 0}), (14573, {'train/ctc_loss': DeviceArray(0.3891327, dtype=float32), 'train/wer': 0.1367129766479456, 'validation/ctc_loss': DeviceArray(0.74100244, dtype=float32), 'validation/wer': 0.2170980906714006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45008618, dtype=float32), 'test/wer': 0.150610362967928, 'test/num_examples': 2472, 'score': 33692.91651511192, 'total_duration': 35427.1232612133, 'accumulated_submission_time': 33692.91651511192, 'accumulated_eval_time': 1733.4713237285614, 'accumulated_logging_time': 0.4732353687286377, 'global_step': 14573, 'preemption_count': 0}), (15614, {'train/ctc_loss': DeviceArray(0.35884073, dtype=float32), 'train/wer': 0.12871488562560318, 'validation/ctc_loss': DeviceArray(0.718602, dtype=float32), 'validation/wer': 0.21240918870418432, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.43000782, dtype=float32), 'test/wer': 0.14409034590620112, 'test/num_examples': 2472, 'score': 36094.48369932175, 'total_duration': 37945.54689049721, 'accumulated_submission_time': 36094.48369932175, 'accumulated_eval_time': 1850.2733764648438, 'accumulated_logging_time': 0.5075218677520752, 'global_step': 15614, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.35746717, dtype=float32), 'train/wer': 0.12777306154176596, 'validation/ctc_loss': DeviceArray(0.7101746, dtype=float32), 'validation/wer': 0.2094086773630233, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4277598, dtype=float32), 'test/wer': 0.1419576300448886, 'test/num_examples': 2472, 'score': 36994.33605790138, 'total_duration': 38961.9516518116, 'accumulated_submission_time': 36994.33605790138, 'accumulated_eval_time': 1966.7799363136292, 'accumulated_logging_time': 0.5436201095581055, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0429 13:20:35.968041 139853969647424 submission_runner.py:581] Timing: 36994.33605790138
I0429 13:20:35.968138 139853969647424 submission_runner.py:582] ====================
I0429 13:20:35.969904 139853969647424 submission_runner.py:645] Final librispeech_deepspeech score: 36994.33605790138
