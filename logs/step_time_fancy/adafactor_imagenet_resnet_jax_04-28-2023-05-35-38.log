python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_04-28-2023-05-35-38.log
I0428 05:36:00.818021 140375271311168 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax.
I0428 05:36:00.895676 140375271311168 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 05:36:01.777628 140375271311168 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0428 05:36:01.778273 140375271311168 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 05:36:01.782465 140375271311168 submission_runner.py:538] Using RNG seed 2429196520
I0428 05:36:04.771570 140375271311168 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 05:36:04.771785 140375271311168 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1.
I0428 05:36:04.772157 140375271311168 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1/hparams.json.
I0428 05:36:04.899549 140375271311168 submission_runner.py:241] Initializing dataset.
I0428 05:36:04.911627 140375271311168 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:36:04.920095 140375271311168 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 05:36:04.920223 140375271311168 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 05:36:05.183329 140375271311168 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:36:06.274011 140375271311168 submission_runner.py:248] Initializing model.
I0428 05:36:18.360949 140375271311168 submission_runner.py:258] Initializing optimizer.
I0428 05:36:21.592947 140375271311168 submission_runner.py:265] Initializing metrics bundle.
I0428 05:36:21.593203 140375271311168 submission_runner.py:282] Initializing checkpoint and logger.
I0428 05:36:21.594262 140375271311168 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0428 05:36:22.475496 140375271311168 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0428 05:36:22.476685 140375271311168 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1/flags_0.json.
I0428 05:36:22.482118 140375271311168 submission_runner.py:318] Starting training loop.
I0428 05:37:54.897425 140193118017280 logging_writer.py:48] [0] global_step=0, grad_norm=0.6139209866523743, loss=6.932281494140625
I0428 05:37:54.918579 140375271311168 spec.py:298] Evaluating on the training split.
I0428 05:37:55.431317 140375271311168 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:37:55.437505 140375271311168 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 05:37:55.437619 140375271311168 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 05:37:55.499183 140375271311168 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:38:07.740418 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 05:38:08.638271 140375271311168 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:38:08.658990 140375271311168 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 05:38:08.659259 140375271311168 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 05:38:08.723916 140375271311168 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:38:26.552569 140375271311168 spec.py:326] Evaluating on the test split.
I0428 05:38:27.004147 140375271311168 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 05:38:27.011376 140375271311168 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 05:38:27.042256 140375271311168 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 05:38:36.317556 140375271311168 submission_runner.py:415] Time since start: 133.84s, 	Step: 1, 	{'train/accuracy': 0.0010961415246129036, 'train/loss': 6.911734104156494, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.912517547607422, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.91221284866333, 'test/num_examples': 10000, 'score': 92.43627095222473, 'total_duration': 133.83537101745605, 'accumulated_submission_time': 92.43627095222473, 'accumulated_eval_time': 41.398921489715576, 'accumulated_logging_time': 0}
I0428 05:38:36.333176 140168740730624 logging_writer.py:48] [1] accumulated_eval_time=41.398921, accumulated_logging_time=0, accumulated_submission_time=92.436271, global_step=1, preemption_count=0, score=92.436271, test/accuracy=0.001000, test/loss=6.912213, test/num_examples=10000, total_duration=133.835371, train/accuracy=0.001096, train/loss=6.911734, validation/accuracy=0.001000, validation/loss=6.912518, validation/num_examples=50000
I0428 05:39:12.374187 140168824592128 logging_writer.py:48] [100] global_step=100, grad_norm=0.6060757637023926, loss=6.870776176452637
I0428 05:39:48.295715 140168740730624 logging_writer.py:48] [200] global_step=200, grad_norm=0.6974949240684509, loss=6.713305950164795
I0428 05:40:24.331480 140168824592128 logging_writer.py:48] [300] global_step=300, grad_norm=0.7559806704521179, loss=6.580535888671875
I0428 05:41:00.275390 140168740730624 logging_writer.py:48] [400] global_step=400, grad_norm=0.7835125923156738, loss=6.34879732131958
I0428 05:41:36.271392 140168824592128 logging_writer.py:48] [500] global_step=500, grad_norm=0.9981560111045837, loss=6.2257304191589355
I0428 05:42:12.219164 140168740730624 logging_writer.py:48] [600] global_step=600, grad_norm=1.793811321258545, loss=6.124340057373047
I0428 05:42:48.309529 140168824592128 logging_writer.py:48] [700] global_step=700, grad_norm=1.828105092048645, loss=6.053458213806152
I0428 05:43:24.260138 140168740730624 logging_writer.py:48] [800] global_step=800, grad_norm=2.3854053020477295, loss=5.934132099151611
I0428 05:44:00.352948 140168824592128 logging_writer.py:48] [900] global_step=900, grad_norm=2.6488876342773438, loss=5.7416791915893555
I0428 05:44:36.294256 140168740730624 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.382755756378174, loss=5.7340521812438965
I0428 05:45:12.321116 140168824592128 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.3154773712158203, loss=5.661549091339111
I0428 05:45:48.277106 140168740730624 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.9676079750061035, loss=5.481890678405762
I0428 05:46:24.314275 140168824592128 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.311748504638672, loss=5.511950492858887
I0428 05:47:00.251185 140168740730624 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.0846734046936035, loss=5.48655891418457
I0428 05:47:06.566914 140375271311168 spec.py:298] Evaluating on the training split.
I0428 05:47:13.735738 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 05:47:21.397000 140375271311168 spec.py:326] Evaluating on the test split.
I0428 05:47:23.734432 140375271311168 submission_runner.py:415] Time since start: 661.25s, 	Step: 1419, 	{'train/accuracy': 0.12065529078245163, 'train/loss': 4.751684665679932, 'validation/accuracy': 0.10421999543905258, 'validation/loss': 4.905856609344482, 'validation/num_examples': 50000, 'test/accuracy': 0.07240000367164612, 'test/loss': 5.256279468536377, 'test/num_examples': 10000, 'score': 602.6431815624237, 'total_duration': 661.2522580623627, 'accumulated_submission_time': 602.6431815624237, 'accumulated_eval_time': 58.566415548324585, 'accumulated_logging_time': 0.02480316162109375}
I0428 05:47:23.742871 140168992380672 logging_writer.py:48] [1419] accumulated_eval_time=58.566416, accumulated_logging_time=0.024803, accumulated_submission_time=602.643182, global_step=1419, preemption_count=0, score=602.643182, test/accuracy=0.072400, test/loss=5.256279, test/num_examples=10000, total_duration=661.252258, train/accuracy=0.120655, train/loss=4.751685, validation/accuracy=0.104220, validation/loss=4.905857, validation/num_examples=50000
I0428 05:47:53.263194 140169000773376 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.538455009460449, loss=5.3229875564575195
I0428 05:48:29.226687 140168992380672 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.16917610168457, loss=5.335609436035156
I0428 05:49:05.268453 140169000773376 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.8621158599853516, loss=5.150933265686035
I0428 05:49:41.231619 140168992380672 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.924464225769043, loss=5.098017692565918
I0428 05:50:17.230519 140169000773376 logging_writer.py:48] [1900] global_step=1900, grad_norm=6.028167724609375, loss=4.943880081176758
I0428 05:50:53.174713 140168992380672 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.7145466804504395, loss=4.973143577575684
I0428 05:51:29.241870 140169000773376 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.657820701599121, loss=4.86854362487793
I0428 05:52:05.244135 140168992380672 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.95422101020813, loss=4.882711887359619
I0428 05:52:41.276301 140169000773376 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.707181453704834, loss=4.775277614593506
I0428 05:53:17.195945 140168992380672 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.1623945236206055, loss=4.650724411010742
I0428 05:53:53.197149 140169000773376 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.29749059677124, loss=4.590620517730713
I0428 05:54:29.123485 140168992380672 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.973793029785156, loss=4.4884843826293945
I0428 05:55:05.169290 140169000773376 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.4918880462646484, loss=4.350401878356934
I0428 05:55:41.128141 140168992380672 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.97041392326355, loss=4.354002475738525
I0428 05:55:53.936087 140375271311168 spec.py:298] Evaluating on the training split.
I0428 05:56:01.031413 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 05:56:08.836228 140375271311168 spec.py:326] Evaluating on the test split.
I0428 05:56:11.056906 140375271311168 submission_runner.py:415] Time since start: 1188.57s, 	Step: 2837, 	{'train/accuracy': 0.2703882157802582, 'train/loss': 3.5469212532043457, 'validation/accuracy': 0.2433599978685379, 'validation/loss': 3.7163445949554443, 'validation/num_examples': 50000, 'test/accuracy': 0.17980000376701355, 'test/loss': 4.252598285675049, 'test/num_examples': 10000, 'score': 1112.811322927475, 'total_duration': 1188.5747330188751, 'accumulated_submission_time': 1112.811322927475, 'accumulated_eval_time': 75.68721008300781, 'accumulated_logging_time': 0.04096055030822754}
I0428 05:56:11.065181 140169000773376 logging_writer.py:48] [2837] accumulated_eval_time=75.687210, accumulated_logging_time=0.040961, accumulated_submission_time=1112.811323, global_step=2837, preemption_count=0, score=1112.811323, test/accuracy=0.179800, test/loss=4.252598, test/num_examples=10000, total_duration=1188.574733, train/accuracy=0.270388, train/loss=3.546921, validation/accuracy=0.243360, validation/loss=3.716345, validation/num_examples=50000
I0428 05:56:34.169430 140168992380672 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.764103412628174, loss=4.322387218475342
I0428 05:57:10.081012 140169000773376 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.710911750793457, loss=4.300087928771973
I0428 05:57:46.075118 140168992380672 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.159824848175049, loss=4.2674055099487305
I0428 05:58:21.965094 140169000773376 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.274651527404785, loss=4.071919918060303
I0428 05:58:57.944824 140168992380672 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.100931167602539, loss=4.130777359008789
I0428 05:59:33.863008 140169000773376 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.3440446853637695, loss=4.140017509460449
I0428 06:00:09.876338 140168992380672 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.5658934116363525, loss=3.9805283546447754
I0428 06:00:45.795414 140169000773376 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.433932065963745, loss=4.076414108276367
I0428 06:01:21.774528 140168992380672 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.214214324951172, loss=4.018660545349121
I0428 06:01:57.697124 140169000773376 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.68013072013855, loss=3.935011386871338
I0428 06:02:33.668529 140168992380672 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.393097400665283, loss=3.874347448348999
I0428 06:03:09.563353 140169000773376 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.594478130340576, loss=3.7936010360717773
I0428 06:03:45.579130 140168992380672 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.0332305431365967, loss=3.7170815467834473
I0428 06:04:21.482595 140169000773376 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.6149646043777466, loss=3.8219735622406006
I0428 06:04:41.096914 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:04:48.462282 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:04:56.274294 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:04:58.231790 140375271311168 submission_runner.py:415] Time since start: 1715.75s, 	Step: 4256, 	{'train/accuracy': 0.4065887928009033, 'train/loss': 2.753617525100708, 'validation/accuracy': 0.37101998925209045, 'validation/loss': 2.9373183250427246, 'validation/num_examples': 50000, 'test/accuracy': 0.28200000524520874, 'test/loss': 3.553682565689087, 'test/num_examples': 10000, 'score': 1622.816936969757, 'total_duration': 1715.7496042251587, 'accumulated_submission_time': 1622.816936969757, 'accumulated_eval_time': 92.82206082344055, 'accumulated_logging_time': 0.057936906814575195}
I0428 06:04:58.241974 140168992380672 logging_writer.py:48] [4256] accumulated_eval_time=92.822061, accumulated_logging_time=0.057937, accumulated_submission_time=1622.816937, global_step=4256, preemption_count=0, score=1622.816937, test/accuracy=0.282000, test/loss=3.553683, test/num_examples=10000, total_duration=1715.749604, train/accuracy=0.406589, train/loss=2.753618, validation/accuracy=0.371020, validation/loss=2.937318, validation/num_examples=50000
I0428 06:05:14.483943 140169000773376 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.91237211227417, loss=3.7484238147735596
I0428 06:05:50.364217 140168992380672 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.1189286708831787, loss=3.8854613304138184
I0428 06:06:26.271696 140169000773376 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.670432686805725, loss=3.763254165649414
I0428 06:07:02.190066 140168992380672 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.6164724826812744, loss=3.7293429374694824
I0428 06:07:38.072462 140169000773376 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.1049513816833496, loss=3.669829845428467
I0428 06:08:14.023333 140168992380672 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.9534944295883179, loss=3.7510299682617188
I0428 06:08:49.968172 140169000773376 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.9162365198135376, loss=3.5561230182647705
I0428 06:09:25.857326 140168992380672 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.451981544494629, loss=3.5284714698791504
I0428 06:10:01.750109 140169000773376 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.9034615755081177, loss=3.4601128101348877
I0428 06:10:37.700220 140168992380672 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.6783945560455322, loss=3.5695102214813232
I0428 06:11:13.589791 140169000773376 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.0403263568878174, loss=3.44057297706604
I0428 06:11:49.526588 140168992380672 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9727040529251099, loss=3.4354498386383057
I0428 06:12:25.483300 140169000773376 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.4677103757858276, loss=3.4215121269226074
I0428 06:13:01.397612 140168992380672 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.293991208076477, loss=3.505275011062622
I0428 06:13:28.543123 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:13:35.329536 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:13:43.334144 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:13:45.564421 140375271311168 submission_runner.py:415] Time since start: 2243.08s, 	Step: 5677, 	{'train/accuracy': 0.5002790093421936, 'train/loss': 2.31588077545166, 'validation/accuracy': 0.462039977312088, 'validation/loss': 2.503946542739868, 'validation/num_examples': 50000, 'test/accuracy': 0.3540000021457672, 'test/loss': 3.1489052772521973, 'test/num_examples': 10000, 'score': 2133.092044353485, 'total_duration': 2243.0822422504425, 'accumulated_submission_time': 2133.092044353485, 'accumulated_eval_time': 109.8433530330658, 'accumulated_logging_time': 0.07629609107971191}
I0428 06:13:45.572556 140169000773376 logging_writer.py:48] [5677] accumulated_eval_time=109.843353, accumulated_logging_time=0.076296, accumulated_submission_time=2133.092044, global_step=5677, preemption_count=0, score=2133.092044, test/accuracy=0.354000, test/loss=3.148905, test/num_examples=10000, total_duration=2243.082242, train/accuracy=0.500279, train/loss=2.315881, validation/accuracy=0.462040, validation/loss=2.503947, validation/num_examples=50000
I0428 06:13:54.333569 140168992380672 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.3853944540023804, loss=3.3688700199127197
I0428 06:14:30.201603 140169000773376 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.3711260557174683, loss=3.370957851409912
I0428 06:15:06.109131 140168992380672 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.3129814863204956, loss=3.3464515209198
I0428 06:15:42.033716 140169000773376 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.5117944478988647, loss=3.3100807666778564
I0428 06:16:17.885225 140168992380672 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.5813865661621094, loss=3.2942769527435303
I0428 06:16:53.811786 140169000773376 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.1127480268478394, loss=3.3548054695129395
I0428 06:17:29.698223 140168992380672 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.7756987810134888, loss=3.295506477355957
I0428 06:18:05.737865 140169000773376 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8797428011894226, loss=3.2973320484161377
I0428 06:18:41.645304 140168992380672 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.1044789552688599, loss=3.3960349559783936
I0428 06:19:17.631386 140169000773376 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.0863618850708008, loss=3.34820294380188
I0428 06:19:53.513687 140168992380672 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.0226969718933105, loss=3.2449381351470947
I0428 06:20:29.445482 140169000773376 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.947786271572113, loss=3.316013813018799
I0428 06:21:05.308191 140168992380672 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8189713358879089, loss=3.254746675491333
I0428 06:21:41.357820 140169000773376 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.457141637802124, loss=3.1701273918151855
I0428 06:22:15.658987 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:22:22.358552 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:22:30.301065 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:22:32.168874 140375271311168 submission_runner.py:415] Time since start: 2769.69s, 	Step: 7097, 	{'train/accuracy': 0.5398995280265808, 'train/loss': 2.026991128921509, 'validation/accuracy': 0.49955999851226807, 'validation/loss': 2.23484468460083, 'validation/num_examples': 50000, 'test/accuracy': 0.38280001282691956, 'test/loss': 2.9803988933563232, 'test/num_examples': 10000, 'score': 2643.153190612793, 'total_duration': 2769.686674594879, 'accumulated_submission_time': 2643.153190612793, 'accumulated_eval_time': 126.35319113731384, 'accumulated_logging_time': 0.09176015853881836}
I0428 06:22:32.178185 140168992380672 logging_writer.py:48] [7097] accumulated_eval_time=126.353191, accumulated_logging_time=0.091760, accumulated_submission_time=2643.153191, global_step=7097, preemption_count=0, score=2643.153191, test/accuracy=0.382800, test/loss=2.980399, test/num_examples=10000, total_duration=2769.686675, train/accuracy=0.539900, train/loss=2.026991, validation/accuracy=0.499560, validation/loss=2.234845, validation/num_examples=50000
I0428 06:22:33.639308 140169000773376 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7418596148490906, loss=3.183138847351074
I0428 06:23:09.684550 140168992380672 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9188739657402039, loss=3.162971258163452
I0428 06:23:45.674983 140169000773376 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7793327569961548, loss=3.2444100379943848
I0428 06:24:21.754661 140168992380672 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8842713236808777, loss=3.145514488220215
I0428 06:24:57.739625 140169000773376 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7190609574317932, loss=3.040398597717285
I0428 06:25:33.822554 140168992380672 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8741471767425537, loss=3.1264634132385254
I0428 06:26:09.825252 140169000773376 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9952266216278076, loss=3.123918056488037
I0428 06:26:45.893249 140168992380672 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7892266511917114, loss=3.1031999588012695
I0428 06:27:21.855825 140169000773376 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.061279296875, loss=3.0278420448303223
I0428 06:27:58.036211 140168992380672 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9537175297737122, loss=3.0537967681884766
I0428 06:28:34.014403 140169000773376 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7219979166984558, loss=3.1195335388183594
I0428 06:29:10.091569 140168992380672 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9383222460746765, loss=3.183789014816284
I0428 06:29:46.066476 140169000773376 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5346741080284119, loss=3.003697156906128
I0428 06:30:22.147355 140168992380672 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7657178044319153, loss=3.087754487991333
I0428 06:30:58.150363 140169000773376 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.700118362903595, loss=3.1720376014709473
I0428 06:31:02.350750 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:31:09.104272 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:31:18.060700 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:31:20.103146 140375271311168 submission_runner.py:415] Time since start: 3297.62s, 	Step: 8513, 	{'train/accuracy': 0.5957828164100647, 'train/loss': 1.7705515623092651, 'validation/accuracy': 0.5486599802970886, 'validation/loss': 2.007369041442871, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.745323657989502, 'test/num_examples': 10000, 'score': 3153.3000948429108, 'total_duration': 3297.6209716796875, 'accumulated_submission_time': 3153.3000948429108, 'accumulated_eval_time': 144.10556650161743, 'accumulated_logging_time': 0.1091463565826416}
I0428 06:31:20.113087 140168992380672 logging_writer.py:48] [8513] accumulated_eval_time=144.105567, accumulated_logging_time=0.109146, accumulated_submission_time=3153.300095, global_step=8513, preemption_count=0, score=3153.300095, test/accuracy=0.416700, test/loss=2.745324, test/num_examples=10000, total_duration=3297.620972, train/accuracy=0.595783, train/loss=1.770552, validation/accuracy=0.548660, validation/loss=2.007369, validation/num_examples=50000
I0428 06:31:55.466670 140169000773376 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6337459683418274, loss=2.9259324073791504
I0428 06:32:31.330070 140168992380672 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7309420108795166, loss=3.002171516418457
I0428 06:33:07.141893 140169000773376 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7444815635681152, loss=3.0887670516967773
I0428 06:33:43.079017 140168992380672 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6721323132514954, loss=2.973862409591675
I0428 06:34:18.920333 140169000773376 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.030024766921997, loss=3.006071090698242
I0428 06:34:54.843909 140168992380672 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5877068042755127, loss=2.9081294536590576
I0428 06:35:30.802531 140169000773376 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6931666731834412, loss=3.024303913116455
I0428 06:36:06.677356 140168992380672 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7594336867332458, loss=2.9830868244171143
I0428 06:36:42.532549 140169000773376 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5731421113014221, loss=2.889366626739502
I0428 06:37:18.457762 140168992380672 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5532833337783813, loss=2.943680763244629
I0428 06:37:54.317087 140169000773376 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7567906379699707, loss=2.9295735359191895
I0428 06:38:30.231047 140168992380672 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5809202790260315, loss=2.990659475326538
I0428 06:39:06.188311 140169000773376 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4796002209186554, loss=2.911864757537842
I0428 06:39:42.037576 140168992380672 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5003859400749207, loss=2.8594253063201904
I0428 06:39:54.105484 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:40:00.938442 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:40:10.287973 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:40:12.512484 140375271311168 submission_runner.py:415] Time since start: 3830.03s, 	Step: 9935, 	{'train/accuracy': 0.6242027878761292, 'train/loss': 1.6778172254562378, 'validation/accuracy': 0.5746200084686279, 'validation/loss': 1.919921875, 'validation/num_examples': 50000, 'test/accuracy': 0.45260003209114075, 'test/loss': 2.587604522705078, 'test/num_examples': 10000, 'score': 3663.572422504425, 'total_duration': 3830.030287027359, 'accumulated_submission_time': 3663.572422504425, 'accumulated_eval_time': 162.5125172138214, 'accumulated_logging_time': 3.821786403656006}
I0428 06:40:12.522969 140169000773376 logging_writer.py:48] [9935] accumulated_eval_time=162.512517, accumulated_logging_time=3.821786, accumulated_submission_time=3663.572423, global_step=9935, preemption_count=0, score=3663.572423, test/accuracy=0.452600, test/loss=2.587605, test/num_examples=10000, total_duration=3830.030287, train/accuracy=0.624203, train/loss=1.677817, validation/accuracy=0.574620, validation/loss=1.919922, validation/num_examples=50000
I0428 06:40:36.262286 140168992380672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4672379493713379, loss=2.910860061645508
I0428 06:41:12.147154 140169000773376 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.47077834606170654, loss=2.9167556762695312
I0428 06:41:48.006525 140168992380672 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5068441033363342, loss=3.026397705078125
I0428 06:42:23.907868 140169000773376 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5372492074966431, loss=2.951416492462158
I0428 06:42:59.743271 140168992380672 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5089327692985535, loss=2.9824132919311523
I0428 06:43:35.691690 140169000773376 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5609696507453918, loss=2.9222989082336426
I0428 06:44:11.537549 140168992380672 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4028453528881073, loss=2.956308603286743
I0428 06:44:47.477799 140169000773376 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5715925097465515, loss=2.794912815093994
I0428 06:45:23.359885 140168992380672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5195081233978271, loss=2.8932666778564453
I0428 06:45:59.385771 140169000773376 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7540171146392822, loss=2.8978452682495117
I0428 06:46:35.238325 140168992380672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.4471819996833801, loss=2.799842119216919
I0428 06:47:11.180890 140169000773376 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3799072802066803, loss=2.7388267517089844
I0428 06:47:46.971800 140168992380672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4710105061531067, loss=2.806030511856079
I0428 06:48:23.007737 140169000773376 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.44873183965682983, loss=2.858942985534668
I0428 06:48:42.573579 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:48:49.494744 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:48:59.369894 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:49:01.590485 140375271311168 submission_runner.py:415] Time since start: 4359.11s, 	Step: 11356, 	{'train/accuracy': 0.6428571343421936, 'train/loss': 1.5879040956497192, 'validation/accuracy': 0.5968799591064453, 'validation/loss': 1.8340562582015991, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.531586170196533, 'test/num_examples': 10000, 'score': 4173.597459793091, 'total_duration': 4359.107352972031, 'accumulated_submission_time': 4173.597459793091, 'accumulated_eval_time': 181.52844738960266, 'accumulated_logging_time': 3.8399691581726074}
I0428 06:49:01.604079 140168992380672 logging_writer.py:48] [11356] accumulated_eval_time=181.528447, accumulated_logging_time=3.839969, accumulated_submission_time=4173.597460, global_step=11356, preemption_count=0, score=4173.597460, test/accuracy=0.463800, test/loss=2.531586, test/num_examples=10000, total_duration=4359.107353, train/accuracy=0.642857, train/loss=1.587904, validation/accuracy=0.596880, validation/loss=1.834056, validation/num_examples=50000
I0428 06:49:17.740729 140169000773376 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5257549285888672, loss=2.866509437561035
I0428 06:49:53.699131 140168992380672 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3825238347053528, loss=2.6755244731903076
I0428 06:50:29.568995 140169000773376 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.43682578206062317, loss=2.718151092529297
I0428 06:51:05.572792 140168992380672 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5207703709602356, loss=2.852648973464966
I0428 06:51:41.434264 140169000773376 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.49499478936195374, loss=2.838315010070801
I0428 06:52:17.404802 140168992380672 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.43923869729042053, loss=2.7527036666870117
I0428 06:52:53.264379 140169000773376 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.41471442580223083, loss=2.8381614685058594
I0428 06:53:29.239066 140168992380672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.38585034012794495, loss=2.779813289642334
I0428 06:54:05.111019 140169000773376 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5411809086799622, loss=2.7934629917144775
I0428 06:54:40.986248 140168992380672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5748104453086853, loss=2.7788619995117188
I0428 06:55:16.905113 140169000773376 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3542093336582184, loss=2.6752676963806152
I0428 06:55:52.773362 140168992380672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3847484588623047, loss=2.685269355773926
I0428 06:56:28.695995 140169000773376 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.44633278250694275, loss=2.7255403995513916
I0428 06:57:04.668434 140168992380672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3663572371006012, loss=2.6536805629730225
I0428 06:57:31.797882 140375271311168 spec.py:298] Evaluating on the training split.
I0428 06:57:39.038993 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 06:57:48.854323 140375271311168 spec.py:326] Evaluating on the test split.
I0428 06:57:50.870091 140375271311168 submission_runner.py:415] Time since start: 4888.39s, 	Step: 12777, 	{'train/accuracy': 0.6744459271430969, 'train/loss': 1.4452184438705444, 'validation/accuracy': 0.6141999959945679, 'validation/loss': 1.72683584690094, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.4149646759033203, 'test/num_examples': 10000, 'score': 4683.763954877853, 'total_duration': 4888.387017250061, 'accumulated_submission_time': 4683.763954877853, 'accumulated_eval_time': 200.5997371673584, 'accumulated_logging_time': 3.8632779121398926}
I0428 06:57:50.878530 140169000773376 logging_writer.py:48] [12777] accumulated_eval_time=200.599737, accumulated_logging_time=3.863278, accumulated_submission_time=4683.763955, global_step=12777, preemption_count=0, score=4683.763955, test/accuracy=0.483800, test/loss=2.414965, test/num_examples=10000, total_duration=4888.387017, train/accuracy=0.674446, train/loss=1.445218, validation/accuracy=0.614200, validation/loss=1.726836, validation/num_examples=50000
I0428 06:57:59.472398 140168992380672 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4246377646923065, loss=2.804622173309326
I0428 06:58:35.384143 140169000773376 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3818110525608063, loss=2.782691478729248
I0428 06:59:11.270213 140168992380672 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3589613139629364, loss=2.706583023071289
I0428 06:59:47.220223 140169000773376 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3387225568294525, loss=2.6037802696228027
I0428 07:00:23.133793 140168992380672 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3950244188308716, loss=2.596163272857666
I0428 07:00:59.048852 140169000773376 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.41111984848976135, loss=2.708643913269043
I0428 07:01:34.989846 140168992380672 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3902619779109955, loss=2.7241668701171875
I0428 07:02:10.852057 140169000773376 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.38281670212745667, loss=2.7614946365356445
I0428 07:02:46.773273 140168992380672 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.43057283759117126, loss=2.6818811893463135
I0428 07:03:22.760202 140169000773376 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3524068593978882, loss=2.728835105895996
I0428 07:03:58.653835 140168992380672 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.35597994923591614, loss=2.638339042663574
I0428 07:04:34.553337 140169000773376 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.33706721663475037, loss=2.621615409851074
I0428 07:05:10.491703 140168992380672 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3299163579940796, loss=2.6614346504211426
I0428 07:05:46.411394 140169000773376 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.4025734066963196, loss=2.635852336883545
I0428 07:06:21.136754 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:06:28.622964 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:06:38.918614 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:06:40.888012 140375271311168 submission_runner.py:415] Time since start: 5418.40s, 	Step: 14198, 	{'train/accuracy': 0.6900510191917419, 'train/loss': 1.341858148574829, 'validation/accuracy': 0.6246199607849121, 'validation/loss': 1.6429492235183716, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.3343467712402344, 'test/num_examples': 10000, 'score': 5193.996488571167, 'total_duration': 5418.404597997665, 'accumulated_submission_time': 5193.996488571167, 'accumulated_eval_time': 220.34972834587097, 'accumulated_logging_time': 3.880359649658203}
I0428 07:06:40.903924 140168992380672 logging_writer.py:48] [14198] accumulated_eval_time=220.349728, accumulated_logging_time=3.880360, accumulated_submission_time=5193.996489, global_step=14198, preemption_count=0, score=5193.996489, test/accuracy=0.501400, test/loss=2.334347, test/num_examples=10000, total_duration=5418.404598, train/accuracy=0.690051, train/loss=1.341858, validation/accuracy=0.624620, validation/loss=1.642949, validation/num_examples=50000
I0428 07:06:42.013799 140169000773376 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.3384198546409607, loss=2.7193617820739746
I0428 07:07:17.998705 140168992380672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3426592946052551, loss=2.633784055709839
I0428 07:07:53.854583 140169000773376 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.40081971883773804, loss=2.6167337894439697
I0428 07:08:29.729237 140168992380672 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.40025216341018677, loss=2.6698410511016846
I0428 07:09:05.677054 140169000773376 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.39123111963272095, loss=2.654008626937866
I0428 07:09:41.548702 140168992380672 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.31058356165885925, loss=2.513000726699829
I0428 07:10:17.481348 140169000773376 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3133522570133209, loss=2.5469634532928467
I0428 07:10:53.321103 140168992380672 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3944016695022583, loss=2.636197328567505
I0428 07:11:29.253702 140169000773376 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4094609320163727, loss=2.669133424758911
I0428 07:12:05.142059 140168992380672 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.38487207889556885, loss=2.612595796585083
I0428 07:12:41.041280 140169000773376 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.31378450989723206, loss=2.6890957355499268
I0428 07:13:16.886287 140168992380672 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3235478103160858, loss=2.6017262935638428
I0428 07:13:52.897585 140169000773376 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.33916619420051575, loss=2.646867036819458
I0428 07:14:28.742475 140168992380672 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.34107086062431335, loss=2.5452868938446045
I0428 07:15:04.763469 140169000773376 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3657497763633728, loss=2.598346710205078
I0428 07:15:11.086443 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:15:18.869652 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:15:29.193928 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:15:31.309098 140375271311168 submission_runner.py:415] Time since start: 5948.83s, 	Step: 15619, 	{'train/accuracy': 0.7529097199440002, 'train/loss': 1.0699976682662964, 'validation/accuracy': 0.647379994392395, 'validation/loss': 1.5456516742706299, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.2468960285186768, 'test/num_examples': 10000, 'score': 5704.1424877643585, 'total_duration': 5948.825696706772, 'accumulated_submission_time': 5704.1424877643585, 'accumulated_eval_time': 240.57113027572632, 'accumulated_logging_time': 3.9152746200561523}
I0428 07:15:31.319180 140168992380672 logging_writer.py:48] [15619] accumulated_eval_time=240.571130, accumulated_logging_time=3.915275, accumulated_submission_time=5704.142488, global_step=15619, preemption_count=0, score=5704.142488, test/accuracy=0.516100, test/loss=2.246896, test/num_examples=10000, total_duration=5948.825697, train/accuracy=0.752910, train/loss=1.069998, validation/accuracy=0.647380, validation/loss=1.545652, validation/num_examples=50000
I0428 07:16:00.725649 140169000773376 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.33076271414756775, loss=2.591520309448242
I0428 07:16:36.625925 140168992380672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.37658095359802246, loss=2.5704429149627686
I0428 07:17:12.568167 140169000773376 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.36147448420524597, loss=2.6282715797424316
I0428 07:17:48.419158 140168992380672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3613256812095642, loss=2.609544038772583
I0428 07:18:24.446545 140169000773376 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.34977537393569946, loss=2.5149364471435547
I0428 07:19:00.416140 140168992380672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3236701488494873, loss=2.5996763706207275
I0428 07:19:36.317358 140169000773376 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3520244061946869, loss=2.593651294708252
I0428 07:20:12.190677 140168992380672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.48279398679733276, loss=2.6791555881500244
I0428 07:20:48.122831 140169000773376 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.37132149934768677, loss=2.597609758377075
I0428 07:21:23.978166 140168992380672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.31631651520729065, loss=2.4850778579711914
I0428 07:21:59.913700 140169000773376 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3358810544013977, loss=2.591879367828369
I0428 07:22:35.774294 140168992380672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3081464469432831, loss=2.475935697555542
I0428 07:23:11.787301 140169000773376 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3687991797924042, loss=2.6041488647460938
I0428 07:23:47.692878 140168992380672 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.329049289226532, loss=2.563013792037964
I0428 07:24:01.555011 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:24:09.339320 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:24:19.563186 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:24:21.740205 140375271311168 submission_runner.py:415] Time since start: 6479.26s, 	Step: 17040, 	{'train/accuracy': 0.7594068646430969, 'train/loss': 1.0625015497207642, 'validation/accuracy': 0.6574599742889404, 'validation/loss': 1.5182002782821655, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.200535774230957, 'test/num_examples': 10000, 'score': 6214.351387023926, 'total_duration': 6479.256758928299, 'accumulated_submission_time': 6214.351387023926, 'accumulated_eval_time': 260.755028963089, 'accumulated_logging_time': 3.9344100952148438}
I0428 07:24:21.754355 140169000773376 logging_writer.py:48] [17040] accumulated_eval_time=260.755029, accumulated_logging_time=3.934410, accumulated_submission_time=6214.351387, global_step=17040, preemption_count=0, score=6214.351387, test/accuracy=0.529700, test/loss=2.200536, test/num_examples=10000, total_duration=6479.256759, train/accuracy=0.759407, train/loss=1.062502, validation/accuracy=0.657460, validation/loss=1.518200, validation/num_examples=50000
I0428 07:24:43.672365 140168992380672 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3347819149494171, loss=2.54681134223938
I0428 07:25:19.541361 140169000773376 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3399266302585602, loss=2.564443826675415
I0428 07:25:55.525762 140168992380672 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.31007030606269836, loss=2.528228759765625
I0428 07:26:31.419294 140169000773376 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.36235707998275757, loss=2.505556583404541
I0428 07:27:07.341708 140168992380672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2879442870616913, loss=2.452521562576294
I0428 07:27:43.248949 140169000773376 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3358873128890991, loss=2.6070919036865234
I0428 07:28:19.181386 140168992380672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.30614957213401794, loss=2.4939327239990234
I0428 07:28:55.052622 140169000773376 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.318426251411438, loss=2.5354812145233154
I0428 07:29:31.004330 140168992380672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.32504090666770935, loss=2.598613977432251
I0428 07:30:06.900097 140169000773376 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.34586840867996216, loss=2.574176788330078
I0428 07:30:42.765110 140168992380672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.31253862380981445, loss=2.4848387241363525
I0428 07:31:18.717530 140169000773376 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3081175684928894, loss=2.5398454666137695
I0428 07:31:54.584481 140168992380672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.27851781249046326, loss=2.469294548034668
I0428 07:32:30.536067 140169000773376 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.30973342061042786, loss=2.505965232849121
I0428 07:32:51.914364 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:32:59.326581 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:33:09.650928 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:33:11.498319 140375271311168 submission_runner.py:415] Time since start: 7009.02s, 	Step: 18461, 	{'train/accuracy': 0.7450574040412903, 'train/loss': 1.1035239696502686, 'validation/accuracy': 0.6446200013160706, 'validation/loss': 1.5551667213439941, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.22076153755188, 'test/num_examples': 10000, 'score': 6724.484362602234, 'total_duration': 7009.0150628089905, 'accumulated_submission_time': 6724.484362602234, 'accumulated_eval_time': 280.33787631988525, 'accumulated_logging_time': 3.95845365524292}
I0428 07:33:11.508557 140168992380672 logging_writer.py:48] [18461] accumulated_eval_time=280.337876, accumulated_logging_time=3.958454, accumulated_submission_time=6724.484363, global_step=18461, preemption_count=0, score=6724.484363, test/accuracy=0.527000, test/loss=2.220762, test/num_examples=10000, total_duration=7009.015063, train/accuracy=0.745057, train/loss=1.103524, validation/accuracy=0.644620, validation/loss=1.555167, validation/num_examples=50000
I0428 07:33:25.848240 140169000773376 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.32194194197654724, loss=2.6260128021240234
I0428 07:34:01.940455 140168992380672 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.29401203989982605, loss=2.574084758758545
I0428 07:34:37.837838 140169000773376 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3141758441925049, loss=2.4965078830718994
I0428 07:35:13.811346 140168992380672 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.28948861360549927, loss=2.3951196670532227
I0428 07:35:49.678862 140169000773376 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.33348292112350464, loss=2.5385165214538574
I0428 07:36:25.633740 140168992380672 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.32974907755851746, loss=2.5034193992614746
I0428 07:37:01.507431 140169000773376 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.29612135887145996, loss=2.4837536811828613
I0428 07:37:37.516232 140168992380672 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3274356424808502, loss=2.4482662677764893
I0428 07:38:13.374814 140169000773376 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3335729241371155, loss=2.468111276626587
I0428 07:38:49.268941 140168992380672 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4076095223426819, loss=2.489898204803467
I0428 07:39:25.209179 140169000773376 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2971039414405823, loss=2.497924327850342
I0428 07:40:01.065228 140168992380672 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2863827049732208, loss=2.473295211791992
I0428 07:40:37.085700 140169000773376 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3250330984592438, loss=2.4893932342529297
I0428 07:41:13.132031 140168992380672 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.34578967094421387, loss=2.4839227199554443
I0428 07:41:41.714405 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:41:49.421069 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:41:59.543641 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:42:01.506389 140375271311168 submission_runner.py:415] Time since start: 7539.02s, 	Step: 19881, 	{'train/accuracy': 0.7553411722183228, 'train/loss': 1.0837185382843018, 'validation/accuracy': 0.6627999544143677, 'validation/loss': 1.5076336860656738, 'validation/num_examples': 50000, 'test/accuracy': 0.5412999987602234, 'test/loss': 2.189544200897217, 'test/num_examples': 10000, 'score': 7234.664556026459, 'total_duration': 7539.02343082428, 'accumulated_submission_time': 7234.664556026459, 'accumulated_eval_time': 300.12905263900757, 'accumulated_logging_time': 3.976736545562744}
I0428 07:42:01.521450 140169000773376 logging_writer.py:48] [19881] accumulated_eval_time=300.129053, accumulated_logging_time=3.976737, accumulated_submission_time=7234.664556, global_step=19881, preemption_count=0, score=7234.664556, test/accuracy=0.541300, test/loss=2.189544, test/num_examples=10000, total_duration=7539.023431, train/accuracy=0.755341, train/loss=1.083719, validation/accuracy=0.662800, validation/loss=1.507634, validation/num_examples=50000
I0428 07:42:08.713400 140168992380672 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.33189162611961365, loss=2.5114784240722656
I0428 07:42:44.608028 140169000773376 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.31431886553764343, loss=2.4364423751831055
I0428 07:43:20.565377 140168992380672 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.2936513125896454, loss=2.426320791244507
I0428 07:43:56.431075 140169000773376 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3222379684448242, loss=2.5068891048431396
I0428 07:44:32.450020 140168992380672 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.29776665568351746, loss=2.4689838886260986
I0428 07:45:08.285481 140169000773376 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.3403644561767578, loss=2.3843839168548584
I0428 07:45:44.294497 140168992380672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3039129376411438, loss=2.4564807415008545
I0428 07:46:20.163650 140169000773376 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.2969318926334381, loss=2.493314027786255
I0428 07:46:56.104295 140168992380672 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.30530887842178345, loss=2.4879939556121826
I0428 07:47:31.988516 140169000773376 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.30841299891471863, loss=2.472710371017456
I0428 07:48:07.943490 140168992380672 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.28321343660354614, loss=2.429652690887451
I0428 07:48:43.806094 140169000773376 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.30888479948043823, loss=2.4152438640594482
I0428 07:49:19.838249 140168992380672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.2856634855270386, loss=2.4189603328704834
I0428 07:49:55.712019 140169000773376 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.2765611708164215, loss=2.4458072185516357
I0428 07:50:31.601229 140168992380672 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.29453590512275696, loss=2.4923949241638184
I0428 07:50:31.611828 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:50:38.865118 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:50:49.121897 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:50:51.308757 140375271311168 submission_runner.py:415] Time since start: 8068.83s, 	Step: 21301, 	{'train/accuracy': 0.7557995915412903, 'train/loss': 1.0667427778244019, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.4904245138168335, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.181627035140991, 'test/num_examples': 10000, 'score': 7744.727421760559, 'total_duration': 8068.825463056564, 'accumulated_submission_time': 7744.727421760559, 'accumulated_eval_time': 319.8248043060303, 'accumulated_logging_time': 4.001823902130127}
I0428 07:50:51.318791 140169000773376 logging_writer.py:48] [21301] accumulated_eval_time=319.824804, accumulated_logging_time=4.001824, accumulated_submission_time=7744.727422, global_step=21301, preemption_count=0, score=7744.727422, test/accuracy=0.529000, test/loss=2.181627, test/num_examples=10000, total_duration=8068.825463, train/accuracy=0.755800, train/loss=1.066743, validation/accuracy=0.662380, validation/loss=1.490425, validation/num_examples=50000
I0428 07:51:27.211290 140168992380672 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.298208624124527, loss=2.4632978439331055
I0428 07:52:03.113343 140169000773376 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2962498664855957, loss=2.4542903900146484
I0428 07:52:39.085095 140168992380672 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.27352267503738403, loss=2.374758720397949
I0428 07:53:14.925280 140169000773376 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.29868778586387634, loss=2.387831687927246
I0428 07:53:50.883063 140168992380672 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.27240827679634094, loss=2.457303285598755
I0428 07:54:26.768227 140169000773376 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3027955889701843, loss=2.4426815509796143
I0428 07:55:02.811683 140168992380672 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.28213366866111755, loss=2.431349754333496
I0428 07:55:38.735219 140169000773376 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.31432634592056274, loss=2.406773805618286
I0428 07:56:14.708653 140168992380672 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.320428729057312, loss=2.4334423542022705
I0428 07:56:50.595727 140169000773376 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.31994912028312683, loss=2.439539909362793
I0428 07:57:26.634104 140168992380672 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.29049152135849, loss=2.4111595153808594
I0428 07:58:02.539118 140169000773376 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.29779285192489624, loss=2.4184067249298096
I0428 07:58:38.562821 140168992380672 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3052276372909546, loss=2.45328950881958
I0428 07:59:14.499270 140169000773376 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.30614998936653137, loss=2.484647512435913
I0428 07:59:21.549480 140375271311168 spec.py:298] Evaluating on the training split.
I0428 07:59:28.764067 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 07:59:39.143854 140375271311168 spec.py:326] Evaluating on the test split.
I0428 07:59:41.349235 140375271311168 submission_runner.py:415] Time since start: 8598.87s, 	Step: 22721, 	{'train/accuracy': 0.7662228941917419, 'train/loss': 1.038856863975525, 'validation/accuracy': 0.6686199903488159, 'validation/loss': 1.4797478914260864, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.185436487197876, 'test/num_examples': 10000, 'score': 8254.932767629623, 'total_duration': 8598.867062807083, 'accumulated_submission_time': 8254.932767629623, 'accumulated_eval_time': 339.62453985214233, 'accumulated_logging_time': 4.019684791564941}
I0428 07:59:41.358421 140168992380672 logging_writer.py:48] [22721] accumulated_eval_time=339.624540, accumulated_logging_time=4.019685, accumulated_submission_time=8254.932768, global_step=22721, preemption_count=0, score=8254.932768, test/accuracy=0.536500, test/loss=2.185436, test/num_examples=10000, total_duration=8598.867063, train/accuracy=0.766223, train/loss=1.038857, validation/accuracy=0.668620, validation/loss=1.479748, validation/num_examples=50000
I0428 08:00:10.085370 140169000773376 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2869139015674591, loss=2.466233253479004
I0428 08:00:46.038550 140168992380672 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.2904742956161499, loss=2.498688220977783
I0428 08:01:21.953044 140169000773376 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.29624849557876587, loss=2.3451478481292725
I0428 08:01:57.922731 140168992380672 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2965713143348694, loss=2.3070268630981445
I0428 08:02:34.017766 140169000773376 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.29398342967033386, loss=2.3311846256256104
I0428 08:03:09.953106 140168992380672 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.32472145557403564, loss=2.397542953491211
I0428 08:03:45.851948 140169000773376 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.2973172664642334, loss=2.326160430908203
I0428 08:04:21.884438 140168992380672 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2871251404285431, loss=2.4275991916656494
I0428 08:04:57.764878 140169000773376 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.28866487741470337, loss=2.410187244415283
I0428 08:05:33.738699 140168992380672 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2841288447380066, loss=2.4499523639678955
I0428 08:06:09.614842 140169000773376 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.2842492163181305, loss=2.373082160949707
I0428 08:06:45.628193 140168992380672 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.2903718650341034, loss=2.3199944496154785
I0428 08:07:21.536329 140169000773376 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.27828851342201233, loss=2.3804242610931396
I0428 08:07:57.608862 140168992380672 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.2704375982284546, loss=2.4725210666656494
I0428 08:08:11.458447 140375271311168 spec.py:298] Evaluating on the training split.
I0428 08:08:18.819930 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 08:08:29.916563 140375271311168 spec.py:326] Evaluating on the test split.
I0428 08:08:32.111547 140375271311168 submission_runner.py:415] Time since start: 9129.63s, 	Step: 24140, 	{'train/accuracy': 0.7686144709587097, 'train/loss': 0.9861076474189758, 'validation/accuracy': 0.6732199788093567, 'validation/loss': 1.4237844944000244, 'validation/num_examples': 50000, 'test/accuracy': 0.5460000038146973, 'test/loss': 2.1341655254364014, 'test/num_examples': 10000, 'score': 8765.007684707642, 'total_duration': 9129.629374742508, 'accumulated_submission_time': 8765.007684707642, 'accumulated_eval_time': 360.2776336669922, 'accumulated_logging_time': 4.036143064498901}
I0428 08:08:32.122748 140169000773376 logging_writer.py:48] [24140] accumulated_eval_time=360.277634, accumulated_logging_time=4.036143, accumulated_submission_time=8765.007685, global_step=24140, preemption_count=0, score=8765.007685, test/accuracy=0.546000, test/loss=2.134166, test/num_examples=10000, total_duration=9129.629375, train/accuracy=0.768614, train/loss=0.986108, validation/accuracy=0.673220, validation/loss=1.423784, validation/num_examples=50000
I0428 08:08:54.036160 140168992380672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.28936076164245605, loss=2.4083187580108643
I0428 08:09:29.937270 140169000773376 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.28377801179885864, loss=2.303548574447632
I0428 08:10:05.934401 140168992380672 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2726195454597473, loss=2.393007755279541
I0428 08:10:41.812112 140169000773376 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3053168058395386, loss=2.4612929821014404
I0428 08:11:17.745633 140168992380672 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.28634247183799744, loss=2.3940646648406982
I0428 08:11:53.731362 140169000773376 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2822987139225006, loss=2.316760301589966
I0428 08:12:29.628932 140168992380672 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.2938368618488312, loss=2.3454837799072266
I0428 08:13:05.498191 140169000773376 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.27477529644966125, loss=2.377346992492676
I0428 08:13:41.465680 140168992380672 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.27144742012023926, loss=2.447967052459717
I0428 08:14:17.317940 140169000773376 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2751092314720154, loss=2.3493728637695312
I0428 08:14:53.260319 140168992380672 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2533556818962097, loss=2.3291993141174316
I0428 08:15:29.103195 140169000773376 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.30628347396850586, loss=2.3733136653900146
I0428 08:16:05.182412 140168992380672 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2812376320362091, loss=2.4477100372314453
I0428 08:16:41.076435 140169000773376 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3045627474784851, loss=2.448568105697632
I0428 08:17:02.457789 140375271311168 spec.py:298] Evaluating on the training split.
I0428 08:17:10.083337 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 08:17:20.684175 140375271311168 spec.py:326] Evaluating on the test split.
I0428 08:17:22.872156 140375271311168 submission_runner.py:415] Time since start: 9660.39s, 	Step: 25561, 	{'train/accuracy': 0.7680962681770325, 'train/loss': 1.001719355583191, 'validation/accuracy': 0.6725800037384033, 'validation/loss': 1.4372594356536865, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.1092770099639893, 'test/num_examples': 10000, 'score': 9275.317922115326, 'total_duration': 9660.388772964478, 'accumulated_submission_time': 9275.317922115326, 'accumulated_eval_time': 380.690767288208, 'accumulated_logging_time': 4.054809808731079}
I0428 08:17:22.884734 140168992380672 logging_writer.py:48] [25561] accumulated_eval_time=380.690767, accumulated_logging_time=4.054810, accumulated_submission_time=9275.317922, global_step=25561, preemption_count=0, score=9275.317922, test/accuracy=0.551200, test/loss=2.109277, test/num_examples=10000, total_duration=9660.388773, train/accuracy=0.768096, train/loss=1.001719, validation/accuracy=0.672580, validation/loss=1.437259, validation/num_examples=50000
I0428 08:17:37.342952 140169000773376 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2711998522281647, loss=2.308870553970337
I0428 08:18:13.193493 140168992380672 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.302059143781662, loss=2.3593358993530273
I0428 08:18:49.106659 140169000773376 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.277364581823349, loss=2.3340559005737305
I0428 08:19:24.954491 140168992380672 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3025340437889099, loss=2.302544593811035
I0428 08:20:00.961218 140169000773376 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.29997700452804565, loss=2.4118385314941406
I0428 08:20:36.834128 140168992380672 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.27869322896003723, loss=2.418588161468506
I0428 08:21:12.701225 140169000773376 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.29142215847969055, loss=2.3258304595947266
I0428 08:21:48.669367 140168992380672 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.26230618357658386, loss=2.414149761199951
I0428 08:22:24.506537 140169000773376 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2871667742729187, loss=2.398979902267456
I0428 08:23:00.455879 140168992380672 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2760353982448578, loss=2.3291821479797363
I0428 08:23:36.427009 140169000773376 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2973024845123291, loss=2.3316333293914795
I0428 08:24:12.324659 140168992380672 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2532620429992676, loss=2.280238389968872
I0428 08:24:48.213486 140169000773376 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.27412694692611694, loss=2.3850913047790527
I0428 08:25:24.169710 140168992380672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2745368182659149, loss=2.3197174072265625
I0428 08:25:53.051498 140375271311168 spec.py:298] Evaluating on the training split.
I0428 08:26:00.486715 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 08:26:11.206834 140375271311168 spec.py:326] Evaluating on the test split.
I0428 08:26:13.331241 140375271311168 submission_runner.py:415] Time since start: 10190.85s, 	Step: 26982, 	{'train/accuracy': 0.7863320708274841, 'train/loss': 0.927854061126709, 'validation/accuracy': 0.6860199570655823, 'validation/loss': 1.3794927597045898, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 2.074612855911255, 'test/num_examples': 10000, 'score': 9785.458061218262, 'total_duration': 10190.847925424576, 'accumulated_submission_time': 9785.458061218262, 'accumulated_eval_time': 400.96934390068054, 'accumulated_logging_time': 4.07645058631897}
I0428 08:26:13.340947 140169000773376 logging_writer.py:48] [26982] accumulated_eval_time=400.969344, accumulated_logging_time=4.076451, accumulated_submission_time=9785.458061, global_step=26982, preemption_count=0, score=9785.458061, test/accuracy=0.560200, test/loss=2.074613, test/num_examples=10000, total_duration=10190.847925, train/accuracy=0.786332, train/loss=0.927854, validation/accuracy=0.686020, validation/loss=1.379493, validation/num_examples=50000
I0428 08:26:20.175724 140168992380672 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.29084622859954834, loss=2.332258939743042
I0428 08:26:56.174577 140169000773376 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2617987394332886, loss=2.269580125808716
I0428 08:27:32.034246 140168992380672 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.307568222284317, loss=2.298473358154297
I0428 08:28:08.125771 140169000773376 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.28108176589012146, loss=2.4276976585388184
I0428 08:28:44.008742 140168992380672 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.26879191398620605, loss=2.3745758533477783
I0428 08:29:20.005813 140169000773376 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.28202641010284424, loss=2.404193878173828
I0428 08:29:55.888757 140168992380672 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.26504412293434143, loss=2.3673572540283203
I0428 08:30:31.807470 140169000773376 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.25776365399360657, loss=2.2585365772247314
I0428 08:31:07.655523 140168992380672 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2849186062812805, loss=2.374157428741455
I0428 08:31:43.747934 140169000773376 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.28949451446533203, loss=2.303541898727417
I0428 08:32:19.153563 140375271311168 spec.py:298] Evaluating on the training split.
I0428 08:32:26.849175 140375271311168 spec.py:310] Evaluating on the validation split.
I0428 08:32:37.306143 140375271311168 spec.py:326] Evaluating on the test split.
I0428 08:32:39.354041 140375271311168 submission_runner.py:415] Time since start: 10576.87s, 	Step: 28000, 	{'train/accuracy': 0.783203125, 'train/loss': 0.9359073638916016, 'validation/accuracy': 0.6788399815559387, 'validation/loss': 1.4047706127166748, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.1070914268493652, 'test/num_examples': 10000, 'score': 10151.249856233597, 'total_duration': 10576.870799064636, 'accumulated_submission_time': 10151.249856233597, 'accumulated_eval_time': 421.16872930526733, 'accumulated_logging_time': 4.094270706176758}
I0428 08:32:39.367075 140168992380672 logging_writer.py:48] [28000] accumulated_eval_time=421.168729, accumulated_logging_time=4.094271, accumulated_submission_time=10151.249856, global_step=28000, preemption_count=0, score=10151.249856, test/accuracy=0.547400, test/loss=2.107091, test/num_examples=10000, total_duration=10576.870799, train/accuracy=0.783203, train/loss=0.935907, validation/accuracy=0.678840, validation/loss=1.404771, validation/num_examples=50000
I0428 08:32:39.386188 140169000773376 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10151.249856
I0428 08:32:39.623616 140375271311168 checkpoints.py:356] Saving checkpoint at step: 28000
I0428 08:32:40.417371 140375271311168 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1/checkpoint_28000
I0428 08:32:40.429406 140375271311168 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_adafactor/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0428 08:32:40.844397 140375271311168 submission_runner.py:578] Tuning trial 1/1
I0428 08:32:40.844566 140375271311168 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0428 08:32:40.850661 140375271311168 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010961415246129036, 'train/loss': 6.911734104156494, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.912517547607422, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.91221284866333, 'test/num_examples': 10000, 'score': 92.43627095222473, 'total_duration': 133.83537101745605, 'accumulated_submission_time': 92.43627095222473, 'accumulated_eval_time': 41.398921489715576, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1419, {'train/accuracy': 0.12065529078245163, 'train/loss': 4.751684665679932, 'validation/accuracy': 0.10421999543905258, 'validation/loss': 4.905856609344482, 'validation/num_examples': 50000, 'test/accuracy': 0.07240000367164612, 'test/loss': 5.256279468536377, 'test/num_examples': 10000, 'score': 602.6431815624237, 'total_duration': 661.2522580623627, 'accumulated_submission_time': 602.6431815624237, 'accumulated_eval_time': 58.566415548324585, 'accumulated_logging_time': 0.02480316162109375, 'global_step': 1419, 'preemption_count': 0}), (2837, {'train/accuracy': 0.2703882157802582, 'train/loss': 3.5469212532043457, 'validation/accuracy': 0.2433599978685379, 'validation/loss': 3.7163445949554443, 'validation/num_examples': 50000, 'test/accuracy': 0.17980000376701355, 'test/loss': 4.252598285675049, 'test/num_examples': 10000, 'score': 1112.811322927475, 'total_duration': 1188.5747330188751, 'accumulated_submission_time': 1112.811322927475, 'accumulated_eval_time': 75.68721008300781, 'accumulated_logging_time': 0.04096055030822754, 'global_step': 2837, 'preemption_count': 0}), (4256, {'train/accuracy': 0.4065887928009033, 'train/loss': 2.753617525100708, 'validation/accuracy': 0.37101998925209045, 'validation/loss': 2.9373183250427246, 'validation/num_examples': 50000, 'test/accuracy': 0.28200000524520874, 'test/loss': 3.553682565689087, 'test/num_examples': 10000, 'score': 1622.816936969757, 'total_duration': 1715.7496042251587, 'accumulated_submission_time': 1622.816936969757, 'accumulated_eval_time': 92.82206082344055, 'accumulated_logging_time': 0.057936906814575195, 'global_step': 4256, 'preemption_count': 0}), (5677, {'train/accuracy': 0.5002790093421936, 'train/loss': 2.31588077545166, 'validation/accuracy': 0.462039977312088, 'validation/loss': 2.503946542739868, 'validation/num_examples': 50000, 'test/accuracy': 0.3540000021457672, 'test/loss': 3.1489052772521973, 'test/num_examples': 10000, 'score': 2133.092044353485, 'total_duration': 2243.0822422504425, 'accumulated_submission_time': 2133.092044353485, 'accumulated_eval_time': 109.8433530330658, 'accumulated_logging_time': 0.07629609107971191, 'global_step': 5677, 'preemption_count': 0}), (7097, {'train/accuracy': 0.5398995280265808, 'train/loss': 2.026991128921509, 'validation/accuracy': 0.49955999851226807, 'validation/loss': 2.23484468460083, 'validation/num_examples': 50000, 'test/accuracy': 0.38280001282691956, 'test/loss': 2.9803988933563232, 'test/num_examples': 10000, 'score': 2643.153190612793, 'total_duration': 2769.686674594879, 'accumulated_submission_time': 2643.153190612793, 'accumulated_eval_time': 126.35319113731384, 'accumulated_logging_time': 0.09176015853881836, 'global_step': 7097, 'preemption_count': 0}), (8513, {'train/accuracy': 0.5957828164100647, 'train/loss': 1.7705515623092651, 'validation/accuracy': 0.5486599802970886, 'validation/loss': 2.007369041442871, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.745323657989502, 'test/num_examples': 10000, 'score': 3153.3000948429108, 'total_duration': 3297.6209716796875, 'accumulated_submission_time': 3153.3000948429108, 'accumulated_eval_time': 144.10556650161743, 'accumulated_logging_time': 0.1091463565826416, 'global_step': 8513, 'preemption_count': 0}), (9935, {'train/accuracy': 0.6242027878761292, 'train/loss': 1.6778172254562378, 'validation/accuracy': 0.5746200084686279, 'validation/loss': 1.919921875, 'validation/num_examples': 50000, 'test/accuracy': 0.45260003209114075, 'test/loss': 2.587604522705078, 'test/num_examples': 10000, 'score': 3663.572422504425, 'total_duration': 3830.030287027359, 'accumulated_submission_time': 3663.572422504425, 'accumulated_eval_time': 162.5125172138214, 'accumulated_logging_time': 3.821786403656006, 'global_step': 9935, 'preemption_count': 0}), (11356, {'train/accuracy': 0.6428571343421936, 'train/loss': 1.5879040956497192, 'validation/accuracy': 0.5968799591064453, 'validation/loss': 1.8340562582015991, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.531586170196533, 'test/num_examples': 10000, 'score': 4173.597459793091, 'total_duration': 4359.107352972031, 'accumulated_submission_time': 4173.597459793091, 'accumulated_eval_time': 181.52844738960266, 'accumulated_logging_time': 3.8399691581726074, 'global_step': 11356, 'preemption_count': 0}), (12777, {'train/accuracy': 0.6744459271430969, 'train/loss': 1.4452184438705444, 'validation/accuracy': 0.6141999959945679, 'validation/loss': 1.72683584690094, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.4149646759033203, 'test/num_examples': 10000, 'score': 4683.763954877853, 'total_duration': 4888.387017250061, 'accumulated_submission_time': 4683.763954877853, 'accumulated_eval_time': 200.5997371673584, 'accumulated_logging_time': 3.8632779121398926, 'global_step': 12777, 'preemption_count': 0}), (14198, {'train/accuracy': 0.6900510191917419, 'train/loss': 1.341858148574829, 'validation/accuracy': 0.6246199607849121, 'validation/loss': 1.6429492235183716, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.3343467712402344, 'test/num_examples': 10000, 'score': 5193.996488571167, 'total_duration': 5418.404597997665, 'accumulated_submission_time': 5193.996488571167, 'accumulated_eval_time': 220.34972834587097, 'accumulated_logging_time': 3.880359649658203, 'global_step': 14198, 'preemption_count': 0}), (15619, {'train/accuracy': 0.7529097199440002, 'train/loss': 1.0699976682662964, 'validation/accuracy': 0.647379994392395, 'validation/loss': 1.5456516742706299, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.2468960285186768, 'test/num_examples': 10000, 'score': 5704.1424877643585, 'total_duration': 5948.825696706772, 'accumulated_submission_time': 5704.1424877643585, 'accumulated_eval_time': 240.57113027572632, 'accumulated_logging_time': 3.9152746200561523, 'global_step': 15619, 'preemption_count': 0}), (17040, {'train/accuracy': 0.7594068646430969, 'train/loss': 1.0625015497207642, 'validation/accuracy': 0.6574599742889404, 'validation/loss': 1.5182002782821655, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.200535774230957, 'test/num_examples': 10000, 'score': 6214.351387023926, 'total_duration': 6479.256758928299, 'accumulated_submission_time': 6214.351387023926, 'accumulated_eval_time': 260.755028963089, 'accumulated_logging_time': 3.9344100952148438, 'global_step': 17040, 'preemption_count': 0}), (18461, {'train/accuracy': 0.7450574040412903, 'train/loss': 1.1035239696502686, 'validation/accuracy': 0.6446200013160706, 'validation/loss': 1.5551667213439941, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.22076153755188, 'test/num_examples': 10000, 'score': 6724.484362602234, 'total_duration': 7009.0150628089905, 'accumulated_submission_time': 6724.484362602234, 'accumulated_eval_time': 280.33787631988525, 'accumulated_logging_time': 3.95845365524292, 'global_step': 18461, 'preemption_count': 0}), (19881, {'train/accuracy': 0.7553411722183228, 'train/loss': 1.0837185382843018, 'validation/accuracy': 0.6627999544143677, 'validation/loss': 1.5076336860656738, 'validation/num_examples': 50000, 'test/accuracy': 0.5412999987602234, 'test/loss': 2.189544200897217, 'test/num_examples': 10000, 'score': 7234.664556026459, 'total_duration': 7539.02343082428, 'accumulated_submission_time': 7234.664556026459, 'accumulated_eval_time': 300.12905263900757, 'accumulated_logging_time': 3.976736545562744, 'global_step': 19881, 'preemption_count': 0}), (21301, {'train/accuracy': 0.7557995915412903, 'train/loss': 1.0667427778244019, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.4904245138168335, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.181627035140991, 'test/num_examples': 10000, 'score': 7744.727421760559, 'total_duration': 8068.825463056564, 'accumulated_submission_time': 7744.727421760559, 'accumulated_eval_time': 319.8248043060303, 'accumulated_logging_time': 4.001823902130127, 'global_step': 21301, 'preemption_count': 0}), (22721, {'train/accuracy': 0.7662228941917419, 'train/loss': 1.038856863975525, 'validation/accuracy': 0.6686199903488159, 'validation/loss': 1.4797478914260864, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.185436487197876, 'test/num_examples': 10000, 'score': 8254.932767629623, 'total_duration': 8598.867062807083, 'accumulated_submission_time': 8254.932767629623, 'accumulated_eval_time': 339.62453985214233, 'accumulated_logging_time': 4.019684791564941, 'global_step': 22721, 'preemption_count': 0}), (24140, {'train/accuracy': 0.7686144709587097, 'train/loss': 0.9861076474189758, 'validation/accuracy': 0.6732199788093567, 'validation/loss': 1.4237844944000244, 'validation/num_examples': 50000, 'test/accuracy': 0.5460000038146973, 'test/loss': 2.1341655254364014, 'test/num_examples': 10000, 'score': 8765.007684707642, 'total_duration': 9129.629374742508, 'accumulated_submission_time': 8765.007684707642, 'accumulated_eval_time': 360.2776336669922, 'accumulated_logging_time': 4.036143064498901, 'global_step': 24140, 'preemption_count': 0}), (25561, {'train/accuracy': 0.7680962681770325, 'train/loss': 1.001719355583191, 'validation/accuracy': 0.6725800037384033, 'validation/loss': 1.4372594356536865, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.1092770099639893, 'test/num_examples': 10000, 'score': 9275.317922115326, 'total_duration': 9660.388772964478, 'accumulated_submission_time': 9275.317922115326, 'accumulated_eval_time': 380.690767288208, 'accumulated_logging_time': 4.054809808731079, 'global_step': 25561, 'preemption_count': 0}), (26982, {'train/accuracy': 0.7863320708274841, 'train/loss': 0.927854061126709, 'validation/accuracy': 0.6860199570655823, 'validation/loss': 1.3794927597045898, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 2.074612855911255, 'test/num_examples': 10000, 'score': 9785.458061218262, 'total_duration': 10190.847925424576, 'accumulated_submission_time': 9785.458061218262, 'accumulated_eval_time': 400.96934390068054, 'accumulated_logging_time': 4.07645058631897, 'global_step': 26982, 'preemption_count': 0}), (28000, {'train/accuracy': 0.783203125, 'train/loss': 0.9359073638916016, 'validation/accuracy': 0.6788399815559387, 'validation/loss': 1.4047706127166748, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.1070914268493652, 'test/num_examples': 10000, 'score': 10151.249856233597, 'total_duration': 10576.870799064636, 'accumulated_submission_time': 10151.249856233597, 'accumulated_eval_time': 421.16872930526733, 'accumulated_logging_time': 4.094270706176758, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0428 08:32:40.850803 140375271311168 submission_runner.py:581] Timing: 10151.249856233597
I0428 08:32:40.850850 140375271311168 submission_runner.py:582] ====================
I0428 08:32:40.850980 140375271311168 submission_runner.py:645] Final imagenet_resnet score: 10151.249856233597
