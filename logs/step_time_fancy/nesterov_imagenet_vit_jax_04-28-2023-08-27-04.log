python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/nesterov/jax/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_nesterov --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_04-28-2023-08-27-04.log
I0428 08:27:26.677598 140468099872576 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax.
I0428 08:27:26.750495 140468099872576 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 08:27:27.612458 140468099872576 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0428 08:27:27.613239 140468099872576 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 08:27:27.617733 140468099872576 submission_runner.py:538] Using RNG seed 1406711482
I0428 08:27:30.555527 140468099872576 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 08:27:30.555724 140468099872576 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1.
I0428 08:27:30.555919 140468099872576 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1/hparams.json.
I0428 08:27:30.678024 140468099872576 submission_runner.py:241] Initializing dataset.
I0428 08:27:30.691050 140468099872576 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:27:30.698873 140468099872576 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 08:27:30.699025 140468099872576 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 08:27:30.965832 140468099872576 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:27:37.676168 140468099872576 submission_runner.py:248] Initializing model.
I0428 08:27:48.776661 140468099872576 submission_runner.py:258] Initializing optimizer.
I0428 08:27:49.276916 140468099872576 submission_runner.py:265] Initializing metrics bundle.
I0428 08:27:49.277109 140468099872576 submission_runner.py:282] Initializing checkpoint and logger.
I0428 08:27:49.278074 140468099872576 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0428 08:27:50.221765 140468099872576 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1/meta_data_0.json.
I0428 08:27:50.222919 140468099872576 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1/flags_0.json.
I0428 08:27:50.227852 140468099872576 submission_runner.py:318] Starting training loop.
I0428 08:28:38.090442 140285724022528 logging_writer.py:48] [0] global_step=0, grad_norm=0.2849441468715668, loss=6.907753944396973
I0428 08:28:38.104427 140468099872576 spec.py:298] Evaluating on the training split.
I0428 08:28:38.110517 140468099872576 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:28:38.116329 140468099872576 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 08:28:38.116444 140468099872576 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 08:28:38.176126 140468099872576 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:28:58.784874 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 08:28:58.792518 140468099872576 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:28:58.804580 140468099872576 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 08:28:58.804890 140468099872576 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 08:28:58.862680 140468099872576 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:29:17.748723 140468099872576 spec.py:326] Evaluating on the test split.
I0428 08:29:17.756414 140468099872576 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 08:29:17.763207 140468099872576 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 08:29:17.794492 140468099872576 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 08:29:28.706932 140468099872576 submission_runner.py:415] Time since start: 98.48s, 	Step: 1, 	{'train/accuracy': 0.0008203124743886292, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 47.87640738487244, 'total_duration': 98.47901773452759, 'accumulated_submission_time': 47.87640738487244, 'accumulated_eval_time': 50.602452516555786, 'accumulated_logging_time': 0}
I0428 08:29:28.723746 140227796530944 logging_writer.py:48] [1] accumulated_eval_time=50.602453, accumulated_logging_time=0, accumulated_submission_time=47.876407, global_step=1, preemption_count=0, score=47.876407, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=98.479018, train/accuracy=0.000820, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0428 08:30:22.557122 140286890014464 logging_writer.py:48] [100] global_step=100, grad_norm=0.2868240177631378, loss=6.9050493240356445
I0428 08:31:02.951723 140286898407168 logging_writer.py:48] [200] global_step=200, grad_norm=0.2876937985420227, loss=6.898165225982666
I0428 08:31:43.415292 140286890014464 logging_writer.py:48] [300] global_step=300, grad_norm=0.30240634083747864, loss=6.886466979980469
I0428 08:32:23.784690 140286898407168 logging_writer.py:48] [400] global_step=400, grad_norm=0.47786635160446167, loss=6.8297882080078125
I0428 08:33:04.200354 140286890014464 logging_writer.py:48] [500] global_step=500, grad_norm=1.1890822649002075, loss=6.7384185791015625
I0428 08:33:45.056658 140286898407168 logging_writer.py:48] [600] global_step=600, grad_norm=0.6923202872276306, loss=6.718652725219727
I0428 08:34:25.581197 140286890014464 logging_writer.py:48] [700] global_step=700, grad_norm=1.2121927738189697, loss=6.627074241638184
I0428 08:35:06.321008 140286898407168 logging_writer.py:48] [800] global_step=800, grad_norm=0.9221752882003784, loss=6.684360504150391
I0428 08:35:46.849750 140286890014464 logging_writer.py:48] [900] global_step=900, grad_norm=0.8905715942382812, loss=6.539871692657471
I0428 08:36:27.489036 140286898407168 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.143614649772644, loss=6.599734306335449
I0428 08:36:28.820183 140468099872576 spec.py:298] Evaluating on the training split.
I0428 08:36:39.902274 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 08:36:46.385762 140468099872576 spec.py:326] Evaluating on the test split.
I0428 08:36:48.467829 140468099872576 submission_runner.py:415] Time since start: 538.24s, 	Step: 1005, 	{'train/accuracy': 0.0373242162168026, 'train/loss': 6.043600559234619, 'validation/accuracy': 0.036240000277757645, 'validation/loss': 6.072470664978027, 'validation/num_examples': 50000, 'test/accuracy': 0.026500001549720764, 'test/loss': 6.168850421905518, 'test/num_examples': 10000, 'score': 467.94335865974426, 'total_duration': 538.2398612499237, 'accumulated_submission_time': 467.94335865974426, 'accumulated_eval_time': 70.25005269050598, 'accumulated_logging_time': 0.024674415588378906}
I0428 08:36:48.492874 140228106897152 logging_writer.py:48] [1005] accumulated_eval_time=70.250053, accumulated_logging_time=0.024674, accumulated_submission_time=467.943359, global_step=1005, preemption_count=0, score=467.943359, test/accuracy=0.026500, test/loss=6.168850, test/num_examples=10000, total_duration=538.239861, train/accuracy=0.037324, train/loss=6.043601, validation/accuracy=0.036240, validation/loss=6.072471, validation/num_examples=50000
I0428 08:37:27.340995 140228115289856 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7082377672195435, loss=6.536848068237305
I0428 08:38:07.651980 140228106897152 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.062682867050171, loss=6.431800842285156
I0428 08:38:47.940709 140228115289856 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0440146923065186, loss=6.79405403137207
I0428 08:39:28.574388 140228106897152 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8344587683677673, loss=6.538260459899902
I0428 08:40:09.078744 140228115289856 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3515098094940186, loss=6.384342193603516
I0428 08:40:49.783766 140228106897152 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.031794786453247, loss=6.33623743057251
I0428 08:41:30.474554 140228115289856 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.193469762802124, loss=6.369488716125488
I0428 08:42:11.172510 140228106897152 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.055382251739502, loss=6.304661273956299
I0428 08:42:51.882555 140228115289856 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8293037414550781, loss=6.273446559906006
I0428 08:43:32.572237 140228106897152 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6363503336906433, loss=6.641701698303223
I0428 08:43:48.873536 140468099872576 spec.py:298] Evaluating on the training split.
I0428 08:44:00.112755 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 08:44:06.694249 140468099872576 spec.py:326] Evaluating on the test split.
I0428 08:44:08.441493 140468099872576 submission_runner.py:415] Time since start: 978.21s, 	Step: 2042, 	{'train/accuracy': 0.07175780832767487, 'train/loss': 5.516951084136963, 'validation/accuracy': 0.0663599967956543, 'validation/loss': 5.558749675750732, 'validation/num_examples': 50000, 'test/accuracy': 0.04580000042915344, 'test/loss': 5.734161853790283, 'test/num_examples': 10000, 'score': 888.2692646980286, 'total_duration': 978.2135407924652, 'accumulated_submission_time': 888.2692646980286, 'accumulated_eval_time': 89.81798124313354, 'accumulated_logging_time': 0.0823972225189209}
I0428 08:44:08.455670 140228115289856 logging_writer.py:48] [2042] accumulated_eval_time=89.817981, accumulated_logging_time=0.082397, accumulated_submission_time=888.269265, global_step=2042, preemption_count=0, score=888.269265, test/accuracy=0.045800, test/loss=5.734162, test/num_examples=10000, total_duration=978.213541, train/accuracy=0.071758, train/loss=5.516951, validation/accuracy=0.066360, validation/loss=5.558750, validation/num_examples=50000
I0428 08:44:32.398542 140228106897152 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0320714712142944, loss=6.5100860595703125
I0428 08:45:12.592333 140228115289856 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8313387036323547, loss=6.234835624694824
I0428 08:45:53.031711 140228106897152 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7450371384620667, loss=6.33955717086792
I0428 08:46:33.815494 140228115289856 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7353945374488831, loss=6.231282711029053
I0428 08:47:14.415492 140228106897152 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7671747803688049, loss=6.256464004516602
I0428 08:47:54.870112 140228115289856 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6736871004104614, loss=6.703645706176758
I0428 08:48:35.623988 140228106897152 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7236543297767639, loss=6.681340217590332
I0428 08:49:16.388552 140228115289856 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.717272937297821, loss=6.17537260055542
I0428 08:49:57.152995 140228106897152 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6895082592964172, loss=6.707160472869873
I0428 08:50:37.615313 140228115289856 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7240861058235168, loss=6.572968482971191
I0428 08:51:08.736298 140468099872576 spec.py:298] Evaluating on the training split.
I0428 08:51:19.814546 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 08:51:26.519082 140468099872576 spec.py:326] Evaluating on the test split.
I0428 08:51:28.238035 140468099872576 submission_runner.py:415] Time since start: 1418.01s, 	Step: 3078, 	{'train/accuracy': 0.09431640058755875, 'train/loss': 5.234086036682129, 'validation/accuracy': 0.08912000060081482, 'validation/loss': 5.284057140350342, 'validation/num_examples': 50000, 'test/accuracy': 0.06710000336170197, 'test/loss': 5.516386985778809, 'test/num_examples': 10000, 'score': 1308.5137221813202, 'total_duration': 1418.0100915431976, 'accumulated_submission_time': 1308.5137221813202, 'accumulated_eval_time': 109.3197009563446, 'accumulated_logging_time': 0.11086511611938477}
I0428 08:51:28.255518 140228106897152 logging_writer.py:48] [3078] accumulated_eval_time=109.319701, accumulated_logging_time=0.110865, accumulated_submission_time=1308.513722, global_step=3078, preemption_count=0, score=1308.513722, test/accuracy=0.067100, test/loss=5.516387, test/num_examples=10000, total_duration=1418.010092, train/accuracy=0.094316, train/loss=5.234086, validation/accuracy=0.089120, validation/loss=5.284057, validation/num_examples=50000
I0428 08:51:37.653127 140228115289856 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8902481198310852, loss=6.270670413970947
I0428 08:52:18.354202 140228106897152 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.009275197982788, loss=6.0420308113098145
I0428 08:52:59.041979 140228115289856 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7257439494132996, loss=6.112186431884766
I0428 08:53:39.715270 140228106897152 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.799048125743866, loss=6.033148288726807
I0428 08:54:20.503360 140228115289856 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7887474894523621, loss=6.10896635055542
I0428 08:55:01.095063 140228106897152 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7549098134040833, loss=6.136207580566406
I0428 08:55:41.621106 140228115289856 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7100561857223511, loss=5.933290004730225
I0428 08:56:22.416664 140228106897152 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5451954007148743, loss=6.628335475921631
I0428 08:57:03.046948 140228115289856 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5547434091567993, loss=6.308719635009766
I0428 08:57:43.723784 140228106897152 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7625338435173035, loss=6.67487096786499
I0428 08:58:24.537890 140228115289856 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8010880947113037, loss=5.895030975341797
I0428 08:58:28.330595 140468099872576 spec.py:298] Evaluating on the training split.
I0428 08:58:39.687739 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 08:58:46.528442 140468099872576 spec.py:326] Evaluating on the test split.
I0428 08:58:48.236885 140468099872576 submission_runner.py:415] Time since start: 1858.01s, 	Step: 4111, 	{'train/accuracy': 0.1280859410762787, 'train/loss': 4.956143379211426, 'validation/accuracy': 0.11177999526262283, 'validation/loss': 5.053415775299072, 'validation/num_examples': 50000, 'test/accuracy': 0.08640000224113464, 'test/loss': 5.311581611633301, 'test/num_examples': 10000, 'score': 1728.5482530593872, 'total_duration': 1858.0089581012726, 'accumulated_submission_time': 1728.5482530593872, 'accumulated_eval_time': 129.22598838806152, 'accumulated_logging_time': 0.14727282524108887}
I0428 08:58:48.248514 140228106897152 logging_writer.py:48] [4111] accumulated_eval_time=129.225988, accumulated_logging_time=0.147273, accumulated_submission_time=1728.548253, global_step=4111, preemption_count=0, score=1728.548253, test/accuracy=0.086400, test/loss=5.311582, test/num_examples=10000, total_duration=1858.008958, train/accuracy=0.128086, train/loss=4.956143, validation/accuracy=0.111780, validation/loss=5.053416, validation/num_examples=50000
I0428 08:59:24.772614 140228115289856 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7856694459915161, loss=5.873292922973633
I0428 09:00:05.218936 140228106897152 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7215954661369324, loss=5.875996112823486
I0428 09:00:45.750170 140228115289856 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6959641575813293, loss=6.569815635681152
I0428 09:01:26.553066 140228106897152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7834970951080322, loss=5.912351608276367
I0428 09:02:07.178369 140228115289856 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5570785403251648, loss=6.402292728424072
I0428 09:02:47.909177 140228106897152 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.740408718585968, loss=6.599083423614502
I0428 09:03:28.677778 140228115289856 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8841177821159363, loss=5.95302152633667
I0428 09:04:09.517047 140228106897152 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5751074552536011, loss=5.844747066497803
I0428 09:04:50.300171 140228115289856 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.687403678894043, loss=5.831976890563965
I0428 09:05:30.913576 140228106897152 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.4677870273590088, loss=6.616180896759033
I0428 09:05:48.565050 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:05:59.716436 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:06:06.606333 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:06:08.315115 140468099872576 submission_runner.py:415] Time since start: 2298.09s, 	Step: 5145, 	{'train/accuracy': 0.15761718153953552, 'train/loss': 4.644299030303955, 'validation/accuracy': 0.14627999067306519, 'validation/loss': 4.7104902267456055, 'validation/num_examples': 50000, 'test/accuracy': 0.11050000786781311, 'test/loss': 5.040230751037598, 'test/num_examples': 10000, 'score': 2148.808512687683, 'total_duration': 2298.0871205329895, 'accumulated_submission_time': 2148.808512687683, 'accumulated_eval_time': 148.97597360610962, 'accumulated_logging_time': 0.19334650039672852}
I0428 09:06:08.329632 140228115289856 logging_writer.py:48] [5145] accumulated_eval_time=148.975974, accumulated_logging_time=0.193347, accumulated_submission_time=2148.808513, global_step=5145, preemption_count=0, score=2148.808513, test/accuracy=0.110500, test/loss=5.040231, test/num_examples=10000, total_duration=2298.087121, train/accuracy=0.157617, train/loss=4.644299, validation/accuracy=0.146280, validation/loss=4.710490, validation/num_examples=50000
I0428 09:06:31.076297 140228106897152 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5783355832099915, loss=6.175654411315918
I0428 09:07:11.451407 140228115289856 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7277013063430786, loss=5.785987854003906
I0428 09:07:52.078176 140228106897152 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7239163517951965, loss=5.813162803649902
I0428 09:08:32.958927 140228115289856 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8203545808792114, loss=5.795345306396484
I0428 09:09:13.790080 140228106897152 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6146856546401978, loss=6.418093681335449
I0428 09:09:54.667367 140228115289856 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8554328083992004, loss=5.649468898773193
I0428 09:10:35.383821 140228106897152 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5442804098129272, loss=6.282063961029053
I0428 09:11:16.123967 140228115289856 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.665314793586731, loss=5.78350305557251
I0428 09:11:57.165906 140228106897152 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5350095629692078, loss=5.8811235427856445
I0428 09:12:37.690336 140228115289856 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6597740054130554, loss=5.576456546783447
I0428 09:13:08.388923 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:13:19.696152 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:13:26.705625 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:13:28.410331 140468099872576 submission_runner.py:415] Time since start: 2738.18s, 	Step: 6177, 	{'train/accuracy': 0.19738280773162842, 'train/loss': 4.310327053070068, 'validation/accuracy': 0.18094000220298767, 'validation/loss': 4.416280746459961, 'validation/num_examples': 50000, 'test/accuracy': 0.13930000364780426, 'test/loss': 4.79020357131958, 'test/num_examples': 10000, 'score': 2568.8283026218414, 'total_duration': 2738.1823987960815, 'accumulated_submission_time': 2568.8283026218414, 'accumulated_eval_time': 168.9973759651184, 'accumulated_logging_time': 0.22543811798095703}
I0428 09:13:28.425468 140228106897152 logging_writer.py:48] [6177] accumulated_eval_time=168.997376, accumulated_logging_time=0.225438, accumulated_submission_time=2568.828303, global_step=6177, preemption_count=0, score=2568.828303, test/accuracy=0.139300, test/loss=4.790204, test/num_examples=10000, total_duration=2738.182399, train/accuracy=0.197383, train/loss=4.310327, validation/accuracy=0.180940, validation/loss=4.416281, validation/num_examples=50000
I0428 09:13:38.205659 140228115289856 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7303836345672607, loss=5.6410231590271
I0428 09:14:18.584241 140228106897152 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7364642024040222, loss=5.632164001464844
I0428 09:14:59.209078 140228115289856 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4714387357234955, loss=6.521387100219727
I0428 09:15:39.935263 140228106897152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8707320094108582, loss=5.603236675262451
I0428 09:16:20.778120 140228115289856 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5863633751869202, loss=5.878839492797852
I0428 09:17:01.573317 140228106897152 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4772718846797943, loss=6.531117916107178
I0428 09:17:42.423169 140228115289856 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.1059751510620117, loss=5.58555269241333
I0428 09:18:23.442209 140228106897152 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.612833559513092, loss=5.703742504119873
I0428 09:19:04.263582 140228115289856 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6462938785552979, loss=5.477114200592041
I0428 09:19:45.303964 140228106897152 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5776693820953369, loss=6.55343770980835
I0428 09:20:26.203663 140228115289856 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.435452938079834, loss=6.447314262390137
I0428 09:20:28.830655 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:20:40.429412 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:20:47.785207 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:20:49.493485 140468099872576 submission_runner.py:415] Time since start: 3179.27s, 	Step: 7208, 	{'train/accuracy': 0.23017577826976776, 'train/loss': 4.0920844078063965, 'validation/accuracy': 0.2101600021123886, 'validation/loss': 4.196775436401367, 'validation/num_examples': 50000, 'test/accuracy': 0.164000004529953, 'test/loss': 4.5667500495910645, 'test/num_examples': 10000, 'score': 2989.196631193161, 'total_duration': 3179.2655251026154, 'accumulated_submission_time': 2989.196631193161, 'accumulated_eval_time': 189.6601767539978, 'accumulated_logging_time': 0.2559056282043457}
I0428 09:20:49.509167 140228106897152 logging_writer.py:48] [7208] accumulated_eval_time=189.660177, accumulated_logging_time=0.255906, accumulated_submission_time=2989.196631, global_step=7208, preemption_count=0, score=2989.196631, test/accuracy=0.164000, test/loss=4.566750, test/num_examples=10000, total_duration=3179.265525, train/accuracy=0.230176, train/loss=4.092084, validation/accuracy=0.210160, validation/loss=4.196775, validation/num_examples=50000
I0428 09:21:27.354442 140228115289856 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7285942435264587, loss=5.4566874504089355
I0428 09:22:07.905420 140228106897152 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6834053993225098, loss=5.486830711364746
I0428 09:22:49.178060 140228115289856 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6721076965332031, loss=5.365970611572266
I0428 09:23:29.997991 140228106897152 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5261594653129578, loss=6.062512397766113
I0428 09:24:10.959904 140228115289856 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6047369837760925, loss=5.432008266448975
I0428 09:24:52.037888 140228106897152 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6865951418876648, loss=5.300708293914795
I0428 09:25:32.829588 140228115289856 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6145620942115784, loss=5.3393144607543945
I0428 09:26:13.483395 140228106897152 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6889062523841858, loss=5.3670196533203125
I0428 09:26:54.521762 140228115289856 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7644529342651367, loss=5.381030082702637
I0428 09:27:36.386346 140228106897152 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7323225736618042, loss=5.397785186767578
I0428 09:27:49.732610 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:28:02.220617 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:28:09.903155 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:28:11.601970 140468099872576 submission_runner.py:415] Time since start: 3621.37s, 	Step: 8234, 	{'train/accuracy': 0.27037107944488525, 'train/loss': 3.848663806915283, 'validation/accuracy': 0.24577999114990234, 'validation/loss': 3.971712350845337, 'validation/num_examples': 50000, 'test/accuracy': 0.18860000371932983, 'test/loss': 4.398263454437256, 'test/num_examples': 10000, 'score': 3409.3865525722504, 'total_duration': 3621.3740136623383, 'accumulated_submission_time': 3409.3865525722504, 'accumulated_eval_time': 211.52949571609497, 'accumulated_logging_time': 0.283541202545166}
I0428 09:28:11.614097 140228115289856 logging_writer.py:48] [8234] accumulated_eval_time=211.529496, accumulated_logging_time=0.283541, accumulated_submission_time=3409.386553, global_step=8234, preemption_count=0, score=3409.386553, test/accuracy=0.188600, test/loss=4.398263, test/num_examples=10000, total_duration=3621.374014, train/accuracy=0.270371, train/loss=3.848664, validation/accuracy=0.245780, validation/loss=3.971712, validation/num_examples=50000
I0428 09:28:38.696964 140228106897152 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.49517354369163513, loss=5.855824947357178
I0428 09:29:19.120345 140228115289856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6908559203147888, loss=5.250202178955078
I0428 09:30:00.087546 140228106897152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.40953922271728516, loss=6.344113349914551
I0428 09:30:43.193181 140228115289856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5583155155181885, loss=5.303342819213867
I0428 09:31:24.079789 140228106897152 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7401326298713684, loss=5.607262134552002
I0428 09:32:05.097382 140228115289856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5954212546348572, loss=5.457907676696777
I0428 09:32:45.670295 140228106897152 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5426655411720276, loss=5.513710021972656
I0428 09:33:26.665986 140228115289856 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6512401700019836, loss=5.172857284545898
I0428 09:34:07.763265 140228106897152 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6903342604637146, loss=5.215285778045654
I0428 09:34:48.570123 140228115289856 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4732823073863983, loss=6.441581726074219
I0428 09:35:11.765543 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:35:26.558320 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:35:35.027528 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:35:36.728672 140468099872576 submission_runner.py:415] Time since start: 4066.50s, 	Step: 9258, 	{'train/accuracy': 0.3178125023841858, 'train/loss': 3.564225435256958, 'validation/accuracy': 0.27493998408317566, 'validation/loss': 3.775895595550537, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.234241008758545, 'test/num_examples': 10000, 'score': 3829.5026593208313, 'total_duration': 4066.500720024109, 'accumulated_submission_time': 3829.5026593208313, 'accumulated_eval_time': 236.4925787448883, 'accumulated_logging_time': 0.3088507652282715}
I0428 09:35:36.738718 140228106897152 logging_writer.py:48] [9258] accumulated_eval_time=236.492579, accumulated_logging_time=0.308851, accumulated_submission_time=3829.502659, global_step=9258, preemption_count=0, score=3829.502659, test/accuracy=0.208200, test/loss=4.234241, test/num_examples=10000, total_duration=4066.500720, train/accuracy=0.317813, train/loss=3.564225, validation/accuracy=0.274940, validation/loss=3.775896, validation/num_examples=50000
I0428 09:35:54.261784 140228115289856 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6111798882484436, loss=5.449893951416016
I0428 09:36:34.704703 140228106897152 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5819723606109619, loss=5.284807205200195
I0428 09:37:15.396195 140228115289856 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4269416630268097, loss=6.300914764404297
I0428 09:37:56.181273 140228106897152 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6040366291999817, loss=5.220287322998047
I0428 09:38:37.403483 140228115289856 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.514968991279602, loss=5.306092262268066
I0428 09:39:18.736593 140228106897152 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5909935235977173, loss=6.337971210479736
I0428 09:39:59.766285 140228115289856 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.638002336025238, loss=5.107320785522461
I0428 09:40:40.885280 140228106897152 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6444078087806702, loss=5.074446201324463
I0428 09:41:21.934490 140228115289856 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.542204737663269, loss=5.307058334350586
I0428 09:42:02.789203 140228106897152 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6486852169036865, loss=5.130373477935791
I0428 09:42:36.872853 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:42:50.576874 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:42:59.220751 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:43:00.929371 140468099872576 submission_runner.py:415] Time since start: 4510.70s, 	Step: 10283, 	{'train/accuracy': 0.3371874988079071, 'train/loss': 3.3881728649139404, 'validation/accuracy': 0.31092000007629395, 'validation/loss': 3.5186643600463867, 'validation/num_examples': 50000, 'test/accuracy': 0.23910000920295715, 'test/loss': 4.009396553039551, 'test/num_examples': 10000, 'score': 4249.602550029755, 'total_duration': 4510.701415300369, 'accumulated_submission_time': 4249.602550029755, 'accumulated_eval_time': 260.54908990859985, 'accumulated_logging_time': 0.3316512107849121}
I0428 09:43:00.945804 140228115289856 logging_writer.py:48] [10283] accumulated_eval_time=260.549090, accumulated_logging_time=0.331651, accumulated_submission_time=4249.602550, global_step=10283, preemption_count=0, score=4249.602550, test/accuracy=0.239100, test/loss=4.009397, test/num_examples=10000, total_duration=4510.701415, train/accuracy=0.337187, train/loss=3.388173, validation/accuracy=0.310920, validation/loss=3.518664, validation/num_examples=50000
I0428 09:43:08.336148 140228106897152 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5512913465499878, loss=5.395778656005859
I0428 09:43:49.092721 140228115289856 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5946789383888245, loss=5.040870666503906
I0428 09:44:30.042855 140228106897152 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6855751276016235, loss=4.947349548339844
I0428 09:45:10.664315 140228115289856 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7144003510475159, loss=5.139742851257324
I0428 09:45:51.278513 140228106897152 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5316574573516846, loss=6.2798004150390625
I0428 09:46:32.004013 140228115289856 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6428194046020508, loss=4.972304344177246
I0428 09:47:12.983528 140228106897152 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7003745436668396, loss=4.993861198425293
I0428 09:47:53.840166 140228115289856 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.4707737863063812, loss=6.325936794281006
I0428 09:48:34.672843 140228106897152 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5415420532226562, loss=6.319750785827637
I0428 09:49:16.009257 140228115289856 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5688300132751465, loss=5.213338375091553
I0428 09:49:57.156944 140228106897152 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6468413472175598, loss=4.968324661254883
I0428 09:50:00.997199 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:50:14.882703 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:50:23.747211 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:50:25.448687 140468099872576 submission_runner.py:415] Time since start: 4955.22s, 	Step: 11311, 	{'train/accuracy': 0.3551953136920929, 'train/loss': 3.2805843353271484, 'validation/accuracy': 0.3273399770259857, 'validation/loss': 3.422231435775757, 'validation/num_examples': 50000, 'test/accuracy': 0.24910001456737518, 'test/loss': 3.9348411560058594, 'test/num_examples': 10000, 'score': 4669.61865401268, 'total_duration': 4955.220681428909, 'accumulated_submission_time': 4669.61865401268, 'accumulated_eval_time': 285.0004780292511, 'accumulated_logging_time': 0.36130452156066895}
I0428 09:50:25.465242 140228115289856 logging_writer.py:48] [11311] accumulated_eval_time=285.000478, accumulated_logging_time=0.361305, accumulated_submission_time=4669.618654, global_step=11311, preemption_count=0, score=4669.618654, test/accuracy=0.249100, test/loss=3.934841, test/num_examples=10000, total_duration=4955.220681, train/accuracy=0.355195, train/loss=3.280584, validation/accuracy=0.327340, validation/loss=3.422231, validation/num_examples=50000
I0428 09:51:01.962799 140228106897152 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5838810801506042, loss=5.475792407989502
I0428 09:51:42.703308 140228115289856 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5286929607391357, loss=5.297787666320801
I0428 09:52:23.489800 140228106897152 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.48930197954177856, loss=5.808340072631836
I0428 09:53:04.271697 140228115289856 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7024090886116028, loss=4.826014518737793
I0428 09:53:45.141406 140228106897152 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4074820578098297, loss=6.095191955566406
I0428 09:54:25.986326 140228115289856 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.43002888560295105, loss=6.290585041046143
I0428 09:55:06.735124 140228106897152 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4951072931289673, loss=5.276059627532959
I0428 09:55:47.797100 140228115289856 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5757842659950256, loss=4.851781845092773
I0428 09:56:29.042303 140228106897152 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8012079000473022, loss=4.86215877532959
I0428 09:57:10.641983 140228115289856 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5007169842720032, loss=6.201958179473877
I0428 09:57:25.540984 140468099872576 spec.py:298] Evaluating on the training split.
I0428 09:57:39.664147 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 09:57:48.408771 140468099872576 spec.py:326] Evaluating on the test split.
I0428 09:57:50.105093 140468099872576 submission_runner.py:415] Time since start: 5399.88s, 	Step: 12338, 	{'train/accuracy': 0.39113280177116394, 'train/loss': 2.9794740676879883, 'validation/accuracy': 0.3610999882221222, 'validation/loss': 3.1409809589385986, 'validation/num_examples': 50000, 'test/accuracy': 0.27890002727508545, 'test/loss': 3.6962387561798096, 'test/num_examples': 10000, 'score': 5089.6630136966705, 'total_duration': 5399.877131700516, 'accumulated_submission_time': 5089.6630136966705, 'accumulated_eval_time': 309.56455731391907, 'accumulated_logging_time': 0.3876206874847412}
I0428 09:57:50.119677 140228106897152 logging_writer.py:48] [12338] accumulated_eval_time=309.564557, accumulated_logging_time=0.387621, accumulated_submission_time=5089.663014, global_step=12338, preemption_count=0, score=5089.663014, test/accuracy=0.278900, test/loss=3.696239, test/num_examples=10000, total_duration=5399.877132, train/accuracy=0.391133, train/loss=2.979474, validation/accuracy=0.361100, validation/loss=3.140981, validation/num_examples=50000
I0428 09:58:15.775142 140228115289856 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4642884433269501, loss=6.273860931396484
I0428 09:58:56.911998 140228106897152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5874898433685303, loss=5.040278434753418
I0428 09:59:37.724286 140228115289856 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5334276556968689, loss=5.578867435455322
I0428 10:00:18.980330 140228106897152 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6236830353736877, loss=4.826674461364746
I0428 10:01:00.278982 140228115289856 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5943673849105835, loss=4.904483318328857
I0428 10:01:41.191468 140228106897152 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.44028767943382263, loss=6.240849018096924
I0428 10:02:22.747728 140228115289856 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5750939249992371, loss=5.267599105834961
I0428 10:03:04.504506 140228106897152 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5550714135169983, loss=6.241358757019043
I0428 10:03:45.785508 140228115289856 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6733863353729248, loss=4.920727252960205
I0428 10:04:26.740710 140228106897152 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4626915454864502, loss=5.987506866455078
I0428 10:04:50.142842 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:05:04.371294 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:05:13.212209 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:05:14.909625 140468099872576 submission_runner.py:415] Time since start: 5844.68s, 	Step: 13359, 	{'train/accuracy': 0.41843748092651367, 'train/loss': 2.9201531410217285, 'validation/accuracy': 0.371319979429245, 'validation/loss': 3.1370153427124023, 'validation/num_examples': 50000, 'test/accuracy': 0.28440001606941223, 'test/loss': 3.6904637813568115, 'test/num_examples': 10000, 'score': 5509.6521809101105, 'total_duration': 5844.681668281555, 'accumulated_submission_time': 5509.6521809101105, 'accumulated_eval_time': 334.3312931060791, 'accumulated_logging_time': 0.41480255126953125}
I0428 10:05:14.925212 140228115289856 logging_writer.py:48] [13359] accumulated_eval_time=334.331293, accumulated_logging_time=0.414803, accumulated_submission_time=5509.652181, global_step=13359, preemption_count=0, score=5509.652181, test/accuracy=0.284400, test/loss=3.690464, test/num_examples=10000, total_duration=5844.681668, train/accuracy=0.418437, train/loss=2.920153, validation/accuracy=0.371320, validation/loss=3.137015, validation/num_examples=50000
I0428 10:05:32.069063 140228106897152 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5834873914718628, loss=4.643638610839844
I0428 10:06:12.894092 140228115289856 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.551609992980957, loss=5.393270492553711
I0428 10:06:54.300730 140228106897152 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6329072117805481, loss=5.384166240692139
I0428 10:07:35.536471 140228115289856 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.696001410484314, loss=4.818761825561523
I0428 10:08:17.363883 140228106897152 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6521565318107605, loss=4.62687873840332
I0428 10:08:58.485287 140228115289856 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.4853675067424774, loss=5.987504005432129
I0428 10:09:39.901784 140228106897152 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.48457401990890503, loss=6.0926289558410645
I0428 10:10:21.402918 140228115289856 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.46494945883750916, loss=6.256737232208252
I0428 10:11:02.812471 140228106897152 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5712367296218872, loss=6.251495838165283
I0428 10:11:44.567961 140228115289856 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5720739960670471, loss=4.963076591491699
I0428 10:12:15.097911 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:12:29.147539 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:12:38.216042 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:12:39.923763 140468099872576 submission_runner.py:415] Time since start: 6289.70s, 	Step: 14375, 	{'train/accuracy': 0.4328710734844208, 'train/loss': 2.8061885833740234, 'validation/accuracy': 0.3980199992656708, 'validation/loss': 2.9603617191314697, 'validation/num_examples': 50000, 'test/accuracy': 0.3019000291824341, 'test/loss': 3.5383048057556152, 'test/num_examples': 10000, 'score': 5929.791624307632, 'total_duration': 6289.695822477341, 'accumulated_submission_time': 5929.791624307632, 'accumulated_eval_time': 359.15713381767273, 'accumulated_logging_time': 0.44176411628723145}
I0428 10:12:39.937591 140228106897152 logging_writer.py:48] [14375] accumulated_eval_time=359.157134, accumulated_logging_time=0.441764, accumulated_submission_time=5929.791624, global_step=14375, preemption_count=0, score=5929.791624, test/accuracy=0.301900, test/loss=3.538305, test/num_examples=10000, total_duration=6289.695822, train/accuracy=0.432871, train/loss=2.806189, validation/accuracy=0.398020, validation/loss=2.960362, validation/num_examples=50000
I0428 10:12:50.564448 140228115289856 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5408843755722046, loss=5.334579944610596
I0428 10:13:31.568368 140228106897152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6431211233139038, loss=4.864581108093262
I0428 10:14:13.209007 140228115289856 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.6234776973724365, loss=4.835857391357422
I0428 10:14:55.025511 140228106897152 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5769559144973755, loss=5.041891098022461
I0428 10:15:36.572353 140228115289856 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5888745188713074, loss=5.006681442260742
I0428 10:16:17.768942 140228106897152 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.4923430383205414, loss=5.5980000495910645
I0428 10:16:59.698760 140228115289856 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5076749324798584, loss=5.888178825378418
I0428 10:17:41.579123 140228106897152 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5319318771362305, loss=5.446612358093262
I0428 10:18:23.208206 140228115289856 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5639534592628479, loss=4.873885631561279
I0428 10:19:05.209565 140228106897152 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6976956129074097, loss=4.7538371086120605
I0428 10:19:40.267952 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:19:54.548898 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:20:03.842389 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:20:05.551778 140468099872576 submission_runner.py:415] Time since start: 6735.32s, 	Step: 15386, 	{'train/accuracy': 0.4485156238079071, 'train/loss': 2.7377374172210693, 'validation/accuracy': 0.411219984292984, 'validation/loss': 2.9108266830444336, 'validation/num_examples': 50000, 'test/accuracy': 0.31940001249313354, 'test/loss': 3.4816079139709473, 'test/num_examples': 10000, 'score': 6350.0647003650665, 'total_duration': 6735.316172361374, 'accumulated_submission_time': 6350.0647003650665, 'accumulated_eval_time': 384.43329763412476, 'accumulated_logging_time': 0.49236249923706055}
I0428 10:20:05.568618 140228115289856 logging_writer.py:48] [15386] accumulated_eval_time=384.433298, accumulated_logging_time=0.492362, accumulated_submission_time=6350.064700, global_step=15386, preemption_count=0, score=6350.064700, test/accuracy=0.319400, test/loss=3.481608, test/num_examples=10000, total_duration=6735.316172, train/accuracy=0.448516, train/loss=2.737737, validation/accuracy=0.411220, validation/loss=2.910827, validation/num_examples=50000
I0428 10:20:11.676565 140228106897152 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5030621290206909, loss=6.194275856018066
I0428 10:20:53.065742 140228115289856 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6508732438087463, loss=4.531126022338867
I0428 10:21:35.281976 140228106897152 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6564246416091919, loss=4.604032516479492
I0428 10:22:16.921708 140228115289856 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6958595514297485, loss=4.596890926361084
I0428 10:22:58.790092 140228106897152 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5414285063743591, loss=4.731337547302246
I0428 10:23:40.659170 140228115289856 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.688657820224762, loss=4.494146823883057
I0428 10:24:22.620643 140228106897152 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6927562355995178, loss=4.569662570953369
I0428 10:25:05.057333 140228115289856 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6819195747375488, loss=4.467034816741943
I0428 10:25:47.131271 140228106897152 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.4839172959327698, loss=6.099409580230713
I0428 10:26:29.571306 140228115289856 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6389093995094299, loss=4.4356369972229
I0428 10:27:05.713917 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:27:19.969146 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:27:29.612031 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:27:31.310096 140468099872576 submission_runner.py:415] Time since start: 7181.08s, 	Step: 16387, 	{'train/accuracy': 0.471503883600235, 'train/loss': 2.5745327472686768, 'validation/accuracy': 0.43303999304771423, 'validation/loss': 2.767664670944214, 'validation/num_examples': 50000, 'test/accuracy': 0.3247000277042389, 'test/loss': 3.367074728012085, 'test/num_examples': 10000, 'score': 6770.164845705032, 'total_duration': 7181.077699184418, 'accumulated_submission_time': 6770.164845705032, 'accumulated_eval_time': 410.0249855518341, 'accumulated_logging_time': 0.5338056087493896}
I0428 10:27:31.327794 140228106897152 logging_writer.py:48] [16387] accumulated_eval_time=410.024986, accumulated_logging_time=0.533806, accumulated_submission_time=6770.164846, global_step=16387, preemption_count=0, score=6770.164846, test/accuracy=0.324700, test/loss=3.367075, test/num_examples=10000, total_duration=7181.077699, train/accuracy=0.471504, train/loss=2.574533, validation/accuracy=0.433040, validation/loss=2.767665, validation/num_examples=50000
I0428 10:27:37.159497 140228115289856 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5989665985107422, loss=4.7380194664001465
I0428 10:28:18.187774 140228106897152 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6782308220863342, loss=4.551816940307617
I0428 10:29:00.304368 140228115289856 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6846292018890381, loss=4.603949069976807
I0428 10:29:41.926675 140228106897152 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6250369548797607, loss=4.484933853149414
I0428 10:30:23.573428 140228115289856 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6641467213630676, loss=4.514952182769775
I0428 10:31:05.167732 140228106897152 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6001530885696411, loss=4.755797863006592
I0428 10:31:46.878936 140228115289856 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6611937880516052, loss=4.47393798828125
I0428 10:32:28.449807 140228106897152 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5208099484443665, loss=5.2057623863220215
I0428 10:33:10.059783 140228115289856 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6052260398864746, loss=4.632546424865723
I0428 10:33:52.245032 140228106897152 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4439007341861725, loss=6.155797004699707
I0428 10:34:31.631965 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:34:45.997939 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:34:55.474699 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:34:57.172388 140468099872576 submission_runner.py:415] Time since start: 7626.94s, 	Step: 17396, 	{'train/accuracy': 0.49335935711860657, 'train/loss': 2.524928569793701, 'validation/accuracy': 0.4441399872303009, 'validation/loss': 2.7468721866607666, 'validation/num_examples': 50000, 'test/accuracy': 0.3472000062465668, 'test/loss': 3.3358805179595947, 'test/num_examples': 10000, 'score': 7190.425662279129, 'total_duration': 7626.9444398880005, 'accumulated_submission_time': 7190.425662279129, 'accumulated_eval_time': 435.56538939476013, 'accumulated_logging_time': 0.5739078521728516}
I0428 10:34:57.188637 140228115289856 logging_writer.py:48] [17396] accumulated_eval_time=435.565389, accumulated_logging_time=0.573908, accumulated_submission_time=7190.425662, global_step=17396, preemption_count=0, score=7190.425662, test/accuracy=0.347200, test/loss=3.335881, test/num_examples=10000, total_duration=7626.944440, train/accuracy=0.493359, train/loss=2.524929, validation/accuracy=0.444140, validation/loss=2.746872, validation/num_examples=50000
I0428 10:34:59.303456 140228106897152 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.4994351267814636, loss=5.70619010925293
I0428 10:35:40.802808 140228115289856 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.4589688181877136, loss=5.701467514038086
I0428 10:36:23.754101 140228106897152 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.557487428188324, loss=5.294402122497559
I0428 10:37:06.558583 140228115289856 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.4726603031158447, loss=5.800786018371582
I0428 10:37:49.310936 140228106897152 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.40441766381263733, loss=5.802361965179443
I0428 10:38:32.172633 140228115289856 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6277050375938416, loss=4.700486660003662
I0428 10:39:14.958336 140228106897152 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6743478775024414, loss=4.300102710723877
I0428 10:39:57.630936 140228115289856 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6365233659744263, loss=4.447709083557129
I0428 10:40:40.386508 140228106897152 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8389737010002136, loss=4.439537048339844
I0428 10:41:23.445079 140228115289856 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5371577739715576, loss=5.753775119781494
I0428 10:41:57.584092 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:42:12.112243 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:42:21.805215 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:42:23.503211 140468099872576 submission_runner.py:415] Time since start: 8073.28s, 	Step: 18381, 	{'train/accuracy': 0.49318358302116394, 'train/loss': 2.522934913635254, 'validation/accuracy': 0.4543199837207794, 'validation/loss': 2.7055320739746094, 'validation/num_examples': 50000, 'test/accuracy': 0.3521000146865845, 'test/loss': 3.2867801189422607, 'test/num_examples': 10000, 'score': 7610.788855552673, 'total_duration': 8073.275284290314, 'accumulated_submission_time': 7610.788855552673, 'accumulated_eval_time': 461.4844949245453, 'accumulated_logging_time': 0.6029655933380127}
I0428 10:42:23.520305 140228106897152 logging_writer.py:48] [18381] accumulated_eval_time=461.484495, accumulated_logging_time=0.602966, accumulated_submission_time=7610.788856, global_step=18381, preemption_count=0, score=7610.788856, test/accuracy=0.352100, test/loss=3.286780, test/num_examples=10000, total_duration=8073.275284, train/accuracy=0.493184, train/loss=2.522935, validation/accuracy=0.454320, validation/loss=2.705532, validation/num_examples=50000
I0428 10:42:31.697855 140228115289856 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6299557089805603, loss=4.506444454193115
I0428 10:43:12.794286 140228106897152 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6113572120666504, loss=4.630958557128906
I0428 10:43:53.804056 140228115289856 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.63129723072052, loss=4.3480730056762695
I0428 10:44:35.115266 140228106897152 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.5715539455413818, loss=4.559642791748047
I0428 10:45:17.085198 140228115289856 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6691504716873169, loss=4.422032356262207
I0428 10:45:59.281787 140228106897152 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6611199975013733, loss=4.401208877563477
I0428 10:46:41.232117 140228115289856 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6258891224861145, loss=4.395545482635498
I0428 10:47:23.136636 140228106897152 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6062754988670349, loss=4.781588554382324
I0428 10:48:04.989316 140228115289856 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6824166774749756, loss=4.548670291900635
I0428 10:48:46.839289 140228106897152 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6228619813919067, loss=4.533417701721191
I0428 10:49:23.850817 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:49:38.064317 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:49:48.522950 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:49:50.218774 140468099872576 submission_runner.py:415] Time since start: 8519.99s, 	Step: 19390, 	{'train/accuracy': 0.5116601586341858, 'train/loss': 2.426043748855591, 'validation/accuracy': 0.4673199951648712, 'validation/loss': 2.624119997024536, 'validation/num_examples': 50000, 'test/accuracy': 0.36490002274513245, 'test/loss': 3.211790084838867, 'test/num_examples': 10000, 'score': 8031.061980247498, 'total_duration': 8519.990839242935, 'accumulated_submission_time': 8031.061980247498, 'accumulated_eval_time': 487.85243678092957, 'accumulated_logging_time': 0.6564743518829346}
I0428 10:49:50.232654 140228115289856 logging_writer.py:48] [19390] accumulated_eval_time=487.852437, accumulated_logging_time=0.656474, accumulated_submission_time=8031.061980, global_step=19390, preemption_count=0, score=8031.061980, test/accuracy=0.364900, test/loss=3.211790, test/num_examples=10000, total_duration=8519.990839, train/accuracy=0.511660, train/loss=2.426044, validation/accuracy=0.467320, validation/loss=2.624120, validation/num_examples=50000
I0428 10:49:54.751853 140228106897152 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5006564259529114, loss=5.238984107971191
I0428 10:50:35.816005 140228115289856 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6610183119773865, loss=4.392593860626221
I0428 10:51:17.105742 140228106897152 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5805235505104065, loss=5.135428428649902
I0428 10:51:58.044033 140228115289856 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6128624677658081, loss=4.417479515075684
I0428 10:52:39.007991 140228106897152 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.43334171175956726, loss=6.037762641906738
I0428 10:53:20.244633 140228115289856 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.510200023651123, loss=5.8372321128845215
I0428 10:54:01.173901 140228106897152 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5488782525062561, loss=5.826704025268555
I0428 10:54:43.007115 140228115289856 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5873141884803772, loss=4.868137359619141
I0428 10:55:24.675331 140228106897152 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4844173192977905, loss=5.6797380447387695
I0428 10:56:05.906449 140228115289856 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6520600914955139, loss=4.271737098693848
I0428 10:56:47.698575 140228106897152 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6772221326828003, loss=4.328517436981201
I0428 10:56:50.322512 140468099872576 spec.py:298] Evaluating on the training split.
I0428 10:57:04.137048 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 10:57:15.249977 140468099872576 spec.py:326] Evaluating on the test split.
I0428 10:57:16.947999 140468099872576 submission_runner.py:415] Time since start: 8966.72s, 	Step: 20408, 	{'train/accuracy': 0.5164648294448853, 'train/loss': 2.4265639781951904, 'validation/accuracy': 0.47435998916625977, 'validation/loss': 2.6201071739196777, 'validation/num_examples': 50000, 'test/accuracy': 0.3637000024318695, 'test/loss': 3.2147316932678223, 'test/num_examples': 10000, 'score': 8451.116686344147, 'total_duration': 8966.720069885254, 'accumulated_submission_time': 8451.116686344147, 'accumulated_eval_time': 514.4779086112976, 'accumulated_logging_time': 0.6836731433868408}
I0428 10:57:16.961023 140228115289856 logging_writer.py:48] [20408] accumulated_eval_time=514.477909, accumulated_logging_time=0.683673, accumulated_submission_time=8451.116686, global_step=20408, preemption_count=0, score=8451.116686, test/accuracy=0.363700, test/loss=3.214732, test/num_examples=10000, total_duration=8966.720070, train/accuracy=0.516465, train/loss=2.426564, validation/accuracy=0.474360, validation/loss=2.620107, validation/num_examples=50000
I0428 10:57:54.636111 140228106897152 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5022947192192078, loss=5.901695728302002
I0428 10:58:36.076280 140228115289856 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.660702109336853, loss=4.3365912437438965
I0428 10:59:17.292957 140228106897152 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.65461665391922, loss=4.228015422821045
I0428 10:59:58.607036 140228115289856 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6488353610038757, loss=4.232273578643799
I0428 11:00:40.124042 140228106897152 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.5067459344863892, loss=5.8333330154418945
I0428 11:01:21.394672 140228115289856 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6373874545097351, loss=4.494572639465332
I0428 11:02:02.651558 140228106897152 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6195846199989319, loss=4.4507317543029785
I0428 11:02:44.212849 140228115289856 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5587846636772156, loss=4.806618690490723
I0428 11:03:25.963222 140228106897152 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7132910490036011, loss=4.283702373504639
I0428 11:04:07.299033 140228115289856 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.5151193141937256, loss=5.400467395782471
I0428 11:04:17.264495 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:04:31.190189 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:04:42.404646 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:04:44.095736 140468099872576 submission_runner.py:415] Time since start: 9413.87s, 	Step: 21426, 	{'train/accuracy': 0.5470117330551147, 'train/loss': 2.193539619445801, 'validation/accuracy': 0.49609997868537903, 'validation/loss': 2.4281210899353027, 'validation/num_examples': 50000, 'test/accuracy': 0.38450002670288086, 'test/loss': 3.0538721084594727, 'test/num_examples': 10000, 'score': 8871.38717007637, 'total_duration': 9413.867795467377, 'accumulated_submission_time': 8871.38717007637, 'accumulated_eval_time': 541.3091309070587, 'accumulated_logging_time': 0.7087078094482422}
I0428 11:04:44.108937 140228106897152 logging_writer.py:48] [21426] accumulated_eval_time=541.309131, accumulated_logging_time=0.708708, accumulated_submission_time=8871.387170, global_step=21426, preemption_count=0, score=8871.387170, test/accuracy=0.384500, test/loss=3.053872, test/num_examples=10000, total_duration=9413.867795, train/accuracy=0.547012, train/loss=2.193540, validation/accuracy=0.496100, validation/loss=2.428121, validation/num_examples=50000
I0428 11:05:14.499469 140228115289856 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5180677771568298, loss=6.015888690948486
I0428 11:05:55.435811 140228106897152 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6577404141426086, loss=4.497220993041992
I0428 11:06:36.395293 140228115289856 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6290029883384705, loss=4.27153205871582
I0428 11:07:17.486277 140228106897152 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6509720683097839, loss=4.274160385131836
I0428 11:07:58.702795 140228115289856 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.544405996799469, loss=5.058729648590088
I0428 11:08:39.769724 140228106897152 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6909940242767334, loss=4.320456027984619
I0428 11:09:20.808820 140228115289856 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.6490157246589661, loss=4.143092632293701
I0428 11:10:02.057377 140228106897152 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6621296405792236, loss=4.658199787139893
I0428 11:10:43.241516 140228115289856 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6547062993049622, loss=4.194088935852051
I0428 11:11:24.128665 140228106897152 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.44434815645217896, loss=5.880684852600098
I0428 11:11:44.263076 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:11:58.121264 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:12:09.054654 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:12:10.753443 140468099872576 submission_runner.py:415] Time since start: 9860.52s, 	Step: 22451, 	{'train/accuracy': 0.568652331829071, 'train/loss': 2.08847975730896, 'validation/accuracy': 0.5014399886131287, 'validation/loss': 2.4025237560272217, 'validation/num_examples': 50000, 'test/accuracy': 0.38960000872612, 'test/loss': 3.026549816131592, 'test/num_examples': 10000, 'score': 9291.507069587708, 'total_duration': 9860.524193525314, 'accumulated_submission_time': 9291.507069587708, 'accumulated_eval_time': 567.7982034683228, 'accumulated_logging_time': 0.7350947856903076}
I0428 11:12:10.771680 140228115289856 logging_writer.py:48] [22451] accumulated_eval_time=567.798203, accumulated_logging_time=0.735095, accumulated_submission_time=9291.507070, global_step=22451, preemption_count=0, score=9291.507070, test/accuracy=0.389600, test/loss=3.026550, test/num_examples=10000, total_duration=9860.524194, train/accuracy=0.568652, train/loss=2.088480, validation/accuracy=0.501440, validation/loss=2.402524, validation/num_examples=50000
I0428 11:12:31.145542 140228106897152 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5495772361755371, loss=5.674308776855469
I0428 11:13:12.868411 140228115289856 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6128289103507996, loss=4.423490524291992
I0428 11:13:54.947204 140228106897152 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6279404759407043, loss=4.527853488922119
I0428 11:14:37.220492 140228115289856 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5988845825195312, loss=4.571977615356445
I0428 11:15:19.438692 140228106897152 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6943475604057312, loss=4.250473976135254
I0428 11:16:01.586297 140228115289856 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4976862967014313, loss=5.9887847900390625
I0428 11:16:44.171898 140228106897152 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.8351042866706848, loss=4.293484687805176
I0428 11:17:26.345064 140228115289856 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6348493099212646, loss=4.267886161804199
I0428 11:18:08.322000 140228106897152 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6203473210334778, loss=4.323523044586182
I0428 11:18:50.582082 140228115289856 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5116420388221741, loss=6.000594139099121
I0428 11:19:10.934191 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:19:25.314040 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:19:35.476854 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:19:37.205318 140468099872576 submission_runner.py:415] Time since start: 10306.97s, 	Step: 23450, 	{'train/accuracy': 0.5594921708106995, 'train/loss': 2.1395883560180664, 'validation/accuracy': 0.513260006904602, 'validation/loss': 2.3608782291412354, 'validation/num_examples': 50000, 'test/accuracy': 0.3986000120639801, 'test/loss': 2.9791321754455566, 'test/num_examples': 10000, 'score': 9711.63210606575, 'total_duration': 10306.973574876785, 'accumulated_submission_time': 9711.63210606575, 'accumulated_eval_time': 594.0654978752136, 'accumulated_logging_time': 0.7708489894866943}
I0428 11:19:37.221087 140228106897152 logging_writer.py:48] [23450] accumulated_eval_time=594.065498, accumulated_logging_time=0.770849, accumulated_submission_time=9711.632106, global_step=23450, preemption_count=0, score=9711.632106, test/accuracy=0.398600, test/loss=2.979132, test/num_examples=10000, total_duration=10306.973575, train/accuracy=0.559492, train/loss=2.139588, validation/accuracy=0.513260, validation/loss=2.360878, validation/num_examples=50000
I0428 11:19:58.260516 140228115289856 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5337069034576416, loss=5.083562850952148
I0428 11:20:43.199440 140228106897152 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6202502846717834, loss=4.383490562438965
I0428 11:21:28.766312 140228115289856 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5912970900535583, loss=4.564203262329102
I0428 11:22:14.401462 140228106897152 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6589173674583435, loss=4.23223352432251
I0428 11:22:59.829035 140228115289856 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6704795956611633, loss=4.197664260864258
I0428 11:23:45.043021 140228106897152 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6261036992073059, loss=4.134590148925781
I0428 11:24:30.546046 140228115289856 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6430766582489014, loss=4.223134517669678
I0428 11:25:16.102410 140228106897152 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.47869813442230225, loss=5.713057518005371
I0428 11:26:01.475013 140228115289856 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6588597893714905, loss=4.211339473724365
I0428 11:26:37.356585 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:26:48.989287 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:27:00.518800 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:27:02.219131 140468099872576 submission_runner.py:415] Time since start: 10751.99s, 	Step: 24380, 	{'train/accuracy': 0.5724023580551147, 'train/loss': 2.0591213703155518, 'validation/accuracy': 0.5222799777984619, 'validation/loss': 2.292414903640747, 'validation/num_examples': 50000, 'test/accuracy': 0.4044000208377838, 'test/loss': 2.930675983428955, 'test/num_examples': 10000, 'score': 10131.71865105629, 'total_duration': 10751.990111112595, 'accumulated_submission_time': 10131.71865105629, 'accumulated_eval_time': 618.9269247055054, 'accumulated_logging_time': 0.8177704811096191}
I0428 11:27:02.238381 140228106897152 logging_writer.py:48] [24380] accumulated_eval_time=618.926925, accumulated_logging_time=0.817770, accumulated_submission_time=10131.718651, global_step=24380, preemption_count=0, score=10131.718651, test/accuracy=0.404400, test/loss=2.930676, test/num_examples=10000, total_duration=10751.990111, train/accuracy=0.572402, train/loss=2.059121, validation/accuracy=0.522280, validation/loss=2.292415, validation/num_examples=50000
I0428 11:27:10.765470 140228115289856 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6608271598815918, loss=4.545074462890625
I0428 11:27:51.541583 140228106897152 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6703383326530457, loss=4.156278610229492
I0428 11:28:32.704058 140228115289856 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5338676571846008, loss=4.972981929779053
I0428 11:29:13.641277 140228106897152 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6612626314163208, loss=4.152409553527832
I0428 11:29:54.839524 140228115289856 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6716905832290649, loss=4.106480121612549
I0428 11:30:35.850477 140228106897152 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6650745272636414, loss=4.238682746887207
I0428 11:31:16.739247 140228115289856 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5432022213935852, loss=5.919161796569824
I0428 11:31:58.068476 140228106897152 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5437068939208984, loss=5.951907634735107
I0428 11:32:39.302481 140228115289856 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6309004426002502, loss=4.298243522644043
I0428 11:33:20.871010 140228106897152 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5298306345939636, loss=5.252917766571045
I0428 11:34:02.305830 140228115289856 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6296166777610779, loss=4.290500164031982
I0428 11:34:02.319640 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:34:13.387073 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:34:25.272182 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:34:26.977427 140468099872576 submission_runner.py:415] Time since start: 11196.75s, 	Step: 25401, 	{'train/accuracy': 0.5847070217132568, 'train/loss': 1.9949675798416138, 'validation/accuracy': 0.5338799953460693, 'validation/loss': 2.2539947032928467, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.8834362030029297, 'test/num_examples': 10000, 'score': 10551.764539003372, 'total_duration': 11196.74793767929, 'accumulated_submission_time': 10551.764539003372, 'accumulated_eval_time': 643.5831274986267, 'accumulated_logging_time': 0.8513400554656982}
I0428 11:34:26.997214 140228106897152 logging_writer.py:48] [25401] accumulated_eval_time=643.583127, accumulated_logging_time=0.851340, accumulated_submission_time=10551.764539, global_step=25401, preemption_count=0, score=10551.764539, test/accuracy=0.416700, test/loss=2.883436, test/num_examples=10000, total_duration=11196.747938, train/accuracy=0.584707, train/loss=1.994968, validation/accuracy=0.533880, validation/loss=2.253995, validation/num_examples=50000
I0428 11:35:07.594118 140228115289856 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6319258809089661, loss=4.206779956817627
I0428 11:35:48.836190 140228106897152 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6147911548614502, loss=4.333883285522461
I0428 11:36:29.841034 140228115289856 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6245542764663696, loss=4.153494358062744
I0428 11:37:10.932169 140228106897152 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6181511878967285, loss=4.232428550720215
I0428 11:37:52.144384 140228115289856 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6359102725982666, loss=4.1689677238464355
I0428 11:38:33.511148 140228106897152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6347569823265076, loss=4.059744358062744
I0428 11:39:14.770200 140228115289856 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.742006242275238, loss=4.223517894744873
I0428 11:39:55.892318 140228106897152 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5687540173530579, loss=5.186503887176514
I0428 11:40:37.502329 140228115289856 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6454398036003113, loss=4.071791172027588
I0428 11:41:18.505467 140228106897152 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5182240009307861, loss=5.4112348556518555
I0428 11:41:27.369299 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:41:38.566421 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:41:50.830004 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:41:52.539860 140468099872576 submission_runner.py:415] Time since start: 11642.31s, 	Step: 26423, 	{'train/accuracy': 0.6031640768051147, 'train/loss': 1.9499934911727905, 'validation/accuracy': 0.5344600081443787, 'validation/loss': 2.268254041671753, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.917604923248291, 'test/num_examples': 10000, 'score': 10972.10148191452, 'total_duration': 11642.310726642609, 'accumulated_submission_time': 10972.10148191452, 'accumulated_eval_time': 668.7524638175964, 'accumulated_logging_time': 0.88498854637146}
I0428 11:41:52.554935 140228115289856 logging_writer.py:48] [26423] accumulated_eval_time=668.752464, accumulated_logging_time=0.884989, accumulated_submission_time=10972.101482, global_step=26423, preemption_count=0, score=10972.101482, test/accuracy=0.416600, test/loss=2.917605, test/num_examples=10000, total_duration=11642.310727, train/accuracy=0.603164, train/loss=1.949993, validation/accuracy=0.534460, validation/loss=2.268254, validation/num_examples=50000
I0428 11:42:24.247304 140228106897152 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.638290524482727, loss=4.0312042236328125
I0428 11:43:05.371293 140228115289856 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7316939830780029, loss=3.9848718643188477
I0428 11:43:46.592681 140228106897152 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5237296223640442, loss=5.76661491394043
I0428 11:44:28.114175 140228115289856 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6798567771911621, loss=3.9951343536376953
I0428 11:45:09.229739 140228106897152 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5481013059616089, loss=5.480855464935303
I0428 11:45:50.413216 140228115289856 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6699532270431519, loss=4.150223731994629
I0428 11:46:31.772642 140228106897152 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6612006425857544, loss=4.154680252075195
I0428 11:47:13.142314 140228115289856 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6276342272758484, loss=4.059000492095947
I0428 11:47:54.417501 140228106897152 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6340026259422302, loss=4.375605583190918
I0428 11:48:35.547979 140228115289856 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.732923686504364, loss=4.084197044372559
I0428 11:48:52.965857 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:49:04.140223 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:49:16.379966 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:49:18.069281 140468099872576 submission_runner.py:415] Time since start: 12087.84s, 	Step: 27443, 	{'train/accuracy': 0.5885546803474426, 'train/loss': 1.9859577417373657, 'validation/accuracy': 0.5408599972724915, 'validation/loss': 2.2073309421539307, 'validation/num_examples': 50000, 'test/accuracy': 0.42180001735687256, 'test/loss': 2.846402645111084, 'test/num_examples': 10000, 'score': 11392.478515386581, 'total_duration': 12087.840026378632, 'accumulated_submission_time': 11392.478515386581, 'accumulated_eval_time': 693.8545475006104, 'accumulated_logging_time': 0.9130558967590332}
I0428 11:49:18.082248 140228106897152 logging_writer.py:48] [27443] accumulated_eval_time=693.854548, accumulated_logging_time=0.913056, accumulated_submission_time=11392.478515, global_step=27443, preemption_count=0, score=11392.478515, test/accuracy=0.421800, test/loss=2.846403, test/num_examples=10000, total_duration=12087.840026, train/accuracy=0.588555, train/loss=1.985958, validation/accuracy=0.540860, validation/loss=2.207331, validation/num_examples=50000
I0428 11:49:41.812911 140228115289856 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6396340727806091, loss=4.0610809326171875
I0428 11:50:22.592624 140228106897152 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5709989666938782, loss=5.002917766571045
I0428 11:51:04.339700 140228115289856 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6636266708374023, loss=4.136009693145752
I0428 11:51:45.439181 140228106897152 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5911970138549805, loss=4.664544582366943
I0428 11:52:26.556549 140228115289856 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6510708928108215, loss=4.012994766235352
I0428 11:53:07.168015 140468099872576 spec.py:298] Evaluating on the training split.
I0428 11:53:18.515851 140468099872576 spec.py:310] Evaluating on the validation split.
I0428 11:53:30.583188 140468099872576 spec.py:326] Evaluating on the test split.
I0428 11:53:32.279311 140468099872576 submission_runner.py:415] Time since start: 12342.05s, 	Step: 28000, 	{'train/accuracy': 0.6021679639816284, 'train/loss': 1.96430504322052, 'validation/accuracy': 0.5461999773979187, 'validation/loss': 2.2281136512756348, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.852972984313965, 'test/num_examples': 10000, 'score': 11621.538528680801, 'total_duration': 12342.050467252731, 'accumulated_submission_time': 11621.538528680801, 'accumulated_eval_time': 718.9649133682251, 'accumulated_logging_time': 0.940218448638916}
I0428 11:53:32.299320 140228106897152 logging_writer.py:48] [28000] accumulated_eval_time=718.964913, accumulated_logging_time=0.940218, accumulated_submission_time=11621.538529, global_step=28000, preemption_count=0, score=11621.538529, test/accuracy=0.430000, test/loss=2.852973, test/num_examples=10000, total_duration=12342.050467, train/accuracy=0.602168, train/loss=1.964305, validation/accuracy=0.546200, validation/loss=2.228114, validation/num_examples=50000
I0428 11:53:32.328212 140228115289856 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11621.538529
I0428 11:53:32.423829 140468099872576 checkpoints.py:356] Saving checkpoint at step: 28000
I0428 11:53:33.355733 140468099872576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_28000
I0428 11:53:33.374224 140468099872576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_28000.
I0428 11:53:34.546692 140468099872576 submission_runner.py:578] Tuning trial 1/1
I0428 11:53:34.547846 140468099872576 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0428 11:53:34.561084 140468099872576 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008203124743886292, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 47.87640738487244, 'total_duration': 98.47901773452759, 'accumulated_submission_time': 47.87640738487244, 'accumulated_eval_time': 50.602452516555786, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1005, {'train/accuracy': 0.0373242162168026, 'train/loss': 6.043600559234619, 'validation/accuracy': 0.036240000277757645, 'validation/loss': 6.072470664978027, 'validation/num_examples': 50000, 'test/accuracy': 0.026500001549720764, 'test/loss': 6.168850421905518, 'test/num_examples': 10000, 'score': 467.94335865974426, 'total_duration': 538.2398612499237, 'accumulated_submission_time': 467.94335865974426, 'accumulated_eval_time': 70.25005269050598, 'accumulated_logging_time': 0.024674415588378906, 'global_step': 1005, 'preemption_count': 0}), (2042, {'train/accuracy': 0.07175780832767487, 'train/loss': 5.516951084136963, 'validation/accuracy': 0.0663599967956543, 'validation/loss': 5.558749675750732, 'validation/num_examples': 50000, 'test/accuracy': 0.04580000042915344, 'test/loss': 5.734161853790283, 'test/num_examples': 10000, 'score': 888.2692646980286, 'total_duration': 978.2135407924652, 'accumulated_submission_time': 888.2692646980286, 'accumulated_eval_time': 89.81798124313354, 'accumulated_logging_time': 0.0823972225189209, 'global_step': 2042, 'preemption_count': 0}), (3078, {'train/accuracy': 0.09431640058755875, 'train/loss': 5.234086036682129, 'validation/accuracy': 0.08912000060081482, 'validation/loss': 5.284057140350342, 'validation/num_examples': 50000, 'test/accuracy': 0.06710000336170197, 'test/loss': 5.516386985778809, 'test/num_examples': 10000, 'score': 1308.5137221813202, 'total_duration': 1418.0100915431976, 'accumulated_submission_time': 1308.5137221813202, 'accumulated_eval_time': 109.3197009563446, 'accumulated_logging_time': 0.11086511611938477, 'global_step': 3078, 'preemption_count': 0}), (4111, {'train/accuracy': 0.1280859410762787, 'train/loss': 4.956143379211426, 'validation/accuracy': 0.11177999526262283, 'validation/loss': 5.053415775299072, 'validation/num_examples': 50000, 'test/accuracy': 0.08640000224113464, 'test/loss': 5.311581611633301, 'test/num_examples': 10000, 'score': 1728.5482530593872, 'total_duration': 1858.0089581012726, 'accumulated_submission_time': 1728.5482530593872, 'accumulated_eval_time': 129.22598838806152, 'accumulated_logging_time': 0.14727282524108887, 'global_step': 4111, 'preemption_count': 0}), (5145, {'train/accuracy': 0.15761718153953552, 'train/loss': 4.644299030303955, 'validation/accuracy': 0.14627999067306519, 'validation/loss': 4.7104902267456055, 'validation/num_examples': 50000, 'test/accuracy': 0.11050000786781311, 'test/loss': 5.040230751037598, 'test/num_examples': 10000, 'score': 2148.808512687683, 'total_duration': 2298.0871205329895, 'accumulated_submission_time': 2148.808512687683, 'accumulated_eval_time': 148.97597360610962, 'accumulated_logging_time': 0.19334650039672852, 'global_step': 5145, 'preemption_count': 0}), (6177, {'train/accuracy': 0.19738280773162842, 'train/loss': 4.310327053070068, 'validation/accuracy': 0.18094000220298767, 'validation/loss': 4.416280746459961, 'validation/num_examples': 50000, 'test/accuracy': 0.13930000364780426, 'test/loss': 4.79020357131958, 'test/num_examples': 10000, 'score': 2568.8283026218414, 'total_duration': 2738.1823987960815, 'accumulated_submission_time': 2568.8283026218414, 'accumulated_eval_time': 168.9973759651184, 'accumulated_logging_time': 0.22543811798095703, 'global_step': 6177, 'preemption_count': 0}), (7208, {'train/accuracy': 0.23017577826976776, 'train/loss': 4.0920844078063965, 'validation/accuracy': 0.2101600021123886, 'validation/loss': 4.196775436401367, 'validation/num_examples': 50000, 'test/accuracy': 0.164000004529953, 'test/loss': 4.5667500495910645, 'test/num_examples': 10000, 'score': 2989.196631193161, 'total_duration': 3179.2655251026154, 'accumulated_submission_time': 2989.196631193161, 'accumulated_eval_time': 189.6601767539978, 'accumulated_logging_time': 0.2559056282043457, 'global_step': 7208, 'preemption_count': 0}), (8234, {'train/accuracy': 0.27037107944488525, 'train/loss': 3.848663806915283, 'validation/accuracy': 0.24577999114990234, 'validation/loss': 3.971712350845337, 'validation/num_examples': 50000, 'test/accuracy': 0.18860000371932983, 'test/loss': 4.398263454437256, 'test/num_examples': 10000, 'score': 3409.3865525722504, 'total_duration': 3621.3740136623383, 'accumulated_submission_time': 3409.3865525722504, 'accumulated_eval_time': 211.52949571609497, 'accumulated_logging_time': 0.283541202545166, 'global_step': 8234, 'preemption_count': 0}), (9258, {'train/accuracy': 0.3178125023841858, 'train/loss': 3.564225435256958, 'validation/accuracy': 0.27493998408317566, 'validation/loss': 3.775895595550537, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.234241008758545, 'test/num_examples': 10000, 'score': 3829.5026593208313, 'total_duration': 4066.500720024109, 'accumulated_submission_time': 3829.5026593208313, 'accumulated_eval_time': 236.4925787448883, 'accumulated_logging_time': 0.3088507652282715, 'global_step': 9258, 'preemption_count': 0}), (10283, {'train/accuracy': 0.3371874988079071, 'train/loss': 3.3881728649139404, 'validation/accuracy': 0.31092000007629395, 'validation/loss': 3.5186643600463867, 'validation/num_examples': 50000, 'test/accuracy': 0.23910000920295715, 'test/loss': 4.009396553039551, 'test/num_examples': 10000, 'score': 4249.602550029755, 'total_duration': 4510.701415300369, 'accumulated_submission_time': 4249.602550029755, 'accumulated_eval_time': 260.54908990859985, 'accumulated_logging_time': 0.3316512107849121, 'global_step': 10283, 'preemption_count': 0}), (11311, {'train/accuracy': 0.3551953136920929, 'train/loss': 3.2805843353271484, 'validation/accuracy': 0.3273399770259857, 'validation/loss': 3.422231435775757, 'validation/num_examples': 50000, 'test/accuracy': 0.24910001456737518, 'test/loss': 3.9348411560058594, 'test/num_examples': 10000, 'score': 4669.61865401268, 'total_duration': 4955.220681428909, 'accumulated_submission_time': 4669.61865401268, 'accumulated_eval_time': 285.0004780292511, 'accumulated_logging_time': 0.36130452156066895, 'global_step': 11311, 'preemption_count': 0}), (12338, {'train/accuracy': 0.39113280177116394, 'train/loss': 2.9794740676879883, 'validation/accuracy': 0.3610999882221222, 'validation/loss': 3.1409809589385986, 'validation/num_examples': 50000, 'test/accuracy': 0.27890002727508545, 'test/loss': 3.6962387561798096, 'test/num_examples': 10000, 'score': 5089.6630136966705, 'total_duration': 5399.877131700516, 'accumulated_submission_time': 5089.6630136966705, 'accumulated_eval_time': 309.56455731391907, 'accumulated_logging_time': 0.3876206874847412, 'global_step': 12338, 'preemption_count': 0}), (13359, {'train/accuracy': 0.41843748092651367, 'train/loss': 2.9201531410217285, 'validation/accuracy': 0.371319979429245, 'validation/loss': 3.1370153427124023, 'validation/num_examples': 50000, 'test/accuracy': 0.28440001606941223, 'test/loss': 3.6904637813568115, 'test/num_examples': 10000, 'score': 5509.6521809101105, 'total_duration': 5844.681668281555, 'accumulated_submission_time': 5509.6521809101105, 'accumulated_eval_time': 334.3312931060791, 'accumulated_logging_time': 0.41480255126953125, 'global_step': 13359, 'preemption_count': 0}), (14375, {'train/accuracy': 0.4328710734844208, 'train/loss': 2.8061885833740234, 'validation/accuracy': 0.3980199992656708, 'validation/loss': 2.9603617191314697, 'validation/num_examples': 50000, 'test/accuracy': 0.3019000291824341, 'test/loss': 3.5383048057556152, 'test/num_examples': 10000, 'score': 5929.791624307632, 'total_duration': 6289.695822477341, 'accumulated_submission_time': 5929.791624307632, 'accumulated_eval_time': 359.15713381767273, 'accumulated_logging_time': 0.44176411628723145, 'global_step': 14375, 'preemption_count': 0}), (15386, {'train/accuracy': 0.4485156238079071, 'train/loss': 2.7377374172210693, 'validation/accuracy': 0.411219984292984, 'validation/loss': 2.9108266830444336, 'validation/num_examples': 50000, 'test/accuracy': 0.31940001249313354, 'test/loss': 3.4816079139709473, 'test/num_examples': 10000, 'score': 6350.0647003650665, 'total_duration': 6735.316172361374, 'accumulated_submission_time': 6350.0647003650665, 'accumulated_eval_time': 384.43329763412476, 'accumulated_logging_time': 0.49236249923706055, 'global_step': 15386, 'preemption_count': 0}), (16387, {'train/accuracy': 0.471503883600235, 'train/loss': 2.5745327472686768, 'validation/accuracy': 0.43303999304771423, 'validation/loss': 2.767664670944214, 'validation/num_examples': 50000, 'test/accuracy': 0.3247000277042389, 'test/loss': 3.367074728012085, 'test/num_examples': 10000, 'score': 6770.164845705032, 'total_duration': 7181.077699184418, 'accumulated_submission_time': 6770.164845705032, 'accumulated_eval_time': 410.0249855518341, 'accumulated_logging_time': 0.5338056087493896, 'global_step': 16387, 'preemption_count': 0}), (17396, {'train/accuracy': 0.49335935711860657, 'train/loss': 2.524928569793701, 'validation/accuracy': 0.4441399872303009, 'validation/loss': 2.7468721866607666, 'validation/num_examples': 50000, 'test/accuracy': 0.3472000062465668, 'test/loss': 3.3358805179595947, 'test/num_examples': 10000, 'score': 7190.425662279129, 'total_duration': 7626.9444398880005, 'accumulated_submission_time': 7190.425662279129, 'accumulated_eval_time': 435.56538939476013, 'accumulated_logging_time': 0.5739078521728516, 'global_step': 17396, 'preemption_count': 0}), (18381, {'train/accuracy': 0.49318358302116394, 'train/loss': 2.522934913635254, 'validation/accuracy': 0.4543199837207794, 'validation/loss': 2.7055320739746094, 'validation/num_examples': 50000, 'test/accuracy': 0.3521000146865845, 'test/loss': 3.2867801189422607, 'test/num_examples': 10000, 'score': 7610.788855552673, 'total_duration': 8073.275284290314, 'accumulated_submission_time': 7610.788855552673, 'accumulated_eval_time': 461.4844949245453, 'accumulated_logging_time': 0.6029655933380127, 'global_step': 18381, 'preemption_count': 0}), (19390, {'train/accuracy': 0.5116601586341858, 'train/loss': 2.426043748855591, 'validation/accuracy': 0.4673199951648712, 'validation/loss': 2.624119997024536, 'validation/num_examples': 50000, 'test/accuracy': 0.36490002274513245, 'test/loss': 3.211790084838867, 'test/num_examples': 10000, 'score': 8031.061980247498, 'total_duration': 8519.990839242935, 'accumulated_submission_time': 8031.061980247498, 'accumulated_eval_time': 487.85243678092957, 'accumulated_logging_time': 0.6564743518829346, 'global_step': 19390, 'preemption_count': 0}), (20408, {'train/accuracy': 0.5164648294448853, 'train/loss': 2.4265639781951904, 'validation/accuracy': 0.47435998916625977, 'validation/loss': 2.6201071739196777, 'validation/num_examples': 50000, 'test/accuracy': 0.3637000024318695, 'test/loss': 3.2147316932678223, 'test/num_examples': 10000, 'score': 8451.116686344147, 'total_duration': 8966.720069885254, 'accumulated_submission_time': 8451.116686344147, 'accumulated_eval_time': 514.4779086112976, 'accumulated_logging_time': 0.6836731433868408, 'global_step': 20408, 'preemption_count': 0}), (21426, {'train/accuracy': 0.5470117330551147, 'train/loss': 2.193539619445801, 'validation/accuracy': 0.49609997868537903, 'validation/loss': 2.4281210899353027, 'validation/num_examples': 50000, 'test/accuracy': 0.38450002670288086, 'test/loss': 3.0538721084594727, 'test/num_examples': 10000, 'score': 8871.38717007637, 'total_duration': 9413.867795467377, 'accumulated_submission_time': 8871.38717007637, 'accumulated_eval_time': 541.3091309070587, 'accumulated_logging_time': 0.7087078094482422, 'global_step': 21426, 'preemption_count': 0}), (22451, {'train/accuracy': 0.568652331829071, 'train/loss': 2.08847975730896, 'validation/accuracy': 0.5014399886131287, 'validation/loss': 2.4025237560272217, 'validation/num_examples': 50000, 'test/accuracy': 0.38960000872612, 'test/loss': 3.026549816131592, 'test/num_examples': 10000, 'score': 9291.507069587708, 'total_duration': 9860.524193525314, 'accumulated_submission_time': 9291.507069587708, 'accumulated_eval_time': 567.7982034683228, 'accumulated_logging_time': 0.7350947856903076, 'global_step': 22451, 'preemption_count': 0}), (23450, {'train/accuracy': 0.5594921708106995, 'train/loss': 2.1395883560180664, 'validation/accuracy': 0.513260006904602, 'validation/loss': 2.3608782291412354, 'validation/num_examples': 50000, 'test/accuracy': 0.3986000120639801, 'test/loss': 2.9791321754455566, 'test/num_examples': 10000, 'score': 9711.63210606575, 'total_duration': 10306.973574876785, 'accumulated_submission_time': 9711.63210606575, 'accumulated_eval_time': 594.0654978752136, 'accumulated_logging_time': 0.7708489894866943, 'global_step': 23450, 'preemption_count': 0}), (24380, {'train/accuracy': 0.5724023580551147, 'train/loss': 2.0591213703155518, 'validation/accuracy': 0.5222799777984619, 'validation/loss': 2.292414903640747, 'validation/num_examples': 50000, 'test/accuracy': 0.4044000208377838, 'test/loss': 2.930675983428955, 'test/num_examples': 10000, 'score': 10131.71865105629, 'total_duration': 10751.990111112595, 'accumulated_submission_time': 10131.71865105629, 'accumulated_eval_time': 618.9269247055054, 'accumulated_logging_time': 0.8177704811096191, 'global_step': 24380, 'preemption_count': 0}), (25401, {'train/accuracy': 0.5847070217132568, 'train/loss': 1.9949675798416138, 'validation/accuracy': 0.5338799953460693, 'validation/loss': 2.2539947032928467, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.8834362030029297, 'test/num_examples': 10000, 'score': 10551.764539003372, 'total_duration': 11196.74793767929, 'accumulated_submission_time': 10551.764539003372, 'accumulated_eval_time': 643.5831274986267, 'accumulated_logging_time': 0.8513400554656982, 'global_step': 25401, 'preemption_count': 0}), (26423, {'train/accuracy': 0.6031640768051147, 'train/loss': 1.9499934911727905, 'validation/accuracy': 0.5344600081443787, 'validation/loss': 2.268254041671753, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.917604923248291, 'test/num_examples': 10000, 'score': 10972.10148191452, 'total_duration': 11642.310726642609, 'accumulated_submission_time': 10972.10148191452, 'accumulated_eval_time': 668.7524638175964, 'accumulated_logging_time': 0.88498854637146, 'global_step': 26423, 'preemption_count': 0}), (27443, {'train/accuracy': 0.5885546803474426, 'train/loss': 1.9859577417373657, 'validation/accuracy': 0.5408599972724915, 'validation/loss': 2.2073309421539307, 'validation/num_examples': 50000, 'test/accuracy': 0.42180001735687256, 'test/loss': 2.846402645111084, 'test/num_examples': 10000, 'score': 11392.478515386581, 'total_duration': 12087.840026378632, 'accumulated_submission_time': 11392.478515386581, 'accumulated_eval_time': 693.8545475006104, 'accumulated_logging_time': 0.9130558967590332, 'global_step': 27443, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6021679639816284, 'train/loss': 1.96430504322052, 'validation/accuracy': 0.5461999773979187, 'validation/loss': 2.2281136512756348, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.852972984313965, 'test/num_examples': 10000, 'score': 11621.538528680801, 'total_duration': 12342.050467252731, 'accumulated_submission_time': 11621.538528680801, 'accumulated_eval_time': 718.9649133682251, 'accumulated_logging_time': 0.940218448638916, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0428 11:53:34.561252 140468099872576 submission_runner.py:581] Timing: 11621.538528680801
I0428 11:53:34.561306 140468099872576 submission_runner.py:582] ====================
I0428 11:53:34.561452 140468099872576 submission_runner.py:645] Final imagenet_vit score: 11621.538528680801
