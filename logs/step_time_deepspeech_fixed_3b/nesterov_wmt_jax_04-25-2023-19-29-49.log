I0425 19:30:10.579733 139952621680448 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax.
I0425 19:30:10.646605 139952621680448 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0425 19:30:11.472168 139952621680448 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0425 19:30:11.473170 139952621680448 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0425 19:30:11.477251 139952621680448 submission_runner.py:528] Using RNG seed 1106161527
I0425 19:30:14.116808 139952621680448 submission_runner.py:537] --- Tuning run 1/1 ---
I0425 19:30:14.117036 139952621680448 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1.
I0425 19:30:14.117285 139952621680448 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/hparams.json.
I0425 19:30:14.240539 139952621680448 submission_runner.py:232] Initializing dataset.
I0425 19:30:14.249189 139952621680448 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0425 19:30:14.252534 139952621680448 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0425 19:30:14.252645 139952621680448 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0425 19:30:14.375864 139952621680448 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0425 19:30:16.290129 139952621680448 submission_runner.py:239] Initializing model.
I0425 19:30:28.398687 139952621680448 submission_runner.py:249] Initializing optimizer.
I0425 19:30:28.963383 139952621680448 submission_runner.py:256] Initializing metrics bundle.
I0425 19:30:28.963586 139952621680448 submission_runner.py:273] Initializing checkpoint and logger.
I0425 19:30:28.964427 139952621680448 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1 with prefix checkpoint_
I0425 19:30:28.964692 139952621680448 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0425 19:30:28.964758 139952621680448 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0425 19:30:29.820691 139952621680448 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/meta_data_0.json.
I0425 19:30:29.821643 139952621680448 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/flags_0.json.
I0425 19:30:29.825612 139952621680448 submission_runner.py:309] Starting training loop.
I0425 19:31:00.553857 139776522974976 logging_writer.py:48] [0] global_step=0, grad_norm=5.232995510101318, loss=11.119587898254395
I0425 19:31:00.567628 139952621680448 spec.py:298] Evaluating on the training split.
I0425 19:31:00.570455 139952621680448 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0425 19:31:00.572805 139952621680448 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0425 19:31:00.572914 139952621680448 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0425 19:31:00.602816 139952621680448 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0425 19:31:09.023941 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 19:36:13.395416 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 19:36:13.399393 139952621680448 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0425 19:36:13.403215 139952621680448 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0425 19:36:13.403332 139952621680448 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0425 19:36:13.433619 139952621680448 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0425 19:36:21.077423 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 19:41:18.659056 139952621680448 spec.py:326] Evaluating on the test split.
I0425 19:41:18.661479 139952621680448 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0425 19:41:18.664420 139952621680448 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0425 19:41:18.664546 139952621680448 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0425 19:41:18.693247 139952621680448 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0425 19:41:25.566233 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 19:46:17.161304 139952621680448 submission_runner.py:406] Time since start: 947.34s, 	Step: 1, 	{'train/accuracy': 0.0005966860917396843, 'train/loss': 11.112687110900879, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.123942375183105, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.1343355178833, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.741838932037354, 'total_duration': 947.3356053829193, 'accumulated_submission_time': 30.741838932037354, 'accumulated_eval_time': 916.5936207771301, 'accumulated_logging_time': 0}
I0425 19:46:17.178762 139765306894080 logging_writer.py:48] [1] accumulated_eval_time=916.593621, accumulated_logging_time=0, accumulated_submission_time=30.741839, global_step=1, preemption_count=0, score=30.741839, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.134336, test/num_examples=3003, total_duration=947.335605, train/accuracy=0.000597, train/bleu=0.000000, train/loss=11.112687, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.123942, validation/num_examples=3000
I0425 19:46:17.714029 139952621680448 checkpoints.py:356] Saving checkpoint at step: 1
I0425 19:46:20.219560 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_1
I0425 19:46:20.222807 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_1.
I0425 19:46:56.042119 139765315286784 logging_writer.py:48] [100] global_step=100, grad_norm=0.17541980743408203, loss=9.061311721801758
I0425 19:47:31.834182 139765399213824 logging_writer.py:48] [200] global_step=200, grad_norm=0.13878574967384338, loss=8.906240463256836
I0425 19:48:07.622347 139765315286784 logging_writer.py:48] [300] global_step=300, grad_norm=0.1473889797925949, loss=8.804389953613281
I0425 19:48:43.433912 139765399213824 logging_writer.py:48] [400] global_step=400, grad_norm=0.24269293248653412, loss=8.646204948425293
I0425 19:49:19.255914 139765315286784 logging_writer.py:48] [500] global_step=500, grad_norm=0.4055745601654053, loss=8.431258201599121
I0425 19:49:55.059488 139765399213824 logging_writer.py:48] [600] global_step=600, grad_norm=0.4873602092266083, loss=8.274396896362305
I0425 19:50:30.912148 139765315286784 logging_writer.py:48] [700] global_step=700, grad_norm=0.9903725981712341, loss=8.141703605651855
I0425 19:51:06.736376 139765399213824 logging_writer.py:48] [800] global_step=800, grad_norm=0.6284357905387878, loss=8.009057998657227
I0425 19:51:42.593143 139765315286784 logging_writer.py:48] [900] global_step=900, grad_norm=0.6726311445236206, loss=7.899189472198486
I0425 19:52:18.457169 139765399213824 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.629952073097229, loss=7.8232951164245605
I0425 19:52:54.294218 139765315286784 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.852893590927124, loss=7.748082160949707
I0425 19:53:30.151984 139765399213824 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7140837907791138, loss=7.638424396514893
I0425 19:54:06.017846 139765315286784 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6575620174407959, loss=7.606476306915283
I0425 19:54:41.888634 139765399213824 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9046682119369507, loss=7.545046806335449
I0425 19:55:17.740810 139765315286784 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7591351866722107, loss=7.453149795532227
I0425 19:55:53.604091 139765399213824 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6370043158531189, loss=7.3696818351745605
I0425 19:56:29.432229 139765315286784 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7023525238037109, loss=7.305290699005127
I0425 19:57:05.297312 139765399213824 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6463305950164795, loss=7.191869735717773
I0425 19:57:41.133500 139765315286784 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5646637678146362, loss=7.0830159187316895
I0425 19:58:16.995033 139765399213824 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7943143844604492, loss=7.111079216003418
I0425 19:58:52.826855 139765315286784 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8477316498756409, loss=7.000683784484863
I0425 19:59:28.709846 139765399213824 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6064943671226501, loss=6.948866844177246
I0425 20:00:04.581751 139765315286784 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7463412284851074, loss=6.850986957550049
I0425 20:00:20.436925 139952621680448 spec.py:298] Evaluating on the training split.
I0425 20:00:23.437317 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:04:53.216964 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 20:04:55.877930 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:09:02.026079 139952621680448 spec.py:326] Evaluating on the test split.
I0425 20:09:04.740186 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:13:38.500495 139952621680448 submission_runner.py:406] Time since start: 2588.67s, 	Step: 2346, 	{'train/accuracy': 0.3033105134963989, 'train/loss': 5.5579938888549805, 'train/bleu': 6.419694748893433, 'validation/accuracy': 0.27818626165390015, 'validation/loss': 5.8347320556640625, 'validation/bleu': 3.301721334439692, 'validation/num_examples': 3000, 'test/accuracy': 0.2580326497554779, 'test/loss': 6.1311936378479, 'test/bleu': 2.437064798580302, 'test/num_examples': 3003, 'score': 870.9246771335602, 'total_duration': 2588.674791574478, 'accumulated_submission_time': 870.9246771335602, 'accumulated_eval_time': 1714.657145023346, 'accumulated_logging_time': 3.064432382583618}
I0425 20:13:38.508688 139765399213824 logging_writer.py:48] [2346] accumulated_eval_time=1714.657145, accumulated_logging_time=3.064432, accumulated_submission_time=870.924677, global_step=2346, preemption_count=0, score=870.924677, test/accuracy=0.258033, test/bleu=2.437065, test/loss=6.131194, test/num_examples=3003, total_duration=2588.674792, train/accuracy=0.303311, train/bleu=6.419695, train/loss=5.557994, validation/accuracy=0.278186, validation/bleu=3.301721, validation/loss=5.834732, validation/num_examples=3000
I0425 20:13:39.022261 139952621680448 checkpoints.py:356] Saving checkpoint at step: 2346
I0425 20:13:41.512537 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_2346
I0425 20:13:41.515764 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_2346.
I0425 20:14:01.252508 139765315286784 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.669134795665741, loss=6.741507053375244
I0425 20:14:37.102597 139765374035712 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6574155688285828, loss=6.792177677154541
I0425 20:15:12.934443 139765315286784 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.723966658115387, loss=6.658698081970215
I0425 20:15:48.794968 139765374035712 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6654025912284851, loss=6.611104965209961
I0425 20:16:24.654696 139765315286784 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5848479866981506, loss=6.476690769195557
I0425 20:17:00.502698 139765374035712 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7947455644607544, loss=6.51821756362915
I0425 20:17:36.363540 139765315286784 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6004452705383301, loss=6.454154014587402
I0425 20:18:12.233774 139765374035712 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9789735674858093, loss=6.3704352378845215
I0425 20:18:48.080188 139765315286784 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6745110154151917, loss=6.386928081512451
I0425 20:19:23.932168 139765374035712 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.620441198348999, loss=6.2997355461120605
I0425 20:19:59.772890 139765315286784 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8690260648727417, loss=6.232412815093994
I0425 20:20:35.626964 139765374035712 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9692065119743347, loss=6.155355930328369
I0425 20:21:11.470722 139765315286784 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7155779600143433, loss=6.113623142242432
I0425 20:21:47.316202 139765374035712 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6682835817337036, loss=6.033997058868408
I0425 20:22:23.184790 139765315286784 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8413301706314087, loss=6.075295925140381
I0425 20:22:59.045892 139765374035712 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5698646306991577, loss=5.8934831619262695
I0425 20:23:34.891122 139765315286784 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8279353380203247, loss=5.9524827003479
I0425 20:24:10.710805 139765374035712 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8715704083442688, loss=5.824318885803223
I0425 20:24:46.551461 139765315286784 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5802631378173828, loss=5.788014888763428
I0425 20:25:22.411324 139765374035712 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.596432626247406, loss=5.725244045257568
I0425 20:25:58.274551 139765315286784 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6709551811218262, loss=5.662350654602051
I0425 20:26:34.129014 139765374035712 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7189066410064697, loss=5.63965368270874
I0425 20:27:09.997230 139765315286784 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.605709969997406, loss=5.634646892547607
I0425 20:27:41.595881 139952621680448 spec.py:298] Evaluating on the training split.
I0425 20:27:44.581280 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:30:56.034504 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 20:30:58.685871 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:34:05.592485 139952621680448 spec.py:326] Evaluating on the test split.
I0425 20:34:08.301072 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:37:48.955328 139952621680448 submission_runner.py:406] Time since start: 4039.13s, 	Step: 4690, 	{'train/accuracy': 0.4561130404472351, 'train/loss': 3.8999946117401123, 'train/bleu': 16.875021082047898, 'validation/accuracy': 0.4479423761367798, 'validation/loss': 3.96743106842041, 'validation/bleu': 12.506404695446117, 'validation/num_examples': 3000, 'test/accuracy': 0.4349311590194702, 'test/loss': 4.14227294921875, 'test/bleu': 10.81789456256121, 'test/num_examples': 3003, 'score': 1710.9734861850739, 'total_duration': 4039.129624605179, 'accumulated_submission_time': 1710.9734861850739, 'accumulated_eval_time': 2322.0165536403656, 'accumulated_logging_time': 6.082303762435913}
I0425 20:37:48.963701 139765374035712 logging_writer.py:48] [4690] accumulated_eval_time=2322.016554, accumulated_logging_time=6.082304, accumulated_submission_time=1710.973486, global_step=4690, preemption_count=0, score=1710.973486, test/accuracy=0.434931, test/bleu=10.817895, test/loss=4.142273, test/num_examples=3003, total_duration=4039.129625, train/accuracy=0.456113, train/bleu=16.875021, train/loss=3.899995, validation/accuracy=0.447942, validation/bleu=12.506405, validation/loss=3.967431, validation/num_examples=3000
I0425 20:37:49.470217 139952621680448 checkpoints.py:356] Saving checkpoint at step: 4690
I0425 20:37:51.978180 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_4690
I0425 20:37:51.981400 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_4690.
I0425 20:37:55.938436 139765315286784 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5282775163650513, loss=5.5245866775512695
I0425 20:38:31.783739 139765365643008 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.4889393448829651, loss=5.493053436279297
I0425 20:39:07.617200 139765315286784 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5611246824264526, loss=5.499726295471191
I0425 20:39:43.450574 139765365643008 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5217470526695251, loss=5.374709606170654
I0425 20:40:19.273154 139765315286784 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5052652955055237, loss=5.36170768737793
I0425 20:40:55.149420 139765365643008 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5258300304412842, loss=5.322601795196533
I0425 20:41:31.010949 139765315286784 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6106690168380737, loss=5.344864368438721
I0425 20:42:06.841397 139765365643008 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5144370198249817, loss=5.345584392547607
I0425 20:42:42.683535 139765315286784 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5873553156852722, loss=5.361457824707031
I0425 20:43:18.543759 139765365643008 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5460478663444519, loss=5.376707553863525
I0425 20:43:54.393957 139765315286784 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4531584680080414, loss=5.273938179016113
I0425 20:44:30.231462 139765365643008 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5055315494537354, loss=5.283976078033447
I0425 20:45:06.067305 139765315286784 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5106257200241089, loss=5.288434028625488
I0425 20:45:41.883682 139765365643008 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4873283803462982, loss=5.22518253326416
I0425 20:46:17.721402 139765315286784 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5136087536811829, loss=5.163610458374023
I0425 20:46:53.552200 139765365643008 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.49236440658569336, loss=5.255710124969482
I0425 20:47:29.391346 139765315286784 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4755268692970276, loss=5.145367622375488
I0425 20:48:05.217920 139765365643008 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4831017553806305, loss=5.078423023223877
I0425 20:48:41.036218 139765315286784 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5640645623207092, loss=5.068565845489502
I0425 20:49:16.901911 139765365643008 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4740096628665924, loss=5.025538444519043
I0425 20:49:52.739500 139765315286784 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4350624084472656, loss=5.039215564727783
I0425 20:50:28.573644 139765365643008 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.43820086121559143, loss=5.048276901245117
I0425 20:51:04.437350 139765315286784 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.468869686126709, loss=5.044816493988037
I0425 20:51:40.306143 139765365643008 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4482637345790863, loss=5.015141487121582
I0425 20:51:52.205917 139952621680448 spec.py:298] Evaluating on the training split.
I0425 20:51:55.203343 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:54:53.940478 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 20:54:56.600897 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 20:57:31.068483 139952621680448 spec.py:326] Evaluating on the test split.
I0425 20:57:33.788547 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:00:00.553190 139952621680448 submission_runner.py:406] Time since start: 5370.73s, 	Step: 7035, 	{'train/accuracy': 0.5387536287307739, 'train/loss': 3.1194348335266113, 'train/bleu': 24.37418896466692, 'validation/accuracy': 0.53580242395401, 'validation/loss': 3.121774673461914, 'validation/bleu': 19.750282301725107, 'validation/num_examples': 3000, 'test/accuracy': 0.5339260101318359, 'test/loss': 3.178410053253174, 'test/bleu': 18.099241096610537, 'test/num_examples': 3003, 'score': 2551.167027950287, 'total_duration': 5370.72748541832, 'accumulated_submission_time': 2551.167027950287, 'accumulated_eval_time': 2810.3637821674347, 'accumulated_logging_time': 9.11095142364502}
I0425 21:00:00.561640 139765315286784 logging_writer.py:48] [7035] accumulated_eval_time=2810.363782, accumulated_logging_time=9.110951, accumulated_submission_time=2551.167028, global_step=7035, preemption_count=0, score=2551.167028, test/accuracy=0.533926, test/bleu=18.099241, test/loss=3.178410, test/num_examples=3003, total_duration=5370.727485, train/accuracy=0.538754, train/bleu=24.374189, train/loss=3.119435, validation/accuracy=0.535802, validation/bleu=19.750282, validation/loss=3.121775, validation/num_examples=3000
I0425 21:00:01.084928 139952621680448 checkpoints.py:356] Saving checkpoint at step: 7035
I0425 21:00:03.603568 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_7035
I0425 21:00:03.606831 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_7035.
I0425 21:00:27.252754 139765365643008 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.40723976492881775, loss=4.993576526641846
I0425 21:01:03.118056 139765357250304 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3996015787124634, loss=4.975756645202637
I0425 21:01:38.948748 139765365643008 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.45556288957595825, loss=4.943857192993164
I0425 21:02:14.791572 139765357250304 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4385644495487213, loss=4.969664573669434
I0425 21:02:50.605211 139765365643008 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.40443554520606995, loss=4.9105939865112305
I0425 21:03:26.444016 139765357250304 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.46045050024986267, loss=4.996777057647705
I0425 21:04:02.264846 139765365643008 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4155980348587036, loss=4.9126811027526855
I0425 21:04:38.100804 139765357250304 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4239521622657776, loss=4.831057071685791
I0425 21:05:13.929707 139765365643008 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.437093585729599, loss=4.963946342468262
I0425 21:05:49.748685 139765357250304 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.387436181306839, loss=4.980956077575684
I0425 21:06:25.565574 139765365643008 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3848515450954437, loss=4.906052589416504
I0425 21:07:01.412935 139765357250304 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.414530873298645, loss=4.866653919219971
I0425 21:07:37.255151 139765365643008 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4027036726474762, loss=4.876532077789307
I0425 21:08:13.052323 139765357250304 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4005511701107025, loss=4.867228984832764
I0425 21:08:48.893083 139765365643008 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.37578290700912476, loss=4.914530277252197
I0425 21:09:24.735095 139765357250304 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.39561110734939575, loss=4.764541149139404
I0425 21:10:00.549067 139765365643008 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3775025010108948, loss=4.727861404418945
I0425 21:10:36.364940 139765357250304 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3941507041454315, loss=4.833106994628906
I0425 21:11:12.201517 139765365643008 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3885315954685211, loss=4.850469589233398
I0425 21:11:48.029499 139765357250304 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3819994330406189, loss=4.720897197723389
I0425 21:12:23.845976 139765365643008 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.38102987408638, loss=4.760120391845703
I0425 21:12:59.644174 139765357250304 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3880724310874939, loss=4.843123912811279
I0425 21:13:35.469224 139765365643008 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.37992727756500244, loss=4.733893871307373
I0425 21:14:03.844137 139952621680448 spec.py:298] Evaluating on the training split.
I0425 21:14:06.836890 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:16:45.964059 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 21:16:48.621927 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:19:17.805289 139952621680448 spec.py:326] Evaluating on the test split.
I0425 21:19:20.514100 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:21:46.136160 139952621680448 submission_runner.py:406] Time since start: 6676.31s, 	Step: 9381, 	{'train/accuracy': 0.5672525763511658, 'train/loss': 2.8628101348876953, 'train/bleu': 26.27841804197872, 'validation/accuracy': 0.5698875188827515, 'validation/loss': 2.783269166946411, 'validation/bleu': 22.141380887962228, 'validation/num_examples': 3000, 'test/accuracy': 0.5700540542602539, 'test/loss': 2.810173273086548, 'test/bleu': 20.791546837028186, 'test/num_examples': 3003, 'score': 3391.3743348121643, 'total_duration': 6676.310456752777, 'accumulated_submission_time': 3391.3743348121643, 'accumulated_eval_time': 3272.6557533740997, 'accumulated_logging_time': 12.16715693473816}
I0425 21:21:46.144750 139765357250304 logging_writer.py:48] [9381] accumulated_eval_time=3272.655753, accumulated_logging_time=12.167157, accumulated_submission_time=3391.374335, global_step=9381, preemption_count=0, score=3391.374335, test/accuracy=0.570054, test/bleu=20.791547, test/loss=2.810173, test/num_examples=3003, total_duration=6676.310457, train/accuracy=0.567253, train/bleu=26.278418, train/loss=2.862810, validation/accuracy=0.569888, validation/bleu=22.141381, validation/loss=2.783269, validation/num_examples=3000
I0425 21:21:46.654840 139952621680448 checkpoints.py:356] Saving checkpoint at step: 9381
I0425 21:21:49.196149 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_9381
I0425 21:21:49.199278 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_9381.
I0425 21:21:56.375344 139765365643008 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.37097224593162537, loss=4.813328742980957
I0425 21:22:32.199739 139765340464896 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.37424638867378235, loss=4.668009281158447
I0425 21:23:08.019277 139765365643008 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.42070773243904114, loss=4.726318359375
I0425 21:23:43.832763 139765340464896 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.366645872592926, loss=4.703852653503418
I0425 21:24:19.627274 139765365643008 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3832147717475891, loss=4.764871597290039
I0425 21:24:55.443322 139765340464896 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3644278645515442, loss=4.643571853637695
I0425 21:25:31.241240 139765365643008 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.379886656999588, loss=4.7509260177612305
I0425 21:26:07.041121 139765340464896 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.37638500332832336, loss=4.7392578125
I0425 21:26:42.855551 139765365643008 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3700634837150574, loss=4.780900001525879
I0425 21:27:18.670436 139765340464896 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3726864159107208, loss=4.7477312088012695
I0425 21:27:54.499545 139765365643008 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.36691612005233765, loss=4.65862512588501
I0425 21:28:30.315225 139765340464896 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3493626117706299, loss=4.639778137207031
I0425 21:29:06.096330 139765365643008 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3672831952571869, loss=4.666698455810547
I0425 21:29:41.916665 139765340464896 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.37760111689567566, loss=4.707006454467773
I0425 21:30:17.732640 139765365643008 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.37400296330451965, loss=4.691683292388916
I0425 21:30:53.544409 139765340464896 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.3608155846595764, loss=4.766794681549072
I0425 21:31:29.349793 139765365643008 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.34710338711738586, loss=4.6566033363342285
I0425 21:32:05.202134 139765340464896 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3540603816509247, loss=4.628365993499756
I0425 21:32:41.002751 139765365643008 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3566068410873413, loss=4.64536190032959
I0425 21:33:16.794743 139765340464896 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.37236785888671875, loss=4.771666049957275
I0425 21:33:52.616483 139765365643008 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.35644394159317017, loss=4.641658306121826
I0425 21:34:28.462332 139765340464896 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.34558889269828796, loss=4.626891613006592
I0425 21:35:04.310647 139765365643008 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.33784350752830505, loss=4.685876369476318
I0425 21:35:40.130369 139765340464896 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3554401695728302, loss=4.682960510253906
I0425 21:35:49.518006 139952621680448 spec.py:298] Evaluating on the training split.
I0425 21:35:52.516336 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:38:26.313017 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 21:38:28.968822 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:40:51.293906 139952621680448 spec.py:326] Evaluating on the test split.
I0425 21:40:54.038218 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:43:14.578908 139952621680448 submission_runner.py:406] Time since start: 7964.75s, 	Step: 11728, 	{'train/accuracy': 0.5857131481170654, 'train/loss': 2.665560483932495, 'train/bleu': 27.03849436979541, 'validation/accuracy': 0.592131495475769, 'validation/loss': 2.571803092956543, 'validation/bleu': 23.48726808448686, 'validation/num_examples': 3000, 'test/accuracy': 0.5934925675392151, 'test/loss': 2.5795488357543945, 'test/bleu': 22.133462524646472, 'test/num_examples': 3003, 'score': 4231.662074804306, 'total_duration': 7964.753214597702, 'accumulated_submission_time': 4231.662074804306, 'accumulated_eval_time': 3717.7166175842285, 'accumulated_logging_time': 15.23289155960083}
I0425 21:43:14.587509 139765365643008 logging_writer.py:48] [11728] accumulated_eval_time=3717.716618, accumulated_logging_time=15.232892, accumulated_submission_time=4231.662075, global_step=11728, preemption_count=0, score=4231.662075, test/accuracy=0.593493, test/bleu=22.133463, test/loss=2.579549, test/num_examples=3003, total_duration=7964.753215, train/accuracy=0.585713, train/bleu=27.038494, train/loss=2.665560, validation/accuracy=0.592131, validation/bleu=23.487268, validation/loss=2.571803, validation/num_examples=3000
I0425 21:43:15.102754 139952621680448 checkpoints.py:356] Saving checkpoint at step: 11728
I0425 21:43:17.668972 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_11728
I0425 21:43:17.672188 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_11728.
I0425 21:43:43.818126 139765340464896 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.3583209216594696, loss=4.579103469848633
I0425 21:44:19.633815 139765332072192 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.33645814657211304, loss=4.686858177185059
I0425 21:44:55.418944 139765340464896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3479762375354767, loss=4.615279674530029
I0425 21:45:31.259091 139765332072192 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3632369637489319, loss=4.6608991622924805
I0425 21:46:07.095173 139765340464896 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3491084575653076, loss=4.547985553741455
I0425 21:46:42.935503 139765332072192 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.34884512424468994, loss=4.635185241699219
I0425 21:47:18.801596 139765340464896 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3262162506580353, loss=4.5063862800598145
I0425 21:47:54.626646 139765332072192 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3546808362007141, loss=4.536556720733643
I0425 21:48:30.438389 139765340464896 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3448704183101654, loss=4.616194725036621
I0425 21:49:06.265441 139765332072192 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3661470413208008, loss=4.6099419593811035
I0425 21:49:42.049473 139765340464896 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3301825523376465, loss=4.583047866821289
I0425 21:50:17.886134 139765332072192 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3341658115386963, loss=4.576150417327881
I0425 21:50:53.696075 139765340464896 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.35588955879211426, loss=4.5595383644104
I0425 21:51:29.567616 139765332072192 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.34235334396362305, loss=4.569304943084717
I0425 21:52:05.378293 139765340464896 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3418472409248352, loss=4.599893093109131
I0425 21:52:41.204351 139765332072192 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.33701851963996887, loss=4.674674987792969
I0425 21:53:16.987892 139765340464896 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3416282534599304, loss=4.509912967681885
I0425 21:53:52.813518 139765332072192 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3297826051712036, loss=4.638762474060059
I0425 21:54:28.645971 139765340464896 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3529216945171356, loss=4.542377948760986
I0425 21:55:04.497935 139765332072192 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3312632143497467, loss=4.524503231048584
I0425 21:55:40.313757 139765340464896 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3393765091896057, loss=4.512801647186279
I0425 21:56:16.130706 139765332072192 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.34104111790657043, loss=4.568507194519043
I0425 21:56:51.943419 139765340464896 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.36099180579185486, loss=4.568517208099365
I0425 21:57:17.824074 139952621680448 spec.py:298] Evaluating on the training split.
I0425 21:57:20.817687 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 21:59:50.417924 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 21:59:53.089851 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:02:14.989115 139952621680448 spec.py:326] Evaluating on the test split.
I0425 22:02:17.703555 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:04:38.093094 139952621680448 submission_runner.py:406] Time since start: 9248.27s, 	Step: 14074, 	{'train/accuracy': 0.5999954342842102, 'train/loss': 2.5163180828094482, 'train/bleu': 28.194325508877238, 'validation/accuracy': 0.6034147143363953, 'validation/loss': 2.4488887786865234, 'validation/bleu': 24.25053557781743, 'validation/num_examples': 3000, 'test/accuracy': 0.607901930809021, 'test/loss': 2.4428367614746094, 'test/bleu': 22.980492940269855, 'test/num_examples': 3003, 'score': 5071.782253503799, 'total_duration': 9248.267380714417, 'accumulated_submission_time': 5071.782253503799, 'accumulated_eval_time': 4157.985598087311, 'accumulated_logging_time': 18.32875347137451}
I0425 22:04:38.103296 139765332072192 logging_writer.py:48] [14074] accumulated_eval_time=4157.985598, accumulated_logging_time=18.328753, accumulated_submission_time=5071.782254, global_step=14074, preemption_count=0, score=5071.782254, test/accuracy=0.607902, test/bleu=22.980493, test/loss=2.442837, test/num_examples=3003, total_duration=9248.267381, train/accuracy=0.599995, train/bleu=28.194326, train/loss=2.516318, validation/accuracy=0.603415, validation/bleu=24.250536, validation/loss=2.448889, validation/num_examples=3000
I0425 22:04:38.619966 139952621680448 checkpoints.py:356] Saving checkpoint at step: 14074
I0425 22:04:41.164383 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_14074
I0425 22:04:41.167652 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_14074.
I0425 22:04:50.854687 139765340464896 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.34852471947669983, loss=4.588819980621338
I0425 22:05:26.670507 139765323679488 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.3359707295894623, loss=4.435423851013184
I0425 22:06:02.520207 139765340464896 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3459842801094055, loss=4.497145175933838
I0425 22:06:38.329605 139765323679488 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.33557945489883423, loss=4.450153350830078
I0425 22:07:14.186745 139765340464896 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.3224359452724457, loss=4.583125114440918
I0425 22:07:50.001942 139765323679488 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3350353240966797, loss=4.535568714141846
I0425 22:08:25.810216 139765340464896 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3426886200904846, loss=4.501892566680908
I0425 22:09:01.622569 139765323679488 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.347314715385437, loss=4.56057071685791
I0425 22:09:37.442256 139765340464896 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3337579667568207, loss=4.5797247886657715
I0425 22:10:13.245729 139765323679488 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3342127799987793, loss=4.529487609863281
I0425 22:10:49.032062 139765340464896 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3321298360824585, loss=4.444108963012695
I0425 22:11:24.885836 139765323679488 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3320900499820709, loss=4.551681041717529
I0425 22:12:00.694377 139765340464896 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.33777517080307007, loss=4.491513729095459
I0425 22:12:36.515366 139765323679488 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3194630444049835, loss=4.478905200958252
I0425 22:13:12.322049 139765340464896 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3243788778781891, loss=4.464554309844971
I0425 22:13:48.182032 139765323679488 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.32407239079475403, loss=4.557934284210205
I0425 22:14:23.980609 139765340464896 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.31895866990089417, loss=4.552333354949951
I0425 22:14:59.789442 139765323679488 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.33910438418388367, loss=4.384931564331055
I0425 22:15:35.606658 139765340464896 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3156711757183075, loss=4.43520450592041
I0425 22:16:11.430174 139765323679488 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3230142295360565, loss=4.5237717628479
I0425 22:16:47.252028 139765340464896 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3440197706222534, loss=4.47094202041626
I0425 22:17:23.094763 139765323679488 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3309904634952545, loss=4.4219889640808105
I0425 22:17:58.896552 139765340464896 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3278844952583313, loss=4.436290740966797
I0425 22:18:34.679689 139765323679488 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3195907473564148, loss=4.416051864624023
I0425 22:18:41.210399 139952621680448 spec.py:298] Evaluating on the training split.
I0425 22:18:44.205397 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:21:17.202842 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 22:21:19.856536 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:23:46.775728 139952621680448 spec.py:326] Evaluating on the test split.
I0425 22:23:49.496542 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:26:01.991365 139952621680448 submission_runner.py:406] Time since start: 10532.17s, 	Step: 16420, 	{'train/accuracy': 0.6040814518928528, 'train/loss': 2.475813865661621, 'train/bleu': 28.921717692622874, 'validation/accuracy': 0.6132472157478333, 'validation/loss': 2.3560404777526855, 'validation/bleu': 25.012000107904797, 'validation/num_examples': 3000, 'test/accuracy': 0.6182674169540405, 'test/loss': 2.3404886722564697, 'test/bleu': 23.866184136573647, 'test/num_examples': 3003, 'score': 5911.793899774551, 'total_duration': 10532.165665864944, 'accumulated_submission_time': 5911.793899774551, 'accumulated_eval_time': 4598.76650929451, 'accumulated_logging_time': 21.405864715576172}
I0425 22:26:02.000738 139765340464896 logging_writer.py:48] [16420] accumulated_eval_time=4598.766509, accumulated_logging_time=21.405865, accumulated_submission_time=5911.793900, global_step=16420, preemption_count=0, score=5911.793900, test/accuracy=0.618267, test/bleu=23.866184, test/loss=2.340489, test/num_examples=3003, total_duration=10532.165666, train/accuracy=0.604081, train/bleu=28.921718, train/loss=2.475814, validation/accuracy=0.613247, validation/bleu=25.012000, validation/loss=2.356040, validation/num_examples=3000
I0425 22:26:02.527211 139952621680448 checkpoints.py:356] Saving checkpoint at step: 16420
I0425 22:26:05.083731 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_16420
I0425 22:26:05.086972 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_16420.
I0425 22:26:34.129177 139765323679488 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.32597628235816956, loss=4.4970526695251465
I0425 22:27:09.972648 139765315286784 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.31570446491241455, loss=4.412877082824707
I0425 22:27:45.823273 139765323679488 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3293832838535309, loss=4.508358478546143
I0425 22:28:21.661966 139765315286784 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.32767289876937866, loss=4.50567102432251
I0425 22:28:57.467862 139765323679488 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.32662561535835266, loss=4.468005180358887
I0425 22:29:33.301789 139765315286784 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3235146403312683, loss=4.46394157409668
I0425 22:30:09.108222 139765323679488 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.316118448972702, loss=4.468106269836426
I0425 22:30:44.938978 139765315286784 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3163934648036957, loss=4.454083442687988
I0425 22:31:20.768613 139765323679488 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3081130087375641, loss=4.361787796020508
I0425 22:31:56.569227 139765315286784 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3190949857234955, loss=4.467110633850098
I0425 22:32:32.392383 139765323679488 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3215978741645813, loss=4.479727745056152
I0425 22:33:08.255298 139765315286784 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.33402019739151, loss=4.421947479248047
I0425 22:33:44.054207 139765323679488 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.313833624124527, loss=4.454640865325928
I0425 22:34:19.877921 139765315286784 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.33201509714126587, loss=4.458728790283203
I0425 22:34:55.711457 139765323679488 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.33674442768096924, loss=4.521246910095215
I0425 22:35:31.546629 139765315286784 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.32285067439079285, loss=4.400381565093994
I0425 22:36:07.360939 139765323679488 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3357706367969513, loss=4.4766950607299805
I0425 22:36:43.190201 139765315286784 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.32746803760528564, loss=4.494295597076416
I0425 22:37:19.046993 139765323679488 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3407902717590332, loss=4.458765983581543
I0425 22:37:54.881960 139765315286784 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3121166527271271, loss=4.369795322418213
I0425 22:38:30.688824 139765323679488 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.32656946778297424, loss=4.414669513702393
I0425 22:39:06.519567 139765315286784 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.32721415162086487, loss=4.366216659545898
I0425 22:39:42.364031 139765323679488 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.31793877482414246, loss=4.41049861907959
I0425 22:40:05.359730 139952621680448 spec.py:298] Evaluating on the training split.
I0425 22:40:08.354177 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:42:45.430048 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 22:42:48.092839 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:45:16.663287 139952621680448 spec.py:326] Evaluating on the test split.
I0425 22:45:19.382545 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:47:38.835370 139952621680448 submission_runner.py:406] Time since start: 11829.01s, 	Step: 18766, 	{'train/accuracy': 0.6214444041252136, 'train/loss': 2.3568406105041504, 'train/bleu': 30.13559524791661, 'validation/accuracy': 0.621628999710083, 'validation/loss': 2.3118882179260254, 'validation/bleu': 25.7477796380635, 'validation/num_examples': 3000, 'test/accuracy': 0.6251118779182434, 'test/loss': 2.2857255935668945, 'test/bleu': 24.498820508110693, 'test/num_examples': 3003, 'score': 6752.035491228104, 'total_duration': 11829.00967335701, 'accumulated_submission_time': 6752.035491228104, 'accumulated_eval_time': 5052.242102146149, 'accumulated_logging_time': 24.50397825241089}
I0425 22:47:38.845403 139765315286784 logging_writer.py:48] [18766] accumulated_eval_time=5052.242102, accumulated_logging_time=24.503978, accumulated_submission_time=6752.035491, global_step=18766, preemption_count=0, score=6752.035491, test/accuracy=0.625112, test/bleu=24.498821, test/loss=2.285726, test/num_examples=3003, total_duration=11829.009673, train/accuracy=0.621444, train/bleu=30.135595, train/loss=2.356841, validation/accuracy=0.621629, validation/bleu=25.747780, validation/loss=2.311888, validation/num_examples=3000
I0425 22:47:39.370123 139952621680448 checkpoints.py:356] Saving checkpoint at step: 18766
I0425 22:47:41.887154 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_18766
I0425 22:47:41.890300 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_18766.
I0425 22:47:54.429296 139765323679488 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3216228783130646, loss=4.497392177581787
I0425 22:48:30.246997 139765306894080 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.32799339294433594, loss=4.4694929122924805
I0425 22:49:06.036991 139765323679488 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.32884863018989563, loss=4.38597297668457
I0425 22:49:41.861749 139765306894080 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3221566379070282, loss=4.381978988647461
I0425 22:50:17.647013 139765323679488 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3347039222717285, loss=4.372628688812256
I0425 22:50:53.439490 139765306894080 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.31472668051719666, loss=4.34294319152832
I0425 22:51:29.238985 139765323679488 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.32614392042160034, loss=4.371296405792236
I0425 22:52:05.033619 139765306894080 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.32776251435279846, loss=4.446969032287598
I0425 22:52:40.852459 139765323679488 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3208167254924774, loss=4.380414009094238
I0425 22:53:16.648773 139765306894080 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.321808785200119, loss=4.4952898025512695
I0425 22:53:52.453312 139765323679488 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.31077226996421814, loss=4.37382173538208
I0425 22:54:28.287486 139765306894080 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.313226580619812, loss=4.4266862869262695
I0425 22:55:03.441479 139952621680448 spec.py:298] Evaluating on the training split.
I0425 22:55:06.430379 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 22:57:37.419131 139952621680448 spec.py:310] Evaluating on the validation split.
I0425 22:57:40.079285 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 23:00:02.502840 139952621680448 spec.py:326] Evaluating on the test split.
I0425 23:00:05.216370 139952621680448 workload.py:179] Translating evaluation dataset.
I0425 23:02:18.006759 139952621680448 submission_runner.py:406] Time since start: 12708.18s, 	Step: 20000, 	{'train/accuracy': 0.6189174652099609, 'train/loss': 2.3463094234466553, 'train/bleu': 29.45858504943814, 'validation/accuracy': 0.6233028769493103, 'validation/loss': 2.2713868618011475, 'validation/bleu': 25.623318889644224, 'validation/num_examples': 3000, 'test/accuracy': 0.6288304328918457, 'test/loss': 2.247371196746826, 'test/bleu': 24.375669970798473, 'test/num_examples': 3003, 'score': 7193.568562269211, 'total_duration': 12708.181058883667, 'accumulated_submission_time': 7193.568562269211, 'accumulated_eval_time': 5486.807333469391, 'accumulated_logging_time': 27.56141757965088}
I0425 23:02:18.016664 139765323679488 logging_writer.py:48] [20000] accumulated_eval_time=5486.807333, accumulated_logging_time=27.561418, accumulated_submission_time=7193.568562, global_step=20000, preemption_count=0, score=7193.568562, test/accuracy=0.628830, test/bleu=24.375670, test/loss=2.247371, test/num_examples=3003, total_duration=12708.181059, train/accuracy=0.618917, train/bleu=29.458585, train/loss=2.346309, validation/accuracy=0.623303, validation/bleu=25.623319, validation/loss=2.271387, validation/num_examples=3000
I0425 23:02:18.546918 139952621680448 checkpoints.py:356] Saving checkpoint at step: 20000
I0425 23:02:21.070091 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_20000
I0425 23:02:21.073269 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_20000.
I0425 23:02:21.083932 139765306894080 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7193.568562
I0425 23:02:21.424476 139952621680448 checkpoints.py:356] Saving checkpoint at step: 20000
I0425 23:02:25.149462 139952621680448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_20000
I0425 23:02:25.152425 139952621680448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/wmt_jax/trial_1/checkpoint_20000.
I0425 23:02:25.187712 139952621680448 submission_runner.py:567] Tuning trial 1/1
I0425 23:02:25.187901 139952621680448 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0425 23:02:25.189186 139952621680448 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005966860917396843, 'train/loss': 11.112687110900879, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.123942375183105, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.1343355178833, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.741838932037354, 'total_duration': 947.3356053829193, 'accumulated_submission_time': 30.741838932037354, 'accumulated_eval_time': 916.5936207771301, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2346, {'train/accuracy': 0.3033105134963989, 'train/loss': 5.5579938888549805, 'train/bleu': 6.419694748893433, 'validation/accuracy': 0.27818626165390015, 'validation/loss': 5.8347320556640625, 'validation/bleu': 3.301721334439692, 'validation/num_examples': 3000, 'test/accuracy': 0.2580326497554779, 'test/loss': 6.1311936378479, 'test/bleu': 2.437064798580302, 'test/num_examples': 3003, 'score': 870.9246771335602, 'total_duration': 2588.674791574478, 'accumulated_submission_time': 870.9246771335602, 'accumulated_eval_time': 1714.657145023346, 'accumulated_logging_time': 3.064432382583618, 'global_step': 2346, 'preemption_count': 0}), (4690, {'train/accuracy': 0.4561130404472351, 'train/loss': 3.8999946117401123, 'train/bleu': 16.875021082047898, 'validation/accuracy': 0.4479423761367798, 'validation/loss': 3.96743106842041, 'validation/bleu': 12.506404695446117, 'validation/num_examples': 3000, 'test/accuracy': 0.4349311590194702, 'test/loss': 4.14227294921875, 'test/bleu': 10.81789456256121, 'test/num_examples': 3003, 'score': 1710.9734861850739, 'total_duration': 4039.129624605179, 'accumulated_submission_time': 1710.9734861850739, 'accumulated_eval_time': 2322.0165536403656, 'accumulated_logging_time': 6.082303762435913, 'global_step': 4690, 'preemption_count': 0}), (7035, {'train/accuracy': 0.5387536287307739, 'train/loss': 3.1194348335266113, 'train/bleu': 24.37418896466692, 'validation/accuracy': 0.53580242395401, 'validation/loss': 3.121774673461914, 'validation/bleu': 19.750282301725107, 'validation/num_examples': 3000, 'test/accuracy': 0.5339260101318359, 'test/loss': 3.178410053253174, 'test/bleu': 18.099241096610537, 'test/num_examples': 3003, 'score': 2551.167027950287, 'total_duration': 5370.72748541832, 'accumulated_submission_time': 2551.167027950287, 'accumulated_eval_time': 2810.3637821674347, 'accumulated_logging_time': 9.11095142364502, 'global_step': 7035, 'preemption_count': 0}), (9381, {'train/accuracy': 0.5672525763511658, 'train/loss': 2.8628101348876953, 'train/bleu': 26.27841804197872, 'validation/accuracy': 0.5698875188827515, 'validation/loss': 2.783269166946411, 'validation/bleu': 22.141380887962228, 'validation/num_examples': 3000, 'test/accuracy': 0.5700540542602539, 'test/loss': 2.810173273086548, 'test/bleu': 20.791546837028186, 'test/num_examples': 3003, 'score': 3391.3743348121643, 'total_duration': 6676.310456752777, 'accumulated_submission_time': 3391.3743348121643, 'accumulated_eval_time': 3272.6557533740997, 'accumulated_logging_time': 12.16715693473816, 'global_step': 9381, 'preemption_count': 0}), (11728, {'train/accuracy': 0.5857131481170654, 'train/loss': 2.665560483932495, 'train/bleu': 27.03849436979541, 'validation/accuracy': 0.592131495475769, 'validation/loss': 2.571803092956543, 'validation/bleu': 23.48726808448686, 'validation/num_examples': 3000, 'test/accuracy': 0.5934925675392151, 'test/loss': 2.5795488357543945, 'test/bleu': 22.133462524646472, 'test/num_examples': 3003, 'score': 4231.662074804306, 'total_duration': 7964.753214597702, 'accumulated_submission_time': 4231.662074804306, 'accumulated_eval_time': 3717.7166175842285, 'accumulated_logging_time': 15.23289155960083, 'global_step': 11728, 'preemption_count': 0}), (14074, {'train/accuracy': 0.5999954342842102, 'train/loss': 2.5163180828094482, 'train/bleu': 28.194325508877238, 'validation/accuracy': 0.6034147143363953, 'validation/loss': 2.4488887786865234, 'validation/bleu': 24.25053557781743, 'validation/num_examples': 3000, 'test/accuracy': 0.607901930809021, 'test/loss': 2.4428367614746094, 'test/bleu': 22.980492940269855, 'test/num_examples': 3003, 'score': 5071.782253503799, 'total_duration': 9248.267380714417, 'accumulated_submission_time': 5071.782253503799, 'accumulated_eval_time': 4157.985598087311, 'accumulated_logging_time': 18.32875347137451, 'global_step': 14074, 'preemption_count': 0}), (16420, {'train/accuracy': 0.6040814518928528, 'train/loss': 2.475813865661621, 'train/bleu': 28.921717692622874, 'validation/accuracy': 0.6132472157478333, 'validation/loss': 2.3560404777526855, 'validation/bleu': 25.012000107904797, 'validation/num_examples': 3000, 'test/accuracy': 0.6182674169540405, 'test/loss': 2.3404886722564697, 'test/bleu': 23.866184136573647, 'test/num_examples': 3003, 'score': 5911.793899774551, 'total_duration': 10532.165665864944, 'accumulated_submission_time': 5911.793899774551, 'accumulated_eval_time': 4598.76650929451, 'accumulated_logging_time': 21.405864715576172, 'global_step': 16420, 'preemption_count': 0}), (18766, {'train/accuracy': 0.6214444041252136, 'train/loss': 2.3568406105041504, 'train/bleu': 30.13559524791661, 'validation/accuracy': 0.621628999710083, 'validation/loss': 2.3118882179260254, 'validation/bleu': 25.7477796380635, 'validation/num_examples': 3000, 'test/accuracy': 0.6251118779182434, 'test/loss': 2.2857255935668945, 'test/bleu': 24.498820508110693, 'test/num_examples': 3003, 'score': 6752.035491228104, 'total_duration': 11829.00967335701, 'accumulated_submission_time': 6752.035491228104, 'accumulated_eval_time': 5052.242102146149, 'accumulated_logging_time': 24.50397825241089, 'global_step': 18766, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6189174652099609, 'train/loss': 2.3463094234466553, 'train/bleu': 29.45858504943814, 'validation/accuracy': 0.6233028769493103, 'validation/loss': 2.2713868618011475, 'validation/bleu': 25.623318889644224, 'validation/num_examples': 3000, 'test/accuracy': 0.6288304328918457, 'test/loss': 2.247371196746826, 'test/bleu': 24.375669970798473, 'test/num_examples': 3003, 'score': 7193.568562269211, 'total_duration': 12708.181058883667, 'accumulated_submission_time': 7193.568562269211, 'accumulated_eval_time': 5486.807333469391, 'accumulated_logging_time': 27.56141757965088, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0425 23:02:25.189329 139952621680448 submission_runner.py:570] Timing: 7193.568562269211
I0425 23:02:25.189374 139952621680448 submission_runner.py:571] ====================
I0425 23:02:25.189495 139952621680448 submission_runner.py:631] Final wmt score: 7193.568562269211
