I0425 18:19:53.834640 139669207562048 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax.
I0425 18:19:53.907588 139669207562048 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0425 18:19:54.743573 139669207562048 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0425 18:19:54.744282 139669207562048 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0425 18:19:54.748507 139669207562048 submission_runner.py:528] Using RNG seed 4272373226
I0425 18:19:57.543848 139669207562048 submission_runner.py:537] --- Tuning run 1/1 ---
I0425 18:19:57.544047 139669207562048 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1.
I0425 18:19:57.544310 139669207562048 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/hparams.json.
I0425 18:19:57.668625 139669207562048 submission_runner.py:232] Initializing dataset.
I0425 18:19:57.902134 139669207562048 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:19:57.907994 139669207562048 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0425 18:19:58.143469 139669207562048 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:19:58.200730 139669207562048 submission_runner.py:239] Initializing model.
I0425 18:20:05.469742 139669207562048 submission_runner.py:249] Initializing optimizer.
I0425 18:20:05.849926 139669207562048 submission_runner.py:256] Initializing metrics bundle.
I0425 18:20:05.850164 139669207562048 submission_runner.py:273] Initializing checkpoint and logger.
I0425 18:20:05.851340 139669207562048 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1 with prefix checkpoint_
I0425 18:20:05.851632 139669207562048 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0425 18:20:05.851702 139669207562048 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0425 18:20:06.702687 139669207562048 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/meta_data_0.json.
I0425 18:20:06.703643 139669207562048 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/flags_0.json.
I0425 18:20:06.709604 139669207562048 submission_runner.py:309] Starting training loop.
I0425 18:20:26.191424 139493248067328 logging_writer.py:48] [0] global_step=0, grad_norm=2.721752166748047, loss=0.7373720407485962
I0425 18:20:26.204239 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:20:26.212275 139669207562048 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:20:26.216370 139669207562048 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0425 18:20:26.271649 139669207562048 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0425 18:20:42.497580 139669207562048 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0425 18:21:57.455827 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:21:57.458922 139669207562048 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:21:57.462881 139669207562048 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0425 18:21:57.516952 139669207562048 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:23:00.497507 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:23:00.500297 139669207562048 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:23:00.504384 139669207562048 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0425 18:23:00.556864 139669207562048 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0425 18:24:03.620700 139669207562048 submission_runner.py:406] Time since start: 236.91s, 	Step: 1, 	{'train/accuracy': 0.5279272794723511, 'train/loss': 0.7360531687736511, 'train/mean_average_precision': 0.023131126022461864, 'validation/accuracy': 0.5234463214874268, 'validation/loss': 0.7393096685409546, 'validation/mean_average_precision': 0.027053965761648615, 'validation/num_examples': 43793, 'test/accuracy': 0.5214398503303528, 'test/loss': 0.7397536039352417, 'test/mean_average_precision': 0.029496470239449072, 'test/num_examples': 43793, 'score': 19.494435787200928, 'total_duration': 236.91104650497437, 'accumulated_submission_time': 19.494435787200928, 'accumulated_eval_time': 217.41644763946533, 'accumulated_logging_time': 0}
I0425 18:24:03.642503 139483441370880 logging_writer.py:48] [1] accumulated_eval_time=217.416448, accumulated_logging_time=0, accumulated_submission_time=19.494436, global_step=1, preemption_count=0, score=19.494436, test/accuracy=0.521440, test/loss=0.739754, test/mean_average_precision=0.029496, test/num_examples=43793, total_duration=236.911047, train/accuracy=0.527927, train/loss=0.736053, train/mean_average_precision=0.023131, validation/accuracy=0.523446, validation/loss=0.739310, validation/mean_average_precision=0.027054, validation/num_examples=43793
I0425 18:24:03.670181 139669207562048 checkpoints.py:356] Saving checkpoint at step: 1
I0425 18:24:03.737552 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_1
I0425 18:24:03.737759 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_1.
I0425 18:24:27.259612 139483449763584 logging_writer.py:48] [100] global_step=100, grad_norm=0.06772170960903168, loss=0.09531522542238235
I0425 18:24:51.033549 139484859148032 logging_writer.py:48] [200] global_step=200, grad_norm=0.017654351890087128, loss=0.053822990506887436
I0425 18:25:14.957553 139483449763584 logging_writer.py:48] [300] global_step=300, grad_norm=0.010448655113577843, loss=0.058161452412605286
I0425 18:25:38.764465 139484859148032 logging_writer.py:48] [400] global_step=400, grad_norm=0.009020230732858181, loss=0.051179010421037674
I0425 18:26:02.484357 139483449763584 logging_writer.py:48] [500] global_step=500, grad_norm=0.012624167837202549, loss=0.05156980827450752
I0425 18:26:26.431695 139484859148032 logging_writer.py:48] [600] global_step=600, grad_norm=0.011280134320259094, loss=0.05947982519865036
I0425 18:26:50.350385 139483449763584 logging_writer.py:48] [700] global_step=700, grad_norm=0.012382453307509422, loss=0.059735074639320374
I0425 18:27:14.124317 139484859148032 logging_writer.py:48] [800] global_step=800, grad_norm=0.02266009710729122, loss=0.051891256123781204
I0425 18:27:37.922535 139483449763584 logging_writer.py:48] [900] global_step=900, grad_norm=0.026923183351755142, loss=0.056760724633932114
I0425 18:28:01.796026 139484859148032 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.029212115332484245, loss=0.05289929360151291
I0425 18:28:03.809662 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:29:18.375490 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:29:20.942855 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:29:23.447808 139669207562048 submission_runner.py:406] Time since start: 556.74s, 	Step: 1009, 	{'train/accuracy': 0.9867992997169495, 'train/loss': 0.05421062558889389, 'train/mean_average_precision': 0.03219433064086116, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06417354196310043, 'validation/mean_average_precision': 0.035083488987328555, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06740381568670273, 'test/mean_average_precision': 0.037477925777762724, 'test/num_examples': 43793, 'score': 259.5566499233246, 'total_duration': 556.7381258010864, 'accumulated_submission_time': 259.5566499233246, 'accumulated_eval_time': 297.05453658103943, 'accumulated_logging_time': 0.11717796325683594}
I0425 18:29:23.456389 139483449763584 logging_writer.py:48] [1009] accumulated_eval_time=297.054537, accumulated_logging_time=0.117178, accumulated_submission_time=259.556650, global_step=1009, preemption_count=0, score=259.556650, test/accuracy=0.983142, test/loss=0.067404, test/mean_average_precision=0.037478, test/num_examples=43793, total_duration=556.738126, train/accuracy=0.986799, train/loss=0.054211, train/mean_average_precision=0.032194, validation/accuracy=0.984118, validation/loss=0.064174, validation/mean_average_precision=0.035083, validation/num_examples=43793
I0425 18:29:23.481616 139669207562048 checkpoints.py:356] Saving checkpoint at step: 1009
I0425 18:29:23.548319 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_1009
I0425 18:29:23.548519 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_1009.
I0425 18:29:45.367131 139484859148032 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.08695270121097565, loss=0.05194398760795593
I0425 18:30:09.379309 139484691687168 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0773160308599472, loss=0.05080149322748184
I0425 18:30:33.327342 139484859148032 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.021924110129475594, loss=0.05509255826473236
I0425 18:30:57.451599 139484691687168 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.07173831760883331, loss=0.047909606248140335
I0425 18:31:21.360696 139484859148032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.057766471058130264, loss=0.05555678904056549
I0425 18:31:45.614755 139484691687168 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.05182386189699173, loss=0.05108330026268959
I0425 18:32:10.078168 139484859148032 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.04266361519694328, loss=0.0499274842441082
I0425 18:32:34.324828 139484691687168 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03136276826262474, loss=0.05400417372584343
I0425 18:32:58.499469 139484859148032 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.09328118711709976, loss=0.05169786140322685
I0425 18:33:22.152831 139484691687168 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.13216346502304077, loss=0.0512307770550251
I0425 18:33:23.779938 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:34:38.310308 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:34:40.841119 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:34:43.329945 139669207562048 submission_runner.py:406] Time since start: 876.62s, 	Step: 2008, 	{'train/accuracy': 0.9866699576377869, 'train/loss': 0.05180283635854721, 'train/mean_average_precision': 0.05700483191804362, 'validation/accuracy': 0.984120786190033, 'validation/loss': 0.0610467791557312, 'validation/mean_average_precision': 0.05281911502317528, 'validation/num_examples': 43793, 'test/accuracy': 0.9831475615501404, 'test/loss': 0.06433193385601044, 'test/mean_average_precision': 0.054804562881464736, 'test/num_examples': 43793, 'score': 499.7782917022705, 'total_duration': 876.6202344894409, 'accumulated_submission_time': 499.7782917022705, 'accumulated_eval_time': 376.60449481010437, 'accumulated_logging_time': 0.2180490493774414}
I0425 18:34:43.337392 139484859148032 logging_writer.py:48] [2008] accumulated_eval_time=376.604495, accumulated_logging_time=0.218049, accumulated_submission_time=499.778292, global_step=2008, preemption_count=0, score=499.778292, test/accuracy=0.983148, test/loss=0.064332, test/mean_average_precision=0.054805, test/num_examples=43793, total_duration=876.620234, train/accuracy=0.986670, train/loss=0.051803, train/mean_average_precision=0.057005, validation/accuracy=0.984121, validation/loss=0.061047, validation/mean_average_precision=0.052819, validation/num_examples=43793
I0425 18:34:43.361784 139669207562048 checkpoints.py:356] Saving checkpoint at step: 2008
I0425 18:34:43.424423 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_2008
I0425 18:34:43.424632 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_2008.
I0425 18:35:05.138795 139484691687168 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.046241018921136856, loss=0.05232171341776848
I0425 18:35:28.179796 139484666509056 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.027438268065452576, loss=0.05260498821735382
I0425 18:35:51.778882 139484691687168 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04795253649353981, loss=0.05793875828385353
I0425 18:36:15.140712 139484666509056 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.02412538230419159, loss=0.05747650936245918
I0425 18:36:38.405864 139484691687168 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.04480214789509773, loss=0.04598071798682213
I0425 18:37:01.759602 139484666509056 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.05046095699071884, loss=0.053810037672519684
I0425 18:37:25.089679 139484691687168 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.05446985736489296, loss=0.05281240493059158
I0425 18:37:48.335442 139484666509056 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.030805697664618492, loss=0.047989264130592346
I0425 18:38:11.708244 139484691687168 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.13777944445610046, loss=0.04721463844180107
I0425 18:38:35.052301 139484666509056 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.04643678292632103, loss=0.045573025941848755
I0425 18:38:43.451091 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:39:57.201766 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:39:59.773142 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:40:02.355360 139669207562048 submission_runner.py:406] Time since start: 1195.65s, 	Step: 3037, 	{'train/accuracy': 0.9870218634605408, 'train/loss': 0.04909785836935043, 'train/mean_average_precision': 0.07216758295337047, 'validation/accuracy': 0.984221875667572, 'validation/loss': 0.05915983393788338, 'validation/mean_average_precision': 0.06886333177490309, 'validation/num_examples': 43793, 'test/accuracy': 0.9832444190979004, 'test/loss': 0.06231933832168579, 'test/mean_average_precision': 0.07418519662775398, 'test/num_examples': 43793, 'score': 739.7952632904053, 'total_duration': 1195.6456789970398, 'accumulated_submission_time': 739.7952632904053, 'accumulated_eval_time': 455.5087134838104, 'accumulated_logging_time': 0.3128964900970459}
I0425 18:40:02.363513 139484691687168 logging_writer.py:48] [3037] accumulated_eval_time=455.508713, accumulated_logging_time=0.312896, accumulated_submission_time=739.795263, global_step=3037, preemption_count=0, score=739.795263, test/accuracy=0.983244, test/loss=0.062319, test/mean_average_precision=0.074185, test/num_examples=43793, total_duration=1195.645679, train/accuracy=0.987022, train/loss=0.049098, train/mean_average_precision=0.072168, validation/accuracy=0.984222, validation/loss=0.059160, validation/mean_average_precision=0.068863, validation/num_examples=43793
I0425 18:40:02.389779 139669207562048 checkpoints.py:356] Saving checkpoint at step: 3037
I0425 18:40:02.449566 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_3037
I0425 18:40:02.449766 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_3037.
I0425 18:40:17.239805 139484666509056 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.04597901925444603, loss=0.05179086700081825
I0425 18:40:40.380557 139484658116352 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.031890869140625, loss=0.045758336782455444
I0425 18:41:03.995253 139484666509056 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.04454975202679634, loss=0.05185958743095398
I0425 18:41:27.198604 139484658116352 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.07032217085361481, loss=0.048208076506853104
I0425 18:41:50.331837 139484666509056 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.20448391139507294, loss=0.05280070751905441
I0425 18:42:13.743459 139484658116352 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.07592079043388367, loss=0.05096839740872383
I0425 18:42:37.217193 139484666509056 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.09051066637039185, loss=0.04884970188140869
I0425 18:43:02.638824 139484658116352 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.046145323663949966, loss=0.04751501604914665
I0425 18:43:25.868364 139484666509056 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.044780440628528595, loss=0.05131088197231293
I0425 18:43:49.170811 139484658116352 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.04741467162966728, loss=0.047261569648981094
I0425 18:44:02.460302 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:45:15.950549 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:45:18.461186 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:45:20.946588 139669207562048 submission_runner.py:406] Time since start: 1514.24s, 	Step: 4058, 	{'train/accuracy': 0.9869559407234192, 'train/loss': 0.04711682349443436, 'train/mean_average_precision': 0.10602670245384556, 'validation/accuracy': 0.9842920899391174, 'validation/loss': 0.056203443557024, 'validation/mean_average_precision': 0.10379701264381098, 'validation/num_examples': 43793, 'test/accuracy': 0.9833299517631531, 'test/loss': 0.059151362627744675, 'test/mean_average_precision': 0.10541523708252658, 'test/num_examples': 43793, 'score': 979.7958779335022, 'total_duration': 1514.2369213104248, 'accumulated_submission_time': 979.7958779335022, 'accumulated_eval_time': 533.994964838028, 'accumulated_logging_time': 0.407482385635376}
I0425 18:45:20.955543 139484666509056 logging_writer.py:48] [4058] accumulated_eval_time=533.994965, accumulated_logging_time=0.407482, accumulated_submission_time=979.795878, global_step=4058, preemption_count=0, score=979.795878, test/accuracy=0.983330, test/loss=0.059151, test/mean_average_precision=0.105415, test/num_examples=43793, total_duration=1514.236921, train/accuracy=0.986956, train/loss=0.047117, train/mean_average_precision=0.106027, validation/accuracy=0.984292, validation/loss=0.056203, validation/mean_average_precision=0.103797, validation/num_examples=43793
I0425 18:45:20.981763 139669207562048 checkpoints.py:356] Saving checkpoint at step: 4058
I0425 18:45:21.038977 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_4058
I0425 18:45:21.039207 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_4058.
I0425 18:45:31.133086 139484658116352 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.043019700795412064, loss=0.04588336870074272
I0425 18:45:54.193926 139484649723648 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.10712242126464844, loss=0.05013212561607361
I0425 18:46:17.237009 139484658116352 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04160069674253464, loss=0.051944080740213394
I0425 18:46:40.409648 139484649723648 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04965362697839737, loss=0.04884018003940582
I0425 18:47:03.615942 139484658116352 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.1502532958984375, loss=0.04969620704650879
I0425 18:47:26.763876 139484649723648 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.08884385228157043, loss=0.04754802957177162
I0425 18:47:49.911051 139484658116352 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.08469343930482864, loss=0.042833633720874786
I0425 18:48:13.557093 139484649723648 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04734190180897713, loss=0.04591316729784012
I0425 18:48:36.875668 139484658116352 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.07249747961759567, loss=0.044743504375219345
I0425 18:49:00.458076 139484649723648 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.14648933708667755, loss=0.0454573854804039
I0425 18:49:21.178010 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:50:33.266273 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:50:35.815625 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:50:38.316161 139669207562048 submission_runner.py:406] Time since start: 1831.61s, 	Step: 5089, 	{'train/accuracy': 0.9877461194992065, 'train/loss': 0.04450018331408501, 'train/mean_average_precision': 0.1381259663002985, 'validation/accuracy': 0.9849395751953125, 'validation/loss': 0.05337567999958992, 'validation/mean_average_precision': 0.13454919921041425, 'validation/num_examples': 43793, 'test/accuracy': 0.9839360117912292, 'test/loss': 0.05604574829339981, 'test/mean_average_precision': 0.13671668157864103, 'test/num_examples': 43793, 'score': 1219.9246366024017, 'total_duration': 1831.606472492218, 'accumulated_submission_time': 1219.9246366024017, 'accumulated_eval_time': 611.1330552101135, 'accumulated_logging_time': 0.500286340713501}
I0425 18:50:38.323814 139484658116352 logging_writer.py:48] [5089] accumulated_eval_time=611.133055, accumulated_logging_time=0.500286, accumulated_submission_time=1219.924637, global_step=5089, preemption_count=0, score=1219.924637, test/accuracy=0.983936, test/loss=0.056046, test/mean_average_precision=0.136717, test/num_examples=43793, total_duration=1831.606472, train/accuracy=0.987746, train/loss=0.044500, train/mean_average_precision=0.138126, validation/accuracy=0.984940, validation/loss=0.053376, validation/mean_average_precision=0.134549, validation/num_examples=43793
I0425 18:50:38.349936 139669207562048 checkpoints.py:356] Saving checkpoint at step: 5089
I0425 18:50:38.403398 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_5089
I0425 18:50:38.403587 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_5089.
I0425 18:50:41.251522 139484649723648 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.05094563961029053, loss=0.04642695561051369
I0425 18:51:04.640525 139484641330944 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.044371772557497025, loss=0.04680050164461136
I0425 18:51:27.606547 139484649723648 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.07959325611591339, loss=0.04534203186631203
I0425 18:51:50.933825 139484641330944 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.059515755623579025, loss=0.042879458516836166
I0425 18:52:14.359296 139484649723648 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.06299781054258347, loss=0.04521678760647774
I0425 18:52:37.579358 139484641330944 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.10031518340110779, loss=0.044945232570171356
I0425 18:53:00.757887 139484649723648 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.04548966884613037, loss=0.04765672981739044
I0425 18:53:24.189629 139484641330944 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04765867814421654, loss=0.046406619250774384
I0425 18:53:47.426109 139484649723648 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.05095779895782471, loss=0.04439649358391762
I0425 18:54:11.029819 139484641330944 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.054183479398489, loss=0.048457950353622437
I0425 18:54:34.434877 139484649723648 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.06126928701996803, loss=0.044458892196416855
I0425 18:54:38.414119 139669207562048 spec.py:298] Evaluating on the training split.
I0425 18:55:51.447836 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 18:55:54.045169 139669207562048 spec.py:326] Evaluating on the test split.
I0425 18:55:56.557572 139669207562048 submission_runner.py:406] Time since start: 2149.85s, 	Step: 6118, 	{'train/accuracy': 0.9877986311912537, 'train/loss': 0.042331814765930176, 'train/mean_average_precision': 0.16776723380815883, 'validation/accuracy': 0.9849996566772461, 'validation/loss': 0.0521932877600193, 'validation/mean_average_precision': 0.15222363673797923, 'validation/num_examples': 43793, 'test/accuracy': 0.9840030074119568, 'test/loss': 0.05512649565935135, 'test/mean_average_precision': 0.15319964526566254, 'test/num_examples': 43793, 'score': 1459.9251809120178, 'total_duration': 2149.847906112671, 'accumulated_submission_time': 1459.9251809120178, 'accumulated_eval_time': 689.276468038559, 'accumulated_logging_time': 0.5878713130950928}
I0425 18:55:56.565395 139484641330944 logging_writer.py:48] [6118] accumulated_eval_time=689.276468, accumulated_logging_time=0.587871, accumulated_submission_time=1459.925181, global_step=6118, preemption_count=0, score=1459.925181, test/accuracy=0.984003, test/loss=0.055126, test/mean_average_precision=0.153200, test/num_examples=43793, total_duration=2149.847906, train/accuracy=0.987799, train/loss=0.042332, train/mean_average_precision=0.167767, validation/accuracy=0.985000, validation/loss=0.052193, validation/mean_average_precision=0.152224, validation/num_examples=43793
I0425 18:55:56.590266 139669207562048 checkpoints.py:356] Saving checkpoint at step: 6118
I0425 18:55:56.650808 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_6118
I0425 18:55:56.651019 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_6118.
I0425 18:56:17.003838 139484649723648 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0519455149769783, loss=0.047137435525655746
I0425 18:56:40.452003 139484632938240 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.05383485555648804, loss=0.04625091701745987
I0425 18:57:03.738827 139484649723648 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.09461651742458344, loss=0.04508243128657341
I0425 18:57:27.211985 139484632938240 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.06238468363881111, loss=0.045713815838098526
I0425 18:57:50.673641 139484649723648 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.05153213441371918, loss=0.04218575730919838
I0425 18:58:14.263648 139484632938240 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.04369407519698143, loss=0.04530829191207886
I0425 18:58:37.539699 139484649723648 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.04911208152770996, loss=0.04470163956284523
I0425 18:59:00.995739 139484632938240 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.04928111657500267, loss=0.043959468603134155
I0425 18:59:24.669728 139484649723648 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.05339597165584564, loss=0.04497199133038521
I0425 18:59:48.045902 139484632938240 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.04969058558344841, loss=0.04560597240924835
I0425 18:59:56.738773 139669207562048 spec.py:298] Evaluating on the training split.
I0425 19:01:09.325011 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 19:01:11.898848 139669207562048 spec.py:326] Evaluating on the test split.
I0425 19:01:14.416618 139669207562048 submission_runner.py:406] Time since start: 2467.71s, 	Step: 7138, 	{'train/accuracy': 0.9880496263504028, 'train/loss': 0.04092901945114136, 'train/mean_average_precision': 0.19425539531692929, 'validation/accuracy': 0.9852330684661865, 'validation/loss': 0.05106215924024582, 'validation/mean_average_precision': 0.17055108339959457, 'validation/num_examples': 43793, 'test/accuracy': 0.9842792749404907, 'test/loss': 0.05395810306072235, 'test/mean_average_precision': 0.17302908715818577, 'test/num_examples': 43793, 'score': 1700.0029392242432, 'total_duration': 2467.7069346904755, 'accumulated_submission_time': 1700.0029392242432, 'accumulated_eval_time': 766.9542598724365, 'accumulated_logging_time': 0.6814978122711182}
I0425 19:01:14.424623 139484649723648 logging_writer.py:48] [7138] accumulated_eval_time=766.954260, accumulated_logging_time=0.681498, accumulated_submission_time=1700.002939, global_step=7138, preemption_count=0, score=1700.002939, test/accuracy=0.984279, test/loss=0.053958, test/mean_average_precision=0.173029, test/num_examples=43793, total_duration=2467.706935, train/accuracy=0.988050, train/loss=0.040929, train/mean_average_precision=0.194255, validation/accuracy=0.985233, validation/loss=0.051062, validation/mean_average_precision=0.170551, validation/num_examples=43793
I0425 19:01:14.449562 139669207562048 checkpoints.py:356] Saving checkpoint at step: 7138
I0425 19:01:14.504117 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_7138
I0425 19:01:14.504309 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_7138.
I0425 19:01:29.321781 139484632938240 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.06316986680030823, loss=0.04471269249916077
I0425 19:01:52.811041 139484624545536 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.048741135746240616, loss=0.043311186134815216
I0425 19:02:16.308218 139484632938240 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.04476145654916763, loss=0.04305650293827057
I0425 19:02:39.681215 139484624545536 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.05883905291557312, loss=0.04815208911895752
I0425 19:03:03.189548 139484632938240 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.04498286545276642, loss=0.04077085852622986
I0425 19:03:26.187295 139484624545536 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03304959833621979, loss=0.043299611657857895
I0425 19:03:49.298299 139484632938240 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.08569003641605377, loss=0.04473428428173065
I0425 19:04:12.740044 139484624545536 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.040406469255685806, loss=0.041354935616254807
I0425 19:04:35.981682 139484632938240 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.04293788596987724, loss=0.043965984135866165
I0425 19:04:59.334388 139484624545536 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0469895675778389, loss=0.04395265132188797
I0425 19:05:14.601192 139669207562048 spec.py:298] Evaluating on the training split.
I0425 19:06:27.753999 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 19:06:30.314613 139669207562048 spec.py:326] Evaluating on the test split.
I0425 19:06:32.798442 139669207562048 submission_runner.py:406] Time since start: 2786.09s, 	Step: 8166, 	{'train/accuracy': 0.9885864853858948, 'train/loss': 0.0390632227063179, 'train/mean_average_precision': 0.21235523907036657, 'validation/accuracy': 0.9855748414993286, 'validation/loss': 0.04941273853182793, 'validation/mean_average_precision': 0.18313720836907552, 'validation/num_examples': 43793, 'test/accuracy': 0.9845869541168213, 'test/loss': 0.052225079387426376, 'test/mean_average_precision': 0.18144379260034715, 'test/num_examples': 43793, 'score': 1940.089873790741, 'total_duration': 2786.0887620449066, 'accumulated_submission_time': 1940.089873790741, 'accumulated_eval_time': 845.1514577865601, 'accumulated_logging_time': 0.7693459987640381}
I0425 19:06:32.806678 139484632938240 logging_writer.py:48] [8166] accumulated_eval_time=845.151458, accumulated_logging_time=0.769346, accumulated_submission_time=1940.089874, global_step=8166, preemption_count=0, score=1940.089874, test/accuracy=0.984587, test/loss=0.052225, test/mean_average_precision=0.181444, test/num_examples=43793, total_duration=2786.088762, train/accuracy=0.988586, train/loss=0.039063, train/mean_average_precision=0.212355, validation/accuracy=0.985575, validation/loss=0.049413, validation/mean_average_precision=0.183137, validation/num_examples=43793
I0425 19:06:32.831429 139669207562048 checkpoints.py:356] Saving checkpoint at step: 8166
I0425 19:06:32.887199 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_8166
I0425 19:06:32.887410 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_8166.
I0425 19:06:41.082539 139484624545536 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.06035829707980156, loss=0.041989851742982864
I0425 19:07:04.677627 139484616152832 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.056106146425008774, loss=0.04013928025960922
I0425 19:07:27.780478 139484624545536 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0428314208984375, loss=0.04225953668355942
I0425 19:07:50.906493 139484616152832 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.06066828966140747, loss=0.04452929273247719
I0425 19:08:14.413127 139484624545536 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.051271967589855194, loss=0.037169087678194046
I0425 19:08:37.775711 139484616152832 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.10414355993270874, loss=0.04877374321222305
I0425 19:09:01.042579 139484624545536 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.11668955534696579, loss=0.043248239904642105
I0425 19:09:24.404834 139484616152832 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.049883268773555756, loss=0.03981202095746994
I0425 19:09:47.836479 139484624545536 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.055500566959381104, loss=0.041855551302433014
I0425 19:10:11.386767 139484616152832 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.08901460468769073, loss=0.045769594609737396
I0425 19:10:33.095634 139669207562048 spec.py:298] Evaluating on the training split.
I0425 19:11:46.424391 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 19:11:48.969821 139669207562048 spec.py:326] Evaluating on the test split.
I0425 19:11:51.478506 139669207562048 submission_runner.py:406] Time since start: 3104.77s, 	Step: 9194, 	{'train/accuracy': 0.9889345169067383, 'train/loss': 0.03836584836244583, 'train/mean_average_precision': 0.22585402485879835, 'validation/accuracy': 0.985910177230835, 'validation/loss': 0.04829385504126549, 'validation/mean_average_precision': 0.18759135553218748, 'validation/num_examples': 43793, 'test/accuracy': 0.9848656058311462, 'test/loss': 0.05099392309784889, 'test/mean_average_precision': 0.1899480665488388, 'test/num_examples': 43793, 'score': 2180.288151025772, 'total_duration': 3104.768837928772, 'accumulated_submission_time': 2180.288151025772, 'accumulated_eval_time': 923.5342991352081, 'accumulated_logging_time': 0.8584833145141602}
I0425 19:11:51.486698 139484624545536 logging_writer.py:48] [9194] accumulated_eval_time=923.534299, accumulated_logging_time=0.858483, accumulated_submission_time=2180.288151, global_step=9194, preemption_count=0, score=2180.288151, test/accuracy=0.984866, test/loss=0.050994, test/mean_average_precision=0.189948, test/num_examples=43793, total_duration=3104.768838, train/accuracy=0.988935, train/loss=0.038366, train/mean_average_precision=0.225854, validation/accuracy=0.985910, validation/loss=0.048294, validation/mean_average_precision=0.187591, validation/num_examples=43793
I0425 19:11:51.511662 139669207562048 checkpoints.py:356] Saving checkpoint at step: 9194
I0425 19:11:51.570084 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_9194
I0425 19:11:51.570273 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_9194.
I0425 19:11:53.245827 139484616152832 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.04228632152080536, loss=0.04033864289522171
I0425 19:12:16.948801 139484607760128 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.05639783665537834, loss=0.045109353959560394
I0425 19:12:40.606378 139484616152832 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.055691640824079514, loss=0.045418839901685715
I0425 19:13:04.189229 139484607760128 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.07568243145942688, loss=0.04382895305752754
I0425 19:13:27.542859 139484616152832 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.07140262424945831, loss=0.04107849299907684
I0425 19:13:51.102727 139484607760128 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.04209694266319275, loss=0.04026418551802635
I0425 19:14:14.619224 139484616152832 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.042079176753759384, loss=0.04177475348114967
I0425 19:14:38.028248 139484607760128 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.07094442844390869, loss=0.04784134402871132
I0425 19:15:01.604695 139484616152832 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.10099378973245621, loss=0.04358338937163353
I0425 19:15:25.137524 139484607760128 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.09226057678461075, loss=0.04576275497674942
I0425 19:15:48.696297 139484616152832 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.059745077043771744, loss=0.04434547945857048
I0425 19:15:51.762289 139669207562048 spec.py:298] Evaluating on the training split.
I0425 19:17:05.437702 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 19:17:08.029932 139669207562048 spec.py:326] Evaluating on the test split.
I0425 19:17:10.507390 139669207562048 submission_runner.py:406] Time since start: 3423.80s, 	Step: 10214, 	{'train/accuracy': 0.9887155890464783, 'train/loss': 0.038060471415519714, 'train/mean_average_precision': 0.23831247483512727, 'validation/accuracy': 0.9857940673828125, 'validation/loss': 0.04844588413834572, 'validation/mean_average_precision': 0.19784774196240365, 'validation/num_examples': 43793, 'test/accuracy': 0.9848605394363403, 'test/loss': 0.05130038410425186, 'test/mean_average_precision': 0.1954345330982322, 'test/num_examples': 43793, 'score': 2420.470150232315, 'total_duration': 3423.797711133957, 'accumulated_submission_time': 2420.470150232315, 'accumulated_eval_time': 1002.2793483734131, 'accumulated_logging_time': 0.9504354000091553}
I0425 19:17:10.515623 139484607760128 logging_writer.py:48] [10214] accumulated_eval_time=1002.279348, accumulated_logging_time=0.950435, accumulated_submission_time=2420.470150, global_step=10214, preemption_count=0, score=2420.470150, test/accuracy=0.984861, test/loss=0.051300, test/mean_average_precision=0.195435, test/num_examples=43793, total_duration=3423.797711, train/accuracy=0.988716, train/loss=0.038060, train/mean_average_precision=0.238312, validation/accuracy=0.985794, validation/loss=0.048446, validation/mean_average_precision=0.197848, validation/num_examples=43793
I0425 19:17:10.540974 139669207562048 checkpoints.py:356] Saving checkpoint at step: 10214
I0425 19:17:10.604549 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_10214
I0425 19:17:10.604750 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_10214.
I0425 19:17:30.946030 139484616152832 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04031796753406525, loss=0.042204923927783966
I0425 19:17:54.599614 139484599367424 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.05330289527773857, loss=0.040818314999341965
I0425 19:18:18.266437 139484616152832 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.04362969473004341, loss=0.03958874195814133
I0425 19:18:41.728546 139484599367424 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.08064346015453339, loss=0.03983268141746521
I0425 19:19:05.542946 139484616152832 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.08248168975114822, loss=0.04426363855600357
I0425 19:19:29.214716 139484599367424 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05840079486370087, loss=0.04078337177634239
I0425 19:19:52.908830 139484616152832 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.06224403157830238, loss=0.04530079662799835
I0425 19:20:16.701722 139484599367424 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.05364794284105301, loss=0.04325638338923454
I0425 19:20:40.434563 139484616152832 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.04176781326532364, loss=0.04019434005022049
I0425 19:21:04.612272 139484599367424 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.048349346965551376, loss=0.04356881603598595
I0425 19:21:10.755316 139669207562048 spec.py:298] Evaluating on the training split.
I0425 19:22:25.266582 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 19:22:27.825729 139669207562048 spec.py:326] Evaluating on the test split.
I0425 19:22:30.309444 139669207562048 submission_runner.py:406] Time since start: 3743.60s, 	Step: 11227, 	{'train/accuracy': 0.9889701008796692, 'train/loss': 0.03712783008813858, 'train/mean_average_precision': 0.2705640913539784, 'validation/accuracy': 0.9859588742256165, 'validation/loss': 0.04811522737145424, 'validation/mean_average_precision': 0.20960748334461418, 'validation/num_examples': 43793, 'test/accuracy': 0.9849713444709778, 'test/loss': 0.05104608088731766, 'test/mean_average_precision': 0.2100202759728666, 'test/num_examples': 43793, 'score': 2660.610537290573, 'total_duration': 3743.599779367447, 'accumulated_submission_time': 2660.610537290573, 'accumulated_eval_time': 1081.8334414958954, 'accumulated_logging_time': 1.0479645729064941}
I0425 19:22:30.317802 139484616152832 logging_writer.py:48] [11227] accumulated_eval_time=1081.833441, accumulated_logging_time=1.047965, accumulated_submission_time=2660.610537, global_step=11227, preemption_count=0, score=2660.610537, test/accuracy=0.984971, test/loss=0.051046, test/mean_average_precision=0.210020, test/num_examples=43793, total_duration=3743.599779, train/accuracy=0.988970, train/loss=0.037128, train/mean_average_precision=0.270564, validation/accuracy=0.985959, validation/loss=0.048115, validation/mean_average_precision=0.209607, validation/num_examples=43793
I0425 19:22:30.342824 139669207562048 checkpoints.py:356] Saving checkpoint at step: 11227
I0425 19:22:30.407192 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_11227
I0425 19:22:30.407409 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_11227.
I0425 19:22:47.722533 139484599367424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.06423266232013702, loss=0.04005170240998268
I0425 19:23:11.436299 139484590974720 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.11968568712472916, loss=0.04418012127280235
I0425 19:23:34.837010 139484599367424 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.06249721720814705, loss=0.03868759050965309
I0425 19:23:58.243315 139484590974720 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.06693075597286224, loss=0.03948507085442543
I0425 19:24:21.967513 139484599367424 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.047510016709566116, loss=0.03962408006191254
I0425 19:24:45.527332 139484590974720 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0632370114326477, loss=0.0395929329097271
I0425 19:25:08.883442 139484599367424 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.08533395826816559, loss=0.040680866688489914
I0425 19:25:32.408172 139669207562048 spec.py:298] Evaluating on the training split.
I0425 19:26:48.227257 139669207562048 spec.py:310] Evaluating on the validation split.
I0425 19:26:50.843727 139669207562048 spec.py:326] Evaluating on the test split.
I0425 19:26:53.363488 139669207562048 submission_runner.py:406] Time since start: 4006.65s, 	Step: 12000, 	{'train/accuracy': 0.988895833492279, 'train/loss': 0.037539415061473846, 'train/mean_average_precision': 0.26223615024367314, 'validation/accuracy': 0.9858314394950867, 'validation/loss': 0.04887103661894798, 'validation/mean_average_precision': 0.21387955491350594, 'validation/num_examples': 43793, 'test/accuracy': 0.9848698377609253, 'test/loss': 0.05185892432928085, 'test/mean_average_precision': 0.21295838674321968, 'test/num_examples': 43793, 'score': 2842.6037259101868, 'total_duration': 4006.6538207530975, 'accumulated_submission_time': 2842.6037259101868, 'accumulated_eval_time': 1162.7887210845947, 'accumulated_logging_time': 1.146127462387085}
I0425 19:26:53.371943 139484590974720 logging_writer.py:48] [12000] accumulated_eval_time=1162.788721, accumulated_logging_time=1.146127, accumulated_submission_time=2842.603726, global_step=12000, preemption_count=0, score=2842.603726, test/accuracy=0.984870, test/loss=0.051859, test/mean_average_precision=0.212958, test/num_examples=43793, total_duration=4006.653821, train/accuracy=0.988896, train/loss=0.037539, train/mean_average_precision=0.262236, validation/accuracy=0.985831, validation/loss=0.048871, validation/mean_average_precision=0.213880, validation/num_examples=43793
I0425 19:26:53.398057 139669207562048 checkpoints.py:356] Saving checkpoint at step: 12000
I0425 19:26:53.483183 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000
I0425 19:26:53.483398 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000.
I0425 19:26:53.490922 139484599367424 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2842.603726
I0425 19:26:53.509213 139669207562048 checkpoints.py:356] Saving checkpoint at step: 12000
I0425 19:26:53.624300 139669207562048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000
I0425 19:26:53.624515 139669207562048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000.
I0425 19:26:53.771686 139669207562048 submission_runner.py:567] Tuning trial 1/1
I0425 19:26:53.771945 139669207562048 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0425 19:26:53.773156 139669207562048 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5279272794723511, 'train/loss': 0.7360531687736511, 'train/mean_average_precision': 0.023131126022461864, 'validation/accuracy': 0.5234463214874268, 'validation/loss': 0.7393096685409546, 'validation/mean_average_precision': 0.027053965761648615, 'validation/num_examples': 43793, 'test/accuracy': 0.5214398503303528, 'test/loss': 0.7397536039352417, 'test/mean_average_precision': 0.029496470239449072, 'test/num_examples': 43793, 'score': 19.494435787200928, 'total_duration': 236.91104650497437, 'accumulated_submission_time': 19.494435787200928, 'accumulated_eval_time': 217.41644763946533, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1009, {'train/accuracy': 0.9867992997169495, 'train/loss': 0.05421062558889389, 'train/mean_average_precision': 0.03219433064086116, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06417354196310043, 'validation/mean_average_precision': 0.035083488987328555, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06740381568670273, 'test/mean_average_precision': 0.037477925777762724, 'test/num_examples': 43793, 'score': 259.5566499233246, 'total_duration': 556.7381258010864, 'accumulated_submission_time': 259.5566499233246, 'accumulated_eval_time': 297.05453658103943, 'accumulated_logging_time': 0.11717796325683594, 'global_step': 1009, 'preemption_count': 0}), (2008, {'train/accuracy': 0.9866699576377869, 'train/loss': 0.05180283635854721, 'train/mean_average_precision': 0.05700483191804362, 'validation/accuracy': 0.984120786190033, 'validation/loss': 0.0610467791557312, 'validation/mean_average_precision': 0.05281911502317528, 'validation/num_examples': 43793, 'test/accuracy': 0.9831475615501404, 'test/loss': 0.06433193385601044, 'test/mean_average_precision': 0.054804562881464736, 'test/num_examples': 43793, 'score': 499.7782917022705, 'total_duration': 876.6202344894409, 'accumulated_submission_time': 499.7782917022705, 'accumulated_eval_time': 376.60449481010437, 'accumulated_logging_time': 0.2180490493774414, 'global_step': 2008, 'preemption_count': 0}), (3037, {'train/accuracy': 0.9870218634605408, 'train/loss': 0.04909785836935043, 'train/mean_average_precision': 0.07216758295337047, 'validation/accuracy': 0.984221875667572, 'validation/loss': 0.05915983393788338, 'validation/mean_average_precision': 0.06886333177490309, 'validation/num_examples': 43793, 'test/accuracy': 0.9832444190979004, 'test/loss': 0.06231933832168579, 'test/mean_average_precision': 0.07418519662775398, 'test/num_examples': 43793, 'score': 739.7952632904053, 'total_duration': 1195.6456789970398, 'accumulated_submission_time': 739.7952632904053, 'accumulated_eval_time': 455.5087134838104, 'accumulated_logging_time': 0.3128964900970459, 'global_step': 3037, 'preemption_count': 0}), (4058, {'train/accuracy': 0.9869559407234192, 'train/loss': 0.04711682349443436, 'train/mean_average_precision': 0.10602670245384556, 'validation/accuracy': 0.9842920899391174, 'validation/loss': 0.056203443557024, 'validation/mean_average_precision': 0.10379701264381098, 'validation/num_examples': 43793, 'test/accuracy': 0.9833299517631531, 'test/loss': 0.059151362627744675, 'test/mean_average_precision': 0.10541523708252658, 'test/num_examples': 43793, 'score': 979.7958779335022, 'total_duration': 1514.2369213104248, 'accumulated_submission_time': 979.7958779335022, 'accumulated_eval_time': 533.994964838028, 'accumulated_logging_time': 0.407482385635376, 'global_step': 4058, 'preemption_count': 0}), (5089, {'train/accuracy': 0.9877461194992065, 'train/loss': 0.04450018331408501, 'train/mean_average_precision': 0.1381259663002985, 'validation/accuracy': 0.9849395751953125, 'validation/loss': 0.05337567999958992, 'validation/mean_average_precision': 0.13454919921041425, 'validation/num_examples': 43793, 'test/accuracy': 0.9839360117912292, 'test/loss': 0.05604574829339981, 'test/mean_average_precision': 0.13671668157864103, 'test/num_examples': 43793, 'score': 1219.9246366024017, 'total_duration': 1831.606472492218, 'accumulated_submission_time': 1219.9246366024017, 'accumulated_eval_time': 611.1330552101135, 'accumulated_logging_time': 0.500286340713501, 'global_step': 5089, 'preemption_count': 0}), (6118, {'train/accuracy': 0.9877986311912537, 'train/loss': 0.042331814765930176, 'train/mean_average_precision': 0.16776723380815883, 'validation/accuracy': 0.9849996566772461, 'validation/loss': 0.0521932877600193, 'validation/mean_average_precision': 0.15222363673797923, 'validation/num_examples': 43793, 'test/accuracy': 0.9840030074119568, 'test/loss': 0.05512649565935135, 'test/mean_average_precision': 0.15319964526566254, 'test/num_examples': 43793, 'score': 1459.9251809120178, 'total_duration': 2149.847906112671, 'accumulated_submission_time': 1459.9251809120178, 'accumulated_eval_time': 689.276468038559, 'accumulated_logging_time': 0.5878713130950928, 'global_step': 6118, 'preemption_count': 0}), (7138, {'train/accuracy': 0.9880496263504028, 'train/loss': 0.04092901945114136, 'train/mean_average_precision': 0.19425539531692929, 'validation/accuracy': 0.9852330684661865, 'validation/loss': 0.05106215924024582, 'validation/mean_average_precision': 0.17055108339959457, 'validation/num_examples': 43793, 'test/accuracy': 0.9842792749404907, 'test/loss': 0.05395810306072235, 'test/mean_average_precision': 0.17302908715818577, 'test/num_examples': 43793, 'score': 1700.0029392242432, 'total_duration': 2467.7069346904755, 'accumulated_submission_time': 1700.0029392242432, 'accumulated_eval_time': 766.9542598724365, 'accumulated_logging_time': 0.6814978122711182, 'global_step': 7138, 'preemption_count': 0}), (8166, {'train/accuracy': 0.9885864853858948, 'train/loss': 0.0390632227063179, 'train/mean_average_precision': 0.21235523907036657, 'validation/accuracy': 0.9855748414993286, 'validation/loss': 0.04941273853182793, 'validation/mean_average_precision': 0.18313720836907552, 'validation/num_examples': 43793, 'test/accuracy': 0.9845869541168213, 'test/loss': 0.052225079387426376, 'test/mean_average_precision': 0.18144379260034715, 'test/num_examples': 43793, 'score': 1940.089873790741, 'total_duration': 2786.0887620449066, 'accumulated_submission_time': 1940.089873790741, 'accumulated_eval_time': 845.1514577865601, 'accumulated_logging_time': 0.7693459987640381, 'global_step': 8166, 'preemption_count': 0}), (9194, {'train/accuracy': 0.9889345169067383, 'train/loss': 0.03836584836244583, 'train/mean_average_precision': 0.22585402485879835, 'validation/accuracy': 0.985910177230835, 'validation/loss': 0.04829385504126549, 'validation/mean_average_precision': 0.18759135553218748, 'validation/num_examples': 43793, 'test/accuracy': 0.9848656058311462, 'test/loss': 0.05099392309784889, 'test/mean_average_precision': 0.1899480665488388, 'test/num_examples': 43793, 'score': 2180.288151025772, 'total_duration': 3104.768837928772, 'accumulated_submission_time': 2180.288151025772, 'accumulated_eval_time': 923.5342991352081, 'accumulated_logging_time': 0.8584833145141602, 'global_step': 9194, 'preemption_count': 0}), (10214, {'train/accuracy': 0.9887155890464783, 'train/loss': 0.038060471415519714, 'train/mean_average_precision': 0.23831247483512727, 'validation/accuracy': 0.9857940673828125, 'validation/loss': 0.04844588413834572, 'validation/mean_average_precision': 0.19784774196240365, 'validation/num_examples': 43793, 'test/accuracy': 0.9848605394363403, 'test/loss': 0.05130038410425186, 'test/mean_average_precision': 0.1954345330982322, 'test/num_examples': 43793, 'score': 2420.470150232315, 'total_duration': 3423.797711133957, 'accumulated_submission_time': 2420.470150232315, 'accumulated_eval_time': 1002.2793483734131, 'accumulated_logging_time': 0.9504354000091553, 'global_step': 10214, 'preemption_count': 0}), (11227, {'train/accuracy': 0.9889701008796692, 'train/loss': 0.03712783008813858, 'train/mean_average_precision': 0.2705640913539784, 'validation/accuracy': 0.9859588742256165, 'validation/loss': 0.04811522737145424, 'validation/mean_average_precision': 0.20960748334461418, 'validation/num_examples': 43793, 'test/accuracy': 0.9849713444709778, 'test/loss': 0.05104608088731766, 'test/mean_average_precision': 0.2100202759728666, 'test/num_examples': 43793, 'score': 2660.610537290573, 'total_duration': 3743.599779367447, 'accumulated_submission_time': 2660.610537290573, 'accumulated_eval_time': 1081.8334414958954, 'accumulated_logging_time': 1.0479645729064941, 'global_step': 11227, 'preemption_count': 0}), (12000, {'train/accuracy': 0.988895833492279, 'train/loss': 0.037539415061473846, 'train/mean_average_precision': 0.26223615024367314, 'validation/accuracy': 0.9858314394950867, 'validation/loss': 0.04887103661894798, 'validation/mean_average_precision': 0.21387955491350594, 'validation/num_examples': 43793, 'test/accuracy': 0.9848698377609253, 'test/loss': 0.05185892432928085, 'test/mean_average_precision': 0.21295838674321968, 'test/num_examples': 43793, 'score': 2842.6037259101868, 'total_duration': 4006.6538207530975, 'accumulated_submission_time': 2842.6037259101868, 'accumulated_eval_time': 1162.7887210845947, 'accumulated_logging_time': 1.146127462387085, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0425 19:26:53.773282 139669207562048 submission_runner.py:570] Timing: 2842.6037259101868
I0425 19:26:53.773325 139669207562048 submission_runner.py:571] ====================
I0425 19:26:53.773435 139669207562048 submission_runner.py:631] Final ogbg score: 2842.6037259101868
