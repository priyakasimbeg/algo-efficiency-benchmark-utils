I0420 17:31:31.573857 140018299942720 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax.
I0420 17:31:31.647961 140018299942720 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 17:31:32.501653 140018299942720 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0420 17:31:32.502613 140018299942720 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 17:31:32.506610 140018299942720 submission_runner.py:528] Using RNG seed 434423982
I0420 17:31:35.089211 140018299942720 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 17:31:35.089401 140018299942720 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1.
I0420 17:31:35.089728 140018299942720 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/hparams.json.
I0420 17:31:35.217139 140018299942720 submission_runner.py:232] Initializing dataset.
I0420 17:31:35.217309 140018299942720 submission_runner.py:239] Initializing model.
I0420 17:31:40.864702 140018299942720 submission_runner.py:249] Initializing optimizer.
I0420 17:31:41.662881 140018299942720 submission_runner.py:256] Initializing metrics bundle.
I0420 17:31:41.663068 140018299942720 submission_runner.py:273] Initializing checkpoint and logger.
I0420 17:31:41.664231 140018299942720 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0420 17:31:41.664483 140018299942720 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 17:31:41.664547 140018299942720 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 17:31:42.443961 140018299942720 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0420 17:31:42.444852 140018299942720 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0420 17:31:42.451192 140018299942720 submission_runner.py:309] Starting training loop.
I0420 17:31:42.644760 140018299942720 input_pipeline.py:20] Loading split = train-clean-100
I0420 17:31:42.675477 140018299942720 input_pipeline.py:20] Loading split = train-clean-360
I0420 17:31:42.992762 140018299942720 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0420 17:32:46.559128 139842339985152 logging_writer.py:48] [0] global_step=0, grad_norm=74.21739196777344, loss=31.32953643798828
I0420 17:32:46.585877 140018299942720 spec.py:298] Evaluating on the training split.
I0420 17:32:46.685696 140018299942720 input_pipeline.py:20] Loading split = train-clean-100
I0420 17:32:46.891953 140018299942720 input_pipeline.py:20] Loading split = train-clean-360
I0420 17:32:46.987671 140018299942720 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0420 17:33:49.127061 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 17:33:49.189002 140018299942720 input_pipeline.py:20] Loading split = dev-clean
I0420 17:33:49.194751 140018299942720 input_pipeline.py:20] Loading split = dev-other
I0420 17:34:32.264822 140018299942720 spec.py:326] Evaluating on the test split.
I0420 17:34:32.326094 140018299942720 input_pipeline.py:20] Loading split = test-clean
I0420 17:35:02.714224 140018299942720 submission_runner.py:406] Time since start: 200.26s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.272085, dtype=float32), 'train/wer': 1.9281073652208316, 'validation/ctc_loss': DeviceArray(29.980951, dtype=float32), 'validation/wer': 1.5048480930833872, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.09849, dtype=float32), 'test/wer': 1.5858062681534744, 'test/num_examples': 2472, 'score': 64.13452005386353, 'total_duration': 200.261545419693, 'accumulated_submission_time': 64.13452005386353, 'accumulated_eval_time': 136.12687611579895, 'accumulated_logging_time': 0}
I0420 17:35:02.734856 139838330238720 logging_writer.py:48] [1] accumulated_eval_time=136.126876, accumulated_logging_time=0, accumulated_submission_time=64.134520, global_step=1, preemption_count=0, score=64.134520, test/ctc_loss=30.09848976135254, test/num_examples=2472, test/wer=1.585806, total_duration=200.261545, train/ctc_loss=31.272085189819336, train/wer=1.928107, validation/ctc_loss=29.9809513092041, validation/num_examples=5348, validation/wer=1.504848
I0420 17:35:03.020534 140018299942720 checkpoints.py:356] Saving checkpoint at step: 1
I0420 17:35:03.890388 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_1
I0420 17:35:03.891903 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_1.
I0420 17:36:36.400933 139843639265024 logging_writer.py:48] [100] global_step=100, grad_norm=5.008289813995361, loss=6.794777870178223
I0420 17:37:52.086640 139843647657728 logging_writer.py:48] [200] global_step=200, grad_norm=0.6040683388710022, loss=5.931142807006836
I0420 17:39:07.867040 139843639265024 logging_writer.py:48] [300] global_step=300, grad_norm=0.48529037833213806, loss=5.8575119972229
I0420 17:40:23.629955 139843647657728 logging_writer.py:48] [400] global_step=400, grad_norm=0.4350467026233673, loss=5.814222812652588
I0420 17:41:39.423759 139843639265024 logging_writer.py:48] [500] global_step=500, grad_norm=0.5567241907119751, loss=5.819005489349365
I0420 17:42:55.210581 139843647657728 logging_writer.py:48] [600] global_step=600, grad_norm=0.6434721946716309, loss=5.80049991607666
I0420 17:44:11.050485 139843639265024 logging_writer.py:48] [700] global_step=700, grad_norm=0.3432975113391876, loss=5.816415786743164
I0420 17:45:26.919147 139843647657728 logging_writer.py:48] [800] global_step=800, grad_norm=0.424277663230896, loss=5.802689552307129
I0420 17:46:42.757958 139843639265024 logging_writer.py:48] [900] global_step=900, grad_norm=2.801054000854492, loss=5.808446884155273
I0420 17:47:58.717076 139843647657728 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.075851321220398, loss=5.7729387283325195
I0420 17:49:17.466309 139844437300992 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8054272532463074, loss=5.78391695022583
I0420 17:50:33.249377 139844428908288 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.3343446254730225, loss=5.758399486541748
I0420 17:51:49.041774 139844437300992 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7162870764732361, loss=5.605977535247803
I0420 17:53:04.842455 139844428908288 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9019510746002197, loss=5.518764495849609
I0420 17:54:20.658241 139844437300992 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8228314518928528, loss=5.513779163360596
I0420 17:55:36.429590 139844428908288 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4217676520347595, loss=5.523019790649414
I0420 17:56:52.129951 139844437300992 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.51136714220047, loss=5.4889936447143555
I0420 17:58:07.872073 139844428908288 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.4623743295669556, loss=5.527220726013184
I0420 17:59:23.141601 139844437300992 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.2615307569503784, loss=5.529788970947266
I0420 18:00:38.253276 139844428908288 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.5310417413711548, loss=5.622654914855957
I0420 18:01:56.284207 139845092660992 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.755273699760437, loss=5.543212413787842
I0420 18:03:11.134270 139845084268288 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8481752276420593, loss=5.549167633056641
I0420 18:04:26.097636 139845092660992 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.4944159984588623, loss=5.548196315765381
I0420 18:05:40.965395 139845084268288 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.375956416130066, loss=5.5127153396606445
I0420 18:06:55.856564 139845092660992 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4794279634952545, loss=5.532398700714111
I0420 18:08:10.722507 139845084268288 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.1903049945831299, loss=5.532769680023193
I0420 18:09:25.767554 139845092660992 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.21168053150177, loss=5.539215087890625
I0420 18:10:40.650744 139845084268288 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.840972900390625, loss=5.514101505279541
I0420 18:11:55.531860 139845092660992 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.752951145172119, loss=5.538902759552002
I0420 18:13:10.401415 139845084268288 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1483533382415771, loss=5.517415523529053
I0420 18:14:28.368410 139844437300992 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0184965133666992, loss=5.528661727905273
I0420 18:15:03.966569 140018299942720 spec.py:298] Evaluating on the training split.
I0420 18:15:30.661090 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 18:16:04.521476 140018299942720 spec.py:326] Evaluating on the test split.
I0420 18:16:21.641695 140018299942720 submission_runner.py:406] Time since start: 2679.19s, 	Step: 3149, 	{'train/ctc_loss': DeviceArray(5.6850686, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.71021, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6882114, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2464.168543577194, 'total_duration': 2679.187164545059, 'accumulated_submission_time': 2464.168543577194, 'accumulated_eval_time': 213.79871273040771, 'accumulated_logging_time': 1.1801538467407227}
I0420 18:16:21.661309 139845092660992 logging_writer.py:48] [3149] accumulated_eval_time=213.798713, accumulated_logging_time=1.180154, accumulated_submission_time=2464.168544, global_step=3149, preemption_count=0, score=2464.168544, test/ctc_loss=5.688211441040039, test/num_examples=2472, test/wer=0.899580, total_duration=2679.187165, train/ctc_loss=5.685068607330322, train/wer=0.944636, validation/ctc_loss=5.710209846496582, validation/num_examples=5348, validation/wer=0.895995
I0420 18:16:21.964351 140018299942720 checkpoints.py:356] Saving checkpoint at step: 3149
I0420 18:16:23.373983 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_3149
I0420 18:16:23.404626 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_3149.
I0420 18:17:02.360133 139845084268288 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.40171480178833, loss=5.52915096282959
I0420 18:18:17.335982 139845042304768 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.004487991333008, loss=5.51964807510376
I0420 18:19:32.124161 139845084268288 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7344338297843933, loss=5.523482322692871
I0420 18:20:46.884844 139845042304768 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2461963891983032, loss=5.525049686431885
I0420 18:22:01.693091 139845084268288 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.326414108276367, loss=5.523656368255615
I0420 18:23:16.613870 139845042304768 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1589975357055664, loss=5.457572937011719
I0420 18:24:31.449180 139845084268288 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.6084444522857666, loss=5.500503063201904
I0420 18:25:46.245832 139845042304768 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.2522484064102173, loss=5.538724422454834
I0420 18:27:01.281411 139845084268288 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.872857093811035, loss=5.473624229431152
I0420 18:28:16.158136 139845042304768 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.128698468208313, loss=5.511600971221924
I0420 18:29:34.122180 139845092660992 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.5683485269546509, loss=5.516018390655518
I0420 18:30:48.856626 139845084268288 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7977652549743652, loss=5.536675453186035
I0420 18:32:03.714687 139845092660992 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.708304226398468, loss=5.498393535614014
I0420 18:33:18.480415 139845084268288 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0825493335723877, loss=5.504349231719971
I0420 18:34:33.284183 139845092660992 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8188058137893677, loss=5.525835990905762
I0420 18:35:48.149912 139845084268288 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.3362147808074951, loss=5.50285005569458
I0420 18:37:02.908170 139845092660992 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5470072627067566, loss=5.491237640380859
I0420 18:38:17.756556 139845084268288 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.4170482158660889, loss=5.539309978485107
I0420 18:39:32.471192 139845092660992 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.125632643699646, loss=5.522615432739258
I0420 18:40:47.175141 139845084268288 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.1759732961654663, loss=5.510884761810303
I0420 18:42:05.130784 139844437300992 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.915460109710693, loss=5.519659996032715
I0420 18:43:19.914946 139844428908288 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.109782099723816, loss=5.580517292022705
I0420 18:44:34.900788 139844437300992 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7785961031913757, loss=5.530047416687012
I0420 18:45:49.629879 139844428908288 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.97646564245224, loss=5.544416904449463
I0420 18:47:04.412543 139844437300992 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7690565586090088, loss=5.547532558441162
I0420 18:48:19.225944 139844428908288 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6149149537086487, loss=5.543616771697998
I0420 18:49:34.096090 139844437300992 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.36940357089042664, loss=5.506680488586426
I0420 18:50:48.942915 139844428908288 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.2735188901424408, loss=5.520000457763672
I0420 18:52:03.768493 139844437300992 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4424351453781128, loss=5.5192155838012695
I0420 18:53:18.606000 139844428908288 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.86131751537323, loss=5.508890151977539
I0420 18:54:36.680462 139845092660992 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7740288376808167, loss=5.509864330291748
I0420 18:55:51.494340 139845084268288 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4905037581920624, loss=5.484311103820801
I0420 18:56:23.997006 140018299942720 spec.py:298] Evaluating on the training split.
I0420 18:56:50.414178 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 18:57:24.694214 140018299942720 spec.py:326] Evaluating on the test split.
I0420 18:57:42.008668 140018299942720 submission_runner.py:406] Time since start: 5159.55s, 	Step: 6345, 	{'train/ctc_loss': DeviceArray(6.208722, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(6.125897, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.069559, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4864.717026233673, 'total_duration': 5159.554322957993, 'accumulated_submission_time': 4864.717026233673, 'accumulated_eval_time': 291.80724334716797, 'accumulated_logging_time': 2.9491517543792725}
I0420 18:57:42.028468 139845092660992 logging_writer.py:48] [6345] accumulated_eval_time=291.807243, accumulated_logging_time=2.949152, accumulated_submission_time=4864.717026, global_step=6345, preemption_count=0, score=4864.717026, test/ctc_loss=6.069559097290039, test/num_examples=2472, test/wer=0.899580, total_duration=5159.554323, train/ctc_loss=6.208722114562988, train/wer=0.942722, validation/ctc_loss=6.12589693069458, validation/num_examples=5348, validation/wer=0.895995
I0420 18:57:42.317976 140018299942720 checkpoints.py:356] Saving checkpoint at step: 6345
I0420 18:57:43.731356 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_6345
I0420 18:57:43.764027 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_6345.
I0420 18:58:25.560063 139845084268288 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5350508689880371, loss=5.487893104553223
I0420 18:59:40.215152 139843139503872 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4061567485332489, loss=5.4927449226379395
I0420 19:00:54.968083 139845084268288 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.465188592672348, loss=5.493770122528076
I0420 19:02:09.827623 139843139503872 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6101462244987488, loss=5.51092529296875
I0420 19:03:24.570895 139845084268288 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7476477026939392, loss=5.510931015014648
I0420 19:04:39.321083 139843139503872 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.5831478834152222, loss=5.527045726776123
I0420 19:05:54.115783 139845084268288 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.558114767074585, loss=5.482558250427246
I0420 19:07:09.036509 139843139503872 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.503351628780365, loss=5.502899646759033
I0420 19:08:23.821280 139845084268288 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4707031548023224, loss=5.489396095275879
I0420 19:09:41.910861 139845092660992 logging_writer.py:48] [7300] global_step=7300, grad_norm=7.311936378479004, loss=5.6783576011657715
I0420 19:10:56.682628 139845084268288 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.33400779962539673, loss=5.49746561050415
I0420 19:12:11.381984 139845092660992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.22844178974628448, loss=5.492080211639404
I0420 19:13:26.071724 139845084268288 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.25036948919296265, loss=5.484242916107178
I0420 19:14:40.804498 139845092660992 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.37844371795654297, loss=5.498055934906006
I0420 19:15:55.688012 139845084268288 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.22006379067897797, loss=5.449488162994385
I0420 19:17:10.475897 139845092660992 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.2309064269065857, loss=5.453354358673096
I0420 19:18:25.228984 139845084268288 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.464586079120636, loss=5.475561141967773
I0420 19:19:40.143768 139845092660992 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6068645119667053, loss=5.45546293258667
I0420 19:20:54.863706 139845084268288 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7381845712661743, loss=5.458291053771973
I0420 19:22:12.835244 139844437300992 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.26075056195259094, loss=5.440914630889893
I0420 19:23:27.701508 139844428908288 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.2865488827228546, loss=5.467365264892578
I0420 19:24:42.453845 139844437300992 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4431275725364685, loss=5.441690444946289
I0420 19:25:57.265600 139844428908288 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3258347809314728, loss=5.422377109527588
I0420 19:27:12.135774 139844437300992 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3733041286468506, loss=5.42422342300415
I0420 19:28:27.038232 139844428908288 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6201928853988647, loss=5.412538051605225
I0420 19:29:41.877221 139844437300992 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.25195804238319397, loss=5.4084086418151855
I0420 19:30:56.820732 139844428908288 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6937921047210693, loss=5.394392967224121
I0420 19:32:11.738514 139844437300992 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4481180012226105, loss=5.317294120788574
I0420 19:33:26.596662 139844428908288 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3097309172153473, loss=5.288675308227539
I0420 19:34:44.603983 139845092660992 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.35117533802986145, loss=5.18787956237793
I0420 19:35:59.499010 139845084268288 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.39520081877708435, loss=4.959498882293701
I0420 19:37:14.373476 139845092660992 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6343424916267395, loss=4.6795549392700195
I0420 19:37:43.943145 140018299942720 spec.py:298] Evaluating on the training split.
I0420 19:38:11.559508 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 19:38:46.271240 140018299942720 spec.py:326] Evaluating on the test split.
I0420 19:39:04.218489 140018299942720 submission_runner.py:406] Time since start: 7641.76s, 	Step: 9541, 	{'train/ctc_loss': DeviceArray(5.813272, dtype=float32), 'train/wer': 0.9337360511514883, 'validation/ctc_loss': DeviceArray(5.85413, dtype=float32), 'validation/wer': 0.8918079286823799, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.738961, dtype=float32), 'test/wer': 0.893871996425162, 'test/num_examples': 2472, 'score': 7264.85174202919, 'total_duration': 7641.763874769211, 'accumulated_submission_time': 7264.85174202919, 'accumulated_eval_time': 372.07923555374146, 'accumulated_logging_time': 4.711084604263306}
I0420 19:39:04.238919 139845164340992 logging_writer.py:48] [9541] accumulated_eval_time=372.079236, accumulated_logging_time=4.711085, accumulated_submission_time=7264.851742, global_step=9541, preemption_count=0, score=7264.851742, test/ctc_loss=5.738961219787598, test/num_examples=2472, test/wer=0.893872, total_duration=7641.763875, train/ctc_loss=5.813271999359131, train/wer=0.933736, validation/ctc_loss=5.854129791259766, validation/num_examples=5348, validation/wer=0.891808
I0420 19:39:04.526882 140018299942720 checkpoints.py:356] Saving checkpoint at step: 9541
I0420 19:39:05.937579 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_9541
I0420 19:39:05.969816 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_9541.
I0420 19:39:50.817279 139845155948288 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.41980865597724915, loss=4.511422634124756
I0420 19:41:05.641852 139845097199360 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1535683870315552, loss=4.27980899810791
I0420 19:42:20.557714 139845155948288 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6805298924446106, loss=4.097888946533203
I0420 19:43:35.417185 139845097199360 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8617636561393738, loss=3.9607303142547607
I0420 19:44:50.265139 139845155948288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5657646656036377, loss=3.8238306045532227
I0420 19:46:05.047215 139845097199360 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.48398709297180176, loss=3.78185772895813
I0420 19:47:19.873543 139845155948288 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5608537197113037, loss=3.6956214904785156
I0420 19:48:37.970230 139845164340992 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.4731888771057129, loss=3.594789505004883
I0420 19:49:52.848721 139845155948288 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5886513590812683, loss=3.4139750003814697
I0420 19:51:07.751630 139845164340992 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5983792543411255, loss=3.449881076812744
I0420 19:52:22.630651 139845155948288 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7125106453895569, loss=3.416278839111328
I0420 19:53:37.539135 139845164340992 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6105312705039978, loss=3.349170207977295
I0420 19:54:52.381814 139845155948288 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6020160913467407, loss=3.218217611312866
I0420 19:56:07.250835 139845164340992 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6724086999893188, loss=3.1896705627441406
I0420 19:57:22.100922 139845155948288 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7624850869178772, loss=3.1465935707092285
I0420 19:58:36.950808 139845164340992 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5640714168548584, loss=3.1286492347717285
I0420 19:59:51.793576 139845155948288 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7331315875053406, loss=3.037992238998413
I0420 20:01:06.647393 139845164340992 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6038353443145752, loss=2.990856409072876
I0420 20:02:24.678848 139845164340992 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6106465458869934, loss=3.000089168548584
I0420 20:03:39.568365 139845155948288 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.798572301864624, loss=2.911231279373169
I0420 20:04:54.492160 139845164340992 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5456684827804565, loss=2.8937690258026123
I0420 20:06:09.386964 139845155948288 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5305047035217285, loss=2.8552913665771484
I0420 20:07:24.236246 139845164340992 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7002708911895752, loss=2.9120073318481445
I0420 20:08:39.091882 139845155948288 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7487159371376038, loss=2.921109914779663
I0420 20:09:54.043655 139845164340992 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5669274926185608, loss=2.8533740043640137
I0420 20:11:08.967386 139845155948288 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6539108157157898, loss=2.8240103721618652
I0420 20:12:23.836822 139845164340992 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7371200919151306, loss=2.759594202041626
I0420 20:13:38.723618 139845155948288 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6983219385147095, loss=2.7121775150299072
I0420 20:14:56.791242 139845164340992 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7195175290107727, loss=2.6900882720947266
I0420 20:16:11.677066 139845155948288 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5943383574485779, loss=2.6577749252319336
I0420 20:17:26.632002 139845164340992 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5364794731140137, loss=2.6273632049560547
I0420 20:18:41.475689 139845155948288 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.528207004070282, loss=2.579800605773926
I0420 20:19:06.587177 140018299942720 spec.py:298] Evaluating on the training split.
I0420 20:19:44.170260 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 20:20:23.002006 140018299942720 spec.py:326] Evaluating on the test split.
I0420 20:20:42.448261 140018299942720 submission_runner.py:406] Time since start: 10139.99s, 	Step: 12735, 	{'train/ctc_loss': DeviceArray(1.3361945, dtype=float32), 'train/wer': 0.4144142257223345, 'validation/ctc_loss': DeviceArray(1.7047826, dtype=float32), 'validation/wer': 0.461770012252892, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3732243, dtype=float32), 'test/wer': 0.40964393800905896, 'test/num_examples': 2472, 'score': 9665.425416231155, 'total_duration': 10139.993323802948, 'accumulated_submission_time': 9665.425416231155, 'accumulated_eval_time': 467.93659710884094, 'accumulated_logging_time': 6.468662261962891}
I0420 20:20:42.469114 139844293932800 logging_writer.py:48] [12735] accumulated_eval_time=467.936597, accumulated_logging_time=6.468662, accumulated_submission_time=9665.425416, global_step=12735, preemption_count=0, score=9665.425416, test/ctc_loss=1.3732242584228516, test/num_examples=2472, test/wer=0.409644, total_duration=10139.993324, train/ctc_loss=1.3361945152282715, train/wer=0.414414, validation/ctc_loss=1.7047826051712036, validation/num_examples=5348, validation/wer=0.461770
I0420 20:20:42.760184 140018299942720 checkpoints.py:356] Saving checkpoint at step: 12735
I0420 20:20:44.175624 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_12735
I0420 20:20:44.208201 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_12735.
I0420 20:21:33.553802 139844285540096 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4881945252418518, loss=2.5769152641296387
I0420 20:22:48.368255 139844218398464 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8049997091293335, loss=2.543048858642578
I0420 20:24:03.146144 139844285540096 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6401615142822266, loss=2.5191385746002197
I0420 20:25:18.035763 139844218398464 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6130828857421875, loss=2.5834298133850098
I0420 20:26:32.937029 139844285540096 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.7358302474021912, loss=2.5383973121643066
I0420 20:27:47.817235 139844218398464 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5609113574028015, loss=2.489259958267212
I0420 20:29:06.143676 139843638572800 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5280537009239197, loss=2.415846109390259
I0420 20:30:21.096992 139843630180096 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.49586063623428345, loss=2.386082649230957
I0420 20:31:35.992626 139843638572800 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5609886646270752, loss=2.390810251235962
I0420 20:32:50.983697 139843630180096 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5213220715522766, loss=2.3769943714141846
I0420 20:34:05.886006 139843638572800 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4958227276802063, loss=2.418574333190918
I0420 20:35:20.746779 139843630180096 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.4631907641887665, loss=2.3942384719848633
I0420 20:36:35.642393 139843638572800 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.46178069710731506, loss=2.302971363067627
I0420 20:37:50.470340 139843630180096 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5930582880973816, loss=2.299428701400757
I0420 20:39:05.335549 139843638572800 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7035426497459412, loss=2.3461599349975586
I0420 20:40:20.387193 139843630180096 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.9021018743515015, loss=2.317412853240967
I0420 20:41:35.304742 139843638572800 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.49259480834007263, loss=2.2638585567474365
I0420 20:42:53.285546 139843638572800 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6931276321411133, loss=2.2588253021240234
I0420 20:44:08.310530 139843630180096 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.779409646987915, loss=2.2336628437042236
I0420 20:45:23.189545 139843638572800 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5590190291404724, loss=2.2253432273864746
I0420 20:46:38.087145 139843630180096 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6693962216377258, loss=2.196718692779541
I0420 20:47:52.965727 139843638572800 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.46238043904304504, loss=2.246208906173706
I0420 20:49:07.900208 139843630180096 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5821659564971924, loss=2.193342447280884
I0420 20:50:22.806327 139843638572800 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6083212494850159, loss=2.171950101852417
I0420 20:51:37.779367 139843630180096 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5281245112419128, loss=2.1493020057678223
I0420 20:52:52.668287 139843638572800 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5249033570289612, loss=2.1353161334991455
I0420 20:54:07.572247 139843630180096 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5888938307762146, loss=2.1515939235687256
I0420 20:55:25.531508 139843310892800 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6163479089736938, loss=2.1549854278564453
I0420 20:56:40.421451 139843302500096 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0340138673782349, loss=2.1390044689178467
I0420 20:57:55.305461 139843310892800 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6434353590011597, loss=2.022836923599243
I0420 20:59:10.136072 139843302500096 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.4700987935066223, loss=2.0721561908721924
I0420 21:00:24.995388 139843310892800 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5322201251983643, loss=2.013293981552124
I0420 21:00:44.890779 140018299942720 spec.py:298] Evaluating on the training split.
I0420 21:01:22.425672 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 21:02:00.457917 140018299942720 spec.py:326] Evaluating on the test split.
I0420 21:02:19.908234 140018299942720 submission_runner.py:406] Time since start: 12637.45s, 	Step: 15928, 	{'train/ctc_loss': DeviceArray(0.7849302, dtype=float32), 'train/wer': 0.2704559786035791, 'validation/ctc_loss': DeviceArray(1.1377124, dtype=float32), 'validation/wer': 0.33588360717421295, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8334967, dtype=float32), 'test/wer': 0.27548595454268476, 'test/num_examples': 2472, 'score': 12066.064540863037, 'total_duration': 12637.453438997269, 'accumulated_submission_time': 12066.064540863037, 'accumulated_eval_time': 562.9505045413971, 'accumulated_logging_time': 8.234748840332031}
I0420 21:02:19.930472 139844734260992 logging_writer.py:48] [15928] accumulated_eval_time=562.950505, accumulated_logging_time=8.234749, accumulated_submission_time=12066.064541, global_step=15928, preemption_count=0, score=12066.064541, test/ctc_loss=0.8334966897964478, test/num_examples=2472, test/wer=0.275486, total_duration=12637.453439, train/ctc_loss=0.7849302291870117, train/wer=0.270456, validation/ctc_loss=1.1377123594284058, validation/num_examples=5348, validation/wer=0.335884
I0420 21:02:20.225335 140018299942720 checkpoints.py:356] Saving checkpoint at step: 15928
I0420 21:02:21.640010 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_15928
I0420 21:02:21.672697 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_15928.
I0420 21:03:16.319616 139844725868288 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6120342016220093, loss=2.044234275817871
I0420 21:04:31.380731 139844650333952 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6251040101051331, loss=2.1047019958496094
I0420 21:05:46.302818 139844725868288 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.367895245552063, loss=2.0894901752471924
I0420 21:07:01.207103 139844650333952 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.47926580905914307, loss=2.0459609031677246
I0420 21:08:16.132142 139844725868288 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5501198172569275, loss=2.041332721710205
I0420 21:09:34.212621 139844734260992 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.504647970199585, loss=1.94061279296875
I0420 21:10:49.179984 139844725868288 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.558115541934967, loss=2.0983810424804688
I0420 21:12:04.154711 139844734260992 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4414617717266083, loss=2.0468013286590576
I0420 21:13:19.088539 139844725868288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5533711910247803, loss=1.9885228872299194
I0420 21:14:34.030716 139844734260992 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5716440677642822, loss=1.9504024982452393
I0420 21:15:48.978865 139844725868288 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.47748613357543945, loss=1.9867732524871826
I0420 21:17:03.866648 139844734260992 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5210039019584656, loss=1.9847949743270874
I0420 21:18:18.726108 139844725868288 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.55438631772995, loss=1.9590163230895996
I0420 21:19:33.702280 139844734260992 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5534921884536743, loss=1.9803619384765625
I0420 21:20:48.701914 139844725868288 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.503863513469696, loss=1.9913126230239868
I0420 21:22:03.646701 139844734260992 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.45952945947647095, loss=1.9442330598831177
I0420 21:23:21.680196 139843423540992 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.4701213836669922, loss=2.009601593017578
I0420 21:24:36.525285 139843415148288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.46137872338294983, loss=1.9190163612365723
I0420 21:25:51.490238 139843423540992 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6165757775306702, loss=1.9680697917938232
I0420 21:27:06.534409 139843415148288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7133763432502747, loss=1.9054336547851562
I0420 21:28:21.469524 139843423540992 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6485480666160583, loss=1.9211822748184204
I0420 21:29:36.405853 139843415148288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4350113570690155, loss=1.877456784248352
I0420 21:30:51.382899 139843423540992 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6167511940002441, loss=1.9640769958496094
I0420 21:32:06.234860 139843415148288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5508067607879639, loss=1.9317522048950195
I0420 21:33:21.106881 139843423540992 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4420733153820038, loss=1.9373469352722168
I0420 21:34:36.037072 139843415148288 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7047114372253418, loss=1.9101859331130981
I0420 21:35:54.060635 139844734260992 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.4967241585254669, loss=1.9064065217971802
I0420 21:37:08.980524 139844725868288 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9564366340637207, loss=1.872018575668335
I0420 21:38:23.856120 139844734260992 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.48120763897895813, loss=1.8578381538391113
I0420 21:39:38.639865 139844725868288 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.46755313873291016, loss=1.882588505744934
I0420 21:40:53.510391 139844734260992 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6151360869407654, loss=1.8654625415802002
I0420 21:42:08.481521 139844725868288 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5757927894592285, loss=1.8318556547164917
I0420 21:42:22.400852 140018299942720 spec.py:298] Evaluating on the training split.
I0420 21:42:59.940669 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 21:43:38.088263 140018299942720 spec.py:326] Evaluating on the test split.
I0420 21:43:57.496590 140018299942720 submission_runner.py:406] Time since start: 15135.04s, 	Step: 19120, 	{'train/ctc_loss': DeviceArray(0.62632924, dtype=float32), 'train/wer': 0.22043627401294205, 'validation/ctc_loss': DeviceArray(0.9311202, dtype=float32), 'validation/wer': 0.28060087410394696, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6446907, dtype=float32), 'test/wer': 0.21981191477261186, 'test/num_examples': 2472, 'score': 14466.749136209488, 'total_duration': 15135.041584968567, 'accumulated_submission_time': 14466.749136209488, 'accumulated_eval_time': 658.0424566268921, 'accumulated_logging_time': 10.00546145439148}
I0420 21:43:57.517419 139845164340992 logging_writer.py:48] [19120] accumulated_eval_time=658.042457, accumulated_logging_time=10.005461, accumulated_submission_time=14466.749136, global_step=19120, preemption_count=0, score=14466.749136, test/ctc_loss=0.6446906924247742, test/num_examples=2472, test/wer=0.219812, total_duration=15135.041585, train/ctc_loss=0.626329243183136, train/wer=0.220436, validation/ctc_loss=0.9311202168464661, validation/num_examples=5348, validation/wer=0.280601
I0420 21:43:57.784837 140018299942720 checkpoints.py:356] Saving checkpoint at step: 19120
I0420 21:43:59.213227 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_19120
I0420 21:43:59.245702 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_19120.
I0420 21:44:59.812867 139845155948288 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7301946878433228, loss=1.889011263847351
I0420 21:46:14.716682 139845072021248 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5579772591590881, loss=1.8535101413726807
I0420 21:47:29.640238 139845155948288 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5676690936088562, loss=1.873487949371338
I0420 21:48:44.561225 139845072021248 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4989486038684845, loss=1.8562244176864624
I0420 21:50:02.713646 139844508980992 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.46752768754959106, loss=1.8254624605178833
I0420 21:51:17.640088 139844500588288 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.54776531457901, loss=1.8033130168914795
I0420 21:52:32.512667 139844508980992 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.48052874207496643, loss=1.9201098680496216
I0420 21:53:47.413385 139844500588288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6740551590919495, loss=1.8705239295959473
I0420 21:55:01.118363 140018299942720 spec.py:298] Evaluating on the training split.
I0420 21:55:38.779643 140018299942720 spec.py:310] Evaluating on the validation split.
I0420 21:56:16.693963 140018299942720 spec.py:326] Evaluating on the test split.
I0420 21:56:36.181798 140018299942720 submission_runner.py:406] Time since start: 15893.73s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.5580307, dtype=float32), 'train/wer': 0.20170484759534596, 'validation/ctc_loss': DeviceArray(0.8836023, dtype=float32), 'validation/wer': 0.26992059740084323, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6084923, dtype=float32), 'test/wer': 0.20872179229378668, 'test/num_examples': 2472, 'score': 15128.605032920837, 'total_duration': 15893.728668928146, 'accumulated_submission_time': 15128.605032920837, 'accumulated_eval_time': 753.1039803028107, 'accumulated_logging_time': 11.76012635231018}
I0420 21:56:36.199675 139845748020992 logging_writer.py:48] [20000] accumulated_eval_time=753.103980, accumulated_logging_time=11.760126, accumulated_submission_time=15128.605033, global_step=20000, preemption_count=0, score=15128.605033, test/ctc_loss=0.6084923148155212, test/num_examples=2472, test/wer=0.208722, total_duration=15893.728669, train/ctc_loss=0.5580307245254517, train/wer=0.201705, validation/ctc_loss=0.8836023211479187, validation/num_examples=5348, validation/wer=0.269921
I0420 21:56:36.482576 140018299942720 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 21:56:37.886860 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0420 21:56:37.919601 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0420 21:56:37.942367 139845739628288 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15128.605033
I0420 21:56:38.149544 140018299942720 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 21:56:39.971602 140018299942720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0420 21:56:40.004510 140018299942720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0420 21:56:41.361373 140018299942720 submission_runner.py:567] Tuning trial 1/1
I0420 21:56:41.361618 140018299942720 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 21:56:41.368275 140018299942720 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.272085, dtype=float32), 'train/wer': 1.9281073652208316, 'validation/ctc_loss': DeviceArray(29.980951, dtype=float32), 'validation/wer': 1.5048480930833872, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.09849, dtype=float32), 'test/wer': 1.5858062681534744, 'test/num_examples': 2472, 'score': 64.13452005386353, 'total_duration': 200.261545419693, 'accumulated_submission_time': 64.13452005386353, 'accumulated_eval_time': 136.12687611579895, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3149, {'train/ctc_loss': DeviceArray(5.6850686, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.71021, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6882114, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2464.168543577194, 'total_duration': 2679.187164545059, 'accumulated_submission_time': 2464.168543577194, 'accumulated_eval_time': 213.79871273040771, 'accumulated_logging_time': 1.1801538467407227, 'global_step': 3149, 'preemption_count': 0}), (6345, {'train/ctc_loss': DeviceArray(6.208722, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(6.125897, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.069559, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4864.717026233673, 'total_duration': 5159.554322957993, 'accumulated_submission_time': 4864.717026233673, 'accumulated_eval_time': 291.80724334716797, 'accumulated_logging_time': 2.9491517543792725, 'global_step': 6345, 'preemption_count': 0}), (9541, {'train/ctc_loss': DeviceArray(5.813272, dtype=float32), 'train/wer': 0.9337360511514883, 'validation/ctc_loss': DeviceArray(5.85413, dtype=float32), 'validation/wer': 0.8918079286823799, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.738961, dtype=float32), 'test/wer': 0.893871996425162, 'test/num_examples': 2472, 'score': 7264.85174202919, 'total_duration': 7641.763874769211, 'accumulated_submission_time': 7264.85174202919, 'accumulated_eval_time': 372.07923555374146, 'accumulated_logging_time': 4.711084604263306, 'global_step': 9541, 'preemption_count': 0}), (12735, {'train/ctc_loss': DeviceArray(1.3361945, dtype=float32), 'train/wer': 0.4144142257223345, 'validation/ctc_loss': DeviceArray(1.7047826, dtype=float32), 'validation/wer': 0.461770012252892, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3732243, dtype=float32), 'test/wer': 0.40964393800905896, 'test/num_examples': 2472, 'score': 9665.425416231155, 'total_duration': 10139.993323802948, 'accumulated_submission_time': 9665.425416231155, 'accumulated_eval_time': 467.93659710884094, 'accumulated_logging_time': 6.468662261962891, 'global_step': 12735, 'preemption_count': 0}), (15928, {'train/ctc_loss': DeviceArray(0.7849302, dtype=float32), 'train/wer': 0.2704559786035791, 'validation/ctc_loss': DeviceArray(1.1377124, dtype=float32), 'validation/wer': 0.33588360717421295, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8334967, dtype=float32), 'test/wer': 0.27548595454268476, 'test/num_examples': 2472, 'score': 12066.064540863037, 'total_duration': 12637.453438997269, 'accumulated_submission_time': 12066.064540863037, 'accumulated_eval_time': 562.9505045413971, 'accumulated_logging_time': 8.234748840332031, 'global_step': 15928, 'preemption_count': 0}), (19120, {'train/ctc_loss': DeviceArray(0.62632924, dtype=float32), 'train/wer': 0.22043627401294205, 'validation/ctc_loss': DeviceArray(0.9311202, dtype=float32), 'validation/wer': 0.28060087410394696, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6446907, dtype=float32), 'test/wer': 0.21981191477261186, 'test/num_examples': 2472, 'score': 14466.749136209488, 'total_duration': 15135.041584968567, 'accumulated_submission_time': 14466.749136209488, 'accumulated_eval_time': 658.0424566268921, 'accumulated_logging_time': 10.00546145439148, 'global_step': 19120, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.5580307, dtype=float32), 'train/wer': 0.20170484759534596, 'validation/ctc_loss': DeviceArray(0.8836023, dtype=float32), 'validation/wer': 0.26992059740084323, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6084923, dtype=float32), 'test/wer': 0.20872179229378668, 'test/num_examples': 2472, 'score': 15128.605032920837, 'total_duration': 15893.728668928146, 'accumulated_submission_time': 15128.605032920837, 'accumulated_eval_time': 753.1039803028107, 'accumulated_logging_time': 11.76012635231018, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0420 21:56:41.368486 140018299942720 submission_runner.py:570] Timing: 15128.605032920837
I0420 21:56:41.368542 140018299942720 submission_runner.py:571] ====================
I0420 21:56:41.369164 140018299942720 submission_runner.py:631] Final librispeech_conformer score: 15128.605032920837
