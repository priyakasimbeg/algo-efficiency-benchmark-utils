python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/momentum/jax/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_jax_upgrade_b/momentum --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-30-2023-19-50-06.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0530 19:50:28.996366 139692798289728 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax.
I0530 19:50:29.920126 139692798289728 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0530 19:50:29.920762 139692798289728 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0530 19:50:29.920894 139692798289728 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0530 19:50:29.926420 139692798289728 submission_runner.py:549] Using RNG seed 2401636715
I0530 19:50:35.052202 139692798289728 submission_runner.py:558] --- Tuning run 1/1 ---
I0530 19:50:35.052379 139692798289728 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1.
I0530 19:50:35.052554 139692798289728 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1/hparams.json.
I0530 19:50:35.231919 139692798289728 submission_runner.py:243] Initializing dataset.
I0530 19:50:35.232105 139692798289728 submission_runner.py:250] Initializing model.
I0530 19:50:37.240897 139692798289728 submission_runner.py:260] Initializing optimizer.
I0530 19:50:37.844430 139692798289728 submission_runner.py:267] Initializing metrics bundle.
I0530 19:50:37.844604 139692798289728 submission_runner.py:285] Initializing checkpoint and logger.
I0530 19:50:37.845570 139692798289728 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0530 19:50:37.845823 139692798289728 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0530 19:50:37.845909 139692798289728 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0530 19:50:38.427423 139692798289728 submission_runner.py:306] Saving meta data to /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0530 19:50:38.429596 139692798289728 submission_runner.py:309] Saving flags to /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0530 19:50:38.435708 139692798289728 submission_runner.py:321] Starting training loop.
I0530 19:50:38.718179 139692798289728 input_pipeline.py:20] Loading split = train-clean-100
I0530 19:50:38.758080 139692798289728 input_pipeline.py:20] Loading split = train-clean-360
I0530 19:50:39.097460 139692798289728 input_pipeline.py:20] Loading split = train-other-500
2023-05-30 19:51:32.011747: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-30 19:51:32.049822: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0530 19:51:36.852390 139527368681216 logging_writer.py:48] [0] global_step=0, grad_norm=21.362668991088867, loss=32.76116180419922
I0530 19:51:36.872906 139692798289728 spec.py:298] Evaluating on the training split.
I0530 19:51:37.119564 139692798289728 input_pipeline.py:20] Loading split = train-clean-100
I0530 19:51:37.152856 139692798289728 input_pipeline.py:20] Loading split = train-clean-360
I0530 19:51:37.457592 139692798289728 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0530 19:53:14.897126 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 19:53:15.085340 139692798289728 input_pipeline.py:20] Loading split = dev-clean
I0530 19:53:15.090440 139692798289728 input_pipeline.py:20] Loading split = dev-other
I0530 19:54:08.951703 139692798289728 spec.py:326] Evaluating on the test split.
I0530 19:54:09.145048 139692798289728 input_pipeline.py:20] Loading split = test-clean
I0530 19:54:45.697171 139692798289728 submission_runner.py:426] Time since start: 247.26s, 	Step: 1, 	{'train/ctc_loss': Array(30.722486, dtype=float32), 'train/wer': 2.8191751868896153, 'validation/ctc_loss': Array(29.554243, dtype=float32), 'validation/wer': 2.698704280793833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.691616, dtype=float32), 'test/wer': 2.727581093981679, 'test/num_examples': 2472, 'score': 58.43704700469971, 'total_duration': 247.2594165802002, 'accumulated_submission_time': 58.43704700469971, 'accumulated_data_selection_time': 4.595896005630493, 'accumulated_eval_time': 188.82223892211914, 'accumulated_logging_time': 0}
I0530 19:54:45.718243 139525384820480 logging_writer.py:48] [1] accumulated_data_selection_time=4.595896, accumulated_eval_time=188.822239, accumulated_logging_time=0, accumulated_submission_time=58.437047, global_step=1, preemption_count=0, score=58.437047, test/ctc_loss=29.69161605834961, test/num_examples=2472, test/wer=2.727581, total_duration=247.259417, train/ctc_loss=30.72248649597168, train/wer=2.819175, validation/ctc_loss=29.554243087768555, validation/num_examples=5348, validation/wer=2.698704
I0530 19:56:07.433488 139534188836608 logging_writer.py:48] [100] global_step=100, grad_norm=7.4978461265563965, loss=6.132356643676758
I0530 19:57:22.985197 139534197229312 logging_writer.py:48] [200] global_step=200, grad_norm=2.788832902908325, loss=5.934468746185303
I0530 19:58:38.673320 139534188836608 logging_writer.py:48] [300] global_step=300, grad_norm=2.4908015727996826, loss=5.890285491943359
I0530 19:59:54.336069 139534197229312 logging_writer.py:48] [400] global_step=400, grad_norm=3.361051321029663, loss=5.875974655151367
I0530 20:01:11.039760 139534188836608 logging_writer.py:48] [500] global_step=500, grad_norm=0.4513954222202301, loss=5.746299743652344
I0530 20:02:26.628320 139534197229312 logging_writer.py:48] [600] global_step=600, grad_norm=1.2966952323913574, loss=5.630888938903809
I0530 20:03:42.509196 139534188836608 logging_writer.py:48] [700] global_step=700, grad_norm=3.091416120529175, loss=8.82715129852295
I0530 20:04:57.703145 139534197229312 logging_writer.py:48] [800] global_step=800, grad_norm=0.9741446375846863, loss=6.880595684051514
I0530 20:06:14.326057 139534188836608 logging_writer.py:48] [900] global_step=900, grad_norm=2.5863640308380127, loss=7.27282190322876
I0530 20:07:29.848959 139534197229312 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1817.9110107421875
I0530 20:08:47.747001 139534239192832 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1838.2752685546875
I0530 20:10:03.585805 139534230800128 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1806.6212158203125
I0530 20:11:18.384690 139534239192832 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1822.0513916015625
I0530 20:12:32.694320 139534230800128 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1776.5318603515625
I0530 20:13:46.980045 139534239192832 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1813.6607666015625
I0530 20:15:01.241540 139534230800128 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1826.8624267578125
I0530 20:16:20.411792 139534239192832 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1825.0390625
I0530 20:17:38.563363 139534230800128 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1746.1170654296875
I0530 20:18:59.100154 139534239192832 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1868.60009765625
I0530 20:20:21.905598 139534230800128 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1811.4791259765625
I0530 20:21:42.446629 139534239192832 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1811.6072998046875
I0530 20:22:56.779335 139534230800128 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1908.156005859375
I0530 20:24:11.472975 139534239192832 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1851.3033447265625
I0530 20:25:25.942269 139534230800128 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1879.437744140625
I0530 20:26:40.276945 139534239192832 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1881.648193359375
I0530 20:27:55.174049 139534230800128 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1849.6981201171875
I0530 20:29:13.911858 139534239192832 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1858.156494140625
I0530 20:30:29.747847 139534230800128 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1869.009521484375
I0530 20:31:49.346323 139534239192832 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1783.462646484375
I0530 20:33:10.441092 139534230800128 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1824.388671875
I0530 20:34:36.510955 139534894552832 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1832.7479248046875
I0530 20:34:46.241007 139692798289728 spec.py:298] Evaluating on the training split.
I0530 20:35:16.207492 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 20:35:52.736388 139692798289728 spec.py:326] Evaluating on the test split.
I0530 20:36:10.253283 139692798289728 submission_runner.py:426] Time since start: 2731.81s, 	Step: 3114, 	{'train/ctc_loss': Array(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2458.9127266407013, 'total_duration': 2731.813805580139, 'accumulated_submission_time': 2458.9127266407013, 'accumulated_data_selection_time': 454.48061966896057, 'accumulated_eval_time': 272.830815076828, 'accumulated_logging_time': 0.032198429107666016}
I0530 20:36:10.272790 139534894552832 logging_writer.py:48] [3114] accumulated_data_selection_time=454.480620, accumulated_eval_time=272.830815, accumulated_logging_time=0.032198, accumulated_submission_time=2458.912727, global_step=3114, preemption_count=0, score=2458.912727, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=2731.813806, train/ctc_loss=1767.6815185546875, train/wer=0.944636, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0530 20:37:15.863136 139534886160128 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1879.02392578125
I0530 20:38:30.340617 139534894552832 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1807.386474609375
I0530 20:39:45.783996 139534886160128 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1878.6102294921875
I0530 20:41:00.352564 139534894552832 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1763.4375
I0530 20:42:14.538356 139534886160128 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1924.3646240234375
I0530 20:43:28.492402 139534894552832 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1812.8902587890625
I0530 20:44:42.305406 139534886160128 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1833.1414794921875
I0530 20:45:58.389510 139534894552832 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1825.1690673828125
I0530 20:47:17.004023 139534886160128 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1872.153076171875
I0530 20:48:36.870507 139534894552832 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1813.9178466796875
I0530 20:49:55.982493 139534239192832 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1838.2752685546875
I0530 20:51:10.045241 139534230800128 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1759.4378662109375
I0530 20:52:24.189172 139534239192832 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1874.620849609375
I0530 20:53:39.202619 139534230800128 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1770.8785400390625
I0530 20:54:53.506119 139534239192832 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1872.427001953125
I0530 20:56:09.533956 139534230800128 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1771.9818115234375
I0530 20:57:25.057297 139534239192832 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1859.1011962890625
I0530 20:58:39.566985 139534230800128 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1813.4039306640625
I0530 21:00:02.988577 139534239192832 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1848.362548828125
I0530 21:01:25.236669 139534230800128 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1770.388671875
I0530 21:02:45.373752 139534894552832 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1809.3026123046875
I0530 21:03:59.728988 139534886160128 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1903.1910400390625
I0530 21:05:14.891857 139534894552832 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1860.9932861328125
I0530 21:06:30.138682 139534886160128 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1781.724853515625
I0530 21:07:44.376183 139534894552832 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1850.2327880859375
I0530 21:08:58.549698 139534886160128 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1779.371826171875
I0530 21:10:19.830421 139534894552832 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1869.1458740234375
I0530 21:11:41.176723 139534886160128 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1837.0880126953125
I0530 21:13:00.493810 139534894552832 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1865.8760986328125
I0530 21:14:19.884854 139534886160128 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1884.973388671875
I0530 21:15:44.078074 139534239192832 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1813.789306640625
I0530 21:16:10.613975 139692798289728 spec.py:298] Evaluating on the training split.
I0530 21:16:40.561826 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 21:17:16.132367 139692798289728 spec.py:326] Evaluating on the test split.
I0530 21:17:33.884276 139692798289728 submission_runner.py:426] Time since start: 5215.44s, 	Step: 6237, 	{'train/ctc_loss': Array(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4859.201548814774, 'total_duration': 5215.444625377655, 'accumulated_submission_time': 4859.201548814774, 'accumulated_data_selection_time': 955.3040475845337, 'accumulated_eval_time': 356.09723114967346, 'accumulated_logging_time': 0.06711554527282715}
I0530 21:17:33.903978 139534464472832 logging_writer.py:48] [6237] accumulated_data_selection_time=955.304048, accumulated_eval_time=356.097231, accumulated_logging_time=0.067116, accumulated_submission_time=4859.201549, global_step=6237, preemption_count=0, score=4859.201549, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5215.444625, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0530 21:18:21.318825 139534456080128 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1811.0946044921875
I0530 21:19:36.098934 139534464472832 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1750.4151611328125
I0530 21:20:51.288630 139534456080128 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1788.3214111328125
I0530 21:22:05.616659 139534464472832 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1844.2347412109375
I0530 21:23:20.504866 139534456080128 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1852.1070556640625
I0530 21:24:36.667834 139534464472832 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1799.5093994140625
I0530 21:25:52.358259 139534456080128 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1789.1964111328125
I0530 21:27:09.837107 139534464472832 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1787.072998046875
I0530 21:28:27.425977 139534456080128 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1758.7127685546875
I0530 21:29:42.861522 139534464472832 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1878.6102294921875
I0530 21:31:01.111544 139534464472832 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1789.1964111328125
I0530 21:32:15.314980 139534456080128 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1832.8790283203125
I0530 21:33:29.364081 139534464472832 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1785.4527587890625
I0530 21:34:43.680406 139534456080128 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1810.582275390625
I0530 21:35:58.356028 139534464472832 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1790.44775390625
I0530 21:37:13.940598 139534456080128 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1852.9112548828125
I0530 21:38:34.630756 139534464472832 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1771.7366943359375
I0530 21:39:54.169785 139534456080128 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1859.7763671875
I0530 21:41:16.583934 139534464472832 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1814.946533203125
I0530 21:42:36.969683 139534456080128 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1866.42041015625
I0530 21:43:58.694229 139534464472832 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1825.9503173828125
I0530 21:45:12.802862 139534456080128 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1842.2440185546875
I0530 21:46:27.183569 139534464472832 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1770.51123046875
I0530 21:47:41.098800 139534456080128 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1824.77880859375
I0530 21:48:56.154240 139534464472832 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1836.29736328125
I0530 21:50:09.910605 139534456080128 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1767.4554443359375
I0530 21:51:29.803885 139534464472832 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1806.23876953125
I0530 21:52:55.403556 139534456080128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1804.58349609375
I0530 21:54:17.853481 139534464472832 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1802.9312744140625
I0530 21:55:40.186159 139534456080128 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1734.8756103515625
I0530 21:56:58.558629 139534464472832 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1814.04638671875
I0530 21:57:34.566280 139692798289728 spec.py:298] Evaluating on the training split.
I0530 21:58:05.806061 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 21:58:41.792175 139692798289728 spec.py:326] Evaluating on the test split.
I0530 21:59:00.834776 139692798289728 submission_runner.py:426] Time since start: 7702.40s, 	Step: 9350, 	{'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7259.809722661972, 'total_duration': 7702.395346164703, 'accumulated_submission_time': 7259.809722661972, 'accumulated_data_selection_time': 1467.0733089447021, 'accumulated_eval_time': 442.3620765209198, 'accumulated_logging_time': 0.10342836380004883}
I0530 21:59:00.855373 139534894552832 logging_writer.py:48] [9350] accumulated_data_selection_time=1467.073309, accumulated_eval_time=442.362077, accumulated_logging_time=0.103428, accumulated_submission_time=7259.809723, global_step=9350, preemption_count=0, score=7259.809723, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7702.395346, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0530 21:59:38.843118 139534886160128 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1779.371826171875
I0530 22:00:53.551424 139534894552832 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1862.753662109375
I0530 22:02:07.987177 139534886160128 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1815.0753173828125
I0530 22:03:21.961496 139534894552832 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1813.789306640625
I0530 22:04:36.298183 139534886160128 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1788.6962890625
I0530 22:05:54.082595 139534894552832 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1836.824462890625
I0530 22:07:12.607142 139534886160128 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1868.8729248046875
I0530 22:08:35.833896 139534894552832 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1871.05859375
I0530 22:09:58.598082 139534886160128 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1889.7042236328125
I0530 22:11:22.231103 139534566872832 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1800.52197265625
I0530 22:12:37.598603 139534558480128 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1830.9127197265625
I0530 22:13:51.984152 139534566872832 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1814.6893310546875
I0530 22:15:05.945743 139534558480128 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1839.5963134765625
I0530 22:16:19.960222 139534566872832 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1840.257568359375
I0530 22:17:35.201907 139534558480128 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1821.0145263671875
I0530 22:18:50.245822 139534566872832 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1788.5712890625
I0530 22:20:04.079574 139534558480128 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1819.849365234375
I0530 22:21:25.907679 139534566872832 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1896.002197265625
I0530 22:22:47.999066 139534558480128 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1847.295654296875
I0530 22:24:08.389920 139534566872832 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1818.427490234375
I0530 22:25:25.222299 139534566872832 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1935.860595703125
I0530 22:26:39.316564 139534558480128 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1748.0247802734375
I0530 22:27:53.664947 139534566872832 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1880.4041748046875
I0530 22:29:07.966788 139534558480128 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1836.692626953125
I0530 22:30:25.073398 139534566872832 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1789.571533203125
I0530 22:31:48.645687 139534558480128 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1830.52001953125
I0530 22:33:11.662944 139534566872832 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1855.1939697265625
I0530 22:34:37.083504 139534558480128 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1822.5703125
I0530 22:36:01.080117 139534566872832 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1796.2265625
I0530 22:37:26.662931 139534558480128 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0, loss=1840.257568359375
I0530 22:38:49.484657 139534566872832 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1864.9246826171875
I0530 22:39:01.161481 139692798289728 spec.py:298] Evaluating on the training split.
I0530 22:39:32.455922 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 22:40:09.733412 139692798289728 spec.py:326] Evaluating on the test split.
I0530 22:40:28.813073 139692798289728 submission_runner.py:426] Time since start: 10190.37s, 	Step: 12417, 	{'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9660.062967777252, 'total_duration': 10190.373483657837, 'accumulated_submission_time': 9660.062967777252, 'accumulated_data_selection_time': 2023.7357060909271, 'accumulated_eval_time': 530.0098531246185, 'accumulated_logging_time': 0.1397709846496582}
I0530 22:40:28.833539 139534275032832 logging_writer.py:48] [12417] accumulated_data_selection_time=2023.735706, accumulated_eval_time=530.009853, accumulated_logging_time=0.139771, accumulated_submission_time=9660.062968, global_step=12417, preemption_count=0, score=9660.062968, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10190.373484, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0530 22:41:31.063449 139534266640128 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1914.8642578125
I0530 22:42:46.244541 139534275032832 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1849.5645751953125
I0530 22:44:00.541821 139534266640128 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1908.8675537109375
I0530 22:45:15.741904 139534275032832 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1823.34912109375
I0530 22:46:29.504159 139534266640128 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1760.8902587890625
I0530 22:47:47.701600 139534275032832 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1852.375
I0530 22:49:08.850212 139534266640128 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1817.394775390625
I0530 22:50:30.741328 139534275032832 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1846.4959716796875
I0530 22:51:53.151898 139534266640128 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1742.669921875
I0530 22:53:15.726718 139534894552832 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1819.9788818359375
I0530 22:54:30.155557 139534886160128 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1870.2384033203125
I0530 22:55:43.981949 139534894552832 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1824.9088134765625
I0530 22:56:58.113463 139534886160128 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1848.6295166015625
I0530 22:58:11.983921 139534894552832 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1859.9114990234375
I0530 22:59:31.942285 139534886160128 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1860.046630859375
I0530 23:00:57.395327 139534894552832 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1777.5184326171875
I0530 23:02:23.703062 139534886160128 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1785.701904296875
I0530 23:03:48.280705 139534894552832 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1828.4281005859375
I0530 23:05:06.785731 139534886160128 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1875.170166015625
I0530 23:06:26.088927 139534894552832 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1817.13671875
I0530 23:07:44.142805 139534275032832 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1844.2347412109375
I0530 23:08:58.129417 139534266640128 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1901.0709228515625
I0530 23:10:12.031419 139534275032832 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1829.604248046875
I0530 23:11:26.081568 139534266640128 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1767.943603515625
I0530 23:12:40.236253 139534275032832 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1891.5194091796875
I0530 23:13:59.678019 139534266640128 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1842.50927734375
I0530 23:15:22.321928 139534275032832 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1892.7779541015625
I0530 23:16:44.148230 139534266640128 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1781.3529052734375
I0530 23:18:04.016322 139534275032832 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1885.6676025390625
I0530 23:19:23.427545 139534266640128 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1919.171142578125
I0530 23:20:29.076527 139692798289728 spec.py:298] Evaluating on the training split.
I0530 23:21:00.224850 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 23:21:37.625549 139692798289728 spec.py:326] Evaluating on the test split.
I0530 23:21:56.320970 139692798289728 submission_runner.py:426] Time since start: 12677.88s, 	Step: 15482, 	{'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12060.251776218414, 'total_duration': 12677.881314992905, 'accumulated_submission_time': 12060.251776218414, 'accumulated_data_selection_time': 2599.0120089054108, 'accumulated_eval_time': 617.250655412674, 'accumulated_logging_time': 0.1765148639678955}
I0530 23:21:56.342362 139534275032832 logging_writer.py:48] [15482] accumulated_data_selection_time=2599.012009, accumulated_eval_time=617.250655, accumulated_logging_time=0.176515, accumulated_submission_time=12060.251776, global_step=15482, preemption_count=0, score=12060.251776, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=12677.881315, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0530 23:22:10.548352 139534266640128 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1845.1651611328125
I0530 23:23:24.538240 139534275032832 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1834.1922607421875
I0530 23:24:39.464637 139534266640128 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1908.5830078125
I0530 23:25:53.534303 139534275032832 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1849.2972412109375
I0530 23:27:07.734722 139534266640128 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1791.95166015625
I0530 23:28:22.509903 139692798289728 spec.py:298] Evaluating on the training split.
I0530 23:28:52.552069 139692798289728 spec.py:310] Evaluating on the validation split.
I0530 23:29:28.458009 139692798289728 spec.py:326] Evaluating on the test split.
I0530 23:29:46.486157 139692798289728 submission_runner.py:426] Time since start: 13148.05s, 	Step: 16000, 	{'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12446.396736621857, 'total_duration': 13148.048114776611, 'accumulated_submission_time': 12446.396736621857, 'accumulated_data_selection_time': 2668.769643306732, 'accumulated_eval_time': 701.2246327400208, 'accumulated_logging_time': 0.21353459358215332}
I0530 23:29:46.504550 139534310872832 logging_writer.py:48] [16000] accumulated_data_selection_time=2668.769643, accumulated_eval_time=701.224633, accumulated_logging_time=0.213535, accumulated_submission_time=12446.396737, global_step=16000, preemption_count=0, score=12446.396737, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=13148.048115, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0530 23:29:46.521474 139534302480128 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12446.396737
I0530 23:29:46.629498 139692798289728 checkpoints.py:490] Saving checkpoint at step: 16000
I0530 23:29:47.230334 139692798289728 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0530 23:29:47.246192 139692798289728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_upgrade_b/momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0530 23:29:48.644606 139692798289728 submission_runner.py:589] Tuning trial 1/1
I0530 23:29:48.644831 139692798289728 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0530 23:29:48.649321 139692798289728 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.722486, dtype=float32), 'train/wer': 2.8191751868896153, 'validation/ctc_loss': Array(29.554243, dtype=float32), 'validation/wer': 2.698704280793833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.691616, dtype=float32), 'test/wer': 2.727581093981679, 'test/num_examples': 2472, 'score': 58.43704700469971, 'total_duration': 247.2594165802002, 'accumulated_submission_time': 58.43704700469971, 'accumulated_data_selection_time': 4.595896005630493, 'accumulated_eval_time': 188.82223892211914, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3114, {'train/ctc_loss': Array(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2458.9127266407013, 'total_duration': 2731.813805580139, 'accumulated_submission_time': 2458.9127266407013, 'accumulated_data_selection_time': 454.48061966896057, 'accumulated_eval_time': 272.830815076828, 'accumulated_logging_time': 0.032198429107666016, 'global_step': 3114, 'preemption_count': 0}), (6237, {'train/ctc_loss': Array(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4859.201548814774, 'total_duration': 5215.444625377655, 'accumulated_submission_time': 4859.201548814774, 'accumulated_data_selection_time': 955.3040475845337, 'accumulated_eval_time': 356.09723114967346, 'accumulated_logging_time': 0.06711554527282715, 'global_step': 6237, 'preemption_count': 0}), (9350, {'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7259.809722661972, 'total_duration': 7702.395346164703, 'accumulated_submission_time': 7259.809722661972, 'accumulated_data_selection_time': 1467.0733089447021, 'accumulated_eval_time': 442.3620765209198, 'accumulated_logging_time': 0.10342836380004883, 'global_step': 9350, 'preemption_count': 0}), (12417, {'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9660.062967777252, 'total_duration': 10190.373483657837, 'accumulated_submission_time': 9660.062967777252, 'accumulated_data_selection_time': 2023.7357060909271, 'accumulated_eval_time': 530.0098531246185, 'accumulated_logging_time': 0.1397709846496582, 'global_step': 12417, 'preemption_count': 0}), (15482, {'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12060.251776218414, 'total_duration': 12677.881314992905, 'accumulated_submission_time': 12060.251776218414, 'accumulated_data_selection_time': 2599.0120089054108, 'accumulated_eval_time': 617.250655412674, 'accumulated_logging_time': 0.1765148639678955, 'global_step': 15482, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12446.396736621857, 'total_duration': 13148.048114776611, 'accumulated_submission_time': 12446.396736621857, 'accumulated_data_selection_time': 2668.769643306732, 'accumulated_eval_time': 701.2246327400208, 'accumulated_logging_time': 0.21353459358215332, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0530 23:29:48.649469 139692798289728 submission_runner.py:592] Timing: 12446.396736621857
I0530 23:29:48.649534 139692798289728 submission_runner.py:593] ====================
I0530 23:29:48.650458 139692798289728 submission_runner.py:661] Final librispeech_deepspeech score: 12446.396736621857
