I0420 08:14:55.518662 139728531793728 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax.
I0420 08:14:55.582127 139728531793728 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 08:14:56.434328 139728531793728 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0420 08:14:56.435145 139728531793728 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 08:14:56.439230 139728531793728 submission_runner.py:528] Using RNG seed 107324755
I0420 08:14:59.234565 139728531793728 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 08:14:59.234822 139728531793728 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1.
I0420 08:14:59.235049 139728531793728 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/hparams.json.
I0420 08:14:59.363875 139728531793728 submission_runner.py:232] Initializing dataset.
I0420 08:14:59.372951 139728531793728 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:14:59.377843 139728531793728 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:14:59.377983 139728531793728 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:14:59.499874 139728531793728 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:15:01.431446 139728531793728 submission_runner.py:239] Initializing model.
I0420 08:15:13.102522 139728531793728 submission_runner.py:249] Initializing optimizer.
I0420 08:15:13.654712 139728531793728 submission_runner.py:256] Initializing metrics bundle.
I0420 08:15:13.654924 139728531793728 submission_runner.py:273] Initializing checkpoint and logger.
I0420 08:15:13.655917 139728531793728 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1 with prefix checkpoint_
I0420 08:15:13.656154 139728531793728 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 08:15:13.656216 139728531793728 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 08:15:14.425610 139728531793728 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/meta_data_0.json.
I0420 08:15:14.426555 139728531793728 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/flags_0.json.
I0420 08:15:14.430772 139728531793728 submission_runner.py:309] Starting training loop.
I0420 08:15:43.829037 139552438085376 logging_writer.py:48] [0] global_step=0, grad_norm=5.084718704223633, loss=11.161172866821289
I0420 08:15:43.840596 139728531793728 spec.py:298] Evaluating on the training split.
I0420 08:15:43.843035 139728531793728 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:15:43.845842 139728531793728 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:15:43.845950 139728531793728 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:15:43.875540 139728531793728 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:15:52.052795 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 08:20:57.939910 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 08:20:57.943877 139728531793728 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:20:57.947637 139728531793728 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:20:57.947753 139728531793728 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:20:57.979573 139728531793728 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:21:05.444159 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 08:26:04.337843 139728531793728 spec.py:326] Evaluating on the test split.
I0420 08:26:04.340365 139728531793728 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:26:04.342879 139728531793728 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:26:04.342999 139728531793728 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:26:04.371408 139728531793728 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:26:11.156060 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 08:31:04.386728 139728531793728 submission_runner.py:406] Time since start: 949.96s, 	Step: 1, 	{'train/accuracy': 0.0006151673151180148, 'train/loss': 11.181036949157715, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.19296932220459, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.201367378234863, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 29.409669637680054, 'total_duration': 949.9558386802673, 'accumulated_submission_time': 29.409669637680054, 'accumulated_eval_time': 920.5460283756256, 'accumulated_logging_time': 0}
I0420 08:31:04.403244 139541272327936 logging_writer.py:48] [1] accumulated_eval_time=920.546028, accumulated_logging_time=0, accumulated_submission_time=29.409670, global_step=1, preemption_count=0, score=29.409670, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.201367, test/num_examples=3003, total_duration=949.955839, train/accuracy=0.000615, train/bleu=0.000000, train/loss=11.181037, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.192969, validation/num_examples=3000
I0420 08:31:05.033210 139728531793728 checkpoints.py:356] Saving checkpoint at step: 1
I0420 08:31:07.259768 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_1
I0420 08:31:07.262229 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_1.
I0420 08:31:43.070715 139541280720640 logging_writer.py:48] [100] global_step=100, grad_norm=0.1631292849779129, loss=9.040814399719238
I0420 08:32:18.855908 139541381433088 logging_writer.py:48] [200] global_step=200, grad_norm=0.1339438259601593, loss=8.912096977233887
I0420 08:32:54.656070 139541280720640 logging_writer.py:48] [300] global_step=300, grad_norm=0.15284040570259094, loss=8.832884788513184
I0420 08:33:30.491862 139541381433088 logging_writer.py:48] [400] global_step=400, grad_norm=0.31809359788894653, loss=8.643613815307617
I0420 08:34:06.304419 139541280720640 logging_writer.py:48] [500] global_step=500, grad_norm=0.3923272490501404, loss=8.468039512634277
I0420 08:34:42.216286 139541381433088 logging_writer.py:48] [600] global_step=600, grad_norm=0.44462713599205017, loss=8.269272804260254
I0420 08:35:18.107911 139541280720640 logging_writer.py:48] [700] global_step=700, grad_norm=0.8250221014022827, loss=8.1412935256958
I0420 08:35:54.010928 139541381433088 logging_writer.py:48] [800] global_step=800, grad_norm=0.6091094017028809, loss=7.994073390960693
I0420 08:36:29.887443 139541280720640 logging_writer.py:48] [900] global_step=900, grad_norm=0.619750440120697, loss=7.9372334480285645
I0420 08:37:05.807299 139541381433088 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7270257472991943, loss=7.801750183105469
I0420 08:37:41.698519 139541280720640 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5932392477989197, loss=7.774026393890381
I0420 08:38:17.622844 139541381433088 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.48036614060401917, loss=7.60603141784668
I0420 08:38:53.509060 139541280720640 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6991567611694336, loss=7.59437894821167
I0420 08:39:29.386284 139541381433088 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7741491198539734, loss=7.5193681716918945
I0420 08:40:05.323359 139541280720640 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6516745686531067, loss=7.428300857543945
I0420 08:40:41.251127 139541381433088 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7610651254653931, loss=7.36524772644043
I0420 08:41:17.129024 139541280720640 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6173411011695862, loss=7.247074604034424
I0420 08:41:53.036871 139541381433088 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6619726419448853, loss=7.123982906341553
I0420 08:42:28.954903 139541280720640 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7033692598342896, loss=7.096120357513428
I0420 08:43:04.908003 139541381433088 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5389997959136963, loss=7.09793758392334
I0420 08:43:40.814175 139541280720640 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.795179545879364, loss=6.9529337882995605
I0420 08:44:16.733246 139541381433088 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7866594791412354, loss=6.970223903656006
I0420 08:44:52.637180 139541280720640 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6349639892578125, loss=6.882957458496094
I0420 08:45:07.422888 139728531793728 spec.py:298] Evaluating on the training split.
I0420 08:45:10.463060 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 08:50:02.149825 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 08:50:04.837518 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 08:54:50.994343 139728531793728 spec.py:326] Evaluating on the test split.
I0420 08:54:53.738402 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 08:59:47.271455 139728531793728 submission_runner.py:406] Time since start: 2672.84s, 	Step: 2343, 	{'train/accuracy': 0.30720096826553345, 'train/loss': 5.55286979675293, 'train/bleu': 7.136993193610818, 'validation/accuracy': 0.28057926893234253, 'validation/loss': 5.818579196929932, 'validation/bleu': 3.4393398230014434, 'validation/num_examples': 3000, 'test/accuracy': 0.25985708832740784, 'test/loss': 6.111303329467773, 'test/bleu': 2.4272030373497335, 'test/num_examples': 3003, 'score': 869.5385911464691, 'total_duration': 2672.840561389923, 'accumulated_submission_time': 869.5385911464691, 'accumulated_eval_time': 1800.3945076465607, 'accumulated_logging_time': 2.878878593444824}
I0420 08:59:47.281560 139541381433088 logging_writer.py:48] [2343] accumulated_eval_time=1800.394508, accumulated_logging_time=2.878879, accumulated_submission_time=869.538591, global_step=2343, preemption_count=0, score=869.538591, test/accuracy=0.259857, test/bleu=2.427203, test/loss=6.111303, test/num_examples=3003, total_duration=2672.840561, train/accuracy=0.307201, train/bleu=7.136993, train/loss=5.552870, validation/accuracy=0.280579, validation/bleu=3.439340, validation/loss=5.818579, validation/num_examples=3000
I0420 08:59:48.213251 139728531793728 checkpoints.py:356] Saving checkpoint at step: 2343
I0420 08:59:51.402403 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_2343
I0420 08:59:51.405897 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_2343.
I0420 09:00:12.240284 139541280720640 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.659929096698761, loss=6.770919322967529
I0420 09:00:48.116319 139541356254976 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6690129041671753, loss=6.788721561431885
I0420 09:01:23.976753 139541280720640 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7242768406867981, loss=6.587606430053711
I0420 09:01:59.854691 139541356254976 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7130438685417175, loss=6.687292575836182
I0420 09:02:35.754290 139541280720640 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.640746533870697, loss=6.628416061401367
I0420 09:03:11.644368 139541356254976 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6920638084411621, loss=6.592278480529785
I0420 09:03:47.584241 139541280720640 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6109700798988342, loss=6.48001766204834
I0420 09:04:23.492488 139541356254976 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6761466860771179, loss=6.421392917633057
I0420 09:04:59.407564 139541280720640 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.864263117313385, loss=6.31657075881958
I0420 09:05:35.258426 139541356254976 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9311025738716125, loss=6.341822147369385
I0420 09:06:11.183244 139541280720640 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7552410364151001, loss=6.268636226654053
I0420 09:06:47.087028 139541356254976 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7411201596260071, loss=6.096569538116455
I0420 09:07:23.003564 139541280720640 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6122312545776367, loss=6.119724750518799
I0420 09:07:58.911108 139541356254976 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6727598905563354, loss=5.939917087554932
I0420 09:08:34.844226 139541280720640 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8218131065368652, loss=5.992504596710205
I0420 09:09:10.771975 139541356254976 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7203618288040161, loss=5.917340278625488
I0420 09:09:46.675467 139541280720640 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6069270968437195, loss=5.911591529846191
I0420 09:10:22.586194 139541356254976 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5920661091804504, loss=5.864960193634033
I0420 09:10:58.503489 139541280720640 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7942450642585754, loss=5.810960292816162
I0420 09:11:34.387817 139541356254976 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5802846550941467, loss=5.775750160217285
I0420 09:12:10.288415 139541280720640 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.600260317325592, loss=5.686234474182129
I0420 09:12:46.174515 139541356254976 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.52342689037323, loss=5.6382365226745605
I0420 09:13:22.090749 139541280720640 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5262917876243591, loss=5.618762493133545
I0420 09:13:51.612832 139728531793728 spec.py:298] Evaluating on the training split.
I0420 09:13:54.635324 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 09:17:42.221451 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 09:17:44.873037 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 09:20:45.251409 139728531793728 spec.py:326] Evaluating on the test split.
I0420 09:20:47.983430 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 09:23:48.891789 139728531793728 submission_runner.py:406] Time since start: 4114.46s, 	Step: 4684, 	{'train/accuracy': 0.45750242471694946, 'train/loss': 3.9175946712493896, 'train/bleu': 17.41974223827672, 'validation/accuracy': 0.4473100006580353, 'validation/loss': 3.9930026531219482, 'validation/bleu': 13.035368082364215, 'validation/num_examples': 3000, 'test/accuracy': 0.4359770119190216, 'test/loss': 4.146965503692627, 'test/bleu': 11.133284114445642, 'test/num_examples': 3003, 'score': 1709.7115106582642, 'total_duration': 4114.460822820663, 'accumulated_submission_time': 1709.7115106582642, 'accumulated_eval_time': 2397.6733055114746, 'accumulated_logging_time': 7.017084121704102}
I0420 09:23:48.901425 139541356254976 logging_writer.py:48] [4684] accumulated_eval_time=2397.673306, accumulated_logging_time=7.017084, accumulated_submission_time=1709.711511, global_step=4684, preemption_count=0, score=1709.711511, test/accuracy=0.435977, test/bleu=11.133284, test/loss=4.146966, test/num_examples=3003, total_duration=4114.460823, train/accuracy=0.457502, train/bleu=17.419742, train/loss=3.917595, validation/accuracy=0.447310, validation/bleu=13.035368, validation/loss=3.993003, validation/num_examples=3000
I0420 09:23:49.517359 139728531793728 checkpoints.py:356] Saving checkpoint at step: 4684
I0420 09:23:51.777395 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_4684
I0420 09:23:51.780329 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_4684.
I0420 09:23:57.870474 139541280720640 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5371978282928467, loss=5.5952887535095215
I0420 09:24:33.692421 139541339469568 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5170469284057617, loss=5.508556365966797
I0420 09:25:09.551081 139541280720640 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6068322062492371, loss=5.512515068054199
I0420 09:25:45.403135 139541339469568 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5682361125946045, loss=5.3872761726379395
I0420 09:26:21.292087 139541280720640 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6264099478721619, loss=5.448021411895752
I0420 09:26:57.180186 139541339469568 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5667192339897156, loss=5.456874370574951
I0420 09:27:33.063914 139541280720640 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5324065685272217, loss=5.3667755126953125
I0420 09:28:08.957312 139541339469568 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5347141027450562, loss=5.399578094482422
I0420 09:28:44.866747 139541280720640 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5159006714820862, loss=5.366998195648193
I0420 09:29:20.749192 139541339469568 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5073119401931763, loss=5.265756607055664
I0420 09:29:56.643796 139541280720640 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4974370300769806, loss=5.310981750488281
I0420 09:30:32.562562 139541339469568 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.47999948263168335, loss=5.150321960449219
I0420 09:31:08.458508 139541280720640 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5828563570976257, loss=5.199150085449219
I0420 09:31:44.393697 139541339469568 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.46738943457603455, loss=5.238585472106934
I0420 09:32:20.277904 139541280720640 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.49384018778800964, loss=5.160620212554932
I0420 09:32:56.198420 139541339469568 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.46072670817375183, loss=5.182596683502197
I0420 09:33:32.100283 139541280720640 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4448249936103821, loss=5.1068620681762695
I0420 09:34:08.016840 139541339469568 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.45682674646377563, loss=5.1673688888549805
I0420 09:34:43.904886 139541280720640 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.45999377965927124, loss=5.1524152755737305
I0420 09:35:19.789345 139541339469568 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.42550235986709595, loss=5.017327308654785
I0420 09:35:55.677324 139541280720640 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.42821183800697327, loss=5.025632381439209
I0420 09:36:31.603388 139541339469568 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4487225115299225, loss=5.1665449142456055
I0420 09:37:07.478646 139541280720640 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4208105504512787, loss=5.006106853485107
I0420 09:37:43.330318 139541339469568 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.45532405376434326, loss=5.106383323669434
I0420 09:37:52.008636 139728531793728 spec.py:298] Evaluating on the training split.
I0420 09:37:55.038084 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 09:40:46.008871 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 09:40:48.666615 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 09:43:20.703789 139728531793728 spec.py:326] Evaluating on the test split.
I0420 09:43:23.422182 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 09:45:45.783117 139728531793728 submission_runner.py:406] Time since start: 5431.35s, 	Step: 7026, 	{'train/accuracy': 0.541467010974884, 'train/loss': 3.1227355003356934, 'train/bleu': 23.817748010571957, 'validation/accuracy': 0.5326406359672546, 'validation/loss': 3.1590590476989746, 'validation/bleu': 19.497936517346037, 'validation/num_examples': 3000, 'test/accuracy': 0.5299866199493408, 'test/loss': 3.221147060394287, 'test/bleu': 18.004295294548797, 'test/num_examples': 3003, 'score': 2549.908703327179, 'total_duration': 5431.35227560997, 'accumulated_submission_time': 2549.908703327179, 'accumulated_eval_time': 2871.447769165039, 'accumulated_logging_time': 9.908944845199585}
I0420 09:45:45.791538 139541280720640 logging_writer.py:48] [7026] accumulated_eval_time=2871.447769, accumulated_logging_time=9.908945, accumulated_submission_time=2549.908703, global_step=7026, preemption_count=0, score=2549.908703, test/accuracy=0.529987, test/bleu=18.004295, test/loss=3.221147, test/num_examples=3003, total_duration=5431.352276, train/accuracy=0.541467, train/bleu=23.817748, train/loss=3.122736, validation/accuracy=0.532641, validation/bleu=19.497937, validation/loss=3.159059, validation/num_examples=3000
I0420 09:45:46.399026 139728531793728 checkpoints.py:356] Saving checkpoint at step: 7026
I0420 09:45:48.597529 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_7026
I0420 09:45:48.600158 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_7026.
I0420 09:46:15.467457 139541339469568 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.42498379945755005, loss=5.028907775878906
I0420 09:46:51.255587 139541331076864 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.42715325951576233, loss=4.97609281539917
I0420 09:47:27.081463 139541339469568 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.39769986271858215, loss=4.885502815246582
I0420 09:48:02.964066 139541331076864 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4062734842300415, loss=4.979740619659424
I0420 09:48:38.838598 139541339469568 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.40838152170181274, loss=4.938365936279297
I0420 09:49:14.717893 139541331076864 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.41084158420562744, loss=4.946470260620117
I0420 09:49:50.628490 139541339469568 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.3936830163002014, loss=4.9910569190979
I0420 09:50:26.549712 139541331076864 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4088219702243805, loss=4.842331886291504
I0420 09:51:02.461907 139541339469568 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3963858485221863, loss=4.88348388671875
I0420 09:51:38.335755 139541331076864 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.39149484038352966, loss=4.846981525421143
I0420 09:52:14.222863 139541339469568 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.36832791566848755, loss=4.816332817077637
I0420 09:52:50.185725 139541331076864 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4123471975326538, loss=4.933294773101807
I0420 09:53:26.069223 139541339469568 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.37435075640678406, loss=4.794372081756592
I0420 09:54:01.959245 139541331076864 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.38342565298080444, loss=4.860122203826904
I0420 09:54:37.869241 139541339469568 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.37637001276016235, loss=4.884091854095459
I0420 09:55:13.740097 139541331076864 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.392706036567688, loss=4.779177665710449
I0420 09:55:49.649429 139541339469568 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.39631178975105286, loss=4.818868160247803
I0420 09:56:25.537312 139541331076864 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.37885114550590515, loss=4.785996437072754
I0420 09:57:01.446579 139541339469568 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3762894570827484, loss=4.891872882843018
I0420 09:57:37.316014 139541331076864 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.36779293417930603, loss=4.791600227355957
I0420 09:58:13.205492 139541339469568 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3693962097167969, loss=4.685198783874512
I0420 09:58:49.072072 139541331076864 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.35637715458869934, loss=4.716585159301758
I0420 09:59:24.963267 139541339469568 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.35899749398231506, loss=4.7565741539001465
I0420 09:59:48.697568 139728531793728 spec.py:298] Evaluating on the training split.
I0420 09:59:51.705229 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:02:42.630107 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 10:02:45.308859 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:05:14.013233 139728531793728 spec.py:326] Evaluating on the test split.
I0420 10:05:16.731097 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:07:42.635858 139728531793728 submission_runner.py:406] Time since start: 6748.20s, 	Step: 9368, 	{'train/accuracy': 0.5727139711380005, 'train/loss': 2.8222641944885254, 'train/bleu': 26.30079335195143, 'validation/accuracy': 0.567854106426239, 'validation/loss': 2.8208985328674316, 'validation/bleu': 22.200068583062695, 'validation/num_examples': 3000, 'test/accuracy': 0.5688222646713257, 'test/loss': 2.8361690044403076, 'test/bleu': 20.536820531869488, 'test/num_examples': 3003, 'score': 3389.9753124713898, 'total_duration': 6748.204926490784, 'accumulated_submission_time': 3389.9753124713898, 'accumulated_eval_time': 3345.385956287384, 'accumulated_logging_time': 12.728832721710205}
I0420 10:07:42.647662 139541331076864 logging_writer.py:48] [9368] accumulated_eval_time=3345.385956, accumulated_logging_time=12.728833, accumulated_submission_time=3389.975312, global_step=9368, preemption_count=0, score=3389.975312, test/accuracy=0.568822, test/bleu=20.536821, test/loss=2.836169, test/num_examples=3003, total_duration=6748.204926, train/accuracy=0.572714, train/bleu=26.300793, train/loss=2.822264, validation/accuracy=0.567854, validation/bleu=22.200069, validation/loss=2.820899, validation/num_examples=3000
I0420 10:07:43.563054 139728531793728 checkpoints.py:356] Saving checkpoint at step: 9368
I0420 10:07:46.761305 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_9368
I0420 10:07:46.764605 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_9368.
I0420 10:07:58.570175 139541339469568 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.36997562646865845, loss=4.698343753814697
I0420 10:08:34.324269 139541052139264 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.35468921065330505, loss=4.676396369934082
I0420 10:09:10.103127 139541339469568 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.36710935831069946, loss=4.794165134429932
I0420 10:09:45.947766 139541052139264 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.37019550800323486, loss=4.7492780685424805
I0420 10:10:21.765714 139541339469568 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3444867432117462, loss=4.665559768676758
I0420 10:10:57.648483 139541052139264 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3649730980396271, loss=4.714270114898682
I0420 10:11:33.550621 139541339469568 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3623628616333008, loss=4.695947170257568
I0420 10:12:09.427636 139541052139264 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.35927650332450867, loss=4.827600479125977
I0420 10:12:45.312407 139541339469568 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.35520514845848083, loss=4.681368827819824
I0420 10:13:21.228654 139541052139264 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.34436142444610596, loss=4.741442680358887
I0420 10:13:57.098391 139541339469568 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3595205545425415, loss=4.746065616607666
I0420 10:14:33.042319 139541052139264 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3599279820919037, loss=4.728408336639404
I0420 10:15:08.923961 139541339469568 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3553306460380554, loss=4.658266067504883
I0420 10:15:44.780924 139541052139264 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.35103267431259155, loss=4.724132061004639
I0420 10:16:20.665702 139541339469568 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3581638038158417, loss=4.708988666534424
I0420 10:16:56.514995 139541052139264 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.35207563638687134, loss=4.630661487579346
I0420 10:17:32.379350 139541339469568 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3634243309497833, loss=4.781071186065674
I0420 10:18:08.282747 139541052139264 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.36160174012184143, loss=4.680723190307617
I0420 10:18:44.164211 139541339469568 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3319540321826935, loss=4.543994903564453
I0420 10:19:20.000594 139541052139264 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3605002164840698, loss=4.650849342346191
I0420 10:19:55.910104 139541339469568 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.34365785121917725, loss=4.668643474578857
I0420 10:20:31.789141 139541052139264 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.35412028431892395, loss=4.701229095458984
I0420 10:21:07.712529 139541339469568 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3456515967845917, loss=4.580478191375732
I0420 10:21:43.583073 139541052139264 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.34855273365974426, loss=4.672757625579834
I0420 10:21:46.881368 139728531793728 spec.py:298] Evaluating on the training split.
I0420 10:21:49.887450 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:24:27.290681 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 10:24:29.946996 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:27:04.364747 139728531793728 spec.py:326] Evaluating on the test split.
I0420 10:27:07.069806 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:29:33.689698 139728531793728 submission_runner.py:406] Time since start: 8059.26s, 	Step: 11711, 	{'train/accuracy': 0.5801190733909607, 'train/loss': 2.7234857082366943, 'train/bleu': 27.379332118896397, 'validation/accuracy': 0.5885481834411621, 'validation/loss': 2.621612071990967, 'validation/bleu': 23.631642966880925, 'validation/num_examples': 3000, 'test/accuracy': 0.5917611122131348, 'test/loss': 2.614142656326294, 'test/bleu': 21.86789138938379, 'test/num_examples': 3003, 'score': 4230.059151172638, 'total_duration': 8059.258825540543, 'accumulated_submission_time': 4230.059151172638, 'accumulated_eval_time': 3812.194211244583, 'accumulated_logging_time': 16.860865116119385}
I0420 10:29:33.698715 139541339469568 logging_writer.py:48] [11711] accumulated_eval_time=3812.194211, accumulated_logging_time=16.860865, accumulated_submission_time=4230.059151, global_step=11711, preemption_count=0, score=4230.059151, test/accuracy=0.591761, test/bleu=21.867891, test/loss=2.614143, test/num_examples=3003, total_duration=8059.258826, train/accuracy=0.580119, train/bleu=27.379332, train/loss=2.723486, validation/accuracy=0.588548, validation/bleu=23.631643, validation/loss=2.621612, validation/num_examples=3000
I0420 10:29:34.317025 139728531793728 checkpoints.py:356] Saving checkpoint at step: 11711
I0420 10:29:36.537391 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_11711
I0420 10:29:36.539911 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_11711.
I0420 10:30:08.715531 139541052139264 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.33618685603141785, loss=4.643725395202637
I0420 10:30:44.505728 139541043746560 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.32756638526916504, loss=4.580484390258789
I0420 10:31:20.352580 139541052139264 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.33912310004234314, loss=4.664438724517822
I0420 10:31:56.186940 139541043746560 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3274795114994049, loss=4.574864387512207
I0420 10:32:32.062540 139541052139264 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3188208043575287, loss=4.577992916107178
I0420 10:33:07.911749 139541043746560 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3190227746963501, loss=4.565374374389648
I0420 10:33:43.758734 139541052139264 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3511884808540344, loss=4.543313503265381
I0420 10:34:19.680802 139541043746560 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3358084261417389, loss=4.603304862976074
I0420 10:34:55.548027 139541052139264 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3262280225753784, loss=4.594804286956787
I0420 10:35:31.424308 139541043746560 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.33351629972457886, loss=4.5398149490356445
I0420 10:36:07.336137 139541052139264 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.324821412563324, loss=4.635583877563477
I0420 10:36:43.275635 139541043746560 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.32875847816467285, loss=4.56541633605957
I0420 10:37:19.147988 139541052139264 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3240700364112854, loss=4.573424816131592
I0420 10:37:55.007373 139541043746560 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.33125096559524536, loss=4.539096355438232
I0420 10:38:30.899451 139541052139264 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3347514271736145, loss=4.659221649169922
I0420 10:39:06.838249 139541043746560 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.33015692234039307, loss=4.545065879821777
I0420 10:39:42.693326 139541052139264 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.32304835319519043, loss=4.590417385101318
I0420 10:40:18.550401 139541043746560 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3336460590362549, loss=4.593221664428711
I0420 10:40:54.427949 139541052139264 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3931233286857605, loss=4.538015842437744
I0420 10:41:30.300380 139541043746560 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.32602494955062866, loss=4.534144401550293
I0420 10:42:06.176769 139541052139264 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3313336968421936, loss=4.564365386962891
I0420 10:42:42.045341 139541043746560 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.32610970735549927, loss=4.598842144012451
I0420 10:43:17.992114 139541052139264 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.326463520526886, loss=4.51202917098999
I0420 10:43:36.722598 139728531793728 spec.py:298] Evaluating on the training split.
I0420 10:43:39.733616 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:46:12.618523 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 10:46:15.262014 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:48:41.484308 139728531793728 spec.py:326] Evaluating on the test split.
I0420 10:48:44.206454 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 10:51:03.503493 139728531793728 submission_runner.py:406] Time since start: 9349.07s, 	Step: 14054, 	{'train/accuracy': 0.5984262824058533, 'train/loss': 2.546327590942383, 'train/bleu': 28.503209053326902, 'validation/accuracy': 0.6035386919975281, 'validation/loss': 2.4655606746673584, 'validation/bleu': 24.68794194382733, 'validation/num_examples': 3000, 'test/accuracy': 0.6062402129173279, 'test/loss': 2.462031364440918, 'test/bleu': 23.214733174067383, 'test/num_examples': 3003, 'score': 5070.210530281067, 'total_duration': 9349.072598695755, 'accumulated_submission_time': 5070.210530281067, 'accumulated_eval_time': 4258.9750118255615, 'accumulated_logging_time': 19.713918924331665}
I0420 10:51:03.513607 139541043746560 logging_writer.py:48] [14054] accumulated_eval_time=4258.975012, accumulated_logging_time=19.713919, accumulated_submission_time=5070.210530, global_step=14054, preemption_count=0, score=5070.210530, test/accuracy=0.606240, test/bleu=23.214733, test/loss=2.462031, test/num_examples=3003, total_duration=9349.072599, train/accuracy=0.598426, train/bleu=28.503209, train/loss=2.546328, validation/accuracy=0.603539, validation/bleu=24.687942, validation/loss=2.465561, validation/num_examples=3000
I0420 10:51:04.428864 139728531793728 checkpoints.py:356] Saving checkpoint at step: 14054
I0420 10:51:07.653815 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_14054
I0420 10:51:07.657173 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_14054.
I0420 10:51:24.493256 139541052139264 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.32041987776756287, loss=4.495828151702881
I0420 10:52:00.279905 139541035353856 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.328567236661911, loss=4.560033798217773
I0420 10:52:36.074420 139541052139264 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3256891667842865, loss=4.495274543762207
I0420 10:53:11.909630 139541035353856 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.32180458307266235, loss=4.540621757507324
I0420 10:53:47.744886 139541052139264 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.32657691836357117, loss=4.481348991394043
I0420 10:54:23.652487 139541035353856 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3228832483291626, loss=4.56045389175415
I0420 10:54:59.537461 139541052139264 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3228977918624878, loss=4.530585765838623
I0420 10:55:35.391090 139541035353856 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.36226341128349304, loss=4.545199871063232
I0420 10:56:11.256986 139541052139264 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3209782838821411, loss=4.503016948699951
I0420 10:56:47.138919 139541035353856 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.32806944847106934, loss=4.460744380950928
I0420 10:57:23.027626 139541052139264 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3180393874645233, loss=4.449398040771484
I0420 10:57:58.896965 139541035353856 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.31764957308769226, loss=4.579621315002441
I0420 10:58:34.765667 139541052139264 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.34119102358818054, loss=4.553308963775635
I0420 10:59:10.633238 139541035353856 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.316740483045578, loss=4.466961860656738
I0420 10:59:46.481378 139541052139264 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3141973912715912, loss=4.490777492523193
I0420 11:00:22.315427 139541035353856 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3235803544521332, loss=4.481350898742676
I0420 11:00:58.159705 139541052139264 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3369179666042328, loss=4.527347087860107
I0420 11:01:34.057920 139541035353856 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3174295425415039, loss=4.445724010467529
I0420 11:02:09.875479 139541052139264 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3307648301124573, loss=4.477229118347168
I0420 11:02:45.746749 139541035353856 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3340928256511688, loss=4.476875305175781
I0420 11:03:21.641616 139541052139264 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3321932852268219, loss=4.560933589935303
I0420 11:03:57.526207 139541035353856 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.332100510597229, loss=4.423099517822266
I0420 11:04:33.406641 139541052139264 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3134784996509552, loss=4.434885025024414
I0420 11:05:07.883376 139728531793728 spec.py:298] Evaluating on the training split.
I0420 11:05:10.883582 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:07:46.784440 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 11:07:49.432744 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:10:13.994393 139728531793728 spec.py:326] Evaluating on the test split.
I0420 11:10:16.697575 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:12:40.144896 139728531793728 submission_runner.py:406] Time since start: 10645.71s, 	Step: 16398, 	{'train/accuracy': 0.6023316383361816, 'train/loss': 2.498037338256836, 'train/bleu': 29.317365268111075, 'validation/accuracy': 0.6126892566680908, 'validation/loss': 2.3706607818603516, 'validation/bleu': 25.045813771655574, 'validation/num_examples': 3000, 'test/accuracy': 0.6180930733680725, 'test/loss': 2.3489699363708496, 'test/bleu': 24.137374611551483, 'test/num_examples': 3003, 'score': 5910.406844139099, 'total_duration': 10645.71402144432, 'accumulated_submission_time': 5910.406844139099, 'accumulated_eval_time': 4711.23646402359, 'accumulated_logging_time': 23.871424913406372}
I0420 11:12:40.154555 139541035353856 logging_writer.py:48] [16398] accumulated_eval_time=4711.236464, accumulated_logging_time=23.871425, accumulated_submission_time=5910.406844, global_step=16398, preemption_count=0, score=5910.406844, test/accuracy=0.618093, test/bleu=24.137375, test/loss=2.348970, test/num_examples=3003, total_duration=10645.714021, train/accuracy=0.602332, train/bleu=29.317365, train/loss=2.498037, validation/accuracy=0.612689, validation/bleu=25.045814, validation/loss=2.370661, validation/num_examples=3000
I0420 11:12:40.779262 139728531793728 checkpoints.py:356] Saving checkpoint at step: 16398
I0420 11:12:42.997586 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_16398
I0420 11:12:43.000607 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_16398.
I0420 11:12:44.090684 139541052139264 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.32654714584350586, loss=4.499532222747803
I0420 11:13:19.832857 139541026961152 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3271063268184662, loss=4.414979934692383
I0420 11:13:55.641057 139541052139264 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3257445991039276, loss=4.393006801605225
I0420 11:14:31.466535 139541026961152 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.31821751594543457, loss=4.376259803771973
I0420 11:15:07.334304 139541052139264 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3166216015815735, loss=4.446163654327393
I0420 11:15:43.197091 139541026961152 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3209238648414612, loss=4.44586181640625
I0420 11:16:19.101102 139541052139264 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3199341595172882, loss=4.549635410308838
I0420 11:16:54.958130 139541026961152 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.32138746976852417, loss=4.469793796539307
I0420 11:17:30.833222 139541052139264 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.31872469186782837, loss=4.395887851715088
I0420 11:18:06.668838 139541026961152 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.32684218883514404, loss=4.439139366149902
I0420 11:18:42.491367 139541052139264 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.32069677114486694, loss=4.483827590942383
I0420 11:19:18.335897 139541026961152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.315923273563385, loss=4.469820499420166
I0420 11:19:54.201351 139541052139264 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3151603937149048, loss=4.456689357757568
I0420 11:20:30.050217 139541026961152 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3147614896297455, loss=4.4104695320129395
I0420 11:21:05.928852 139541052139264 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.31518658995628357, loss=4.461441516876221
I0420 11:21:41.774896 139541026961152 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.30388161540031433, loss=4.420594692230225
I0420 11:22:17.654325 139541052139264 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.31241485476493835, loss=4.4289374351501465
I0420 11:22:53.542238 139541026961152 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.317920058965683, loss=4.428005695343018
I0420 11:23:29.422707 139541052139264 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.2998717427253723, loss=4.448205947875977
I0420 11:24:05.308606 139541026961152 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.31769442558288574, loss=4.390143394470215
I0420 11:24:41.161267 139541052139264 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.31411272287368774, loss=4.3788957595825195
I0420 11:25:16.994142 139541026961152 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.31070882081985474, loss=4.4248552322387695
I0420 11:25:52.831035 139541052139264 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3412560522556305, loss=4.453932762145996
I0420 11:26:28.691714 139541026961152 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3220415413379669, loss=4.391251087188721
I0420 11:26:43.100318 139728531793728 spec.py:298] Evaluating on the training split.
I0420 11:26:46.101025 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:29:25.141550 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 11:29:27.801667 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:31:52.464780 139728531793728 spec.py:326] Evaluating on the test split.
I0420 11:31:55.172162 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:34:15.419956 139728531793728 submission_runner.py:406] Time since start: 11940.99s, 	Step: 18742, 	{'train/accuracy': 0.6109029054641724, 'train/loss': 2.4326374530792236, 'train/bleu': 29.78237893601002, 'validation/accuracy': 0.6195955276489258, 'validation/loss': 2.3138153553009033, 'validation/bleu': 25.185605463990058, 'validation/num_examples': 3000, 'test/accuracy': 0.6268898248672485, 'test/loss': 2.286189556121826, 'test/bleu': 24.480147829813127, 'test/num_examples': 3003, 'score': 6750.471117019653, 'total_duration': 11940.989099502563, 'accumulated_submission_time': 6750.471117019653, 'accumulated_eval_time': 5163.556041479111, 'accumulated_logging_time': 26.730600357055664}
I0420 11:34:15.429608 139541052139264 logging_writer.py:48] [18742] accumulated_eval_time=5163.556041, accumulated_logging_time=26.730600, accumulated_submission_time=6750.471117, global_step=18742, preemption_count=0, score=6750.471117, test/accuracy=0.626890, test/bleu=24.480148, test/loss=2.286190, test/num_examples=3003, total_duration=11940.989100, train/accuracy=0.610903, train/bleu=29.782379, train/loss=2.432637, validation/accuracy=0.619596, validation/bleu=25.185605, validation/loss=2.313815, validation/num_examples=3000
I0420 11:34:16.044447 139728531793728 checkpoints.py:356] Saving checkpoint at step: 18742
I0420 11:34:18.225277 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_18742
I0420 11:34:18.227840 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_18742.
I0420 11:34:39.348789 139541026961152 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.33724018931388855, loss=4.3913116455078125
I0420 11:35:15.126834 139541018568448 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3128487169742584, loss=4.359811782836914
I0420 11:35:50.954257 139541026961152 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3046286404132843, loss=4.34393310546875
I0420 11:36:26.827336 139541018568448 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3322007656097412, loss=4.398376941680908
I0420 11:37:02.680633 139541026961152 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3136329650878906, loss=4.35028076171875
I0420 11:37:38.568481 139541018568448 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3141971528530121, loss=4.383072376251221
I0420 11:38:14.450488 139541026961152 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.31007421016693115, loss=4.416755676269531
I0420 11:38:50.322318 139541018568448 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3081699013710022, loss=4.329200744628906
I0420 11:39:26.210303 139541026961152 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.31665632128715515, loss=4.4886674880981445
I0420 11:40:02.184897 139541018568448 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3091478645801544, loss=4.414046764373779
I0420 11:40:38.033584 139541026961152 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3171656131744385, loss=4.47170877456665
I0420 11:41:13.910248 139541018568448 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3164357841014862, loss=4.421181678771973
I0420 11:41:49.092984 139728531793728 spec.py:298] Evaluating on the training split.
I0420 11:41:52.095805 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:44:24.491976 139728531793728 spec.py:310] Evaluating on the validation split.
I0420 11:44:27.151144 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:46:50.468193 139728531793728 spec.py:326] Evaluating on the test split.
I0420 11:46:53.173570 139728531793728 workload.py:179] Translating evaluation dataset.
I0420 11:49:11.748334 139728531793728 submission_runner.py:406] Time since start: 12837.32s, 	Step: 20000, 	{'train/accuracy': 0.6194024682044983, 'train/loss': 2.3617804050445557, 'train/bleu': 29.842817498139656, 'validation/accuracy': 0.6216537952423096, 'validation/loss': 2.2842652797698975, 'validation/bleu': 26.082377294480104, 'validation/num_examples': 3000, 'test/accuracy': 0.6308407783508301, 'test/loss': 2.254014253616333, 'test/bleu': 24.64322232672652, 'test/num_examples': 3003, 'score': 7201.316343784332, 'total_duration': 12837.317479372025, 'accumulated_submission_time': 7201.316343784332, 'accumulated_eval_time': 5606.211343050003, 'accumulated_logging_time': 29.541651010513306}
I0420 11:49:11.757027 139541026961152 logging_writer.py:48] [20000] accumulated_eval_time=5606.211343, accumulated_logging_time=29.541651, accumulated_submission_time=7201.316344, global_step=20000, preemption_count=0, score=7201.316344, test/accuracy=0.630841, test/bleu=24.643222, test/loss=2.254014, test/num_examples=3003, total_duration=12837.317479, train/accuracy=0.619402, train/bleu=29.842817, train/loss=2.361780, validation/accuracy=0.621654, validation/bleu=26.082377, validation/loss=2.284265, validation/num_examples=3000
I0420 11:49:12.371658 139728531793728 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 11:49:14.539990 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_20000
I0420 11:49:14.542483 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_20000.
I0420 11:49:14.552933 139541018568448 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7201.316344
I0420 11:49:14.912632 139728531793728 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 11:49:18.247686 139728531793728 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_20000
I0420 11:49:18.250041 139728531793728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/wmt_jax/trial_1/checkpoint_20000.
I0420 11:49:18.302531 139728531793728 submission_runner.py:567] Tuning trial 1/1
I0420 11:49:18.302706 139728531793728 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0420 11:49:18.303862 139728531793728 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006151673151180148, 'train/loss': 11.181036949157715, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.19296932220459, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.201367378234863, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 29.409669637680054, 'total_duration': 949.9558386802673, 'accumulated_submission_time': 29.409669637680054, 'accumulated_eval_time': 920.5460283756256, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2343, {'train/accuracy': 0.30720096826553345, 'train/loss': 5.55286979675293, 'train/bleu': 7.136993193610818, 'validation/accuracy': 0.28057926893234253, 'validation/loss': 5.818579196929932, 'validation/bleu': 3.4393398230014434, 'validation/num_examples': 3000, 'test/accuracy': 0.25985708832740784, 'test/loss': 6.111303329467773, 'test/bleu': 2.4272030373497335, 'test/num_examples': 3003, 'score': 869.5385911464691, 'total_duration': 2672.840561389923, 'accumulated_submission_time': 869.5385911464691, 'accumulated_eval_time': 1800.3945076465607, 'accumulated_logging_time': 2.878878593444824, 'global_step': 2343, 'preemption_count': 0}), (4684, {'train/accuracy': 0.45750242471694946, 'train/loss': 3.9175946712493896, 'train/bleu': 17.41974223827672, 'validation/accuracy': 0.4473100006580353, 'validation/loss': 3.9930026531219482, 'validation/bleu': 13.035368082364215, 'validation/num_examples': 3000, 'test/accuracy': 0.4359770119190216, 'test/loss': 4.146965503692627, 'test/bleu': 11.133284114445642, 'test/num_examples': 3003, 'score': 1709.7115106582642, 'total_duration': 4114.460822820663, 'accumulated_submission_time': 1709.7115106582642, 'accumulated_eval_time': 2397.6733055114746, 'accumulated_logging_time': 7.017084121704102, 'global_step': 4684, 'preemption_count': 0}), (7026, {'train/accuracy': 0.541467010974884, 'train/loss': 3.1227355003356934, 'train/bleu': 23.817748010571957, 'validation/accuracy': 0.5326406359672546, 'validation/loss': 3.1590590476989746, 'validation/bleu': 19.497936517346037, 'validation/num_examples': 3000, 'test/accuracy': 0.5299866199493408, 'test/loss': 3.221147060394287, 'test/bleu': 18.004295294548797, 'test/num_examples': 3003, 'score': 2549.908703327179, 'total_duration': 5431.35227560997, 'accumulated_submission_time': 2549.908703327179, 'accumulated_eval_time': 2871.447769165039, 'accumulated_logging_time': 9.908944845199585, 'global_step': 7026, 'preemption_count': 0}), (9368, {'train/accuracy': 0.5727139711380005, 'train/loss': 2.8222641944885254, 'train/bleu': 26.30079335195143, 'validation/accuracy': 0.567854106426239, 'validation/loss': 2.8208985328674316, 'validation/bleu': 22.200068583062695, 'validation/num_examples': 3000, 'test/accuracy': 0.5688222646713257, 'test/loss': 2.8361690044403076, 'test/bleu': 20.536820531869488, 'test/num_examples': 3003, 'score': 3389.9753124713898, 'total_duration': 6748.204926490784, 'accumulated_submission_time': 3389.9753124713898, 'accumulated_eval_time': 3345.385956287384, 'accumulated_logging_time': 12.728832721710205, 'global_step': 9368, 'preemption_count': 0}), (11711, {'train/accuracy': 0.5801190733909607, 'train/loss': 2.7234857082366943, 'train/bleu': 27.379332118896397, 'validation/accuracy': 0.5885481834411621, 'validation/loss': 2.621612071990967, 'validation/bleu': 23.631642966880925, 'validation/num_examples': 3000, 'test/accuracy': 0.5917611122131348, 'test/loss': 2.614142656326294, 'test/bleu': 21.86789138938379, 'test/num_examples': 3003, 'score': 4230.059151172638, 'total_duration': 8059.258825540543, 'accumulated_submission_time': 4230.059151172638, 'accumulated_eval_time': 3812.194211244583, 'accumulated_logging_time': 16.860865116119385, 'global_step': 11711, 'preemption_count': 0}), (14054, {'train/accuracy': 0.5984262824058533, 'train/loss': 2.546327590942383, 'train/bleu': 28.503209053326902, 'validation/accuracy': 0.6035386919975281, 'validation/loss': 2.4655606746673584, 'validation/bleu': 24.68794194382733, 'validation/num_examples': 3000, 'test/accuracy': 0.6062402129173279, 'test/loss': 2.462031364440918, 'test/bleu': 23.214733174067383, 'test/num_examples': 3003, 'score': 5070.210530281067, 'total_duration': 9349.072598695755, 'accumulated_submission_time': 5070.210530281067, 'accumulated_eval_time': 4258.9750118255615, 'accumulated_logging_time': 19.713918924331665, 'global_step': 14054, 'preemption_count': 0}), (16398, {'train/accuracy': 0.6023316383361816, 'train/loss': 2.498037338256836, 'train/bleu': 29.317365268111075, 'validation/accuracy': 0.6126892566680908, 'validation/loss': 2.3706607818603516, 'validation/bleu': 25.045813771655574, 'validation/num_examples': 3000, 'test/accuracy': 0.6180930733680725, 'test/loss': 2.3489699363708496, 'test/bleu': 24.137374611551483, 'test/num_examples': 3003, 'score': 5910.406844139099, 'total_duration': 10645.71402144432, 'accumulated_submission_time': 5910.406844139099, 'accumulated_eval_time': 4711.23646402359, 'accumulated_logging_time': 23.871424913406372, 'global_step': 16398, 'preemption_count': 0}), (18742, {'train/accuracy': 0.6109029054641724, 'train/loss': 2.4326374530792236, 'train/bleu': 29.78237893601002, 'validation/accuracy': 0.6195955276489258, 'validation/loss': 2.3138153553009033, 'validation/bleu': 25.185605463990058, 'validation/num_examples': 3000, 'test/accuracy': 0.6268898248672485, 'test/loss': 2.286189556121826, 'test/bleu': 24.480147829813127, 'test/num_examples': 3003, 'score': 6750.471117019653, 'total_duration': 11940.989099502563, 'accumulated_submission_time': 6750.471117019653, 'accumulated_eval_time': 5163.556041479111, 'accumulated_logging_time': 26.730600357055664, 'global_step': 18742, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6194024682044983, 'train/loss': 2.3617804050445557, 'train/bleu': 29.842817498139656, 'validation/accuracy': 0.6216537952423096, 'validation/loss': 2.2842652797698975, 'validation/bleu': 26.082377294480104, 'validation/num_examples': 3000, 'test/accuracy': 0.6308407783508301, 'test/loss': 2.254014253616333, 'test/bleu': 24.64322232672652, 'test/num_examples': 3003, 'score': 7201.316343784332, 'total_duration': 12837.317479372025, 'accumulated_submission_time': 7201.316343784332, 'accumulated_eval_time': 5606.211343050003, 'accumulated_logging_time': 29.541651010513306, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0420 11:49:18.303967 139728531793728 submission_runner.py:570] Timing: 7201.316343784332
I0420 11:49:18.304017 139728531793728 submission_runner.py:571] ====================
I0420 11:49:18.304129 139728531793728 submission_runner.py:631] Final wmt score: 7201.316343784332
