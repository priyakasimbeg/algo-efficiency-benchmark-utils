I0420 00:13:26.360222 140068904109888 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax.
I0420 00:13:26.424051 140068904109888 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 00:13:27.238621 140068904109888 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0420 00:13:27.239232 140068904109888 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 00:13:27.243302 140068904109888 submission_runner.py:528] Using RNG seed 1783998622
I0420 00:13:29.974015 140068904109888 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 00:13:29.974203 140068904109888 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1.
I0420 00:13:29.974388 140068904109888 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/hparams.json.
I0420 00:13:30.096214 140068904109888 submission_runner.py:232] Initializing dataset.
I0420 00:13:30.109081 140068904109888 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:13:30.116286 140068904109888 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:13:30.116408 140068904109888 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:13:30.384770 140068904109888 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:13:31.466847 140068904109888 submission_runner.py:239] Initializing model.
I0420 00:13:42.737317 140068904109888 submission_runner.py:249] Initializing optimizer.
I0420 00:13:43.680000 140068904109888 submission_runner.py:256] Initializing metrics bundle.
I0420 00:13:43.680172 140068904109888 submission_runner.py:273] Initializing checkpoint and logger.
I0420 00:13:43.681091 140068904109888 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0420 00:13:44.399452 140068904109888 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0420 00:13:44.400376 140068904109888 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/flags_0.json.
I0420 00:13:44.405333 140068904109888 submission_runner.py:309] Starting training loop.
I0420 00:14:30.358271 139890352183040 logging_writer.py:48] [0] global_step=0, grad_norm=0.5508369207382202, loss=6.9304118156433105
I0420 00:14:30.371763 140068904109888 spec.py:298] Evaluating on the training split.
I0420 00:14:30.857257 140068904109888 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:14:30.863551 140068904109888 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:14:30.863673 140068904109888 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:14:30.925350 140068904109888 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:14:42.225142 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 00:14:43.000607 140068904109888 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:14:43.014423 140068904109888 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:14:43.014729 140068904109888 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:14:43.080344 140068904109888 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:15:00.500776 140068904109888 spec.py:326] Evaluating on the test split.
I0420 00:15:00.904264 140068904109888 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 00:15:00.909066 140068904109888 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0420 00:15:00.938774 140068904109888 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 00:15:10.205935 140068904109888 submission_runner.py:406] Time since start: 85.80s, 	Step: 1, 	{'train/accuracy': 0.0008171236841008067, 'train/loss': 6.911888599395752, 'validation/accuracy': 0.000859999970998615, 'validation/loss': 6.911709785461426, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.911829948425293, 'test/num_examples': 10000, 'score': 45.966253995895386, 'total_duration': 85.80054950714111, 'accumulated_submission_time': 45.966253995895386, 'accumulated_eval_time': 39.834142208099365, 'accumulated_logging_time': 0}
I0420 00:15:10.225382 139864150349568 logging_writer.py:48] [1] accumulated_eval_time=39.834142, accumulated_logging_time=0, accumulated_submission_time=45.966254, global_step=1, preemption_count=0, score=45.966254, test/accuracy=0.001000, test/loss=6.911830, test/num_examples=10000, total_duration=85.800550, train/accuracy=0.000817, train/loss=6.911889, validation/accuracy=0.000860, validation/loss=6.911710, validation/num_examples=50000
I0420 00:15:10.333909 140068904109888 checkpoints.py:356] Saving checkpoint at step: 1
I0420 00:15:10.769818 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1
I0420 00:15:10.770629 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1.
I0420 00:15:44.640484 139864158742272 logging_writer.py:48] [100] global_step=100, grad_norm=0.5283296704292297, loss=6.891551494598389
I0420 00:16:18.677733 139864435570432 logging_writer.py:48] [200] global_step=200, grad_norm=0.5762955546379089, loss=6.812623023986816
I0420 00:16:52.647028 139864158742272 logging_writer.py:48] [300] global_step=300, grad_norm=0.6149449348449707, loss=6.661405086517334
I0420 00:17:26.671929 139864435570432 logging_writer.py:48] [400] global_step=400, grad_norm=0.6177487969398499, loss=6.570437431335449
I0420 00:18:00.391968 139864158742272 logging_writer.py:48] [500] global_step=500, grad_norm=0.6550677418708801, loss=6.570300102233887
I0420 00:18:34.352631 139864435570432 logging_writer.py:48] [600] global_step=600, grad_norm=0.6748517751693726, loss=6.477603912353516
I0420 00:19:08.346835 139864158742272 logging_writer.py:48] [700] global_step=700, grad_norm=0.6578951478004456, loss=6.416675090789795
I0420 00:19:42.310944 139864435570432 logging_writer.py:48] [800] global_step=800, grad_norm=1.1006689071655273, loss=6.401067733764648
I0420 00:20:16.301584 139864158742272 logging_writer.py:48] [900] global_step=900, grad_norm=0.7321570515632629, loss=6.314414024353027
I0420 00:20:50.060811 139864435570432 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7433691024780273, loss=6.247334003448486
I0420 00:21:24.031841 139864158742272 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7392691373825073, loss=6.205959320068359
I0420 00:21:57.899462 139864435570432 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7468467354774475, loss=6.150190353393555
I0420 00:22:31.787311 139864158742272 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8438236713409424, loss=6.213440895080566
I0420 00:23:05.817051 139864435570432 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7928230166435242, loss=6.0604143142700195
I0420 00:23:39.847418 139864158742272 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8782216906547546, loss=6.020294666290283
I0420 00:23:40.921689 140068904109888 spec.py:298] Evaluating on the training split.
I0420 00:23:47.629841 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 00:23:55.433007 140068904109888 spec.py:326] Evaluating on the test split.
I0420 00:23:57.477028 140068904109888 submission_runner.py:406] Time since start: 613.07s, 	Step: 1505, 	{'train/accuracy': 0.0715680792927742, 'train/loss': 5.438858985900879, 'validation/accuracy': 0.06384000182151794, 'validation/loss': 5.518141746520996, 'validation/num_examples': 50000, 'test/accuracy': 0.04610000178217888, 'test/loss': 5.737082481384277, 'test/num_examples': 10000, 'score': 556.0927176475525, 'total_duration': 613.0716388225555, 'accumulated_submission_time': 556.0927176475525, 'accumulated_eval_time': 56.38944411277771, 'accumulated_logging_time': 0.568462610244751}
I0420 00:23:57.484402 139864452355840 logging_writer.py:48] [1505] accumulated_eval_time=56.389444, accumulated_logging_time=0.568463, accumulated_submission_time=556.092718, global_step=1505, preemption_count=0, score=556.092718, test/accuracy=0.046100, test/loss=5.737082, test/num_examples=10000, total_duration=613.071639, train/accuracy=0.071568, train/loss=5.438859, validation/accuracy=0.063840, validation/loss=5.518142, validation/num_examples=50000
I0420 00:23:57.597763 140068904109888 checkpoints.py:356] Saving checkpoint at step: 1505
I0420 00:23:58.030534 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1505
I0420 00:23:58.031317 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1505.
I0420 00:24:30.820716 139864460748544 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.070537805557251, loss=5.984245300292969
I0420 00:25:04.730811 139891832772352 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8626440167427063, loss=5.963004112243652
I0420 00:25:38.901897 139864460748544 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8665345311164856, loss=5.850961685180664
I0420 00:26:12.928344 139891832772352 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9106679558753967, loss=5.752407550811768
I0420 00:26:46.779484 139864460748544 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0242490768432617, loss=5.8520331382751465
I0420 00:27:20.648167 139891832772352 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8400035500526428, loss=5.763426303863525
I0420 00:27:54.597073 139864460748544 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7885783314704895, loss=5.652599334716797
I0420 00:28:28.466329 139891832772352 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8675856590270996, loss=5.643564701080322
I0420 00:29:02.443457 139864460748544 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8880568146705627, loss=5.602960109710693
I0420 00:29:36.361500 139891832772352 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.869251549243927, loss=5.514301776885986
I0420 00:30:10.289231 139864460748544 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.010138988494873, loss=5.508857250213623
I0420 00:30:44.199931 139891832772352 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8559301495552063, loss=5.403364658355713
I0420 00:31:18.356371 139864460748544 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8061726093292236, loss=5.377763271331787
I0420 00:31:52.230948 139891832772352 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9104242324829102, loss=5.300149917602539
I0420 00:32:25.980015 139864460748544 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.822273313999176, loss=5.2782087326049805
I0420 00:32:28.122479 140068904109888 spec.py:298] Evaluating on the training split.
I0420 00:32:34.862454 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 00:32:42.562154 140068904109888 spec.py:326] Evaluating on the test split.
I0420 00:32:44.777839 140068904109888 submission_runner.py:406] Time since start: 1140.37s, 	Step: 3008, 	{'train/accuracy': 0.19114716351032257, 'train/loss': 4.260368824005127, 'validation/accuracy': 0.17431999742984772, 'validation/loss': 4.357460021972656, 'validation/num_examples': 50000, 'test/accuracy': 0.12520000338554382, 'test/loss': 4.772185325622559, 'test/num_examples': 10000, 'score': 1066.1593174934387, 'total_duration': 1140.372457742691, 'accumulated_submission_time': 1066.1593174934387, 'accumulated_eval_time': 73.04477286338806, 'accumulated_logging_time': 1.1268928050994873}
I0420 00:32:44.786075 139891832772352 logging_writer.py:48] [3008] accumulated_eval_time=73.044773, accumulated_logging_time=1.126893, accumulated_submission_time=1066.159317, global_step=3008, preemption_count=0, score=1066.159317, test/accuracy=0.125200, test/loss=4.772185, test/num_examples=10000, total_duration=1140.372458, train/accuracy=0.191147, train/loss=4.260369, validation/accuracy=0.174320, validation/loss=4.357460, validation/num_examples=50000
I0420 00:32:44.905248 140068904109888 checkpoints.py:356] Saving checkpoint at step: 3008
I0420 00:32:45.321848 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_3008
I0420 00:32:45.322600 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_3008.
I0420 00:33:16.932654 139864460748544 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8430625200271606, loss=5.196760177612305
I0420 00:33:50.877916 139891799201536 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8847596049308777, loss=5.18160343170166
I0420 00:34:24.802054 139864460748544 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8078742623329163, loss=5.118869781494141
I0420 00:34:58.746951 139891799201536 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7798467874526978, loss=5.081365585327148
I0420 00:35:32.522757 139864460748544 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8105080127716064, loss=5.042079925537109
I0420 00:36:06.470173 139891799201536 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.779150664806366, loss=4.983709335327148
I0420 00:36:40.292250 139864460748544 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.755989670753479, loss=4.984144687652588
I0420 00:37:14.217323 139891799201536 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8614035248756409, loss=4.9006452560424805
I0420 00:37:48.001849 139864460748544 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7435904145240784, loss=4.9027886390686035
I0420 00:38:21.884263 139891799201536 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7794322967529297, loss=4.807119846343994
I0420 00:38:55.888431 139864460748544 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7629159092903137, loss=4.761533737182617
I0420 00:39:29.779155 139891799201536 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.77736496925354, loss=4.769802093505859
I0420 00:40:03.793563 139864460748544 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.795315146446228, loss=4.705610275268555
I0420 00:40:37.813297 139891799201536 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7321071624755859, loss=4.636853218078613
I0420 00:41:11.630264 139864460748544 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7127767205238342, loss=4.688149929046631
I0420 00:41:15.377656 140068904109888 spec.py:298] Evaluating on the training split.
I0420 00:41:22.012489 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 00:41:29.991175 140068904109888 spec.py:326] Evaluating on the test split.
I0420 00:41:32.013260 140068904109888 submission_runner.py:406] Time since start: 1667.61s, 	Step: 4513, 	{'train/accuracy': 0.3252550959587097, 'train/loss': 3.363584518432617, 'validation/accuracy': 0.29889997839927673, 'validation/loss': 3.489629030227661, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.0616374015808105, 'test/num_examples': 10000, 'score': 1576.1911327838898, 'total_duration': 1667.6078803539276, 'accumulated_submission_time': 1576.1911327838898, 'accumulated_eval_time': 89.68035078048706, 'accumulated_logging_time': 1.6749224662780762}
I0420 00:41:32.020693 139891799201536 logging_writer.py:48] [4513] accumulated_eval_time=89.680351, accumulated_logging_time=1.674922, accumulated_submission_time=1576.191133, global_step=4513, preemption_count=0, score=1576.191133, test/accuracy=0.220100, test/loss=4.061637, test/num_examples=10000, total_duration=1667.607880, train/accuracy=0.325255, train/loss=3.363585, validation/accuracy=0.298900, validation/loss=3.489629, validation/num_examples=50000
I0420 00:41:32.121332 140068904109888 checkpoints.py:356] Saving checkpoint at step: 4513
I0420 00:41:32.535580 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_4513
I0420 00:41:32.536460 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_4513.
I0420 00:42:02.581898 139864460748544 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6961060166358948, loss=4.7331767082214355
I0420 00:42:36.669245 139892554184448 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7641331553459167, loss=4.711523056030273
I0420 00:43:11.063511 139864460748544 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6718487739562988, loss=4.551105499267578
I0420 00:43:45.076302 139892554184448 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7728201746940613, loss=4.594954490661621
I0420 00:44:19.188426 139864460748544 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.71603924036026, loss=4.503346920013428
I0420 00:44:53.469718 139892554184448 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6919673085212708, loss=4.489957332611084
I0420 00:45:27.708893 139864460748544 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7035509943962097, loss=4.542742729187012
I0420 00:46:01.706279 139892554184448 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6750723719596863, loss=4.487390995025635
I0420 00:46:35.769624 139864460748544 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6453459858894348, loss=4.386946678161621
I0420 00:47:09.929887 139892554184448 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6379612684249878, loss=4.419192790985107
I0420 00:47:44.121047 139864460748544 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6213583946228027, loss=4.357687950134277
I0420 00:48:18.226127 139892554184448 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6153779029846191, loss=4.275096893310547
I0420 00:48:52.387496 139864460748544 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6618186831474304, loss=4.314368724822998
I0420 00:49:26.433161 139892554184448 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6371917128562927, loss=4.268456935882568
I0420 00:50:00.467405 139864460748544 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.644924521446228, loss=4.239063739776611
I0420 00:50:02.637799 140068904109888 spec.py:298] Evaluating on the training split.
I0420 00:50:09.123484 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 00:50:16.955493 140068904109888 spec.py:326] Evaluating on the test split.
I0420 00:50:18.886437 140068904109888 submission_runner.py:406] Time since start: 2194.48s, 	Step: 6008, 	{'train/accuracy': 0.41095343232154846, 'train/loss': 2.8420662879943848, 'validation/accuracy': 0.38053998351097107, 'validation/loss': 2.9859683513641357, 'validation/num_examples': 50000, 'test/accuracy': 0.28790000081062317, 'test/loss': 3.5911738872528076, 'test/num_examples': 10000, 'score': 2086.2687759399414, 'total_duration': 2194.4810535907745, 'accumulated_submission_time': 2086.2687759399414, 'accumulated_eval_time': 105.92895460128784, 'accumulated_logging_time': 2.2021138668060303}
I0420 00:50:18.896307 139892554184448 logging_writer.py:48] [6008] accumulated_eval_time=105.928955, accumulated_logging_time=2.202114, accumulated_submission_time=2086.268776, global_step=6008, preemption_count=0, score=2086.268776, test/accuracy=0.287900, test/loss=3.591174, test/num_examples=10000, total_duration=2194.481054, train/accuracy=0.410953, train/loss=2.842066, validation/accuracy=0.380540, validation/loss=2.985968, validation/num_examples=50000
I0420 00:50:19.025696 140068904109888 checkpoints.py:356] Saving checkpoint at step: 6008
I0420 00:50:19.574924 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_6008
I0420 00:50:19.586553 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_6008.
I0420 00:50:51.326640 139864460748544 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6206598877906799, loss=4.254388809204102
I0420 00:51:25.420406 139892881336064 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.653783917427063, loss=4.265134811401367
I0420 00:51:59.731827 139864460748544 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6762022376060486, loss=4.336040496826172
I0420 00:52:33.873347 139892881336064 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6563807725906372, loss=4.221380233764648
I0420 00:53:08.247078 139864460748544 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6207595467567444, loss=4.207100868225098
I0420 00:53:42.371444 139892881336064 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6179600358009338, loss=4.212247848510742
I0420 00:54:16.699481 139864460748544 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6278485059738159, loss=4.137148380279541
I0420 00:54:50.870872 139892881336064 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6167488098144531, loss=4.164422988891602
I0420 00:55:25.159177 139864460748544 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5810735821723938, loss=4.0898966789245605
I0420 00:55:59.391630 139892881336064 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6087002754211426, loss=4.214875221252441
I0420 00:56:33.411413 139864460748544 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5840477347373962, loss=4.121637344360352
I0420 00:57:07.777356 139892881336064 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5960800647735596, loss=4.060304641723633
I0420 00:57:41.921580 139864460748544 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.566898763179779, loss=3.95418119430542
I0420 00:58:15.928421 139892881336064 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5788902640342712, loss=4.04536247253418
I0420 00:58:50.031181 139864460748544 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6011810898780823, loss=4.0456647872924805
I0420 00:58:50.037436 140068904109888 spec.py:298] Evaluating on the training split.
I0420 00:58:56.484830 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 00:59:04.422068 140068904109888 spec.py:326] Evaluating on the test split.
I0420 00:59:06.464716 140068904109888 submission_runner.py:406] Time since start: 2722.06s, 	Step: 7501, 	{'train/accuracy': 0.47345343232154846, 'train/loss': 2.640165090560913, 'validation/accuracy': 0.4425399899482727, 'validation/loss': 2.774324417114258, 'validation/num_examples': 50000, 'test/accuracy': 0.3354000151157379, 'test/loss': 3.3978633880615234, 'test/num_examples': 10000, 'score': 2596.697090625763, 'total_duration': 2722.059328556061, 'accumulated_submission_time': 2596.697090625763, 'accumulated_eval_time': 122.35618114471436, 'accumulated_logging_time': 2.906073808670044}
I0420 00:59:06.472783 139892881336064 logging_writer.py:48] [7501] accumulated_eval_time=122.356181, accumulated_logging_time=2.906074, accumulated_submission_time=2596.697091, global_step=7501, preemption_count=0, score=2596.697091, test/accuracy=0.335400, test/loss=3.397863, test/num_examples=10000, total_duration=2722.059329, train/accuracy=0.473453, train/loss=2.640165, validation/accuracy=0.442540, validation/loss=2.774324, validation/num_examples=50000
I0420 00:59:06.589407 140068904109888 checkpoints.py:356] Saving checkpoint at step: 7501
I0420 00:59:07.218147 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_7501
I0420 00:59:07.227151 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_7501.
I0420 00:59:41.163546 139864460748544 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5804858803749084, loss=4.124629497528076
I0420 01:00:15.267311 139892176709376 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5716037750244141, loss=4.042196273803711
I0420 01:00:49.191473 139864460748544 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5657635927200317, loss=3.976670503616333
I0420 01:01:23.346559 139892176709376 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5678920149803162, loss=3.921455144882202
I0420 01:01:57.606678 139864460748544 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5665216445922852, loss=4.047168254852295
I0420 01:02:31.642083 139892176709376 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.566684901714325, loss=4.004608154296875
I0420 01:03:05.793275 139864460748544 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5314859747886658, loss=3.910309314727783
I0420 01:03:39.718582 139892176709376 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5460406541824341, loss=3.94069242477417
I0420 01:04:13.832487 139864460748544 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5699849128723145, loss=4.015252113342285
I0420 01:04:48.071307 139892176709376 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5446082353591919, loss=3.990799903869629
I0420 01:05:22.151281 139864460748544 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5429273247718811, loss=3.9157962799072266
I0420 01:05:56.359416 139892176709376 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5169421434402466, loss=3.9097847938537598
I0420 01:06:30.669371 139864460748544 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5273456573486328, loss=3.864287853240967
I0420 01:07:04.761223 139892176709376 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5305624008178711, loss=3.7958312034606934
I0420 01:07:37.243941 140068904109888 spec.py:298] Evaluating on the training split.
I0420 01:07:43.732604 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 01:07:52.527266 140068904109888 spec.py:326] Evaluating on the test split.
I0420 01:07:55.604997 140068904109888 submission_runner.py:406] Time since start: 3251.20s, 	Step: 8997, 	{'train/accuracy': 0.5253108739852905, 'train/loss': 2.280762195587158, 'validation/accuracy': 0.4882799983024597, 'validation/loss': 2.4523775577545166, 'validation/num_examples': 50000, 'test/accuracy': 0.37380000948905945, 'test/loss': 3.1191022396087646, 'test/num_examples': 10000, 'score': 3106.686896085739, 'total_duration': 3251.199585199356, 'accumulated_submission_time': 3106.686896085739, 'accumulated_eval_time': 140.71718549728394, 'accumulated_logging_time': 3.676142454147339}
I0420 01:07:55.617191 139864460748544 logging_writer.py:48] [8997] accumulated_eval_time=140.717185, accumulated_logging_time=3.676142, accumulated_submission_time=3106.686896, global_step=8997, preemption_count=0, score=3106.686896, test/accuracy=0.373800, test/loss=3.119102, test/num_examples=10000, total_duration=3251.199585, train/accuracy=0.525311, train/loss=2.280762, validation/accuracy=0.488280, validation/loss=2.452378, validation/num_examples=50000
I0420 01:07:55.788616 140068904109888 checkpoints.py:356] Saving checkpoint at step: 8997
I0420 01:07:56.387652 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_8997
I0420 01:07:56.401329 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_8997.
I0420 01:07:57.812775 139892176709376 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5595596432685852, loss=3.988569498062134
I0420 01:08:31.897956 139888271808256 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5371587872505188, loss=3.8992137908935547
I0420 01:09:05.984967 139892176709376 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5140059590339661, loss=3.850491523742676
I0420 01:09:40.070539 139888271808256 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5021560788154602, loss=3.7378597259521484
I0420 01:10:14.171574 139892176709376 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5148100852966309, loss=3.803114175796509
I0420 01:10:48.392806 139888271808256 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5062522888183594, loss=3.792100667953491
I0420 01:11:22.596667 139892176709376 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5070033669471741, loss=3.800734519958496
I0420 01:11:56.901815 139888271808256 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5058174133300781, loss=3.832888603210449
I0420 01:12:30.983286 139892176709376 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5092745423316956, loss=3.804936408996582
I0420 01:13:05.270494 139888271808256 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.49816086888313293, loss=3.794118881225586
I0420 01:13:39.519212 139892176709376 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.49989983439445496, loss=3.7359061241149902
I0420 01:14:13.660084 139888271808256 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5007254481315613, loss=3.775733470916748
I0420 01:14:47.752400 139892176709376 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5069093108177185, loss=3.750960350036621
I0420 01:15:21.872212 139888271808256 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5138450860977173, loss=3.752772092819214
I0420 01:15:55.940836 139892176709376 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.48997053503990173, loss=3.645810604095459
I0420 01:16:26.548367 140068904109888 spec.py:298] Evaluating on the training split.
I0420 01:16:33.231974 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 01:16:42.465775 140068904109888 spec.py:326] Evaluating on the test split.
I0420 01:16:44.593006 140068904109888 submission_runner.py:406] Time since start: 3780.19s, 	Step: 10491, 	{'train/accuracy': 0.5982341766357422, 'train/loss': 1.9569659233093262, 'validation/accuracy': 0.5369399785995483, 'validation/loss': 2.2377190589904785, 'validation/num_examples': 50000, 'test/accuracy': 0.4125000238418579, 'test/loss': 2.9078216552734375, 'test/num_examples': 10000, 'score': 3616.8029968738556, 'total_duration': 3780.18762922287, 'accumulated_submission_time': 3616.8029968738556, 'accumulated_eval_time': 158.76182413101196, 'accumulated_logging_time': 4.484162092208862}
I0420 01:16:44.601866 139888271808256 logging_writer.py:48] [10491] accumulated_eval_time=158.761824, accumulated_logging_time=4.484162, accumulated_submission_time=3616.802997, global_step=10491, preemption_count=0, score=3616.802997, test/accuracy=0.412500, test/loss=2.907822, test/num_examples=10000, total_duration=3780.187629, train/accuracy=0.598234, train/loss=1.956966, validation/accuracy=0.536940, validation/loss=2.237719, validation/num_examples=50000
I0420 01:16:44.749823 140068904109888 checkpoints.py:356] Saving checkpoint at step: 10491
I0420 01:16:45.369275 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_10491
I0420 01:16:45.378600 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_10491.
I0420 01:16:48.791558 139892176709376 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4960992634296417, loss=3.765267848968506
I0420 01:17:22.648451 139888255022848 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.48325595259666443, loss=3.7149546146392822
I0420 01:17:56.461368 139892176709376 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5007174015045166, loss=3.7066032886505127
I0420 01:18:30.481786 139888255022848 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5138611793518066, loss=3.688541889190674
I0420 01:19:04.379141 139892176709376 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.49173855781555176, loss=3.7161173820495605
I0420 01:19:38.345886 139888255022848 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5055961012840271, loss=3.644627332687378
I0420 01:20:12.382602 139892176709376 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4806854724884033, loss=3.687290668487549
I0420 01:20:46.323756 139888255022848 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4745839238166809, loss=3.6428933143615723
I0420 01:21:20.165986 139892176709376 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.47435858845710754, loss=3.5729215145111084
I0420 01:21:53.961514 139888255022848 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.4877699613571167, loss=3.6219635009765625
I0420 01:22:27.922519 139892176709376 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.4890318810939789, loss=3.6635735034942627
I0420 01:23:01.871770 139888255022848 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5008761286735535, loss=3.638578176498413
I0420 01:23:35.701543 139892176709376 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.47483888268470764, loss=3.63881778717041
I0420 01:24:09.661638 139888255022848 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4858337938785553, loss=3.593796491622925
I0420 01:24:43.513715 139892176709376 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4884372055530548, loss=3.602595090866089
I0420 01:25:15.462888 140068904109888 spec.py:298] Evaluating on the training split.
I0420 01:25:22.074612 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 01:25:32.113623 140068904109888 spec.py:326] Evaluating on the test split.
I0420 01:25:34.264464 140068904109888 submission_runner.py:406] Time since start: 4309.86s, 	Step: 11996, 	{'train/accuracy': 0.6256377696990967, 'train/loss': 1.8207343816757202, 'validation/accuracy': 0.5673199892044067, 'validation/loss': 2.0809600353240967, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.729384422302246, 'test/num_examples': 10000, 'score': 4126.86368727684, 'total_duration': 4309.859075546265, 'accumulated_submission_time': 4126.86368727684, 'accumulated_eval_time': 177.5633647441864, 'accumulated_logging_time': 5.27420711517334}
I0420 01:25:34.273443 139888255022848 logging_writer.py:48] [11996] accumulated_eval_time=177.563365, accumulated_logging_time=5.274207, accumulated_submission_time=4126.863687, global_step=11996, preemption_count=0, score=4126.863687, test/accuracy=0.443500, test/loss=2.729384, test/num_examples=10000, total_duration=4309.859076, train/accuracy=0.625638, train/loss=1.820734, validation/accuracy=0.567320, validation/loss=2.080960, validation/num_examples=50000
I0420 01:25:34.403067 140068904109888 checkpoints.py:356] Saving checkpoint at step: 11996
I0420 01:25:34.967239 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_11996
I0420 01:25:34.976521 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_11996.
I0420 01:25:36.695775 139892176709376 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4717145562171936, loss=3.5060315132141113
I0420 01:26:10.483645 139890423486208 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.45898404717445374, loss=3.508305788040161
I0420 01:26:44.453366 139892176709376 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.47681424021720886, loss=3.6004486083984375
I0420 01:27:18.333972 139890423486208 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.4670031666755676, loss=3.5269978046417236
I0420 01:27:52.184304 139892176709376 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5027386546134949, loss=3.644998788833618
I0420 01:28:25.935428 139890423486208 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.47575536370277405, loss=3.5615878105163574
I0420 01:28:59.850574 139892176709376 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.469503253698349, loss=3.552401065826416
I0420 01:29:33.847132 139890423486208 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4862816333770752, loss=3.5968987941741943
I0420 01:30:07.778290 139892176709376 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.47404319047927856, loss=3.609597682952881
I0420 01:30:41.727689 139890423486208 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.4664546847343445, loss=3.515594482421875
I0420 01:31:15.591429 139892176709376 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.46898195147514343, loss=3.5240368843078613
I0420 01:31:49.536261 139890423486208 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.49841028451919556, loss=3.6376869678497314
I0420 01:32:23.376716 139892176709376 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4561925232410431, loss=3.481265068054199
I0420 01:32:57.233656 139890423486208 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.46899622678756714, loss=3.502969741821289
I0420 01:33:31.132538 139892176709376 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.47576966881752014, loss=3.5288751125335693
I0420 01:34:04.978902 139890423486208 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.47682812809944153, loss=3.551577568054199
I0420 01:34:04.985452 140068904109888 spec.py:298] Evaluating on the training split.
I0420 01:34:12.423889 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 01:34:22.298255 140068904109888 spec.py:326] Evaluating on the test split.
I0420 01:34:24.408837 140068904109888 submission_runner.py:406] Time since start: 4840.00s, 	Step: 13501, 	{'train/accuracy': 0.6454878449440002, 'train/loss': 1.7378125190734863, 'validation/accuracy': 0.5874599814414978, 'validation/loss': 1.993247628211975, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.6715526580810547, 'test/num_examples': 10000, 'score': 4636.850937366486, 'total_duration': 4840.00221157074, 'accumulated_submission_time': 4636.850937366486, 'accumulated_eval_time': 196.98548936843872, 'accumulated_logging_time': 5.988356590270996}
I0420 01:34:24.422634 139892176709376 logging_writer.py:48] [13501] accumulated_eval_time=196.985489, accumulated_logging_time=5.988357, accumulated_submission_time=4636.850937, global_step=13501, preemption_count=0, score=4636.850937, test/accuracy=0.455400, test/loss=2.671553, test/num_examples=10000, total_duration=4840.002212, train/accuracy=0.645488, train/loss=1.737813, validation/accuracy=0.587460, validation/loss=1.993248, validation/num_examples=50000
I0420 01:34:24.597709 140068904109888 checkpoints.py:356] Saving checkpoint at step: 13501
I0420 01:34:25.247040 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_13501
I0420 01:34:25.256177 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_13501.
I0420 01:34:59.161552 139890423486208 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4634460210800171, loss=3.4934275150299072
I0420 01:35:33.260105 139890398308096 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.46834710240364075, loss=3.5227227210998535
I0420 01:36:07.181264 139890423486208 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4663228988647461, loss=3.5436439514160156
I0420 01:36:41.334152 139890398308096 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.47989287972450256, loss=3.5455727577209473
I0420 01:37:15.497312 139890423486208 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.47413498163223267, loss=3.512660503387451
I0420 01:37:49.569681 139890398308096 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.46264705061912537, loss=3.3908309936523438
I0420 01:38:23.833516 139890423486208 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.45152634382247925, loss=3.396365165710449
I0420 01:38:57.954984 139890398308096 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.4768454432487488, loss=3.4814300537109375
I0420 01:39:31.900943 139890423486208 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.47470083832740784, loss=3.496673583984375
I0420 01:40:05.957538 139890398308096 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.45447632670402527, loss=3.405764579772949
I0420 01:40:40.068129 139890423486208 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.45410478115081787, loss=3.382889986038208
I0420 01:41:14.105587 139890398308096 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4436020255088806, loss=3.4138760566711426
I0420 01:41:48.152778 139890423486208 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.4553240239620209, loss=3.459562301635742
I0420 01:42:22.165010 139890398308096 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.45165517926216125, loss=3.4761199951171875
I0420 01:42:55.436999 140068904109888 spec.py:298] Evaluating on the training split.
I0420 01:43:02.308387 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 01:43:12.108983 140068904109888 spec.py:326] Evaluating on the test split.
I0420 01:43:14.230114 140068904109888 submission_runner.py:406] Time since start: 5369.82s, 	Step: 14999, 	{'train/accuracy': 0.6673110723495483, 'train/loss': 1.5694421529769897, 'validation/accuracy': 0.6063599586486816, 'validation/loss': 1.8422223329544067, 'validation/num_examples': 50000, 'test/accuracy': 0.47840002179145813, 'test/loss': 2.5081822872161865, 'test/num_examples': 10000, 'score': 5147.007830381393, 'total_duration': 5369.823487520218, 'accumulated_submission_time': 5147.007830381393, 'accumulated_eval_time': 215.77732944488525, 'accumulated_logging_time': 6.840044021606445}
I0420 01:43:14.239699 139890423486208 logging_writer.py:48] [14999] accumulated_eval_time=215.777329, accumulated_logging_time=6.840044, accumulated_submission_time=5147.007830, global_step=14999, preemption_count=0, score=5147.007830, test/accuracy=0.478400, test/loss=2.508182, test/num_examples=10000, total_duration=5369.823488, train/accuracy=0.667311, train/loss=1.569442, validation/accuracy=0.606360, validation/loss=1.842222, validation/num_examples=50000
I0420 01:43:14.413721 140068904109888 checkpoints.py:356] Saving checkpoint at step: 14999
I0420 01:43:15.046046 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_14999
I0420 01:43:15.054594 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_14999.
I0420 01:43:15.763898 139890398308096 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.46786266565322876, loss=3.547532558441162
I0420 01:43:49.727791 139890389915392 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.44873109459877014, loss=3.3995347023010254
I0420 01:44:23.623792 139890398308096 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.46325767040252686, loss=3.40179443359375
I0420 01:44:57.454419 139890389915392 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.4796101748943329, loss=3.5159332752227783
I0420 01:45:31.200684 139890398308096 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.4627259373664856, loss=3.348531723022461
I0420 01:46:05.171512 139890389915392 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.4610554575920105, loss=3.4295716285705566
I0420 01:46:38.953950 139890398308096 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4645850360393524, loss=3.41849422454834
I0420 01:47:12.970845 139890389915392 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.4696029722690582, loss=3.457991600036621
I0420 01:47:46.905546 139890398308096 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.4465431272983551, loss=3.4039671421051025
I0420 01:48:20.844866 139890389915392 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.4719071686267853, loss=3.417113780975342
I0420 01:48:54.687983 139890398308096 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.4563591182231903, loss=3.366027593612671
I0420 01:49:28.504780 139890389915392 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.4751858115196228, loss=3.438856840133667
I0420 01:50:02.534127 139890398308096 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.4580911099910736, loss=3.4309709072113037
I0420 01:50:36.383606 139890389915392 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.443154901266098, loss=3.365799903869629
I0420 01:51:10.244946 139890398308096 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.45302489399909973, loss=3.3440165519714355
I0420 01:51:44.117616 139890389915392 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.45391255617141724, loss=3.3537325859069824
I0420 01:51:45.227109 140068904109888 spec.py:298] Evaluating on the training split.
I0420 01:51:52.678310 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 01:52:02.821354 140068904109888 spec.py:326] Evaluating on the test split.
I0420 01:52:04.964280 140068904109888 submission_runner.py:406] Time since start: 5900.56s, 	Step: 16505, 	{'train/accuracy': 0.6804846525192261, 'train/loss': 1.6167914867401123, 'validation/accuracy': 0.6204999685287476, 'validation/loss': 1.8714947700500488, 'validation/num_examples': 50000, 'test/accuracy': 0.49660003185272217, 'test/loss': 2.5076074600219727, 'test/num_examples': 10000, 'score': 5657.1570773124695, 'total_duration': 5900.557867527008, 'accumulated_submission_time': 5657.1570773124695, 'accumulated_eval_time': 235.51344180107117, 'accumulated_logging_time': 7.668473720550537}
I0420 01:52:04.977971 139890398308096 logging_writer.py:48] [16505] accumulated_eval_time=235.513442, accumulated_logging_time=7.668474, accumulated_submission_time=5657.157077, global_step=16505, preemption_count=0, score=5657.157077, test/accuracy=0.496600, test/loss=2.507607, test/num_examples=10000, total_duration=5900.557868, train/accuracy=0.680485, train/loss=1.616791, validation/accuracy=0.620500, validation/loss=1.871495, validation/num_examples=50000
I0420 01:52:05.164481 140068904109888 checkpoints.py:356] Saving checkpoint at step: 16505
I0420 01:52:05.814021 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_16505
I0420 01:52:05.825143 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_16505.
I0420 01:52:38.205253 139890389915392 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.462362676858902, loss=3.3173465728759766
I0420 01:53:11.968786 139890381522688 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.46047332882881165, loss=3.389159917831421
I0420 01:53:46.033893 139890389915392 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.45361217856407166, loss=3.2904481887817383
I0420 01:54:20.133226 139890381522688 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.452695369720459, loss=3.3573660850524902
I0420 01:54:54.035587 139890389915392 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.43915823101997375, loss=3.3306989669799805
I0420 01:55:27.670836 139890381522688 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.44816675782203674, loss=3.3105552196502686
I0420 01:56:01.476611 139890389915392 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.4493511915206909, loss=3.3805654048919678
I0420 01:56:35.367228 139890381522688 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4691791832447052, loss=3.4197990894317627
I0420 01:57:09.313698 139890389915392 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.45450708270072937, loss=3.3731746673583984
I0420 01:57:43.133371 139890381522688 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.44670602679252625, loss=3.363224983215332
I0420 01:58:16.903025 139890389915392 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.4392923414707184, loss=3.3218092918395996
I0420 01:58:50.770981 139890381522688 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.4568743109703064, loss=3.414456605911255
I0420 01:59:24.556602 139890389915392 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.4540298879146576, loss=3.3500075340270996
I0420 01:59:58.337289 139890381522688 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.44577300548553467, loss=3.370903491973877
I0420 02:00:32.186086 139890389915392 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.45860496163368225, loss=3.3769538402557373
I0420 02:00:35.909624 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:00:42.845577 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:00:53.052841 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:00:55.184718 140068904109888 submission_runner.py:406] Time since start: 6430.78s, 	Step: 18013, 	{'train/accuracy': 0.696707546710968, 'train/loss': 1.455159068107605, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.730525255203247, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.369504690170288, 'test/num_examples': 10000, 'score': 6167.218094587326, 'total_duration': 6430.778029680252, 'accumulated_submission_time': 6167.218094587326, 'accumulated_eval_time': 254.7871961593628, 'accumulated_logging_time': 8.533998012542725}
I0420 02:00:55.193727 139890381522688 logging_writer.py:48] [18013] accumulated_eval_time=254.787196, accumulated_logging_time=8.533998, accumulated_submission_time=6167.218095, global_step=18013, preemption_count=0, score=6167.218095, test/accuracy=0.510800, test/loss=2.369505, test/num_examples=10000, total_duration=6430.778030, train/accuracy=0.696708, train/loss=1.455159, validation/accuracy=0.632200, validation/loss=1.730525, validation/num_examples=50000
I0420 02:00:55.361766 140068904109888 checkpoints.py:356] Saving checkpoint at step: 18013
I0420 02:00:55.997581 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_18013
I0420 02:00:56.007197 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_18013.
I0420 02:01:25.836826 139890389915392 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.46312209963798523, loss=3.417163372039795
I0420 02:01:59.514157 139886883501824 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.46291297674179077, loss=3.3579821586608887
I0420 02:02:33.330476 139890389915392 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4497108459472656, loss=3.366880416870117
I0420 02:03:07.155578 139886883501824 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.44782179594039917, loss=3.317552328109741
I0420 02:03:40.870451 139890389915392 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4509251117706299, loss=3.3026697635650635
I0420 02:04:14.796743 139886883501824 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.4524775445461273, loss=3.3583452701568604
I0420 02:04:48.532732 139890389915392 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.44454729557037354, loss=3.2593421936035156
I0420 02:05:22.338570 139886883501824 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.45366519689559937, loss=3.329373836517334
I0420 02:05:56.173851 139890389915392 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.44242581725120544, loss=3.310192584991455
I0420 02:06:30.021706 139886883501824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.45518097281455994, loss=3.2955520153045654
I0420 02:07:03.832142 139890389915392 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.4475180506706238, loss=3.2192206382751465
I0420 02:07:37.642857 139886883501824 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.44305869936943054, loss=3.3164169788360596
I0420 02:08:11.538538 139890389915392 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.4408951699733734, loss=3.327371597290039
I0420 02:08:45.242753 139886883501824 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.44992125034332275, loss=3.341562271118164
I0420 02:09:19.098833 139890389915392 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.45020565390586853, loss=3.2771058082580566
I0420 02:09:26.231426 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:09:33.763566 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:09:43.995359 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:09:46.136337 140068904109888 submission_runner.py:406] Time since start: 6961.73s, 	Step: 19523, 	{'train/accuracy': 0.7276785373687744, 'train/loss': 1.3121131658554077, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.7250874042510986, 'validation/num_examples': 50000, 'test/accuracy': 0.5026000142097473, 'test/loss': 2.4261507987976074, 'test/num_examples': 10000, 'score': 6677.419997692108, 'total_duration': 6961.729952335358, 'accumulated_submission_time': 6677.419997692108, 'accumulated_eval_time': 274.69107127189636, 'accumulated_logging_time': 9.359956502914429}
I0420 02:09:46.150867 139886883501824 logging_writer.py:48] [19523] accumulated_eval_time=274.691071, accumulated_logging_time=9.359957, accumulated_submission_time=6677.419998, global_step=19523, preemption_count=0, score=6677.419998, test/accuracy=0.502600, test/loss=2.426151, test/num_examples=10000, total_duration=6961.729952, train/accuracy=0.727679, train/loss=1.312113, validation/accuracy=0.632200, validation/loss=1.725087, validation/num_examples=50000
I0420 02:09:46.319225 140068904109888 checkpoints.py:356] Saving checkpoint at step: 19523
I0420 02:09:46.980039 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_19523
I0420 02:09:46.989428 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_19523.
I0420 02:10:13.473292 139890389915392 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.4471701979637146, loss=3.293397903442383
I0420 02:10:47.272988 139886875109120 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4430958926677704, loss=3.233092784881592
I0420 02:11:21.183099 139890389915392 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4296218454837799, loss=3.1992874145507812
I0420 02:11:55.015908 139886875109120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.4490521550178528, loss=3.2882890701293945
I0420 02:12:28.878536 139890389915392 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.4653228521347046, loss=3.343160629272461
I0420 02:13:02.842849 139886875109120 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.44790926575660706, loss=3.297938108444214
I0420 02:13:36.695393 139890389915392 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4488774836063385, loss=3.350125789642334
I0420 02:14:10.750110 139886875109120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4556732475757599, loss=3.3444392681121826
I0420 02:14:44.702218 139890389915392 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.4314707815647125, loss=3.182821750640869
I0420 02:15:18.609438 139886875109120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.44739535450935364, loss=3.2404074668884277
I0420 02:15:52.358797 139890389915392 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.45569542050361633, loss=3.292956829071045
I0420 02:16:26.110968 139886875109120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.4474206566810608, loss=3.3037145137786865
I0420 02:16:59.920778 139890389915392 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.46573498845100403, loss=3.2530324459075928
I0420 02:17:33.939205 139886875109120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.4498085677623749, loss=3.262589693069458
I0420 02:18:07.942989 139890389915392 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4484850764274597, loss=3.3633129596710205
I0420 02:18:17.270138 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:18:24.313525 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:18:34.643222 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:18:36.773750 140068904109888 submission_runner.py:406] Time since start: 7492.37s, 	Step: 21029, 	{'train/accuracy': 0.7311065196990967, 'train/loss': 1.2657337188720703, 'validation/accuracy': 0.6487599611282349, 'validation/loss': 1.6399296522140503, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.327130079269409, 'test/num_examples': 10000, 'score': 7187.675060033798, 'total_duration': 7492.367063522339, 'accumulated_submission_time': 7187.675060033798, 'accumulated_eval_time': 294.19334650039673, 'accumulated_logging_time': 10.219239711761475}
I0420 02:18:36.783070 139886875109120 logging_writer.py:48] [21029] accumulated_eval_time=294.193347, accumulated_logging_time=10.219240, accumulated_submission_time=7187.675060, global_step=21029, preemption_count=0, score=7187.675060, test/accuracy=0.513400, test/loss=2.327130, test/num_examples=10000, total_duration=7492.367064, train/accuracy=0.731107, train/loss=1.265734, validation/accuracy=0.648760, validation/loss=1.639930, validation/num_examples=50000
I0420 02:18:36.944881 140068904109888 checkpoints.py:356] Saving checkpoint at step: 21029
I0420 02:18:37.584094 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_21029
I0420 02:18:37.595623 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_21029.
I0420 02:19:02.044785 139890389915392 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.45142969489097595, loss=3.260495662689209
I0420 02:19:36.021180 139886866716416 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4453061521053314, loss=3.291370153427124
I0420 02:20:10.238646 139890389915392 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4576234519481659, loss=3.253466844558716
I0420 02:20:44.149862 139886866716416 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.45635223388671875, loss=3.2743167877197266
I0420 02:21:18.112666 139890389915392 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.4424184262752533, loss=3.214228391647339
I0420 02:21:52.153220 139886866716416 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.4366922676563263, loss=3.1905465126037598
I0420 02:22:26.234825 139890389915392 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.4504159390926361, loss=3.2126200199127197
I0420 02:23:00.059773 139886866716416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.44418999552726746, loss=3.183826208114624
I0420 02:23:34.260285 139890389915392 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.4353185296058655, loss=3.182157516479492
I0420 02:24:08.267646 139886866716416 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4484228789806366, loss=3.2461633682250977
I0420 02:24:42.204493 139890389915392 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.46655556559562683, loss=3.273538827896118
I0420 02:25:16.112730 139886866716416 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.44900965690612793, loss=3.275639295578003
I0420 02:25:50.107918 139890389915392 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.4388829171657562, loss=3.224919557571411
I0420 02:26:24.163663 139886866716416 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4441626965999603, loss=3.2513558864593506
I0420 02:26:58.161578 139890389915392 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.43277841806411743, loss=3.1821517944335938
I0420 02:27:07.868957 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:27:15.498526 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:27:25.718348 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:27:27.834493 140068904109888 submission_runner.py:406] Time since start: 8023.43s, 	Step: 22530, 	{'train/accuracy': 0.7328802347183228, 'train/loss': 1.2759041786193848, 'validation/accuracy': 0.6511600017547607, 'validation/loss': 1.6299868822097778, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.295003652572632, 'test/num_examples': 10000, 'score': 7697.922146081924, 'total_duration': 8023.428182601929, 'accumulated_submission_time': 7697.922146081924, 'accumulated_eval_time': 314.1579234600067, 'accumulated_logging_time': 11.048299789428711}
I0420 02:27:27.848762 139886866716416 logging_writer.py:48] [22530] accumulated_eval_time=314.157923, accumulated_logging_time=11.048300, accumulated_submission_time=7697.922146, global_step=22530, preemption_count=0, score=7697.922146, test/accuracy=0.523200, test/loss=2.295004, test/num_examples=10000, total_duration=8023.428183, train/accuracy=0.732880, train/loss=1.275904, validation/accuracy=0.651160, validation/loss=1.629987, validation/num_examples=50000
I0420 02:27:28.014082 140068904109888 checkpoints.py:356] Saving checkpoint at step: 22530
I0420 02:27:28.669499 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_22530
I0420 02:27:28.681409 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_22530.
I0420 02:27:52.704145 139890389915392 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.441019207239151, loss=3.159247636795044
I0420 02:28:26.720789 139886858323712 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.45601415634155273, loss=3.2193877696990967
I0420 02:29:00.706798 139890389915392 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.4485919773578644, loss=3.303297758102417
I0420 02:29:34.645119 139886858323712 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.4421035945415497, loss=3.1976428031921387
I0420 02:30:08.598922 139890389915392 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.43952688574790955, loss=3.1768314838409424
I0420 02:30:42.629564 139886858323712 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.44476476311683655, loss=3.217381238937378
I0420 02:31:16.604736 139890389915392 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4389306306838989, loss=3.220468282699585
I0420 02:31:50.450420 139886858323712 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.43351227045059204, loss=3.168073892593384
I0420 02:32:24.345569 139890389915392 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4461342692375183, loss=3.184169054031372
I0420 02:32:58.216464 139886858323712 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4541543424129486, loss=3.2197012901306152
I0420 02:33:32.145174 139890389915392 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.4322633147239685, loss=3.1189939975738525
I0420 02:34:06.291461 139886858323712 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.45909467339515686, loss=3.289212226867676
I0420 02:34:40.286756 139890389915392 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.45051154494285583, loss=3.2679386138916016
I0420 02:35:14.227089 139886858323712 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4493662118911743, loss=3.194979429244995
I0420 02:35:48.264374 139890389915392 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.4495353102684021, loss=3.160301923751831
I0420 02:35:58.906300 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:36:06.021127 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:36:16.399563 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:36:18.339477 140068904109888 submission_runner.py:406] Time since start: 8553.93s, 	Step: 24033, 	{'train/accuracy': 0.7382413744926453, 'train/loss': 1.2593361139297485, 'validation/accuracy': 0.6553199887275696, 'validation/loss': 1.6311320066452026, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.2739436626434326, 'test/num_examples': 10000, 'score': 8208.122561693192, 'total_duration': 8553.933279752731, 'accumulated_submission_time': 8208.122561693192, 'accumulated_eval_time': 333.590256690979, 'accumulated_logging_time': 11.901039123535156}
I0420 02:36:18.349148 139886858323712 logging_writer.py:48] [24033] accumulated_eval_time=333.590257, accumulated_logging_time=11.901039, accumulated_submission_time=8208.122562, global_step=24033, preemption_count=0, score=8208.122562, test/accuracy=0.530600, test/loss=2.273944, test/num_examples=10000, total_duration=8553.933280, train/accuracy=0.738241, train/loss=1.259336, validation/accuracy=0.655320, validation/loss=1.631132, validation/num_examples=50000
I0420 02:36:18.537297 140068904109888 checkpoints.py:356] Saving checkpoint at step: 24033
I0420 02:36:19.203119 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_24033
I0420 02:36:19.214761 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_24033.
I0420 02:36:42.371276 139890389915392 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.47245725989341736, loss=3.297006845474243
I0420 02:37:16.211778 139888628332288 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.4526650905609131, loss=3.2324836254119873
I0420 02:37:50.312341 139890389915392 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.43488407135009766, loss=3.1146934032440186
I0420 02:38:24.348831 139888628332288 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4394371509552002, loss=3.1508469581604004
I0420 02:38:58.380286 139890389915392 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.43904194235801697, loss=3.189602851867676
I0420 02:39:32.484158 139888628332288 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4409867227077484, loss=3.1902942657470703
I0420 02:40:06.590378 139890389915392 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.44227319955825806, loss=3.223970890045166
I0420 02:40:40.718653 139888628332288 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.42844974994659424, loss=3.1103591918945312
I0420 02:41:14.804856 139890389915392 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.43478885293006897, loss=3.1277456283569336
I0420 02:41:48.863177 139888628332288 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.4595889747142792, loss=3.2078046798706055
I0420 02:42:22.826940 139890389915392 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.44232895970344543, loss=3.1864278316497803
I0420 02:42:56.889133 139888628332288 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4425475299358368, loss=3.1817994117736816
I0420 02:43:30.759746 139890389915392 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.44778066873550415, loss=3.2625796794891357
I0420 02:44:04.688674 139888628332288 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.43951430916786194, loss=3.213702917098999
I0420 02:44:38.754581 139890389915392 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.44399169087409973, loss=3.156777858734131
I0420 02:44:49.272757 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:44:56.446359 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:45:07.057573 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:45:09.176681 140068904109888 submission_runner.py:406] Time since start: 9084.77s, 	Step: 25533, 	{'train/accuracy': 0.7434829473495483, 'train/loss': 1.288411021232605, 'validation/accuracy': 0.6614800095558167, 'validation/loss': 1.6455656290054321, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2954516410827637, 'test/num_examples': 10000, 'score': 8718.152958631516, 'total_duration': 9084.770256757736, 'accumulated_submission_time': 8718.152958631516, 'accumulated_eval_time': 353.49310636520386, 'accumulated_logging_time': 12.78539752960205}
I0420 02:45:09.186472 139888628332288 logging_writer.py:48] [25533] accumulated_eval_time=353.493106, accumulated_logging_time=12.785398, accumulated_submission_time=8718.152959, global_step=25533, preemption_count=0, score=8718.152959, test/accuracy=0.538600, test/loss=2.295452, test/num_examples=10000, total_duration=9084.770257, train/accuracy=0.743483, train/loss=1.288411, validation/accuracy=0.661480, validation/loss=1.645566, validation/num_examples=50000
I0420 02:45:09.345461 140068904109888 checkpoints.py:356] Saving checkpoint at step: 25533
I0420 02:45:09.985043 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_25533
I0420 02:45:09.995332 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_25533.
I0420 02:45:32.857212 139890389915392 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4441852867603302, loss=3.1250016689300537
I0420 02:46:06.712490 139888812840704 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.44605523347854614, loss=3.1975221633911133
I0420 02:46:40.347489 139890389915392 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.430046945810318, loss=3.159891128540039
I0420 02:47:14.192517 139888812840704 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.44115331768989563, loss=3.1274499893188477
I0420 02:47:48.000739 139890389915392 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4366868734359741, loss=3.136298179626465
I0420 02:48:21.917883 139888812840704 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.42995914816856384, loss=3.117715358734131
I0420 02:48:55.728845 139890389915392 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4376581907272339, loss=3.159529685974121
I0420 02:49:29.634088 139888812840704 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.45335653424263, loss=3.2171566486358643
I0420 02:50:03.496578 139890389915392 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4430961012840271, loss=3.1448802947998047
I0420 02:50:37.225815 139888812840704 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.4353373944759369, loss=3.1194205284118652
I0420 02:51:11.111384 139890389915392 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.43982914090156555, loss=3.1447970867156982
I0420 02:51:44.970379 139888812840704 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.43259602785110474, loss=3.099198579788208
I0420 02:52:18.765766 139890389915392 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.444307804107666, loss=3.14156174659729
I0420 02:52:52.578603 139888812840704 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4527296721935272, loss=3.1426219940185547
I0420 02:53:26.494671 139890389915392 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.430807501077652, loss=3.113602876663208
I0420 02:53:40.087474 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:53:47.499906 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:53:57.903041 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:54:00.023994 140068904109888 submission_runner.py:406] Time since start: 9615.62s, 	Step: 27042, 	{'train/accuracy': 0.7563377022743225, 'train/loss': 1.1697587966918945, 'validation/accuracy': 0.672279953956604, 'validation/loss': 1.5339521169662476, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.1986913681030273, 'test/num_examples': 10000, 'score': 9228.215273141861, 'total_duration': 9615.617651224136, 'accumulated_submission_time': 9228.215273141861, 'accumulated_eval_time': 373.42863416671753, 'accumulated_logging_time': 13.6155526638031}
I0420 02:54:00.033394 139888812840704 logging_writer.py:48] [27042] accumulated_eval_time=373.428634, accumulated_logging_time=13.615553, accumulated_submission_time=9228.215273, global_step=27042, preemption_count=0, score=9228.215273, test/accuracy=0.541700, test/loss=2.198691, test/num_examples=10000, total_duration=9615.617651, train/accuracy=0.756338, train/loss=1.169759, validation/accuracy=0.672280, validation/loss=1.533952, validation/num_examples=50000
I0420 02:54:00.202884 140068904109888 checkpoints.py:356] Saving checkpoint at step: 27042
I0420 02:54:00.859752 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_27042
I0420 02:54:00.870598 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_27042.
I0420 02:54:21.010524 139890389915392 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.43922320008277893, loss=3.1284561157226562
I0420 02:54:54.765419 139888586368768 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.44626089930534363, loss=3.1992998123168945
I0420 02:55:28.550995 139890389915392 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.4293537735939026, loss=3.1169822216033936
I0420 02:56:02.246177 139888586368768 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.44242674112319946, loss=3.1305997371673584
I0420 02:56:35.963326 139890389915392 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.43698355555534363, loss=3.1144888401031494
I0420 02:57:09.702574 139888586368768 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.4476570785045624, loss=3.169480562210083
I0420 02:57:43.441363 139890389915392 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.44119107723236084, loss=3.140515089035034
I0420 02:58:17.409811 139888586368768 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4193304479122162, loss=3.0771992206573486
I0420 02:58:51.171929 139890389915392 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.43693000078201294, loss=3.1180672645568848
I0420 02:59:24.321820 140068904109888 spec.py:298] Evaluating on the training split.
I0420 02:59:31.735030 140068904109888 spec.py:310] Evaluating on the validation split.
I0420 02:59:42.697558 140068904109888 spec.py:326] Evaluating on the test split.
I0420 02:59:44.666091 140068904109888 submission_runner.py:406] Time since start: 9960.26s, 	Step: 28000, 	{'train/accuracy': 0.7609614133834839, 'train/loss': 1.1848136186599731, 'validation/accuracy': 0.6717399954795837, 'validation/loss': 1.576155424118042, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.2461929321289062, 'test/num_examples': 10000, 'score': 9551.647817850113, 'total_duration': 9960.259217977524, 'accumulated_submission_time': 9551.647817850113, 'accumulated_eval_time': 393.7713861465454, 'accumulated_logging_time': 14.468707084655762}
I0420 02:59:44.678034 139888586368768 logging_writer.py:48] [28000] accumulated_eval_time=393.771386, accumulated_logging_time=14.468707, accumulated_submission_time=9551.647818, global_step=28000, preemption_count=0, score=9551.647818, test/accuracy=0.538300, test/loss=2.246193, test/num_examples=10000, total_duration=9960.259218, train/accuracy=0.760961, train/loss=1.184814, validation/accuracy=0.671740, validation/loss=1.576155, validation/num_examples=50000
I0420 02:59:44.865096 140068904109888 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 02:59:45.526876 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_28000
I0420 02:59:45.540184 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0420 02:59:45.563877 139890389915392 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=9551.647818
I0420 02:59:45.761633 140068904109888 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 02:59:46.685634 140068904109888 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_28000
I0420 02:59:46.696620 140068904109888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0420 02:59:47.079482 140068904109888 submission_runner.py:567] Tuning trial 1/1
I0420 02:59:47.080496 140068904109888 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0420 02:59:47.083438 140068904109888 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008171236841008067, 'train/loss': 6.911888599395752, 'validation/accuracy': 0.000859999970998615, 'validation/loss': 6.911709785461426, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.911829948425293, 'test/num_examples': 10000, 'score': 45.966253995895386, 'total_duration': 85.80054950714111, 'accumulated_submission_time': 45.966253995895386, 'accumulated_eval_time': 39.834142208099365, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1505, {'train/accuracy': 0.0715680792927742, 'train/loss': 5.438858985900879, 'validation/accuracy': 0.06384000182151794, 'validation/loss': 5.518141746520996, 'validation/num_examples': 50000, 'test/accuracy': 0.04610000178217888, 'test/loss': 5.737082481384277, 'test/num_examples': 10000, 'score': 556.0927176475525, 'total_duration': 613.0716388225555, 'accumulated_submission_time': 556.0927176475525, 'accumulated_eval_time': 56.38944411277771, 'accumulated_logging_time': 0.568462610244751, 'global_step': 1505, 'preemption_count': 0}), (3008, {'train/accuracy': 0.19114716351032257, 'train/loss': 4.260368824005127, 'validation/accuracy': 0.17431999742984772, 'validation/loss': 4.357460021972656, 'validation/num_examples': 50000, 'test/accuracy': 0.12520000338554382, 'test/loss': 4.772185325622559, 'test/num_examples': 10000, 'score': 1066.1593174934387, 'total_duration': 1140.372457742691, 'accumulated_submission_time': 1066.1593174934387, 'accumulated_eval_time': 73.04477286338806, 'accumulated_logging_time': 1.1268928050994873, 'global_step': 3008, 'preemption_count': 0}), (4513, {'train/accuracy': 0.3252550959587097, 'train/loss': 3.363584518432617, 'validation/accuracy': 0.29889997839927673, 'validation/loss': 3.489629030227661, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.0616374015808105, 'test/num_examples': 10000, 'score': 1576.1911327838898, 'total_duration': 1667.6078803539276, 'accumulated_submission_time': 1576.1911327838898, 'accumulated_eval_time': 89.68035078048706, 'accumulated_logging_time': 1.6749224662780762, 'global_step': 4513, 'preemption_count': 0}), (6008, {'train/accuracy': 0.41095343232154846, 'train/loss': 2.8420662879943848, 'validation/accuracy': 0.38053998351097107, 'validation/loss': 2.9859683513641357, 'validation/num_examples': 50000, 'test/accuracy': 0.28790000081062317, 'test/loss': 3.5911738872528076, 'test/num_examples': 10000, 'score': 2086.2687759399414, 'total_duration': 2194.4810535907745, 'accumulated_submission_time': 2086.2687759399414, 'accumulated_eval_time': 105.92895460128784, 'accumulated_logging_time': 2.2021138668060303, 'global_step': 6008, 'preemption_count': 0}), (7501, {'train/accuracy': 0.47345343232154846, 'train/loss': 2.640165090560913, 'validation/accuracy': 0.4425399899482727, 'validation/loss': 2.774324417114258, 'validation/num_examples': 50000, 'test/accuracy': 0.3354000151157379, 'test/loss': 3.3978633880615234, 'test/num_examples': 10000, 'score': 2596.697090625763, 'total_duration': 2722.059328556061, 'accumulated_submission_time': 2596.697090625763, 'accumulated_eval_time': 122.35618114471436, 'accumulated_logging_time': 2.906073808670044, 'global_step': 7501, 'preemption_count': 0}), (8997, {'train/accuracy': 0.5253108739852905, 'train/loss': 2.280762195587158, 'validation/accuracy': 0.4882799983024597, 'validation/loss': 2.4523775577545166, 'validation/num_examples': 50000, 'test/accuracy': 0.37380000948905945, 'test/loss': 3.1191022396087646, 'test/num_examples': 10000, 'score': 3106.686896085739, 'total_duration': 3251.199585199356, 'accumulated_submission_time': 3106.686896085739, 'accumulated_eval_time': 140.71718549728394, 'accumulated_logging_time': 3.676142454147339, 'global_step': 8997, 'preemption_count': 0}), (10491, {'train/accuracy': 0.5982341766357422, 'train/loss': 1.9569659233093262, 'validation/accuracy': 0.5369399785995483, 'validation/loss': 2.2377190589904785, 'validation/num_examples': 50000, 'test/accuracy': 0.4125000238418579, 'test/loss': 2.9078216552734375, 'test/num_examples': 10000, 'score': 3616.8029968738556, 'total_duration': 3780.18762922287, 'accumulated_submission_time': 3616.8029968738556, 'accumulated_eval_time': 158.76182413101196, 'accumulated_logging_time': 4.484162092208862, 'global_step': 10491, 'preemption_count': 0}), (11996, {'train/accuracy': 0.6256377696990967, 'train/loss': 1.8207343816757202, 'validation/accuracy': 0.5673199892044067, 'validation/loss': 2.0809600353240967, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.729384422302246, 'test/num_examples': 10000, 'score': 4126.86368727684, 'total_duration': 4309.859075546265, 'accumulated_submission_time': 4126.86368727684, 'accumulated_eval_time': 177.5633647441864, 'accumulated_logging_time': 5.27420711517334, 'global_step': 11996, 'preemption_count': 0}), (13501, {'train/accuracy': 0.6454878449440002, 'train/loss': 1.7378125190734863, 'validation/accuracy': 0.5874599814414978, 'validation/loss': 1.993247628211975, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.6715526580810547, 'test/num_examples': 10000, 'score': 4636.850937366486, 'total_duration': 4840.00221157074, 'accumulated_submission_time': 4636.850937366486, 'accumulated_eval_time': 196.98548936843872, 'accumulated_logging_time': 5.988356590270996, 'global_step': 13501, 'preemption_count': 0}), (14999, {'train/accuracy': 0.6673110723495483, 'train/loss': 1.5694421529769897, 'validation/accuracy': 0.6063599586486816, 'validation/loss': 1.8422223329544067, 'validation/num_examples': 50000, 'test/accuracy': 0.47840002179145813, 'test/loss': 2.5081822872161865, 'test/num_examples': 10000, 'score': 5147.007830381393, 'total_duration': 5369.823487520218, 'accumulated_submission_time': 5147.007830381393, 'accumulated_eval_time': 215.77732944488525, 'accumulated_logging_time': 6.840044021606445, 'global_step': 14999, 'preemption_count': 0}), (16505, {'train/accuracy': 0.6804846525192261, 'train/loss': 1.6167914867401123, 'validation/accuracy': 0.6204999685287476, 'validation/loss': 1.8714947700500488, 'validation/num_examples': 50000, 'test/accuracy': 0.49660003185272217, 'test/loss': 2.5076074600219727, 'test/num_examples': 10000, 'score': 5657.1570773124695, 'total_duration': 5900.557867527008, 'accumulated_submission_time': 5657.1570773124695, 'accumulated_eval_time': 235.51344180107117, 'accumulated_logging_time': 7.668473720550537, 'global_step': 16505, 'preemption_count': 0}), (18013, {'train/accuracy': 0.696707546710968, 'train/loss': 1.455159068107605, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.730525255203247, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.369504690170288, 'test/num_examples': 10000, 'score': 6167.218094587326, 'total_duration': 6430.778029680252, 'accumulated_submission_time': 6167.218094587326, 'accumulated_eval_time': 254.7871961593628, 'accumulated_logging_time': 8.533998012542725, 'global_step': 18013, 'preemption_count': 0}), (19523, {'train/accuracy': 0.7276785373687744, 'train/loss': 1.3121131658554077, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.7250874042510986, 'validation/num_examples': 50000, 'test/accuracy': 0.5026000142097473, 'test/loss': 2.4261507987976074, 'test/num_examples': 10000, 'score': 6677.419997692108, 'total_duration': 6961.729952335358, 'accumulated_submission_time': 6677.419997692108, 'accumulated_eval_time': 274.69107127189636, 'accumulated_logging_time': 9.359956502914429, 'global_step': 19523, 'preemption_count': 0}), (21029, {'train/accuracy': 0.7311065196990967, 'train/loss': 1.2657337188720703, 'validation/accuracy': 0.6487599611282349, 'validation/loss': 1.6399296522140503, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.327130079269409, 'test/num_examples': 10000, 'score': 7187.675060033798, 'total_duration': 7492.367063522339, 'accumulated_submission_time': 7187.675060033798, 'accumulated_eval_time': 294.19334650039673, 'accumulated_logging_time': 10.219239711761475, 'global_step': 21029, 'preemption_count': 0}), (22530, {'train/accuracy': 0.7328802347183228, 'train/loss': 1.2759041786193848, 'validation/accuracy': 0.6511600017547607, 'validation/loss': 1.6299868822097778, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.295003652572632, 'test/num_examples': 10000, 'score': 7697.922146081924, 'total_duration': 8023.428182601929, 'accumulated_submission_time': 7697.922146081924, 'accumulated_eval_time': 314.1579234600067, 'accumulated_logging_time': 11.048299789428711, 'global_step': 22530, 'preemption_count': 0}), (24033, {'train/accuracy': 0.7382413744926453, 'train/loss': 1.2593361139297485, 'validation/accuracy': 0.6553199887275696, 'validation/loss': 1.6311320066452026, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.2739436626434326, 'test/num_examples': 10000, 'score': 8208.122561693192, 'total_duration': 8553.933279752731, 'accumulated_submission_time': 8208.122561693192, 'accumulated_eval_time': 333.590256690979, 'accumulated_logging_time': 11.901039123535156, 'global_step': 24033, 'preemption_count': 0}), (25533, {'train/accuracy': 0.7434829473495483, 'train/loss': 1.288411021232605, 'validation/accuracy': 0.6614800095558167, 'validation/loss': 1.6455656290054321, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2954516410827637, 'test/num_examples': 10000, 'score': 8718.152958631516, 'total_duration': 9084.770256757736, 'accumulated_submission_time': 8718.152958631516, 'accumulated_eval_time': 353.49310636520386, 'accumulated_logging_time': 12.78539752960205, 'global_step': 25533, 'preemption_count': 0}), (27042, {'train/accuracy': 0.7563377022743225, 'train/loss': 1.1697587966918945, 'validation/accuracy': 0.672279953956604, 'validation/loss': 1.5339521169662476, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.1986913681030273, 'test/num_examples': 10000, 'score': 9228.215273141861, 'total_duration': 9615.617651224136, 'accumulated_submission_time': 9228.215273141861, 'accumulated_eval_time': 373.42863416671753, 'accumulated_logging_time': 13.6155526638031, 'global_step': 27042, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7609614133834839, 'train/loss': 1.1848136186599731, 'validation/accuracy': 0.6717399954795837, 'validation/loss': 1.576155424118042, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.2461929321289062, 'test/num_examples': 10000, 'score': 9551.647817850113, 'total_duration': 9960.259217977524, 'accumulated_submission_time': 9551.647817850113, 'accumulated_eval_time': 393.7713861465454, 'accumulated_logging_time': 14.468707084655762, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0420 02:59:47.083570 140068904109888 submission_runner.py:570] Timing: 9551.647817850113
I0420 02:59:47.083624 140068904109888 submission_runner.py:571] ====================
I0420 02:59:47.083741 140068904109888 submission_runner.py:631] Final imagenet_resnet score: 9551.647817850113
