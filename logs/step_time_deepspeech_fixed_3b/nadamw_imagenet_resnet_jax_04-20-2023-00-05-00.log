I0420 00:05:22.481708 140600545822528 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax.
I0420 00:05:22.553282 140600545822528 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 00:05:23.391292 140600545822528 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0420 00:05:23.391931 140600545822528 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 00:05:23.395561 140600545822528 submission_runner.py:528] Using RNG seed 3818660314
I0420 00:05:26.057260 140600545822528 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 00:05:26.057457 140600545822528 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1.
I0420 00:05:26.058311 140600545822528 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/hparams.json.
I0420 00:05:26.182455 140600545822528 submission_runner.py:232] Initializing dataset.
I0420 00:05:26.195467 140600545822528 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:05:26.202739 140600545822528 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:05:26.202853 140600545822528 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:05:26.460492 140600545822528 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:05:27.547646 140600545822528 submission_runner.py:239] Initializing model.
I0420 00:05:39.113758 140600545822528 submission_runner.py:249] Initializing optimizer.
I0420 00:05:40.213759 140600545822528 submission_runner.py:256] Initializing metrics bundle.
I0420 00:05:40.213946 140600545822528 submission_runner.py:273] Initializing checkpoint and logger.
I0420 00:05:40.215174 140600545822528 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0420 00:05:41.099388 140600545822528 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0420 00:05:41.100503 140600545822528 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0420 00:05:41.105340 140600545822528 submission_runner.py:309] Starting training loop.
I0420 00:06:30.860600 140423821506304 logging_writer.py:48] [0] global_step=0, grad_norm=0.5954963564872742, loss=6.924801349639893
I0420 00:06:30.878659 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:06:31.376672 140600545822528 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:06:31.383496 140600545822528 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:06:31.383631 140600545822528 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:06:31.448825 140600545822528 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:06:42.988553 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:06:43.696578 140600545822528 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:06:43.715055 140600545822528 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:06:43.715354 140600545822528 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:06:43.775794 140600545822528 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:07:02.282973 140600545822528 spec.py:326] Evaluating on the test split.
I0420 00:07:02.695016 140600545822528 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 00:07:02.700345 140600545822528 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0420 00:07:02.730901 140600545822528 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 00:07:11.760911 140600545822528 submission_runner.py:406] Time since start: 90.66s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.911112308502197, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.911139965057373, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911125183105469, 'test/num_examples': 10000, 'score': 49.77313995361328, 'total_duration': 90.65549921989441, 'accumulated_submission_time': 49.77313995361328, 'accumulated_eval_time': 40.88221287727356, 'accumulated_logging_time': 0}
I0420 00:07:11.777570 140393320527616 logging_writer.py:48] [1] accumulated_eval_time=40.882213, accumulated_logging_time=0, accumulated_submission_time=49.773140, global_step=1, preemption_count=0, score=49.773140, test/accuracy=0.001100, test/loss=6.911125, test/num_examples=10000, total_duration=90.655499, train/accuracy=0.000977, train/loss=6.911112, validation/accuracy=0.000960, validation/loss=6.911140, validation/num_examples=50000
I0420 00:07:11.964010 140600545822528 checkpoints.py:356] Saving checkpoint at step: 1
I0420 00:07:12.659622 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1
I0420 00:07:12.660877 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1.
I0420 00:07:46.310803 140393328920320 logging_writer.py:48] [100] global_step=100, grad_norm=0.5924111008644104, loss=6.878419399261475
I0420 00:08:20.225607 140393471530752 logging_writer.py:48] [200] global_step=200, grad_norm=0.6491056680679321, loss=6.760222911834717
I0420 00:08:53.772422 140393328920320 logging_writer.py:48] [300] global_step=300, grad_norm=0.7173323631286621, loss=6.564553260803223
I0420 00:09:27.311549 140393471530752 logging_writer.py:48] [400] global_step=400, grad_norm=0.7758700847625732, loss=6.455845832824707
I0420 00:10:01.009708 140393328920320 logging_writer.py:48] [500] global_step=500, grad_norm=0.9618086814880371, loss=6.278682708740234
I0420 00:10:34.775031 140393471530752 logging_writer.py:48] [600] global_step=600, grad_norm=1.3608119487762451, loss=6.150920867919922
I0420 00:11:08.398619 140393328920320 logging_writer.py:48] [700] global_step=700, grad_norm=3.0193445682525635, loss=6.008503437042236
I0420 00:11:42.149650 140393471530752 logging_writer.py:48] [800] global_step=800, grad_norm=1.9581544399261475, loss=5.9335126876831055
I0420 00:12:15.813292 140393328920320 logging_writer.py:48] [900] global_step=900, grad_norm=2.1359121799468994, loss=5.790750026702881
I0420 00:12:49.556121 140393471530752 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.097092866897583, loss=5.659275054931641
I0420 00:13:23.223833 140393328920320 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.4777352809906006, loss=5.622535705566406
I0420 00:13:57.091032 140393471530752 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.139392852783203, loss=5.5135650634765625
I0420 00:14:30.870937 140393328920320 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.9160256385803223, loss=5.369144439697266
I0420 00:15:04.573544 140393471530752 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.6276190280914307, loss=5.360458850860596
I0420 00:15:38.246321 140393328920320 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.7594213485717773, loss=5.154259204864502
I0420 00:15:42.698228 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:15:49.524863 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:15:57.284959 140600545822528 spec.py:326] Evaluating on the test split.
I0420 00:15:59.539622 140600545822528 submission_runner.py:406] Time since start: 618.43s, 	Step: 1515, 	{'train/accuracy': 0.1436941921710968, 'train/loss': 4.550468921661377, 'validation/accuracy': 0.12814000248908997, 'validation/loss': 4.667636394500732, 'validation/num_examples': 50000, 'test/accuracy': 0.09540000557899475, 'test/loss': 5.030379295349121, 'test/num_examples': 10000, 'score': 559.7901608943939, 'total_duration': 618.4342210292816, 'accumulated_submission_time': 559.7901608943939, 'accumulated_eval_time': 57.72358989715576, 'accumulated_logging_time': 0.9014766216278076}
I0420 00:15:59.548675 140393488316160 logging_writer.py:48] [1515] accumulated_eval_time=57.723590, accumulated_logging_time=0.901477, accumulated_submission_time=559.790161, global_step=1515, preemption_count=0, score=559.790161, test/accuracy=0.095400, test/loss=5.030379, test/num_examples=10000, total_duration=618.434221, train/accuracy=0.143694, train/loss=4.550469, validation/accuracy=0.128140, validation/loss=4.667636, validation/num_examples=50000
I0420 00:15:59.745365 140600545822528 checkpoints.py:356] Saving checkpoint at step: 1515
I0420 00:16:00.399938 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1515
I0420 00:16:00.401082 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1515.
I0420 00:16:29.458749 140393572177664 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.174432277679443, loss=5.075930595397949
I0420 00:17:03.184239 140422336718592 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.659100294113159, loss=5.177820205688477
I0420 00:17:36.855208 140393572177664 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.5210657119750977, loss=5.036067008972168
I0420 00:18:10.461631 140422336718592 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.581991672515869, loss=4.987592697143555
I0420 00:18:44.234252 140393572177664 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.680875778198242, loss=4.809523105621338
I0420 00:19:18.121629 140422336718592 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.435493469238281, loss=4.8488054275512695
I0420 00:19:51.765201 140393572177664 logging_writer.py:48] [2200] global_step=2200, grad_norm=6.414990425109863, loss=4.835914611816406
I0420 00:20:25.416773 140422336718592 logging_writer.py:48] [2300] global_step=2300, grad_norm=6.129147529602051, loss=4.736692428588867
I0420 00:20:59.284958 140393572177664 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.590876579284668, loss=4.628095626831055
I0420 00:21:32.903753 140422336718592 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.9682044982910156, loss=4.592250347137451
I0420 00:22:06.679218 140393572177664 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.2228078842163086, loss=4.613847732543945
I0420 00:22:40.342015 140422336718592 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.725109577178955, loss=4.503683090209961
I0420 00:23:13.978128 140393572177664 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.323596000671387, loss=4.416024684906006
I0420 00:23:47.786366 140422336718592 logging_writer.py:48] [2900] global_step=2900, grad_norm=6.596567630767822, loss=4.36386775970459
I0420 00:24:21.354928 140393572177664 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.4721293449401855, loss=4.269611358642578
I0420 00:24:30.551981 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:24:37.404372 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:24:45.245738 140600545822528 spec.py:326] Evaluating on the test split.
I0420 00:24:47.313934 140600545822528 submission_runner.py:406] Time since start: 1146.21s, 	Step: 3029, 	{'train/accuracy': 0.3011997640132904, 'train/loss': 3.3297603130340576, 'validation/accuracy': 0.27261999249458313, 'validation/loss': 3.5079665184020996, 'validation/num_examples': 50000, 'test/accuracy': 0.20670001208782196, 'test/loss': 4.086902141571045, 'test/num_examples': 10000, 'score': 1069.9186408519745, 'total_duration': 1146.2085344791412, 'accumulated_submission_time': 1069.9186408519745, 'accumulated_eval_time': 74.48555088043213, 'accumulated_logging_time': 1.7671406269073486}
I0420 00:24:47.321913 140422336718592 logging_writer.py:48] [3029] accumulated_eval_time=74.485551, accumulated_logging_time=1.767141, accumulated_submission_time=1069.918641, global_step=3029, preemption_count=0, score=1069.918641, test/accuracy=0.206700, test/loss=4.086902, test/num_examples=10000, total_duration=1146.208534, train/accuracy=0.301200, train/loss=3.329760, validation/accuracy=0.272620, validation/loss=3.507967, validation/num_examples=50000
I0420 00:24:47.530310 140600545822528 checkpoints.py:356] Saving checkpoint at step: 3029
I0420 00:24:48.203195 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_3029
I0420 00:24:48.204368 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_3029.
I0420 00:25:12.433472 140393572177664 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.592986822128296, loss=4.181492328643799
I0420 00:25:45.987954 140422303147776 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.252791881561279, loss=4.298484802246094
I0420 00:26:19.569040 140393572177664 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.065631866455078, loss=4.101778030395508
I0420 00:26:53.328501 140422303147776 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.1719613075256348, loss=4.108560562133789
I0420 00:27:27.014414 140393572177664 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.524623394012451, loss=4.171136856079102
I0420 00:28:00.653654 140422303147776 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.1676199436187744, loss=4.142172336578369
I0420 00:28:34.505551 140393572177664 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.4057745933532715, loss=3.9817609786987305
I0420 00:29:08.098431 140422303147776 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.193914413452148, loss=4.027154445648193
I0420 00:29:41.787490 140393572177664 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.8199353218078613, loss=4.040243625640869
I0420 00:30:15.218244 140422303147776 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.881824016571045, loss=3.915627956390381
I0420 00:30:49.024698 140393572177664 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.5822668075561523, loss=3.777595043182373
I0420 00:31:22.641206 140422303147776 logging_writer.py:48] [4200] global_step=4200, grad_norm=5.4560980796813965, loss=3.799166440963745
I0420 00:31:56.270488 140393572177664 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.441262722015381, loss=3.7961270809173584
I0420 00:32:30.048136 140422303147776 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.5047404766082764, loss=3.6490485668182373
I0420 00:33:03.624786 140393572177664 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.8032681941986084, loss=3.800604820251465
I0420 00:33:18.467097 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:33:25.312811 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:33:33.215504 140600545822528 spec.py:326] Evaluating on the test split.
I0420 00:33:35.177053 140600545822528 submission_runner.py:406] Time since start: 1674.07s, 	Step: 4546, 	{'train/accuracy': 0.41376355290412903, 'train/loss': 2.7099032402038574, 'validation/accuracy': 0.3866199851036072, 'validation/loss': 2.8762223720550537, 'validation/num_examples': 50000, 'test/accuracy': 0.28060001134872437, 'test/loss': 3.5524067878723145, 'test/num_examples': 10000, 'score': 1580.1585001945496, 'total_duration': 1674.071632385254, 'accumulated_submission_time': 1580.1585001945496, 'accumulated_eval_time': 91.19546103477478, 'accumulated_logging_time': 2.6622729301452637}
I0420 00:33:35.186266 140422303147776 logging_writer.py:48] [4546] accumulated_eval_time=91.195461, accumulated_logging_time=2.662273, accumulated_submission_time=1580.158500, global_step=4546, preemption_count=0, score=1580.158500, test/accuracy=0.280600, test/loss=3.552407, test/num_examples=10000, total_duration=1674.071632, train/accuracy=0.413764, train/loss=2.709903, validation/accuracy=0.386620, validation/loss=2.876222, validation/num_examples=50000
I0420 00:33:35.433542 140600545822528 checkpoints.py:356] Saving checkpoint at step: 4546
I0420 00:33:36.140132 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_4546
I0420 00:33:36.141278 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_4546.
I0420 00:33:54.606142 140393572177664 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.179443359375, loss=3.6647472381591797
I0420 00:34:28.215677 140422294755072 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.2438087463378906, loss=3.7105064392089844
I0420 00:35:01.780530 140393572177664 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.6049129962921143, loss=3.5912020206451416
I0420 00:35:35.278401 140422294755072 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.0613672733306885, loss=3.7579917907714844
I0420 00:36:09.124983 140393572177664 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.5009689331054688, loss=3.5293703079223633
I0420 00:36:42.715191 140422294755072 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.0273821353912354, loss=3.6319336891174316
I0420 00:37:16.280304 140393572177664 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.520824909210205, loss=3.5173590183258057
I0420 00:37:49.949088 140422294755072 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.948350429534912, loss=3.567225217819214
I0420 00:38:23.434215 140393572177664 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.492341995239258, loss=3.495791435241699
I0420 00:38:57.073535 140422294755072 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.047543048858643, loss=3.463437080383301
I0420 00:39:30.606281 140393572177664 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.849216938018799, loss=3.473921298980713
I0420 00:40:04.248133 140422294755072 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.279649496078491, loss=3.4526360034942627
I0420 00:40:37.895900 140393572177664 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.018913984298706, loss=3.3680529594421387
I0420 00:41:11.591398 140422294755072 logging_writer.py:48] [5900] global_step=5900, grad_norm=4.5132622718811035, loss=3.4950079917907715
I0420 00:41:45.432108 140393572177664 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.622084140777588, loss=3.4443259239196777
I0420 00:42:06.370986 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:42:13.128230 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:42:20.866803 140600545822528 spec.py:326] Evaluating on the test split.
I0420 00:42:23.079429 140600545822528 submission_runner.py:406] Time since start: 2201.97s, 	Step: 6064, 	{'train/accuracy': 0.4940609037876129, 'train/loss': 2.2376341819763184, 'validation/accuracy': 0.460099995136261, 'validation/loss': 2.41156268119812, 'validation/num_examples': 50000, 'test/accuracy': 0.35500001907348633, 'test/loss': 3.0813822746276855, 'test/num_examples': 10000, 'score': 2090.365306377411, 'total_duration': 2201.974023103714, 'accumulated_submission_time': 2090.365306377411, 'accumulated_eval_time': 107.90387511253357, 'accumulated_logging_time': 3.6314122676849365}
I0420 00:42:23.087480 140422294755072 logging_writer.py:48] [6064] accumulated_eval_time=107.903875, accumulated_logging_time=3.631412, accumulated_submission_time=2090.365306, global_step=6064, preemption_count=0, score=2090.365306, test/accuracy=0.355000, test/loss=3.081382, test/num_examples=10000, total_duration=2201.974023, train/accuracy=0.494061, train/loss=2.237634, validation/accuracy=0.460100, validation/loss=2.411563, validation/num_examples=50000
I0420 00:42:23.354370 140600545822528 checkpoints.py:356] Saving checkpoint at step: 6064
I0420 00:42:24.344885 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_6064
I0420 00:42:24.357439 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_6064.
I0420 00:42:36.864945 140393572177664 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.662309169769287, loss=3.3324923515319824
I0420 00:43:10.603266 140421741147904 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.7508962154388428, loss=3.342960834503174
I0420 00:43:44.272217 140393572177664 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.049119472503662, loss=3.3046438694000244
I0420 00:44:18.009802 140421741147904 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.7909295558929443, loss=3.345710277557373
I0420 00:44:51.664073 140393572177664 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.461972713470459, loss=3.3907036781311035
I0420 00:45:25.328698 140421741147904 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.732295036315918, loss=3.3641889095306396
I0420 00:45:58.981351 140393572177664 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.4726860523223877, loss=3.2361249923706055
I0420 00:46:32.629579 140421741147904 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.8917536735534668, loss=3.3377602100372314
I0420 00:47:06.257251 140393572177664 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.5952389240264893, loss=3.2889907360076904
I0420 00:47:39.915599 140421741147904 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.2572765350341797, loss=3.2838895320892334
I0420 00:48:13.712832 140393572177664 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.1052303314208984, loss=3.2737927436828613
I0420 00:48:47.266062 140421741147904 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.736814498901367, loss=3.3763701915740967
I0420 00:49:20.883383 140393572177664 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.028528928756714, loss=3.227719783782959
I0420 00:49:54.649401 140421741147904 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.4694623947143555, loss=3.2718701362609863
I0420 00:50:28.119385 140393572177664 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.314439535140991, loss=3.0803818702697754
I0420 00:50:54.415121 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:51:01.194042 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:51:09.161429 140600545822528 spec.py:326] Evaluating on the test split.
I0420 00:51:11.326201 140600545822528 submission_runner.py:406] Time since start: 2730.22s, 	Step: 7580, 	{'train/accuracy': 0.5524752736091614, 'train/loss': 1.9987709522247314, 'validation/accuracy': 0.5173199772834778, 'validation/loss': 2.1648716926574707, 'validation/num_examples': 50000, 'test/accuracy': 0.40150001645088196, 'test/loss': 2.843662738800049, 'test/num_examples': 10000, 'score': 2600.3966233730316, 'total_duration': 2730.22079205513, 'accumulated_submission_time': 2600.3966233730316, 'accumulated_eval_time': 124.81492257118225, 'accumulated_logging_time': 4.91765832901001}
I0420 00:51:11.334464 140421741147904 logging_writer.py:48] [7580] accumulated_eval_time=124.814923, accumulated_logging_time=4.917658, accumulated_submission_time=2600.396623, global_step=7580, preemption_count=0, score=2600.396623, test/accuracy=0.401500, test/loss=2.843663, test/num_examples=10000, total_duration=2730.220792, train/accuracy=0.552475, train/loss=1.998771, validation/accuracy=0.517320, validation/loss=2.164872, validation/num_examples=50000
I0420 00:51:11.598238 140600545822528 checkpoints.py:356] Saving checkpoint at step: 7580
I0420 00:51:12.589419 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_7580
I0420 00:51:12.602824 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_7580.
I0420 00:51:19.715588 140393572177664 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.143544912338257, loss=3.1397087574005127
I0420 00:51:53.317893 140420533188352 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.2861907482147217, loss=3.3117740154266357
I0420 00:52:27.086219 140393572177664 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.8898510932922363, loss=3.251487970352173
I0420 00:53:00.753969 140420533188352 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.1330673694610596, loss=3.1628341674804688
I0420 00:53:34.326814 140393572177664 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.4225001335144043, loss=3.0599043369293213
I0420 00:54:07.817014 140420533188352 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.9765243530273438, loss=3.070021629333496
I0420 00:54:41.314909 140393572177664 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.6383671760559082, loss=3.1316442489624023
I0420 00:55:14.938554 140420533188352 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8944363594055176, loss=3.0382862091064453
I0420 00:55:48.629454 140393572177664 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.366180896759033, loss=3.150947332382202
I0420 00:56:22.194348 140420533188352 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.5765388011932373, loss=3.1488866806030273
I0420 00:56:55.899458 140393572177664 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.199843645095825, loss=3.145569086074829
I0420 00:57:29.530839 140420533188352 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.9578492641448975, loss=3.132113218307495
I0420 00:58:03.033654 140393572177664 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.0437004566192627, loss=3.2424240112304688
I0420 00:58:36.571783 140420533188352 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.908388614654541, loss=3.096616268157959
I0420 00:59:10.236993 140393572177664 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4949802160263062, loss=3.0436835289001465
I0420 00:59:42.786508 140600545822528 spec.py:298] Evaluating on the training split.
I0420 00:59:49.633696 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 00:59:58.753258 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:00:04.235161 140600545822528 submission_runner.py:406] Time since start: 3263.13s, 	Step: 9099, 	{'train/accuracy': 0.6389110088348389, 'train/loss': 1.5943671464920044, 'validation/accuracy': 0.5543199777603149, 'validation/loss': 1.9836335182189941, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.6646745204925537, 'test/num_examples': 10000, 'score': 3110.5579011440277, 'total_duration': 3263.1297419071198, 'accumulated_submission_time': 3110.5579011440277, 'accumulated_eval_time': 146.2635464668274, 'accumulated_logging_time': 6.198667764663696}
I0420 01:00:04.248919 140420533188352 logging_writer.py:48] [9099] accumulated_eval_time=146.263546, accumulated_logging_time=6.198668, accumulated_submission_time=3110.557901, global_step=9099, preemption_count=0, score=3110.557901, test/accuracy=0.435300, test/loss=2.664675, test/num_examples=10000, total_duration=3263.129742, train/accuracy=0.638911, train/loss=1.594367, validation/accuracy=0.554320, validation/loss=1.983634, validation/num_examples=50000
I0420 01:00:04.513951 140600545822528 checkpoints.py:356] Saving checkpoint at step: 9099
I0420 01:00:05.273957 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_9099
I0420 01:00:05.275268 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_9099.
I0420 01:00:05.978750 140393572177664 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.314406633377075, loss=3.08782958984375
I0420 01:00:39.591850 140420524795648 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.0242655277252197, loss=3.1066858768463135
I0420 01:01:13.191550 140393572177664 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.8781707286834717, loss=3.015279769897461
I0420 01:01:46.586177 140420524795648 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.6731288433074951, loss=2.9988596439361572
I0420 01:02:20.113112 140393572177664 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.4471354484558105, loss=3.0310733318328857
I0420 01:02:53.723613 140420524795648 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.6760109663009644, loss=3.0295658111572266
I0420 01:03:27.151443 140393572177664 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.7167179584503174, loss=2.9315669536590576
I0420 01:04:00.742631 140420524795648 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.603074073791504, loss=3.0712361335754395
I0420 01:04:34.224766 140393572177664 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.7786669731140137, loss=3.014479160308838
I0420 01:05:07.584269 140420524795648 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4897600412368774, loss=2.9467709064483643
I0420 01:05:41.226757 140393572177664 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.9217009544372559, loss=2.9593515396118164
I0420 01:06:14.745650 140420524795648 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.3977800607681274, loss=3.0825905799865723
I0420 01:06:48.160774 140393572177664 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.5038796663284302, loss=2.8625428676605225
I0420 01:07:21.554014 140420524795648 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.5793700218200684, loss=3.04109525680542
I0420 01:07:55.037930 140393572177664 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.9042342901229858, loss=2.8413381576538086
I0420 01:08:28.742290 140420524795648 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.0755107402801514, loss=2.949739456176758
I0420 01:08:35.505739 140600545822528 spec.py:298] Evaluating on the training split.
I0420 01:08:42.358738 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 01:08:50.822258 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:08:52.673381 140600545822528 submission_runner.py:406] Time since start: 3791.57s, 	Step: 10622, 	{'train/accuracy': 0.6453483700752258, 'train/loss': 1.564949631690979, 'validation/accuracy': 0.5787000060081482, 'validation/loss': 1.8903650045394897, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.582447052001953, 'test/num_examples': 10000, 'score': 3620.7630009651184, 'total_duration': 3791.567970275879, 'accumulated_submission_time': 3620.7630009651184, 'accumulated_eval_time': 163.43114829063416, 'accumulated_logging_time': 7.246370077133179}
I0420 01:08:52.682913 140393572177664 logging_writer.py:48] [10622] accumulated_eval_time=163.431148, accumulated_logging_time=7.246370, accumulated_submission_time=3620.763001, global_step=10622, preemption_count=0, score=3620.763001, test/accuracy=0.458000, test/loss=2.582447, test/num_examples=10000, total_duration=3791.567970, train/accuracy=0.645348, train/loss=1.564950, validation/accuracy=0.578700, validation/loss=1.890365, validation/num_examples=50000
I0420 01:08:52.999848 140600545822528 checkpoints.py:356] Saving checkpoint at step: 10622
I0420 01:08:53.932070 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_10622
I0420 01:08:53.946081 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_10622.
I0420 01:09:20.451479 140420524795648 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.061065196990967, loss=2.9163856506347656
I0420 01:09:54.033006 140423309797120 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.682102084159851, loss=2.9328160285949707
I0420 01:10:27.690283 140420524795648 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.963224172592163, loss=2.8918628692626953
I0420 01:11:01.244190 140423309797120 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.381345510482788, loss=2.7804877758026123
I0420 01:11:34.849210 140420524795648 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.3934156894683838, loss=2.912184476852417
I0420 01:12:08.368357 140423309797120 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.3338228464126587, loss=2.8567728996276855
I0420 01:12:41.898677 140420524795648 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.6329466104507446, loss=2.94160532951355
I0420 01:13:15.336673 140423309797120 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0895240306854248, loss=2.8914968967437744
I0420 01:13:49.074172 140420524795648 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.203227162361145, loss=2.8919548988342285
I0420 01:14:22.646376 140423309797120 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.5931988954544067, loss=2.7556815147399902
I0420 01:14:56.105706 140420524795648 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.1918303966522217, loss=2.8495492935180664
I0420 01:15:29.651283 140423309797120 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.3566606044769287, loss=2.898566722869873
I0420 01:16:03.271852 140420524795648 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0295214653015137, loss=2.876976490020752
I0420 01:16:36.728915 140423309797120 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.5606768131256104, loss=2.7667179107666016
I0420 01:17:10.212205 140420524795648 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.4694570302963257, loss=2.7920782566070557
I0420 01:17:24.038495 140600545822528 spec.py:298] Evaluating on the training split.
I0420 01:17:30.980403 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 01:17:40.374815 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:17:42.396877 140600545822528 submission_runner.py:406] Time since start: 4321.29s, 	Step: 12143, 	{'train/accuracy': 0.6630261540412903, 'train/loss': 1.4628506898880005, 'validation/accuracy': 0.6026999950408936, 'validation/loss': 1.7409168481826782, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.4606869220733643, 'test/num_examples': 10000, 'score': 4130.833471059799, 'total_duration': 4321.2903571128845, 'accumulated_submission_time': 4130.833471059799, 'accumulated_eval_time': 181.78838205337524, 'accumulated_logging_time': 8.523331642150879}
I0420 01:17:42.406648 140423309797120 logging_writer.py:48] [12143] accumulated_eval_time=181.788382, accumulated_logging_time=8.523332, accumulated_submission_time=4130.833471, global_step=12143, preemption_count=0, score=4130.833471, test/accuracy=0.472600, test/loss=2.460687, test/num_examples=10000, total_duration=4321.290357, train/accuracy=0.663026, train/loss=1.462851, validation/accuracy=0.602700, validation/loss=1.740917, validation/num_examples=50000
I0420 01:17:42.711054 140600545822528 checkpoints.py:356] Saving checkpoint at step: 12143
I0420 01:17:43.581748 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_12143
I0420 01:17:43.591713 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_12143.
I0420 01:18:02.968383 140420524795648 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.2593047618865967, loss=2.8967061042785645
I0420 01:18:36.657703 140422932322048 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.2538036108016968, loss=2.773778200149536
I0420 01:19:10.395172 140420524795648 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5040892362594604, loss=2.812533140182495
I0420 01:19:43.984100 140422932322048 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.030517339706421, loss=2.8487932682037354
I0420 01:20:17.653874 140420524795648 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.1692509651184082, loss=2.795207977294922
I0420 01:20:51.307139 140422932322048 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.3042386770248413, loss=2.7808473110198975
I0420 01:21:24.890031 140420524795648 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.801959753036499, loss=2.8393638134002686
I0420 01:21:58.420132 140422932322048 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.325275182723999, loss=2.8434853553771973
I0420 01:22:31.910323 140420524795648 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2945570945739746, loss=2.682626962661743
I0420 01:23:05.513526 140422932322048 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.4694629907608032, loss=2.6808345317840576
I0420 01:23:39.202839 140420524795648 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1581401824951172, loss=2.6392204761505127
I0420 01:24:12.704786 140422932322048 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.2166948318481445, loss=2.75709867477417
I0420 01:24:46.190546 140420524795648 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0796372890472412, loss=2.7721951007843018
I0420 01:25:19.672305 140422932322048 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.2380621433258057, loss=2.7244129180908203
I0420 01:25:53.078203 140420524795648 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.3721507787704468, loss=2.7579994201660156
I0420 01:26:13.708707 140600545822528 spec.py:298] Evaluating on the training split.
I0420 01:26:21.110722 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 01:26:30.846041 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:26:32.977173 140600545822528 submission_runner.py:406] Time since start: 4851.87s, 	Step: 13663, 	{'train/accuracy': 0.6822584271430969, 'train/loss': 1.3913222551345825, 'validation/accuracy': 0.6205599904060364, 'validation/loss': 1.68320631980896, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.381999969482422, 'test/num_examples': 10000, 'score': 4640.92102599144, 'total_duration': 4851.8709416389465, 'accumulated_submission_time': 4640.92102599144, 'accumulated_eval_time': 201.05599927902222, 'accumulated_logging_time': 9.729688882827759}
I0420 01:26:32.991280 140422932322048 logging_writer.py:48] [13663] accumulated_eval_time=201.055999, accumulated_logging_time=9.729689, accumulated_submission_time=4640.921026, global_step=13663, preemption_count=0, score=4640.921026, test/accuracy=0.491000, test/loss=2.382000, test/num_examples=10000, total_duration=4851.870942, train/accuracy=0.682258, train/loss=1.391322, validation/accuracy=0.620560, validation/loss=1.683206, validation/num_examples=50000
I0420 01:26:33.222216 140600545822528 checkpoints.py:356] Saving checkpoint at step: 13663
I0420 01:26:34.178447 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_13663
I0420 01:26:34.193331 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_13663.
I0420 01:26:46.995131 140420524795648 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0790258646011353, loss=2.716338634490967
I0420 01:27:20.559529 140422923929344 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.0284109115600586, loss=2.7018394470214844
I0420 01:27:54.008185 140420524795648 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3885886669158936, loss=2.7763478755950928
I0420 01:28:27.484644 140422923929344 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.4511531591415405, loss=2.776418447494507
I0420 01:29:00.953157 140420524795648 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.1422053575515747, loss=2.671952962875366
I0420 01:29:34.366033 140422923929344 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0822312831878662, loss=2.658717393875122
I0420 01:30:07.938932 140420524795648 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.2411860227584839, loss=2.7462644577026367
I0420 01:30:41.394506 140422923929344 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0888057947158813, loss=2.6828410625457764
I0420 01:31:14.976354 140420524795648 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8906733989715576, loss=2.6635735034942627
I0420 01:31:48.422777 140422923929344 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.356346607208252, loss=2.5803723335266113
I0420 01:32:21.783889 140420524795648 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.1510223150253296, loss=2.727952718734741
I0420 01:32:55.294824 140422923929344 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.3031660318374634, loss=2.6917412281036377
I0420 01:33:28.718330 140420524795648 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9716936945915222, loss=2.6886348724365234
I0420 01:34:02.328630 140422923929344 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.3923115730285645, loss=2.5951056480407715
I0420 01:34:35.847030 140420524795648 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.4321101903915405, loss=2.762476682662964
I0420 01:35:04.463165 140600545822528 spec.py:298] Evaluating on the training split.
I0420 01:35:11.441006 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 01:35:21.555847 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:35:23.514791 140600545822528 submission_runner.py:406] Time since start: 5382.41s, 	Step: 15187, 	{'train/accuracy': 0.6968271732330322, 'train/loss': 1.3315984010696411, 'validation/accuracy': 0.6311599612236023, 'validation/loss': 1.6334296464920044, 'validation/num_examples': 50000, 'test/accuracy': 0.503000020980835, 'test/loss': 2.3141348361968994, 'test/num_examples': 10000, 'score': 5151.167583703995, 'total_duration': 5382.40825176239, 'accumulated_submission_time': 5151.167583703995, 'accumulated_eval_time': 220.1064829826355, 'accumulated_logging_time': 10.951069593429565}
I0420 01:35:23.523890 140422923929344 logging_writer.py:48] [15187] accumulated_eval_time=220.106483, accumulated_logging_time=10.951070, accumulated_submission_time=5151.167584, global_step=15187, preemption_count=0, score=5151.167584, test/accuracy=0.503000, test/loss=2.314135, test/num_examples=10000, total_duration=5382.408252, train/accuracy=0.696827, train/loss=1.331598, validation/accuracy=0.631160, validation/loss=1.633430, validation/num_examples=50000
I0420 01:35:23.783016 140600545822528 checkpoints.py:356] Saving checkpoint at step: 15187
I0420 01:35:24.779309 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_15187
I0420 01:35:24.793800 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_15187.
I0420 01:35:29.460269 140420524795648 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9369916319847107, loss=2.7656705379486084
I0420 01:36:02.781419 140422915536640 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9772252440452576, loss=2.6542654037475586
I0420 01:36:36.218135 140420524795648 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9805263876914978, loss=2.769547700881958
I0420 01:37:09.740894 140422915536640 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.3599953651428223, loss=2.6045620441436768
I0420 01:37:43.315414 140420524795648 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.7085672616958618, loss=2.7267813682556152
I0420 01:38:16.750228 140422915536640 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9908024072647095, loss=2.5828158855438232
I0420 01:38:50.343960 140420524795648 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0182658433914185, loss=2.570950746536255
I0420 01:39:23.949614 140422915536640 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0388903617858887, loss=2.627061128616333
I0420 01:39:57.452867 140420524795648 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0683070421218872, loss=2.675795793533325
I0420 01:40:31.062730 140422915536640 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0184413194656372, loss=2.6685144901275635
I0420 01:41:04.544864 140420524795648 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9633378982543945, loss=2.568511486053467
I0420 01:41:38.206990 140422915536640 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.1167069673538208, loss=2.6614620685577393
I0420 01:42:11.621534 140420524795648 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.3724626302719116, loss=2.5373823642730713
I0420 01:42:45.151444 140422915536640 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.11664617061615, loss=2.687798023223877
I0420 01:43:18.861129 140420524795648 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0040338039398193, loss=2.5772223472595215
I0420 01:43:52.392131 140422915536640 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9695819616317749, loss=2.543107271194458
I0420 01:43:54.809703 140600545822528 spec.py:298] Evaluating on the training split.
I0420 01:44:02.575457 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 01:44:12.972641 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:44:14.891021 140600545822528 submission_runner.py:406] Time since start: 5913.78s, 	Step: 16709, 	{'train/accuracy': 0.7126315236091614, 'train/loss': 1.2631067037582397, 'validation/accuracy': 0.6458399891853333, 'validation/loss': 1.5668915510177612, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.2512903213500977, 'test/num_examples': 10000, 'score': 5661.157910823822, 'total_duration': 5913.784823894501, 'accumulated_submission_time': 5661.157910823822, 'accumulated_eval_time': 240.18697547912598, 'accumulated_logging_time': 12.238005876541138}
I0420 01:44:14.904935 140420524795648 logging_writer.py:48] [16709] accumulated_eval_time=240.186975, accumulated_logging_time=12.238006, accumulated_submission_time=5661.157911, global_step=16709, preemption_count=0, score=5661.157911, test/accuracy=0.519000, test/loss=2.251290, test/num_examples=10000, total_duration=5913.784824, train/accuracy=0.712632, train/loss=1.263107, validation/accuracy=0.645840, validation/loss=1.566892, validation/num_examples=50000
I0420 01:44:15.187517 140600545822528 checkpoints.py:356] Saving checkpoint at step: 16709
I0420 01:44:16.206625 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_16709
I0420 01:44:16.223638 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_16709.
I0420 01:44:47.157350 140422915536640 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.8540856838226318, loss=2.6119918823242188
I0420 01:45:20.675883 140422848395008 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0935728549957275, loss=2.573275566101074
I0420 01:45:54.236955 140422915536640 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8516561388969421, loss=2.6467669010162354
I0420 01:46:27.682410 140422848395008 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.982783854007721, loss=2.6472432613372803
I0420 01:47:01.129724 140422915536640 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0754755735397339, loss=2.6023333072662354
I0420 01:47:34.694510 140422848395008 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.171974539756775, loss=2.527374029159546
I0420 01:48:08.199673 140422915536640 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.0648102760314941, loss=2.6837880611419678
I0420 01:48:41.740381 140422848395008 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2450475692749023, loss=2.6473934650421143
I0420 01:49:15.219129 140422915536640 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0887534618377686, loss=2.6463558673858643
I0420 01:49:48.891002 140422848395008 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.1342436075210571, loss=2.589381694793701
I0420 01:50:22.447780 140422915536640 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.102007508277893, loss=2.6095354557037354
I0420 01:50:56.154801 140422848395008 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0150450468063354, loss=2.517561674118042
I0420 01:51:29.658969 140422915536640 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1282267570495605, loss=2.551149606704712
I0420 01:52:03.159946 140422848395008 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9015679955482483, loss=2.6003799438476562
I0420 01:52:36.654948 140422915536640 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9571219682693481, loss=2.492530345916748
I0420 01:52:46.373106 140600545822528 spec.py:298] Evaluating on the training split.
I0420 01:52:54.243175 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 01:53:04.669574 140600545822528 spec.py:326] Evaluating on the test split.
I0420 01:53:06.821538 140600545822528 submission_runner.py:406] Time since start: 6445.72s, 	Step: 18231, 	{'train/accuracy': 0.7649075388908386, 'train/loss': 1.0175938606262207, 'validation/accuracy': 0.6548799872398376, 'validation/loss': 1.503351092338562, 'validation/num_examples': 50000, 'test/accuracy': 0.5241000056266785, 'test/loss': 2.2087440490722656, 'test/num_examples': 10000, 'score': 6171.284921169281, 'total_duration': 6445.715316057205, 'accumulated_submission_time': 6171.284921169281, 'accumulated_eval_time': 260.63455629348755, 'accumulated_logging_time': 13.575388193130493}
I0420 01:53:06.839876 140422848395008 logging_writer.py:48] [18231] accumulated_eval_time=260.634556, accumulated_logging_time=13.575388, accumulated_submission_time=6171.284921, global_step=18231, preemption_count=0, score=6171.284921, test/accuracy=0.524100, test/loss=2.208744, test/num_examples=10000, total_duration=6445.715316, train/accuracy=0.764908, train/loss=1.017594, validation/accuracy=0.654880, validation/loss=1.503351, validation/num_examples=50000
I0420 01:53:07.090176 140600545822528 checkpoints.py:356] Saving checkpoint at step: 18231
I0420 01:53:08.109812 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_18231
I0420 01:53:08.127826 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_18231.
I0420 01:53:31.650421 140422915536640 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0045280456542969, loss=2.575758457183838
I0420 01:54:05.239145 140422345111296 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9553530812263489, loss=2.5644631385803223
I0420 01:54:38.751591 140422915536640 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.961789071559906, loss=2.5043880939483643
I0420 01:55:12.245549 140422345111296 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7799781560897827, loss=2.6320526599884033
I0420 01:55:45.874534 140422915536640 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8618229031562805, loss=2.562129497528076
I0420 01:56:19.466073 140422345111296 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6713328957557678, loss=2.5473756790161133
I0420 01:56:52.999498 140422915536640 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2751471996307373, loss=2.5358352661132812
I0420 01:57:26.431617 140422345111296 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7797996401786804, loss=2.5573408603668213
I0420 01:57:59.992710 140422915536640 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7149189114570618, loss=2.513167381286621
I0420 01:58:33.422021 140422345111296 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9850635528564453, loss=2.6259045600891113
I0420 01:59:06.974770 140422915536640 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8067566156387329, loss=2.5157370567321777
I0420 01:59:40.643185 140422345111296 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.894572913646698, loss=2.5036463737487793
I0420 02:00:14.056277 140422915536640 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8186362981796265, loss=2.424790859222412
I0420 02:00:47.855431 140422345111296 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.029784917831421, loss=2.604372024536133
I0420 02:01:21.460389 140422915536640 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8465811014175415, loss=2.576385974884033
I0420 02:01:38.335367 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:01:45.574609 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:01:55.943527 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:01:58.105710 140600545822528 submission_runner.py:406] Time since start: 6977.00s, 	Step: 19752, 	{'train/accuracy': 0.7556201815605164, 'train/loss': 1.0837923288345337, 'validation/accuracy': 0.6590399742126465, 'validation/loss': 1.5099282264709473, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.2003285884857178, 'test/num_examples': 10000, 'score': 6681.4698033332825, 'total_duration': 6976.999369621277, 'accumulated_submission_time': 6681.4698033332825, 'accumulated_eval_time': 280.4039480686188, 'accumulated_logging_time': 14.886697769165039}
I0420 02:01:58.115499 140422345111296 logging_writer.py:48] [19752] accumulated_eval_time=280.403948, accumulated_logging_time=14.886698, accumulated_submission_time=6681.469803, global_step=19752, preemption_count=0, score=6681.469803, test/accuracy=0.533000, test/loss=2.200329, test/num_examples=10000, total_duration=6976.999370, train/accuracy=0.755620, train/loss=1.083792, validation/accuracy=0.659040, validation/loss=1.509928, validation/num_examples=50000
I0420 02:01:58.376045 140600545822528 checkpoints.py:356] Saving checkpoint at step: 19752
I0420 02:01:59.367352 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_19752
I0420 02:01:59.383575 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_19752.
I0420 02:02:15.929631 140422915536640 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8478406667709351, loss=2.517026901245117
I0420 02:02:49.469126 140422336718592 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.1657239198684692, loss=2.5497491359710693
I0420 02:03:23.095192 140422915536640 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7589919567108154, loss=2.4386777877807617
I0420 02:03:56.532781 140422336718592 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8161336183547974, loss=2.4383416175842285
I0420 02:04:30.247110 140422915536640 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1576703786849976, loss=2.538424491882324
I0420 02:05:03.775573 140422336718592 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7398430109024048, loss=2.5347635746002197
I0420 02:05:37.425904 140422915536640 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7192927598953247, loss=2.5455434322357178
I0420 02:06:11.017500 140422336718592 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8376609086990356, loss=2.530501127243042
I0420 02:06:44.411744 140422915536640 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8659072518348694, loss=2.4346094131469727
I0420 02:07:17.873306 140422336718592 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.014294981956482, loss=2.5051488876342773
I0420 02:07:51.547570 140422915536640 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5890416502952576, loss=2.4472391605377197
I0420 02:08:25.135333 140422336718592 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7702862620353699, loss=2.6138110160827637
I0420 02:08:58.603386 140422915536640 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8708118200302124, loss=2.53983473777771
I0420 02:09:32.145610 140422336718592 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.687519371509552, loss=2.4150466918945312
I0420 02:10:05.591505 140422915536640 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.728286623954773, loss=2.5425548553466797
I0420 02:10:29.427870 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:10:37.313703 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:10:47.762849 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:10:49.894247 140600545822528 submission_runner.py:406] Time since start: 7508.79s, 	Step: 21273, 	{'train/accuracy': 0.7481863498687744, 'train/loss': 1.089377760887146, 'validation/accuracy': 0.6569399833679199, 'validation/loss': 1.5011718273162842, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.186185359954834, 'test/num_examples': 10000, 'score': 7191.486765861511, 'total_duration': 7508.787938117981, 'accumulated_submission_time': 7191.486765861511, 'accumulated_eval_time': 300.86938977241516, 'accumulated_logging_time': 16.174423933029175}
I0420 02:10:49.909002 140422336718592 logging_writer.py:48] [21273] accumulated_eval_time=300.869390, accumulated_logging_time=16.174424, accumulated_submission_time=7191.486766, global_step=21273, preemption_count=0, score=7191.486766, test/accuracy=0.530000, test/loss=2.186185, test/num_examples=10000, total_duration=7508.787938, train/accuracy=0.748186, train/loss=1.089378, validation/accuracy=0.656940, validation/loss=1.501172, validation/num_examples=50000
I0420 02:10:50.167236 140600545822528 checkpoints.py:356] Saving checkpoint at step: 21273
I0420 02:10:51.176609 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_21273
I0420 02:10:51.194124 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_21273.
I0420 02:11:00.540019 140422915536640 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7328269481658936, loss=2.483966827392578
I0420 02:11:34.153187 140422328325888 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8265499472618103, loss=2.4502062797546387
I0420 02:12:07.575012 140422915536640 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8303934335708618, loss=2.4618992805480957
I0420 02:12:41.068872 140422328325888 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7645516395568848, loss=2.5142807960510254
I0420 02:13:14.475605 140422915536640 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.3621083498001099, loss=2.5445213317871094
I0420 02:13:48.009413 140422328325888 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8095089197158813, loss=2.464742660522461
I0420 02:14:21.534092 140422915536640 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6175255179405212, loss=2.484111785888672
I0420 02:14:55.137800 140422328325888 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9751808643341064, loss=2.4463906288146973
I0420 02:15:28.727463 140422915536640 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7754654288291931, loss=2.522474527359009
I0420 02:16:02.245079 140422328325888 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6597007513046265, loss=2.4447033405303955
I0420 02:16:35.889876 140422915536640 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9980574250221252, loss=2.4707300662994385
I0420 02:17:09.439503 140422328325888 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.858362078666687, loss=2.4822072982788086
I0420 02:17:43.087498 140422915536640 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6423115730285645, loss=2.484205722808838
I0420 02:18:16.619308 140422328325888 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7113702297210693, loss=2.4034149646759033
I0420 02:18:50.112860 140422915536640 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.8916839361190796, loss=2.450246572494507
I0420 02:19:21.428415 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:19:28.843295 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:19:39.232146 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:19:41.537802 140600545822528 submission_runner.py:406] Time since start: 8040.43s, 	Step: 22795, 	{'train/accuracy': 0.7533880472183228, 'train/loss': 1.0920625925064087, 'validation/accuracy': 0.6692999601364136, 'validation/loss': 1.4812936782836914, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.135310173034668, 'test/num_examples': 10000, 'score': 7701.6983597278595, 'total_duration': 8040.431544780731, 'accumulated_submission_time': 7701.6983597278595, 'accumulated_eval_time': 320.9779169559479, 'accumulated_logging_time': 17.479902505874634}
I0420 02:19:41.547382 140422328325888 logging_writer.py:48] [22795] accumulated_eval_time=320.977917, accumulated_logging_time=17.479903, accumulated_submission_time=7701.698360, global_step=22795, preemption_count=0, score=7701.698360, test/accuracy=0.542500, test/loss=2.135310, test/num_examples=10000, total_duration=8040.431545, train/accuracy=0.753388, train/loss=1.092063, validation/accuracy=0.669300, validation/loss=1.481294, validation/num_examples=50000
I0420 02:19:41.813559 140600545822528 checkpoints.py:356] Saving checkpoint at step: 22795
I0420 02:19:42.812237 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_22795
I0420 02:19:42.828879 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_22795.
I0420 02:19:44.829928 140422915536640 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6744053959846497, loss=2.4588027000427246
I0420 02:20:18.390485 140422319933184 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7444233894348145, loss=2.5579283237457275
I0420 02:20:51.800362 140422915536640 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7551698088645935, loss=2.4152190685272217
I0420 02:21:25.266540 140422319933184 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7049179673194885, loss=2.4226231575012207
I0420 02:21:58.727740 140422915536640 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.7412651181221008, loss=2.4143497943878174
I0420 02:22:32.305071 140422319933184 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9561182260513306, loss=2.4343221187591553
I0420 02:23:05.743129 140422915536640 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.7005374431610107, loss=2.480541944503784
I0420 02:23:39.105498 140422319933184 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8138562440872192, loss=2.401498794555664
I0420 02:24:12.662062 140422915536640 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6796401739120483, loss=2.4111080169677734
I0420 02:24:46.227579 140422319933184 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6170459985733032, loss=2.4125847816467285
I0420 02:25:19.648128 140422915536640 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6629504561424255, loss=2.4082162380218506
I0420 02:25:53.208018 140422319933184 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9123793840408325, loss=2.395522356033325
I0420 02:26:26.811204 140422915536640 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.86020827293396, loss=2.480370044708252
I0420 02:27:00.348178 140422319933184 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7026047110557556, loss=2.4057259559631348
I0420 02:27:33.721659 140422915536640 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7974187135696411, loss=2.401263475418091
I0420 02:28:07.293825 140422319933184 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6043534874916077, loss=2.447129249572754
I0420 02:28:13.084452 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:28:21.098344 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:28:31.566010 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:28:33.701971 140600545822528 submission_runner.py:406] Time since start: 8572.60s, 	Step: 24319, 	{'train/accuracy': 0.7596061825752258, 'train/loss': 1.0270179510116577, 'validation/accuracy': 0.6723200082778931, 'validation/loss': 1.4264025688171387, 'validation/num_examples': 50000, 'test/accuracy': 0.5422000288963318, 'test/loss': 2.1290903091430664, 'test/num_examples': 10000, 'score': 8211.932029247284, 'total_duration': 8572.595902204514, 'accumulated_submission_time': 8211.932029247284, 'accumulated_eval_time': 341.5947380065918, 'accumulated_logging_time': 18.775909423828125}
I0420 02:28:33.717190 140422915536640 logging_writer.py:48] [24319] accumulated_eval_time=341.594738, accumulated_logging_time=18.775909, accumulated_submission_time=8211.932029, global_step=24319, preemption_count=0, score=8211.932029, test/accuracy=0.542200, test/loss=2.129090, test/num_examples=10000, total_duration=8572.595902, train/accuracy=0.759606, train/loss=1.027018, validation/accuracy=0.672320, validation/loss=1.426403, validation/num_examples=50000
I0420 02:28:33.988180 140600545822528 checkpoints.py:356] Saving checkpoint at step: 24319
I0420 02:28:35.014942 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_24319
I0420 02:28:35.036095 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_24319.
I0420 02:29:02.622807 140422319933184 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8168578147888184, loss=2.398693084716797
I0420 02:29:36.259555 140422311540480 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7948978543281555, loss=2.413036346435547
I0420 02:30:09.795549 140422319933184 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.7693198919296265, loss=2.3791232109069824
I0420 02:30:43.236351 140422311540480 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.689666211605072, loss=2.422112226486206
I0420 02:31:16.670902 140422319933184 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8797636032104492, loss=2.383589744567871
I0420 02:31:50.138717 140422311540480 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6397278308868408, loss=2.397672176361084
I0420 02:32:23.647295 140422319933184 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7319165468215942, loss=2.370482921600342
I0420 02:32:57.090595 140422311540480 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.732048749923706, loss=2.3912978172302246
I0420 02:33:30.671294 140422319933184 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.705418050289154, loss=2.416569948196411
I0420 02:34:04.094601 140422311540480 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6781901717185974, loss=2.408799886703491
I0420 02:34:37.592056 140422319933184 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7145925164222717, loss=2.4162404537200928
I0420 02:35:11.176233 140422311540480 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6448217630386353, loss=2.4304158687591553
I0420 02:35:44.676377 140422319933184 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6913370490074158, loss=2.395977258682251
I0420 02:36:18.233074 140422311540480 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.7168433666229248, loss=2.4104175567626953
I0420 02:36:51.701180 140422319933184 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.659105122089386, loss=2.345954179763794
I0420 02:37:05.172891 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:37:12.597012 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:37:23.213825 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:37:25.365413 140600545822528 submission_runner.py:406] Time since start: 9104.26s, 	Step: 25842, 	{'train/accuracy': 0.7612802982330322, 'train/loss': 1.0424156188964844, 'validation/accuracy': 0.6717399954795837, 'validation/loss': 1.4530476331710815, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1623427867889404, 'test/num_examples': 10000, 'score': 8722.045892238617, 'total_duration': 9104.258799552917, 'accumulated_submission_time': 8722.045892238617, 'accumulated_eval_time': 361.78603076934814, 'accumulated_logging_time': 20.115727424621582}
I0420 02:37:25.380458 140422311540480 logging_writer.py:48] [25842] accumulated_eval_time=361.786031, accumulated_logging_time=20.115727, accumulated_submission_time=8722.045892, global_step=25842, preemption_count=0, score=8722.045892, test/accuracy=0.539800, test/loss=2.162343, test/num_examples=10000, total_duration=9104.258800, train/accuracy=0.761280, train/loss=1.042416, validation/accuracy=0.671740, validation/loss=1.453048, validation/num_examples=50000
I0420 02:37:25.729596 140600545822528 checkpoints.py:356] Saving checkpoint at step: 25842
I0420 02:37:26.782947 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_25842
I0420 02:37:26.802614 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_25842.
I0420 02:37:46.653303 140422319933184 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5746665000915527, loss=2.3932459354400635
I0420 02:38:20.168290 140420357007104 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6872860193252563, loss=2.447277545928955
I0420 02:38:53.738970 140422319933184 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.648967444896698, loss=2.3839399814605713
I0420 02:39:27.383366 140420357007104 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7959585189819336, loss=2.386820077896118
I0420 02:40:01.000200 140422319933184 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7163272500038147, loss=2.2956299781799316
I0420 02:40:34.394940 140420357007104 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5853769779205322, loss=2.315756320953369
I0420 02:41:07.860864 140422319933184 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9060569405555725, loss=2.5233731269836426
I0420 02:41:41.484703 140420357007104 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6086505055427551, loss=2.3700711727142334
I0420 02:42:14.863130 140422319933184 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6588804125785828, loss=2.3602700233459473
I0420 02:42:48.333897 140420357007104 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6507426500320435, loss=2.402416944503784
I0420 02:43:22.042030 140422319933184 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.698768675327301, loss=2.3412652015686035
I0420 02:43:55.496438 140420357007104 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.600412130355835, loss=2.348029613494873
I0420 02:44:29.052759 140422319933184 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7271646857261658, loss=2.348249673843384
I0420 02:45:02.436095 140420357007104 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8429037928581238, loss=2.4015614986419678
I0420 02:45:35.903329 140422319933184 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.797287106513977, loss=2.386547088623047
I0420 02:45:56.820916 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:46:04.413617 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:46:15.040052 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:46:17.163617 140600545822528 submission_runner.py:406] Time since start: 9636.06s, 	Step: 27364, 	{'train/accuracy': 0.7956592440605164, 'train/loss': 0.913195013999939, 'validation/accuracy': 0.6757799983024597, 'validation/loss': 1.4284703731536865, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.1155343055725098, 'test/num_examples': 10000, 'score': 9232.035513877869, 'total_duration': 9636.057227611542, 'accumulated_submission_time': 9232.035513877869, 'accumulated_eval_time': 382.12771439552307, 'accumulated_logging_time': 21.564436674118042}
I0420 02:46:17.173537 140420357007104 logging_writer.py:48] [27364] accumulated_eval_time=382.127714, accumulated_logging_time=21.564437, accumulated_submission_time=9232.035514, global_step=27364, preemption_count=0, score=9232.035514, test/accuracy=0.548800, test/loss=2.115534, test/num_examples=10000, total_duration=9636.057228, train/accuracy=0.795659, train/loss=0.913195, validation/accuracy=0.675780, validation/loss=1.428470, validation/num_examples=50000
I0420 02:46:17.447791 140600545822528 checkpoints.py:356] Saving checkpoint at step: 27364
I0420 02:46:18.463270 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_27364
I0420 02:46:18.484935 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_27364.
I0420 02:46:30.797662 140422319933184 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7795097827911377, loss=2.370037078857422
I0420 02:47:04.333173 140420348614400 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8341110348701477, loss=2.3367269039154053
I0420 02:47:37.746929 140422319933184 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6523061394691467, loss=2.3162975311279297
I0420 02:48:11.240917 140420348614400 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5262682437896729, loss=2.2989296913146973
I0420 02:48:44.656935 140422319933184 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5673347115516663, loss=2.332901954650879
I0420 02:49:18.191031 140420348614400 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5190641283988953, loss=2.3130438327789307
I0420 02:49:51.053380 140600545822528 spec.py:298] Evaluating on the training split.
I0420 02:49:58.770917 140600545822528 spec.py:310] Evaluating on the validation split.
I0420 02:50:09.284746 140600545822528 spec.py:326] Evaluating on the test split.
I0420 02:50:11.414242 140600545822528 submission_runner.py:406] Time since start: 9870.31s, 	Step: 28000, 	{'train/accuracy': 0.7782007455825806, 'train/loss': 0.9584642052650452, 'validation/accuracy': 0.6809399724006653, 'validation/loss': 1.3968218564987183, 'validation/num_examples': 50000, 'test/accuracy': 0.5526000261306763, 'test/loss': 2.0817413330078125, 'test/num_examples': 10000, 'score': 9444.590440750122, 'total_duration': 9870.307851552963, 'accumulated_submission_time': 9444.590440750122, 'accumulated_eval_time': 402.4875576496124, 'accumulated_logging_time': 22.891883611679077}
I0420 02:50:11.425862 140422319933184 logging_writer.py:48] [28000] accumulated_eval_time=402.487558, accumulated_logging_time=22.891884, accumulated_submission_time=9444.590441, global_step=28000, preemption_count=0, score=9444.590441, test/accuracy=0.552600, test/loss=2.081741, test/num_examples=10000, total_duration=9870.307852, train/accuracy=0.778201, train/loss=0.958464, validation/accuracy=0.680940, validation/loss=1.396822, validation/num_examples=50000
I0420 02:50:11.781830 140600545822528 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 02:50:12.845261 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0420 02:50:12.867046 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0420 02:50:12.886588 140420348614400 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=9444.590441
I0420 02:50:13.062067 140600545822528 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 02:50:14.450104 140600545822528 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0420 02:50:14.468084 140600545822528 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0420 02:50:14.846956 140600545822528 submission_runner.py:567] Tuning trial 1/1
I0420 02:50:14.847987 140600545822528 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 02:50:14.851762 140600545822528 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.911112308502197, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.911139965057373, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911125183105469, 'test/num_examples': 10000, 'score': 49.77313995361328, 'total_duration': 90.65549921989441, 'accumulated_submission_time': 49.77313995361328, 'accumulated_eval_time': 40.88221287727356, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1515, {'train/accuracy': 0.1436941921710968, 'train/loss': 4.550468921661377, 'validation/accuracy': 0.12814000248908997, 'validation/loss': 4.667636394500732, 'validation/num_examples': 50000, 'test/accuracy': 0.09540000557899475, 'test/loss': 5.030379295349121, 'test/num_examples': 10000, 'score': 559.7901608943939, 'total_duration': 618.4342210292816, 'accumulated_submission_time': 559.7901608943939, 'accumulated_eval_time': 57.72358989715576, 'accumulated_logging_time': 0.9014766216278076, 'global_step': 1515, 'preemption_count': 0}), (3029, {'train/accuracy': 0.3011997640132904, 'train/loss': 3.3297603130340576, 'validation/accuracy': 0.27261999249458313, 'validation/loss': 3.5079665184020996, 'validation/num_examples': 50000, 'test/accuracy': 0.20670001208782196, 'test/loss': 4.086902141571045, 'test/num_examples': 10000, 'score': 1069.9186408519745, 'total_duration': 1146.2085344791412, 'accumulated_submission_time': 1069.9186408519745, 'accumulated_eval_time': 74.48555088043213, 'accumulated_logging_time': 1.7671406269073486, 'global_step': 3029, 'preemption_count': 0}), (4546, {'train/accuracy': 0.41376355290412903, 'train/loss': 2.7099032402038574, 'validation/accuracy': 0.3866199851036072, 'validation/loss': 2.8762223720550537, 'validation/num_examples': 50000, 'test/accuracy': 0.28060001134872437, 'test/loss': 3.5524067878723145, 'test/num_examples': 10000, 'score': 1580.1585001945496, 'total_duration': 1674.071632385254, 'accumulated_submission_time': 1580.1585001945496, 'accumulated_eval_time': 91.19546103477478, 'accumulated_logging_time': 2.6622729301452637, 'global_step': 4546, 'preemption_count': 0}), (6064, {'train/accuracy': 0.4940609037876129, 'train/loss': 2.2376341819763184, 'validation/accuracy': 0.460099995136261, 'validation/loss': 2.41156268119812, 'validation/num_examples': 50000, 'test/accuracy': 0.35500001907348633, 'test/loss': 3.0813822746276855, 'test/num_examples': 10000, 'score': 2090.365306377411, 'total_duration': 2201.974023103714, 'accumulated_submission_time': 2090.365306377411, 'accumulated_eval_time': 107.90387511253357, 'accumulated_logging_time': 3.6314122676849365, 'global_step': 6064, 'preemption_count': 0}), (7580, {'train/accuracy': 0.5524752736091614, 'train/loss': 1.9987709522247314, 'validation/accuracy': 0.5173199772834778, 'validation/loss': 2.1648716926574707, 'validation/num_examples': 50000, 'test/accuracy': 0.40150001645088196, 'test/loss': 2.843662738800049, 'test/num_examples': 10000, 'score': 2600.3966233730316, 'total_duration': 2730.22079205513, 'accumulated_submission_time': 2600.3966233730316, 'accumulated_eval_time': 124.81492257118225, 'accumulated_logging_time': 4.91765832901001, 'global_step': 7580, 'preemption_count': 0}), (9099, {'train/accuracy': 0.6389110088348389, 'train/loss': 1.5943671464920044, 'validation/accuracy': 0.5543199777603149, 'validation/loss': 1.9836335182189941, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.6646745204925537, 'test/num_examples': 10000, 'score': 3110.5579011440277, 'total_duration': 3263.1297419071198, 'accumulated_submission_time': 3110.5579011440277, 'accumulated_eval_time': 146.2635464668274, 'accumulated_logging_time': 6.198667764663696, 'global_step': 9099, 'preemption_count': 0}), (10622, {'train/accuracy': 0.6453483700752258, 'train/loss': 1.564949631690979, 'validation/accuracy': 0.5787000060081482, 'validation/loss': 1.8903650045394897, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.582447052001953, 'test/num_examples': 10000, 'score': 3620.7630009651184, 'total_duration': 3791.567970275879, 'accumulated_submission_time': 3620.7630009651184, 'accumulated_eval_time': 163.43114829063416, 'accumulated_logging_time': 7.246370077133179, 'global_step': 10622, 'preemption_count': 0}), (12143, {'train/accuracy': 0.6630261540412903, 'train/loss': 1.4628506898880005, 'validation/accuracy': 0.6026999950408936, 'validation/loss': 1.7409168481826782, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.4606869220733643, 'test/num_examples': 10000, 'score': 4130.833471059799, 'total_duration': 4321.2903571128845, 'accumulated_submission_time': 4130.833471059799, 'accumulated_eval_time': 181.78838205337524, 'accumulated_logging_time': 8.523331642150879, 'global_step': 12143, 'preemption_count': 0}), (13663, {'train/accuracy': 0.6822584271430969, 'train/loss': 1.3913222551345825, 'validation/accuracy': 0.6205599904060364, 'validation/loss': 1.68320631980896, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.381999969482422, 'test/num_examples': 10000, 'score': 4640.92102599144, 'total_duration': 4851.8709416389465, 'accumulated_submission_time': 4640.92102599144, 'accumulated_eval_time': 201.05599927902222, 'accumulated_logging_time': 9.729688882827759, 'global_step': 13663, 'preemption_count': 0}), (15187, {'train/accuracy': 0.6968271732330322, 'train/loss': 1.3315984010696411, 'validation/accuracy': 0.6311599612236023, 'validation/loss': 1.6334296464920044, 'validation/num_examples': 50000, 'test/accuracy': 0.503000020980835, 'test/loss': 2.3141348361968994, 'test/num_examples': 10000, 'score': 5151.167583703995, 'total_duration': 5382.40825176239, 'accumulated_submission_time': 5151.167583703995, 'accumulated_eval_time': 220.1064829826355, 'accumulated_logging_time': 10.951069593429565, 'global_step': 15187, 'preemption_count': 0}), (16709, {'train/accuracy': 0.7126315236091614, 'train/loss': 1.2631067037582397, 'validation/accuracy': 0.6458399891853333, 'validation/loss': 1.5668915510177612, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.2512903213500977, 'test/num_examples': 10000, 'score': 5661.157910823822, 'total_duration': 5913.784823894501, 'accumulated_submission_time': 5661.157910823822, 'accumulated_eval_time': 240.18697547912598, 'accumulated_logging_time': 12.238005876541138, 'global_step': 16709, 'preemption_count': 0}), (18231, {'train/accuracy': 0.7649075388908386, 'train/loss': 1.0175938606262207, 'validation/accuracy': 0.6548799872398376, 'validation/loss': 1.503351092338562, 'validation/num_examples': 50000, 'test/accuracy': 0.5241000056266785, 'test/loss': 2.2087440490722656, 'test/num_examples': 10000, 'score': 6171.284921169281, 'total_duration': 6445.715316057205, 'accumulated_submission_time': 6171.284921169281, 'accumulated_eval_time': 260.63455629348755, 'accumulated_logging_time': 13.575388193130493, 'global_step': 18231, 'preemption_count': 0}), (19752, {'train/accuracy': 0.7556201815605164, 'train/loss': 1.0837923288345337, 'validation/accuracy': 0.6590399742126465, 'validation/loss': 1.5099282264709473, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.2003285884857178, 'test/num_examples': 10000, 'score': 6681.4698033332825, 'total_duration': 6976.999369621277, 'accumulated_submission_time': 6681.4698033332825, 'accumulated_eval_time': 280.4039480686188, 'accumulated_logging_time': 14.886697769165039, 'global_step': 19752, 'preemption_count': 0}), (21273, {'train/accuracy': 0.7481863498687744, 'train/loss': 1.089377760887146, 'validation/accuracy': 0.6569399833679199, 'validation/loss': 1.5011718273162842, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.186185359954834, 'test/num_examples': 10000, 'score': 7191.486765861511, 'total_duration': 7508.787938117981, 'accumulated_submission_time': 7191.486765861511, 'accumulated_eval_time': 300.86938977241516, 'accumulated_logging_time': 16.174423933029175, 'global_step': 21273, 'preemption_count': 0}), (22795, {'train/accuracy': 0.7533880472183228, 'train/loss': 1.0920625925064087, 'validation/accuracy': 0.6692999601364136, 'validation/loss': 1.4812936782836914, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.135310173034668, 'test/num_examples': 10000, 'score': 7701.6983597278595, 'total_duration': 8040.431544780731, 'accumulated_submission_time': 7701.6983597278595, 'accumulated_eval_time': 320.9779169559479, 'accumulated_logging_time': 17.479902505874634, 'global_step': 22795, 'preemption_count': 0}), (24319, {'train/accuracy': 0.7596061825752258, 'train/loss': 1.0270179510116577, 'validation/accuracy': 0.6723200082778931, 'validation/loss': 1.4264025688171387, 'validation/num_examples': 50000, 'test/accuracy': 0.5422000288963318, 'test/loss': 2.1290903091430664, 'test/num_examples': 10000, 'score': 8211.932029247284, 'total_duration': 8572.595902204514, 'accumulated_submission_time': 8211.932029247284, 'accumulated_eval_time': 341.5947380065918, 'accumulated_logging_time': 18.775909423828125, 'global_step': 24319, 'preemption_count': 0}), (25842, {'train/accuracy': 0.7612802982330322, 'train/loss': 1.0424156188964844, 'validation/accuracy': 0.6717399954795837, 'validation/loss': 1.4530476331710815, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1623427867889404, 'test/num_examples': 10000, 'score': 8722.045892238617, 'total_duration': 9104.258799552917, 'accumulated_submission_time': 8722.045892238617, 'accumulated_eval_time': 361.78603076934814, 'accumulated_logging_time': 20.115727424621582, 'global_step': 25842, 'preemption_count': 0}), (27364, {'train/accuracy': 0.7956592440605164, 'train/loss': 0.913195013999939, 'validation/accuracy': 0.6757799983024597, 'validation/loss': 1.4284703731536865, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.1155343055725098, 'test/num_examples': 10000, 'score': 9232.035513877869, 'total_duration': 9636.057227611542, 'accumulated_submission_time': 9232.035513877869, 'accumulated_eval_time': 382.12771439552307, 'accumulated_logging_time': 21.564436674118042, 'global_step': 27364, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7782007455825806, 'train/loss': 0.9584642052650452, 'validation/accuracy': 0.6809399724006653, 'validation/loss': 1.3968218564987183, 'validation/num_examples': 50000, 'test/accuracy': 0.5526000261306763, 'test/loss': 2.0817413330078125, 'test/num_examples': 10000, 'score': 9444.590440750122, 'total_duration': 9870.307851552963, 'accumulated_submission_time': 9444.590440750122, 'accumulated_eval_time': 402.4875576496124, 'accumulated_logging_time': 22.891883611679077, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0420 02:50:14.851880 140600545822528 submission_runner.py:570] Timing: 9444.590440750122
I0420 02:50:14.851943 140600545822528 submission_runner.py:571] ====================
I0420 02:50:14.852071 140600545822528 submission_runner.py:631] Final imagenet_resnet score: 9444.590440750122
