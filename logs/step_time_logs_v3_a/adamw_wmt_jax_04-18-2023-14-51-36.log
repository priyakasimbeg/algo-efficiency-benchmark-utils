I0418 14:51:58.069368 140715416078144 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_adamw/wmt_jax.
I0418 14:51:58.132836 140715416078144 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 14:51:58.989480 140715416078144 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0418 14:51:58.990179 140715416078144 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 14:51:58.995315 140715416078144 submission_runner.py:528] Using RNG seed 2999311776
I0418 14:52:01.649708 140715416078144 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 14:52:01.649934 140715416078144 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1.
I0418 14:52:01.650222 140715416078144 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/hparams.json.
I0418 14:52:01.784685 140715416078144 submission_runner.py:232] Initializing dataset.
I0418 14:52:01.793534 140715416078144 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:52:01.797609 140715416078144 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:52:01.797759 140715416078144 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:52:01.922675 140715416078144 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:52:03.874718 140715416078144 submission_runner.py:239] Initializing model.
I0418 14:52:15.739949 140715416078144 submission_runner.py:249] Initializing optimizer.
I0418 14:52:16.643342 140715416078144 submission_runner.py:256] Initializing metrics bundle.
I0418 14:52:16.643531 140715416078144 submission_runner.py:273] Initializing checkpoint and logger.
I0418 14:52:16.644626 140715416078144 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1 with prefix checkpoint_
I0418 14:52:16.644880 140715416078144 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 14:52:16.644941 140715416078144 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 14:52:17.613542 140715416078144 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/meta_data_0.json.
I0418 14:52:17.614420 140715416078144 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/flags_0.json.
I0418 14:52:17.618607 140715416078144 submission_runner.py:309] Starting training loop.
I0418 14:52:50.725557 140539408148224 logging_writer.py:48] [0] global_step=0, grad_norm=4.915829181671143, loss=10.979955673217773
I0418 14:52:50.741518 140715416078144 spec.py:298] Evaluating on the training split.
I0418 14:52:50.744197 140715416078144 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:52:50.746823 140715416078144 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:52:50.746932 140715416078144 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:52:50.777481 140715416078144 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:52:58.777853 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 14:58:03.834229 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 14:58:03.838064 140715416078144 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 14:58:03.841644 140715416078144 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:58:03.841759 140715416078144 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:58:03.871800 140715416078144 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 14:58:11.159475 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:03:08.896910 140715416078144 spec.py:326] Evaluating on the test split.
I0418 15:03:08.899363 140715416078144 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:03:08.902068 140715416078144 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 15:03:08.902215 140715416078144 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 15:03:08.931668 140715416078144 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:03:16.004801 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:08:07.980830 140715416078144 submission_runner.py:406] Time since start: 950.36s, 	Step: 1, 	{'train/accuracy': 0.0005528552574105561, 'train/loss': 10.994253158569336, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.98425006866455, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.002135276794434, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 33.12275242805481, 'total_duration': 950.3621320724487, 'accumulated_submission_time': 33.12275242805481, 'accumulated_eval_time': 917.2392385005951, 'accumulated_logging_time': 0}
I0418 15:08:08.000302 140528325433088 logging_writer.py:48] [1] accumulated_eval_time=917.239239, accumulated_logging_time=0, accumulated_submission_time=33.122752, global_step=1, preemption_count=0, score=33.122752, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.002135, test/num_examples=3003, total_duration=950.362132, train/accuracy=0.000553, train/bleu=0.000000, train/loss=10.994253, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.984250, validation/num_examples=3000
I0418 15:08:09.051639 140715416078144 checkpoints.py:356] Saving checkpoint at step: 1
I0418 15:08:12.783660 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_1
I0418 15:08:12.787974 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_1.
I0418 15:08:48.782146 140528283469568 logging_writer.py:48] [100] global_step=100, grad_norm=0.23834729194641113, loss=8.538520812988281
I0418 15:09:24.880103 140527856670464 logging_writer.py:48] [200] global_step=200, grad_norm=1.2441974878311157, loss=7.999460697174072
I0418 15:10:00.979748 140528283469568 logging_writer.py:48] [300] global_step=300, grad_norm=0.9642991423606873, loss=7.490293502807617
I0418 15:10:37.116830 140527856670464 logging_writer.py:48] [400] global_step=400, grad_norm=0.8068519234657288, loss=7.246336460113525
I0418 15:11:13.210363 140528283469568 logging_writer.py:48] [500] global_step=500, grad_norm=0.8309080004692078, loss=6.915376663208008
I0418 15:11:49.323098 140527856670464 logging_writer.py:48] [600] global_step=600, grad_norm=0.61894690990448, loss=6.5901198387146
I0418 15:12:25.470191 140528283469568 logging_writer.py:48] [700] global_step=700, grad_norm=2.283278465270996, loss=7.33607816696167
I0418 15:13:01.567102 140527856670464 logging_writer.py:48] [800] global_step=800, grad_norm=0.506405234336853, loss=6.1789021492004395
I0418 15:13:37.630072 140528283469568 logging_writer.py:48] [900] global_step=900, grad_norm=0.5129897594451904, loss=5.986870288848877
I0418 15:14:13.765543 140527856670464 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5198413133621216, loss=5.857384204864502
I0418 15:14:49.886567 140528283469568 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.693895697593689, loss=8.217592239379883
I0418 15:15:25.900758 140527856670464 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.556347668170929, loss=6.5172624588012695
I0418 15:16:01.957347 140528283469568 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8126475811004639, loss=6.302739143371582
I0418 15:16:38.073050 140527856670464 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.4641331732273102, loss=5.548433303833008
I0418 15:17:14.158298 140528283469568 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.43494316935539246, loss=5.386086940765381
I0418 15:17:50.294155 140527856670464 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6069052219390869, loss=5.289709091186523
I0418 15:18:26.380268 140528283469568 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.697279155254364, loss=5.143756866455078
I0418 15:19:02.502449 140527856670464 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5581275820732117, loss=4.934695720672607
I0418 15:19:38.561344 140528283469568 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5073124170303345, loss=4.835169792175293
I0418 15:20:14.652019 140527856670464 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5448537468910217, loss=4.697158336639404
I0418 15:20:50.723104 140528283469568 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5157755017280579, loss=4.576305389404297
I0418 15:21:26.815426 140527856670464 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.47833240032196045, loss=4.524591445922852
I0418 15:22:02.909328 140528283469568 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5441077947616577, loss=4.4334540367126465
I0418 15:22:13.087600 140715416078144 spec.py:298] Evaluating on the training split.
I0418 15:22:16.072436 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:26:35.308194 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 15:26:37.966307 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:30:56.016216 140715416078144 spec.py:326] Evaluating on the test split.
I0418 15:30:58.715853 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:34:41.009435 140715416078144 submission_runner.py:406] Time since start: 2543.39s, 	Step: 2330, 	{'train/accuracy': 0.45732381939888, 'train/loss': 3.49275803565979, 'train/bleu': 19.191653415648354, 'validation/accuracy': 0.44964104890823364, 'validation/loss': 3.5597143173217773, 'validation/bleu': 14.477704757034, 'validation/num_examples': 3000, 'test/accuracy': 0.44463425874710083, 'test/loss': 3.672243595123291, 'test/bleu': 13.248477625266506, 'test/num_examples': 3003, 'score': 873.3912582397461, 'total_duration': 2543.3907449245453, 'accumulated_submission_time': 873.3912582397461, 'accumulated_eval_time': 1665.1610267162323, 'accumulated_logging_time': 4.810873985290527}
I0418 15:34:41.017864 140528275076864 logging_writer.py:48] [2330] accumulated_eval_time=1665.161027, accumulated_logging_time=4.810874, accumulated_submission_time=873.391258, global_step=2330, preemption_count=0, score=873.391258, test/accuracy=0.444634, test/bleu=13.248478, test/loss=3.672244, test/num_examples=3003, total_duration=2543.390745, train/accuracy=0.457324, train/bleu=19.191653, train/loss=3.492758, validation/accuracy=0.449641, validation/bleu=14.477705, validation/loss=3.559714, validation/num_examples=3000
I0418 15:34:42.048612 140715416078144 checkpoints.py:356] Saving checkpoint at step: 2330
I0418 15:34:45.784239 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_2330
I0418 15:34:45.788594 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_2330.
I0418 15:35:11.330615 140528266684160 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.03057825192809105, loss=7.466826438903809
I0418 15:35:47.358115 140527962568448 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.43636012077331543, loss=6.784225940704346
I0418 15:36:23.377320 140528266684160 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.42550304532051086, loss=6.419971466064453
I0418 15:36:59.407602 140527962568448 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.39753714203834534, loss=6.121487140655518
I0418 15:37:35.411653 140528266684160 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.23396562039852142, loss=5.788193225860596
I0418 15:38:11.418231 140527962568448 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.24936354160308838, loss=5.693335056304932
I0418 15:38:47.474051 140528266684160 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1924641132354736, loss=7.300548553466797
I0418 15:39:23.510439 140527962568448 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.21066467463970184, loss=5.659905433654785
I0418 15:39:59.585599 140528266684160 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.22146552801132202, loss=5.544084548950195
I0418 15:40:35.633602 140527962568448 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.37377601861953735, loss=5.53800630569458
I0418 15:41:11.686753 140528266684160 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.4160097539424896, loss=5.285577774047852
I0418 15:41:47.797798 140527962568448 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.373618483543396, loss=4.540211200714111
I0418 15:42:23.881499 140528266684160 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.38840922713279724, loss=4.4457316398620605
I0418 15:42:59.994262 140527962568448 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.3331765830516815, loss=4.253300666809082
I0418 15:43:36.102153 140528266684160 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.34921255707740784, loss=4.192796230316162
I0418 15:44:12.215035 140527962568448 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.34007394313812256, loss=4.114290237426758
I0418 15:44:48.297395 140528266684160 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3324846625328064, loss=4.07235050201416
I0418 15:45:24.355582 140527962568448 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.3077903091907501, loss=3.947751045227051
I0418 15:46:00.448243 140528266684160 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.30667921900749207, loss=4.0302205085754395
I0418 15:46:36.506058 140527962568448 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3071724772453308, loss=3.9420149326324463
I0418 15:47:12.576639 140528266684160 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.2957323491573334, loss=3.9000189304351807
I0418 15:47:48.673025 140527962568448 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.2759360671043396, loss=3.837449550628662
I0418 15:48:24.765245 140528266684160 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.27282121777534485, loss=3.79256534576416
I0418 15:48:46.108330 140715416078144 spec.py:298] Evaluating on the training split.
I0418 15:48:49.089731 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:51:32.228327 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 15:51:34.875996 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:54:12.747695 140715416078144 spec.py:326] Evaluating on the test split.
I0418 15:54:15.446198 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 15:56:45.843099 140715416078144 submission_runner.py:406] Time since start: 3868.22s, 	Step: 4661, 	{'train/accuracy': 0.5308015942573547, 'train/loss': 2.7565908432006836, 'train/bleu': 23.485023413829285, 'validation/accuracy': 0.53364497423172, 'validation/loss': 2.7434918880462646, 'validation/bleu': 19.346343337672156, 'validation/num_examples': 3000, 'test/accuracy': 0.5307071208953857, 'test/loss': 2.7937731742858887, 'test/bleu': 18.05757484371864, 'test/num_examples': 3003, 'score': 1713.6809618473053, 'total_duration': 3868.224392414093, 'accumulated_submission_time': 1713.6809618473053, 'accumulated_eval_time': 2144.895729780197, 'accumulated_logging_time': 9.593433380126953}
I0418 15:56:45.851007 140527962568448 logging_writer.py:48] [4661] accumulated_eval_time=2144.895730, accumulated_logging_time=9.593433, accumulated_submission_time=1713.680962, global_step=4661, preemption_count=0, score=1713.680962, test/accuracy=0.530707, test/bleu=18.057575, test/loss=2.793773, test/num_examples=3003, total_duration=3868.224392, train/accuracy=0.530802, train/bleu=23.485023, train/loss=2.756591, validation/accuracy=0.533645, validation/bleu=19.346343, validation/loss=2.743492, validation/num_examples=3000
I0418 15:56:46.890197 140715416078144 checkpoints.py:356] Saving checkpoint at step: 4661
I0418 15:56:50.653646 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_4661
I0418 15:56:50.657998 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_4661.
I0418 15:57:05.081954 140528266684160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.24687352776527405, loss=3.7506799697875977
I0418 15:57:41.150069 140527954175744 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.23548835515975952, loss=3.734931230545044
I0418 15:58:17.240645 140528266684160 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.27213895320892334, loss=3.7021539211273193
I0418 15:58:53.324616 140527954175744 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.23485463857650757, loss=3.6655595302581787
I0418 15:59:29.361162 140528266684160 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.2682259678840637, loss=3.6549601554870605
I0418 16:00:05.429872 140527954175744 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.253231406211853, loss=3.751155376434326
I0418 16:00:41.469643 140528266684160 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.24749349057674408, loss=3.7095823287963867
I0418 16:01:17.561286 140527954175744 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.22200778126716614, loss=3.6749844551086426
I0418 16:01:53.612534 140528266684160 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.2079329639673233, loss=3.6745195388793945
I0418 16:02:29.710521 140527954175744 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.19189530611038208, loss=3.597776174545288
I0418 16:03:05.777500 140528266684160 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.2152767926454544, loss=3.6150832176208496
I0418 16:03:41.836919 140527954175744 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.21049216389656067, loss=3.5321249961853027
I0418 16:04:17.868629 140528266684160 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.24512051045894623, loss=3.5815587043762207
I0418 16:04:53.942748 140527954175744 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.1837664097547531, loss=3.5089516639709473
I0418 16:05:29.986222 140528266684160 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.23312048614025116, loss=3.6157615184783936
I0418 16:06:06.047200 140527954175744 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.17377662658691406, loss=3.4705896377563477
I0418 16:06:42.118352 140528266684160 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.1713930368423462, loss=3.491429328918457
I0418 16:07:18.164289 140527954175744 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.174185648560524, loss=3.628040075302124
I0418 16:07:54.214792 140528266684160 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.19156956672668457, loss=3.468134880065918
I0418 16:08:30.208804 140527954175744 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.1625150442123413, loss=3.3765172958374023
I0418 16:09:06.294082 140528266684160 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.17683136463165283, loss=3.348345994949341
I0418 16:09:42.331223 140527954175744 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.16899670660495758, loss=3.3816182613372803
I0418 16:10:18.384812 140528266684160 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.174581378698349, loss=3.41237473487854
I0418 16:10:50.872525 140715416078144 spec.py:298] Evaluating on the training split.
I0418 16:10:53.855112 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:13:36.415891 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 16:13:39.070665 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:16:07.232983 140715416078144 spec.py:326] Evaluating on the test split.
I0418 16:16:09.950550 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:18:35.752015 140715416078144 submission_runner.py:406] Time since start: 5178.13s, 	Step: 6992, 	{'train/accuracy': 0.5851579904556274, 'train/loss': 2.2720530033111572, 'train/bleu': 27.3559208993471, 'validation/accuracy': 0.5922431349754333, 'validation/loss': 2.236572265625, 'validation/bleu': 23.567265847033987, 'validation/num_examples': 3000, 'test/accuracy': 0.5949799418449402, 'test/loss': 2.2268874645233154, 'test/bleu': 22.070120127554063, 'test/num_examples': 3003, 'score': 2553.866166830063, 'total_duration': 5178.133287191391, 'accumulated_submission_time': 2553.866166830063, 'accumulated_eval_time': 2609.775131702423, 'accumulated_logging_time': 14.411986351013184}
I0418 16:18:35.760970 140527954175744 logging_writer.py:48] [6992] accumulated_eval_time=2609.775132, accumulated_logging_time=14.411986, accumulated_submission_time=2553.866167, global_step=6992, preemption_count=0, score=2553.866167, test/accuracy=0.594980, test/bleu=22.070120, test/loss=2.226887, test/num_examples=3003, total_duration=5178.133287, train/accuracy=0.585158, train/bleu=27.355921, train/loss=2.272053, validation/accuracy=0.592243, validation/bleu=23.567266, validation/loss=2.236572, validation/num_examples=3000
I0418 16:18:36.789409 140715416078144 checkpoints.py:356] Saving checkpoint at step: 6992
I0418 16:18:40.529172 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_6992
I0418 16:18:40.533567 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_6992.
I0418 16:18:43.801459 140528266684160 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.15505945682525635, loss=3.432631254196167
I0418 16:19:19.866321 140527945783040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.14848020672798157, loss=3.4304451942443848
I0418 16:19:55.894513 140528266684160 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.14694009721279144, loss=3.359814405441284
I0418 16:20:31.981234 140527945783040 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.15232481062412262, loss=3.308748960494995
I0418 16:21:08.054663 140528266684160 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.1410457193851471, loss=3.3487141132354736
I0418 16:21:44.081375 140527945783040 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.13979202508926392, loss=3.3784079551696777
I0418 16:22:20.113062 140528266684160 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.13047146797180176, loss=3.3492228984832764
I0418 16:22:56.156619 140527945783040 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.13866464793682098, loss=3.3086557388305664
I0418 16:23:32.248564 140528266684160 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.17834709584712982, loss=3.3585078716278076
I0418 16:24:08.286867 140527945783040 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.1414088010787964, loss=3.439396381378174
I0418 16:24:44.338937 140528266684160 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.13988104462623596, loss=3.309950113296509
I0418 16:25:20.426622 140527945783040 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.13128875195980072, loss=3.336435556411743
I0418 16:25:56.434946 140528266684160 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1522868275642395, loss=3.3015146255493164
I0418 16:26:32.451821 140527945783040 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.17791016399860382, loss=3.2069220542907715
I0418 16:27:08.468519 140528266684160 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.1518733650445938, loss=3.2392818927764893
I0418 16:27:44.500327 140527945783040 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.12796491384506226, loss=3.2995011806488037
I0418 16:28:20.500579 140528266684160 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.14449577033519745, loss=3.2430617809295654
I0418 16:28:56.559388 140527945783040 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.13768255710601807, loss=3.213499069213867
I0418 16:29:32.554198 140528266684160 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1275501251220703, loss=3.2905495166778564
I0418 16:30:08.588445 140527945783040 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.13964730501174927, loss=3.2282142639160156
I0418 16:30:44.686190 140528266684160 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1177162304520607, loss=3.2572219371795654
I0418 16:31:20.697560 140527945783040 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.12338472902774811, loss=3.212064743041992
I0418 16:31:56.748324 140528266684160 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1418214589357376, loss=3.3315043449401855
I0418 16:32:32.838901 140527945783040 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.1426984965801239, loss=3.2604682445526123
I0418 16:32:40.839416 140715416078144 spec.py:298] Evaluating on the training split.
I0418 16:32:43.815426 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:35:52.550600 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 16:35:55.196911 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:38:22.407978 140715416078144 spec.py:326] Evaluating on the test split.
I0418 16:38:25.105664 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:40:44.795274 140715416078144 submission_runner.py:406] Time since start: 6507.18s, 	Step: 9324, 	{'train/accuracy': 0.6039654016494751, 'train/loss': 2.12834095954895, 'train/bleu': 28.855309321299373, 'validation/accuracy': 0.6176984906196594, 'validation/loss': 2.0252463817596436, 'validation/bleu': 25.236275868449926, 'validation/num_examples': 3000, 'test/accuracy': 0.6244843602180481, 'test/loss': 1.986832618713379, 'test/bleu': 24.15057416841689, 'test/num_examples': 3003, 'score': 3394.1434614658356, 'total_duration': 6507.176578044891, 'accumulated_submission_time': 3394.1434614658356, 'accumulated_eval_time': 3093.730932712555, 'accumulated_logging_time': 19.19673180580139}
I0418 16:40:44.803570 140528266684160 logging_writer.py:48] [9324] accumulated_eval_time=3093.730933, accumulated_logging_time=19.196732, accumulated_submission_time=3394.143461, global_step=9324, preemption_count=0, score=3394.143461, test/accuracy=0.624484, test/bleu=24.150574, test/loss=1.986833, test/num_examples=3003, total_duration=6507.176578, train/accuracy=0.603965, train/bleu=28.855309, train/loss=2.128341, validation/accuracy=0.617698, validation/bleu=25.236276, validation/loss=2.025246, validation/num_examples=3000
I0418 16:40:45.838461 140715416078144 checkpoints.py:356] Saving checkpoint at step: 9324
I0418 16:40:49.490875 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_9324
I0418 16:40:49.495310 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_9324.
I0418 16:41:17.262077 140527945783040 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.13234657049179077, loss=3.2433454990386963
I0418 16:41:53.341411 140527937390336 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.1296759992837906, loss=3.174276113510132
I0418 16:42:29.438721 140527945783040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.13914290070533752, loss=3.2174417972564697
I0418 16:43:05.509840 140527937390336 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1368490606546402, loss=3.1767983436584473
I0418 16:43:41.575010 140527945783040 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.18032598495483398, loss=3.3201537132263184
I0418 16:44:17.578307 140527937390336 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.1270075887441635, loss=3.204860210418701
I0418 16:44:53.605134 140527945783040 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.13255852460861206, loss=3.2523341178894043
I0418 16:45:29.661391 140527937390336 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.15854831039905548, loss=3.2496697902679443
I0418 16:46:05.696617 140527945783040 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.17390026152133942, loss=3.197038173675537
I0418 16:46:41.778353 140527937390336 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.11392338573932648, loss=3.1598448753356934
I0418 16:47:17.862245 140527945783040 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.14011625945568085, loss=3.13415265083313
I0418 16:47:53.908093 140527937390336 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.1237793043255806, loss=3.1507692337036133
I0418 16:48:29.973693 140527945783040 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.12844465672969818, loss=3.1159310340881348
I0418 16:49:06.019588 140527937390336 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.1739775836467743, loss=3.1822824478149414
I0418 16:49:42.099067 140527945783040 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.12440409511327744, loss=3.168654203414917
I0418 16:50:18.133239 140527937390336 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.125103160738945, loss=3.2256104946136475
I0418 16:50:54.190354 140527945783040 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.14530423283576965, loss=3.209726333618164
I0418 16:51:30.279005 140527937390336 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.16166047751903534, loss=3.151973009109497
I0418 16:52:06.365831 140527945783040 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.13435931503772736, loss=3.126127004623413
I0418 16:52:42.425548 140527937390336 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.16287881135940552, loss=3.138350009918213
I0418 16:53:18.470182 140527945783040 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1683448702096939, loss=3.144770383834839
I0418 16:53:54.520230 140527937390336 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17853239178657532, loss=3.2258856296539307
I0418 16:54:30.564287 140527945783040 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.14451922476291656, loss=3.174504280090332
I0418 16:54:49.727494 140715416078144 spec.py:298] Evaluating on the training split.
I0418 16:54:52.713563 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 16:57:54.704871 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 16:57:57.349126 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:00:25.598215 140715416078144 spec.py:326] Evaluating on the test split.
I0418 17:00:28.305290 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:02:54.431597 140715416078144 submission_runner.py:406] Time since start: 7836.81s, 	Step: 11655, 	{'train/accuracy': 0.6159423589706421, 'train/loss': 2.0386085510253906, 'train/bleu': 29.63912301335959, 'validation/accuracy': 0.6319326162338257, 'validation/loss': 1.9100861549377441, 'validation/bleu': 26.23921655083185, 'validation/num_examples': 3000, 'test/accuracy': 0.6404392719268799, 'test/loss': 1.8646249771118164, 'test/bleu': 25.407876432259638, 'test/num_examples': 3003, 'score': 4234.346390962601, 'total_duration': 7836.812881946564, 'accumulated_submission_time': 4234.346390962601, 'accumulated_eval_time': 3578.4349608421326, 'accumulated_logging_time': 23.900001525878906}
I0418 17:02:54.440499 140527937390336 logging_writer.py:48] [11655] accumulated_eval_time=3578.434961, accumulated_logging_time=23.900002, accumulated_submission_time=4234.346391, global_step=11655, preemption_count=0, score=4234.346391, test/accuracy=0.640439, test/bleu=25.407876, test/loss=1.864625, test/num_examples=3003, total_duration=7836.812882, train/accuracy=0.615942, train/bleu=29.639123, train/loss=2.038609, validation/accuracy=0.631933, validation/bleu=26.239217, validation/loss=1.910086, validation/num_examples=3000
I0418 17:02:55.483778 140715416078144 checkpoints.py:356] Saving checkpoint at step: 11655
I0418 17:02:59.161685 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_11655
I0418 17:02:59.166057 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_11655.
I0418 17:03:15.731590 140527945783040 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.14199566841125488, loss=3.120755910873413
I0418 17:03:51.747423 140527928997632 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1294117569923401, loss=3.135347604751587
I0418 17:04:27.816092 140527945783040 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.12231533974409103, loss=3.112308979034424
I0418 17:05:03.895384 140527928997632 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.13371171057224274, loss=3.1388120651245117
I0418 17:05:39.909425 140527945783040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.10651271045207977, loss=3.12526535987854
I0418 17:06:15.970520 140527928997632 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1778682917356491, loss=3.013667345046997
I0418 17:06:52.046872 140527945783040 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.12674669921398163, loss=3.164034843444824
I0418 17:07:28.077281 140527928997632 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.19033898413181305, loss=3.153036594390869
I0418 17:08:04.113610 140527945783040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1256336271762848, loss=3.098431348800659
I0418 17:08:40.157284 140527928997632 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.1549663245677948, loss=3.1329872608184814
I0418 17:09:16.235298 140527945783040 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1144268587231636, loss=3.1436045169830322
I0418 17:09:52.301670 140527928997632 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.14263851940631866, loss=3.1136529445648193
I0418 17:10:28.382868 140527945783040 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.12672872841358185, loss=3.026413917541504
I0418 17:11:04.455530 140527928997632 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.1574191004037857, loss=3.097654104232788
I0418 17:11:40.513376 140527945783040 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.19891515374183655, loss=3.1778042316436768
I0418 17:12:16.546939 140527928997632 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.11895042657852173, loss=3.0817220211029053
I0418 17:12:52.627557 140527945783040 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.12983326613903046, loss=3.0758848190307617
I0418 17:13:28.679702 140527928997632 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1288999617099762, loss=3.0813302993774414
I0418 17:14:04.730873 140527945783040 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.12107793241739273, loss=3.099900960922241
I0418 17:14:40.790929 140527928997632 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.1284429430961609, loss=3.058717727661133
I0418 17:15:16.847180 140527945783040 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.12633775174617767, loss=3.1526739597320557
I0418 17:15:52.879074 140527928997632 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2032429724931717, loss=3.1013214588165283
I0418 17:16:28.896768 140527945783040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1255803108215332, loss=3.049563407897949
I0418 17:16:59.243250 140715416078144 spec.py:298] Evaluating on the training split.
I0418 17:17:02.238943 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:20:06.285960 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 17:20:08.948586 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:22:33.140737 140715416078144 spec.py:326] Evaluating on the test split.
I0418 17:22:35.861505 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:24:57.651520 140715416078144 submission_runner.py:406] Time since start: 9160.03s, 	Step: 13986, 	{'train/accuracy': 0.6313157081604004, 'train/loss': 1.9062542915344238, 'train/bleu': 30.773204511994493, 'validation/accuracy': 0.64402174949646, 'validation/loss': 1.8263567686080933, 'validation/bleu': 27.291204643785424, 'validation/num_examples': 3000, 'test/accuracy': 0.6524548530578613, 'test/loss': 1.7722917795181274, 'test/bleu': 26.302156885980477, 'test/num_examples': 3003, 'score': 5074.393873214722, 'total_duration': 9160.03282570839, 'accumulated_submission_time': 5074.393873214722, 'accumulated_eval_time': 4056.84317779541, 'accumulated_logging_time': 28.637954711914062}
I0418 17:24:57.660040 140527928997632 logging_writer.py:48] [13986] accumulated_eval_time=4056.843178, accumulated_logging_time=28.637955, accumulated_submission_time=5074.393873, global_step=13986, preemption_count=0, score=5074.393873, test/accuracy=0.652455, test/bleu=26.302157, test/loss=1.772292, test/num_examples=3003, total_duration=9160.032826, train/accuracy=0.631316, train/bleu=30.773205, train/loss=1.906254, validation/accuracy=0.644022, validation/bleu=27.291205, validation/loss=1.826357, validation/num_examples=3000
I0418 17:24:58.698983 140715416078144 checkpoints.py:356] Saving checkpoint at step: 13986
I0418 17:25:02.415737 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_13986
I0418 17:25:02.420257 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_13986.
I0418 17:25:07.840546 140527945783040 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.13076229393482208, loss=3.088480234146118
I0418 17:25:43.897099 140527920604928 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.14806383848190308, loss=3.0599358081817627
I0418 17:26:19.967141 140527945783040 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.1330278217792511, loss=3.0589346885681152
I0418 17:26:56.035423 140527920604928 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1579897552728653, loss=3.086667776107788
I0418 17:27:32.084206 140527945783040 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.14062167704105377, loss=3.0531933307647705
I0418 17:28:08.154511 140527920604928 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1307167410850525, loss=3.096935749053955
I0418 17:28:44.241779 140527945783040 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.12433221936225891, loss=3.0744330883026123
I0418 17:29:20.295513 140527920604928 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.12404725700616837, loss=3.1180975437164307
I0418 17:29:56.326670 140527945783040 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.1838884800672531, loss=3.039433240890503
I0418 17:30:32.394901 140527920604928 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.15664924681186676, loss=3.0386385917663574
I0418 17:31:08.448822 140527945783040 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2066676765680313, loss=3.0622403621673584
I0418 17:31:44.526639 140527920604928 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.12036450952291489, loss=3.04256272315979
I0418 17:32:20.633230 140527945783040 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.11469081044197083, loss=3.0363779067993164
I0418 17:32:56.695010 140527920604928 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.12928645312786102, loss=3.1118037700653076
I0418 17:33:32.770217 140527945783040 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.13885676860809326, loss=3.0315957069396973
I0418 17:34:08.829521 140527920604928 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.12843747437000275, loss=3.090895652770996
I0418 17:34:44.919466 140527945783040 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.13262200355529785, loss=3.12288498878479
I0418 17:35:20.969818 140527920604928 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.15407699346542358, loss=3.0339131355285645
I0418 17:35:57.072748 140527945783040 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.17581191658973694, loss=2.9897682666778564
I0418 17:36:33.102832 140527920604928 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.20464769005775452, loss=2.9586377143859863
I0418 17:37:09.162350 140527945783040 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.12174707651138306, loss=3.0313591957092285
I0418 17:37:45.192162 140527920604928 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1767362803220749, loss=3.0890190601348877
I0418 17:38:21.240173 140527945783040 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.13161084055900574, loss=3.0258333683013916
I0418 17:38:57.298753 140527920604928 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.15803897380828857, loss=3.020066022872925
I0418 17:39:02.773823 140715416078144 spec.py:298] Evaluating on the training split.
I0418 17:39:05.743493 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:41:47.241831 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 17:41:49.875984 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:44:04.320194 140715416078144 spec.py:326] Evaluating on the test split.
I0418 17:44:07.003614 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 17:46:39.954022 140715416078144 submission_runner.py:406] Time since start: 10462.34s, 	Step: 16317, 	{'train/accuracy': 0.6324387788772583, 'train/loss': 1.8962022066116333, 'train/bleu': 30.7472465730952, 'validation/accuracy': 0.651523232460022, 'validation/loss': 1.773362636566162, 'validation/bleu': 27.530528872839465, 'validation/num_examples': 3000, 'test/accuracy': 0.6603102684020996, 'test/loss': 1.7082411050796509, 'test/bleu': 26.832006786460305, 'test/num_examples': 3003, 'score': 5914.717568635941, 'total_duration': 10462.335293769836, 'accumulated_submission_time': 5914.717568635941, 'accumulated_eval_time': 4514.023285150528, 'accumulated_logging_time': 33.41043043136597}
I0418 17:46:39.963138 140527945783040 logging_writer.py:48] [16317] accumulated_eval_time=4514.023285, accumulated_logging_time=33.410430, accumulated_submission_time=5914.717569, global_step=16317, preemption_count=0, score=5914.717569, test/accuracy=0.660310, test/bleu=26.832007, test/loss=1.708241, test/num_examples=3003, total_duration=10462.335294, train/accuracy=0.632439, train/bleu=30.747247, train/loss=1.896202, validation/accuracy=0.651523, validation/bleu=27.530529, validation/loss=1.773363, validation/num_examples=3000
I0418 17:46:41.000256 140715416078144 checkpoints.py:356] Saving checkpoint at step: 16317
I0418 17:46:44.640713 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_16317
I0418 17:46:44.645070 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_16317.
I0418 17:47:14.945097 140527920604928 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.12864375114440918, loss=3.0035688877105713
I0418 17:47:50.988668 140527912212224 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.1242428869009018, loss=3.041141986846924
I0418 17:48:27.057930 140527920604928 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.1245783194899559, loss=3.0101230144500732
I0418 17:49:03.076013 140527912212224 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.11930819600820541, loss=3.011141777038574
I0418 17:49:39.129496 140527920604928 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.22331686317920685, loss=2.9601311683654785
I0418 17:50:15.206496 140527912212224 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15015627443790436, loss=3.0290844440460205
I0418 17:50:51.258839 140527920604928 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.12618540227413177, loss=3.0966544151306152
I0418 17:51:27.314486 140527912212224 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.11966415494680405, loss=2.979357957839966
I0418 17:52:03.399014 140527920604928 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.14691543579101562, loss=3.060018539428711
I0418 17:52:39.429698 140527912212224 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.11190565675497055, loss=2.974534749984741
I0418 17:53:15.476542 140527920604928 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.12254532426595688, loss=2.987912893295288
I0418 17:53:51.551405 140527912212224 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.14104528725147247, loss=2.9942305088043213
I0418 17:54:27.581507 140527920604928 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.1900988072156906, loss=3.017274856567383
I0418 17:55:03.616591 140527912212224 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.26398932933807373, loss=2.9861998558044434
I0418 17:55:39.656708 140527920604928 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.13389921188354492, loss=2.9753267765045166
I0418 17:56:15.695143 140527912212224 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.19593693315982819, loss=2.9556028842926025
I0418 17:56:51.772296 140527920604928 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.24230295419692993, loss=2.9666476249694824
I0418 17:57:27.806178 140527912212224 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.17768913507461548, loss=3.0923171043395996
I0418 17:58:03.844499 140527920604928 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.14386020600795746, loss=2.9055185317993164
I0418 17:58:39.939303 140527912212224 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.12843169271945953, loss=2.9624648094177246
I0418 17:59:15.990417 140527920604928 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.1337973028421402, loss=3.0160040855407715
I0418 17:59:52.013949 140527912212224 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.20423179864883423, loss=2.958517551422119
I0418 18:00:28.019338 140527920604928 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.13354390859603882, loss=3.0220675468444824
I0418 18:00:44.700548 140715416078144 spec.py:298] Evaluating on the training split.
I0418 18:00:47.676651 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 18:03:30.939112 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 18:03:33.584476 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 18:06:02.923331 140715416078144 spec.py:326] Evaluating on the test split.
I0418 18:06:05.609611 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 18:08:36.019334 140715416078144 submission_runner.py:406] Time since start: 11778.40s, 	Step: 18648, 	{'train/accuracy': 0.6420944333076477, 'train/loss': 1.846184492111206, 'train/bleu': 31.13386837116556, 'validation/accuracy': 0.6554909348487854, 'validation/loss': 1.7341680526733398, 'validation/bleu': 27.65914035887369, 'validation/num_examples': 3000, 'test/accuracy': 0.6643425822257996, 'test/loss': 1.6798421144485474, 'test/bleu': 27.0042380377819, 'test/num_examples': 3003, 'score': 6754.743549823761, 'total_duration': 11778.400636911392, 'accumulated_submission_time': 6754.743549823761, 'accumulated_eval_time': 4985.342017412186, 'accumulated_logging_time': 38.105262756347656}
I0418 18:08:36.028232 140527912212224 logging_writer.py:48] [18648] accumulated_eval_time=4985.342017, accumulated_logging_time=38.105263, accumulated_submission_time=6754.743550, global_step=18648, preemption_count=0, score=6754.743550, test/accuracy=0.664343, test/bleu=27.004238, test/loss=1.679842, test/num_examples=3003, total_duration=11778.400637, train/accuracy=0.642094, train/bleu=31.133868, train/loss=1.846184, validation/accuracy=0.655491, validation/bleu=27.659140, validation/loss=1.734168, validation/num_examples=3000
I0418 18:08:37.071632 140715416078144 checkpoints.py:356] Saving checkpoint at step: 18648
I0418 18:08:40.766316 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_18648
I0418 18:08:40.770756 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_18648.
I0418 18:08:59.897483 140527920604928 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.12868943810462952, loss=2.9661319255828857
I0418 18:09:35.910555 140527903819520 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.27830594778060913, loss=3.0159778594970703
I0418 18:10:11.970049 140527920604928 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.16436909139156342, loss=2.9455642700195312
I0418 18:10:48.028457 140527903819520 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.12849679589271545, loss=2.9434120655059814
I0418 18:11:24.074028 140527920604928 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.21927239000797272, loss=2.9923720359802246
I0418 18:12:00.099472 140527903819520 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.12851914763450623, loss=2.9988579750061035
I0418 18:12:36.135090 140527920604928 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.10872732102870941, loss=2.932163953781128
I0418 18:13:12.191753 140527903819520 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.2004385143518448, loss=2.9440064430236816
I0418 18:13:48.262486 140527920604928 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.11639800667762756, loss=2.945017099380493
I0418 18:14:24.315004 140527903819520 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1256791204214096, loss=2.963953733444214
I0418 18:15:00.405384 140527920604928 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.16105248034000397, loss=2.9359970092773438
I0418 18:15:36.477446 140527903819520 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.12392093241214752, loss=2.9789481163024902
I0418 18:16:12.555186 140527920604928 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.12603920698165894, loss=2.969716787338257
I0418 18:16:47.969359 140715416078144 spec.py:298] Evaluating on the training split.
I0418 18:16:50.953057 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 18:19:24.610208 140715416078144 spec.py:310] Evaluating on the validation split.
I0418 18:19:27.257074 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 18:21:53.781218 140715416078144 spec.py:326] Evaluating on the test split.
I0418 18:21:56.479475 140715416078144 workload.py:179] Translating evaluation dataset.
I0418 18:24:24.303762 140715416078144 submission_runner.py:406] Time since start: 12726.69s, 	Step: 20000, 	{'train/accuracy': 0.6517546772956848, 'train/loss': 1.7585418224334717, 'train/bleu': 32.057597390380444, 'validation/accuracy': 0.6593222618103027, 'validation/loss': 1.698831558227539, 'validation/bleu': 28.307336588401117, 'validation/num_examples': 3000, 'test/accuracy': 0.6688745617866516, 'test/loss': 1.6379631757736206, 'test/bleu': 27.666061055348294, 'test/num_examples': 3003, 'score': 7241.922610998154, 'total_duration': 12726.68507552147, 'accumulated_submission_time': 7241.922610998154, 'accumulated_eval_time': 5441.676376819611, 'accumulated_logging_time': 42.86041855812073}
I0418 18:24:24.312781 140527903819520 logging_writer.py:48] [20000] accumulated_eval_time=5441.676377, accumulated_logging_time=42.860419, accumulated_submission_time=7241.922611, global_step=20000, preemption_count=0, score=7241.922611, test/accuracy=0.668875, test/bleu=27.666061, test/loss=1.637963, test/num_examples=3003, total_duration=12726.685076, train/accuracy=0.651755, train/bleu=32.057597, train/loss=1.758542, validation/accuracy=0.659322, validation/bleu=28.307337, validation/loss=1.698832, validation/num_examples=3000
I0418 18:24:25.359116 140715416078144 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:24:29.069844 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_20000
I0418 18:24:29.074143 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_20000.
I0418 18:24:29.085321 140527920604928 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7241.922611
I0418 18:24:29.649438 140715416078144 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:24:35.133435 140715416078144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_20000
I0418 18:24:35.137779 140715416078144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/wmt_jax/trial_1/checkpoint_20000.
I0418 18:24:35.214822 140715416078144 submission_runner.py:567] Tuning trial 1/1
I0418 18:24:35.215012 140715416078144 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 18:24:35.216650 140715416078144 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005528552574105561, 'train/loss': 10.994253158569336, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.98425006866455, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.002135276794434, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 33.12275242805481, 'total_duration': 950.3621320724487, 'accumulated_submission_time': 33.12275242805481, 'accumulated_eval_time': 917.2392385005951, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2330, {'train/accuracy': 0.45732381939888, 'train/loss': 3.49275803565979, 'train/bleu': 19.191653415648354, 'validation/accuracy': 0.44964104890823364, 'validation/loss': 3.5597143173217773, 'validation/bleu': 14.477704757034, 'validation/num_examples': 3000, 'test/accuracy': 0.44463425874710083, 'test/loss': 3.672243595123291, 'test/bleu': 13.248477625266506, 'test/num_examples': 3003, 'score': 873.3912582397461, 'total_duration': 2543.3907449245453, 'accumulated_submission_time': 873.3912582397461, 'accumulated_eval_time': 1665.1610267162323, 'accumulated_logging_time': 4.810873985290527, 'global_step': 2330, 'preemption_count': 0}), (4661, {'train/accuracy': 0.5308015942573547, 'train/loss': 2.7565908432006836, 'train/bleu': 23.485023413829285, 'validation/accuracy': 0.53364497423172, 'validation/loss': 2.7434918880462646, 'validation/bleu': 19.346343337672156, 'validation/num_examples': 3000, 'test/accuracy': 0.5307071208953857, 'test/loss': 2.7937731742858887, 'test/bleu': 18.05757484371864, 'test/num_examples': 3003, 'score': 1713.6809618473053, 'total_duration': 3868.224392414093, 'accumulated_submission_time': 1713.6809618473053, 'accumulated_eval_time': 2144.895729780197, 'accumulated_logging_time': 9.593433380126953, 'global_step': 4661, 'preemption_count': 0}), (6992, {'train/accuracy': 0.5851579904556274, 'train/loss': 2.2720530033111572, 'train/bleu': 27.3559208993471, 'validation/accuracy': 0.5922431349754333, 'validation/loss': 2.236572265625, 'validation/bleu': 23.567265847033987, 'validation/num_examples': 3000, 'test/accuracy': 0.5949799418449402, 'test/loss': 2.2268874645233154, 'test/bleu': 22.070120127554063, 'test/num_examples': 3003, 'score': 2553.866166830063, 'total_duration': 5178.133287191391, 'accumulated_submission_time': 2553.866166830063, 'accumulated_eval_time': 2609.775131702423, 'accumulated_logging_time': 14.411986351013184, 'global_step': 6992, 'preemption_count': 0}), (9324, {'train/accuracy': 0.6039654016494751, 'train/loss': 2.12834095954895, 'train/bleu': 28.855309321299373, 'validation/accuracy': 0.6176984906196594, 'validation/loss': 2.0252463817596436, 'validation/bleu': 25.236275868449926, 'validation/num_examples': 3000, 'test/accuracy': 0.6244843602180481, 'test/loss': 1.986832618713379, 'test/bleu': 24.15057416841689, 'test/num_examples': 3003, 'score': 3394.1434614658356, 'total_duration': 6507.176578044891, 'accumulated_submission_time': 3394.1434614658356, 'accumulated_eval_time': 3093.730932712555, 'accumulated_logging_time': 19.19673180580139, 'global_step': 9324, 'preemption_count': 0}), (11655, {'train/accuracy': 0.6159423589706421, 'train/loss': 2.0386085510253906, 'train/bleu': 29.63912301335959, 'validation/accuracy': 0.6319326162338257, 'validation/loss': 1.9100861549377441, 'validation/bleu': 26.23921655083185, 'validation/num_examples': 3000, 'test/accuracy': 0.6404392719268799, 'test/loss': 1.8646249771118164, 'test/bleu': 25.407876432259638, 'test/num_examples': 3003, 'score': 4234.346390962601, 'total_duration': 7836.812881946564, 'accumulated_submission_time': 4234.346390962601, 'accumulated_eval_time': 3578.4349608421326, 'accumulated_logging_time': 23.900001525878906, 'global_step': 11655, 'preemption_count': 0}), (13986, {'train/accuracy': 0.6313157081604004, 'train/loss': 1.9062542915344238, 'train/bleu': 30.773204511994493, 'validation/accuracy': 0.64402174949646, 'validation/loss': 1.8263567686080933, 'validation/bleu': 27.291204643785424, 'validation/num_examples': 3000, 'test/accuracy': 0.6524548530578613, 'test/loss': 1.7722917795181274, 'test/bleu': 26.302156885980477, 'test/num_examples': 3003, 'score': 5074.393873214722, 'total_duration': 9160.03282570839, 'accumulated_submission_time': 5074.393873214722, 'accumulated_eval_time': 4056.84317779541, 'accumulated_logging_time': 28.637954711914062, 'global_step': 13986, 'preemption_count': 0}), (16317, {'train/accuracy': 0.6324387788772583, 'train/loss': 1.8962022066116333, 'train/bleu': 30.7472465730952, 'validation/accuracy': 0.651523232460022, 'validation/loss': 1.773362636566162, 'validation/bleu': 27.530528872839465, 'validation/num_examples': 3000, 'test/accuracy': 0.6603102684020996, 'test/loss': 1.7082411050796509, 'test/bleu': 26.832006786460305, 'test/num_examples': 3003, 'score': 5914.717568635941, 'total_duration': 10462.335293769836, 'accumulated_submission_time': 5914.717568635941, 'accumulated_eval_time': 4514.023285150528, 'accumulated_logging_time': 33.41043043136597, 'global_step': 16317, 'preemption_count': 0}), (18648, {'train/accuracy': 0.6420944333076477, 'train/loss': 1.846184492111206, 'train/bleu': 31.13386837116556, 'validation/accuracy': 0.6554909348487854, 'validation/loss': 1.7341680526733398, 'validation/bleu': 27.65914035887369, 'validation/num_examples': 3000, 'test/accuracy': 0.6643425822257996, 'test/loss': 1.6798421144485474, 'test/bleu': 27.0042380377819, 'test/num_examples': 3003, 'score': 6754.743549823761, 'total_duration': 11778.400636911392, 'accumulated_submission_time': 6754.743549823761, 'accumulated_eval_time': 4985.342017412186, 'accumulated_logging_time': 38.105262756347656, 'global_step': 18648, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6517546772956848, 'train/loss': 1.7585418224334717, 'train/bleu': 32.057597390380444, 'validation/accuracy': 0.6593222618103027, 'validation/loss': 1.698831558227539, 'validation/bleu': 28.307336588401117, 'validation/num_examples': 3000, 'test/accuracy': 0.6688745617866516, 'test/loss': 1.6379631757736206, 'test/bleu': 27.666061055348294, 'test/num_examples': 3003, 'score': 7241.922610998154, 'total_duration': 12726.68507552147, 'accumulated_submission_time': 7241.922610998154, 'accumulated_eval_time': 5441.676376819611, 'accumulated_logging_time': 42.86041855812073, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0418 18:24:35.216796 140715416078144 submission_runner.py:570] Timing: 7241.922610998154
I0418 18:24:35.216884 140715416078144 submission_runner.py:571] ====================
I0418 18:24:35.217049 140715416078144 submission_runner.py:631] Final wmt score: 7241.922610998154
