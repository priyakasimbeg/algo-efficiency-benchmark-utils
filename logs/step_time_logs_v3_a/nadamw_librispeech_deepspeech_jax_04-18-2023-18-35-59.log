I0418 18:36:20.811506 139924329011008 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax.
I0418 18:36:20.878896 139924329011008 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 18:36:21.654000 139924329011008 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0418 18:36:21.654601 139924329011008 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 18:36:21.658470 139924329011008 submission_runner.py:528] Using RNG seed 3067997960
I0418 18:36:24.767082 139924329011008 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 18:36:24.767319 139924329011008 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1.
I0418 18:36:24.767559 139924329011008 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0418 18:36:24.902868 139924329011008 submission_runner.py:232] Initializing dataset.
I0418 18:36:24.903130 139924329011008 submission_runner.py:239] Initializing model.
I0418 18:36:43.269066 139924329011008 submission_runner.py:249] Initializing optimizer.
I0418 18:36:43.943485 139924329011008 submission_runner.py:256] Initializing metrics bundle.
I0418 18:36:43.943759 139924329011008 submission_runner.py:273] Initializing checkpoint and logger.
I0418 18:36:43.944831 139924329011008 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0418 18:36:43.945155 139924329011008 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 18:36:43.945225 139924329011008 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 18:36:44.894257 139924329011008 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0418 18:36:44.895339 139924329011008 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0418 18:36:44.902280 139924329011008 submission_runner.py:309] Starting training loop.
I0418 18:36:45.096113 139924329011008 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:36:45.131323 139924329011008 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:36:45.522860 139924329011008 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0418 18:37:44.214359 139857716307712 logging_writer.py:48] [0] global_step=0, grad_norm=36.30210876464844, loss=33.459835052490234
I0418 18:37:44.240086 139924329011008 spec.py:298] Evaluating on the training split.
I0418 18:37:44.375222 139924329011008 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:37:44.405343 139924329011008 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:37:44.771494 139924329011008 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0418 18:39:13.112415 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 18:39:13.207523 139924329011008 input_pipeline.py:20] Loading split = dev-clean
I0418 18:39:13.212133 139924329011008 input_pipeline.py:20] Loading split = dev-other
I0418 18:40:09.030554 139924329011008 spec.py:326] Evaluating on the test split.
I0418 18:40:09.127228 139924329011008 input_pipeline.py:20] Loading split = test-clean
I0418 18:40:44.113611 139924329011008 submission_runner.py:406] Time since start: 239.21s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.194836, dtype=float32), 'train/wer': 3.9521166940840846, 'validation/ctc_loss': DeviceArray(31.155926, dtype=float32), 'validation/wer': 3.5124410269274184, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.306555, dtype=float32), 'test/wer': 3.795462393110312, 'test/num_examples': 2472, 'score': 59.33759689331055, 'total_duration': 239.20953631401062, 'accumulated_submission_time': 59.33759689331055, 'accumulated_eval_time': 179.87175464630127, 'accumulated_logging_time': 0}
I0418 18:40:44.138643 139743757063936 logging_writer.py:48] [1] accumulated_eval_time=179.871755, accumulated_logging_time=0, accumulated_submission_time=59.337597, global_step=1, preemption_count=0, score=59.337597, test/ctc_loss=31.306554794311523, test/num_examples=2472, test/wer=3.795462, total_duration=239.209536, train/ctc_loss=32.1948356628418, train/wer=3.952117, validation/ctc_loss=31.155925750732422, validation/num_examples=5348, validation/wer=3.512441
I0418 18:40:44.288108 139924329011008 checkpoints.py:356] Saving checkpoint at step: 1
I0418 18:40:44.802561 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0418 18:40:44.803560 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0418 18:43:01.355257 139747472262912 logging_writer.py:48] [100] global_step=100, grad_norm=6.180538654327393, loss=9.397784233093262
I0418 18:44:58.365095 139747480655616 logging_writer.py:48] [200] global_step=200, grad_norm=2.1697192192077637, loss=6.671131134033203
I0418 18:46:52.258171 139747472262912 logging_writer.py:48] [300] global_step=300, grad_norm=0.9453680515289307, loss=5.890831470489502
I0418 18:48:47.283019 139747480655616 logging_writer.py:48] [400] global_step=400, grad_norm=0.7517170310020447, loss=5.860264778137207
I0418 18:50:43.796103 139747472262912 logging_writer.py:48] [500] global_step=500, grad_norm=0.4152078926563263, loss=5.807398796081543
I0418 18:52:40.135773 139747480655616 logging_writer.py:48] [600] global_step=600, grad_norm=0.6293975710868835, loss=5.780287265777588
I0418 18:54:33.577235 139747472262912 logging_writer.py:48] [700] global_step=700, grad_norm=0.5823041200637817, loss=5.7188944816589355
I0418 18:56:28.589299 139747480655616 logging_writer.py:48] [800] global_step=800, grad_norm=0.5935745239257812, loss=5.603245258331299
I0418 18:58:23.922592 139747472262912 logging_writer.py:48] [900] global_step=900, grad_norm=0.6372302174568176, loss=5.536031723022461
I0418 19:00:20.366827 139747480655616 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.914778470993042, loss=5.419442653656006
I0418 19:02:17.346583 139747614938880 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2864301204681396, loss=5.202159881591797
I0418 19:04:14.938733 139747606546176 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7673156261444092, loss=4.890572547912598
I0418 19:06:11.936323 139747614938880 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.9455695152282715, loss=4.480541229248047
I0418 19:08:08.952995 139747606546176 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.529176950454712, loss=4.113020420074463
I0418 19:10:03.424033 139747614938880 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.9376962184906006, loss=3.786485433578491
I0418 19:11:56.718847 139747606546176 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.925290584564209, loss=3.686398983001709
I0418 19:13:51.181138 139747614938880 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.2216989994049072, loss=3.544063091278076
I0418 19:15:43.919022 139747606546176 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.4877521991729736, loss=3.3899688720703125
I0418 19:17:36.648018 139747614938880 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.7440052032470703, loss=3.2472169399261475
I0418 19:19:29.220532 139747606546176 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.2619376182556152, loss=3.1275794506073
I0418 19:20:45.696688 139924329011008 spec.py:298] Evaluating on the training split.
I0418 19:21:15.449060 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 19:21:51.183600 139924329011008 spec.py:326] Evaluating on the test split.
I0418 19:22:08.535090 139924329011008 submission_runner.py:406] Time since start: 2723.63s, 	Step: 2065, 	{'train/ctc_loss': DeviceArray(5.841448, dtype=float32), 'train/wer': 0.9228749150377953, 'validation/ctc_loss': DeviceArray(5.808645, dtype=float32), 'validation/wer': 0.883529990641492, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.7093706, dtype=float32), 'test/wer': 0.8849349013872809, 'test/num_examples': 2472, 'score': 2460.20360660553, 'total_duration': 2723.629309654236, 'accumulated_submission_time': 2460.20360660553, 'accumulated_eval_time': 262.706857919693, 'accumulated_logging_time': 0.6953322887420654}
I0418 19:22:08.553918 139747184858880 logging_writer.py:48] [2065] accumulated_eval_time=262.706858, accumulated_logging_time=0.695332, accumulated_submission_time=2460.203607, global_step=2065, preemption_count=0, score=2460.203607, test/ctc_loss=5.7093706130981445, test/num_examples=2472, test/wer=0.884935, total_duration=2723.629310, train/ctc_loss=5.841447830200195, train/wer=0.922875, validation/ctc_loss=5.808644771575928, validation/num_examples=5348, validation/wer=0.883530
I0418 19:22:08.708178 139924329011008 checkpoints.py:356] Saving checkpoint at step: 2065
I0418 19:22:09.322866 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_2065
I0418 19:22:09.335895 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_2065.
I0418 19:22:49.872804 139747176466176 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.2763583660125732, loss=3.075965404510498
I0418 19:24:43.226993 139747134502656 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.183382272720337, loss=2.9691710472106934
I0418 19:26:39.956240 139747176466176 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.3414885997772217, loss=2.955803632736206
I0418 19:28:34.547541 139747134502656 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.9616055488586426, loss=2.8780786991119385
I0418 19:30:28.045648 139747176466176 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.2581787109375, loss=2.758820056915283
I0418 19:32:24.482203 139747134502656 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.5551881790161133, loss=2.7004106044769287
I0418 19:34:20.827885 139747176466176 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.1848561763763428, loss=2.779998540878296
I0418 19:36:13.885396 139747134502656 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.237974166870117, loss=2.673130512237549
I0418 19:38:06.131108 139747176466176 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.6158952713012695, loss=2.6154894828796387
I0418 19:39:59.156988 139747134502656 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.193162441253662, loss=2.523768186569214
I0418 19:41:59.330770 139747184858880 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.015404462814331, loss=2.51189923286438
I0418 19:43:56.715736 139747176466176 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.869236946105957, loss=2.4519731998443604
I0418 19:45:52.195008 139747184858880 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2731595039367676, loss=2.429241418838501
I0418 19:47:46.051311 139747176466176 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.042564630508423, loss=2.418262004852295
I0418 19:49:38.201755 139747184858880 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.02193021774292, loss=2.3713107109069824
I0418 19:51:35.713870 139747176466176 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.2159619331359863, loss=2.382561206817627
I0418 19:53:27.798972 139747184858880 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.585805654525757, loss=2.3602442741394043
I0418 19:55:19.969682 139747176466176 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.9969472885131836, loss=2.2867908477783203
I0418 19:57:12.158649 139747184858880 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.7710635662078857, loss=2.247333288192749
I0418 19:59:04.506844 139747176466176 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.2066473960876465, loss=2.264016628265381
I0418 20:00:56.701217 139747184858880 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.4055423736572266, loss=2.232710361480713
I0418 20:02:10.096025 139924329011008 spec.py:298] Evaluating on the training split.
I0418 20:02:47.737425 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 20:03:24.916871 139924329011008 spec.py:326] Evaluating on the test split.
I0418 20:03:44.290971 139924329011008 submission_runner.py:406] Time since start: 5219.39s, 	Step: 4163, 	{'train/ctc_loss': DeviceArray(0.96720725, dtype=float32), 'train/wer': 0.3010308179953653, 'validation/ctc_loss': DeviceArray(1.3528419, dtype=float32), 'validation/wer': 0.36282067361962006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.99833095, dtype=float32), 'test/wer': 0.3015863343692239, 'test/num_examples': 2472, 'score': 4860.936844587326, 'total_duration': 5219.385549783707, 'accumulated_submission_time': 4860.936844587326, 'accumulated_eval_time': 356.8988709449768, 'accumulated_logging_time': 1.5015225410461426}
I0418 20:03:44.311200 139748341978880 logging_writer.py:48] [4163] accumulated_eval_time=356.898871, accumulated_logging_time=1.501523, accumulated_submission_time=4860.936845, global_step=4163, preemption_count=0, score=4860.936845, test/ctc_loss=0.9983309507369995, test/num_examples=2472, test/wer=0.301586, total_duration=5219.385550, train/ctc_loss=0.9672072529792786, train/wer=0.301031, validation/ctc_loss=1.352841854095459, validation/num_examples=5348, validation/wer=0.362821
I0418 20:03:44.462184 139924329011008 checkpoints.py:356] Saving checkpoint at step: 4163
I0418 20:03:45.068707 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_4163
I0418 20:03:45.081914 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_4163.
I0418 20:04:29.621736 139748333586176 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.062662363052368, loss=2.128445625305176
I0418 20:06:27.120054 139748283229952 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.795039415359497, loss=2.1606366634368896
I0418 20:08:23.061127 139748333586176 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.694199323654175, loss=2.094313621520996
I0418 20:10:15.443052 139748283229952 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.7270567417144775, loss=2.437018871307373
I0418 20:12:08.907758 139748333586176 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.83168625831604, loss=2.1884281635284424
I0418 20:14:03.464584 139748283229952 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.156733274459839, loss=2.139927387237549
I0418 20:15:59.414995 139748333586176 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.590974807739258, loss=2.1525352001190186
I0418 20:17:51.418019 139748283229952 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.318720817565918, loss=2.0975067615509033
I0418 20:19:46.095700 139748333586176 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.775227427482605, loss=2.0562286376953125
I0418 20:21:39.177197 139748283229952 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.112342357635498, loss=2.1063544750213623
I0418 20:23:35.843566 139748341978880 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.906800389289856, loss=2.0749361515045166
I0418 20:25:27.820581 139748333586176 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.251025438308716, loss=2.0721468925476074
I0418 20:27:21.192362 139748341978880 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.8263556957244873, loss=2.0684127807617188
I0418 20:29:17.945536 139748333586176 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.6178510189056396, loss=1.9867428541183472
I0418 20:31:16.631739 139748341978880 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.0192227363586426, loss=2.093118190765381
I0418 20:33:14.158449 139748333586176 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9407211542129517, loss=1.9789977073669434
I0418 20:35:09.757130 139748341978880 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.4264602661132812, loss=1.9550880193710327
I0418 20:37:04.665386 139748333586176 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.047715663909912, loss=1.9655002355575562
I0418 20:38:59.240721 139748341978880 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.7971291542053223, loss=1.936385154724121
I0418 20:40:54.884553 139748333586176 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.5737950801849365, loss=1.9282071590423584
I0418 20:42:51.097602 139748341978880 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.6620984077453613, loss=1.9575248956680298
I0418 20:43:45.833633 139924329011008 spec.py:298] Evaluating on the training split.
I0418 20:44:24.130583 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 20:45:02.635250 139924329011008 spec.py:326] Evaluating on the test split.
I0418 20:45:21.655666 139924329011008 submission_runner.py:406] Time since start: 7716.75s, 	Step: 6248, 	{'train/ctc_loss': DeviceArray(0.6769674, dtype=float32), 'train/wer': 0.2335447074761084, 'validation/ctc_loss': DeviceArray(1.1094986, dtype=float32), 'validation/wer': 0.30856062287142183, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.75664675, dtype=float32), 'test/wer': 0.23866106067068835, 'test/num_examples': 2472, 'score': 7261.663007974625, 'total_duration': 7716.749802112579, 'accumulated_submission_time': 7261.663007974625, 'accumulated_eval_time': 452.7173788547516, 'accumulated_logging_time': 2.2972617149353027}
I0418 20:45:21.675595 139748925658880 logging_writer.py:48] [6248] accumulated_eval_time=452.717379, accumulated_logging_time=2.297262, accumulated_submission_time=7261.663008, global_step=6248, preemption_count=0, score=7261.663008, test/ctc_loss=0.7566467523574829, test/num_examples=2472, test/wer=0.238661, total_duration=7716.749802, train/ctc_loss=0.6769673824310303, train/wer=0.233545, validation/ctc_loss=1.1094986200332642, validation/num_examples=5348, validation/wer=0.308561
I0418 20:45:21.826540 139924329011008 checkpoints.py:356] Saving checkpoint at step: 6248
I0418 20:45:22.440972 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_6248
I0418 20:45:22.453903 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_6248.
I0418 20:46:24.382439 139748917266176 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.6806576251983643, loss=1.9166221618652344
I0418 20:48:19.925257 139748858517248 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.1697323322296143, loss=1.92794668674469
I0418 20:50:13.715519 139748917266176 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.141817808151245, loss=1.9066684246063232
I0418 20:52:06.475560 139748858517248 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.094425916671753, loss=1.926690697669983
I0418 20:54:01.901181 139748917266176 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.7279443740844727, loss=1.9284852743148804
I0418 20:55:53.855911 139748858517248 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.5092086791992188, loss=1.8968329429626465
I0418 20:57:49.045032 139748917266176 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.9311366081237793, loss=1.9358394145965576
I0418 20:59:43.563461 139748858517248 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.6804044246673584, loss=1.851676106452942
I0418 21:01:35.829257 139748917266176 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.6506879329681396, loss=1.915502905845642
I0418 21:03:32.086824 139748858517248 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.6914875507354736, loss=1.8805692195892334
I0418 21:05:29.757926 139748925658880 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.0424444675445557, loss=1.8481338024139404
I0418 21:07:22.254428 139748917266176 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.2904818058013916, loss=1.8837589025497437
I0418 21:09:17.500914 139748925658880 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2840280532836914, loss=1.8232035636901855
I0418 21:11:09.731678 139748917266176 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.4988760948181152, loss=1.8761560916900635
I0418 21:13:02.843720 139748925658880 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.457338333129883, loss=1.8081557750701904
I0418 21:14:54.778557 139748917266176 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.408022165298462, loss=1.8153053522109985
I0418 21:16:50.773630 139748925658880 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.6321861743927, loss=1.8256009817123413
I0418 21:18:47.923069 139748917266176 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.5296828746795654, loss=1.7854626178741455
I0418 21:20:43.757865 139748925658880 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.8708994388580322, loss=1.875091314315796
I0418 21:22:40.780432 139748917266176 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.515352487564087, loss=1.7709221839904785
I0418 21:24:39.429213 139748925658880 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.4362740516662598, loss=1.8253986835479736
I0418 21:25:22.909727 139924329011008 spec.py:298] Evaluating on the training split.
I0418 21:26:01.259629 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 21:26:38.833063 139924329011008 spec.py:326] Evaluating on the test split.
I0418 21:26:59.228524 139924329011008 submission_runner.py:406] Time since start: 10214.32s, 	Step: 8340, 	{'train/ctc_loss': DeviceArray(0.5578871, dtype=float32), 'train/wer': 0.18940401512216068, 'validation/ctc_loss': DeviceArray(0.93425065, dtype=float32), 'validation/wer': 0.26321527462879524, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6081645, dtype=float32), 'test/wer': 0.1971238803241728, 'test/num_examples': 2472, 'score': 9662.092720746994, 'total_duration': 10214.323073625565, 'accumulated_submission_time': 9662.092720746994, 'accumulated_eval_time': 549.0330476760864, 'accumulated_logging_time': 3.1008143424987793}
I0418 21:26:59.247588 139748925658880 logging_writer.py:48] [8340] accumulated_eval_time=549.033048, accumulated_logging_time=3.100814, accumulated_submission_time=9662.092721, global_step=8340, preemption_count=0, score=9662.092721, test/ctc_loss=0.6081644892692566, test/num_examples=2472, test/wer=0.197124, total_duration=10214.323074, train/ctc_loss=0.557887077331543, train/wer=0.189404, validation/ctc_loss=0.9342506527900696, validation/num_examples=5348, validation/wer=0.263215
I0418 21:26:59.401489 139924329011008 checkpoints.py:356] Saving checkpoint at step: 8340
I0418 21:26:59.950255 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8340
I0418 21:26:59.963525 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8340.
I0418 21:28:08.331925 139748917266176 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.166372060775757, loss=1.8200846910476685
I0418 21:30:05.641041 139748092364544 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.336379289627075, loss=1.7692980766296387
I0418 21:32:03.427175 139748917266176 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.649078607559204, loss=1.7912362813949585
I0418 21:34:01.289871 139748092364544 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.7039742469787598, loss=1.7894855737686157
I0418 21:35:56.787036 139748917266176 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.342419147491455, loss=1.7472339868545532
I0418 21:37:51.174409 139748092364544 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.4991226196289062, loss=1.79583740234375
I0418 21:39:43.320940 139748917266176 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.8168904781341553, loss=1.7976311445236206
I0418 21:41:37.164466 139748092364544 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.0873591899871826, loss=1.746195912361145
I0418 21:43:34.581961 139748917266176 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.857369899749756, loss=1.7584511041641235
I0418 21:45:30.684727 139748597978880 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.769639730453491, loss=1.6627449989318848
I0418 21:47:24.135859 139748589586176 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.003413200378418, loss=1.7950282096862793
I0418 21:49:22.020467 139748597978880 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.303182601928711, loss=1.7044532299041748
I0418 21:51:18.746973 139748589586176 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.32313871383667, loss=1.706074833869934
I0418 21:53:12.369835 139748597978880 logging_writer.py:48] [9700] global_step=9700, grad_norm=4.106650352478027, loss=1.7348301410675049
I0418 21:55:07.741195 139748589586176 logging_writer.py:48] [9800] global_step=9800, grad_norm=14.70309066772461, loss=1.7613747119903564
I0418 21:57:03.866432 139748597978880 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.394639015197754, loss=1.7599318027496338
I0418 21:58:56.618230 139748589586176 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.773082971572876, loss=1.7614538669586182
I0418 22:00:48.840713 139748597978880 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.669116735458374, loss=1.6856417655944824
I0418 22:02:43.897310 139748589586176 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.5961568355560303, loss=1.6915305852890015
I0418 22:04:43.768361 139748597978880 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.0516481399536133, loss=1.744528889656067
I0418 22:06:37.080733 139748589586176 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.68159556388855, loss=1.6662386655807495
I0418 22:07:00.401409 139924329011008 spec.py:298] Evaluating on the training split.
I0418 22:07:38.476160 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 22:08:16.766995 139924329011008 spec.py:326] Evaluating on the test split.
I0418 22:08:35.762203 139924329011008 submission_runner.py:406] Time since start: 12710.86s, 	Step: 10422, 	{'train/ctc_loss': DeviceArray(0.4439354, dtype=float32), 'train/wer': 0.15718432668681642, 'validation/ctc_loss': DeviceArray(0.79936516, dtype=float32), 'validation/wer': 0.23123233219809164, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4972256, dtype=float32), 'test/wer': 0.16586435927122054, 'test/num_examples': 2472, 'score': 12062.505290746689, 'total_duration': 12710.856485128403, 'accumulated_submission_time': 12062.505290746689, 'accumulated_eval_time': 644.3904392719269, 'accumulated_logging_time': 3.8404881954193115}
I0418 22:08:35.781739 139748997338880 logging_writer.py:48] [10422] accumulated_eval_time=644.390439, accumulated_logging_time=3.840488, accumulated_submission_time=12062.505291, global_step=10422, preemption_count=0, score=12062.505291, test/ctc_loss=0.4972256124019623, test/num_examples=2472, test/wer=0.165864, total_duration=12710.856485, train/ctc_loss=0.4439353942871094, train/wer=0.157184, validation/ctc_loss=0.7993651628494263, validation/num_examples=5348, validation/wer=0.231232
I0418 22:08:35.928871 139924329011008 checkpoints.py:356] Saving checkpoint at step: 10422
I0418 22:08:36.514179 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_10422
I0418 22:08:36.527195 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_10422.
I0418 22:10:09.482749 139748988946176 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.2338294982910156, loss=1.6323745250701904
I0418 22:12:07.227088 139748581193472 logging_writer.py:48] [10600] global_step=10600, grad_norm=4.194153785705566, loss=1.737971544265747
I0418 22:14:02.196231 139748988946176 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.196898937225342, loss=1.7091407775878906
I0418 22:15:54.129552 139748581193472 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.1549911499023438, loss=1.7734827995300293
I0418 22:17:47.455599 139748988946176 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.213647842407227, loss=1.6846576929092407
I0418 22:19:40.504910 139748581193472 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.4040071964263916, loss=1.7331253290176392
I0418 22:21:33.597272 139748988946176 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.368210792541504, loss=1.6685758829116821
I0418 22:23:29.400361 139748581193472 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.620038986206055, loss=1.70314621925354
I0418 22:25:23.693200 139748988946176 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.699064254760742, loss=1.7706308364868164
I0418 22:27:18.953925 139748997338880 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.7439157962799072, loss=1.6514722108840942
I0418 22:29:10.981675 139748988946176 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.0867815017700195, loss=1.6288888454437256
I0418 22:31:03.546437 139748997338880 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.79449725151062, loss=1.6683794260025024
I0418 22:32:58.920182 139748988946176 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.386924982070923, loss=1.6242942810058594
I0418 22:34:55.900494 139748997338880 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.984076976776123, loss=1.6770894527435303
I0418 22:36:52.362247 139748988946176 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.879666328430176, loss=1.6536701917648315
I0418 22:38:48.622445 139748997338880 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.3414828777313232, loss=1.6376410722732544
I0418 22:40:43.199090 139748988946176 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.196375846862793, loss=1.6252968311309814
I0418 22:42:39.406891 139748997338880 logging_writer.py:48] [12200] global_step=12200, grad_norm=5.567744731903076, loss=1.7126232385635376
I0418 22:44:35.092107 139748988946176 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.2054810523986816, loss=1.6304320096969604
I0418 22:46:33.083258 139748997338880 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.312344551086426, loss=1.6785399913787842
I0418 22:48:30.768268 139748988946176 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.7591423988342285, loss=1.6260181665420532
I0418 22:48:37.646153 139924329011008 spec.py:298] Evaluating on the training split.
I0418 22:49:16.118959 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 22:49:54.123723 139924329011008 spec.py:326] Evaluating on the test split.
I0418 22:50:13.455069 139924329011008 submission_runner.py:406] Time since start: 15208.55s, 	Step: 12507, 	{'train/ctc_loss': DeviceArray(0.4390433, dtype=float32), 'train/wer': 0.1483160932723752, 'validation/ctc_loss': DeviceArray(0.7635379, dtype=float32), 'validation/wer': 0.21932676629779352, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4566116, dtype=float32), 'test/wer': 0.15067129770682267, 'test/num_examples': 2472, 'score': 14463.598352432251, 'total_duration': 15208.54952454567, 'accumulated_submission_time': 14463.598352432251, 'accumulated_eval_time': 740.1961116790771, 'accumulated_logging_time': 4.610456943511963}
I0418 22:50:13.475073 139748669654784 logging_writer.py:48] [12507] accumulated_eval_time=740.196112, accumulated_logging_time=4.610457, accumulated_submission_time=14463.598352, global_step=12507, preemption_count=0, score=14463.598352, test/ctc_loss=0.45661160349845886, test/num_examples=2472, test/wer=0.150671, total_duration=15208.549525, train/ctc_loss=0.4390433132648468, train/wer=0.148316, validation/ctc_loss=0.7635378837585449, validation/num_examples=5348, validation/wer=0.219327
I0418 22:50:13.626792 139924329011008 checkpoints.py:356] Saving checkpoint at step: 12507
I0418 22:50:14.183010 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_12507
I0418 22:50:14.196100 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_12507.
I0418 22:52:03.488802 139748661262080 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.965428352355957, loss=1.6258985996246338
I0418 22:53:59.484235 139747917436672 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.379971981048584, loss=1.5877436399459839
I0418 22:55:55.304748 139748661262080 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.8233413696289062, loss=1.6095502376556396
I0418 22:57:52.318058 139747917436672 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.700899839401245, loss=1.6128852367401123
I0418 22:59:48.829309 139748661262080 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.3089799880981445, loss=1.6374486684799194
I0418 23:01:43.447503 139747917436672 logging_writer.py:48] [13100] global_step=13100, grad_norm=5.139903545379639, loss=1.6759986877441406
I0418 23:03:40.174371 139748661262080 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.694491863250732, loss=1.6840949058532715
I0418 23:05:36.244897 139747917436672 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.650078773498535, loss=1.6812635660171509
I0418 23:07:32.931314 139748669654784 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.771723747253418, loss=1.6907016038894653
I0418 23:09:29.278093 139748661262080 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.099700450897217, loss=1.672000765800476
I0418 23:11:25.539017 139748669654784 logging_writer.py:48] [13600] global_step=13600, grad_norm=6.705267429351807, loss=1.632849097251892
I0418 23:13:19.464253 139748661262080 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.191227912902832, loss=1.5655245780944824
I0418 23:15:11.695575 139748669654784 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.8753790855407715, loss=1.6097652912139893
I0418 23:17:03.989833 139748661262080 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.023845195770264, loss=1.5589134693145752
I0418 23:18:56.612111 139748669654784 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.055481433868408, loss=1.5783846378326416
I0418 23:20:53.163862 139748661262080 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.693530559539795, loss=1.5844651460647583
I0418 23:22:50.471910 139748669654784 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.712162971496582, loss=1.6032155752182007
I0418 23:24:45.955024 139748661262080 logging_writer.py:48] [14300] global_step=14300, grad_norm=4.982414245605469, loss=1.618841528892517
I0418 23:26:37.910708 139748669654784 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.037032127380371, loss=1.611441969871521
I0418 23:28:33.179329 139748669654784 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.703575849533081, loss=1.570410132408142
I0418 23:30:14.929029 139924329011008 spec.py:298] Evaluating on the training split.
I0418 23:30:53.370351 139924329011008 spec.py:310] Evaluating on the validation split.
I0418 23:31:32.747206 139924329011008 spec.py:326] Evaluating on the test split.
I0418 23:31:52.095543 139924329011008 submission_runner.py:406] Time since start: 17707.19s, 	Step: 14592, 	{'train/ctc_loss': DeviceArray(0.37595496, dtype=float32), 'train/wer': 0.1303309722842618, 'validation/ctc_loss': DeviceArray(0.71450657, dtype=float32), 'validation/wer': 0.2061766153074318, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41957152, dtype=float32), 'test/wer': 0.1383624804501046, 'test/num_examples': 2472, 'score': 16864.30488371849, 'total_duration': 17707.19000530243, 'accumulated_submission_time': 16864.30488371849, 'accumulated_eval_time': 837.3594124317169, 'accumulated_logging_time': 5.356308460235596}
I0418 23:31:52.115675 139749150938880 logging_writer.py:48] [14592] accumulated_eval_time=837.359412, accumulated_logging_time=5.356308, accumulated_submission_time=16864.304884, global_step=14592, preemption_count=0, score=16864.304884, test/ctc_loss=0.41957151889801025, test/num_examples=2472, test/wer=0.138362, total_duration=17707.190005, train/ctc_loss=0.3759549558162689, train/wer=0.130331, validation/ctc_loss=0.7145065665245056, validation/num_examples=5348, validation/wer=0.206177
I0418 23:31:52.263047 139924329011008 checkpoints.py:356] Saving checkpoint at step: 14592
I0418 23:31:52.849386 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_14592
I0418 23:31:52.862229 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_14592.
I0418 23:32:03.374139 139749142546176 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.005033254623413, loss=1.5426334142684937
I0418 23:33:55.920981 139749050226432 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.8568942546844482, loss=1.591858148574829
I0418 23:35:48.658297 139749142546176 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.6073970794677734, loss=1.6441333293914795
I0418 23:37:44.684853 139749050226432 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.2126052379608154, loss=1.6398049592971802
I0418 23:39:37.431386 139749142546176 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.4328670501708984, loss=1.5824538469314575
I0418 23:41:31.651363 139749050226432 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.912020206451416, loss=1.5770277976989746
I0418 23:43:29.221305 139749142546176 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.8071506023406982, loss=1.5945178270339966
I0418 23:45:21.701954 139749050226432 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.054441213607788, loss=1.6308631896972656
I0418 23:47:14.079542 139749142546176 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.030969619750977, loss=1.6253727674484253
I0418 23:49:13.319674 139749150938880 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.81068229675293, loss=1.60883629322052
I0418 23:51:09.493119 139749142546176 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.5519094467163086, loss=1.5848312377929688
I0418 23:53:07.898216 139749150938880 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.456644058227539, loss=1.553421974182129
I0418 23:55:05.158504 139749142546176 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.655605792999268, loss=1.6076765060424805
I0418 23:56:59.032829 139749150938880 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.7704739570617676, loss=1.5802305936813354
I0418 23:58:49.623919 139924329011008 spec.py:298] Evaluating on the training split.
I0418 23:59:28.353693 139924329011008 spec.py:310] Evaluating on the validation split.
I0419 00:00:06.158226 139924329011008 spec.py:326] Evaluating on the test split.
I0419 00:00:25.096496 139924329011008 submission_runner.py:406] Time since start: 19420.19s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.33503035, dtype=float32), 'train/wer': 0.1193322759302114, 'validation/ctc_loss': DeviceArray(0.69772094, dtype=float32), 'validation/wer': 0.20014664878580593, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41033486, dtype=float32), 'test/wer': 0.1344220340015843, 'test/num_examples': 2472, 'score': 18481.047250270844, 'total_duration': 19420.19229531288, 'accumulated_submission_time': 18481.047250270844, 'accumulated_eval_time': 932.8301181793213, 'accumulated_logging_time': 6.12778115272522}
I0419 00:00:25.116957 139748567258880 logging_writer.py:48] [16000] accumulated_eval_time=932.830118, accumulated_logging_time=6.127781, accumulated_submission_time=18481.047250, global_step=16000, preemption_count=0, score=18481.047250, test/ctc_loss=0.41033485531806946, test/num_examples=2472, test/wer=0.134422, total_duration=19420.192295, train/ctc_loss=0.33503034710884094, train/wer=0.119332, validation/ctc_loss=0.6977209448814392, validation/num_examples=5348, validation/wer=0.200147
I0419 00:00:25.266412 139924329011008 checkpoints.py:356] Saving checkpoint at step: 16000
I0419 00:00:25.830963 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0419 00:00:25.843805 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0419 00:00:25.857460 139748558866176 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18481.047250
I0419 00:00:25.966031 139924329011008 checkpoints.py:356] Saving checkpoint at step: 16000
I0419 00:00:26.757068 139924329011008 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0419 00:00:26.769937 139924329011008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0419 00:00:27.946464 139924329011008 submission_runner.py:567] Tuning trial 1/1
I0419 00:00:27.946675 139924329011008 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0419 00:00:27.952026 139924329011008 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.194836, dtype=float32), 'train/wer': 3.9521166940840846, 'validation/ctc_loss': DeviceArray(31.155926, dtype=float32), 'validation/wer': 3.5124410269274184, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.306555, dtype=float32), 'test/wer': 3.795462393110312, 'test/num_examples': 2472, 'score': 59.33759689331055, 'total_duration': 239.20953631401062, 'accumulated_submission_time': 59.33759689331055, 'accumulated_eval_time': 179.87175464630127, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2065, {'train/ctc_loss': DeviceArray(5.841448, dtype=float32), 'train/wer': 0.9228749150377953, 'validation/ctc_loss': DeviceArray(5.808645, dtype=float32), 'validation/wer': 0.883529990641492, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.7093706, dtype=float32), 'test/wer': 0.8849349013872809, 'test/num_examples': 2472, 'score': 2460.20360660553, 'total_duration': 2723.629309654236, 'accumulated_submission_time': 2460.20360660553, 'accumulated_eval_time': 262.706857919693, 'accumulated_logging_time': 0.6953322887420654, 'global_step': 2065, 'preemption_count': 0}), (4163, {'train/ctc_loss': DeviceArray(0.96720725, dtype=float32), 'train/wer': 0.3010308179953653, 'validation/ctc_loss': DeviceArray(1.3528419, dtype=float32), 'validation/wer': 0.36282067361962006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.99833095, dtype=float32), 'test/wer': 0.3015863343692239, 'test/num_examples': 2472, 'score': 4860.936844587326, 'total_duration': 5219.385549783707, 'accumulated_submission_time': 4860.936844587326, 'accumulated_eval_time': 356.8988709449768, 'accumulated_logging_time': 1.5015225410461426, 'global_step': 4163, 'preemption_count': 0}), (6248, {'train/ctc_loss': DeviceArray(0.6769674, dtype=float32), 'train/wer': 0.2335447074761084, 'validation/ctc_loss': DeviceArray(1.1094986, dtype=float32), 'validation/wer': 0.30856062287142183, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.75664675, dtype=float32), 'test/wer': 0.23866106067068835, 'test/num_examples': 2472, 'score': 7261.663007974625, 'total_duration': 7716.749802112579, 'accumulated_submission_time': 7261.663007974625, 'accumulated_eval_time': 452.7173788547516, 'accumulated_logging_time': 2.2972617149353027, 'global_step': 6248, 'preemption_count': 0}), (8340, {'train/ctc_loss': DeviceArray(0.5578871, dtype=float32), 'train/wer': 0.18940401512216068, 'validation/ctc_loss': DeviceArray(0.93425065, dtype=float32), 'validation/wer': 0.26321527462879524, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6081645, dtype=float32), 'test/wer': 0.1971238803241728, 'test/num_examples': 2472, 'score': 9662.092720746994, 'total_duration': 10214.323073625565, 'accumulated_submission_time': 9662.092720746994, 'accumulated_eval_time': 549.0330476760864, 'accumulated_logging_time': 3.1008143424987793, 'global_step': 8340, 'preemption_count': 0}), (10422, {'train/ctc_loss': DeviceArray(0.4439354, dtype=float32), 'train/wer': 0.15718432668681642, 'validation/ctc_loss': DeviceArray(0.79936516, dtype=float32), 'validation/wer': 0.23123233219809164, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4972256, dtype=float32), 'test/wer': 0.16586435927122054, 'test/num_examples': 2472, 'score': 12062.505290746689, 'total_duration': 12710.856485128403, 'accumulated_submission_time': 12062.505290746689, 'accumulated_eval_time': 644.3904392719269, 'accumulated_logging_time': 3.8404881954193115, 'global_step': 10422, 'preemption_count': 0}), (12507, {'train/ctc_loss': DeviceArray(0.4390433, dtype=float32), 'train/wer': 0.1483160932723752, 'validation/ctc_loss': DeviceArray(0.7635379, dtype=float32), 'validation/wer': 0.21932676629779352, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4566116, dtype=float32), 'test/wer': 0.15067129770682267, 'test/num_examples': 2472, 'score': 14463.598352432251, 'total_duration': 15208.54952454567, 'accumulated_submission_time': 14463.598352432251, 'accumulated_eval_time': 740.1961116790771, 'accumulated_logging_time': 4.610456943511963, 'global_step': 12507, 'preemption_count': 0}), (14592, {'train/ctc_loss': DeviceArray(0.37595496, dtype=float32), 'train/wer': 0.1303309722842618, 'validation/ctc_loss': DeviceArray(0.71450657, dtype=float32), 'validation/wer': 0.2061766153074318, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41957152, dtype=float32), 'test/wer': 0.1383624804501046, 'test/num_examples': 2472, 'score': 16864.30488371849, 'total_duration': 17707.19000530243, 'accumulated_submission_time': 16864.30488371849, 'accumulated_eval_time': 837.3594124317169, 'accumulated_logging_time': 5.356308460235596, 'global_step': 14592, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.33503035, dtype=float32), 'train/wer': 0.1193322759302114, 'validation/ctc_loss': DeviceArray(0.69772094, dtype=float32), 'validation/wer': 0.20014664878580593, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41033486, dtype=float32), 'test/wer': 0.1344220340015843, 'test/num_examples': 2472, 'score': 18481.047250270844, 'total_duration': 19420.19229531288, 'accumulated_submission_time': 18481.047250270844, 'accumulated_eval_time': 932.8301181793213, 'accumulated_logging_time': 6.12778115272522, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0419 00:00:27.952204 139924329011008 submission_runner.py:570] Timing: 18481.047250270844
I0419 00:00:27.952256 139924329011008 submission_runner.py:571] ====================
I0419 00:00:27.952802 139924329011008 submission_runner.py:631] Final librispeech_deepspeech score: 18481.047250270844
