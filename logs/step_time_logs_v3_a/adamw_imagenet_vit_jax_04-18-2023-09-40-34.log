I0418 09:40:56.286487 140594386749248 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax.
I0418 09:40:56.358853 140594386749248 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 09:40:57.240718 140594386749248 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0418 09:40:57.242212 140594386749248 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 09:40:57.246299 140594386749248 submission_runner.py:528] Using RNG seed 4264660896
I0418 09:40:59.964951 140594386749248 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 09:40:59.965193 140594386749248 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1.
I0418 09:40:59.967181 140594386749248 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/hparams.json.
I0418 09:41:00.089331 140594386749248 submission_runner.py:232] Initializing dataset.
I0418 09:41:00.102251 140594386749248 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:41:00.110744 140594386749248 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 09:41:00.110879 140594386749248 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 09:41:00.372101 140594386749248 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:41:07.123735 140594386749248 submission_runner.py:239] Initializing model.
I0418 09:41:18.129634 140594386749248 submission_runner.py:249] Initializing optimizer.
I0418 09:41:18.764878 140594386749248 submission_runner.py:256] Initializing metrics bundle.
I0418 09:41:18.765057 140594386749248 submission_runner.py:273] Initializing checkpoint and logger.
I0418 09:41:18.766068 140594386749248 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0418 09:41:19.619806 140594386749248 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0418 09:41:19.620813 140594386749248 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/flags_0.json.
I0418 09:41:19.626997 140594386749248 submission_runner.py:309] Starting training loop.
I0418 09:42:10.868719 140417798493952 logging_writer.py:48] [0] global_step=0, grad_norm=0.31896522641181946, loss=6.907756805419922
I0418 09:42:10.886117 140594386749248 spec.py:298] Evaluating on the training split.
I0418 09:42:10.892357 140594386749248 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:42:10.899513 140594386749248 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 09:42:10.899627 140594386749248 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 09:42:10.960143 140594386749248 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:42:29.966744 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 09:42:29.975239 140594386749248 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:42:29.990782 140594386749248 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 09:42:29.991042 140594386749248 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 09:42:30.044115 140594386749248 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:42:48.931255 140594386749248 spec.py:326] Evaluating on the test split.
I0418 09:42:48.937638 140594386749248 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 09:42:48.942161 140594386749248 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0418 09:42:48.972883 140594386749248 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 09:42:59.418769 140594386749248 submission_runner.py:406] Time since start: 99.79s, 	Step: 1, 	{'train/accuracy': 0.0008789062267169356, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 51.25894021987915, 'total_duration': 99.7916989326477, 'accumulated_submission_time': 51.25894021987915, 'accumulated_eval_time': 48.53258752822876, 'accumulated_logging_time': 0}
I0418 09:42:59.437515 140356628764416 logging_writer.py:48] [1] accumulated_eval_time=48.532588, accumulated_logging_time=0, accumulated_submission_time=51.258940, global_step=1, preemption_count=0, score=51.258940, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=99.791699, train/accuracy=0.000879, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0418 09:42:59.646206 140594386749248 checkpoints.py:356] Saving checkpoint at step: 1
I0418 09:43:00.215372 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1
I0418 09:43:00.216417 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1.
I0418 09:43:56.207936 140413193090816 logging_writer.py:48] [100] global_step=100, grad_norm=0.45940232276916504, loss=6.896728992462158
I0418 09:44:35.519440 140413201483520 logging_writer.py:48] [200] global_step=200, grad_norm=0.5117663741111755, loss=6.836793422698975
I0418 09:45:15.168883 140413193090816 logging_writer.py:48] [300] global_step=300, grad_norm=0.4893476366996765, loss=6.805127143859863
I0418 09:45:56.075056 140413201483520 logging_writer.py:48] [400] global_step=400, grad_norm=0.5717605948448181, loss=6.710897922515869
I0418 09:46:36.809251 140413193090816 logging_writer.py:48] [500] global_step=500, grad_norm=0.5795246362686157, loss=6.790187835693359
I0418 09:47:17.768273 140413201483520 logging_writer.py:48] [600] global_step=600, grad_norm=0.6656085848808289, loss=6.600324630737305
I0418 09:47:58.313052 140413193090816 logging_writer.py:48] [700] global_step=700, grad_norm=0.8067451119422913, loss=6.586702823638916
I0418 09:48:39.337881 140413201483520 logging_writer.py:48] [800] global_step=800, grad_norm=0.7428120374679565, loss=6.514688968658447
I0418 09:49:20.198591 140413193090816 logging_writer.py:48] [900] global_step=900, grad_norm=0.6565683484077454, loss=6.530139446258545
I0418 09:50:00.878740 140413201483520 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6723088622093201, loss=6.604422092437744
I0418 09:50:00.895191 140594386749248 spec.py:298] Evaluating on the training split.
I0418 09:50:11.664803 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 09:50:18.109375 140594386749248 spec.py:326] Evaluating on the test split.
I0418 09:50:20.138794 140594386749248 submission_runner.py:406] Time since start: 540.51s, 	Step: 1001, 	{'train/accuracy': 0.025644531473517418, 'train/loss': 6.0797014236450195, 'validation/accuracy': 0.023520000278949738, 'validation/loss': 6.1089091300964355, 'validation/num_examples': 50000, 'test/accuracy': 0.01860000006854534, 'test/loss': 6.193573951721191, 'test/num_examples': 10000, 'score': 471.9129114151001, 'total_duration': 540.5116999149323, 'accumulated_submission_time': 471.9129114151001, 'accumulated_eval_time': 67.77616310119629, 'accumulated_logging_time': 0.7992143630981445}
I0418 09:50:20.153385 140358105159424 logging_writer.py:48] [1001] accumulated_eval_time=67.776163, accumulated_logging_time=0.799214, accumulated_submission_time=471.912911, global_step=1001, preemption_count=0, score=471.912911, test/accuracy=0.018600, test/loss=6.193574, test/num_examples=10000, total_duration=540.511700, train/accuracy=0.025645, train/loss=6.079701, validation/accuracy=0.023520, validation/loss=6.108909, validation/num_examples=50000
I0418 09:50:21.868088 140594386749248 checkpoints.py:356] Saving checkpoint at step: 1001
I0418 09:50:23.185320 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1001
I0418 09:50:23.186339 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1001.
I0418 09:51:02.578185 140358113552128 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8901934027671814, loss=6.351008892059326
I0418 09:51:42.061123 140417504880384 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1108559370040894, loss=6.405468940734863
I0418 09:52:22.822086 140358113552128 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9372630715370178, loss=6.232530117034912
I0418 09:53:03.522073 140417504880384 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0468419790267944, loss=6.738705635070801
I0418 09:53:44.133420 140358113552128 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0834871530532837, loss=6.238401412963867
I0418 09:54:24.912015 140417504880384 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0118908882141113, loss=6.133125305175781
I0418 09:55:05.354802 140358113552128 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8032374382019043, loss=6.145610809326172
I0418 09:55:46.142470 140417504880384 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0345830917358398, loss=6.069850921630859
I0418 09:56:26.691808 140358113552128 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9742763638496399, loss=6.093089580535889
I0418 09:57:07.523248 140417504880384 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8091135025024414, loss=6.480836391448975
I0418 09:57:23.476483 140594386749248 spec.py:298] Evaluating on the training split.
I0418 09:57:35.042777 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 09:57:41.547881 140594386749248 spec.py:326] Evaluating on the test split.
I0418 09:57:43.233910 140594386749248 submission_runner.py:406] Time since start: 983.61s, 	Step: 2041, 	{'train/accuracy': 0.061601560562849045, 'train/loss': 5.483028888702393, 'validation/accuracy': 0.05838000029325485, 'validation/loss': 5.506394863128662, 'validation/num_examples': 50000, 'test/accuracy': 0.0471000038087368, 'test/loss': 5.674988746643066, 'test/num_examples': 10000, 'score': 892.1779072284698, 'total_duration': 983.6068339347839, 'accumulated_submission_time': 892.1779072284698, 'accumulated_eval_time': 87.53355884552002, 'accumulated_logging_time': 3.8484058380126953}
I0418 09:57:43.246059 140358113552128 logging_writer.py:48] [2041] accumulated_eval_time=87.533559, accumulated_logging_time=3.848406, accumulated_submission_time=892.177907, global_step=2041, preemption_count=0, score=892.177907, test/accuracy=0.047100, test/loss=5.674989, test/num_examples=10000, total_duration=983.606834, train/accuracy=0.061602, train/loss=5.483029, validation/accuracy=0.058380, validation/loss=5.506395, validation/num_examples=50000
I0418 09:57:43.413273 140594386749248 checkpoints.py:356] Saving checkpoint at step: 2041
I0418 09:57:47.045379 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_2041
I0418 09:57:47.061211 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_2041.
I0418 09:58:10.685541 140417504880384 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6952579021453857, loss=6.1366705894470215
I0418 09:58:50.151174 140417471309568 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8333428502082825, loss=6.729050636291504
I0418 09:59:30.364241 140417504880384 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.909914493560791, loss=5.896649360656738
I0418 10:00:11.063976 140417471309568 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9628135561943054, loss=5.854226589202881
I0418 10:00:51.929052 140417504880384 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9653551578521729, loss=6.321137428283691
I0418 10:01:32.749781 140417471309568 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8548441529273987, loss=5.847036361694336
I0418 10:02:13.437009 140417504880384 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9414037466049194, loss=5.81909704208374
I0418 10:02:53.977008 140417471309568 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9178197383880615, loss=5.83183479309082
I0418 10:03:34.923723 140417504880384 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8843780159950256, loss=5.728785514831543
I0418 10:04:15.875785 140417471309568 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9267135262489319, loss=5.763836860656738
I0418 10:04:47.332275 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:04:59.097513 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:05:05.850513 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:05:07.521157 140594386749248 submission_runner.py:406] Time since start: 1427.89s, 	Step: 3079, 	{'train/accuracy': 0.11640624701976776, 'train/loss': 4.889256000518799, 'validation/accuracy': 0.10723999887704849, 'validation/loss': 4.95236873626709, 'validation/num_examples': 50000, 'test/accuracy': 0.0852000042796135, 'test/loss': 5.202913761138916, 'test/num_examples': 10000, 'score': 1312.4231100082397, 'total_duration': 1427.8940751552582, 'accumulated_submission_time': 1312.4231100082397, 'accumulated_eval_time': 107.72242283821106, 'accumulated_logging_time': 7.677483797073364}
I0418 10:05:07.534550 140417504880384 logging_writer.py:48] [3079] accumulated_eval_time=107.722423, accumulated_logging_time=7.677484, accumulated_submission_time=1312.423110, global_step=3079, preemption_count=0, score=1312.423110, test/accuracy=0.085200, test/loss=5.202914, test/num_examples=10000, total_duration=1427.894075, train/accuracy=0.116406, train/loss=4.889256, validation/accuracy=0.107240, validation/loss=4.952369, validation/num_examples=50000
I0418 10:05:07.817070 140594386749248 checkpoints.py:356] Saving checkpoint at step: 3079
I0418 10:05:11.287244 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_3079
I0418 10:05:11.301636 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_3079.
I0418 10:05:19.970795 140417471309568 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9229761362075806, loss=5.840202331542969
I0418 10:05:59.519221 140417186121472 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6709504723548889, loss=6.568915843963623
I0418 10:06:38.917394 140417471309568 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8632488250732422, loss=5.682302474975586
I0418 10:07:19.664584 140417186121472 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.4464269876480103, loss=5.762965679168701
I0418 10:08:00.601535 140417471309568 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9350591897964478, loss=5.604158401489258
I0418 10:08:41.620049 140417186121472 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8962015509605408, loss=5.574679851531982
I0418 10:09:22.373101 140417471309568 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6730594038963318, loss=6.49502420425415
I0418 10:10:03.164849 140417186121472 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8194902539253235, loss=5.500467300415039
I0418 10:10:43.748955 140417471309568 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8939370512962341, loss=5.576570987701416
I0418 10:11:24.412637 140417186121472 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7851846814155579, loss=5.726649284362793
I0418 10:12:04.982771 140417471309568 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9512329697608948, loss=5.391525745391846
I0418 10:12:11.637810 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:12:23.391294 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:12:30.968154 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:12:32.613889 140594386749248 submission_runner.py:406] Time since start: 1872.99s, 	Step: 4118, 	{'train/accuracy': 0.16347655653953552, 'train/loss': 4.485172748565674, 'validation/accuracy': 0.14761999249458313, 'validation/loss': 4.594143867492676, 'validation/num_examples': 50000, 'test/accuracy': 0.11150000244379044, 'test/loss': 4.9295172691345215, 'test/num_examples': 10000, 'score': 1732.7340202331543, 'total_duration': 1872.9867980480194, 'accumulated_submission_time': 1732.7340202331543, 'accumulated_eval_time': 128.69847440719604, 'accumulated_logging_time': 11.459540605545044}
I0418 10:12:32.626413 140417186121472 logging_writer.py:48] [4118] accumulated_eval_time=128.698474, accumulated_logging_time=11.459541, accumulated_submission_time=1732.734020, global_step=4118, preemption_count=0, score=1732.734020, test/accuracy=0.111500, test/loss=4.929517, test/num_examples=10000, total_duration=1872.986798, train/accuracy=0.163477, train/loss=4.485173, validation/accuracy=0.147620, validation/loss=4.594144, validation/num_examples=50000
I0418 10:12:32.769285 140594386749248 checkpoints.py:356] Saving checkpoint at step: 4118
I0418 10:12:35.369985 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_4118
I0418 10:12:35.390693 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_4118.
I0418 10:13:08.345307 140417471309568 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8019422888755798, loss=6.554832935333252
I0418 10:13:47.931507 140417177728768 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8677143454551697, loss=5.4549641609191895
I0418 10:14:28.761065 140417471309568 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8018628358840942, loss=5.367767333984375
I0418 10:15:09.579692 140417177728768 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8225592374801636, loss=5.3728766441345215
I0418 10:15:50.424199 140417471309568 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7029150724411011, loss=5.237166404724121
I0418 10:16:31.138107 140417177728768 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6933444738388062, loss=6.269097328186035
I0418 10:17:12.068900 140417471309568 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8033777475357056, loss=6.168234825134277
I0418 10:17:52.999053 140417177728768 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8782148957252502, loss=5.698638439178467
I0418 10:18:33.910503 140417471309568 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8071298003196716, loss=5.4769721031188965
I0418 10:19:14.830712 140417177728768 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7704609036445618, loss=5.420008182525635
I0418 10:19:35.711043 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:19:47.773609 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:19:56.785289 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:19:58.439018 140594386749248 submission_runner.py:406] Time since start: 2318.81s, 	Step: 5153, 	{'train/accuracy': 0.18675780296325684, 'train/loss': 4.352721691131592, 'validation/accuracy': 0.17401999235153198, 'validation/loss': 4.4328813552856445, 'validation/num_examples': 50000, 'test/accuracy': 0.13650000095367432, 'test/loss': 4.757118225097656, 'test/num_examples': 10000, 'score': 2153.0280542373657, 'total_duration': 2318.8119418621063, 'accumulated_submission_time': 2153.0280542373657, 'accumulated_eval_time': 151.42643427848816, 'accumulated_logging_time': 14.23920464515686}
I0418 10:19:58.449367 140417471309568 logging_writer.py:48] [5153] accumulated_eval_time=151.426434, accumulated_logging_time=14.239205, accumulated_submission_time=2153.028054, global_step=5153, preemption_count=0, score=2153.028054, test/accuracy=0.136500, test/loss=4.757118, test/num_examples=10000, total_duration=2318.811942, train/accuracy=0.186758, train/loss=4.352722, validation/accuracy=0.174020, validation/loss=4.432881, validation/num_examples=50000
I0418 10:19:58.588105 140594386749248 checkpoints.py:356] Saving checkpoint at step: 5153
I0418 10:20:00.097844 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_5153
I0418 10:20:00.116597 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_5153.
I0418 10:20:19.120352 140417177728768 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6832810640335083, loss=6.020887851715088
I0418 10:20:58.539859 140417169336064 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7269068360328674, loss=5.219681739807129
I0418 10:21:38.551449 140417177728768 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6317825317382812, loss=6.192663192749023
I0418 10:22:19.544220 140417169336064 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7316210269927979, loss=5.156739711761475
I0418 10:23:00.296305 140417177728768 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7663491368293762, loss=5.196871757507324
I0418 10:23:40.868173 140417169336064 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7500892877578735, loss=5.369110107421875
I0418 10:24:22.021986 140417177728768 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6193343997001648, loss=6.077001571655273
I0418 10:25:02.793725 140417169336064 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7568958401679993, loss=5.162786960601807
I0418 10:25:43.638506 140417177728768 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8393937349319458, loss=5.008876800537109
I0418 10:26:24.419456 140417169336064 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8789135217666626, loss=5.176519393920898
I0418 10:27:00.266336 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:27:13.414015 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:27:23.366014 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:27:25.019120 140594386749248 submission_runner.py:406] Time since start: 2765.39s, 	Step: 6190, 	{'train/accuracy': 0.24308593571186066, 'train/loss': 3.8328914642333984, 'validation/accuracy': 0.2274399995803833, 'validation/loss': 3.937016725540161, 'validation/num_examples': 50000, 'test/accuracy': 0.16930000483989716, 'test/loss': 4.3815598487854, 'test/num_examples': 10000, 'score': 2573.151726961136, 'total_duration': 2765.3920097351074, 'accumulated_submission_time': 2573.151726961136, 'accumulated_eval_time': 176.17916798591614, 'accumulated_logging_time': 15.919315814971924}
I0418 10:27:25.032350 140417177728768 logging_writer.py:48] [6190] accumulated_eval_time=176.179168, accumulated_logging_time=15.919316, accumulated_submission_time=2573.151727, global_step=6190, preemption_count=0, score=2573.151727, test/accuracy=0.169300, test/loss=4.381560, test/num_examples=10000, total_duration=2765.392010, train/accuracy=0.243086, train/loss=3.832891, validation/accuracy=0.227440, validation/loss=3.937017, validation/num_examples=50000
I0418 10:27:25.170382 140594386749248 checkpoints.py:356] Saving checkpoint at step: 6190
I0418 10:27:26.312957 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_6190
I0418 10:27:26.327457 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_6190.
I0418 10:27:31.101316 140417169336064 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8687741756439209, loss=5.1762590408325195
I0418 10:28:10.741648 140417160943360 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.749587893486023, loss=6.3087263107299805
I0418 10:28:50.555382 140417169336064 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7329402565956116, loss=5.167359828948975
I0418 10:29:31.578005 140417160943360 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7272890210151672, loss=5.119746208190918
I0418 10:30:12.480322 140417169336064 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.757140576839447, loss=4.900157451629639
I0418 10:30:53.439594 140417160943360 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8397676348686218, loss=4.97113037109375
I0418 10:31:34.158004 140417169336064 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6788594126701355, loss=5.13746452331543
I0418 10:32:15.110175 140417160943360 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5274121761322021, loss=6.257410049438477
I0418 10:32:56.188953 140417169336064 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7827356457710266, loss=4.870342254638672
I0418 10:33:36.900240 140417160943360 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5599297881126404, loss=5.790589809417725
I0418 10:34:17.772860 140417169336064 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7244753241539001, loss=4.748142242431641
I0418 10:34:26.596952 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:34:41.087910 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:34:51.225562 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:34:52.874618 140594386749248 submission_runner.py:406] Time since start: 3213.25s, 	Step: 7223, 	{'train/accuracy': 0.29179686307907104, 'train/loss': 3.5589025020599365, 'validation/accuracy': 0.26725998520851135, 'validation/loss': 3.697038412094116, 'validation/num_examples': 50000, 'test/accuracy': 0.20120000839233398, 'test/loss': 4.182796478271484, 'test/num_examples': 10000, 'score': 2993.395399570465, 'total_duration': 3213.247522830963, 'accumulated_submission_time': 2993.395399570465, 'accumulated_eval_time': 202.45682644844055, 'accumulated_logging_time': 17.230114936828613}
I0418 10:34:52.888616 140417160943360 logging_writer.py:48] [7223] accumulated_eval_time=202.456826, accumulated_logging_time=17.230115, accumulated_submission_time=2993.395400, global_step=7223, preemption_count=0, score=2993.395400, test/accuracy=0.201200, test/loss=4.182796, test/num_examples=10000, total_duration=3213.247523, train/accuracy=0.291797, train/loss=3.558903, validation/accuracy=0.267260, validation/loss=3.697038, validation/num_examples=50000
I0418 10:34:53.116119 140594386749248 checkpoints.py:356] Saving checkpoint at step: 7223
I0418 10:34:54.406351 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_7223
I0418 10:34:54.421298 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_7223.
I0418 10:35:25.202622 140417169336064 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.0150376558303833, loss=4.867895126342773
I0418 10:36:05.457359 140417152550656 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6194504499435425, loss=4.7962493896484375
I0418 10:36:46.548970 140417169336064 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7984526753425598, loss=5.170312404632568
I0418 10:37:27.386764 140417152550656 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5687094330787659, loss=6.12684440612793
I0418 10:38:08.663225 140417169336064 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.586029589176178, loss=5.824521064758301
I0418 10:38:49.433104 140417152550656 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.794637143611908, loss=4.769301414489746
I0418 10:39:30.377582 140417169336064 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7202861905097961, loss=4.7929301261901855
I0418 10:40:11.270555 140417152550656 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6924481391906738, loss=4.766666412353516
I0418 10:40:52.011362 140417169336064 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7939202189445496, loss=4.56663179397583
I0418 10:41:32.970364 140417152550656 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6572244763374329, loss=4.7286376953125
I0418 10:41:54.425382 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:42:09.241206 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:42:19.290315 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:42:20.967834 140594386749248 submission_runner.py:406] Time since start: 3661.34s, 	Step: 8254, 	{'train/accuracy': 0.33542966842651367, 'train/loss': 3.2091894149780273, 'validation/accuracy': 0.30469998717308044, 'validation/loss': 3.369858980178833, 'validation/num_examples': 50000, 'test/accuracy': 0.22800001502037048, 'test/loss': 3.906121253967285, 'test/num_examples': 10000, 'score': 3413.3734250068665, 'total_duration': 3661.340737104416, 'accumulated_submission_time': 3413.3734250068665, 'accumulated_eval_time': 228.99924182891846, 'accumulated_logging_time': 18.77948832511902}
I0418 10:42:20.982749 140417169336064 logging_writer.py:48] [8254] accumulated_eval_time=228.999242, accumulated_logging_time=18.779488, accumulated_submission_time=3413.373425, global_step=8254, preemption_count=0, score=3413.373425, test/accuracy=0.228000, test/loss=3.906121, test/num_examples=10000, total_duration=3661.340737, train/accuracy=0.335430, train/loss=3.209189, validation/accuracy=0.304700, validation/loss=3.369859, validation/num_examples=50000
I0418 10:42:21.238674 140594386749248 checkpoints.py:356] Saving checkpoint at step: 8254
I0418 10:42:22.549917 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_8254
I0418 10:42:22.566442 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_8254.
I0418 10:42:41.109824 140417152550656 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7014958262443542, loss=4.509073734283447
I0418 10:43:20.650760 140417144157952 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7286776900291443, loss=4.5246124267578125
I0418 10:44:01.583656 140417152550656 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6057737469673157, loss=5.056282043457031
I0418 10:44:42.447762 140417144157952 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7878695726394653, loss=4.560858249664307
I0418 10:45:24.117771 140417152550656 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7438469529151917, loss=4.528848648071289
I0418 10:46:05.412891 140417144157952 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6969659328460693, loss=4.854323387145996
I0418 10:46:46.360907 140417152550656 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.882391095161438, loss=4.591514587402344
I0418 10:47:27.298689 140417144157952 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7091677784919739, loss=4.527159214019775
I0418 10:48:08.018722 140417152550656 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6193467974662781, loss=4.4473371505737305
I0418 10:48:48.781430 140417144157952 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.827400803565979, loss=4.705177307128906
I0418 10:49:22.747232 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:49:37.090719 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:49:47.918966 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:49:49.580758 140594386749248 submission_runner.py:406] Time since start: 4109.95s, 	Step: 9285, 	{'train/accuracy': 0.38496091961860657, 'train/loss': 2.920070171356201, 'validation/accuracy': 0.3336399793624878, 'validation/loss': 3.195288896560669, 'validation/num_examples': 50000, 'test/accuracy': 0.2597000002861023, 'test/loss': 3.758755922317505, 'test/num_examples': 10000, 'score': 3833.5198884010315, 'total_duration': 4109.95365524292, 'accumulated_submission_time': 3833.5198884010315, 'accumulated_eval_time': 255.8327248096466, 'accumulated_logging_time': 20.38954210281372}
I0418 10:49:49.595377 140417152550656 logging_writer.py:48] [9285] accumulated_eval_time=255.832725, accumulated_logging_time=20.389542, accumulated_submission_time=3833.519888, global_step=9285, preemption_count=0, score=3833.519888, test/accuracy=0.259700, test/loss=3.758756, test/num_examples=10000, total_duration=4109.953655, train/accuracy=0.384961, train/loss=2.920070, validation/accuracy=0.333640, validation/loss=3.195289, validation/num_examples=50000
I0418 10:49:49.805844 140594386749248 checkpoints.py:356] Saving checkpoint at step: 9285
I0418 10:49:50.917414 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_9285
I0418 10:49:50.932511 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_9285.
I0418 10:49:57.241914 140417144157952 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6703363060951233, loss=5.139385223388672
I0418 10:50:36.675219 140417135765248 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7146289348602295, loss=4.456268787384033
I0418 10:51:17.535708 140417144157952 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8798040747642517, loss=4.402740478515625
I0418 10:51:58.285890 140417135765248 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5082157850265503, loss=5.594795227050781
I0418 10:52:39.178536 140417135765248 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8009157180786133, loss=4.367194175720215
I0418 10:53:20.246322 140417144157952 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6386389136314392, loss=4.526745319366455
I0418 10:54:01.725747 140417135765248 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5338625907897949, loss=5.762764930725098
I0418 10:54:43.511089 140417144157952 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7518543004989624, loss=4.481768608093262
I0418 10:55:24.881952 140417135765248 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6466073989868164, loss=4.243359565734863
I0418 10:56:06.019649 140417144157952 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8422145843505859, loss=4.414887428283691
I0418 10:56:47.293511 140417135765248 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6984500885009766, loss=4.267468452453613
I0418 10:56:51.087034 140594386749248 spec.py:298] Evaluating on the training split.
I0418 10:57:06.173702 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 10:57:16.553442 140594386749248 spec.py:326] Evaluating on the test split.
I0418 10:57:18.207088 140594386749248 submission_runner.py:406] Time since start: 4558.58s, 	Step: 10311, 	{'train/accuracy': 0.39482420682907104, 'train/loss': 2.8886733055114746, 'validation/accuracy': 0.3696799874305725, 'validation/loss': 3.031848669052124, 'validation/num_examples': 50000, 'test/accuracy': 0.27740001678466797, 'test/loss': 3.6257095336914062, 'test/num_examples': 10000, 'score': 4253.649785995483, 'total_duration': 4558.57998585701, 'accumulated_submission_time': 4253.649785995483, 'accumulated_eval_time': 282.95273542404175, 'accumulated_logging_time': 21.743231773376465}
I0418 10:57:18.222866 140417144157952 logging_writer.py:48] [10311] accumulated_eval_time=282.952735, accumulated_logging_time=21.743232, accumulated_submission_time=4253.649786, global_step=10311, preemption_count=0, score=4253.649786, test/accuracy=0.277400, test/loss=3.625710, test/num_examples=10000, total_duration=4558.579986, train/accuracy=0.394824, train/loss=2.888673, validation/accuracy=0.369680, validation/loss=3.031849, validation/num_examples=50000
I0418 10:57:18.451759 140594386749248 checkpoints.py:356] Saving checkpoint at step: 10311
I0418 10:57:19.622313 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_10311
I0418 10:57:19.637651 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_10311.
I0418 10:57:55.163947 140417135765248 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7220168709754944, loss=4.334403038024902
I0418 10:58:36.180343 140417127372544 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6929288506507874, loss=4.466891765594482
I0418 10:59:17.029178 140417135765248 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7230287790298462, loss=6.1837849617004395
I0418 10:59:57.695520 140417127372544 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6920846104621887, loss=4.670846462249756
I0418 11:00:39.040384 140417135765248 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6368304491043091, loss=4.859008312225342
I0418 11:01:20.331001 140417127372544 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6436854004859924, loss=4.119275093078613
I0418 11:02:01.394659 140417135765248 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7205830812454224, loss=4.349943161010742
I0418 11:02:42.843432 140417127372544 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6470636129379272, loss=4.2961554527282715
I0418 11:03:24.104339 140417135765248 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5705732703208923, loss=5.4947075843811035
I0418 11:04:05.688186 140417127372544 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5584433078765869, loss=4.942842960357666
I0418 11:04:19.802345 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:04:34.721761 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:04:45.246561 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:04:46.916981 140594386749248 submission_runner.py:406] Time since start: 5007.29s, 	Step: 11336, 	{'train/accuracy': 0.439453125, 'train/loss': 2.6458230018615723, 'validation/accuracy': 0.4043999910354614, 'validation/loss': 2.824176549911499, 'validation/num_examples': 50000, 'test/accuracy': 0.30730000138282776, 'test/loss': 3.4183876514434814, 'test/num_examples': 10000, 'score': 4673.78355717659, 'total_duration': 5007.289900779724, 'accumulated_submission_time': 4673.78355717659, 'accumulated_eval_time': 310.0673408508301, 'accumulated_logging_time': 23.18234920501709}
I0418 11:04:46.927967 140417135765248 logging_writer.py:48] [11336] accumulated_eval_time=310.067341, accumulated_logging_time=23.182349, accumulated_submission_time=4673.783557, global_step=11336, preemption_count=0, score=4673.783557, test/accuracy=0.307300, test/loss=3.418388, test/num_examples=10000, total_duration=5007.289901, train/accuracy=0.439453, train/loss=2.645823, validation/accuracy=0.404400, validation/loss=2.824177, validation/num_examples=50000
I0418 11:04:47.091445 140594386749248 checkpoints.py:356] Saving checkpoint at step: 11336
I0418 11:04:48.167104 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_11336
I0418 11:04:48.186815 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_11336.
I0418 11:05:13.757586 140417127372544 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7139626741409302, loss=4.258740425109863
I0418 11:05:54.496771 140415701350144 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7243432998657227, loss=4.0979719161987305
I0418 11:06:35.809522 140417127372544 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6575100421905518, loss=4.148377418518066
I0418 11:07:17.226009 140415701350144 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5252373814582825, loss=5.102695941925049
I0418 11:07:58.847581 140417127372544 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.693931519985199, loss=4.145581245422363
I0418 11:08:39.964566 140415701350144 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6210206151008606, loss=6.0017194747924805
I0418 11:09:21.374666 140417127372544 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.645237386226654, loss=5.978996753692627
I0418 11:10:03.266840 140415701350144 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5605510473251343, loss=5.941394329071045
I0418 11:10:44.568168 140417127372544 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6612719893455505, loss=5.19644832611084
I0418 11:11:26.212237 140415701350144 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.8423546552658081, loss=4.146171569824219
I0418 11:11:48.603214 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:12:04.078089 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:12:14.926113 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:12:16.592676 140594386749248 submission_runner.py:406] Time since start: 5456.97s, 	Step: 12356, 	{'train/accuracy': 0.4659179449081421, 'train/loss': 2.4429397583007812, 'validation/accuracy': 0.4273200035095215, 'validation/loss': 2.6514198780059814, 'validation/num_examples': 50000, 'test/accuracy': 0.3297000229358673, 'test/loss': 3.2757463455200195, 'test/num_examples': 10000, 'score': 5094.169572353363, 'total_duration': 5456.965566635132, 'accumulated_submission_time': 5094.169572353363, 'accumulated_eval_time': 338.0567395687103, 'accumulated_logging_time': 24.45993971824646}
I0418 11:12:16.606575 140417127372544 logging_writer.py:48] [12356] accumulated_eval_time=338.056740, accumulated_logging_time=24.459940, accumulated_submission_time=5094.169572, global_step=12356, preemption_count=0, score=5094.169572, test/accuracy=0.329700, test/loss=3.275746, test/num_examples=10000, total_duration=5456.965567, train/accuracy=0.465918, train/loss=2.442940, validation/accuracy=0.427320, validation/loss=2.651420, validation/num_examples=50000
I0418 11:12:16.904206 140594386749248 checkpoints.py:356] Saving checkpoint at step: 12356
I0418 11:12:17.997378 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12356
I0418 11:12:18.016779 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12356.
I0418 11:12:35.735301 140415701350144 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7089191675186157, loss=4.88448429107666
I0418 11:13:17.064075 140415692957440 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.606220543384552, loss=4.524942874908447
I0418 11:13:58.611691 140415701350144 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7018565535545349, loss=3.967527389526367
I0418 11:14:40.134624 140415692957440 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.717033326625824, loss=3.9833312034606934
I0418 11:15:21.918911 140415701350144 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7409943342208862, loss=4.080528736114502
I0418 11:16:03.715635 140415692957440 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7512326240539551, loss=4.440145969390869
I0418 11:16:45.553768 140415701350144 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6473018527030945, loss=4.542328834533691
I0418 11:17:27.750759 140415692957440 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7409568428993225, loss=4.362127780914307
I0418 11:18:09.761863 140415701350144 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4876154363155365, loss=5.473581314086914
I0418 11:18:51.386801 140415692957440 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7477649450302124, loss=4.0460405349731445
I0418 11:19:18.255799 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:19:33.891656 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:19:44.607074 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:19:46.265129 140594386749248 submission_runner.py:406] Time since start: 5906.64s, 	Step: 13366, 	{'train/accuracy': 0.49916014075279236, 'train/loss': 2.3219447135925293, 'validation/accuracy': 0.4532800018787384, 'validation/loss': 2.5538370609283447, 'validation/num_examples': 50000, 'test/accuracy': 0.3508000075817108, 'test/loss': 3.1772141456604004, 'test/num_examples': 10000, 'score': 5514.375582695007, 'total_duration': 5906.638055324554, 'accumulated_submission_time': 5514.375582695007, 'accumulated_eval_time': 366.06607723236084, 'accumulated_logging_time': 25.895101308822632}
I0418 11:19:46.277190 140415701350144 logging_writer.py:48] [13366] accumulated_eval_time=366.066077, accumulated_logging_time=25.895101, accumulated_submission_time=5514.375583, global_step=13366, preemption_count=0, score=5514.375583, test/accuracy=0.350800, test/loss=3.177214, test/num_examples=10000, total_duration=5906.638055, train/accuracy=0.499160, train/loss=2.321945, validation/accuracy=0.453280, validation/loss=2.553837, validation/num_examples=50000
I0418 11:19:46.508887 140594386749248 checkpoints.py:356] Saving checkpoint at step: 13366
I0418 11:19:47.782354 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_13366
I0418 11:19:47.804208 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_13366.
I0418 11:20:01.610913 140415692957440 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6096916794776917, loss=4.104886054992676
I0418 11:20:42.037070 140415684564736 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6921805143356323, loss=3.879486560821533
I0418 11:21:23.322363 140415692957440 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6999033689498901, loss=3.8498146533966064
I0418 11:22:05.098993 140415684564736 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6295786499977112, loss=5.912707328796387
I0418 11:22:47.114530 140415692957440 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7490953803062439, loss=3.9380669593811035
I0418 11:23:28.825965 140415684564736 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7469982504844666, loss=3.8293495178222656
I0418 11:24:10.968293 140415692957440 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7417631149291992, loss=3.891420602798462
I0418 11:24:53.395130 140415684564736 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.645264208316803, loss=3.943246364593506
I0418 11:25:35.602005 140415692957440 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7099794745445251, loss=4.287804126739502
I0418 11:26:17.463777 140415684564736 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7141287326812744, loss=3.833466053009033
I0418 11:26:47.990797 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:27:02.699622 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:27:14.366193 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:27:16.008061 140594386749248 submission_runner.py:406] Time since start: 6356.38s, 	Step: 14376, 	{'train/accuracy': 0.49345701932907104, 'train/loss': 2.3743441104888916, 'validation/accuracy': 0.4593599736690521, 'validation/loss': 2.546607494354248, 'validation/num_examples': 50000, 'test/accuracy': 0.358100026845932, 'test/loss': 3.1483547687530518, 'test/num_examples': 10000, 'score': 5934.52911567688, 'total_duration': 6356.380882740021, 'accumulated_submission_time': 5934.52911567688, 'accumulated_eval_time': 394.0832166671753, 'accumulated_logging_time': 27.445635557174683}
I0418 11:27:16.020659 140415692957440 logging_writer.py:48] [14376] accumulated_eval_time=394.083217, accumulated_logging_time=27.445636, accumulated_submission_time=5934.529116, global_step=14376, preemption_count=0, score=5934.529116, test/accuracy=0.358100, test/loss=3.148355, test/num_examples=10000, total_duration=6356.380883, train/accuracy=0.493457, train/loss=2.374344, validation/accuracy=0.459360, validation/loss=2.546607, validation/num_examples=50000
I0418 11:27:16.248790 140594386749248 checkpoints.py:356] Saving checkpoint at step: 14376
I0418 11:27:17.611830 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14376
I0418 11:27:17.636983 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14376.
I0418 11:27:27.474815 140415684564736 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6799523830413818, loss=3.8205971717834473
I0418 11:28:08.220292 140415676172032 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7668868899345398, loss=3.6988632678985596
I0418 11:28:49.890446 140415684564736 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7420541048049927, loss=3.873133659362793
I0418 11:29:31.651347 140415676172032 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7988373637199402, loss=4.273769855499268
I0418 11:30:13.465053 140415684564736 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7249255180358887, loss=4.7149763107299805
I0418 11:30:55.495383 140415676172032 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.759160578250885, loss=3.7629261016845703
I0418 11:31:37.532723 140415684564736 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8808262944221497, loss=3.9336767196655273
I0418 11:32:19.063506 140415676172032 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7339217066764832, loss=5.642215728759766
I0418 11:33:00.838507 140415684564736 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5582462549209595, loss=5.578455448150635
I0418 11:33:42.913537 140415676172032 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6397860050201416, loss=5.438478946685791
I0418 11:34:17.953153 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:34:32.720193 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:34:44.315379 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:34:45.982841 140594386749248 submission_runner.py:406] Time since start: 6806.36s, 	Step: 15385, 	{'train/accuracy': 0.515332043170929, 'train/loss': 2.1830995082855225, 'validation/accuracy': 0.47689998149871826, 'validation/loss': 2.3813555240631104, 'validation/num_examples': 50000, 'test/accuracy': 0.3725000321865082, 'test/loss': 3.0237486362457275, 'test/num_examples': 10000, 'score': 6354.821929931641, 'total_duration': 6806.355770349503, 'accumulated_submission_time': 6354.821929931641, 'accumulated_eval_time': 422.1128902435303, 'accumulated_logging_time': 29.076247930526733}
I0418 11:34:45.996087 140415684564736 logging_writer.py:48] [15385] accumulated_eval_time=422.112890, accumulated_logging_time=29.076248, accumulated_submission_time=6354.821930, global_step=15385, preemption_count=0, score=6354.821930, test/accuracy=0.372500, test/loss=3.023749, test/num_examples=10000, total_duration=6806.355770, train/accuracy=0.515332, train/loss=2.183100, validation/accuracy=0.476900, validation/loss=2.381356, validation/num_examples=50000
I0418 11:34:46.225986 140594386749248 checkpoints.py:356] Saving checkpoint at step: 15385
I0418 11:34:47.581380 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_15385
I0418 11:34:47.604074 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_15385.
I0418 11:34:53.975381 140415676172032 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6895650029182434, loss=4.528477668762207
I0418 11:35:34.186939 140415667779328 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9160476922988892, loss=3.825139045715332
I0418 11:36:15.607367 140415676172032 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6878389120101929, loss=4.676087379455566
I0418 11:36:57.411103 140415667779328 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.7164241075515747, loss=3.752324104309082
I0418 11:37:39.283143 140415676172032 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7248451709747314, loss=4.148309707641602
I0418 11:38:21.184422 140415667779328 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7253370881080627, loss=3.76108980178833
I0418 11:39:03.529360 140415676172032 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7253170013427734, loss=3.698211193084717
I0418 11:39:45.653321 140415667779328 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8462052345275879, loss=3.8678219318389893
I0418 11:40:27.697424 140415676172032 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6838630437850952, loss=5.39792537689209
I0418 11:41:09.813630 140415667779328 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6641739010810852, loss=5.436915397644043
I0418 11:41:47.631341 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:42:02.273569 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:42:13.931608 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:42:15.593335 140594386749248 submission_runner.py:406] Time since start: 7255.97s, 	Step: 16392, 	{'train/accuracy': 0.5444530844688416, 'train/loss': 2.094566822052002, 'validation/accuracy': 0.5009799599647522, 'validation/loss': 2.3031516075134277, 'validation/num_examples': 50000, 'test/accuracy': 0.3856000304222107, 'test/loss': 2.942469596862793, 'test/num_examples': 10000, 'score': 6774.813853502274, 'total_duration': 7255.966229200363, 'accumulated_submission_time': 6774.813853502274, 'accumulated_eval_time': 450.0748815536499, 'accumulated_logging_time': 30.711421012878418}
I0418 11:42:15.611113 140415676172032 logging_writer.py:48] [16392] accumulated_eval_time=450.074882, accumulated_logging_time=30.711421, accumulated_submission_time=6774.813854, global_step=16392, preemption_count=0, score=6774.813854, test/accuracy=0.385600, test/loss=2.942470, test/num_examples=10000, total_duration=7255.966229, train/accuracy=0.544453, train/loss=2.094567, validation/accuracy=0.500980, validation/loss=2.303152, validation/num_examples=50000
I0418 11:42:15.861207 140594386749248 checkpoints.py:356] Saving checkpoint at step: 16392
I0418 11:42:17.247148 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_16392
I0418 11:42:17.273189 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_16392.
I0418 11:42:20.894979 140415667779328 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7462964057922363, loss=3.5971426963806152
I0418 11:43:01.356634 140415659386624 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6202243566513062, loss=3.829643964767456
I0418 11:43:42.508817 140415667779328 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6961023211479187, loss=3.70025372505188
I0418 11:44:24.854291 140415659386624 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7040964961051941, loss=4.73097562789917
I0418 11:45:07.115814 140415667779328 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6747434735298157, loss=3.6010255813598633
I0418 11:45:48.615007 140415659386624 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7613457441329956, loss=3.727996826171875
I0418 11:46:30.631681 140415667779328 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6454277634620667, loss=3.6233575344085693
I0418 11:47:12.616970 140415659386624 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7786459922790527, loss=3.6736178398132324
I0418 11:47:54.077431 140415667779328 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8861302733421326, loss=3.907670497894287
I0418 11:48:36.098105 140415659386624 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7732353806495667, loss=4.435538291931152
I0418 11:49:17.478849 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:49:32.229228 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:49:44.204463 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:49:45.858252 140594386749248 submission_runner.py:406] Time since start: 7706.23s, 	Step: 17400, 	{'train/accuracy': 0.5547851324081421, 'train/loss': 2.0698306560516357, 'validation/accuracy': 0.5034199953079224, 'validation/loss': 2.3168983459472656, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.948770761489868, 'test/num_examples': 10000, 'score': 7194.995788812637, 'total_duration': 7706.231145620346, 'accumulated_submission_time': 7194.995788812637, 'accumulated_eval_time': 478.45423913002014, 'accumulated_logging_time': 32.393452882766724}
I0418 11:49:45.875887 140415667779328 logging_writer.py:48] [17400] accumulated_eval_time=478.454239, accumulated_logging_time=32.393453, accumulated_submission_time=7194.995789, global_step=17400, preemption_count=0, score=7194.995789, test/accuracy=0.392400, test/loss=2.948771, test/num_examples=10000, total_duration=7706.231146, train/accuracy=0.554785, train/loss=2.069831, validation/accuracy=0.503420, validation/loss=2.316898, validation/num_examples=50000
I0418 11:49:46.227014 140594386749248 checkpoints.py:356] Saving checkpoint at step: 17400
I0418 11:49:47.460439 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_17400
I0418 11:49:47.486129 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_17400.
I0418 11:49:47.936379 140415659386624 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7504270672798157, loss=3.746273994445801
I0418 11:50:28.282310 140415650993920 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6516397595405579, loss=5.6253485679626465
I0418 11:51:10.671764 140415659386624 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6759956479072571, loss=5.7212395668029785
I0418 11:51:52.755047 140415650993920 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7183830142021179, loss=5.414825916290283
I0418 11:52:35.200271 140415659386624 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7112147808074951, loss=3.5380470752716064
I0418 11:53:17.444164 140415650993920 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6575517654418945, loss=4.183906555175781
I0418 11:53:59.624507 140415659386624 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7205820083618164, loss=3.6370160579681396
I0418 11:54:42.150597 140415650993920 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.1530325412750244, loss=3.651909828186035
I0418 11:55:24.700152 140415659386624 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6970491409301758, loss=5.582296371459961
I0418 11:56:06.732530 140415650993920 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7153745293617249, loss=3.5205440521240234
I0418 11:56:48.137469 140415659386624 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7312636375427246, loss=3.590228319168091
I0418 11:56:48.154359 140594386749248 spec.py:298] Evaluating on the training split.
I0418 11:57:02.878897 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 11:57:14.528593 140594386749248 spec.py:326] Evaluating on the test split.
I0418 11:57:16.189421 140594386749248 submission_runner.py:406] Time since start: 8156.56s, 	Step: 18401, 	{'train/accuracy': 0.570996105670929, 'train/loss': 1.9350695610046387, 'validation/accuracy': 0.5205000042915344, 'validation/loss': 2.1594789028167725, 'validation/num_examples': 50000, 'test/accuracy': 0.406000018119812, 'test/loss': 2.805384874343872, 'test/num_examples': 10000, 'score': 7615.640611171722, 'total_duration': 8156.562297105789, 'accumulated_submission_time': 7615.640611171722, 'accumulated_eval_time': 506.4892382621765, 'accumulated_logging_time': 34.02327847480774}
I0418 11:57:16.205519 140415650993920 logging_writer.py:48] [18401] accumulated_eval_time=506.489238, accumulated_logging_time=34.023278, accumulated_submission_time=7615.640611, global_step=18401, preemption_count=0, score=7615.640611, test/accuracy=0.406000, test/loss=2.805385, test/num_examples=10000, total_duration=8156.562297, train/accuracy=0.570996, train/loss=1.935070, validation/accuracy=0.520500, validation/loss=2.159479, validation/num_examples=50000
I0418 11:57:16.525203 140594386749248 checkpoints.py:356] Saving checkpoint at step: 18401
I0418 11:57:17.707159 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_18401
I0418 11:57:17.732208 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_18401.
I0418 11:57:58.271286 140415659386624 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9123315215110779, loss=3.5421762466430664
I0418 11:58:40.374671 140415432914688 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8170792460441589, loss=3.5351974964141846
I0418 11:59:22.660255 140415659386624 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7492533326148987, loss=5.544139385223389
I0418 12:00:05.267841 140415432914688 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.8072561025619507, loss=5.604240894317627
I0418 12:00:47.430165 140415659386624 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7392613291740417, loss=3.5912857055664062
I0418 12:01:29.477141 140415432914688 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7132847309112549, loss=4.233644962310791
I0418 12:02:11.745785 140415659386624 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.735371470451355, loss=3.4996774196624756
I0418 12:02:53.830118 140415432914688 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7941368818283081, loss=3.882983922958374
I0418 12:03:35.580880 140415659386624 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7445778250694275, loss=5.285545349121094
I0418 12:04:17.545794 140415432914688 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.8180149793624878, loss=5.268374443054199
I0418 12:04:18.162040 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:04:32.924476 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:04:45.028584 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:04:46.675352 140594386749248 submission_runner.py:406] Time since start: 8607.05s, 	Step: 19403, 	{'train/accuracy': 0.579394519329071, 'train/loss': 1.8892959356307983, 'validation/accuracy': 0.5363999605178833, 'validation/loss': 2.0957555770874023, 'validation/num_examples': 50000, 'test/accuracy': 0.42190003395080566, 'test/loss': 2.741122245788574, 'test/num_examples': 10000, 'score': 8036.034633398056, 'total_duration': 8607.048247814178, 'accumulated_submission_time': 8036.034633398056, 'accumulated_eval_time': 535.0024814605713, 'accumulated_logging_time': 35.58020496368408}
I0418 12:04:46.688618 140415659386624 logging_writer.py:48] [19403] accumulated_eval_time=535.002481, accumulated_logging_time=35.580205, accumulated_submission_time=8036.034633, global_step=19403, preemption_count=0, score=8036.034633, test/accuracy=0.421900, test/loss=2.741122, test/num_examples=10000, total_duration=8607.048248, train/accuracy=0.579395, train/loss=1.889296, validation/accuracy=0.536400, validation/loss=2.095756, validation/num_examples=50000
I0418 12:04:46.979015 140594386749248 checkpoints.py:356] Saving checkpoint at step: 19403
I0418 12:04:48.338212 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_19403
I0418 12:04:48.360457 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_19403.
I0418 12:05:27.976703 140415432914688 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7381958961486816, loss=4.247858047485352
I0418 12:06:09.653898 140415298696960 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7048362493515015, loss=5.119431018829346
I0418 12:06:51.385178 140415432914688 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8670447468757629, loss=5.618223190307617
I0418 12:07:33.124900 140415298696960 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8186945915222168, loss=3.5297701358795166
I0418 12:08:15.425964 140415432914688 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7189510464668274, loss=3.862180709838867
I0418 12:08:58.021742 140415298696960 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7322027087211609, loss=3.5104970932006836
I0418 12:09:40.889164 140415432914688 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6953067183494568, loss=4.5768914222717285
I0418 12:10:23.321008 140415298696960 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7837827205657959, loss=3.447388172149658
I0418 12:11:06.248899 140415432914688 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8811182379722595, loss=4.178904056549072
I0418 12:11:48.870771 140415298696960 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8849278092384338, loss=3.427457571029663
I0418 12:11:48.882460 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:12:04.627238 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:12:15.831959 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:12:17.479645 140594386749248 submission_runner.py:406] Time since start: 9057.85s, 	Step: 20401, 	{'train/accuracy': 0.5851953029632568, 'train/loss': 1.8952921628952026, 'validation/accuracy': 0.5391600131988525, 'validation/loss': 2.1126699447631836, 'validation/num_examples': 50000, 'test/accuracy': 0.4239000082015991, 'test/loss': 2.756401538848877, 'test/num_examples': 10000, 'score': 8456.512947797775, 'total_duration': 9057.852578163147, 'accumulated_submission_time': 8456.512947797775, 'accumulated_eval_time': 563.599636554718, 'accumulated_logging_time': 37.28787803649902}
I0418 12:12:17.491450 140415432914688 logging_writer.py:48] [20401] accumulated_eval_time=563.599637, accumulated_logging_time=37.287878, accumulated_submission_time=8456.512948, global_step=20401, preemption_count=0, score=8456.512948, test/accuracy=0.423900, test/loss=2.756402, test/num_examples=10000, total_duration=9057.852578, train/accuracy=0.585195, train/loss=1.895292, validation/accuracy=0.539160, validation/loss=2.112670, validation/num_examples=50000
I0418 12:12:17.844128 140594386749248 checkpoints.py:356] Saving checkpoint at step: 20401
I0418 12:12:19.279981 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_20401
I0418 12:12:19.304696 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_20401.
I0418 12:12:58.925780 140415298696960 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7198664546012878, loss=3.5104260444641113
I0418 12:13:39.937724 140415290304256 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6984850764274597, loss=3.7330551147460938
I0418 12:14:22.121813 140415298696960 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7508317232131958, loss=3.864095449447632
I0418 12:15:03.980864 140415290304256 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7977945804595947, loss=5.527156352996826
I0418 12:15:45.332528 140415298696960 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8041046261787415, loss=4.569491386413574
I0418 12:16:26.721926 140415290304256 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7271581888198853, loss=3.3647446632385254
I0418 12:17:08.188500 140415298696960 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7698672413825989, loss=3.8017661571502686
I0418 12:17:49.856726 140415290304256 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.8487399220466614, loss=3.553617000579834
I0418 12:18:31.623715 140415298696960 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7124441862106323, loss=3.452834367752075
I0418 12:19:13.013430 140415290304256 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9431386590003967, loss=3.5295462608337402
I0418 12:19:19.671131 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:19:33.932512 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:19:47.204286 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:19:48.861841 140594386749248 submission_runner.py:406] Time since start: 9509.23s, 	Step: 21418, 	{'train/accuracy': 0.6039062142372131, 'train/loss': 1.8064926862716675, 'validation/accuracy': 0.5517199635505676, 'validation/loss': 2.050670862197876, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.685436487197876, 'test/num_examples': 10000, 'score': 8876.855148553848, 'total_duration': 9509.234711647034, 'accumulated_submission_time': 8876.855148553848, 'accumulated_eval_time': 592.7902672290802, 'accumulated_logging_time': 39.11472201347351}
I0418 12:19:48.873944 140415298696960 logging_writer.py:48] [21418] accumulated_eval_time=592.790267, accumulated_logging_time=39.114722, accumulated_submission_time=8876.855149, global_step=21418, preemption_count=0, score=8876.855149, test/accuracy=0.430000, test/loss=2.685436, test/num_examples=10000, total_duration=9509.234712, train/accuracy=0.603906, train/loss=1.806493, validation/accuracy=0.551720, validation/loss=2.050671, validation/num_examples=50000
I0418 12:19:49.198222 140594386749248 checkpoints.py:356] Saving checkpoint at step: 21418
I0418 12:19:50.459486 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_21418
I0418 12:19:50.484425 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_21418.
I0418 12:20:24.053614 140415290304256 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7744508385658264, loss=5.373741149902344
I0418 12:21:06.179047 140415281911552 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7371376752853394, loss=3.828390598297119
I0418 12:21:48.937495 140415290304256 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8400676846504211, loss=3.4305596351623535
I0418 12:22:31.481520 140415281911552 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7747311592102051, loss=3.3621039390563965
I0418 12:23:14.357865 140415290304256 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8751242160797119, loss=3.4555904865264893
I0418 12:23:56.773755 140415281911552 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7233861684799194, loss=3.3707823753356934
I0418 12:24:39.920239 140415290304256 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.8106979131698608, loss=3.9795243740081787
I0418 12:25:22.391138 140415281911552 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.8769837617874146, loss=5.473164081573486
I0418 12:26:05.024027 140415290304256 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9685478806495667, loss=3.498202323913574
I0418 12:26:47.829027 140415281911552 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7135056853294373, loss=4.473172187805176
I0418 12:26:50.665303 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:27:05.031445 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:27:17.901307 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:27:19.558934 140594386749248 submission_runner.py:406] Time since start: 9959.93s, 	Step: 22408, 	{'train/accuracy': 0.6345312595367432, 'train/loss': 1.6439292430877686, 'validation/accuracy': 0.5633599758148193, 'validation/loss': 1.9520474672317505, 'validation/num_examples': 50000, 'test/accuracy': 0.4455000162124634, 'test/loss': 2.6046109199523926, 'test/num_examples': 10000, 'score': 9296.99292564392, 'total_duration': 9959.93182516098, 'accumulated_submission_time': 9296.99292564392, 'accumulated_eval_time': 621.6838374137878, 'accumulated_logging_time': 40.75931668281555}
I0418 12:27:19.576259 140415290304256 logging_writer.py:48] [22408] accumulated_eval_time=621.683837, accumulated_logging_time=40.759317, accumulated_submission_time=9296.992926, global_step=22408, preemption_count=0, score=9296.992926, test/accuracy=0.445500, test/loss=2.604611, test/num_examples=10000, total_duration=9959.931825, train/accuracy=0.634531, train/loss=1.643929, validation/accuracy=0.563360, validation/loss=1.952047, validation/num_examples=50000
I0418 12:27:19.954711 140594386749248 checkpoints.py:356] Saving checkpoint at step: 22408
I0418 12:27:21.546457 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_22408
I0418 12:27:21.569009 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_22408.
I0418 12:27:59.310853 140415281911552 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7763803601264954, loss=3.4247519969940186
I0418 12:28:42.482654 140415273518848 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6958773732185364, loss=4.674846649169922
I0418 12:29:25.074859 140415281911552 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7317416071891785, loss=4.3478264808654785
I0418 12:30:08.269627 140415273518848 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7515689134597778, loss=3.3720507621765137
I0418 12:30:51.450527 140415281911552 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8443134427070618, loss=3.5785903930664062
I0418 12:31:34.090626 140415273518848 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.81429523229599, loss=3.3274238109588623
I0418 12:32:16.940518 140415281911552 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7292974591255188, loss=3.456580400466919
I0418 12:32:59.809561 140415273518848 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.7852213382720947, loss=3.499847888946533
I0418 12:33:42.764089 140415281911552 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.7913111448287964, loss=3.315643787384033
I0418 12:34:21.612346 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:34:36.598582 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:34:47.953266 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:34:49.651123 140594386749248 submission_runner.py:406] Time since start: 10410.02s, 	Step: 23392, 	{'train/accuracy': 0.6178515553474426, 'train/loss': 1.674094319343567, 'validation/accuracy': 0.5678200125694275, 'validation/loss': 1.909401774406433, 'validation/num_examples': 50000, 'test/accuracy': 0.4447000324726105, 'test/loss': 2.5869715213775635, 'test/num_examples': 10000, 'score': 9717.012940645218, 'total_duration': 10410.020280122757, 'accumulated_submission_time': 9717.012940645218, 'accumulated_eval_time': 649.7188606262207, 'accumulated_logging_time': 42.77150774002075}
I0418 12:34:49.672143 140415273518848 logging_writer.py:48] [23392] accumulated_eval_time=649.718861, accumulated_logging_time=42.771508, accumulated_submission_time=9717.012941, global_step=23392, preemption_count=0, score=9717.012941, test/accuracy=0.444700, test/loss=2.586972, test/num_examples=10000, total_duration=10410.020280, train/accuracy=0.617852, train/loss=1.674094, validation/accuracy=0.567820, validation/loss=1.909402, validation/num_examples=50000
I0418 12:34:50.064855 140594386749248 checkpoints.py:356] Saving checkpoint at step: 23392
I0418 12:34:51.429668 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_23392
I0418 12:34:51.453669 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_23392.
I0418 12:34:55.097650 140415281911552 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8340893983840942, loss=4.870519161224365
I0418 12:35:36.662981 140415265126144 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8270310163497925, loss=3.57798433303833
I0418 12:36:20.218134 140415281911552 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7518652081489563, loss=5.337136745452881
I0418 12:37:03.906157 140415265126144 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7172554731369019, loss=4.045327663421631
I0418 12:37:47.581641 140415281911552 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8829793930053711, loss=4.36852502822876
I0418 12:38:31.566550 140415265126144 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.892056405544281, loss=3.269070863723755
I0418 12:39:15.468801 140415281911552 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8095925450325012, loss=3.578766107559204
I0418 12:39:59.529569 140415265126144 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.8270552158355713, loss=4.580968379974365
I0418 12:40:43.649844 140415281911552 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8663280010223389, loss=5.349320888519287
I0418 12:41:28.158932 140415265126144 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7774737477302551, loss=3.4226481914520264
I0418 12:41:51.478749 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:42:02.916645 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:42:14.967881 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:42:16.647027 140594386749248 submission_runner.py:406] Time since start: 10857.02s, 	Step: 24354, 	{'train/accuracy': 0.6248242259025574, 'train/loss': 1.6561514139175415, 'validation/accuracy': 0.5770399570465088, 'validation/loss': 1.8987233638763428, 'validation/num_examples': 50000, 'test/accuracy': 0.4490000307559967, 'test/loss': 2.5611367225646973, 'test/num_examples': 10000, 'score': 10136.996198654175, 'total_duration': 10857.018958568573, 'accumulated_submission_time': 10136.996198654175, 'accumulated_eval_time': 674.8861167430878, 'accumulated_logging_time': 44.59566330909729}
I0418 12:42:16.658786 140415281911552 logging_writer.py:48] [24354] accumulated_eval_time=674.886117, accumulated_logging_time=44.595663, accumulated_submission_time=10136.996199, global_step=24354, preemption_count=0, score=10136.996199, test/accuracy=0.449000, test/loss=2.561137, test/num_examples=10000, total_duration=10857.018959, train/accuracy=0.624824, train/loss=1.656151, validation/accuracy=0.577040, validation/loss=1.898723, validation/num_examples=50000
I0418 12:42:16.981871 140594386749248 checkpoints.py:356] Saving checkpoint at step: 24354
I0418 12:42:18.398702 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_24354
I0418 12:42:18.427535 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_24354.
I0418 12:42:36.855064 140415265126144 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.9375609755516052, loss=5.266891956329346
I0418 12:43:17.757594 140415256733440 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8495734930038452, loss=4.096830368041992
I0418 12:43:59.385751 140415265126144 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.8656476736068726, loss=3.2661325931549072
I0418 12:44:41.508700 140415256733440 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8492909073829651, loss=3.6350362300872803
I0418 12:45:23.821255 140415265126144 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8142034411430359, loss=3.852299690246582
I0418 12:46:05.114340 140415256733440 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.9110063314437866, loss=5.515395164489746
I0418 12:46:46.926046 140415265126144 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8305524587631226, loss=3.273648262023926
I0418 12:47:29.390279 140415256733440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8154972791671753, loss=3.3654000759124756
I0418 12:48:11.229812 140415265126144 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.8033117055892944, loss=4.264766216278076
I0418 12:48:53.272797 140415256733440 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8406002521514893, loss=5.268914699554443
I0418 12:49:18.495175 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:49:29.023823 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:49:41.641706 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:49:43.298513 140594386749248 submission_runner.py:406] Time since start: 11303.67s, 	Step: 25362, 	{'train/accuracy': 0.6364843845367432, 'train/loss': 1.6743377447128296, 'validation/accuracy': 0.5799799561500549, 'validation/loss': 1.9440792798995972, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.5710415840148926, 'test/num_examples': 10000, 'score': 10557.040154218674, 'total_duration': 11303.67039680481, 'accumulated_submission_time': 10557.040154218674, 'accumulated_eval_time': 699.6883912086487, 'accumulated_logging_time': 46.37833523750305}
I0418 12:49:43.316908 140415265126144 logging_writer.py:48] [25362] accumulated_eval_time=699.688391, accumulated_logging_time=46.378335, accumulated_submission_time=10557.040154, global_step=25362, preemption_count=0, score=10557.040154, test/accuracy=0.459300, test/loss=2.571042, test/num_examples=10000, total_duration=11303.670397, train/accuracy=0.636484, train/loss=1.674338, validation/accuracy=0.579980, validation/loss=1.944079, validation/num_examples=50000
I0418 12:49:43.661407 140594386749248 checkpoints.py:356] Saving checkpoint at step: 25362
I0418 12:49:44.981191 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_25362
I0418 12:49:45.009670 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_25362.
I0418 12:50:00.360628 140415256733440 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8065295815467834, loss=5.375559329986572
I0418 12:50:41.530663 140415248340736 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8999052047729492, loss=3.2511537075042725
I0418 12:51:23.416275 140415256733440 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8621553182601929, loss=3.3557181358337402
I0418 12:52:05.412667 140415248340736 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8594373464584351, loss=3.234553337097168
I0418 12:52:47.284249 140415256733440 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7815058827400208, loss=3.3947699069976807
I0418 12:53:28.993391 140415248340736 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.780931293964386, loss=3.2673919200897217
I0418 12:54:10.824174 140415256733440 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8410699963569641, loss=4.542697429656982
I0418 12:54:52.998708 140415248340736 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9820378422737122, loss=5.3886542320251465
I0418 12:55:35.679734 140415256733440 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8018196225166321, loss=3.2319700717926025
I0418 12:56:17.994140 140415248340736 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.859155535697937, loss=3.2185418605804443
I0418 12:56:45.219613 140594386749248 spec.py:298] Evaluating on the training split.
I0418 12:56:55.897853 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 12:57:08.402917 140594386749248 spec.py:326] Evaluating on the test split.
I0418 12:57:10.058655 140594386749248 submission_runner.py:406] Time since start: 11750.43s, 	Step: 26367, 	{'train/accuracy': 0.6668944954872131, 'train/loss': 1.481590747833252, 'validation/accuracy': 0.5948399901390076, 'validation/loss': 1.8130745887756348, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.467430591583252, 'test/num_examples': 10000, 'score': 10977.226530790329, 'total_duration': 11750.430648803711, 'accumulated_submission_time': 10977.226530790329, 'accumulated_eval_time': 724.5264637470245, 'accumulated_logging_time': 48.09145903587341}
I0418 12:57:10.070855 140415256733440 logging_writer.py:48] [26367] accumulated_eval_time=724.526464, accumulated_logging_time=48.091459, accumulated_submission_time=10977.226531, global_step=26367, preemption_count=0, score=10977.226531, test/accuracy=0.469300, test/loss=2.467431, test/num_examples=10000, total_duration=11750.430649, train/accuracy=0.666894, train/loss=1.481591, validation/accuracy=0.594840, validation/loss=1.813075, validation/num_examples=50000
I0418 12:57:10.436108 140594386749248 checkpoints.py:356] Saving checkpoint at step: 26367
I0418 12:57:11.694172 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_26367
I0418 12:57:11.720094 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_26367.
I0418 12:57:25.035409 140415248340736 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8466137051582336, loss=3.35366153717041
I0418 12:58:06.210099 140415164479232 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.850753128528595, loss=3.9945430755615234
I0418 12:58:48.921607 140415248340736 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8771337866783142, loss=3.1825714111328125
I0418 12:59:31.208513 140415164479232 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9619717597961426, loss=5.235907554626465
I0418 13:00:13.847205 140415248340736 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8602923154830933, loss=3.2806577682495117
I0418 13:00:56.342294 140415164479232 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.8962473273277283, loss=5.226428985595703
I0418 13:01:38.642091 140415248340736 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.848701000213623, loss=3.2050068378448486
I0418 13:02:21.478806 140415164479232 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8222070932388306, loss=3.172935962677002
I0418 13:03:03.763571 140415248340736 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7697533369064331, loss=3.15366530418396
I0418 13:03:46.061680 140415164479232 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8411555886268616, loss=3.567974090576172
I0418 13:04:12.154534 140594386749248 spec.py:298] Evaluating on the training split.
I0418 13:04:22.564112 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 13:04:35.194445 140594386749248 spec.py:326] Evaluating on the test split.
I0418 13:04:36.843088 140594386749248 submission_runner.py:406] Time since start: 12197.22s, 	Step: 27363, 	{'train/accuracy': 0.6419531106948853, 'train/loss': 1.6507896184921265, 'validation/accuracy': 0.591759979724884, 'validation/loss': 1.871789813041687, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.5002119541168213, 'test/num_examples': 10000, 'score': 11397.638077497482, 'total_duration': 12197.215053796768, 'accumulated_submission_time': 11397.638077497482, 'accumulated_eval_time': 749.2140369415283, 'accumulated_logging_time': 49.75471472740173}
I0418 13:04:36.854971 140415248340736 logging_writer.py:48] [27363] accumulated_eval_time=749.214037, accumulated_logging_time=49.754715, accumulated_submission_time=11397.638077, global_step=27363, preemption_count=0, score=11397.638077, test/accuracy=0.468900, test/loss=2.500212, test/num_examples=10000, total_duration=12197.215054, train/accuracy=0.641953, train/loss=1.650790, validation/accuracy=0.591760, validation/loss=1.871790, validation/num_examples=50000
I0418 13:04:37.228476 140594386749248 checkpoints.py:356] Saving checkpoint at step: 27363
I0418 13:04:38.484248 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_27363
I0418 13:04:38.509392 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_27363.
I0418 13:04:53.475894 140415164479232 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.8637791872024536, loss=3.2543725967407227
I0418 13:05:34.752232 140415156086528 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8872905373573303, loss=5.120014667510986
I0418 13:06:16.481988 140415164479232 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8195021152496338, loss=4.02731466293335
I0418 13:06:58.839805 140415156086528 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7922551035881042, loss=2.98230242729187
I0418 13:07:41.385226 140415164479232 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8151395320892334, loss=3.245840311050415
I0418 13:08:23.395057 140415156086528 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8185480237007141, loss=3.1307101249694824
I0418 13:09:05.098556 140594386749248 spec.py:298] Evaluating on the training split.
I0418 13:09:15.755203 140594386749248 spec.py:310] Evaluating on the validation split.
I0418 13:09:28.399703 140594386749248 spec.py:326] Evaluating on the test split.
I0418 13:09:30.048634 140594386749248 submission_runner.py:406] Time since start: 12490.42s, 	Step: 28000, 	{'train/accuracy': 0.6635351181030273, 'train/loss': 1.4752415418624878, 'validation/accuracy': 0.6047599911689758, 'validation/loss': 1.746179461479187, 'validation/num_examples': 50000, 'test/accuracy': 0.48180001974105835, 'test/loss': 2.3862826824188232, 'test/num_examples': 10000, 'score': 11664.211855649948, 'total_duration': 12490.420605182648, 'accumulated_submission_time': 11664.211855649948, 'accumulated_eval_time': 774.163138628006, 'accumulated_logging_time': 51.422988414764404}
I0418 13:09:30.064246 140415164479232 logging_writer.py:48] [28000] accumulated_eval_time=774.163139, accumulated_logging_time=51.422988, accumulated_submission_time=11664.211856, global_step=28000, preemption_count=0, score=11664.211856, test/accuracy=0.481800, test/loss=2.386283, test/num_examples=10000, total_duration=12490.420605, train/accuracy=0.663535, train/loss=1.475242, validation/accuracy=0.604760, validation/loss=1.746179, validation/num_examples=50000
I0418 13:09:30.340711 140594386749248 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 13:09:31.742765 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000
I0418 13:09:31.769839 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000.
I0418 13:09:31.806461 140415156086528 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11664.211856
I0418 13:09:32.082093 140594386749248 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 13:09:33.223664 140594386749248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000
I0418 13:09:33.245045 140594386749248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000.
I0418 13:09:34.239732 140594386749248 submission_runner.py:567] Tuning trial 1/1
I0418 13:09:34.240000 140594386749248 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 13:09:34.244472 140594386749248 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008789062267169356, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 51.25894021987915, 'total_duration': 99.7916989326477, 'accumulated_submission_time': 51.25894021987915, 'accumulated_eval_time': 48.53258752822876, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1001, {'train/accuracy': 0.025644531473517418, 'train/loss': 6.0797014236450195, 'validation/accuracy': 0.023520000278949738, 'validation/loss': 6.1089091300964355, 'validation/num_examples': 50000, 'test/accuracy': 0.01860000006854534, 'test/loss': 6.193573951721191, 'test/num_examples': 10000, 'score': 471.9129114151001, 'total_duration': 540.5116999149323, 'accumulated_submission_time': 471.9129114151001, 'accumulated_eval_time': 67.77616310119629, 'accumulated_logging_time': 0.7992143630981445, 'global_step': 1001, 'preemption_count': 0}), (2041, {'train/accuracy': 0.061601560562849045, 'train/loss': 5.483028888702393, 'validation/accuracy': 0.05838000029325485, 'validation/loss': 5.506394863128662, 'validation/num_examples': 50000, 'test/accuracy': 0.0471000038087368, 'test/loss': 5.674988746643066, 'test/num_examples': 10000, 'score': 892.1779072284698, 'total_duration': 983.6068339347839, 'accumulated_submission_time': 892.1779072284698, 'accumulated_eval_time': 87.53355884552002, 'accumulated_logging_time': 3.8484058380126953, 'global_step': 2041, 'preemption_count': 0}), (3079, {'train/accuracy': 0.11640624701976776, 'train/loss': 4.889256000518799, 'validation/accuracy': 0.10723999887704849, 'validation/loss': 4.95236873626709, 'validation/num_examples': 50000, 'test/accuracy': 0.0852000042796135, 'test/loss': 5.202913761138916, 'test/num_examples': 10000, 'score': 1312.4231100082397, 'total_duration': 1427.8940751552582, 'accumulated_submission_time': 1312.4231100082397, 'accumulated_eval_time': 107.72242283821106, 'accumulated_logging_time': 7.677483797073364, 'global_step': 3079, 'preemption_count': 0}), (4118, {'train/accuracy': 0.16347655653953552, 'train/loss': 4.485172748565674, 'validation/accuracy': 0.14761999249458313, 'validation/loss': 4.594143867492676, 'validation/num_examples': 50000, 'test/accuracy': 0.11150000244379044, 'test/loss': 4.9295172691345215, 'test/num_examples': 10000, 'score': 1732.7340202331543, 'total_duration': 1872.9867980480194, 'accumulated_submission_time': 1732.7340202331543, 'accumulated_eval_time': 128.69847440719604, 'accumulated_logging_time': 11.459540605545044, 'global_step': 4118, 'preemption_count': 0}), (5153, {'train/accuracy': 0.18675780296325684, 'train/loss': 4.352721691131592, 'validation/accuracy': 0.17401999235153198, 'validation/loss': 4.4328813552856445, 'validation/num_examples': 50000, 'test/accuracy': 0.13650000095367432, 'test/loss': 4.757118225097656, 'test/num_examples': 10000, 'score': 2153.0280542373657, 'total_duration': 2318.8119418621063, 'accumulated_submission_time': 2153.0280542373657, 'accumulated_eval_time': 151.42643427848816, 'accumulated_logging_time': 14.23920464515686, 'global_step': 5153, 'preemption_count': 0}), (6190, {'train/accuracy': 0.24308593571186066, 'train/loss': 3.8328914642333984, 'validation/accuracy': 0.2274399995803833, 'validation/loss': 3.937016725540161, 'validation/num_examples': 50000, 'test/accuracy': 0.16930000483989716, 'test/loss': 4.3815598487854, 'test/num_examples': 10000, 'score': 2573.151726961136, 'total_duration': 2765.3920097351074, 'accumulated_submission_time': 2573.151726961136, 'accumulated_eval_time': 176.17916798591614, 'accumulated_logging_time': 15.919315814971924, 'global_step': 6190, 'preemption_count': 0}), (7223, {'train/accuracy': 0.29179686307907104, 'train/loss': 3.5589025020599365, 'validation/accuracy': 0.26725998520851135, 'validation/loss': 3.697038412094116, 'validation/num_examples': 50000, 'test/accuracy': 0.20120000839233398, 'test/loss': 4.182796478271484, 'test/num_examples': 10000, 'score': 2993.395399570465, 'total_duration': 3213.247522830963, 'accumulated_submission_time': 2993.395399570465, 'accumulated_eval_time': 202.45682644844055, 'accumulated_logging_time': 17.230114936828613, 'global_step': 7223, 'preemption_count': 0}), (8254, {'train/accuracy': 0.33542966842651367, 'train/loss': 3.2091894149780273, 'validation/accuracy': 0.30469998717308044, 'validation/loss': 3.369858980178833, 'validation/num_examples': 50000, 'test/accuracy': 0.22800001502037048, 'test/loss': 3.906121253967285, 'test/num_examples': 10000, 'score': 3413.3734250068665, 'total_duration': 3661.340737104416, 'accumulated_submission_time': 3413.3734250068665, 'accumulated_eval_time': 228.99924182891846, 'accumulated_logging_time': 18.77948832511902, 'global_step': 8254, 'preemption_count': 0}), (9285, {'train/accuracy': 0.38496091961860657, 'train/loss': 2.920070171356201, 'validation/accuracy': 0.3336399793624878, 'validation/loss': 3.195288896560669, 'validation/num_examples': 50000, 'test/accuracy': 0.2597000002861023, 'test/loss': 3.758755922317505, 'test/num_examples': 10000, 'score': 3833.5198884010315, 'total_duration': 4109.95365524292, 'accumulated_submission_time': 3833.5198884010315, 'accumulated_eval_time': 255.8327248096466, 'accumulated_logging_time': 20.38954210281372, 'global_step': 9285, 'preemption_count': 0}), (10311, {'train/accuracy': 0.39482420682907104, 'train/loss': 2.8886733055114746, 'validation/accuracy': 0.3696799874305725, 'validation/loss': 3.031848669052124, 'validation/num_examples': 50000, 'test/accuracy': 0.27740001678466797, 'test/loss': 3.6257095336914062, 'test/num_examples': 10000, 'score': 4253.649785995483, 'total_duration': 4558.57998585701, 'accumulated_submission_time': 4253.649785995483, 'accumulated_eval_time': 282.95273542404175, 'accumulated_logging_time': 21.743231773376465, 'global_step': 10311, 'preemption_count': 0}), (11336, {'train/accuracy': 0.439453125, 'train/loss': 2.6458230018615723, 'validation/accuracy': 0.4043999910354614, 'validation/loss': 2.824176549911499, 'validation/num_examples': 50000, 'test/accuracy': 0.30730000138282776, 'test/loss': 3.4183876514434814, 'test/num_examples': 10000, 'score': 4673.78355717659, 'total_duration': 5007.289900779724, 'accumulated_submission_time': 4673.78355717659, 'accumulated_eval_time': 310.0673408508301, 'accumulated_logging_time': 23.18234920501709, 'global_step': 11336, 'preemption_count': 0}), (12356, {'train/accuracy': 0.4659179449081421, 'train/loss': 2.4429397583007812, 'validation/accuracy': 0.4273200035095215, 'validation/loss': 2.6514198780059814, 'validation/num_examples': 50000, 'test/accuracy': 0.3297000229358673, 'test/loss': 3.2757463455200195, 'test/num_examples': 10000, 'score': 5094.169572353363, 'total_duration': 5456.965566635132, 'accumulated_submission_time': 5094.169572353363, 'accumulated_eval_time': 338.0567395687103, 'accumulated_logging_time': 24.45993971824646, 'global_step': 12356, 'preemption_count': 0}), (13366, {'train/accuracy': 0.49916014075279236, 'train/loss': 2.3219447135925293, 'validation/accuracy': 0.4532800018787384, 'validation/loss': 2.5538370609283447, 'validation/num_examples': 50000, 'test/accuracy': 0.3508000075817108, 'test/loss': 3.1772141456604004, 'test/num_examples': 10000, 'score': 5514.375582695007, 'total_duration': 5906.638055324554, 'accumulated_submission_time': 5514.375582695007, 'accumulated_eval_time': 366.06607723236084, 'accumulated_logging_time': 25.895101308822632, 'global_step': 13366, 'preemption_count': 0}), (14376, {'train/accuracy': 0.49345701932907104, 'train/loss': 2.3743441104888916, 'validation/accuracy': 0.4593599736690521, 'validation/loss': 2.546607494354248, 'validation/num_examples': 50000, 'test/accuracy': 0.358100026845932, 'test/loss': 3.1483547687530518, 'test/num_examples': 10000, 'score': 5934.52911567688, 'total_duration': 6356.380882740021, 'accumulated_submission_time': 5934.52911567688, 'accumulated_eval_time': 394.0832166671753, 'accumulated_logging_time': 27.445635557174683, 'global_step': 14376, 'preemption_count': 0}), (15385, {'train/accuracy': 0.515332043170929, 'train/loss': 2.1830995082855225, 'validation/accuracy': 0.47689998149871826, 'validation/loss': 2.3813555240631104, 'validation/num_examples': 50000, 'test/accuracy': 0.3725000321865082, 'test/loss': 3.0237486362457275, 'test/num_examples': 10000, 'score': 6354.821929931641, 'total_duration': 6806.355770349503, 'accumulated_submission_time': 6354.821929931641, 'accumulated_eval_time': 422.1128902435303, 'accumulated_logging_time': 29.076247930526733, 'global_step': 15385, 'preemption_count': 0}), (16392, {'train/accuracy': 0.5444530844688416, 'train/loss': 2.094566822052002, 'validation/accuracy': 0.5009799599647522, 'validation/loss': 2.3031516075134277, 'validation/num_examples': 50000, 'test/accuracy': 0.3856000304222107, 'test/loss': 2.942469596862793, 'test/num_examples': 10000, 'score': 6774.813853502274, 'total_duration': 7255.966229200363, 'accumulated_submission_time': 6774.813853502274, 'accumulated_eval_time': 450.0748815536499, 'accumulated_logging_time': 30.711421012878418, 'global_step': 16392, 'preemption_count': 0}), (17400, {'train/accuracy': 0.5547851324081421, 'train/loss': 2.0698306560516357, 'validation/accuracy': 0.5034199953079224, 'validation/loss': 2.3168983459472656, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.948770761489868, 'test/num_examples': 10000, 'score': 7194.995788812637, 'total_duration': 7706.231145620346, 'accumulated_submission_time': 7194.995788812637, 'accumulated_eval_time': 478.45423913002014, 'accumulated_logging_time': 32.393452882766724, 'global_step': 17400, 'preemption_count': 0}), (18401, {'train/accuracy': 0.570996105670929, 'train/loss': 1.9350695610046387, 'validation/accuracy': 0.5205000042915344, 'validation/loss': 2.1594789028167725, 'validation/num_examples': 50000, 'test/accuracy': 0.406000018119812, 'test/loss': 2.805384874343872, 'test/num_examples': 10000, 'score': 7615.640611171722, 'total_duration': 8156.562297105789, 'accumulated_submission_time': 7615.640611171722, 'accumulated_eval_time': 506.4892382621765, 'accumulated_logging_time': 34.02327847480774, 'global_step': 18401, 'preemption_count': 0}), (19403, {'train/accuracy': 0.579394519329071, 'train/loss': 1.8892959356307983, 'validation/accuracy': 0.5363999605178833, 'validation/loss': 2.0957555770874023, 'validation/num_examples': 50000, 'test/accuracy': 0.42190003395080566, 'test/loss': 2.741122245788574, 'test/num_examples': 10000, 'score': 8036.034633398056, 'total_duration': 8607.048247814178, 'accumulated_submission_time': 8036.034633398056, 'accumulated_eval_time': 535.0024814605713, 'accumulated_logging_time': 35.58020496368408, 'global_step': 19403, 'preemption_count': 0}), (20401, {'train/accuracy': 0.5851953029632568, 'train/loss': 1.8952921628952026, 'validation/accuracy': 0.5391600131988525, 'validation/loss': 2.1126699447631836, 'validation/num_examples': 50000, 'test/accuracy': 0.4239000082015991, 'test/loss': 2.756401538848877, 'test/num_examples': 10000, 'score': 8456.512947797775, 'total_duration': 9057.852578163147, 'accumulated_submission_time': 8456.512947797775, 'accumulated_eval_time': 563.599636554718, 'accumulated_logging_time': 37.28787803649902, 'global_step': 20401, 'preemption_count': 0}), (21418, {'train/accuracy': 0.6039062142372131, 'train/loss': 1.8064926862716675, 'validation/accuracy': 0.5517199635505676, 'validation/loss': 2.050670862197876, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.685436487197876, 'test/num_examples': 10000, 'score': 8876.855148553848, 'total_duration': 9509.234711647034, 'accumulated_submission_time': 8876.855148553848, 'accumulated_eval_time': 592.7902672290802, 'accumulated_logging_time': 39.11472201347351, 'global_step': 21418, 'preemption_count': 0}), (22408, {'train/accuracy': 0.6345312595367432, 'train/loss': 1.6439292430877686, 'validation/accuracy': 0.5633599758148193, 'validation/loss': 1.9520474672317505, 'validation/num_examples': 50000, 'test/accuracy': 0.4455000162124634, 'test/loss': 2.6046109199523926, 'test/num_examples': 10000, 'score': 9296.99292564392, 'total_duration': 9959.93182516098, 'accumulated_submission_time': 9296.99292564392, 'accumulated_eval_time': 621.6838374137878, 'accumulated_logging_time': 40.75931668281555, 'global_step': 22408, 'preemption_count': 0}), (23392, {'train/accuracy': 0.6178515553474426, 'train/loss': 1.674094319343567, 'validation/accuracy': 0.5678200125694275, 'validation/loss': 1.909401774406433, 'validation/num_examples': 50000, 'test/accuracy': 0.4447000324726105, 'test/loss': 2.5869715213775635, 'test/num_examples': 10000, 'score': 9717.012940645218, 'total_duration': 10410.020280122757, 'accumulated_submission_time': 9717.012940645218, 'accumulated_eval_time': 649.7188606262207, 'accumulated_logging_time': 42.77150774002075, 'global_step': 23392, 'preemption_count': 0}), (24354, {'train/accuracy': 0.6248242259025574, 'train/loss': 1.6561514139175415, 'validation/accuracy': 0.5770399570465088, 'validation/loss': 1.8987233638763428, 'validation/num_examples': 50000, 'test/accuracy': 0.4490000307559967, 'test/loss': 2.5611367225646973, 'test/num_examples': 10000, 'score': 10136.996198654175, 'total_duration': 10857.018958568573, 'accumulated_submission_time': 10136.996198654175, 'accumulated_eval_time': 674.8861167430878, 'accumulated_logging_time': 44.59566330909729, 'global_step': 24354, 'preemption_count': 0}), (25362, {'train/accuracy': 0.6364843845367432, 'train/loss': 1.6743377447128296, 'validation/accuracy': 0.5799799561500549, 'validation/loss': 1.9440792798995972, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.5710415840148926, 'test/num_examples': 10000, 'score': 10557.040154218674, 'total_duration': 11303.67039680481, 'accumulated_submission_time': 10557.040154218674, 'accumulated_eval_time': 699.6883912086487, 'accumulated_logging_time': 46.37833523750305, 'global_step': 25362, 'preemption_count': 0}), (26367, {'train/accuracy': 0.6668944954872131, 'train/loss': 1.481590747833252, 'validation/accuracy': 0.5948399901390076, 'validation/loss': 1.8130745887756348, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.467430591583252, 'test/num_examples': 10000, 'score': 10977.226530790329, 'total_duration': 11750.430648803711, 'accumulated_submission_time': 10977.226530790329, 'accumulated_eval_time': 724.5264637470245, 'accumulated_logging_time': 48.09145903587341, 'global_step': 26367, 'preemption_count': 0}), (27363, {'train/accuracy': 0.6419531106948853, 'train/loss': 1.6507896184921265, 'validation/accuracy': 0.591759979724884, 'validation/loss': 1.871789813041687, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.5002119541168213, 'test/num_examples': 10000, 'score': 11397.638077497482, 'total_duration': 12197.215053796768, 'accumulated_submission_time': 11397.638077497482, 'accumulated_eval_time': 749.2140369415283, 'accumulated_logging_time': 49.75471472740173, 'global_step': 27363, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6635351181030273, 'train/loss': 1.4752415418624878, 'validation/accuracy': 0.6047599911689758, 'validation/loss': 1.746179461479187, 'validation/num_examples': 50000, 'test/accuracy': 0.48180001974105835, 'test/loss': 2.3862826824188232, 'test/num_examples': 10000, 'score': 11664.211855649948, 'total_duration': 12490.420605182648, 'accumulated_submission_time': 11664.211855649948, 'accumulated_eval_time': 774.163138628006, 'accumulated_logging_time': 51.422988414764404, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0418 13:09:34.244606 140594386749248 submission_runner.py:570] Timing: 11664.211855649948
I0418 13:09:34.244658 140594386749248 submission_runner.py:571] ====================
I0418 13:09:34.244787 140594386749248 submission_runner.py:631] Final imagenet_vit score: 11664.211855649948
