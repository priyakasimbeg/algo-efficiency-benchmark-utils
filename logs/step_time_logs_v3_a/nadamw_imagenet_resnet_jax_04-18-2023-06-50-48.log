I0418 06:51:10.092885 140667534567232 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax.
I0418 06:51:10.154921 140667534567232 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 06:51:10.949492 140667534567232 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0418 06:51:10.950154 140667534567232 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 06:51:10.954020 140667534567232 submission_runner.py:528] Using RNG seed 2538109656
I0418 06:51:13.577658 140667534567232 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 06:51:13.577852 140667534567232 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1.
I0418 06:51:13.578054 140667534567232 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/hparams.json.
I0418 06:51:13.696869 140667534567232 submission_runner.py:232] Initializing dataset.
I0418 06:51:13.708641 140667534567232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:13.715643 140667534567232 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:51:13.715750 140667534567232 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:51:13.966848 140667534567232 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:15.027931 140667534567232 submission_runner.py:239] Initializing model.
I0418 06:51:26.029070 140667534567232 submission_runner.py:249] Initializing optimizer.
I0418 06:51:27.088700 140667534567232 submission_runner.py:256] Initializing metrics bundle.
I0418 06:51:27.088869 140667534567232 submission_runner.py:273] Initializing checkpoint and logger.
I0418 06:51:27.089889 140667534567232 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0418 06:51:27.741125 140667534567232 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0418 06:51:27.742025 140667534567232 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0418 06:51:27.746646 140667534567232 submission_runner.py:309] Starting training loop.
I0418 06:52:15.908766 140490548684544 logging_writer.py:48] [0] global_step=0, grad_norm=0.6015462875366211, loss=6.926770210266113
I0418 06:52:15.927941 140667534567232 spec.py:298] Evaluating on the training split.
I0418 06:52:16.412635 140667534567232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:16.418692 140667534567232 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:52:16.418817 140667534567232 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:52:16.482322 140667534567232 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:27.879083 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 06:52:28.640680 140667534567232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:28.663152 140667534567232 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:52:28.663410 140667534567232 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:52:28.716439 140667534567232 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:46.256537 140667534567232 spec.py:326] Evaluating on the test split.
I0418 06:52:46.655480 140667534567232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 06:52:46.659991 140667534567232 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0418 06:52:46.688761 140667534567232 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 06:52:55.823387 140667534567232 submission_runner.py:406] Time since start: 88.08s, 	Step: 1, 	{'train/accuracy': 0.0011758609907701612, 'train/loss': 6.912867069244385, 'validation/accuracy': 0.0010599999222904444, 'validation/loss': 6.912654876708984, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.912196636199951, 'test/num_examples': 10000, 'score': 48.18110656738281, 'total_duration': 88.07666754722595, 'accumulated_submission_time': 48.18110656738281, 'accumulated_eval_time': 39.895402908325195, 'accumulated_logging_time': 0}
I0418 06:52:55.840315 140462459442944 logging_writer.py:48] [1] accumulated_eval_time=39.895403, accumulated_logging_time=0, accumulated_submission_time=48.181107, global_step=1, preemption_count=0, score=48.181107, test/accuracy=0.001100, test/loss=6.912197, test/num_examples=10000, total_duration=88.076668, train/accuracy=0.001176, train/loss=6.912867, validation/accuracy=0.001060, validation/loss=6.912655, validation/num_examples=50000
I0418 06:52:56.002057 140667534567232 checkpoints.py:356] Saving checkpoint at step: 1
I0418 06:52:56.631443 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1
I0418 06:52:56.632529 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1.
I0418 06:53:30.547063 140462467835648 logging_writer.py:48] [100] global_step=100, grad_norm=0.6097198724746704, loss=6.880649089813232
I0418 06:54:04.526461 140462702700288 logging_writer.py:48] [200] global_step=200, grad_norm=0.6608439087867737, loss=6.753422260284424
I0418 06:54:38.579862 140462467835648 logging_writer.py:48] [300] global_step=300, grad_norm=0.720367968082428, loss=6.578003883361816
I0418 06:55:12.545273 140462702700288 logging_writer.py:48] [400] global_step=400, grad_norm=0.7546696066856384, loss=6.4889678955078125
I0418 06:55:46.497637 140462467835648 logging_writer.py:48] [500] global_step=500, grad_norm=0.9684922695159912, loss=6.298405170440674
I0418 06:56:20.385414 140462702700288 logging_writer.py:48] [600] global_step=600, grad_norm=1.2450449466705322, loss=6.132909297943115
I0418 06:56:54.228603 140462467835648 logging_writer.py:48] [700] global_step=700, grad_norm=3.1323347091674805, loss=6.044245719909668
I0418 06:57:27.967244 140462702700288 logging_writer.py:48] [800] global_step=800, grad_norm=3.66567325592041, loss=5.942067623138428
I0418 06:58:01.932644 140462467835648 logging_writer.py:48] [900] global_step=900, grad_norm=2.3072855472564697, loss=5.87131404876709
I0418 06:58:35.805006 140462702700288 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.1044209003448486, loss=5.678201675415039
I0418 06:59:09.874242 140462467835648 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.2592296600341797, loss=5.573094844818115
I0418 06:59:43.756664 140462702700288 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.3070363998413086, loss=5.489081382751465
I0418 07:00:17.616620 140462467835648 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.7135257720947266, loss=5.479437828063965
I0418 07:00:51.424090 140462702700288 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.3148529529571533, loss=5.324373722076416
I0418 07:01:25.452272 140462467835648 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.735117197036743, loss=5.3039655685424805
I0418 07:01:26.885543 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:01:33.482323 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:01:41.131948 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:01:43.254713 140667534567232 submission_runner.py:406] Time since start: 615.51s, 	Step: 1506, 	{'train/accuracy': 0.13576211035251617, 'train/loss': 4.579771518707275, 'validation/accuracy': 0.1198199987411499, 'validation/loss': 4.699863910675049, 'validation/num_examples': 50000, 'test/accuracy': 0.09190000593662262, 'test/loss': 5.0460686683654785, 'test/num_examples': 10000, 'score': 558.4132807254791, 'total_duration': 615.5079982280731, 'accumulated_submission_time': 558.4132807254791, 'accumulated_eval_time': 56.264535427093506, 'accumulated_logging_time': 0.8105435371398926}
I0418 07:01:43.262318 140462719485696 logging_writer.py:48] [1506] accumulated_eval_time=56.264535, accumulated_logging_time=0.810544, accumulated_submission_time=558.413281, global_step=1506, preemption_count=0, score=558.413281, test/accuracy=0.091900, test/loss=5.046069, test/num_examples=10000, total_duration=615.507998, train/accuracy=0.135762, train/loss=4.579772, validation/accuracy=0.119820, validation/loss=4.699864, validation/num_examples=50000
I0418 07:01:43.437800 140667534567232 checkpoints.py:356] Saving checkpoint at step: 1506
I0418 07:01:44.051743 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1506
I0418 07:01:44.052590 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1506.
I0418 07:02:16.452682 140462727878400 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.350670337677002, loss=5.139887809753418
I0418 07:02:50.295897 140490309621504 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.0867562294006348, loss=5.118339538574219
I0418 07:03:24.458950 140462727878400 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.951922416687012, loss=4.981532573699951
I0418 07:03:58.523641 140490309621504 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.762892484664917, loss=4.907573699951172
I0418 07:04:32.505901 140462727878400 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.736556529998779, loss=4.936758995056152
I0418 07:05:06.357894 140490309621504 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.5356125831604, loss=4.874063491821289
I0418 07:05:40.442251 140462727878400 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.973285675048828, loss=4.766516208648682
I0418 07:06:14.486703 140490309621504 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.06198787689209, loss=4.658626079559326
I0418 07:06:48.635550 140462727878400 logging_writer.py:48] [2400] global_step=2400, grad_norm=6.108870029449463, loss=4.616745948791504
I0418 07:07:22.739313 140490309621504 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.06492280960083, loss=4.516534328460693
I0418 07:07:56.653007 140462727878400 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.099061489105225, loss=4.530571937561035
I0418 07:08:30.729650 140490309621504 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.150635242462158, loss=4.444026470184326
I0418 07:09:04.789521 140462727878400 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.4333155155181885, loss=4.400724411010742
I0418 07:09:38.820297 140490309621504 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.376587152481079, loss=4.434764862060547
I0418 07:10:12.850123 140462727878400 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.910460472106934, loss=4.232824325561523
I0418 07:10:14.282124 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:10:20.879440 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:10:28.661452 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:10:30.822584 140667534567232 submission_runner.py:406] Time since start: 1143.08s, 	Step: 3006, 	{'train/accuracy': 0.30090081691741943, 'train/loss': 3.381056785583496, 'validation/accuracy': 0.2691600024700165, 'validation/loss': 3.5463645458221436, 'validation/num_examples': 50000, 'test/accuracy': 0.20650000870227814, 'test/loss': 4.0554585456848145, 'test/num_examples': 10000, 'score': 1068.6198675632477, 'total_duration': 1143.0758271217346, 'accumulated_submission_time': 1068.6198675632477, 'accumulated_eval_time': 72.8049156665802, 'accumulated_logging_time': 1.6123292446136475}
I0418 07:10:30.831560 140490309621504 logging_writer.py:48] [3006] accumulated_eval_time=72.804916, accumulated_logging_time=1.612329, accumulated_submission_time=1068.619868, global_step=3006, preemption_count=0, score=1068.619868, test/accuracy=0.206500, test/loss=4.055459, test/num_examples=10000, total_duration=1143.075827, train/accuracy=0.300901, train/loss=3.381057, validation/accuracy=0.269160, validation/loss=3.546365, validation/num_examples=50000
I0418 07:10:31.065239 140667534567232 checkpoints.py:356] Saving checkpoint at step: 3006
I0418 07:10:31.921238 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_3006
I0418 07:10:31.922441 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_3006.
I0418 07:11:04.198934 140462727878400 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5187973976135254, loss=4.266934871673584
I0418 07:11:38.282406 140490276050688 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.5499186515808105, loss=4.155706405639648
I0418 07:12:12.313804 140462727878400 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.736570358276367, loss=4.19596004486084
I0418 07:12:46.342577 140490276050688 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.3735454082489014, loss=4.054039478302002
I0418 07:13:20.421758 140462727878400 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.99414587020874, loss=4.115898132324219
I0418 07:13:54.380766 140490276050688 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.912665367126465, loss=4.082711219787598
I0418 07:14:28.364782 140462727878400 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.229842662811279, loss=3.990262031555176
I0418 07:15:02.335393 140490276050688 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.627588272094727, loss=3.975213050842285
I0418 07:15:36.165258 140462727878400 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.928424119949341, loss=3.8795549869537354
I0418 07:16:10.151813 140490276050688 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.2067108154296875, loss=3.936591148376465
I0418 07:16:44.024875 140462727878400 logging_writer.py:48] [4100] global_step=4100, grad_norm=5.180722713470459, loss=3.8983120918273926
I0418 07:17:17.912068 140462727878400 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.199349880218506, loss=3.8375110626220703
I0418 07:17:52.006997 140490276050688 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.9137496948242188, loss=3.7761783599853516
I0418 07:18:25.861429 140462727878400 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.6546273231506348, loss=3.701687812805176
I0418 07:18:59.774955 140490276050688 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.8378734588623047, loss=3.8123650550842285
I0418 07:19:01.964325 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:19:08.674903 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:19:16.322057 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:19:18.442295 140667534567232 submission_runner.py:406] Time since start: 1670.70s, 	Step: 4508, 	{'train/accuracy': 0.42434629797935486, 'train/loss': 2.6144611835479736, 'validation/accuracy': 0.3957599997520447, 'validation/loss': 2.7852745056152344, 'validation/num_examples': 50000, 'test/accuracy': 0.29330000281333923, 'test/loss': 3.4409170150756836, 'test/num_examples': 10000, 'score': 1578.6377086639404, 'total_duration': 1670.6955924034119, 'accumulated_submission_time': 1578.6377086639404, 'accumulated_eval_time': 89.2828619480133, 'accumulated_logging_time': 2.716933012008667}
I0418 07:19:18.449710 140462727878400 logging_writer.py:48] [4508] accumulated_eval_time=89.282862, accumulated_logging_time=2.716933, accumulated_submission_time=1578.637709, global_step=4508, preemption_count=0, score=1578.637709, test/accuracy=0.293300, test/loss=3.440917, test/num_examples=10000, total_duration=1670.695592, train/accuracy=0.424346, train/loss=2.614461, validation/accuracy=0.395760, validation/loss=2.785275, validation/num_examples=50000
I0418 07:19:18.632649 140667534567232 checkpoints.py:356] Saving checkpoint at step: 4508
I0418 07:19:19.233774 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_4508
I0418 07:19:19.234621 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_4508.
I0418 07:19:50.877661 140490276050688 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.145294666290283, loss=3.5687379837036133
I0418 07:20:24.864021 140490267657984 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.548682928085327, loss=3.671855926513672
I0418 07:20:58.744512 140490276050688 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.3787686824798584, loss=3.608717918395996
I0418 07:21:32.708306 140490267657984 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.9349071979522705, loss=3.5869851112365723
I0418 07:22:06.627104 140490276050688 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.568423271179199, loss=3.5937938690185547
I0418 07:22:40.538736 140490267657984 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.5605173110961914, loss=3.5783793926239014
I0418 07:23:14.441990 140490276050688 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.332810878753662, loss=3.498723268508911
I0418 07:23:48.364204 140490267657984 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.7154834270477295, loss=3.5420775413513184
I0418 07:24:22.332041 140490276050688 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.4720866680145264, loss=3.6221566200256348
I0418 07:24:56.344910 140490267657984 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.5335614681243896, loss=3.5289084911346436
I0418 07:25:30.273907 140490276050688 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.7036855220794678, loss=3.5374982357025146
I0418 07:26:04.294040 140490267657984 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.6890645027160645, loss=3.422415018081665
I0418 07:26:38.205431 140490276050688 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.1464879512786865, loss=3.485729694366455
I0418 07:27:12.030987 140490267657984 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.4282045364379883, loss=3.403193712234497
I0418 07:27:45.767512 140490276050688 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.5468175411224365, loss=3.42834210395813
I0418 07:27:49.569158 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:27:56.163124 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:28:03.961084 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:28:06.082423 140667534567232 submission_runner.py:406] Time since start: 2198.34s, 	Step: 6013, 	{'train/accuracy': 0.48036909103393555, 'train/loss': 2.3798084259033203, 'validation/accuracy': 0.4461999833583832, 'validation/loss': 2.5537285804748535, 'validation/num_examples': 50000, 'test/accuracy': 0.34040001034736633, 'test/loss': 3.2112255096435547, 'test/num_examples': 10000, 'score': 2088.9492032527924, 'total_duration': 2198.33572101593, 'accumulated_submission_time': 2088.9492032527924, 'accumulated_eval_time': 105.79610276222229, 'accumulated_logging_time': 3.513601303100586}
I0418 07:28:06.090228 140490267657984 logging_writer.py:48] [6013] accumulated_eval_time=105.796103, accumulated_logging_time=3.513601, accumulated_submission_time=2088.949203, global_step=6013, preemption_count=0, score=2088.949203, test/accuracy=0.340400, test/loss=3.211226, test/num_examples=10000, total_duration=2198.335721, train/accuracy=0.480369, train/loss=2.379808, validation/accuracy=0.446200, validation/loss=2.553729, validation/num_examples=50000
I0418 07:28:06.272454 140667534567232 checkpoints.py:356] Saving checkpoint at step: 6013
I0418 07:28:06.885988 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_6013
I0418 07:28:06.886942 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_6013.
I0418 07:28:36.617782 140490276050688 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.219909429550171, loss=3.4257078170776367
I0418 07:29:10.317074 140490250872576 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.3167827129364014, loss=3.417558193206787
I0418 07:29:44.017905 140490276050688 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.338536262512207, loss=3.423367500305176
I0418 07:30:17.748263 140490250872576 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.0858066082000732, loss=3.436530113220215
I0418 07:30:51.395639 140490276050688 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.731623649597168, loss=3.3633029460906982
I0418 07:31:25.248519 140490250872576 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.15728497505188, loss=3.2958052158355713
I0418 07:31:59.150872 140490276050688 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.5432136058807373, loss=3.2747857570648193
I0418 07:32:33.019360 140490250872576 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.0805749893188477, loss=3.3963868618011475
I0418 07:33:06.808234 140490276050688 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.0997073650360107, loss=3.226719617843628
I0418 07:33:40.537659 140490250872576 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.5303430557250977, loss=3.2734344005584717
I0418 07:34:14.305931 140490276050688 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.1385879516601562, loss=3.1201272010803223
I0418 07:34:48.007978 140490250872576 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.793996810913086, loss=3.1711645126342773
I0418 07:35:21.754837 140490276050688 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.9247609376907349, loss=3.2927305698394775
I0418 07:35:55.462733 140490250872576 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.293156623840332, loss=3.2182705402374268
I0418 07:36:29.240603 140490276050688 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2129647731781006, loss=3.2386178970336914
I0418 07:36:37.121919 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:36:43.678596 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:36:51.366816 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:36:53.250427 140667534567232 submission_runner.py:406] Time since start: 2725.50s, 	Step: 7525, 	{'train/accuracy': 0.5518773794174194, 'train/loss': 2.000563859939575, 'validation/accuracy': 0.5174800157546997, 'validation/loss': 2.172894239425659, 'validation/num_examples': 50000, 'test/accuracy': 0.39560002088546753, 'test/loss': 2.8650150299072266, 'test/num_examples': 10000, 'score': 2599.1635286808014, 'total_duration': 2725.5037174224854, 'accumulated_submission_time': 2599.1635286808014, 'accumulated_eval_time': 121.92457365989685, 'accumulated_logging_time': 4.320331573486328}
I0418 07:36:53.258171 140490250872576 logging_writer.py:48] [7525] accumulated_eval_time=121.924574, accumulated_logging_time=4.320332, accumulated_submission_time=2599.163529, global_step=7525, preemption_count=0, score=2599.163529, test/accuracy=0.395600, test/loss=2.865015, test/num_examples=10000, total_duration=2725.503717, train/accuracy=0.551877, train/loss=2.000564, validation/accuracy=0.517480, validation/loss=2.172894, validation/num_examples=50000
I0418 07:36:53.435401 140667534567232 checkpoints.py:356] Saving checkpoint at step: 7525
I0418 07:36:54.356260 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_7525
I0418 07:36:54.370128 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_7525.
I0418 07:37:20.176789 140490276050688 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.4914863109588623, loss=3.2179970741271973
I0418 07:37:54.089999 140490242479872 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.178171157836914, loss=3.206080675125122
I0418 07:38:27.892584 140490276050688 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.9403743743896484, loss=3.199139356613159
I0418 07:39:01.745611 140490242479872 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.857430338859558, loss=3.2714524269104004
I0418 07:39:35.609212 140490276050688 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.6985644102096558, loss=3.126601457595825
I0418 07:40:09.613989 140490242479872 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.2017006874084473, loss=3.1541762351989746
I0418 07:40:43.634554 140490276050688 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.0250604152679443, loss=3.1415977478027344
I0418 07:41:17.491668 140490242479872 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8307418823242188, loss=3.200404644012451
I0418 07:41:51.295586 140490276050688 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.2540104389190674, loss=3.1306324005126953
I0418 07:42:25.331576 140490242479872 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.6442052125930786, loss=3.1746325492858887
I0418 07:42:59.252616 140490276050688 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.6002697944641113, loss=3.1050355434417725
I0418 07:43:33.286624 140490242479872 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.385774850845337, loss=3.1216626167297363
I0418 07:44:07.350317 140490276050688 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.178203582763672, loss=3.1784205436706543
I0418 07:44:41.329591 140490242479872 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.3909550905227661, loss=2.983929395675659
I0418 07:45:15.182512 140490276050688 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.208291530609131, loss=3.0980658531188965
I0418 07:45:24.390402 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:45:30.981580 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:45:38.868565 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:45:40.750264 140667534567232 submission_runner.py:406] Time since start: 3253.00s, 	Step: 9029, 	{'train/accuracy': 0.5920360088348389, 'train/loss': 1.8293614387512207, 'validation/accuracy': 0.5486999750137329, 'validation/loss': 2.0303893089294434, 'validation/num_examples': 50000, 'test/accuracy': 0.42640000581741333, 'test/loss': 2.7096686363220215, 'test/num_examples': 10000, 'score': 3109.15859746933, 'total_duration': 3253.003559112549, 'accumulated_submission_time': 3109.15859746933, 'accumulated_eval_time': 138.28440928459167, 'accumulated_logging_time': 5.447415113449097}
I0418 07:45:44.339000 140490242479872 logging_writer.py:48] [9029] accumulated_eval_time=138.284409, accumulated_logging_time=5.447415, accumulated_submission_time=3109.158597, global_step=9029, preemption_count=0, score=3109.158597, test/accuracy=0.426400, test/loss=2.709669, test/num_examples=10000, total_duration=3253.003559, train/accuracy=0.592036, train/loss=1.829361, validation/accuracy=0.548700, validation/loss=2.030389, validation/num_examples=50000
I0418 07:45:44.622852 140667534567232 checkpoints.py:356] Saving checkpoint at step: 9029
I0418 07:45:45.259233 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_9029
I0418 07:45:45.260466 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_9029.
I0418 07:46:09.693278 140490276050688 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.861029028892517, loss=3.061450958251953
I0418 07:46:43.478072 140490234087168 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.2240993976593018, loss=3.1050407886505127
I0418 07:47:17.489417 140490276050688 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.740574836730957, loss=3.030292510986328
I0418 07:47:51.287438 140490234087168 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.058851718902588, loss=3.0210206508636475
I0418 07:48:25.134052 140490276050688 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.8141201734542847, loss=3.061230182647705
I0418 07:48:59.056657 140490234087168 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.43663489818573, loss=3.0769364833831787
I0418 07:49:33.032299 140490276050688 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.768216609954834, loss=2.926513910293579
I0418 07:50:07.034806 140490234087168 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.003169536590576, loss=3.0312728881835938
I0418 07:50:41.019467 140490276050688 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.7635008096694946, loss=3.027924060821533
I0418 07:51:15.047507 140490234087168 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.6841870546340942, loss=2.9671225547790527
I0418 07:51:48.684625 140490276050688 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.5474358797073364, loss=2.946005344390869
I0418 07:52:22.467527 140490234087168 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.4159104824066162, loss=2.949974298477173
I0418 07:52:56.492198 140490276050688 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.3684598207473755, loss=3.007694721221924
I0418 07:53:30.386235 140490234087168 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.3595243692398071, loss=2.783507823944092
I0418 07:54:04.268960 140490276050688 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.1006271839141846, loss=2.9342191219329834
I0418 07:54:15.505979 140667534567232 spec.py:298] Evaluating on the training split.
I0418 07:54:22.100311 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 07:54:30.532764 140667534567232 spec.py:326] Evaluating on the test split.
I0418 07:54:32.633727 140667534567232 submission_runner.py:406] Time since start: 3784.89s, 	Step: 10535, 	{'train/accuracy': 0.6546755433082581, 'train/loss': 1.5315134525299072, 'validation/accuracy': 0.58024001121521, 'validation/loss': 1.8964067697525024, 'validation/num_examples': 50000, 'test/accuracy': 0.44670000672340393, 'test/loss': 2.6032543182373047, 'test/num_examples': 10000, 'score': 3619.3847959041595, 'total_duration': 3784.88702917099, 'accumulated_submission_time': 3619.3847959041595, 'accumulated_eval_time': 155.4121344089508, 'accumulated_logging_time': 9.958916902542114}
I0418 07:54:32.641749 140490234087168 logging_writer.py:48] [10535] accumulated_eval_time=155.412134, accumulated_logging_time=9.958917, accumulated_submission_time=3619.384796, global_step=10535, preemption_count=0, score=3619.384796, test/accuracy=0.446700, test/loss=2.603254, test/num_examples=10000, total_duration=3784.887029, train/accuracy=0.654676, train/loss=1.531513, validation/accuracy=0.580240, validation/loss=1.896407, validation/num_examples=50000
I0418 07:54:32.872956 140667534567232 checkpoints.py:356] Saving checkpoint at step: 10535
I0418 07:54:33.782883 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_10535
I0418 07:54:33.795643 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_10535.
I0418 07:54:56.154474 140490276050688 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.7806017398834229, loss=2.918494462966919
I0418 07:55:29.958504 140490225694464 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.8640307188034058, loss=2.945233106613159
I0418 07:56:03.778369 140490276050688 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.7689766883850098, loss=2.971238613128662
I0418 07:56:37.674106 140490225694464 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.3465644121170044, loss=2.9477479457855225
I0418 07:57:11.534975 140490276050688 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.5005947351455688, loss=2.842866897583008
I0418 07:57:45.384551 140490225694464 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.412066102027893, loss=2.927706003189087
I0418 07:58:19.091111 140490276050688 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.151949405670166, loss=2.8770713806152344
I0418 07:58:52.906350 140490225694464 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.7032968997955322, loss=2.8203673362731934
I0418 07:59:26.729529 140490276050688 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0527230501174927, loss=2.8272182941436768
I0418 08:00:00.570683 140490225694464 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.7289328575134277, loss=2.8864293098449707
I0418 08:00:34.145350 140490276050688 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.4942272901535034, loss=2.74529767036438
I0418 08:01:07.988795 140490225694464 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.1622189283370972, loss=2.7808384895324707
I0418 08:01:41.848501 140490276050688 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.3693326711654663, loss=2.7845754623413086
I0418 08:02:15.632151 140490225694464 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.5141639709472656, loss=2.7605111598968506
I0418 08:02:49.350701 140490276050688 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.3899909257888794, loss=2.8174216747283936
I0418 08:03:03.990439 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:03:10.738190 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:03:20.420352 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:03:22.542490 140667534567232 submission_runner.py:406] Time since start: 4314.79s, 	Step: 12045, 	{'train/accuracy': 0.6685466766357422, 'train/loss': 1.4761199951171875, 'validation/accuracy': 0.6002599596977234, 'validation/loss': 1.787896990776062, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.4780313968658447, 'test/num_examples': 10000, 'score': 4129.55771946907, 'total_duration': 4314.79471039772, 'accumulated_submission_time': 4129.55771946907, 'accumulated_eval_time': 173.96310377120972, 'accumulated_logging_time': 11.124778747558594}
I0418 08:03:22.557484 140490225694464 logging_writer.py:48] [12045] accumulated_eval_time=173.963104, accumulated_logging_time=11.124779, accumulated_submission_time=4129.557719, global_step=12045, preemption_count=0, score=4129.557719, test/accuracy=0.468000, test/loss=2.478031, test/num_examples=10000, total_duration=4314.794710, train/accuracy=0.668547, train/loss=1.476120, validation/accuracy=0.600260, validation/loss=1.787897, validation/num_examples=50000
I0418 08:03:22.806316 140667534567232 checkpoints.py:356] Saving checkpoint at step: 12045
I0418 08:03:23.762881 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_12045
I0418 08:03:23.777569 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_12045.
I0418 08:03:42.837172 140490276050688 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1590155363082886, loss=2.790130853652954
I0418 08:04:16.754660 140490217301760 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.4076075553894043, loss=2.8502140045166016
I0418 08:04:50.625473 140490276050688 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.1329917907714844, loss=2.7622904777526855
I0418 08:05:24.308230 140490217301760 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.6447490453720093, loss=2.9009850025177
I0418 08:05:58.273537 140490276050688 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.2118381261825562, loss=2.7339847087860107
I0418 08:06:32.087963 140490217301760 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.8211945295333862, loss=2.7957377433776855
I0418 08:07:05.761185 140490276050688 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.2511862516403198, loss=2.814872980117798
I0418 08:07:39.411735 140490217301760 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.4479669332504272, loss=2.7979960441589355
I0418 08:08:13.206634 140490276050688 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.3257331848144531, loss=2.7488951683044434
I0418 08:08:47.095148 140490217301760 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.1717321872711182, loss=2.8183014392852783
I0418 08:09:21.015670 140490276050688 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.2573366165161133, loss=2.690009117126465
I0418 08:09:54.710998 140490217301760 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4262535572052002, loss=2.737475633621216
I0418 08:10:28.488095 140490276050688 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.5822962522506714, loss=2.711662530899048
I0418 08:11:02.195888 140490217301760 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.1466596126556396, loss=2.819722890853882
I0418 08:11:36.233892 140490276050688 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3846443891525269, loss=2.701244354248047
I0418 08:11:53.921559 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:12:01.165195 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:12:10.963560 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:12:12.852301 140667534567232 submission_runner.py:406] Time since start: 4845.10s, 	Step: 13554, 	{'train/accuracy': 0.6848094463348389, 'train/loss': 1.3774724006652832, 'validation/accuracy': 0.6178399920463562, 'validation/loss': 1.686216950416565, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.3747177124023438, 'test/num_examples': 10000, 'score': 4639.675585985184, 'total_duration': 4845.104496955872, 'accumulated_submission_time': 4639.675585985184, 'accumulated_eval_time': 192.8927445411682, 'accumulated_logging_time': 12.36813235282898}
I0418 08:12:12.860695 140490217301760 logging_writer.py:48] [13554] accumulated_eval_time=192.892745, accumulated_logging_time=12.368132, accumulated_submission_time=4639.675586, global_step=13554, preemption_count=0, score=4639.675586, test/accuracy=0.498800, test/loss=2.374718, test/num_examples=10000, total_duration=4845.104497, train/accuracy=0.684809, train/loss=1.377472, validation/accuracy=0.617840, validation/loss=1.686217, validation/num_examples=50000
I0418 08:12:13.071111 140667534567232 checkpoints.py:356] Saving checkpoint at step: 13554
I0418 08:12:13.965247 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_13554
I0418 08:12:13.978008 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_13554.
I0418 08:12:29.877395 140490276050688 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.5016727447509766, loss=2.8161754608154297
I0418 08:13:03.743096 140490208909056 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.053322672843933, loss=2.7671351432800293
I0418 08:13:37.471681 140490276050688 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9688584804534912, loss=2.635742425918579
I0418 08:14:11.229570 140490208909056 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.2932018041610718, loss=2.7591774463653564
I0418 08:14:45.085053 140490276050688 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.2476736307144165, loss=2.6989905834198
I0418 08:15:18.989424 140490208909056 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.2974988222122192, loss=2.7162575721740723
I0418 08:15:52.899285 140490276050688 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0642927885055542, loss=2.7233660221099854
I0418 08:16:26.864482 140490208909056 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.20180082321167, loss=2.7076027393341064
I0418 08:17:00.617701 140490276050688 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.1866002082824707, loss=2.688156843185425
I0418 08:17:34.649634 140490208909056 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6441841125488281, loss=2.7896170616149902
I0418 08:18:08.434363 140490276050688 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.3876349925994873, loss=2.739522933959961
I0418 08:18:42.344889 140490208909056 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.3465466499328613, loss=2.641692638397217
I0418 08:19:15.994976 140490276050688 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0853928327560425, loss=2.585428237915039
I0418 08:19:49.935858 140490208909056 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0621076822280884, loss=2.7177438735961914
I0418 08:20:23.895488 140490276050688 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8644626140594482, loss=2.7305030822753906
I0418 08:20:44.215668 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:20:51.364385 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:21:01.115894 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:21:03.008928 140667534567232 submission_runner.py:406] Time since start: 5375.26s, 	Step: 15062, 	{'train/accuracy': 0.6949737071990967, 'train/loss': 1.338767170906067, 'validation/accuracy': 0.6303399801254272, 'validation/loss': 1.6399836540222168, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.329158306121826, 'test/num_examples': 10000, 'score': 5149.891433954239, 'total_duration': 5375.261265993118, 'accumulated_submission_time': 5149.891433954239, 'accumulated_eval_time': 211.6850152015686, 'accumulated_logging_time': 13.497699975967407}
I0418 08:21:03.023414 140490208909056 logging_writer.py:48] [15062] accumulated_eval_time=211.685015, accumulated_logging_time=13.497700, accumulated_submission_time=5149.891434, global_step=15062, preemption_count=0, score=5149.891434, test/accuracy=0.501800, test/loss=2.329158, test/num_examples=10000, total_duration=5375.261266, train/accuracy=0.694974, train/loss=1.338767, validation/accuracy=0.630340, validation/loss=1.639984, validation/num_examples=50000
I0418 08:21:03.265367 140667534567232 checkpoints.py:356] Saving checkpoint at step: 15062
I0418 08:21:04.224536 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_15062
I0418 08:21:04.238731 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_15062.
I0418 08:21:17.530880 140490276050688 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.1328880786895752, loss=2.6755189895629883
I0418 08:21:51.503693 140489856644864 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1188279390335083, loss=2.700208902359009
I0418 08:22:25.314981 140490276050688 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.2511231899261475, loss=2.7094333171844482
I0418 08:22:59.100530 140489856644864 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.2864511013031006, loss=2.7291579246520996
I0418 08:23:33.147954 140490276050688 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.1095799207687378, loss=2.5869765281677246
I0418 08:24:07.178896 140489856644864 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.1030386686325073, loss=2.6800336837768555
I0418 08:24:41.001498 140490276050688 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.2637910842895508, loss=2.68115234375
I0418 08:25:14.894760 140489856644864 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.2329529523849487, loss=2.6181488037109375
I0418 08:25:48.737651 140490276050688 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.9526444673538208, loss=2.6478629112243652
I0418 08:26:22.559310 140489856644864 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.1657147407531738, loss=2.59757924079895
I0418 08:26:56.627557 140490276050688 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.228238821029663, loss=2.6335716247558594
I0418 08:27:30.561501 140489856644864 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7858680486679077, loss=2.6273200511932373
I0418 08:28:04.439640 140490276050688 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.1928027868270874, loss=2.6585612297058105
I0418 08:28:38.189327 140489856644864 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.196794867515564, loss=2.583122491836548
I0418 08:29:12.298118 140490276050688 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.373293161392212, loss=2.5686593055725098
I0418 08:29:34.498633 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:29:41.799093 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:29:51.786899 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:29:53.665159 140667534567232 submission_runner.py:406] Time since start: 5905.92s, 	Step: 16567, 	{'train/accuracy': 0.7135084271430969, 'train/loss': 1.2548784017562866, 'validation/accuracy': 0.6442199945449829, 'validation/loss': 1.5712519884109497, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.263230323791504, 'test/num_examples': 10000, 'score': 5660.128768920898, 'total_duration': 5905.917297840118, 'accumulated_submission_time': 5660.128768920898, 'accumulated_eval_time': 230.8503658771515, 'accumulated_logging_time': 14.732025623321533}
I0418 08:29:53.675268 140489856644864 logging_writer.py:48] [16567] accumulated_eval_time=230.850366, accumulated_logging_time=14.732026, accumulated_submission_time=5660.128769, global_step=16567, preemption_count=0, score=5660.128769, test/accuracy=0.511800, test/loss=2.263230, test/num_examples=10000, total_duration=5905.917298, train/accuracy=0.713508, train/loss=1.254878, validation/accuracy=0.644220, validation/loss=1.571252, validation/num_examples=50000
I0418 08:29:53.997102 140667534567232 checkpoints.py:356] Saving checkpoint at step: 16567
I0418 08:29:55.191271 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_16567
I0418 08:29:55.210692 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_16567.
I0418 08:30:06.674206 140490276050688 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0034099817276, loss=2.6106679439544678
I0418 08:30:40.305051 140488380249856 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9028410315513611, loss=2.5911269187927246
I0418 08:31:14.171711 140490276050688 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.965608537197113, loss=2.5127203464508057
I0418 08:31:47.990693 140488380249856 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.2729922533035278, loss=2.5651233196258545
I0418 08:32:21.669296 140490276050688 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7148778438568115, loss=2.5406808853149414
I0418 08:32:55.615275 140488380249856 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.073165774345398, loss=2.677060604095459
I0418 08:33:29.427631 140490276050688 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.294143557548523, loss=2.677706718444824
I0418 08:34:03.176899 140488380249856 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0630967617034912, loss=2.6035618782043457
I0418 08:34:36.916341 140490276050688 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.0228168964385986, loss=2.5487542152404785
I0418 08:35:10.747751 140488380249856 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.779593825340271, loss=2.5293936729431152
I0418 08:35:44.572136 140490276050688 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.2743881940841675, loss=2.605797052383423
I0418 08:36:18.586558 140488380249856 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8854478001594543, loss=2.580225944519043
I0418 08:36:52.538903 140490276050688 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7985816597938538, loss=2.548827886581421
I0418 08:37:26.395298 140488380249856 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9661773443222046, loss=2.578594923019409
I0418 08:38:00.138731 140490276050688 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7462654113769531, loss=2.508213996887207
I0418 08:38:25.226449 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:38:32.502130 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:38:42.578562 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:38:44.629894 140667534567232 submission_runner.py:406] Time since start: 6436.88s, 	Step: 18076, 	{'train/accuracy': 0.7252271771430969, 'train/loss': 1.1911572217941284, 'validation/accuracy': 0.6494399905204773, 'validation/loss': 1.5245273113250732, 'validation/num_examples': 50000, 'test/accuracy': 0.5240000486373901, 'test/loss': 2.2242636680603027, 'test/num_examples': 10000, 'score': 6170.117515087128, 'total_duration': 6436.88224697113, 'accumulated_submission_time': 6170.117515087128, 'accumulated_eval_time': 250.25283217430115, 'accumulated_logging_time': 16.28660225868225}
I0418 08:38:44.644050 140488380249856 logging_writer.py:48] [18076] accumulated_eval_time=250.252832, accumulated_logging_time=16.286602, accumulated_submission_time=6170.117515, global_step=18076, preemption_count=0, score=6170.117515, test/accuracy=0.524000, test/loss=2.224264, test/num_examples=10000, total_duration=6436.882247, train/accuracy=0.725227, train/loss=1.191157, validation/accuracy=0.649440, validation/loss=1.524527, validation/num_examples=50000
I0418 08:38:44.892058 140667534567232 checkpoints.py:356] Saving checkpoint at step: 18076
I0418 08:38:45.838103 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_18076
I0418 08:38:45.853414 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_18076.
I0418 08:38:54.313194 140490276050688 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7738311886787415, loss=2.5303685665130615
I0418 08:39:28.134987 140486702528256 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8982072472572327, loss=2.516577959060669
I0418 08:40:01.973761 140490276050688 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9330933094024658, loss=2.555391311645508
I0418 08:40:35.821608 140486702528256 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8292798399925232, loss=2.551992177963257
I0418 08:41:09.655934 140490276050688 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8069469332695007, loss=2.552578926086426
I0418 08:41:43.380574 140486702528256 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.4802488088607788, loss=2.56528377532959
I0418 08:42:17.175723 140490276050688 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.1315371990203857, loss=2.5511629581451416
I0418 08:42:50.909782 140486702528256 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.1369436979293823, loss=2.534170150756836
I0418 08:43:24.697687 140490276050688 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.0197614431381226, loss=2.5334980487823486
I0418 08:43:58.408082 140486702528256 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8740689754486084, loss=2.5229458808898926
I0418 08:44:32.151857 140490276050688 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.190535545349121, loss=2.52839732170105
I0418 08:45:05.845072 140486702528256 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8453274965286255, loss=2.4264142513275146
I0418 08:45:39.674214 140490276050688 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.9439637660980225, loss=2.5852675437927246
I0418 08:46:13.310364 140486702528256 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0266270637512207, loss=2.5917768478393555
I0418 08:46:47.051297 140490276050688 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8347295522689819, loss=2.5558154582977295
I0418 08:47:15.875616 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:47:23.495797 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:47:33.624596 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:47:35.616408 140667534567232 submission_runner.py:406] Time since start: 6967.87s, 	Step: 19587, 	{'train/accuracy': 0.76566481590271, 'train/loss': 1.0317158699035645, 'validation/accuracy': 0.6578399538993835, 'validation/loss': 1.507950782775879, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.220456838607788, 'test/num_examples': 10000, 'score': 6680.117534637451, 'total_duration': 6967.868787527084, 'accumulated_submission_time': 6680.117534637451, 'accumulated_eval_time': 269.9926846027374, 'accumulated_logging_time': 17.514888048171997}
I0418 08:47:35.629868 140486702528256 logging_writer.py:48] [19587] accumulated_eval_time=269.992685, accumulated_logging_time=17.514888, accumulated_submission_time=6680.117535, global_step=19587, preemption_count=0, score=6680.117535, test/accuracy=0.528600, test/loss=2.220457, test/num_examples=10000, total_duration=6967.868788, train/accuracy=0.765665, train/loss=1.031716, validation/accuracy=0.657840, validation/loss=1.507951, validation/num_examples=50000
I0418 08:47:35.893963 140667534567232 checkpoints.py:356] Saving checkpoint at step: 19587
I0418 08:47:36.841414 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_19587
I0418 08:47:36.858451 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_19587.
I0418 08:47:41.572932 140490276050688 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9884887933731079, loss=2.4801254272460938
I0418 08:48:15.461691 140486694135552 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9785193204879761, loss=2.475800037384033
I0418 08:48:49.339204 140490276050688 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7755730748176575, loss=2.513977527618408
I0418 08:49:23.237451 140486694135552 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6737425327301025, loss=2.49576735496521
I0418 08:49:57.094875 140490276050688 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.903031051158905, loss=2.51118803024292
I0418 08:50:31.037212 140486694135552 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8163363933563232, loss=2.402827501296997
I0418 08:51:04.840914 140490276050688 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.132861852645874, loss=2.448835849761963
I0418 08:51:38.772947 140486694135552 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7714047431945801, loss=2.579545259475708
I0418 08:52:12.694552 140490276050688 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.9096022248268127, loss=2.4992833137512207
I0418 08:52:46.691642 140486694135552 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8657436966896057, loss=2.535712718963623
I0418 08:53:20.598410 140490276050688 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.1218959093093872, loss=2.4973702430725098
I0418 08:53:54.454110 140486694135552 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.0422511100769043, loss=2.5353870391845703
I0418 08:54:28.380510 140490276050688 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7475226521492004, loss=2.4043898582458496
I0418 08:55:02.224410 140486694135552 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8852477073669434, loss=2.572368860244751
I0418 08:55:36.233460 140490276050688 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8759490847587585, loss=2.414611339569092
I0418 08:56:07.159855 140667534567232 spec.py:298] Evaluating on the training split.
I0418 08:56:14.913162 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 08:56:25.001147 140667534567232 spec.py:326] Evaluating on the test split.
I0418 08:56:27.124403 140667534567232 submission_runner.py:406] Time since start: 7499.38s, 	Step: 21093, 	{'train/accuracy': 0.7586694955825806, 'train/loss': 1.0575735569000244, 'validation/accuracy': 0.6634399890899658, 'validation/loss': 1.4816827774047852, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.176906108856201, 'test/num_examples': 10000, 'score': 7190.395640611649, 'total_duration': 7499.376618385315, 'accumulated_submission_time': 7190.395640611649, 'accumulated_eval_time': 289.95612812042236, 'accumulated_logging_time': 18.762418031692505}
I0418 08:56:27.133552 140486694135552 logging_writer.py:48] [21093] accumulated_eval_time=289.956128, accumulated_logging_time=18.762418, accumulated_submission_time=7190.395641, global_step=21093, preemption_count=0, score=7190.395641, test/accuracy=0.530000, test/loss=2.176906, test/num_examples=10000, total_duration=7499.376618, train/accuracy=0.758669, train/loss=1.057574, validation/accuracy=0.663440, validation/loss=1.481683, validation/num_examples=50000
I0418 08:56:27.391224 140667534567232 checkpoints.py:356] Saving checkpoint at step: 21093
I0418 08:56:28.314224 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_21093
I0418 08:56:28.328825 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_21093.
I0418 08:56:31.116050 140490276050688 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8589997291564941, loss=2.4935972690582275
I0418 08:57:04.961440 140486685742848 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5578095316886902, loss=2.4408864974975586
I0418 08:57:38.898868 140490276050688 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8724396228790283, loss=2.4659972190856934
I0418 08:58:12.708888 140486685742848 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7077388763427734, loss=2.4207117557525635
I0418 08:58:46.843367 140490276050688 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7490378022193909, loss=2.443851947784424
I0418 08:59:20.762473 140486685742848 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.604972243309021, loss=2.418722152709961
I0418 08:59:54.481486 140490276050688 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7401285171508789, loss=2.4757239818573
I0418 09:00:28.334554 140486685742848 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8234935402870178, loss=2.5070505142211914
I0418 09:01:02.154557 140490276050688 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.1012988090515137, loss=2.5500993728637695
I0418 09:01:36.149933 140486685742848 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9202694892883301, loss=2.416492462158203
I0418 09:02:10.060152 140490276050688 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7191222310066223, loss=2.4478540420532227
I0418 09:02:43.875015 140486685742848 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7605273723602295, loss=2.539116382598877
I0418 09:03:17.759274 140490276050688 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8218101859092712, loss=2.383733034133911
I0418 09:03:51.822309 140486685742848 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7307441830635071, loss=2.4995408058166504
I0418 09:04:25.857551 140490276050688 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.662473201751709, loss=2.368241548538208
I0418 09:04:58.625957 140667534567232 spec.py:298] Evaluating on the training split.
I0418 09:05:06.036650 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 09:05:16.034014 140667534567232 spec.py:326] Evaluating on the test split.
I0418 09:05:17.833885 140667534567232 submission_runner.py:406] Time since start: 8030.09s, 	Step: 22598, 	{'train/accuracy': 0.7564970850944519, 'train/loss': 1.0595335960388184, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.4604707956314087, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.152249574661255, 'test/num_examples': 10000, 'score': 7700.6712038517, 'total_duration': 8030.086052894592, 'accumulated_submission_time': 7700.6712038517, 'accumulated_eval_time': 309.1629161834717, 'accumulated_logging_time': 19.970874786376953}
I0418 09:05:17.849021 140486685742848 logging_writer.py:48] [22598] accumulated_eval_time=309.162916, accumulated_logging_time=19.970875, accumulated_submission_time=7700.671204, global_step=22598, preemption_count=0, score=7700.671204, test/accuracy=0.534700, test/loss=2.152250, test/num_examples=10000, total_duration=8030.086053, train/accuracy=0.756497, train/loss=1.059534, validation/accuracy=0.669760, validation/loss=1.460471, validation/num_examples=50000
I0418 09:05:18.124017 140667534567232 checkpoints.py:356] Saving checkpoint at step: 22598
I0418 09:05:19.104723 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_22598
I0418 09:05:19.122572 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_22598.
I0418 09:05:20.137869 140490276050688 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7413849830627441, loss=2.363349199295044
I0418 09:05:53.930722 140486677350144 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9306592345237732, loss=2.453947067260742
I0418 09:06:27.715552 140490276050688 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8771064281463623, loss=2.4466869831085205
I0418 09:07:01.346012 140486677350144 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7006931304931641, loss=2.422105550765991
I0418 09:07:35.184287 140490276050688 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7186415195465088, loss=2.4182848930358887
I0418 09:08:09.055360 140486677350144 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.8351179361343384, loss=2.388242721557617
I0418 09:08:42.748960 140490276050688 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8598491549491882, loss=2.551799774169922
I0418 09:09:16.574562 140486677350144 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6577072143554688, loss=2.4678778648376465
I0418 09:09:50.379658 140490276050688 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6858758330345154, loss=2.3783297538757324
I0418 09:10:24.063965 140486677350144 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9095235466957092, loss=2.4216136932373047
I0418 09:10:57.885742 140490276050688 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6956021785736084, loss=2.3758463859558105
I0418 09:11:31.739330 140486677350144 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5907871723175049, loss=2.433764934539795
I0418 09:12:05.594362 140490276050688 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6431890726089478, loss=2.3698208332061768
I0418 09:12:39.425170 140486677350144 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8016995787620544, loss=2.475083112716675
I0418 09:13:13.239249 140490276050688 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5843873023986816, loss=2.344177007675171
I0418 09:13:47.155234 140486677350144 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7470841407775879, loss=2.378291368484497
I0418 09:13:49.271883 140667534567232 spec.py:298] Evaluating on the training split.
I0418 09:13:56.970108 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 09:14:07.173716 140667534567232 spec.py:326] Evaluating on the test split.
I0418 09:14:09.040161 140667534567232 submission_runner.py:406] Time since start: 8561.29s, 	Step: 24108, 	{'train/accuracy': 0.7626355290412903, 'train/loss': 1.0143303871154785, 'validation/accuracy': 0.6715799570083618, 'validation/loss': 1.4273982048034668, 'validation/num_examples': 50000, 'test/accuracy': 0.5408000349998474, 'test/loss': 2.1191704273223877, 'test/num_examples': 10000, 'score': 8210.797550201416, 'total_duration': 8561.292462348938, 'accumulated_submission_time': 8210.797550201416, 'accumulated_eval_time': 328.93018674850464, 'accumulated_logging_time': 21.26505398750305}
I0418 09:14:09.048979 140490276050688 logging_writer.py:48] [24108] accumulated_eval_time=328.930187, accumulated_logging_time=21.265054, accumulated_submission_time=8210.797550, global_step=24108, preemption_count=0, score=8210.797550, test/accuracy=0.540800, test/loss=2.119170, test/num_examples=10000, total_duration=8561.292462, train/accuracy=0.762636, train/loss=1.014330, validation/accuracy=0.671580, validation/loss=1.427398, validation/num_examples=50000
I0418 09:14:09.302669 140667534567232 checkpoints.py:356] Saving checkpoint at step: 24108
I0418 09:14:10.243055 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_24108
I0418 09:14:10.258847 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_24108.
I0418 09:14:41.802170 140486677350144 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7428261637687683, loss=2.477100133895874
I0418 09:15:15.625690 140486668957440 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.675013542175293, loss=2.3378984928131104
I0418 09:15:49.478472 140486677350144 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.9563699960708618, loss=2.407377243041992
I0418 09:16:23.297137 140486668957440 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6935100555419922, loss=2.440741539001465
I0418 09:16:57.277742 140486677350144 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5814178586006165, loss=2.4267263412475586
I0418 09:17:31.003239 140486668957440 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6208985447883606, loss=2.4069714546203613
I0418 09:18:04.664607 140486677350144 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5273630619049072, loss=2.339944839477539
I0418 09:18:38.376395 140486668957440 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6557348966598511, loss=2.339456558227539
I0418 09:19:12.195771 140486677350144 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6907134652137756, loss=2.352186918258667
I0418 09:19:45.899967 140486668957440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8242407441139221, loss=2.427387237548828
I0418 09:20:19.684426 140486677350144 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.7729439735412598, loss=2.435014486312866
I0418 09:20:53.543967 140486668957440 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7134177088737488, loss=2.335252046585083
I0418 09:21:27.472551 140486677350144 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6994627118110657, loss=2.419558048248291
I0418 09:22:01.146551 140486668957440 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6818158030509949, loss=2.351353406906128
I0418 09:22:34.817465 140486677350144 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6807839870452881, loss=2.46097993850708
I0418 09:22:40.319974 140667534567232 spec.py:298] Evaluating on the training split.
I0418 09:22:47.633688 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 09:22:57.865761 140667534567232 spec.py:326] Evaluating on the test split.
I0418 09:22:59.931063 140667534567232 submission_runner.py:406] Time since start: 9092.18s, 	Step: 25618, 	{'train/accuracy': 0.7664620280265808, 'train/loss': 1.027692437171936, 'validation/accuracy': 0.6736999750137329, 'validation/loss': 1.4471747875213623, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.1267874240875244, 'test/num_examples': 10000, 'score': 8720.833670377731, 'total_duration': 9092.183110237122, 'accumulated_submission_time': 8720.833670377731, 'accumulated_eval_time': 348.53999376296997, 'accumulated_logging_time': 22.49098515510559}
I0418 09:22:59.940555 140486668957440 logging_writer.py:48] [25618] accumulated_eval_time=348.539994, accumulated_logging_time=22.490985, accumulated_submission_time=8720.833670, global_step=25618, preemption_count=0, score=8720.833670, test/accuracy=0.544200, test/loss=2.126787, test/num_examples=10000, total_duration=9092.183110, train/accuracy=0.766462, train/loss=1.027692, validation/accuracy=0.673700, validation/loss=1.447175, validation/num_examples=50000
I0418 09:23:00.195894 140667534567232 checkpoints.py:356] Saving checkpoint at step: 25618
I0418 09:23:01.162258 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_25618
I0418 09:23:01.180430 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_25618.
I0418 09:23:29.459426 140486677350144 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5549241304397583, loss=2.3615643978118896
I0418 09:24:03.524998 140490905208576 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6860198378562927, loss=2.337019205093384
I0418 09:24:37.425052 140486677350144 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6478456854820251, loss=2.3919153213500977
I0418 09:25:11.314388 140490905208576 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5627831816673279, loss=2.372023820877075
I0418 09:25:45.406827 140486677350144 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6758279204368591, loss=2.3223354816436768
I0418 09:26:19.388618 140490905208576 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6299809813499451, loss=2.3867621421813965
I0418 09:26:53.328918 140486677350144 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6782439947128296, loss=2.341235637664795
I0418 09:27:27.393016 140490905208576 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6423015594482422, loss=2.3206820487976074
I0418 09:28:01.114962 140486677350144 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7300974726676941, loss=2.3769283294677734
I0418 09:28:34.982083 140490905208576 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7051140666007996, loss=2.4154062271118164
I0418 09:29:09.016730 140486677350144 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5743169784545898, loss=2.393502712249756
I0418 09:29:43.178401 140490905208576 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6440550684928894, loss=2.3752646446228027
I0418 09:30:17.006612 140486677350144 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6520121097564697, loss=2.434767723083496
I0418 09:30:50.870407 140490905208576 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7607957124710083, loss=2.4712283611297607
I0418 09:31:24.807202 140486677350144 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6410117149353027, loss=2.3429903984069824
I0418 09:31:31.278640 140667534567232 spec.py:298] Evaluating on the training split.
I0418 09:31:38.522303 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 09:31:48.814115 140667534567232 spec.py:326] Evaluating on the test split.
I0418 09:31:50.878862 140667534567232 submission_runner.py:406] Time since start: 9623.13s, 	Step: 27121, 	{'train/accuracy': 0.7732580900192261, 'train/loss': 0.9736858606338501, 'validation/accuracy': 0.6804999709129333, 'validation/loss': 1.3935911655426025, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.1140594482421875, 'test/num_examples': 10000, 'score': 9230.910021305084, 'total_duration': 9623.130892038345, 'accumulated_submission_time': 9230.910021305084, 'accumulated_eval_time': 368.1389343738556, 'accumulated_logging_time': 23.74420189857483}
I0418 09:31:50.888640 140490905208576 logging_writer.py:48] [27121] accumulated_eval_time=368.138934, accumulated_logging_time=23.744202, accumulated_submission_time=9230.910021, global_step=27121, preemption_count=0, score=9230.910021, test/accuracy=0.545900, test/loss=2.114059, test/num_examples=10000, total_duration=9623.130892, train/accuracy=0.773258, train/loss=0.973686, validation/accuracy=0.680500, validation/loss=1.393591, validation/num_examples=50000
I0418 09:31:51.157553 140667534567232 checkpoints.py:356] Saving checkpoint at step: 27121
I0418 09:31:52.104146 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_27121
I0418 09:31:52.121598 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_27121.
I0418 09:32:19.245371 140486677350144 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7107358574867249, loss=2.3129730224609375
I0418 09:32:53.029115 140490729060096 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.7686444520950317, loss=2.299025058746338
I0418 09:33:26.841746 140486677350144 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7532828450202942, loss=2.4435806274414062
I0418 09:34:00.584732 140490729060096 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5731368064880371, loss=2.226447105407715
I0418 09:34:34.353839 140486677350144 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6120398044586182, loss=2.365724802017212
I0418 09:35:08.065778 140490729060096 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6689137816429138, loss=2.3239567279815674
I0418 09:35:42.031800 140486677350144 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7325111627578735, loss=2.35255765914917
I0418 09:36:15.884898 140490729060096 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.7194467186927795, loss=2.3946895599365234
I0418 09:36:49.178491 140667534567232 spec.py:298] Evaluating on the training split.
I0418 09:36:56.459739 140667534567232 spec.py:310] Evaluating on the validation split.
I0418 09:37:07.283186 140667534567232 spec.py:326] Evaluating on the test split.
I0418 09:37:09.271568 140667534567232 submission_runner.py:406] Time since start: 9941.52s, 	Step: 28000, 	{'train/accuracy': 0.7804328799247742, 'train/loss': 0.9680830240249634, 'validation/accuracy': 0.679419994354248, 'validation/loss': 1.406130075454712, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.108294725418091, 'test/num_examples': 10000, 'score': 9527.951814174652, 'total_duration': 9941.523412704468, 'accumulated_submission_time': 9527.951814174652, 'accumulated_eval_time': 388.23052740097046, 'accumulated_logging_time': 24.991530895233154}
I0418 09:37:09.280865 140486677350144 logging_writer.py:48] [28000] accumulated_eval_time=388.230527, accumulated_logging_time=24.991531, accumulated_submission_time=9527.951814, global_step=28000, preemption_count=0, score=9527.951814, test/accuracy=0.546300, test/loss=2.108295, test/num_examples=10000, total_duration=9941.523413, train/accuracy=0.780433, train/loss=0.968083, validation/accuracy=0.679420, validation/loss=1.406130, validation/num_examples=50000
I0418 09:37:09.560688 140667534567232 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 09:37:10.518360 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0418 09:37:10.535465 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0418 09:37:10.551783 140490729060096 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=9527.951814
I0418 09:37:10.717439 140667534567232 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 09:37:12.047451 140667534567232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0418 09:37:12.062488 140667534567232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0418 09:37:12.413049 140667534567232 submission_runner.py:567] Tuning trial 1/1
I0418 09:37:12.414110 140667534567232 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 09:37:12.427551 140667534567232 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011758609907701612, 'train/loss': 6.912867069244385, 'validation/accuracy': 0.0010599999222904444, 'validation/loss': 6.912654876708984, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.912196636199951, 'test/num_examples': 10000, 'score': 48.18110656738281, 'total_duration': 88.07666754722595, 'accumulated_submission_time': 48.18110656738281, 'accumulated_eval_time': 39.895402908325195, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1506, {'train/accuracy': 0.13576211035251617, 'train/loss': 4.579771518707275, 'validation/accuracy': 0.1198199987411499, 'validation/loss': 4.699863910675049, 'validation/num_examples': 50000, 'test/accuracy': 0.09190000593662262, 'test/loss': 5.0460686683654785, 'test/num_examples': 10000, 'score': 558.4132807254791, 'total_duration': 615.5079982280731, 'accumulated_submission_time': 558.4132807254791, 'accumulated_eval_time': 56.264535427093506, 'accumulated_logging_time': 0.8105435371398926, 'global_step': 1506, 'preemption_count': 0}), (3006, {'train/accuracy': 0.30090081691741943, 'train/loss': 3.381056785583496, 'validation/accuracy': 0.2691600024700165, 'validation/loss': 3.5463645458221436, 'validation/num_examples': 50000, 'test/accuracy': 0.20650000870227814, 'test/loss': 4.0554585456848145, 'test/num_examples': 10000, 'score': 1068.6198675632477, 'total_duration': 1143.0758271217346, 'accumulated_submission_time': 1068.6198675632477, 'accumulated_eval_time': 72.8049156665802, 'accumulated_logging_time': 1.6123292446136475, 'global_step': 3006, 'preemption_count': 0}), (4508, {'train/accuracy': 0.42434629797935486, 'train/loss': 2.6144611835479736, 'validation/accuracy': 0.3957599997520447, 'validation/loss': 2.7852745056152344, 'validation/num_examples': 50000, 'test/accuracy': 0.29330000281333923, 'test/loss': 3.4409170150756836, 'test/num_examples': 10000, 'score': 1578.6377086639404, 'total_duration': 1670.6955924034119, 'accumulated_submission_time': 1578.6377086639404, 'accumulated_eval_time': 89.2828619480133, 'accumulated_logging_time': 2.716933012008667, 'global_step': 4508, 'preemption_count': 0}), (6013, {'train/accuracy': 0.48036909103393555, 'train/loss': 2.3798084259033203, 'validation/accuracy': 0.4461999833583832, 'validation/loss': 2.5537285804748535, 'validation/num_examples': 50000, 'test/accuracy': 0.34040001034736633, 'test/loss': 3.2112255096435547, 'test/num_examples': 10000, 'score': 2088.9492032527924, 'total_duration': 2198.33572101593, 'accumulated_submission_time': 2088.9492032527924, 'accumulated_eval_time': 105.79610276222229, 'accumulated_logging_time': 3.513601303100586, 'global_step': 6013, 'preemption_count': 0}), (7525, {'train/accuracy': 0.5518773794174194, 'train/loss': 2.000563859939575, 'validation/accuracy': 0.5174800157546997, 'validation/loss': 2.172894239425659, 'validation/num_examples': 50000, 'test/accuracy': 0.39560002088546753, 'test/loss': 2.8650150299072266, 'test/num_examples': 10000, 'score': 2599.1635286808014, 'total_duration': 2725.5037174224854, 'accumulated_submission_time': 2599.1635286808014, 'accumulated_eval_time': 121.92457365989685, 'accumulated_logging_time': 4.320331573486328, 'global_step': 7525, 'preemption_count': 0}), (9029, {'train/accuracy': 0.5920360088348389, 'train/loss': 1.8293614387512207, 'validation/accuracy': 0.5486999750137329, 'validation/loss': 2.0303893089294434, 'validation/num_examples': 50000, 'test/accuracy': 0.42640000581741333, 'test/loss': 2.7096686363220215, 'test/num_examples': 10000, 'score': 3109.15859746933, 'total_duration': 3253.003559112549, 'accumulated_submission_time': 3109.15859746933, 'accumulated_eval_time': 138.28440928459167, 'accumulated_logging_time': 5.447415113449097, 'global_step': 9029, 'preemption_count': 0}), (10535, {'train/accuracy': 0.6546755433082581, 'train/loss': 1.5315134525299072, 'validation/accuracy': 0.58024001121521, 'validation/loss': 1.8964067697525024, 'validation/num_examples': 50000, 'test/accuracy': 0.44670000672340393, 'test/loss': 2.6032543182373047, 'test/num_examples': 10000, 'score': 3619.3847959041595, 'total_duration': 3784.88702917099, 'accumulated_submission_time': 3619.3847959041595, 'accumulated_eval_time': 155.4121344089508, 'accumulated_logging_time': 9.958916902542114, 'global_step': 10535, 'preemption_count': 0}), (12045, {'train/accuracy': 0.6685466766357422, 'train/loss': 1.4761199951171875, 'validation/accuracy': 0.6002599596977234, 'validation/loss': 1.787896990776062, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.4780313968658447, 'test/num_examples': 10000, 'score': 4129.55771946907, 'total_duration': 4314.79471039772, 'accumulated_submission_time': 4129.55771946907, 'accumulated_eval_time': 173.96310377120972, 'accumulated_logging_time': 11.124778747558594, 'global_step': 12045, 'preemption_count': 0}), (13554, {'train/accuracy': 0.6848094463348389, 'train/loss': 1.3774724006652832, 'validation/accuracy': 0.6178399920463562, 'validation/loss': 1.686216950416565, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.3747177124023438, 'test/num_examples': 10000, 'score': 4639.675585985184, 'total_duration': 4845.104496955872, 'accumulated_submission_time': 4639.675585985184, 'accumulated_eval_time': 192.8927445411682, 'accumulated_logging_time': 12.36813235282898, 'global_step': 13554, 'preemption_count': 0}), (15062, {'train/accuracy': 0.6949737071990967, 'train/loss': 1.338767170906067, 'validation/accuracy': 0.6303399801254272, 'validation/loss': 1.6399836540222168, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.329158306121826, 'test/num_examples': 10000, 'score': 5149.891433954239, 'total_duration': 5375.261265993118, 'accumulated_submission_time': 5149.891433954239, 'accumulated_eval_time': 211.6850152015686, 'accumulated_logging_time': 13.497699975967407, 'global_step': 15062, 'preemption_count': 0}), (16567, {'train/accuracy': 0.7135084271430969, 'train/loss': 1.2548784017562866, 'validation/accuracy': 0.6442199945449829, 'validation/loss': 1.5712519884109497, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.263230323791504, 'test/num_examples': 10000, 'score': 5660.128768920898, 'total_duration': 5905.917297840118, 'accumulated_submission_time': 5660.128768920898, 'accumulated_eval_time': 230.8503658771515, 'accumulated_logging_time': 14.732025623321533, 'global_step': 16567, 'preemption_count': 0}), (18076, {'train/accuracy': 0.7252271771430969, 'train/loss': 1.1911572217941284, 'validation/accuracy': 0.6494399905204773, 'validation/loss': 1.5245273113250732, 'validation/num_examples': 50000, 'test/accuracy': 0.5240000486373901, 'test/loss': 2.2242636680603027, 'test/num_examples': 10000, 'score': 6170.117515087128, 'total_duration': 6436.88224697113, 'accumulated_submission_time': 6170.117515087128, 'accumulated_eval_time': 250.25283217430115, 'accumulated_logging_time': 16.28660225868225, 'global_step': 18076, 'preemption_count': 0}), (19587, {'train/accuracy': 0.76566481590271, 'train/loss': 1.0317158699035645, 'validation/accuracy': 0.6578399538993835, 'validation/loss': 1.507950782775879, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.220456838607788, 'test/num_examples': 10000, 'score': 6680.117534637451, 'total_duration': 6967.868787527084, 'accumulated_submission_time': 6680.117534637451, 'accumulated_eval_time': 269.9926846027374, 'accumulated_logging_time': 17.514888048171997, 'global_step': 19587, 'preemption_count': 0}), (21093, {'train/accuracy': 0.7586694955825806, 'train/loss': 1.0575735569000244, 'validation/accuracy': 0.6634399890899658, 'validation/loss': 1.4816827774047852, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.176906108856201, 'test/num_examples': 10000, 'score': 7190.395640611649, 'total_duration': 7499.376618385315, 'accumulated_submission_time': 7190.395640611649, 'accumulated_eval_time': 289.95612812042236, 'accumulated_logging_time': 18.762418031692505, 'global_step': 21093, 'preemption_count': 0}), (22598, {'train/accuracy': 0.7564970850944519, 'train/loss': 1.0595335960388184, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.4604707956314087, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.152249574661255, 'test/num_examples': 10000, 'score': 7700.6712038517, 'total_duration': 8030.086052894592, 'accumulated_submission_time': 7700.6712038517, 'accumulated_eval_time': 309.1629161834717, 'accumulated_logging_time': 19.970874786376953, 'global_step': 22598, 'preemption_count': 0}), (24108, {'train/accuracy': 0.7626355290412903, 'train/loss': 1.0143303871154785, 'validation/accuracy': 0.6715799570083618, 'validation/loss': 1.4273982048034668, 'validation/num_examples': 50000, 'test/accuracy': 0.5408000349998474, 'test/loss': 2.1191704273223877, 'test/num_examples': 10000, 'score': 8210.797550201416, 'total_duration': 8561.292462348938, 'accumulated_submission_time': 8210.797550201416, 'accumulated_eval_time': 328.93018674850464, 'accumulated_logging_time': 21.26505398750305, 'global_step': 24108, 'preemption_count': 0}), (25618, {'train/accuracy': 0.7664620280265808, 'train/loss': 1.027692437171936, 'validation/accuracy': 0.6736999750137329, 'validation/loss': 1.4471747875213623, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.1267874240875244, 'test/num_examples': 10000, 'score': 8720.833670377731, 'total_duration': 9092.183110237122, 'accumulated_submission_time': 8720.833670377731, 'accumulated_eval_time': 348.53999376296997, 'accumulated_logging_time': 22.49098515510559, 'global_step': 25618, 'preemption_count': 0}), (27121, {'train/accuracy': 0.7732580900192261, 'train/loss': 0.9736858606338501, 'validation/accuracy': 0.6804999709129333, 'validation/loss': 1.3935911655426025, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.1140594482421875, 'test/num_examples': 10000, 'score': 9230.910021305084, 'total_duration': 9623.130892038345, 'accumulated_submission_time': 9230.910021305084, 'accumulated_eval_time': 368.1389343738556, 'accumulated_logging_time': 23.74420189857483, 'global_step': 27121, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7804328799247742, 'train/loss': 0.9680830240249634, 'validation/accuracy': 0.679419994354248, 'validation/loss': 1.406130075454712, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.108294725418091, 'test/num_examples': 10000, 'score': 9527.951814174652, 'total_duration': 9941.523412704468, 'accumulated_submission_time': 9527.951814174652, 'accumulated_eval_time': 388.23052740097046, 'accumulated_logging_time': 24.991530895233154, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0418 09:37:12.427664 140667534567232 submission_runner.py:570] Timing: 9527.951814174652
I0418 09:37:12.427706 140667534567232 submission_runner.py:571] ====================
I0418 09:37:12.427830 140667534567232 submission_runner.py:631] Final imagenet_resnet score: 9527.951814174652
