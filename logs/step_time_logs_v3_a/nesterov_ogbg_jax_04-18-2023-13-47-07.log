I0418 13:47:28.717541 140598912194368 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax.
I0418 13:47:28.784242 140598912194368 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 13:47:29.798346 140598912194368 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0418 13:47:29.799000 140598912194368 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 13:47:29.803106 140598912194368 submission_runner.py:528] Using RNG seed 1679003917
I0418 13:47:32.348420 140598912194368 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 13:47:32.348620 140598912194368 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1.
I0418 13:47:32.348797 140598912194368 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/hparams.json.
I0418 13:47:32.471766 140598912194368 submission_runner.py:232] Initializing dataset.
I0418 13:47:32.702741 140598912194368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:47:32.707343 140598912194368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:47:32.925036 140598912194368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:47:32.978049 140598912194368 submission_runner.py:239] Initializing model.
I0418 13:47:39.992368 140598912194368 submission_runner.py:249] Initializing optimizer.
I0418 13:47:40.339615 140598912194368 submission_runner.py:256] Initializing metrics bundle.
I0418 13:47:40.339787 140598912194368 submission_runner.py:273] Initializing checkpoint and logger.
I0418 13:47:40.340905 140598912194368 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1 with prefix checkpoint_
I0418 13:47:40.341131 140598912194368 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 13:47:40.341200 140598912194368 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 13:47:41.045185 140598912194368 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/meta_data_0.json.
I0418 13:47:41.046061 140598912194368 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/flags_0.json.
I0418 13:47:41.051581 140598912194368 submission_runner.py:309] Starting training loop.
I0418 13:48:00.466474 140422974269184 logging_writer.py:48] [0] global_step=0, grad_norm=3.151045560836792, loss=0.7418863773345947
I0418 13:48:00.477836 140598912194368 spec.py:298] Evaluating on the training split.
I0418 13:48:00.485820 140598912194368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:48:00.489673 140598912194368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:48:00.546017 140598912194368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0418 13:48:16.305950 140598912194368 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0418 13:49:30.552068 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 13:49:30.554598 140598912194368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:49:30.558113 140598912194368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:49:30.610577 140598912194368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:50:32.911434 140598912194368 spec.py:326] Evaluating on the test split.
I0418 13:50:32.913941 140598912194368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:50:32.917489 140598912194368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:50:32.968972 140598912194368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:51:34.926778 140598912194368 submission_runner.py:406] Time since start: 233.88s, 	Step: 1, 	{'train/accuracy': 0.5851648449897766, 'train/loss': 0.7420417070388794, 'train/mean_average_precision': 0.021668887065090927, 'validation/accuracy': 0.5899277925491333, 'validation/loss': 0.7371640205383301, 'validation/mean_average_precision': 0.025907461759495806, 'validation/num_examples': 43793, 'test/accuracy': 0.592805802822113, 'test/loss': 0.7339466214179993, 'test/mean_average_precision': 0.027859805599282896, 'test/num_examples': 43793, 'score': 19.426052808761597, 'total_duration': 233.8751618862152, 'accumulated_submission_time': 19.426052808761597, 'accumulated_eval_time': 214.4489245414734, 'accumulated_logging_time': 0}
I0418 13:51:34.941717 140413092038400 logging_writer.py:48] [1] accumulated_eval_time=214.448925, accumulated_logging_time=0, accumulated_submission_time=19.426053, global_step=1, preemption_count=0, score=19.426053, test/accuracy=0.592806, test/loss=0.733947, test/mean_average_precision=0.027860, test/num_examples=43793, total_duration=233.875162, train/accuracy=0.585165, train/loss=0.742042, train/mean_average_precision=0.021669, validation/accuracy=0.589928, validation/loss=0.737164, validation/mean_average_precision=0.025907, validation/num_examples=43793
I0418 13:51:34.966766 140598912194368 checkpoints.py:356] Saving checkpoint at step: 1
I0418 13:51:35.032573 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_1
I0418 13:51:35.032791 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_1.
I0418 13:51:57.488332 140413100431104 logging_writer.py:48] [100] global_step=100, grad_norm=0.06622866541147232, loss=0.0909038856625557
I0418 13:52:19.936852 140414224955136 logging_writer.py:48] [200] global_step=200, grad_norm=0.01711977832019329, loss=0.06235390156507492
I0418 13:52:42.310822 140413100431104 logging_writer.py:48] [300] global_step=300, grad_norm=0.009318479336798191, loss=0.05440475419163704
I0418 13:53:04.769601 140414224955136 logging_writer.py:48] [400] global_step=400, grad_norm=0.01032259687781334, loss=0.05486445128917694
I0418 13:53:27.541948 140413100431104 logging_writer.py:48] [500] global_step=500, grad_norm=0.007173830643296242, loss=0.05370985344052315
I0418 13:53:50.070527 140414224955136 logging_writer.py:48] [600] global_step=600, grad_norm=0.008667713031172752, loss=0.054114267230033875
I0418 13:54:12.880628 140413100431104 logging_writer.py:48] [700] global_step=700, grad_norm=0.018346229568123817, loss=0.05862962082028389
I0418 13:54:35.545434 140414224955136 logging_writer.py:48] [800] global_step=800, grad_norm=0.013508010655641556, loss=0.055265896022319794
I0418 13:54:58.397468 140413100431104 logging_writer.py:48] [900] global_step=900, grad_norm=0.05839010700583458, loss=0.05562926083803177
I0418 13:55:20.870150 140414224955136 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.06859498471021652, loss=0.05083506926894188
I0418 13:55:35.266860 140598912194368 spec.py:298] Evaluating on the training split.
I0418 13:56:46.441393 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 13:56:48.942074 140598912194368 spec.py:326] Evaluating on the test split.
I0418 13:56:51.370689 140598912194368 submission_runner.py:406] Time since start: 550.32s, 	Step: 1065, 	{'train/accuracy': 0.9867406487464905, 'train/loss': 0.05441141128540039, 'train/mean_average_precision': 0.03644545994948914, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06470400840044022, 'validation/mean_average_precision': 0.03963068704622649, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0680670365691185, 'test/mean_average_precision': 0.041031736552491434, 'test/num_examples': 43793, 'score': 259.6519031524658, 'total_duration': 550.319061756134, 'accumulated_submission_time': 259.6519031524658, 'accumulated_eval_time': 290.55273270606995, 'accumulated_logging_time': 0.1061239242553711}
I0418 13:56:51.377653 140413100431104 logging_writer.py:48] [1065] accumulated_eval_time=290.552733, accumulated_logging_time=0.106124, accumulated_submission_time=259.651903, global_step=1065, preemption_count=0, score=259.651903, test/accuracy=0.983142, test/loss=0.068067, test/mean_average_precision=0.041032, test/num_examples=43793, total_duration=550.319062, train/accuracy=0.986741, train/loss=0.054411, train/mean_average_precision=0.036445, validation/accuracy=0.984118, validation/loss=0.064704, validation/mean_average_precision=0.039631, validation/num_examples=43793
I0418 13:56:51.402284 140598912194368 checkpoints.py:356] Saving checkpoint at step: 1065
I0418 13:56:51.467217 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_1065
I0418 13:56:51.467403 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_1065.
I0418 13:56:59.565896 140414224955136 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.024971256032586098, loss=0.051290057599544525
I0418 13:57:22.303655 140414199777024 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02716376818716526, loss=0.05314129963517189
I0418 13:57:44.960694 140414224955136 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.06512635201215744, loss=0.06114012375473976
I0418 13:58:07.858323 140414199777024 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02752542309463024, loss=0.05742800235748291
I0418 13:58:30.357824 140414224955136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04471627622842789, loss=0.05479474738240242
I0418 13:58:53.006272 140414199777024 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.03705660626292229, loss=0.048785533756017685
I0418 13:59:15.703490 140414224955136 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03007384203374386, loss=0.05326682701706886
I0418 13:59:38.225534 140414199777024 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.05585218966007233, loss=0.049742892384529114
I0418 14:00:00.790471 140414224955136 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.04246468096971512, loss=0.050288889557123184
I0418 14:00:23.351938 140414199777024 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.026951037347316742, loss=0.05041905492544174
I0418 14:00:45.842255 140414224955136 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.06128770112991333, loss=0.05691929906606674
I0418 14:00:51.659209 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:02:03.389480 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:02:05.864845 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:02:08.293400 140598912194368 submission_runner.py:406] Time since start: 867.24s, 	Step: 2127, 	{'train/accuracy': 0.9867504239082336, 'train/loss': 0.051603276282548904, 'train/mean_average_precision': 0.05766840858766916, 'validation/accuracy': 0.984119176864624, 'validation/loss': 0.0613492988049984, 'validation/mean_average_precision': 0.05522809789741908, 'validation/num_examples': 43793, 'test/accuracy': 0.9831412434577942, 'test/loss': 0.06458266824483871, 'test/mean_average_precision': 0.055741406756206306, 'test/num_examples': 43793, 'score': 499.8357071876526, 'total_duration': 867.2417707443237, 'accumulated_submission_time': 499.8357071876526, 'accumulated_eval_time': 367.18688893318176, 'accumulated_logging_time': 0.2029876708984375}
I0418 14:02:08.300710 140414199777024 logging_writer.py:48] [2127] accumulated_eval_time=367.186889, accumulated_logging_time=0.202988, accumulated_submission_time=499.835707, global_step=2127, preemption_count=0, score=499.835707, test/accuracy=0.983141, test/loss=0.064583, test/mean_average_precision=0.055741, test/num_examples=43793, total_duration=867.241771, train/accuracy=0.986750, train/loss=0.051603, train/mean_average_precision=0.057668, validation/accuracy=0.984119, validation/loss=0.061349, validation/mean_average_precision=0.055228, validation/num_examples=43793
I0418 14:02:08.326490 140598912194368 checkpoints.py:356] Saving checkpoint at step: 2127
I0418 14:02:08.386442 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_2127
I0418 14:02:08.386617 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_2127.
I0418 14:02:24.977013 140414224955136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.036732085049152374, loss=0.05057830736041069
I0418 14:02:47.432957 140414174598912 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.049275901168584824, loss=0.050679877400398254
I0418 14:03:09.952417 140414224955136 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.054600026458501816, loss=0.04764075204730034
I0418 14:03:32.552562 140414174598912 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0317053385078907, loss=0.04810681194067001
I0418 14:03:55.180369 140414224955136 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0378977432847023, loss=0.05263634771108627
I0418 14:04:17.589323 140414174598912 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1391996145248413, loss=0.049003954976797104
I0418 14:04:40.437200 140414224955136 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.02709704264998436, loss=0.05434781312942505
I0418 14:05:05.053576 140414174598912 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.06754821538925171, loss=0.04617728665471077
I0418 14:05:29.884181 140414224955136 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.057760655879974365, loss=0.05206617712974548
I0418 14:05:52.905325 140414174598912 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.030768781900405884, loss=0.04787927120923996
I0418 14:06:08.421346 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:07:19.467963 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:07:21.941522 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:07:24.398146 140598912194368 submission_runner.py:406] Time since start: 1183.35s, 	Step: 3170, 	{'train/accuracy': 0.9871504902839661, 'train/loss': 0.047445327043533325, 'train/mean_average_precision': 0.0991868137650982, 'validation/accuracy': 0.9843952059745789, 'validation/loss': 0.05740876495838165, 'validation/mean_average_precision': 0.09603262721468366, 'validation/num_examples': 43793, 'test/accuracy': 0.9833973050117493, 'test/loss': 0.06043630838394165, 'test/mean_average_precision': 0.10122517150408011, 'test/num_examples': 43793, 'score': 739.8623418807983, 'total_duration': 1183.3465030193329, 'accumulated_submission_time': 739.8623418807983, 'accumulated_eval_time': 443.1636381149292, 'accumulated_logging_time': 0.2963860034942627}
I0418 14:07:24.406277 140414224955136 logging_writer.py:48] [3170] accumulated_eval_time=443.163638, accumulated_logging_time=0.296386, accumulated_submission_time=739.862342, global_step=3170, preemption_count=0, score=739.862342, test/accuracy=0.983397, test/loss=0.060436, test/mean_average_precision=0.101225, test/num_examples=43793, total_duration=1183.346503, train/accuracy=0.987150, train/loss=0.047445, train/mean_average_precision=0.099187, validation/accuracy=0.984395, validation/loss=0.057409, validation/mean_average_precision=0.096033, validation/num_examples=43793
I0418 14:07:24.431627 140598912194368 checkpoints.py:356] Saving checkpoint at step: 3170
I0418 14:07:24.486027 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_3170
I0418 14:07:24.486210 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_3170.
I0418 14:07:31.440176 140414174598912 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05960920453071594, loss=0.04791571944952011
I0418 14:07:53.864605 140414166206208 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07724739611148834, loss=0.05598951131105423
I0418 14:08:16.538418 140414174598912 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.10367030650377274, loss=0.04919121786952019
I0418 14:08:39.253417 140414166206208 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.07108182460069656, loss=0.04994251951575279
I0418 14:09:01.803131 140414174598912 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03656209260225296, loss=0.05150201916694641
I0418 14:09:24.457123 140414166206208 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.03648585453629494, loss=0.04615311324596405
I0418 14:09:47.386953 140414174598912 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.07337101548910141, loss=0.049755532294511795
I0418 14:10:10.083018 140414166206208 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.07891834527254105, loss=0.046748388558626175
I0418 14:10:32.680749 140414174598912 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.03835968300700188, loss=0.05079422891139984
I0418 14:10:55.712684 140414166206208 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.029738053679466248, loss=0.04752511903643608
I0418 14:11:18.269327 140414174598912 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.10664521157741547, loss=0.048526789993047714
I0418 14:11:24.559481 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:12:35.855549 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:12:38.343299 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:12:40.779072 140598912194368 submission_runner.py:406] Time since start: 1499.73s, 	Step: 4229, 	{'train/accuracy': 0.9874129891395569, 'train/loss': 0.04666562378406525, 'train/mean_average_precision': 0.11644610856421783, 'validation/accuracy': 0.9847304821014404, 'validation/loss': 0.05501586198806763, 'validation/mean_average_precision': 0.11156971337446672, 'validation/num_examples': 43793, 'test/accuracy': 0.9836935997009277, 'test/loss': 0.05777660384774208, 'test/mean_average_precision': 0.11503555563380898, 'test/num_examples': 43793, 'score': 979.9274423122406, 'total_duration': 1499.727421283722, 'accumulated_submission_time': 979.9274423122406, 'accumulated_eval_time': 519.3831729888916, 'accumulated_logging_time': 0.38455820083618164}
I0418 14:12:40.786486 140414166206208 logging_writer.py:48] [4229] accumulated_eval_time=519.383173, accumulated_logging_time=0.384558, accumulated_submission_time=979.927442, global_step=4229, preemption_count=0, score=979.927442, test/accuracy=0.983694, test/loss=0.057777, test/mean_average_precision=0.115036, test/num_examples=43793, total_duration=1499.727421, train/accuracy=0.987413, train/loss=0.046666, train/mean_average_precision=0.116446, validation/accuracy=0.984730, validation/loss=0.055016, validation/mean_average_precision=0.111570, validation/num_examples=43793
I0418 14:12:40.811662 140598912194368 checkpoints.py:356] Saving checkpoint at step: 4229
I0418 14:12:40.865559 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_4229
I0418 14:12:40.865739 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_4229.
I0418 14:12:57.144539 140414174598912 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05887770652770996, loss=0.047396279871463776
I0418 14:13:19.893948 140414157813504 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04083598032593727, loss=0.05030418932437897
I0418 14:13:42.747483 140414174598912 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.07366370409727097, loss=0.045378148555755615
I0418 14:14:05.803240 140414157813504 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0543532595038414, loss=0.047970905900001526
I0418 14:14:28.478574 140414174598912 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.06378016620874405, loss=0.04605899378657341
I0418 14:14:51.445871 140414157813504 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.08216506242752075, loss=0.04895588010549545
I0418 14:15:14.172142 140414174598912 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.04955187067389488, loss=0.04401116445660591
I0418 14:15:36.880888 140414157813504 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.05392521619796753, loss=0.0488949716091156
I0418 14:15:59.548450 140414174598912 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.06224915012717247, loss=0.04685758799314499
I0418 14:16:22.347521 140414157813504 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.05667269229888916, loss=0.04735162854194641
I0418 14:16:41.033206 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:17:52.288338 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:17:54.782025 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:17:57.215962 140598912194368 submission_runner.py:406] Time since start: 1816.16s, 	Step: 5283, 	{'train/accuracy': 0.9875574707984924, 'train/loss': 0.04382091388106346, 'train/mean_average_precision': 0.14701590786434002, 'validation/accuracy': 0.9848185777664185, 'validation/loss': 0.05372907221317291, 'validation/mean_average_precision': 0.13599771868973828, 'validation/num_examples': 43793, 'test/accuracy': 0.9838336706161499, 'test/loss': 0.05673304572701454, 'test/mean_average_precision': 0.13803483870319536, 'test/num_examples': 43793, 'score': 1220.0867431163788, 'total_duration': 1816.1643269062042, 'accumulated_submission_time': 1220.0867431163788, 'accumulated_eval_time': 595.5658853054047, 'accumulated_logging_time': 0.471343994140625}
I0418 14:17:57.223461 140414174598912 logging_writer.py:48] [5283] accumulated_eval_time=595.565885, accumulated_logging_time=0.471344, accumulated_submission_time=1220.086743, global_step=5283, preemption_count=0, score=1220.086743, test/accuracy=0.983834, test/loss=0.056733, test/mean_average_precision=0.138035, test/num_examples=43793, total_duration=1816.164327, train/accuracy=0.987557, train/loss=0.043821, train/mean_average_precision=0.147016, validation/accuracy=0.984819, validation/loss=0.053729, validation/mean_average_precision=0.135998, validation/num_examples=43793
I0418 14:17:57.247662 140598912194368 checkpoints.py:356] Saving checkpoint at step: 5283
I0418 14:17:57.302196 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_5283
I0418 14:17:57.302372 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_5283.
I0418 14:18:01.410311 140414157813504 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.13973291218280792, loss=0.04768240079283714
I0418 14:18:24.472885 140414149420800 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.14691725373268127, loss=0.04916347935795784
I0418 14:18:47.147015 140414157813504 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.08017381280660629, loss=0.046308569610118866
I0418 14:19:09.935719 140414149420800 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.08277367800474167, loss=0.04427238181233406
I0418 14:19:32.522464 140414157813504 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.043415673077106476, loss=0.045366473495960236
I0418 14:19:55.205639 140414149420800 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04411594569683075, loss=0.048087529838085175
I0418 14:20:17.832182 140414157813504 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0505802147090435, loss=0.04565385356545448
I0418 14:20:40.203587 140414149420800 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.04861943796277046, loss=0.041005268692970276
I0418 14:21:02.986469 140414157813504 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.07708760350942612, loss=0.04434461146593094
I0418 14:21:27.144678 140414149420800 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.06630991399288177, loss=0.04582206904888153
I0418 14:21:53.796097 140414157813504 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.18484364449977875, loss=0.048849787563085556
I0418 14:21:57.501581 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:23:09.206186 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:23:11.726933 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:23:14.172053 140598912194368 submission_runner.py:406] Time since start: 2133.12s, 	Step: 6315, 	{'train/accuracy': 0.9880219101905823, 'train/loss': 0.04172036424279213, 'train/mean_average_precision': 0.1744965010496106, 'validation/accuracy': 0.9851802587509155, 'validation/loss': 0.05133609473705292, 'validation/mean_average_precision': 0.15425384267947756, 'validation/num_examples': 43793, 'test/accuracy': 0.984219491481781, 'test/loss': 0.05421918258070946, 'test/mean_average_precision': 0.15725323646577458, 'test/num_examples': 43793, 'score': 1460.2777907848358, 'total_duration': 2133.1204228401184, 'accumulated_submission_time': 1460.2777907848358, 'accumulated_eval_time': 672.2363300323486, 'accumulated_logging_time': 0.557898998260498}
I0418 14:23:14.179899 140414149420800 logging_writer.py:48] [6315] accumulated_eval_time=672.236330, accumulated_logging_time=0.557899, accumulated_submission_time=1460.277791, global_step=6315, preemption_count=0, score=1460.277791, test/accuracy=0.984219, test/loss=0.054219, test/mean_average_precision=0.157253, test/num_examples=43793, total_duration=2133.120423, train/accuracy=0.988022, train/loss=0.041720, train/mean_average_precision=0.174497, validation/accuracy=0.985180, validation/loss=0.051336, validation/mean_average_precision=0.154254, validation/num_examples=43793
I0418 14:23:14.204886 140598912194368 checkpoints.py:356] Saving checkpoint at step: 6315
I0418 14:23:14.273598 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_6315
I0418 14:23:14.273815 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_6315.
I0418 14:23:33.781034 140414157813504 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.13720308244228363, loss=0.04796299710869789
I0418 14:23:56.398725 140414141028096 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.058935534209012985, loss=0.04194316267967224
I0418 14:24:19.365633 140414157813504 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0426151305437088, loss=0.04435949772596359
I0418 14:24:41.896825 140414141028096 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.058404915034770966, loss=0.04512615129351616
I0418 14:25:04.530632 140414157813504 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.04411369189620018, loss=0.0453469417989254
I0418 14:25:27.830629 140414141028096 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.07366923242807388, loss=0.045596152544021606
I0418 14:25:51.530988 140414157813504 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.061517536640167236, loss=0.042763907462358475
I0418 14:26:14.620463 140414141028096 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.042434923350811005, loss=0.04565856233239174
I0418 14:26:37.681211 140414157813504 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.10013747960329056, loss=0.04838252067565918
I0418 14:27:01.221652 140414141028096 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0447196327149868, loss=0.04219415783882141
I0418 14:27:14.436305 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:28:26.294895 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:28:28.822248 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:28:31.270879 140598912194368 submission_runner.py:406] Time since start: 2450.22s, 	Step: 7358, 	{'train/accuracy': 0.9883137941360474, 'train/loss': 0.040378108620643616, 'train/mean_average_precision': 0.19876115770734815, 'validation/accuracy': 0.9854802489280701, 'validation/loss': 0.05016922205686569, 'validation/mean_average_precision': 0.1717316876058707, 'validation/num_examples': 43793, 'test/accuracy': 0.9844481945037842, 'test/loss': 0.05315406247973442, 'test/mean_average_precision': 0.16888058283235668, 'test/num_examples': 43793, 'score': 1700.4308385849, 'total_duration': 2450.219235420227, 'accumulated_submission_time': 1700.4308385849, 'accumulated_eval_time': 749.0708730220795, 'accumulated_logging_time': 0.6598460674285889}
I0418 14:28:31.279021 140414157813504 logging_writer.py:48] [7358] accumulated_eval_time=749.070873, accumulated_logging_time=0.659846, accumulated_submission_time=1700.430839, global_step=7358, preemption_count=0, score=1700.430839, test/accuracy=0.984448, test/loss=0.053154, test/mean_average_precision=0.168881, test/num_examples=43793, total_duration=2450.219235, train/accuracy=0.988314, train/loss=0.040378, train/mean_average_precision=0.198761, validation/accuracy=0.985480, validation/loss=0.050169, validation/mean_average_precision=0.171732, validation/num_examples=43793
I0418 14:28:31.303801 140598912194368 checkpoints.py:356] Saving checkpoint at step: 7358
I0418 14:28:31.360291 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_7358
I0418 14:28:31.360505 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_7358.
I0418 14:28:41.212462 140414141028096 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.08173049986362457, loss=0.045974548906087875
I0418 14:29:04.420150 140414132635392 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.04007255285978317, loss=0.04507075250148773
I0418 14:29:27.446005 140414141028096 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0673823207616806, loss=0.0446670725941658
I0418 14:29:50.187764 140414132635392 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.05129237473011017, loss=0.0444466769695282
I0418 14:30:12.982658 140414141028096 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.05400572717189789, loss=0.04417409747838974
I0418 14:30:35.686509 140414132635392 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.03725815936923027, loss=0.04075387492775917
I0418 14:30:58.482594 140414141028096 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.06881548464298248, loss=0.04384384676814079
I0418 14:31:20.989232 140414132635392 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0487753190100193, loss=0.04164177179336548
I0418 14:31:43.364047 140414141028096 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.06844473630189896, loss=0.04581572860479355
I0418 14:32:05.670632 140414132635392 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.06772930175065994, loss=0.04304259642958641
I0418 14:32:27.976710 140414141028096 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.04370446130633354, loss=0.041860662400722504
I0418 14:32:31.534235 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:33:41.116407 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:33:43.588913 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:33:46.021829 140598912194368 submission_runner.py:406] Time since start: 2764.97s, 	Step: 8417, 	{'train/accuracy': 0.9884981513023376, 'train/loss': 0.03954225033521652, 'train/mean_average_precision': 0.2170284207415576, 'validation/accuracy': 0.9853353500366211, 'validation/loss': 0.049768559634685516, 'validation/mean_average_precision': 0.18362715727737117, 'validation/num_examples': 43793, 'test/accuracy': 0.9844368100166321, 'test/loss': 0.052610382437705994, 'test/mean_average_precision': 0.18050683981554205, 'test/num_examples': 43793, 'score': 1940.5963053703308, 'total_duration': 2764.9702003002167, 'accumulated_submission_time': 1940.5963053703308, 'accumulated_eval_time': 823.5584321022034, 'accumulated_logging_time': 0.7496092319488525}
I0418 14:33:46.030030 140414132635392 logging_writer.py:48] [8417] accumulated_eval_time=823.558432, accumulated_logging_time=0.749609, accumulated_submission_time=1940.596305, global_step=8417, preemption_count=0, score=1940.596305, test/accuracy=0.984437, test/loss=0.052610, test/mean_average_precision=0.180507, test/num_examples=43793, total_duration=2764.970200, train/accuracy=0.988498, train/loss=0.039542, train/mean_average_precision=0.217028, validation/accuracy=0.985335, validation/loss=0.049769, validation/mean_average_precision=0.183627, validation/num_examples=43793
I0418 14:33:46.054141 140598912194368 checkpoints.py:356] Saving checkpoint at step: 8417
I0418 14:33:46.109841 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_8417
I0418 14:33:46.110005 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_8417.
I0418 14:34:04.888457 140414141028096 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.04507046192884445, loss=0.04054731875658035
I0418 14:34:26.800864 140414124242688 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03810252621769905, loss=0.03914770856499672
I0418 14:34:48.794457 140414141028096 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.06702440977096558, loss=0.04647410288453102
I0418 14:35:11.526506 140414124242688 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0470450222492218, loss=0.041558269411325455
I0418 14:35:33.622288 140414141028096 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.057279303669929504, loss=0.04460322484374046
I0418 14:35:55.655109 140414124242688 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.07206898927688599, loss=0.045246899127960205
I0418 14:36:17.903241 140414141028096 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.04189597815275192, loss=0.044740356504917145
I0418 14:36:40.148350 140414124242688 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.06285468488931656, loss=0.04022856056690216
I0418 14:37:02.286298 140414141028096 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.05596234276890755, loss=0.041725680232048035
I0418 14:37:24.160382 140414124242688 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.04298346862196922, loss=0.04115670919418335
I0418 14:37:46.325676 140414141028096 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.049529366195201874, loss=0.04209781065583229
I0418 14:37:46.329537 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:38:56.789785 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:38:59.285046 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:39:01.957782 140598912194368 submission_runner.py:406] Time since start: 3080.91s, 	Step: 9501, 	{'train/accuracy': 0.9889048337936401, 'train/loss': 0.038169775158166885, 'train/mean_average_precision': 0.2348096489094842, 'validation/accuracy': 0.985857367515564, 'validation/loss': 0.048387449234724045, 'validation/mean_average_precision': 0.19314761227856478, 'validation/num_examples': 43793, 'test/accuracy': 0.9848765730857849, 'test/loss': 0.051004454493522644, 'test/mean_average_precision': 0.19512524718764632, 'test/num_examples': 43793, 'score': 2180.8075652122498, 'total_duration': 3080.906135082245, 'accumulated_submission_time': 2180.8075652122498, 'accumulated_eval_time': 899.186606168747, 'accumulated_logging_time': 0.8378987312316895}
I0418 14:39:01.966182 140414124242688 logging_writer.py:48] [9501] accumulated_eval_time=899.186606, accumulated_logging_time=0.837899, accumulated_submission_time=2180.807565, global_step=9501, preemption_count=0, score=2180.807565, test/accuracy=0.984877, test/loss=0.051004, test/mean_average_precision=0.195125, test/num_examples=43793, total_duration=3080.906135, train/accuracy=0.988905, train/loss=0.038170, train/mean_average_precision=0.234810, validation/accuracy=0.985857, validation/loss=0.048387, validation/mean_average_precision=0.193148, validation/num_examples=43793
I0418 14:39:01.992518 140598912194368 checkpoints.py:356] Saving checkpoint at step: 9501
I0418 14:39:02.049459 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_9501
I0418 14:39:02.049684 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_9501.
I0418 14:39:23.981935 140414141028096 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.04909771680831909, loss=0.045790329575538635
I0418 14:39:46.273657 140414115849984 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.04554268717765808, loss=0.03960869088768959
I0418 14:40:08.509853 140414141028096 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.036063313484191895, loss=0.039152588695287704
I0418 14:40:30.456398 140414115849984 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.04446318373084068, loss=0.04138577729463577
I0418 14:40:52.447091 140414141028096 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0502791665494442, loss=0.04198732599616051
I0418 14:41:14.373607 140414115849984 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.06746986508369446, loss=0.04463993385434151
I0418 14:41:36.125918 140414141028096 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.06010384485125542, loss=0.04305730760097504
I0418 14:41:57.902403 140414115849984 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.069786936044693, loss=0.043702252209186554
I0418 14:42:19.845233 140414141028096 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.05342790484428406, loss=0.03855196014046669
I0418 14:42:41.911876 140414115849984 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.04366990551352501, loss=0.04327474161982536
I0418 14:43:02.218064 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:44:11.185044 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:44:13.674223 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:44:16.090986 140598912194368 submission_runner.py:406] Time since start: 3395.04s, 	Step: 10592, 	{'train/accuracy': 0.9892134070396423, 'train/loss': 0.03704057261347771, 'train/mean_average_precision': 0.2548034020337374, 'validation/accuracy': 0.9860055446624756, 'validation/loss': 0.047551535069942474, 'validation/mean_average_precision': 0.2055965982516159, 'validation/num_examples': 43793, 'test/accuracy': 0.9851073622703552, 'test/loss': 0.05013575404882431, 'test/mean_average_precision': 0.20804380687719953, 'test/num_examples': 43793, 'score': 2420.9676554203033, 'total_duration': 3395.0393345355988, 'accumulated_submission_time': 2420.9676554203033, 'accumulated_eval_time': 973.0594685077667, 'accumulated_logging_time': 0.9299914836883545}
I0418 14:44:16.098981 140414141028096 logging_writer.py:48] [10592] accumulated_eval_time=973.059469, accumulated_logging_time=0.929991, accumulated_submission_time=2420.967655, global_step=10592, preemption_count=0, score=2420.967655, test/accuracy=0.985107, test/loss=0.050136, test/mean_average_precision=0.208044, test/num_examples=43793, total_duration=3395.039335, train/accuracy=0.989213, train/loss=0.037041, train/mean_average_precision=0.254803, validation/accuracy=0.986006, validation/loss=0.047552, validation/mean_average_precision=0.205597, validation/num_examples=43793
I0418 14:44:16.124861 140598912194368 checkpoints.py:356] Saving checkpoint at step: 10592
I0418 14:44:16.182855 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_10592
I0418 14:44:16.183059 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_10592.
I0418 14:44:18.176678 140414115849984 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.060684673488140106, loss=0.04450768232345581
I0418 14:44:40.519747 140414107457280 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.06268947571516037, loss=0.041109923273324966
I0418 14:45:02.846711 140414115849984 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0440826378762722, loss=0.038697633892297745
I0418 14:45:25.021343 140414107457280 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.05729472637176514, loss=0.041582655161619186
I0418 14:45:47.343601 140414115849984 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.055089518427848816, loss=0.04345376789569855
I0418 14:46:09.602482 140414107457280 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03751390427350998, loss=0.039700232446193695
I0418 14:46:31.637907 140414115849984 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.050927381962537766, loss=0.03975212201476097
I0418 14:46:53.941271 140414107457280 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.060119740664958954, loss=0.0475749708712101
I0418 14:47:16.927440 140414115849984 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.060335464775562286, loss=0.04405675828456879
I0418 14:47:39.393769 140414107457280 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.04694310203194618, loss=0.04162735119462013
I0418 14:48:01.798260 140414115849984 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.07466951012611389, loss=0.039993155747652054
I0418 14:48:16.286606 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:49:28.868809 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:49:31.347954 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:49:33.788491 140598912194368 submission_runner.py:406] Time since start: 3712.74s, 	Step: 11667, 	{'train/accuracy': 0.9888572096824646, 'train/loss': 0.037298671901226044, 'train/mean_average_precision': 0.26751090449165044, 'validation/accuracy': 0.9859389662742615, 'validation/loss': 0.04783329367637634, 'validation/mean_average_precision': 0.21787094044586483, 'validation/num_examples': 43793, 'test/accuracy': 0.9849510788917542, 'test/loss': 0.0507492832839489, 'test/mean_average_precision': 0.21182906405747753, 'test/num_examples': 43793, 'score': 2661.0627822875977, 'total_duration': 3712.7368397712708, 'accumulated_submission_time': 2661.0627822875977, 'accumulated_eval_time': 1050.5612955093384, 'accumulated_logging_time': 1.0221798419952393}
I0418 14:49:33.796289 140414107457280 logging_writer.py:48] [11667] accumulated_eval_time=1050.561296, accumulated_logging_time=1.022180, accumulated_submission_time=2661.062782, global_step=11667, preemption_count=0, score=2661.062782, test/accuracy=0.984951, test/loss=0.050749, test/mean_average_precision=0.211829, test/num_examples=43793, total_duration=3712.736840, train/accuracy=0.988857, train/loss=0.037299, train/mean_average_precision=0.267511, validation/accuracy=0.985939, validation/loss=0.047833, validation/mean_average_precision=0.217871, validation/num_examples=43793
I0418 14:49:33.820966 140598912194368 checkpoints.py:356] Saving checkpoint at step: 11667
I0418 14:49:33.879712 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_11667
I0418 14:49:33.879897 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_11667.
I0418 14:49:41.313261 140414115849984 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.06790520995855331, loss=0.03992072492837906
I0418 14:50:03.924095 140414023628544 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.04393516853451729, loss=0.03845975548028946
I0418 14:50:26.196491 140414115849984 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.06220576539635658, loss=0.04376071318984032
I0418 14:50:48.223698 140598912194368 spec.py:298] Evaluating on the training split.
I0418 14:51:55.787655 140598912194368 spec.py:310] Evaluating on the validation split.
I0418 14:51:58.268501 140598912194368 spec.py:326] Evaluating on the test split.
I0418 14:52:00.694553 140598912194368 submission_runner.py:406] Time since start: 3859.64s, 	Step: 12000, 	{'train/accuracy': 0.9889175891876221, 'train/loss': 0.037058211863040924, 'train/mean_average_precision': 0.2732697603695987, 'validation/accuracy': 0.985923171043396, 'validation/loss': 0.04798232764005661, 'validation/mean_average_precision': 0.21045080335075714, 'validation/num_examples': 43793, 'test/accuracy': 0.9849094152450562, 'test/loss': 0.050874292850494385, 'test/mean_average_precision': 0.20864109354112553, 'test/num_examples': 43793, 'score': 2735.4038186073303, 'total_duration': 3859.6428830623627, 'accumulated_submission_time': 2735.4038186073303, 'accumulated_eval_time': 1123.0320718288422, 'accumulated_logging_time': 1.1136713027954102}
I0418 14:52:00.702764 140414023628544 logging_writer.py:48] [12000] accumulated_eval_time=1123.032072, accumulated_logging_time=1.113671, accumulated_submission_time=2735.403819, global_step=12000, preemption_count=0, score=2735.403819, test/accuracy=0.984909, test/loss=0.050874, test/mean_average_precision=0.208641, test/num_examples=43793, total_duration=3859.642883, train/accuracy=0.988918, train/loss=0.037058, train/mean_average_precision=0.273270, validation/accuracy=0.985923, validation/loss=0.047982, validation/mean_average_precision=0.210451, validation/num_examples=43793
I0418 14:52:00.726813 140598912194368 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:52:00.795516 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000
I0418 14:52:00.795705 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:52:00.802782 140414115849984 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2735.403819
I0418 14:52:00.820607 140598912194368 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:52:00.911322 140598912194368 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000
I0418 14:52:00.911545 140598912194368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:52:01.046382 140598912194368 submission_runner.py:567] Tuning trial 1/1
I0418 14:52:01.046596 140598912194368 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0418 14:52:01.047600 140598912194368 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5851648449897766, 'train/loss': 0.7420417070388794, 'train/mean_average_precision': 0.021668887065090927, 'validation/accuracy': 0.5899277925491333, 'validation/loss': 0.7371640205383301, 'validation/mean_average_precision': 0.025907461759495806, 'validation/num_examples': 43793, 'test/accuracy': 0.592805802822113, 'test/loss': 0.7339466214179993, 'test/mean_average_precision': 0.027859805599282896, 'test/num_examples': 43793, 'score': 19.426052808761597, 'total_duration': 233.8751618862152, 'accumulated_submission_time': 19.426052808761597, 'accumulated_eval_time': 214.4489245414734, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1065, {'train/accuracy': 0.9867406487464905, 'train/loss': 0.05441141128540039, 'train/mean_average_precision': 0.03644545994948914, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06470400840044022, 'validation/mean_average_precision': 0.03963068704622649, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0680670365691185, 'test/mean_average_precision': 0.041031736552491434, 'test/num_examples': 43793, 'score': 259.6519031524658, 'total_duration': 550.319061756134, 'accumulated_submission_time': 259.6519031524658, 'accumulated_eval_time': 290.55273270606995, 'accumulated_logging_time': 0.1061239242553711, 'global_step': 1065, 'preemption_count': 0}), (2127, {'train/accuracy': 0.9867504239082336, 'train/loss': 0.051603276282548904, 'train/mean_average_precision': 0.05766840858766916, 'validation/accuracy': 0.984119176864624, 'validation/loss': 0.0613492988049984, 'validation/mean_average_precision': 0.05522809789741908, 'validation/num_examples': 43793, 'test/accuracy': 0.9831412434577942, 'test/loss': 0.06458266824483871, 'test/mean_average_precision': 0.055741406756206306, 'test/num_examples': 43793, 'score': 499.8357071876526, 'total_duration': 867.2417707443237, 'accumulated_submission_time': 499.8357071876526, 'accumulated_eval_time': 367.18688893318176, 'accumulated_logging_time': 0.2029876708984375, 'global_step': 2127, 'preemption_count': 0}), (3170, {'train/accuracy': 0.9871504902839661, 'train/loss': 0.047445327043533325, 'train/mean_average_precision': 0.0991868137650982, 'validation/accuracy': 0.9843952059745789, 'validation/loss': 0.05740876495838165, 'validation/mean_average_precision': 0.09603262721468366, 'validation/num_examples': 43793, 'test/accuracy': 0.9833973050117493, 'test/loss': 0.06043630838394165, 'test/mean_average_precision': 0.10122517150408011, 'test/num_examples': 43793, 'score': 739.8623418807983, 'total_duration': 1183.3465030193329, 'accumulated_submission_time': 739.8623418807983, 'accumulated_eval_time': 443.1636381149292, 'accumulated_logging_time': 0.2963860034942627, 'global_step': 3170, 'preemption_count': 0}), (4229, {'train/accuracy': 0.9874129891395569, 'train/loss': 0.04666562378406525, 'train/mean_average_precision': 0.11644610856421783, 'validation/accuracy': 0.9847304821014404, 'validation/loss': 0.05501586198806763, 'validation/mean_average_precision': 0.11156971337446672, 'validation/num_examples': 43793, 'test/accuracy': 0.9836935997009277, 'test/loss': 0.05777660384774208, 'test/mean_average_precision': 0.11503555563380898, 'test/num_examples': 43793, 'score': 979.9274423122406, 'total_duration': 1499.727421283722, 'accumulated_submission_time': 979.9274423122406, 'accumulated_eval_time': 519.3831729888916, 'accumulated_logging_time': 0.38455820083618164, 'global_step': 4229, 'preemption_count': 0}), (5283, {'train/accuracy': 0.9875574707984924, 'train/loss': 0.04382091388106346, 'train/mean_average_precision': 0.14701590786434002, 'validation/accuracy': 0.9848185777664185, 'validation/loss': 0.05372907221317291, 'validation/mean_average_precision': 0.13599771868973828, 'validation/num_examples': 43793, 'test/accuracy': 0.9838336706161499, 'test/loss': 0.05673304572701454, 'test/mean_average_precision': 0.13803483870319536, 'test/num_examples': 43793, 'score': 1220.0867431163788, 'total_duration': 1816.1643269062042, 'accumulated_submission_time': 1220.0867431163788, 'accumulated_eval_time': 595.5658853054047, 'accumulated_logging_time': 0.471343994140625, 'global_step': 5283, 'preemption_count': 0}), (6315, {'train/accuracy': 0.9880219101905823, 'train/loss': 0.04172036424279213, 'train/mean_average_precision': 0.1744965010496106, 'validation/accuracy': 0.9851802587509155, 'validation/loss': 0.05133609473705292, 'validation/mean_average_precision': 0.15425384267947756, 'validation/num_examples': 43793, 'test/accuracy': 0.984219491481781, 'test/loss': 0.05421918258070946, 'test/mean_average_precision': 0.15725323646577458, 'test/num_examples': 43793, 'score': 1460.2777907848358, 'total_duration': 2133.1204228401184, 'accumulated_submission_time': 1460.2777907848358, 'accumulated_eval_time': 672.2363300323486, 'accumulated_logging_time': 0.557898998260498, 'global_step': 6315, 'preemption_count': 0}), (7358, {'train/accuracy': 0.9883137941360474, 'train/loss': 0.040378108620643616, 'train/mean_average_precision': 0.19876115770734815, 'validation/accuracy': 0.9854802489280701, 'validation/loss': 0.05016922205686569, 'validation/mean_average_precision': 0.1717316876058707, 'validation/num_examples': 43793, 'test/accuracy': 0.9844481945037842, 'test/loss': 0.05315406247973442, 'test/mean_average_precision': 0.16888058283235668, 'test/num_examples': 43793, 'score': 1700.4308385849, 'total_duration': 2450.219235420227, 'accumulated_submission_time': 1700.4308385849, 'accumulated_eval_time': 749.0708730220795, 'accumulated_logging_time': 0.6598460674285889, 'global_step': 7358, 'preemption_count': 0}), (8417, {'train/accuracy': 0.9884981513023376, 'train/loss': 0.03954225033521652, 'train/mean_average_precision': 0.2170284207415576, 'validation/accuracy': 0.9853353500366211, 'validation/loss': 0.049768559634685516, 'validation/mean_average_precision': 0.18362715727737117, 'validation/num_examples': 43793, 'test/accuracy': 0.9844368100166321, 'test/loss': 0.052610382437705994, 'test/mean_average_precision': 0.18050683981554205, 'test/num_examples': 43793, 'score': 1940.5963053703308, 'total_duration': 2764.9702003002167, 'accumulated_submission_time': 1940.5963053703308, 'accumulated_eval_time': 823.5584321022034, 'accumulated_logging_time': 0.7496092319488525, 'global_step': 8417, 'preemption_count': 0}), (9501, {'train/accuracy': 0.9889048337936401, 'train/loss': 0.038169775158166885, 'train/mean_average_precision': 0.2348096489094842, 'validation/accuracy': 0.985857367515564, 'validation/loss': 0.048387449234724045, 'validation/mean_average_precision': 0.19314761227856478, 'validation/num_examples': 43793, 'test/accuracy': 0.9848765730857849, 'test/loss': 0.051004454493522644, 'test/mean_average_precision': 0.19512524718764632, 'test/num_examples': 43793, 'score': 2180.8075652122498, 'total_duration': 3080.906135082245, 'accumulated_submission_time': 2180.8075652122498, 'accumulated_eval_time': 899.186606168747, 'accumulated_logging_time': 0.8378987312316895, 'global_step': 9501, 'preemption_count': 0}), (10592, {'train/accuracy': 0.9892134070396423, 'train/loss': 0.03704057261347771, 'train/mean_average_precision': 0.2548034020337374, 'validation/accuracy': 0.9860055446624756, 'validation/loss': 0.047551535069942474, 'validation/mean_average_precision': 0.2055965982516159, 'validation/num_examples': 43793, 'test/accuracy': 0.9851073622703552, 'test/loss': 0.05013575404882431, 'test/mean_average_precision': 0.20804380687719953, 'test/num_examples': 43793, 'score': 2420.9676554203033, 'total_duration': 3395.0393345355988, 'accumulated_submission_time': 2420.9676554203033, 'accumulated_eval_time': 973.0594685077667, 'accumulated_logging_time': 0.9299914836883545, 'global_step': 10592, 'preemption_count': 0}), (11667, {'train/accuracy': 0.9888572096824646, 'train/loss': 0.037298671901226044, 'train/mean_average_precision': 0.26751090449165044, 'validation/accuracy': 0.9859389662742615, 'validation/loss': 0.04783329367637634, 'validation/mean_average_precision': 0.21787094044586483, 'validation/num_examples': 43793, 'test/accuracy': 0.9849510788917542, 'test/loss': 0.0507492832839489, 'test/mean_average_precision': 0.21182906405747753, 'test/num_examples': 43793, 'score': 2661.0627822875977, 'total_duration': 3712.7368397712708, 'accumulated_submission_time': 2661.0627822875977, 'accumulated_eval_time': 1050.5612955093384, 'accumulated_logging_time': 1.0221798419952393, 'global_step': 11667, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9889175891876221, 'train/loss': 0.037058211863040924, 'train/mean_average_precision': 0.2732697603695987, 'validation/accuracy': 0.985923171043396, 'validation/loss': 0.04798232764005661, 'validation/mean_average_precision': 0.21045080335075714, 'validation/num_examples': 43793, 'test/accuracy': 0.9849094152450562, 'test/loss': 0.050874292850494385, 'test/mean_average_precision': 0.20864109354112553, 'test/num_examples': 43793, 'score': 2735.4038186073303, 'total_duration': 3859.6428830623627, 'accumulated_submission_time': 2735.4038186073303, 'accumulated_eval_time': 1123.0320718288422, 'accumulated_logging_time': 1.1136713027954102, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0418 14:52:01.047730 140598912194368 submission_runner.py:570] Timing: 2735.4038186073303
I0418 14:52:01.047781 140598912194368 submission_runner.py:571] ====================
I0418 14:52:01.047900 140598912194368 submission_runner.py:631] Final ogbg score: 2735.4038186073303
