torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_timing/adamw --overwrite=True --save_checkpoints=False --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=True 2>&1 | tee -a /logs/imagenet_resnet_pytorch_07-26-2023-03-24-24.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-26 03:24:34.624765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 03:24:34.624763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0726 03:24:49.546348 139993717790528 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0726 03:24:49.546385 140550474999616 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0726 03:24:49.546411 139820978124608 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0726 03:24:49.547331 139997138270016 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0726 03:24:49.547532 140295211624256 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0726 03:24:49.547511 139899946698560 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0726 03:24:49.547680 139642652653376 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0726 03:24:49.547918 139899946698560 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.547948 139642652653376 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.547920 140284670572352 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0726 03:24:49.548223 140284670572352 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.556929 139993717790528 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.557032 140550474999616 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.557058 139820978124608 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.557934 139997138270016 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 03:24:49.558108 140295211624256 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0726 03:24:50.911121 140284670572352 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch because --overwrite was set.
I0726 03:24:50.913271 140284670572352 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch.
W0726 03:24:50.948829 139997138270016 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.948829 140550474999616 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.948830 139820978124608 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.949025 139642652653376 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.949582 140295211624256 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.950031 139993717790528 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.950838 140284670572352 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 03:24:50.952640 139899946698560 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0726 03:24:50.956192 140284670572352 submission_runner.py:490] Using RNG seed 3303172248
I0726 03:24:50.957656 140284670572352 submission_runner.py:499] --- Tuning run 1/1 ---
I0726 03:24:50.957772 140284670572352 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch/trial_1.
I0726 03:24:50.958052 140284670572352 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0726 03:24:50.958961 140284670572352 submission_runner.py:176] Initializing dataset.
I0726 03:24:57.461532 140284670572352 submission_runner.py:183] Initializing model.
I0726 03:25:02.225549 140284670572352 submission_runner.py:214] Performing `torch.compile`.
I0726 03:25:02.822910 140284670572352 submission_runner.py:217] Initializing optimizer.
I0726 03:25:02.824352 140284670572352 submission_runner.py:224] Initializing metrics bundle.
I0726 03:25:02.824471 140284670572352 submission_runner.py:242] Initializing checkpoint and logger.
I0726 03:25:03.312861 140284670572352 submission_runner.py:263] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0726 03:25:03.313750 140284670572352 submission_runner.py:266] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0726 03:25:03.398213 140284670572352 submission_runner.py:276] Starting training loop.
[2023-07-26 03:25:05,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:05,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:25:07,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,510] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,511] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,519] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,543] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,548] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,570] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,659] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,663] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,664] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,686] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,690] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,690] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,760] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,763] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,764] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:07,799] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,803] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,804] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,856] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:07,860] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:07,861] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:07,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:25:08,002] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:25:08,005] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:25:08,006] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:25:18,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:18,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 03:25:27,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:27,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 03:25:32,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,323] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:32,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 03:25:33,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:33,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:33,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:34,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:34,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:34,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:34,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:34,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 03:25:35,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,136] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 03:25:35,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:35,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:36,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:36,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:36,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:36,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:36,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:36,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 03:25:37,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:37,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:37,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:37,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:37,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:37,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:38,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:38,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 03:25:38,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:38,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:38,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:38,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:38,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:38,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:38,929] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:38,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:38,937] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:38,944] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:38,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:38,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:38,977] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:38,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:38,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:39,000] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:39,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:39,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:39,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:39,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:39,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 03:25:39,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:39,249] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:39,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:39,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:39,263] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:39,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:39,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:39,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:39,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 03:25:39,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 03:25:39,389] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:25:45,286] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 03:25:45,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:45,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 03:25:46,104] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:46,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 03:25:47,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:47,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 03:25:48,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:48,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 03:25:49,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:49,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 03:25:50,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:51,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 03:25:53,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,553] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:53,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 03:25:54,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,877] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:25:54,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 03:26:00,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 03:26:00,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0726 03:26:04.174640 140257341183744 logging_writer.py:48] [0] global_step=0, grad_norm=0.592319, loss=6.919806
I0726 03:26:04.209877 140284670572352 submission.py:119] 0) loss = 6.920, grad_norm = 0.592
I0726 03:26:04.211216 140284670572352 spec.py:320] Evaluating on the training split.
[2023-07-26 03:26:14,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,337] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:14,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:26:15,778] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:15,799] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:15,802] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:15,803] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,314] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,318] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,322] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,322] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,336] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,340] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,340] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,402] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,427] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,454] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,455] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,459] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,459] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,467] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,475] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,478] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,479] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,488] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,491] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:16,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:26:16,969] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:26:16,973] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:26:16,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:26:18,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:18,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:19,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:19,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:19,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:19,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:19,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:19,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 03:26:22,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:22,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:22,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:22,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:22,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:22,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:22,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:23,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 03:26:24,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:24,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:24,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:24,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:24,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:24,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:24,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:25,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 03:26:25,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:25,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:25,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:26,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:26,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:26,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:26,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:26,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:26,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 03:26:27,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:27,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 03:26:27,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:27,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:27,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:27,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:27,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:27,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:28,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:28,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 03:26:28,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,527] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:28,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,586] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:28,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:28,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,726] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:28,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,745] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:28,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:28,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 03:26:28,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,823] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:28,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:28,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:28,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:26:29,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 03:26:29,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 03:26:29,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 03:26:29,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0726 03:27:20.348598 140284670572352 spec.py:332] Evaluating on the validation split.
[2023-07-26 03:28:12,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:12,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:12,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:12,843] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:12,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:13,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:13,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:13,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:13,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:13,863] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:13,881] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:13,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:13,884] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:13,885] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:13,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:13,887] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:14,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:14,050] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:14,053] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:14,054] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:14,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:14,192] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:14,195] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:14,196] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:14,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:14,230] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:14,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:14,234] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:14,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:14,486] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:14,489] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:14,489] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:14,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:14,621] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:14,624] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:14,625] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:14,833] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:14,853] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:14,856] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:14,856] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:16,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:16,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:16,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:16,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:16,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:17,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:17,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:17,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 03:28:19,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:19,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:19,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:19,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:19,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:20,279] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:20,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:20,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 03:28:21,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:21,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:21,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:21,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:22,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:22,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:22,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:22,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 03:28:22,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:22,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:23,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:23,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:23,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:23,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:23,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:23,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:23,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:23,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:23,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:24,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 03:28:24,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:24,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:24,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:24,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 03:28:24,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:24,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:24,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:24,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:25,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:25,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 03:28:25,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:25,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:25,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:25,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:25,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:25,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:25,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:25,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:25,612] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:25,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:25,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:25,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:25,697] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:25,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:25,704] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:25,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:25,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 03:28:25,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:25,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:25,894] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:26,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:26,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:26,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:26,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:26,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:26,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:26,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:26,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:26,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:26,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:26,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:26,340] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:26,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 03:28:26,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 03:28:26,591] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 03:28:26,596] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0726 03:28:28.343701 140284670572352 spec.py:348] Evaluating on the test split.
I0726 03:28:28.360582 140284670572352 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0726 03:28:28.366705 140284670572352 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0726 03:28:28.435183 140284670572352 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-07-26 03:28:30,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:30,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:30,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:30,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:30,683] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:30,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:30,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:31,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:28:33,486] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,509] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,510] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,546] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,565] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,568] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,604] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,605] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,607] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,620] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,623] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,624] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,731] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,731] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,794] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:28:33,813] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,816] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,816] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:33,820] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:28:33,823] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:28:33,823] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:28:36,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,136] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:36,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 03:28:42,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:42,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:42,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:42,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:43,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:43,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:43,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:43,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 03:28:44,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:44,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:45,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:45,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:45,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:45,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:45,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:45,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 03:28:49,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:49,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 03:28:50,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 03:28:50,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:50,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:50,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:50,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:51,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:51,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:51,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:51,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 03:28:51,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:51,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 03:28:53,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:53,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:53,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:53,933] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,481] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,669] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 03:28:54,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,720] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,748] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:28:54,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 03:28:54,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 03:28:54,809] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:04,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:05,788] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:06,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:06,789] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:06,843] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:06,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:06,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:06,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 03:29:07,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:07,273] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:07,276] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:07,277] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:07,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:07,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:07,669] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:07,669] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:07,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:07,962] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:07,965] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:07,966] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:08,175] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:08,196] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:08,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:08,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:08,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:08,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:08,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:08,218] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:08,219] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:08,233] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:08,236] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:08,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:08,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:08,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 03:29:08,316] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:08,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:08,320] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:08,322] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 03:29:08,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 03:29:08,325] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 03:29:10,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:10,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 03:29:14,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:14,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:15,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:15,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:15,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:15,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:15,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:15,279] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 03:29:16,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:16,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:17,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:17,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:17,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:17,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:17,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:17,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 03:29:19,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:19,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:20,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:20,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 03:29:20,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:20,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:20,805] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:20,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:20,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:20,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:21,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:21,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 03:29:21,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:21,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:21,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:21,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:21,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:21,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:21,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:21,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 03:29:21,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:21,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:21,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:22,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:22,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:22,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 03:29:22,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:22,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:22,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,005] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,136] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,634] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,715] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 03:29:23,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,915] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 03:29:23,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 03:29:23,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 03:29:23,975] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0726 03:29:28.315409 140284670572352 submission_runner.py:364] Time since start: 264.92s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.924272809709821, 'validation/accuracy': 0.00114, 'validation/loss': 6.924665625, 'validation/num_examples': 50000, 'test/accuracy': 0.0008, 'test/loss': 6.92733515625, 'test/num_examples': 10000, 'score': 60.81288266181946, 'total_duration': 264.91762137413025, 'accumulated_submission_time': 60.81288266181946, 'accumulated_eval_time': 204.10420083999634, 'accumulated_logging_time': 0}
I0726 03:29:28.334497 140241478346496 logging_writer.py:48] [1] accumulated_eval_time=204.104201, accumulated_logging_time=0, accumulated_submission_time=60.812883, global_step=1, preemption_count=0, score=60.812883, test/accuracy=0.000800, test/loss=6.927335, test/num_examples=10000, total_duration=264.917621, train/accuracy=0.000977, train/loss=6.924273, validation/accuracy=0.001140, validation/loss=6.924666, validation/num_examples=50000
I0726 03:29:28.363185 139997138270016 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.363223 140284670572352 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.363542 139899946698560 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.363584 139642652653376 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.363505 139993717790528 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.363626 140295211624256 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.365996 139820978124608 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 03:29:28.365996 140550474999616 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I0726 03:29:28.910502 140241469953792 logging_writer.py:48] [1] global_step=1, grad_norm=0.625066, loss=6.931113
I0726 03:29:28.914175 140284670572352 submission.py:119] 1) loss = 6.931, grad_norm = 0.625
I0726 03:29:29.258079 140241478346496 logging_writer.py:48] [2] global_step=2, grad_norm=0.609055, loss=6.927052
I0726 03:29:29.261943 140284670572352 submission.py:119] 2) loss = 6.927, grad_norm = 0.609
I0726 03:29:29.605739 140241469953792 logging_writer.py:48] [3] global_step=3, grad_norm=0.620541, loss=6.936377
I0726 03:29:29.609485 140284670572352 submission.py:119] 3) loss = 6.936, grad_norm = 0.621
I0726 03:29:29.956087 140241478346496 logging_writer.py:48] [4] global_step=4, grad_norm=0.598946, loss=6.912228
I0726 03:29:29.961634 140284670572352 submission.py:119] 4) loss = 6.912, grad_norm = 0.599
I0726 03:29:30.309056 140241469953792 logging_writer.py:48] [5] global_step=5, grad_norm=0.605583, loss=6.931119
I0726 03:29:30.314245 140284670572352 submission.py:119] 5) loss = 6.931, grad_norm = 0.606
I0726 03:29:30.664197 140241478346496 logging_writer.py:48] [6] global_step=6, grad_norm=0.619158, loss=6.927839
I0726 03:29:30.668446 140284670572352 submission.py:119] 6) loss = 6.928, grad_norm = 0.619
I0726 03:29:31.025960 140241469953792 logging_writer.py:48] [7] global_step=7, grad_norm=0.627735, loss=6.933795
I0726 03:29:31.030452 140284670572352 submission.py:119] 7) loss = 6.934, grad_norm = 0.628
I0726 03:29:31.383166 140241478346496 logging_writer.py:48] [8] global_step=8, grad_norm=0.608893, loss=6.925479
I0726 03:29:31.388277 140284670572352 submission.py:119] 8) loss = 6.925, grad_norm = 0.609
I0726 03:29:31.745896 140241469953792 logging_writer.py:48] [9] global_step=9, grad_norm=0.628149, loss=6.925093
I0726 03:29:31.749675 140284670572352 submission.py:119] 9) loss = 6.925, grad_norm = 0.628
I0726 03:29:32.106679 140241478346496 logging_writer.py:48] [10] global_step=10, grad_norm=0.610749, loss=6.924409
I0726 03:29:32.115029 140284670572352 submission.py:119] 10) loss = 6.924, grad_norm = 0.611
I0726 03:29:32.466435 140241469953792 logging_writer.py:48] [11] global_step=11, grad_norm=0.606321, loss=6.929543
I0726 03:29:32.471487 140284670572352 submission.py:119] 11) loss = 6.930, grad_norm = 0.606
I0726 03:29:32.818785 140241478346496 logging_writer.py:48] [12] global_step=12, grad_norm=0.598487, loss=6.926879
I0726 03:29:32.822528 140284670572352 submission.py:119] 12) loss = 6.927, grad_norm = 0.598
I0726 03:29:33.176188 140241469953792 logging_writer.py:48] [13] global_step=13, grad_norm=0.604378, loss=6.925539
I0726 03:29:33.180862 140284670572352 submission.py:119] 13) loss = 6.926, grad_norm = 0.604
I0726 03:29:33.591921 140241478346496 logging_writer.py:48] [14] global_step=14, grad_norm=0.617137, loss=6.924892
I0726 03:29:33.596968 140284670572352 submission.py:119] 14) loss = 6.925, grad_norm = 0.617
I0726 03:29:33.946931 140241469953792 logging_writer.py:48] [15] global_step=15, grad_norm=0.596014, loss=6.925114
I0726 03:29:33.952394 140284670572352 submission.py:119] 15) loss = 6.925, grad_norm = 0.596
I0726 03:29:34.308355 140241478346496 logging_writer.py:48] [16] global_step=16, grad_norm=0.604230, loss=6.927367
I0726 03:29:34.313856 140284670572352 submission.py:119] 16) loss = 6.927, grad_norm = 0.604
I0726 03:29:34.672347 140241469953792 logging_writer.py:48] [17] global_step=17, grad_norm=0.607117, loss=6.921576
I0726 03:29:34.676199 140284670572352 submission.py:119] 17) loss = 6.922, grad_norm = 0.607
I0726 03:29:35.027654 140241478346496 logging_writer.py:48] [18] global_step=18, grad_norm=0.613261, loss=6.928566
I0726 03:29:35.033866 140284670572352 submission.py:119] 18) loss = 6.929, grad_norm = 0.613
I0726 03:29:35.390756 140241469953792 logging_writer.py:48] [19] global_step=19, grad_norm=0.614833, loss=6.932058
I0726 03:29:35.395670 140284670572352 submission.py:119] 19) loss = 6.932, grad_norm = 0.615
I0726 03:29:35.749365 140241478346496 logging_writer.py:48] [20] global_step=20, grad_norm=0.614662, loss=6.924892
I0726 03:29:35.753099 140284670572352 submission.py:119] 20) loss = 6.925, grad_norm = 0.615
I0726 03:29:36.108586 140241469953792 logging_writer.py:48] [21] global_step=21, grad_norm=0.607930, loss=6.933281
I0726 03:29:36.113068 140284670572352 submission.py:119] 21) loss = 6.933, grad_norm = 0.608
I0726 03:29:36.467129 140241478346496 logging_writer.py:48] [22] global_step=22, grad_norm=0.610179, loss=6.926146
I0726 03:29:36.472443 140284670572352 submission.py:119] 22) loss = 6.926, grad_norm = 0.610
I0726 03:29:36.823501 140241469953792 logging_writer.py:48] [23] global_step=23, grad_norm=0.601867, loss=6.916452
I0726 03:29:36.827501 140284670572352 submission.py:119] 23) loss = 6.916, grad_norm = 0.602
I0726 03:29:37.181410 140241478346496 logging_writer.py:48] [24] global_step=24, grad_norm=0.607962, loss=6.919896
I0726 03:29:37.186545 140284670572352 submission.py:119] 24) loss = 6.920, grad_norm = 0.608
I0726 03:29:37.539809 140241469953792 logging_writer.py:48] [25] global_step=25, grad_norm=0.601098, loss=6.938353
I0726 03:29:37.545798 140284670572352 submission.py:119] 25) loss = 6.938, grad_norm = 0.601
I0726 03:29:37.896690 140241478346496 logging_writer.py:48] [26] global_step=26, grad_norm=0.612761, loss=6.916511
I0726 03:29:37.901427 140284670572352 submission.py:119] 26) loss = 6.917, grad_norm = 0.613
I0726 03:29:38.253640 140241469953792 logging_writer.py:48] [27] global_step=27, grad_norm=0.597644, loss=6.925234
I0726 03:29:38.259401 140284670572352 submission.py:119] 27) loss = 6.925, grad_norm = 0.598
I0726 03:29:38.610095 140241478346496 logging_writer.py:48] [28] global_step=28, grad_norm=0.611032, loss=6.923694
I0726 03:29:38.616652 140284670572352 submission.py:119] 28) loss = 6.924, grad_norm = 0.611
I0726 03:29:38.970461 140241469953792 logging_writer.py:48] [29] global_step=29, grad_norm=0.607266, loss=6.917439
I0726 03:29:38.974687 140284670572352 submission.py:119] 29) loss = 6.917, grad_norm = 0.607
I0726 03:29:39.328292 140241478346496 logging_writer.py:48] [30] global_step=30, grad_norm=0.609898, loss=6.918488
I0726 03:29:39.333281 140284670572352 submission.py:119] 30) loss = 6.918, grad_norm = 0.610
I0726 03:29:39.753325 140241469953792 logging_writer.py:48] [31] global_step=31, grad_norm=0.593106, loss=6.923013
I0726 03:29:39.758944 140284670572352 submission.py:119] 31) loss = 6.923, grad_norm = 0.593
I0726 03:29:40.114679 140241478346496 logging_writer.py:48] [32] global_step=32, grad_norm=0.590274, loss=6.916970
I0726 03:29:40.120904 140284670572352 submission.py:119] 32) loss = 6.917, grad_norm = 0.590
I0726 03:29:40.476336 140241469953792 logging_writer.py:48] [33] global_step=33, grad_norm=0.624373, loss=6.912644
I0726 03:29:40.480245 140284670572352 submission.py:119] 33) loss = 6.913, grad_norm = 0.624
I0726 03:29:40.840287 140241478346496 logging_writer.py:48] [34] global_step=34, grad_norm=0.602291, loss=6.915155
I0726 03:29:40.845425 140284670572352 submission.py:119] 34) loss = 6.915, grad_norm = 0.602
I0726 03:29:41.200067 140241469953792 logging_writer.py:48] [35] global_step=35, grad_norm=0.594494, loss=6.919913
I0726 03:29:41.206409 140284670572352 submission.py:119] 35) loss = 6.920, grad_norm = 0.594
I0726 03:29:41.558471 140241478346496 logging_writer.py:48] [36] global_step=36, grad_norm=0.612116, loss=6.902953
I0726 03:29:41.562440 140284670572352 submission.py:119] 36) loss = 6.903, grad_norm = 0.612
I0726 03:29:41.915815 140241469953792 logging_writer.py:48] [37] global_step=37, grad_norm=0.606939, loss=6.919006
I0726 03:29:41.921564 140284670572352 submission.py:119] 37) loss = 6.919, grad_norm = 0.607
I0726 03:29:42.278966 140241478346496 logging_writer.py:48] [38] global_step=38, grad_norm=0.592899, loss=6.912445
I0726 03:29:42.285699 140284670572352 submission.py:119] 38) loss = 6.912, grad_norm = 0.593
I0726 03:29:42.635834 140241469953792 logging_writer.py:48] [39] global_step=39, grad_norm=0.601331, loss=6.915974
I0726 03:29:42.641566 140284670572352 submission.py:119] 39) loss = 6.916, grad_norm = 0.601
I0726 03:29:42.992261 140241478346496 logging_writer.py:48] [40] global_step=40, grad_norm=0.608245, loss=6.919831
I0726 03:29:42.998261 140284670572352 submission.py:119] 40) loss = 6.920, grad_norm = 0.608
I0726 03:29:43.351261 140241469953792 logging_writer.py:48] [41] global_step=41, grad_norm=0.589705, loss=6.921213
I0726 03:29:43.357164 140284670572352 submission.py:119] 41) loss = 6.921, grad_norm = 0.590
I0726 03:29:43.720021 140241478346496 logging_writer.py:48] [42] global_step=42, grad_norm=0.607047, loss=6.920269
I0726 03:29:43.725885 140284670572352 submission.py:119] 42) loss = 6.920, grad_norm = 0.607
I0726 03:29:44.088944 140241469953792 logging_writer.py:48] [43] global_step=43, grad_norm=0.609759, loss=6.907369
I0726 03:29:44.093529 140284670572352 submission.py:119] 43) loss = 6.907, grad_norm = 0.610
I0726 03:29:44.456815 140241478346496 logging_writer.py:48] [44] global_step=44, grad_norm=0.597680, loss=6.913294
I0726 03:29:44.462128 140284670572352 submission.py:119] 44) loss = 6.913, grad_norm = 0.598
I0726 03:29:44.816679 140241469953792 logging_writer.py:48] [45] global_step=45, grad_norm=0.593892, loss=6.913379
I0726 03:29:44.822124 140284670572352 submission.py:119] 45) loss = 6.913, grad_norm = 0.594
I0726 03:29:45.175849 140241478346496 logging_writer.py:48] [46] global_step=46, grad_norm=0.617043, loss=6.914164
I0726 03:29:45.182764 140284670572352 submission.py:119] 46) loss = 6.914, grad_norm = 0.617
I0726 03:29:45.539861 140241469953792 logging_writer.py:48] [47] global_step=47, grad_norm=0.600562, loss=6.912895
I0726 03:29:45.543655 140284670572352 submission.py:119] 47) loss = 6.913, grad_norm = 0.601
I0726 03:29:45.927229 140241478346496 logging_writer.py:48] [48] global_step=48, grad_norm=0.590502, loss=6.910755
I0726 03:29:45.931395 140284670572352 submission.py:119] 48) loss = 6.911, grad_norm = 0.591
I0726 03:29:46.284229 140241469953792 logging_writer.py:48] [49] global_step=49, grad_norm=0.607623, loss=6.910393
I0726 03:29:46.289014 140284670572352 submission.py:119] 49) loss = 6.910, grad_norm = 0.608
I0726 03:29:46.647182 140241478346496 logging_writer.py:48] [50] global_step=50, grad_norm=0.595612, loss=6.907043
I0726 03:29:46.654610 140284670572352 submission.py:119] 50) loss = 6.907, grad_norm = 0.596
I0726 03:29:47.012850 140241469953792 logging_writer.py:48] [51] global_step=51, grad_norm=0.592643, loss=6.905071
I0726 03:29:47.019103 140284670572352 submission.py:119] 51) loss = 6.905, grad_norm = 0.593
I0726 03:29:47.379667 140241478346496 logging_writer.py:48] [52] global_step=52, grad_norm=0.590543, loss=6.908768
I0726 03:29:47.385831 140284670572352 submission.py:119] 52) loss = 6.909, grad_norm = 0.591
I0726 03:29:47.745624 140241469953792 logging_writer.py:48] [53] global_step=53, grad_norm=0.590971, loss=6.910365
I0726 03:29:47.750845 140284670572352 submission.py:119] 53) loss = 6.910, grad_norm = 0.591
I0726 03:29:48.112519 140241478346496 logging_writer.py:48] [54] global_step=54, grad_norm=0.615201, loss=6.908815
I0726 03:29:48.121818 140284670572352 submission.py:119] 54) loss = 6.909, grad_norm = 0.615
I0726 03:29:48.472946 140241469953792 logging_writer.py:48] [55] global_step=55, grad_norm=0.594193, loss=6.917713
I0726 03:29:48.477577 140284670572352 submission.py:119] 55) loss = 6.918, grad_norm = 0.594
I0726 03:29:48.829436 140241478346496 logging_writer.py:48] [56] global_step=56, grad_norm=0.604988, loss=6.905724
I0726 03:29:48.836250 140284670572352 submission.py:119] 56) loss = 6.906, grad_norm = 0.605
I0726 03:29:49.191816 140241469953792 logging_writer.py:48] [57] global_step=57, grad_norm=0.606367, loss=6.908663
I0726 03:29:49.197091 140284670572352 submission.py:119] 57) loss = 6.909, grad_norm = 0.606
I0726 03:29:49.561355 140241478346496 logging_writer.py:48] [58] global_step=58, grad_norm=0.616110, loss=6.903298
I0726 03:29:49.566557 140284670572352 submission.py:119] 58) loss = 6.903, grad_norm = 0.616
I0726 03:29:49.924326 140241469953792 logging_writer.py:48] [59] global_step=59, grad_norm=0.616354, loss=6.921964
I0726 03:29:49.930620 140284670572352 submission.py:119] 59) loss = 6.922, grad_norm = 0.616
I0726 03:29:50.291838 140241478346496 logging_writer.py:48] [60] global_step=60, grad_norm=0.601479, loss=6.898018
I0726 03:29:50.296469 140284670572352 submission.py:119] 60) loss = 6.898, grad_norm = 0.601
I0726 03:29:50.648249 140241469953792 logging_writer.py:48] [61] global_step=61, grad_norm=0.578918, loss=6.901516
I0726 03:29:50.653150 140284670572352 submission.py:119] 61) loss = 6.902, grad_norm = 0.579
I0726 03:29:51.004467 140241478346496 logging_writer.py:48] [62] global_step=62, grad_norm=0.589337, loss=6.914696
I0726 03:29:51.010800 140284670572352 submission.py:119] 62) loss = 6.915, grad_norm = 0.589
I0726 03:29:51.362292 140241469953792 logging_writer.py:48] [63] global_step=63, grad_norm=0.607338, loss=6.899895
I0726 03:29:51.368485 140284670572352 submission.py:119] 63) loss = 6.900, grad_norm = 0.607
I0726 03:29:51.721475 140241478346496 logging_writer.py:48] [64] global_step=64, grad_norm=0.625117, loss=6.900600
I0726 03:29:51.726576 140284670572352 submission.py:119] 64) loss = 6.901, grad_norm = 0.625
I0726 03:29:52.091200 140241469953792 logging_writer.py:48] [65] global_step=65, grad_norm=0.592110, loss=6.890833
I0726 03:29:52.095999 140284670572352 submission.py:119] 65) loss = 6.891, grad_norm = 0.592
I0726 03:29:52.448357 140241478346496 logging_writer.py:48] [66] global_step=66, grad_norm=0.600048, loss=6.902133
I0726 03:29:52.452477 140284670572352 submission.py:119] 66) loss = 6.902, grad_norm = 0.600
I0726 03:29:52.806968 140241469953792 logging_writer.py:48] [67] global_step=67, grad_norm=0.608830, loss=6.905004
I0726 03:29:52.812541 140284670572352 submission.py:119] 67) loss = 6.905, grad_norm = 0.609
I0726 03:29:53.171206 140241478346496 logging_writer.py:48] [68] global_step=68, grad_norm=0.604402, loss=6.891619
I0726 03:29:53.176066 140284670572352 submission.py:119] 68) loss = 6.892, grad_norm = 0.604
I0726 03:29:53.566450 140241469953792 logging_writer.py:48] [69] global_step=69, grad_norm=0.582788, loss=6.905311
I0726 03:29:53.571501 140284670572352 submission.py:119] 69) loss = 6.905, grad_norm = 0.583
I0726 03:29:53.928489 140241478346496 logging_writer.py:48] [70] global_step=70, grad_norm=0.595115, loss=6.892936
I0726 03:29:53.934728 140284670572352 submission.py:119] 70) loss = 6.893, grad_norm = 0.595
I0726 03:29:54.288919 140241469953792 logging_writer.py:48] [71] global_step=71, grad_norm=0.600802, loss=6.900352
I0726 03:29:54.293836 140284670572352 submission.py:119] 71) loss = 6.900, grad_norm = 0.601
I0726 03:29:54.648656 140241478346496 logging_writer.py:48] [72] global_step=72, grad_norm=0.601314, loss=6.896802
I0726 03:29:54.653496 140284670572352 submission.py:119] 72) loss = 6.897, grad_norm = 0.601
I0726 03:29:55.012816 140241469953792 logging_writer.py:48] [73] global_step=73, grad_norm=0.616595, loss=6.896156
I0726 03:29:55.017762 140284670572352 submission.py:119] 73) loss = 6.896, grad_norm = 0.617
I0726 03:29:55.373804 140241478346496 logging_writer.py:48] [74] global_step=74, grad_norm=0.588493, loss=6.903061
I0726 03:29:55.379506 140284670572352 submission.py:119] 74) loss = 6.903, grad_norm = 0.588
I0726 03:29:55.743753 140241469953792 logging_writer.py:48] [75] global_step=75, grad_norm=0.591760, loss=6.891124
I0726 03:29:55.748303 140284670572352 submission.py:119] 75) loss = 6.891, grad_norm = 0.592
I0726 03:29:56.104567 140241478346496 logging_writer.py:48] [76] global_step=76, grad_norm=0.593407, loss=6.896261
I0726 03:29:56.108307 140284670572352 submission.py:119] 76) loss = 6.896, grad_norm = 0.593
I0726 03:29:56.464889 140241469953792 logging_writer.py:48] [77] global_step=77, grad_norm=0.596931, loss=6.891637
I0726 03:29:56.469553 140284670572352 submission.py:119] 77) loss = 6.892, grad_norm = 0.597
I0726 03:29:56.829381 140241478346496 logging_writer.py:48] [78] global_step=78, grad_norm=0.602665, loss=6.891727
I0726 03:29:56.833902 140284670572352 submission.py:119] 78) loss = 6.892, grad_norm = 0.603
I0726 03:29:57.187599 140241469953792 logging_writer.py:48] [79] global_step=79, grad_norm=0.616615, loss=6.895238
I0726 03:29:57.192997 140284670572352 submission.py:119] 79) loss = 6.895, grad_norm = 0.617
I0726 03:29:57.547585 140241478346496 logging_writer.py:48] [80] global_step=80, grad_norm=0.581271, loss=6.891221
I0726 03:29:57.554193 140284670572352 submission.py:119] 80) loss = 6.891, grad_norm = 0.581
I0726 03:29:57.909973 140241469953792 logging_writer.py:48] [81] global_step=81, grad_norm=0.584704, loss=6.888218
I0726 03:29:57.914267 140284670572352 submission.py:119] 81) loss = 6.888, grad_norm = 0.585
I0726 03:29:58.276099 140241478346496 logging_writer.py:48] [82] global_step=82, grad_norm=0.591245, loss=6.889840
I0726 03:29:58.281811 140284670572352 submission.py:119] 82) loss = 6.890, grad_norm = 0.591
I0726 03:29:58.634619 140241469953792 logging_writer.py:48] [83] global_step=83, grad_norm=0.601852, loss=6.882484
I0726 03:29:58.641202 140284670572352 submission.py:119] 83) loss = 6.882, grad_norm = 0.602
I0726 03:29:59.000798 140241478346496 logging_writer.py:48] [84] global_step=84, grad_norm=0.589923, loss=6.886459
I0726 03:29:59.006188 140284670572352 submission.py:119] 84) loss = 6.886, grad_norm = 0.590
I0726 03:29:59.360092 140241469953792 logging_writer.py:48] [85] global_step=85, grad_norm=0.588608, loss=6.879539
I0726 03:29:59.364282 140284670572352 submission.py:119] 85) loss = 6.880, grad_norm = 0.589
I0726 03:29:59.735421 140241478346496 logging_writer.py:48] [86] global_step=86, grad_norm=0.611721, loss=6.886363
I0726 03:29:59.741527 140284670572352 submission.py:119] 86) loss = 6.886, grad_norm = 0.612
I0726 03:30:00.096405 140241469953792 logging_writer.py:48] [87] global_step=87, grad_norm=0.598708, loss=6.879116
I0726 03:30:00.101107 140284670572352 submission.py:119] 87) loss = 6.879, grad_norm = 0.599
I0726 03:30:00.456833 140241478346496 logging_writer.py:48] [88] global_step=88, grad_norm=0.598978, loss=6.888535
I0726 03:30:00.462377 140284670572352 submission.py:119] 88) loss = 6.889, grad_norm = 0.599
I0726 03:30:00.822420 140241469953792 logging_writer.py:48] [89] global_step=89, grad_norm=0.591345, loss=6.875835
I0726 03:30:00.827359 140284670572352 submission.py:119] 89) loss = 6.876, grad_norm = 0.591
I0726 03:30:01.181641 140241478346496 logging_writer.py:48] [90] global_step=90, grad_norm=0.614249, loss=6.897190
I0726 03:30:01.186466 140284670572352 submission.py:119] 90) loss = 6.897, grad_norm = 0.614
I0726 03:30:01.546723 140241469953792 logging_writer.py:48] [91] global_step=91, grad_norm=0.593001, loss=6.888443
I0726 03:30:01.551213 140284670572352 submission.py:119] 91) loss = 6.888, grad_norm = 0.593
I0726 03:30:01.907347 140241478346496 logging_writer.py:48] [92] global_step=92, grad_norm=0.611530, loss=6.878565
I0726 03:30:01.913930 140284670572352 submission.py:119] 92) loss = 6.879, grad_norm = 0.612
I0726 03:30:02.281666 140241469953792 logging_writer.py:48] [93] global_step=93, grad_norm=0.599953, loss=6.882167
I0726 03:30:02.289374 140284670572352 submission.py:119] 93) loss = 6.882, grad_norm = 0.600
I0726 03:30:02.651724 140241478346496 logging_writer.py:48] [94] global_step=94, grad_norm=0.589057, loss=6.884959
I0726 03:30:02.658178 140284670572352 submission.py:119] 94) loss = 6.885, grad_norm = 0.589
I0726 03:30:03.013335 140241469953792 logging_writer.py:48] [95] global_step=95, grad_norm=0.601100, loss=6.885503
I0726 03:30:03.018283 140284670572352 submission.py:119] 95) loss = 6.886, grad_norm = 0.601
I0726 03:30:03.375514 140241478346496 logging_writer.py:48] [96] global_step=96, grad_norm=0.591443, loss=6.880283
I0726 03:30:03.381732 140284670572352 submission.py:119] 96) loss = 6.880, grad_norm = 0.591
I0726 03:30:03.737296 140241469953792 logging_writer.py:48] [97] global_step=97, grad_norm=0.600191, loss=6.871652
I0726 03:30:03.741194 140284670572352 submission.py:119] 97) loss = 6.872, grad_norm = 0.600
I0726 03:30:04.095157 140241478346496 logging_writer.py:48] [98] global_step=98, grad_norm=0.599744, loss=6.877541
I0726 03:30:04.100174 140284670572352 submission.py:119] 98) loss = 6.878, grad_norm = 0.600
I0726 03:30:04.458375 140241469953792 logging_writer.py:48] [99] global_step=99, grad_norm=0.604252, loss=6.877665
I0726 03:30:04.462345 140284670572352 submission.py:119] 99) loss = 6.878, grad_norm = 0.604
I0726 03:30:04.814630 140241478346496 logging_writer.py:48] [100] global_step=100, grad_norm=0.603890, loss=6.879291
I0726 03:30:04.820415 140284670572352 submission.py:119] 100) loss = 6.879, grad_norm = 0.604
I0726 03:32:26.531286 140241469953792 logging_writer.py:48] [500] global_step=500, grad_norm=1.245297, loss=6.307108
I0726 03:32:26.538704 140284670572352 submission.py:119] 500) loss = 6.307, grad_norm = 1.245
I0726 03:35:25.728841 140241478346496 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.444319, loss=5.716222
I0726 03:35:25.733403 140284670572352 submission.py:119] 1000) loss = 5.716, grad_norm = 3.444
I0726 03:37:58.551327 140284670572352 spec.py:320] Evaluating on the training split.
I0726 03:38:40.199357 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 03:39:36.046784 140284670572352 spec.py:348] Evaluating on the test split.
I0726 03:39:37.169214 140284670572352 submission_runner.py:364] Time since start: 873.77s, 	Step: 1432, 	{'train/accuracy': 0.11021205357142858, 'train/loss': 4.878156700912787, 'validation/accuracy': 0.09946, 'validation/loss': 4.95929875, 'validation/num_examples': 50000, 'test/accuracy': 0.0654, 'test/loss': 5.403047265625, 'test/num_examples': 10000, 'score': 569.8282465934753, 'total_duration': 873.7714483737946, 'accumulated_submission_time': 569.8282465934753, 'accumulated_eval_time': 302.7221646308899, 'accumulated_logging_time': 0.029776334762573242}
I0726 03:39:37.194589 140241486739200 logging_writer.py:48] [1432] accumulated_eval_time=302.722165, accumulated_logging_time=0.029776, accumulated_submission_time=569.828247, global_step=1432, preemption_count=0, score=569.828247, test/accuracy=0.065400, test/loss=5.403047, test/num_examples=10000, total_duration=873.771448, train/accuracy=0.110212, train/loss=4.878157, validation/accuracy=0.099460, validation/loss=4.959299, validation/num_examples=50000
I0726 03:40:01.696760 140241570617088 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.082086, loss=5.294439
I0726 03:40:01.700632 140284670572352 submission.py:119] 1500) loss = 5.294, grad_norm = 4.082
I0726 03:42:54.489096 140241486739200 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.600446, loss=4.916643
I0726 03:42:54.493560 140284670572352 submission.py:119] 2000) loss = 4.917, grad_norm = 3.600
I0726 03:45:47.311097 140241570617088 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.795334, loss=4.635949
I0726 03:45:47.315432 140284670572352 submission.py:119] 2500) loss = 4.636, grad_norm = 3.795
I0726 03:48:07.242562 140284670572352 spec.py:320] Evaluating on the training split.
I0726 03:48:52.885192 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 03:49:48.796884 140284670572352 spec.py:348] Evaluating on the test split.
I0726 03:49:49.906794 140284670572352 submission_runner.py:364] Time since start: 1486.51s, 	Step: 2901, 	{'train/accuracy': 0.25442442602040816, 'train/loss': 3.658829980966996, 'validation/accuracy': 0.23042, 'validation/loss': 3.8043084375, 'validation/num_examples': 50000, 'test/accuracy': 0.1708, 'test/loss': 4.392040625, 'test/num_examples': 10000, 'score': 1078.1647171974182, 'total_duration': 1486.5090510845184, 'accumulated_submission_time': 1078.1647171974182, 'accumulated_eval_time': 405.38660192489624, 'accumulated_logging_time': 0.5497500896453857}
I0726 03:49:49.925958 140241486739200 logging_writer.py:48] [2901] accumulated_eval_time=405.386602, accumulated_logging_time=0.549750, accumulated_submission_time=1078.164717, global_step=2901, preemption_count=0, score=1078.164717, test/accuracy=0.170800, test/loss=4.392041, test/num_examples=10000, total_duration=1486.509051, train/accuracy=0.254424, train/loss=3.658830, validation/accuracy=0.230420, validation/loss=3.804308, validation/num_examples=50000
I0726 03:50:25.184500 140241570617088 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.024215, loss=4.325765
I0726 03:50:25.189278 140284670572352 submission.py:119] 3000) loss = 4.326, grad_norm = 3.024
I0726 03:53:17.979804 140241486739200 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.488625, loss=3.983632
I0726 03:53:17.985876 140284670572352 submission.py:119] 3500) loss = 3.984, grad_norm = 2.489
I0726 03:56:12.393019 140241570617088 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.642026, loss=3.976052
I0726 03:56:12.398008 140284670572352 submission.py:119] 4000) loss = 3.976, grad_norm = 2.642
I0726 03:58:20.136100 140284670572352 spec.py:320] Evaluating on the training split.
I0726 03:59:03.545862 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 04:00:00.739163 140284670572352 spec.py:348] Evaluating on the test split.
I0726 04:00:01.843877 140284670572352 submission_runner.py:364] Time since start: 2098.45s, 	Step: 4371, 	{'train/accuracy': 0.3748405612244898, 'train/loss': 2.9473076645208867, 'validation/accuracy': 0.34382, 'validation/loss': 3.121466875, 'validation/num_examples': 50000, 'test/accuracy': 0.2574, 'test/loss': 3.736438671875, 'test/num_examples': 10000, 'score': 1586.70037317276, 'total_duration': 2098.446138381958, 'accumulated_submission_time': 1586.70037317276, 'accumulated_eval_time': 507.0946273803711, 'accumulated_logging_time': 1.0437686443328857}
I0726 04:00:01.859768 140241486739200 logging_writer.py:48] [4371] accumulated_eval_time=507.094627, accumulated_logging_time=1.043769, accumulated_submission_time=1586.700373, global_step=4371, preemption_count=0, score=1586.700373, test/accuracy=0.257400, test/loss=3.736439, test/num_examples=10000, total_duration=2098.446138, train/accuracy=0.374841, train/loss=2.947308, validation/accuracy=0.343820, validation/loss=3.121467, validation/num_examples=50000
I0726 04:00:47.417822 140241570617088 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.191948, loss=3.642656
I0726 04:00:47.423122 140284670572352 submission.py:119] 4500) loss = 3.643, grad_norm = 2.192
I0726 04:03:40.086130 140241486739200 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.265650, loss=3.630650
I0726 04:03:40.090243 140284670572352 submission.py:119] 5000) loss = 3.631, grad_norm = 2.266
I0726 04:06:34.326744 140241570617088 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.591226, loss=3.568928
I0726 04:06:34.332884 140284670572352 submission.py:119] 5500) loss = 3.569, grad_norm = 2.591
I0726 04:08:32.021017 140284670572352 spec.py:320] Evaluating on the training split.
I0726 04:09:14.313100 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 04:10:09.657368 140284670572352 spec.py:348] Evaluating on the test split.
I0726 04:10:10.757847 140284670572352 submission_runner.py:364] Time since start: 2707.36s, 	Step: 5842, 	{'train/accuracy': 0.45545679209183676, 'train/loss': 2.4981038619060905, 'validation/accuracy': 0.41622, 'validation/loss': 2.709475625, 'validation/num_examples': 50000, 'test/accuracy': 0.3112, 'test/loss': 3.38281875, 'test/num_examples': 10000, 'score': 2095.1555104255676, 'total_duration': 2707.3600928783417, 'accumulated_submission_time': 2095.1555104255676, 'accumulated_eval_time': 605.8315143585205, 'accumulated_logging_time': 1.5673778057098389}
I0726 04:10:10.778883 140241486739200 logging_writer.py:48] [5842] accumulated_eval_time=605.831514, accumulated_logging_time=1.567378, accumulated_submission_time=2095.155510, global_step=5842, preemption_count=0, score=2095.155510, test/accuracy=0.311200, test/loss=3.382819, test/num_examples=10000, total_duration=2707.360093, train/accuracy=0.455457, train/loss=2.498104, validation/accuracy=0.416220, validation/loss=2.709476, validation/num_examples=50000
I0726 04:11:06.339041 140241570617088 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.455062, loss=3.508476
I0726 04:11:06.344033 140284670572352 submission.py:119] 6000) loss = 3.508, grad_norm = 1.455
I0726 04:14:00.572250 140241486739200 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.768196, loss=3.321009
I0726 04:14:00.577039 140284670572352 submission.py:119] 6500) loss = 3.321, grad_norm = 1.768
I0726 04:16:53.117146 140241570617088 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.570509, loss=3.200149
I0726 04:16:53.121571 140284670572352 submission.py:119] 7000) loss = 3.200, grad_norm = 1.571
I0726 04:18:40.799188 140284670572352 spec.py:320] Evaluating on the training split.
I0726 04:19:23.954344 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 04:20:19.331112 140284670572352 spec.py:348] Evaluating on the test split.
I0726 04:20:20.431445 140284670572352 submission_runner.py:364] Time since start: 3317.03s, 	Step: 7313, 	{'train/accuracy': 0.5230389030612245, 'train/loss': 2.1814751138492507, 'validation/accuracy': 0.47922, 'validation/loss': 2.405143125, 'validation/num_examples': 50000, 'test/accuracy': 0.3543, 'test/loss': 3.1502837890625, 'test/num_examples': 10000, 'score': 2603.4971730709076, 'total_duration': 3317.032431125641, 'accumulated_submission_time': 2603.4971730709076, 'accumulated_eval_time': 705.4625315666199, 'accumulated_logging_time': 2.057823657989502}
I0726 04:20:20.448645 140241486739200 logging_writer.py:48] [7313] accumulated_eval_time=705.462532, accumulated_logging_time=2.057824, accumulated_submission_time=2603.497173, global_step=7313, preemption_count=0, score=2603.497173, test/accuracy=0.354300, test/loss=3.150284, test/num_examples=10000, total_duration=3317.032431, train/accuracy=0.523039, train/loss=2.181475, validation/accuracy=0.479220, validation/loss=2.405143, validation/num_examples=50000
I0726 04:21:25.846410 140241570617088 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.422670, loss=3.240208
I0726 04:21:25.849642 140284670572352 submission.py:119] 7500) loss = 3.240, grad_norm = 1.423
I0726 04:24:19.858314 140241486739200 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.130689, loss=3.141498
I0726 04:24:19.862729 140284670572352 submission.py:119] 8000) loss = 3.141, grad_norm = 1.131
I0726 04:27:12.307109 140241570617088 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.103961, loss=3.043767
I0726 04:27:12.314595 140284670572352 submission.py:119] 8500) loss = 3.044, grad_norm = 1.104
I0726 04:28:50.498680 140284670572352 spec.py:320] Evaluating on the training split.
I0726 04:29:33.545752 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 04:30:29.045076 140284670572352 spec.py:348] Evaluating on the test split.
I0726 04:30:30.143172 140284670572352 submission_runner.py:364] Time since start: 3926.75s, 	Step: 8781, 	{'train/accuracy': 0.5534319196428571, 'train/loss': 1.9834874990035076, 'validation/accuracy': 0.5061, 'validation/loss': 2.2321646875, 'validation/num_examples': 50000, 'test/accuracy': 0.3749, 'test/loss': 3.0223150390625, 'test/num_examples': 10000, 'score': 3111.866776227951, 'total_duration': 3926.7454528808594, 'accumulated_submission_time': 3111.866776227951, 'accumulated_eval_time': 805.1070523262024, 'accumulated_logging_time': 2.55049467086792}
I0726 04:30:30.161139 140241486739200 logging_writer.py:48] [8781] accumulated_eval_time=805.107052, accumulated_logging_time=2.550495, accumulated_submission_time=3111.866776, global_step=8781, preemption_count=0, score=3111.866776, test/accuracy=0.374900, test/loss=3.022315, test/num_examples=10000, total_duration=3926.745453, train/accuracy=0.553432, train/loss=1.983487, validation/accuracy=0.506100, validation/loss=2.232165, validation/num_examples=50000
I0726 04:31:46.662089 140241570617088 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.115436, loss=3.107693
I0726 04:31:46.666585 140284670572352 submission.py:119] 9000) loss = 3.108, grad_norm = 1.115
I0726 04:34:39.079730 140241486739200 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.002113, loss=3.035329
I0726 04:34:39.088945 140284670572352 submission.py:119] 9500) loss = 3.035, grad_norm = 1.002
I0726 04:37:31.543424 140241570617088 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.793434, loss=2.879753
I0726 04:37:31.548321 140284670572352 submission.py:119] 10000) loss = 2.880, grad_norm = 0.793
I0726 04:39:00.373508 140284670572352 spec.py:320] Evaluating on the training split.
I0726 04:39:43.793698 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 04:40:39.703919 140284670572352 spec.py:348] Evaluating on the test split.
I0726 04:40:40.801147 140284670572352 submission_runner.py:364] Time since start: 4537.40s, 	Step: 10254, 	{'train/accuracy': 0.6109295280612245, 'train/loss': 1.7129004342215401, 'validation/accuracy': 0.5571, 'validation/loss': 1.98694109375, 'validation/num_examples': 50000, 'test/accuracy': 0.4293, 'test/loss': 2.71708984375, 'test/num_examples': 10000, 'score': 3620.3855023384094, 'total_duration': 4537.403398513794, 'accumulated_submission_time': 3620.3855023384094, 'accumulated_eval_time': 905.5346195697784, 'accumulated_logging_time': 3.054478406906128}
I0726 04:40:40.816534 140241486739200 logging_writer.py:48] [10254] accumulated_eval_time=905.534620, accumulated_logging_time=3.054478, accumulated_submission_time=3620.385502, global_step=10254, preemption_count=0, score=3620.385502, test/accuracy=0.429300, test/loss=2.717090, test/num_examples=10000, total_duration=4537.403399, train/accuracy=0.610930, train/loss=1.712900, validation/accuracy=0.557100, validation/loss=1.986941, validation/num_examples=50000
I0726 04:42:06.504602 140241570617088 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.805283, loss=2.978862
I0726 04:42:06.510608 140284670572352 submission.py:119] 10500) loss = 2.979, grad_norm = 0.805
I0726 04:44:58.844652 140241486739200 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.763160, loss=2.863817
I0726 04:44:58.849408 140284670572352 submission.py:119] 11000) loss = 2.864, grad_norm = 0.763
I0726 04:47:52.843894 140241570617088 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.706807, loss=2.789880
I0726 04:47:52.849987 140284670572352 submission.py:119] 11500) loss = 2.790, grad_norm = 0.707
I0726 04:49:11.089493 140284670572352 spec.py:320] Evaluating on the training split.
I0726 04:49:54.376728 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 04:50:50.786509 140284670572352 spec.py:348] Evaluating on the test split.
I0726 04:50:51.883290 140284670572352 submission_runner.py:364] Time since start: 5148.49s, 	Step: 11728, 	{'train/accuracy': 0.6250597895408163, 'train/loss': 1.6812054381078603, 'validation/accuracy': 0.56712, 'validation/loss': 1.9615865625, 'validation/num_examples': 50000, 'test/accuracy': 0.4313, 'test/loss': 2.69260078125, 'test/num_examples': 10000, 'score': 4128.9997799396515, 'total_duration': 5148.4855790138245, 'accumulated_submission_time': 4128.9997799396515, 'accumulated_eval_time': 1006.3286542892456, 'accumulated_logging_time': 3.5266478061676025}
I0726 04:50:51.899502 140241486739200 logging_writer.py:48] [11728] accumulated_eval_time=1006.328654, accumulated_logging_time=3.526648, accumulated_submission_time=4128.999780, global_step=11728, preemption_count=0, score=4128.999780, test/accuracy=0.431300, test/loss=2.692601, test/num_examples=10000, total_duration=5148.485579, train/accuracy=0.625060, train/loss=1.681205, validation/accuracy=0.567120, validation/loss=1.961587, validation/num_examples=50000
I0726 04:52:26.554581 140241570617088 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.603793, loss=2.713184
I0726 04:52:26.559388 140284670572352 submission.py:119] 12000) loss = 2.713, grad_norm = 0.604
I0726 04:55:18.828992 140241486739200 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.663962, loss=2.823106
I0726 04:55:18.834864 140284670572352 submission.py:119] 12500) loss = 2.823, grad_norm = 0.664
I0726 04:58:12.777650 140241570617088 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.856217, loss=2.603129
I0726 04:58:12.783202 140284670572352 submission.py:119] 13000) loss = 2.603, grad_norm = 0.856
I0726 04:59:22.050485 140284670572352 spec.py:320] Evaluating on the training split.
I0726 05:00:05.012434 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 05:01:01.015663 140284670572352 spec.py:348] Evaluating on the test split.
I0726 05:01:02.116911 140284670572352 submission_runner.py:364] Time since start: 5758.72s, 	Step: 13202, 	{'train/accuracy': 0.6520448022959183, 'train/loss': 1.550375490772481, 'validation/accuracy': 0.58126, 'validation/loss': 1.87277875, 'validation/num_examples': 50000, 'test/accuracy': 0.4392, 'test/loss': 2.6634138671875, 'test/num_examples': 10000, 'score': 4637.454354763031, 'total_duration': 5758.719096899033, 'accumulated_submission_time': 4637.454354763031, 'accumulated_eval_time': 1106.3950440883636, 'accumulated_logging_time': 4.033369779586792}
I0726 05:01:02.133583 140241486739200 logging_writer.py:48] [13202] accumulated_eval_time=1106.395044, accumulated_logging_time=4.033370, accumulated_submission_time=4637.454355, global_step=13202, preemption_count=0, score=4637.454355, test/accuracy=0.439200, test/loss=2.663414, test/num_examples=10000, total_duration=5758.719097, train/accuracy=0.652045, train/loss=1.550375, validation/accuracy=0.581260, validation/loss=1.872779, validation/num_examples=50000
I0726 05:02:45.810447 140241570617088 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.623972, loss=2.677320
I0726 05:02:45.816851 140284670572352 submission.py:119] 13500) loss = 2.677, grad_norm = 0.624
I0726 05:05:41.105270 140284670572352 spec.py:320] Evaluating on the training split.
I0726 05:06:26.931181 140284670572352 spec.py:332] Evaluating on the validation split.
I0726 05:07:19.933805 140284670572352 spec.py:348] Evaluating on the test split.
I0726 05:07:21.031671 140284670572352 submission_runner.py:364] Time since start: 6137.63s, 	Step: 14000, 	{'train/accuracy': 0.6774553571428571, 'train/loss': 1.4360303294901946, 'validation/accuracy': 0.6051, 'validation/loss': 1.77763234375, 'validation/num_examples': 50000, 'test/accuracy': 0.471, 'test/loss': 2.5243970703125, 'test/num_examples': 10000, 'score': 4915.284462451935, 'total_duration': 6137.632894515991, 'accumulated_submission_time': 4915.284462451935, 'accumulated_eval_time': 1206.3204634189606, 'accumulated_logging_time': 4.530287027359009}
I0726 05:07:21.052254 140241486739200 logging_writer.py:48] [14000] accumulated_eval_time=1206.320463, accumulated_logging_time=4.530287, accumulated_submission_time=4915.284462, global_step=14000, preemption_count=0, score=4915.284462, test/accuracy=0.471000, test/loss=2.524397, test/num_examples=10000, total_duration=6137.632895, train/accuracy=0.677455, train/loss=1.436030, validation/accuracy=0.605100, validation/loss=1.777632, validation/num_examples=50000
I0726 05:07:21.518693 140241570617088 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4915.284462
I0726 05:07:22.282471 140284670572352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0726 05:07:22.761250 140284670572352 submission_runner.py:530] Tuning trial 1/1
I0726 05:07:22.761603 140284670572352 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0726 05:07:22.762672 140284670572352 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.924272809709821, 'validation/accuracy': 0.00114, 'validation/loss': 6.924665625, 'validation/num_examples': 50000, 'test/accuracy': 0.0008, 'test/loss': 6.92733515625, 'test/num_examples': 10000, 'score': 60.81288266181946, 'total_duration': 264.91762137413025, 'accumulated_submission_time': 60.81288266181946, 'accumulated_eval_time': 204.10420083999634, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1432, {'train/accuracy': 0.11021205357142858, 'train/loss': 4.878156700912787, 'validation/accuracy': 0.09946, 'validation/loss': 4.95929875, 'validation/num_examples': 50000, 'test/accuracy': 0.0654, 'test/loss': 5.403047265625, 'test/num_examples': 10000, 'score': 569.8282465934753, 'total_duration': 873.7714483737946, 'accumulated_submission_time': 569.8282465934753, 'accumulated_eval_time': 302.7221646308899, 'accumulated_logging_time': 0.029776334762573242, 'global_step': 1432, 'preemption_count': 0}), (2901, {'train/accuracy': 0.25442442602040816, 'train/loss': 3.658829980966996, 'validation/accuracy': 0.23042, 'validation/loss': 3.8043084375, 'validation/num_examples': 50000, 'test/accuracy': 0.1708, 'test/loss': 4.392040625, 'test/num_examples': 10000, 'score': 1078.1647171974182, 'total_duration': 1486.5090510845184, 'accumulated_submission_time': 1078.1647171974182, 'accumulated_eval_time': 405.38660192489624, 'accumulated_logging_time': 0.5497500896453857, 'global_step': 2901, 'preemption_count': 0}), (4371, {'train/accuracy': 0.3748405612244898, 'train/loss': 2.9473076645208867, 'validation/accuracy': 0.34382, 'validation/loss': 3.121466875, 'validation/num_examples': 50000, 'test/accuracy': 0.2574, 'test/loss': 3.736438671875, 'test/num_examples': 10000, 'score': 1586.70037317276, 'total_duration': 2098.446138381958, 'accumulated_submission_time': 1586.70037317276, 'accumulated_eval_time': 507.0946273803711, 'accumulated_logging_time': 1.0437686443328857, 'global_step': 4371, 'preemption_count': 0}), (5842, {'train/accuracy': 0.45545679209183676, 'train/loss': 2.4981038619060905, 'validation/accuracy': 0.41622, 'validation/loss': 2.709475625, 'validation/num_examples': 50000, 'test/accuracy': 0.3112, 'test/loss': 3.38281875, 'test/num_examples': 10000, 'score': 2095.1555104255676, 'total_duration': 2707.3600928783417, 'accumulated_submission_time': 2095.1555104255676, 'accumulated_eval_time': 605.8315143585205, 'accumulated_logging_time': 1.5673778057098389, 'global_step': 5842, 'preemption_count': 0}), (7313, {'train/accuracy': 0.5230389030612245, 'train/loss': 2.1814751138492507, 'validation/accuracy': 0.47922, 'validation/loss': 2.405143125, 'validation/num_examples': 50000, 'test/accuracy': 0.3543, 'test/loss': 3.1502837890625, 'test/num_examples': 10000, 'score': 2603.4971730709076, 'total_duration': 3317.032431125641, 'accumulated_submission_time': 2603.4971730709076, 'accumulated_eval_time': 705.4625315666199, 'accumulated_logging_time': 2.057823657989502, 'global_step': 7313, 'preemption_count': 0}), (8781, {'train/accuracy': 0.5534319196428571, 'train/loss': 1.9834874990035076, 'validation/accuracy': 0.5061, 'validation/loss': 2.2321646875, 'validation/num_examples': 50000, 'test/accuracy': 0.3749, 'test/loss': 3.0223150390625, 'test/num_examples': 10000, 'score': 3111.866776227951, 'total_duration': 3926.7454528808594, 'accumulated_submission_time': 3111.866776227951, 'accumulated_eval_time': 805.1070523262024, 'accumulated_logging_time': 2.55049467086792, 'global_step': 8781, 'preemption_count': 0}), (10254, {'train/accuracy': 0.6109295280612245, 'train/loss': 1.7129004342215401, 'validation/accuracy': 0.5571, 'validation/loss': 1.98694109375, 'validation/num_examples': 50000, 'test/accuracy': 0.4293, 'test/loss': 2.71708984375, 'test/num_examples': 10000, 'score': 3620.3855023384094, 'total_duration': 4537.403398513794, 'accumulated_submission_time': 3620.3855023384094, 'accumulated_eval_time': 905.5346195697784, 'accumulated_logging_time': 3.054478406906128, 'global_step': 10254, 'preemption_count': 0}), (11728, {'train/accuracy': 0.6250597895408163, 'train/loss': 1.6812054381078603, 'validation/accuracy': 0.56712, 'validation/loss': 1.9615865625, 'validation/num_examples': 50000, 'test/accuracy': 0.4313, 'test/loss': 2.69260078125, 'test/num_examples': 10000, 'score': 4128.9997799396515, 'total_duration': 5148.4855790138245, 'accumulated_submission_time': 4128.9997799396515, 'accumulated_eval_time': 1006.3286542892456, 'accumulated_logging_time': 3.5266478061676025, 'global_step': 11728, 'preemption_count': 0}), (13202, {'train/accuracy': 0.6520448022959183, 'train/loss': 1.550375490772481, 'validation/accuracy': 0.58126, 'validation/loss': 1.87277875, 'validation/num_examples': 50000, 'test/accuracy': 0.4392, 'test/loss': 2.6634138671875, 'test/num_examples': 10000, 'score': 4637.454354763031, 'total_duration': 5758.719096899033, 'accumulated_submission_time': 4637.454354763031, 'accumulated_eval_time': 1106.3950440883636, 'accumulated_logging_time': 4.033369779586792, 'global_step': 13202, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6774553571428571, 'train/loss': 1.4360303294901946, 'validation/accuracy': 0.6051, 'validation/loss': 1.77763234375, 'validation/num_examples': 50000, 'test/accuracy': 0.471, 'test/loss': 2.5243970703125, 'test/num_examples': 10000, 'score': 4915.284462451935, 'total_duration': 6137.632894515991, 'accumulated_submission_time': 4915.284462451935, 'accumulated_eval_time': 1206.3204634189606, 'accumulated_logging_time': 4.530287027359009, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0726 05:07:22.762833 140284670572352 submission_runner.py:533] Timing: 4915.284462451935
I0726 05:07:22.762916 140284670572352 submission_runner.py:535] Total number of evals: 11
I0726 05:07:22.762991 140284670572352 submission_runner.py:536] ====================
I0726 05:07:22.763180 140284670572352 submission_runner.py:604] Final imagenet_resnet score: 4915.284462451935
