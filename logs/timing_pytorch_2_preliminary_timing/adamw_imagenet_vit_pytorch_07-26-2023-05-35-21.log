torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_timing/adamw --overwrite=True --save_checkpoints=False --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=True 2>&1 | tee -a /logs/imagenet_vit_pytorch_07-26-2023-05-35-21.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-26 05:35:32.224746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 05:35:32.224751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0726 05:35:46.938432 140490582206272 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0726 05:35:46.938466 139853140531008 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0726 05:35:46.938432 139947015128896 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0726 05:35:46.938498 140616417462080 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0726 05:35:47.924194 140421890758464 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0726 05:35:47.924264 139713679292224 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0726 05:35:47.924294 139998767007552 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0726 05:35:47.926033 140031612139328 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0726 05:35:47.926416 140031612139328 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.931511 140490582206272 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.934934 140421890758464 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.934951 139998767007552 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.934971 139713679292224 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.935600 139947015128896 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.935700 139853140531008 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 05:35:47.935729 140616417462080 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0726 05:35:49.329477 140031612139328 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_vit_pytorch.
W0726 05:35:49.371027 140616417462080 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.371028 139713679292224 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.371028 139947015128896 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.371027 139853140531008 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.371028 140421890758464 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.371043 140031612139328 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.372001 140490582206272 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 05:35:49.374267 139998767007552 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0726 05:35:49.377522 140031612139328 submission_runner.py:490] Using RNG seed 1638907264
I0726 05:35:49.379488 140031612139328 submission_runner.py:499] --- Tuning run 1/1 ---
I0726 05:35:49.379613 140031612139328 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_vit_pytorch/trial_1.
I0726 05:35:49.379853 140031612139328 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0726 05:35:49.380656 140031612139328 submission_runner.py:176] Initializing dataset.
I0726 05:35:55.744320 140031612139328 submission_runner.py:183] Initializing model.
I0726 05:36:00.579346 140031612139328 submission_runner.py:214] Performing `torch.compile`.
I0726 05:36:01.165128 140031612139328 submission_runner.py:217] Initializing optimizer.
I0726 05:36:01.167178 140031612139328 submission_runner.py:224] Initializing metrics bundle.
I0726 05:36:01.167315 140031612139328 submission_runner.py:242] Initializing checkpoint and logger.
I0726 05:36:01.699109 140031612139328 submission_runner.py:263] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0726 05:36:01.700199 140031612139328 submission_runner.py:266] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0726 05:36:01.788830 140031612139328 submission_runner.py:276] Starting training loop.
[2023-07-26 05:36:04,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,516] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:04,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:36:06,472] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:06,529] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:06,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:06,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:06,550] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:06,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:06,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,644] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:06,649] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:06,649] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:06,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,852] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:06,857] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:06,862] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:06,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,957] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,993] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:06,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:36:06,998] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:06,998] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:07,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:07,011] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:07,011] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:07,043] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:07,048] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:07,048] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:07,058] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:36:07,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:36:07,067] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:36:12,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:12,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-07-26 05:36:18,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:18,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-07-26 05:36:21,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:21,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:22,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:22,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:22,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:22,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:22,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:22,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-07-26 05:36:28,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:28,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:28,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:28,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:28,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:28,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:29,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:29,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-07-26 05:36:32,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:32,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-07-26 05:36:33,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,550] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:33,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-07-26 05:36:36,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:36,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:36,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:36,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:36,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:36,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:37,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:37,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-07-26 05:36:40,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:40,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:40,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:40,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:40,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:40,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:40,430] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:40,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:40,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:40,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:40,499] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:40,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:40,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:40,531] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:40,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:40,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:40,662] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:40,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:40,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:40,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:40,717] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:40,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:40,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:40,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:41,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:41,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-07-26 05:36:41,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:41,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:41,359] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:41,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-07-26 05:36:41,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-07-26 05:36:41,471] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:36:49,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:49,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-07-26 05:36:50,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:50,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-07-26 05:36:50,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-07-26 05:36:54,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,058] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:55,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-07-26 05:36:56,877] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:36:57,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-07-26 05:37:02,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:02,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-07-26 05:37:03,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:03,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-07-26 05:37:05,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:05,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:05,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:05,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:05,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:05,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:05,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:05,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:06,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:06,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:06,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:06,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:06,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:06,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:06,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-07-26 05:37:06,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-07-26 05:37:06,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-07-26 05:37:06,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0726 05:37:07.252854 140003006998272 logging_writer.py:48] [0] global_step=0, grad_norm=0.335445, loss=6.907756
I0726 05:37:07.278802 140031612139328 submission.py:119] 0) loss = 6.908, grad_norm = 0.335
I0726 05:37:07.320076 140031612139328 spec.py:320] Evaluating on the training split.
[2023-07-26 05:37:19,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,253] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,436] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:19,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:37:20,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:20,766] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:20,770] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:20,770] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:20,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:20,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:20,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:20,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:20,960] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:20,964] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:20,965] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:20,969] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:20,973] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:20,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:20,981] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:20,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:20,985] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:20,985] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:21,002] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:21,006] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:21,006] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:21,028] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:21,032] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:21,033] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:21,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:21,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:21,163] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:21,164] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:21,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:37:21,337] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:37:21,341] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:37:21,342] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:37:21,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:21,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-07-26 05:37:23,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:23,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:23,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:23,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:24,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:24,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:24,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:24,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-07-26 05:37:25,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:25,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-07-26 05:37:28,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,147] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:29,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-07-26 05:37:30,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:30,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:30,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:30,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:30,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:30,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:31,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:31,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-07-26 05:37:31,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:31,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:31,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:31,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:31,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:31,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:32,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:32,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-07-26 05:37:32,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,184] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,566] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:33,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-07-26 05:37:36,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:36,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:36,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:36,893] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:36,899] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:36,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:36,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:36,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:37,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,048] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:37,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:37,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,148] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:37,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,197] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:37,199] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:37,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,265] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:37,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:37,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-07-26 05:37:37,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,515] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:37:37,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-07-26 05:37:37,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-07-26 05:37:37,704] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0726 05:38:29.479305 140031612139328 spec.py:332] Evaluating on the validation split.
[2023-07-26 05:39:20,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:20,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:20,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:21,802] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:21,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:21,965] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:21,968] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:21,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:22,322] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:22,363] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:22,367] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:22,367] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:22,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:22,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:22,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:22,537] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:22,541] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:22,542] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:22,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:23,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:23,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:23,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:23,800] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:23,804] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:23,805] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:24,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:24,303] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:24,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:24,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:24,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:24,580] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:24,581] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:24,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:25,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:25,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:25,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:25,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:25,764] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:25,769] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:25,770] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:26,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:26,191] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:26,195] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:26,196] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:26,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:26,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:26,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:26,305] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:26,309] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:26,310] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:26,570] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:26,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:26,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:26,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-07-26 05:39:26,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:27,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:28,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:28,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:29,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:29,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:29,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-07-26 05:39:30,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:30,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:30,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:30,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:30,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:30,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-07-26 05:39:31,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:32,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:32,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:32,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:32,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:33,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:33,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:33,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:33,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:34,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:34,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:34,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:34,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:34,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-07-26 05:39:35,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:35,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:35,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:35,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:35,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:36,375] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:36,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:36,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-07-26 05:39:37,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:37,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:37,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:37,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:37,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:37,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:37,466] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:37,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-07-26 05:39:37,721] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:37,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:37,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:37,943] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:38,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:38,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:38,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:38,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:38,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:38,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:39,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-07-26 05:39:39,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:39,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:39,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:39,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:39,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:40,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:40,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:40,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:41,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:41,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:41,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:41,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:41,532] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:41,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:41,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:41,736] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:39:41,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-07-26 05:39:41,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-07-26 05:39:41,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-07-26 05:39:41,955] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0726 05:39:45.036334 140031612139328 spec.py:348] Evaluating on the test split.
I0726 05:39:45.053580 140031612139328 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0726 05:39:45.059777 140031612139328 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0726 05:39:45.136551 140031612139328 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-07-26 05:39:48,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:48,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:39:53,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,628] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,632] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,632] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,687] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,691] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,715] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,730] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,734] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,734] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,797] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,800] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,834] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:39:53,871] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,874] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,875] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,875] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,877] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,878] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:53,892] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:39:53,896] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:39:53,896] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:39:54,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-07-26 05:39:54,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:54,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:55,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:55,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:55,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:55,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:55,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:55,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-07-26 05:39:56,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:56,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-07-26 05:39:57,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:57,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:57,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:57,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:57,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:58,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:58,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:58,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-07-26 05:39:59,360] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:39:59,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-07-26 05:40:00,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:00,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-07-26 05:40:01,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:01,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:01,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:02,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:02,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:02,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:02,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:02,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-07-26 05:40:03,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:03,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:03,470] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:03,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:03,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:03,529] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:03,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:03,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:03,610] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:03,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:03,768] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:03,774] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:03,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:03,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:03,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:03,818] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:03,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:03,832] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:03,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:03,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-07-26 05:40:04,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:04,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:04,378] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:04,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-07-26 05:40:04,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-07-26 05:40:04,567] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:07,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:07,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:07,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:08,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:08,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:08,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:08,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:08,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-07-26 05:40:09,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:09,704] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:09,708] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:09,708] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:09,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:09,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:09,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:09,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:09,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:09,769] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:09,773] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:09,774] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:09,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:09,965] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:09,969] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:09,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:09,991] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:10,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:10,034] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:10,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:10,039] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:10,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:10,048] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:10,048] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:10,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:10,192] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:10,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:10,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:10,208] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-07-26 05:40:10,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,270] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-07-26 05:40:10,274] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-07-26 05:40:10,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-07-26 05:40:10,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:10,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-07-26 05:40:11,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:11,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-07-26 05:40:12,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:12,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:12,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:12,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:13,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:13,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:13,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:13,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-07-26 05:40:14,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:14,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-07-26 05:40:15,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:15,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-07-26 05:40:16,805] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:16,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:17,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:17,104] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:17,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:17,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:17,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:17,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-07-26 05:40:18,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:18,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-07-26 05:40:20,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:20,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,233] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,261] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,632] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,746] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-07-26 05:40:21,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:21,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-07-26 05:40:21,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-07-26 05:40:21,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-07-26 05:40:22,000] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0726 05:40:22.784274 140031612139328 submission_runner.py:364] Time since start: 261.00s, 	Step: 1, 	{'train/accuracy': 0.00181640625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.0018, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 65.53112864494324, 'total_duration': 260.9958744049072, 'accumulated_submission_time': 65.53112864494324, 'accumulated_eval_time': 195.4641137123108, 'accumulated_logging_time': 0}
I0726 05:40:22.804912 139998351324928 logging_writer.py:48] [1] accumulated_eval_time=195.464114, accumulated_logging_time=0, accumulated_submission_time=65.531129, global_step=1, preemption_count=0, score=65.531129, test/accuracy=0.001100, test/loss=6.907755, test/num_examples=10000, total_duration=260.995874, train/accuracy=0.001816, train/loss=6.907756, validation/accuracy=0.001800, validation/loss=6.907756, validation/num_examples=50000
I0726 05:40:22.837916 140031612139328 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.839446 140421890758464 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.841784 139713679292224 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.841790 140616417462080 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.841922 139853140531008 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.841918 139998767007552 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.841883 139947015128896 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:22.842203 140490582206272 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 05:40:23.566963 139998342932224 logging_writer.py:48] [1] global_step=1, grad_norm=0.345549, loss=6.907756
I0726 05:40:23.570203 140031612139328 submission.py:119] 1) loss = 6.908, grad_norm = 0.346
I0726 05:40:24.030455 139998351324928 logging_writer.py:48] [2] global_step=2, grad_norm=0.349176, loss=6.907755
I0726 05:40:24.035051 140031612139328 submission.py:119] 2) loss = 6.908, grad_norm = 0.349
I0726 05:40:24.420523 139998342932224 logging_writer.py:48] [3] global_step=3, grad_norm=0.342293, loss=6.907752
I0726 05:40:24.424598 140031612139328 submission.py:119] 3) loss = 6.908, grad_norm = 0.342
I0726 05:40:24.803213 139998351324928 logging_writer.py:48] [4] global_step=4, grad_norm=0.338658, loss=6.907754
I0726 05:40:24.808677 140031612139328 submission.py:119] 4) loss = 6.908, grad_norm = 0.339
I0726 05:40:25.188163 139998342932224 logging_writer.py:48] [5] global_step=5, grad_norm=0.339291, loss=6.907756
I0726 05:40:25.192249 140031612139328 submission.py:119] 5) loss = 6.908, grad_norm = 0.339
I0726 05:40:25.571852 139998351324928 logging_writer.py:48] [6] global_step=6, grad_norm=0.355800, loss=6.907744
I0726 05:40:25.576020 140031612139328 submission.py:119] 6) loss = 6.908, grad_norm = 0.356
I0726 05:40:25.957957 139998342932224 logging_writer.py:48] [7] global_step=7, grad_norm=0.348754, loss=6.907744
I0726 05:40:25.962162 140031612139328 submission.py:119] 7) loss = 6.908, grad_norm = 0.349
I0726 05:40:26.345536 139998351324928 logging_writer.py:48] [8] global_step=8, grad_norm=0.350354, loss=6.907732
I0726 05:40:26.352304 140031612139328 submission.py:119] 8) loss = 6.908, grad_norm = 0.350
I0726 05:40:26.732768 139998342932224 logging_writer.py:48] [9] global_step=9, grad_norm=0.347649, loss=6.907742
I0726 05:40:26.738389 140031612139328 submission.py:119] 9) loss = 6.908, grad_norm = 0.348
I0726 05:40:27.124269 139998351324928 logging_writer.py:48] [10] global_step=10, grad_norm=0.343317, loss=6.907720
I0726 05:40:27.129923 140031612139328 submission.py:119] 10) loss = 6.908, grad_norm = 0.343
I0726 05:40:27.519152 139998342932224 logging_writer.py:48] [11] global_step=11, grad_norm=0.346923, loss=6.907709
I0726 05:40:27.524998 140031612139328 submission.py:119] 11) loss = 6.908, grad_norm = 0.347
I0726 05:40:27.909670 139998351324928 logging_writer.py:48] [12] global_step=12, grad_norm=0.344756, loss=6.907738
I0726 05:40:27.915979 140031612139328 submission.py:119] 12) loss = 6.908, grad_norm = 0.345
I0726 05:40:28.302572 139998342932224 logging_writer.py:48] [13] global_step=13, grad_norm=0.342786, loss=6.907707
I0726 05:40:28.307827 140031612139328 submission.py:119] 13) loss = 6.908, grad_norm = 0.343
I0726 05:40:28.693138 139998351324928 logging_writer.py:48] [14] global_step=14, grad_norm=0.345619, loss=6.907745
I0726 05:40:28.698087 140031612139328 submission.py:119] 14) loss = 6.908, grad_norm = 0.346
I0726 05:40:29.085356 139998342932224 logging_writer.py:48] [15] global_step=15, grad_norm=0.346260, loss=6.907692
I0726 05:40:29.089979 140031612139328 submission.py:119] 15) loss = 6.908, grad_norm = 0.346
I0726 05:40:29.476585 139998351324928 logging_writer.py:48] [16] global_step=16, grad_norm=0.341032, loss=6.907687
I0726 05:40:29.481443 140031612139328 submission.py:119] 16) loss = 6.908, grad_norm = 0.341
I0726 05:40:29.872714 139998342932224 logging_writer.py:48] [17] global_step=17, grad_norm=0.331081, loss=6.907680
I0726 05:40:29.879574 140031612139328 submission.py:119] 17) loss = 6.908, grad_norm = 0.331
I0726 05:40:30.263350 139998351324928 logging_writer.py:48] [18] global_step=18, grad_norm=0.344936, loss=6.907657
I0726 05:40:30.268166 140031612139328 submission.py:119] 18) loss = 6.908, grad_norm = 0.345
I0726 05:40:30.650941 139998342932224 logging_writer.py:48] [19] global_step=19, grad_norm=0.335391, loss=6.907695
I0726 05:40:30.655554 140031612139328 submission.py:119] 19) loss = 6.908, grad_norm = 0.335
I0726 05:40:31.037783 139998351324928 logging_writer.py:48] [20] global_step=20, grad_norm=0.338863, loss=6.907603
I0726 05:40:31.043502 140031612139328 submission.py:119] 20) loss = 6.908, grad_norm = 0.339
I0726 05:40:31.422997 139998342932224 logging_writer.py:48] [21] global_step=21, grad_norm=0.341978, loss=6.907577
I0726 05:40:31.428441 140031612139328 submission.py:119] 21) loss = 6.908, grad_norm = 0.342
I0726 05:40:31.809356 139998351324928 logging_writer.py:48] [22] global_step=22, grad_norm=0.337863, loss=6.907555
I0726 05:40:31.814778 140031612139328 submission.py:119] 22) loss = 6.908, grad_norm = 0.338
I0726 05:40:32.196733 139998342932224 logging_writer.py:48] [23] global_step=23, grad_norm=0.338505, loss=6.907520
I0726 05:40:32.202165 140031612139328 submission.py:119] 23) loss = 6.908, grad_norm = 0.339
I0726 05:40:32.586277 139998351324928 logging_writer.py:48] [24] global_step=24, grad_norm=0.345230, loss=6.907494
I0726 05:40:32.591196 140031612139328 submission.py:119] 24) loss = 6.907, grad_norm = 0.345
I0726 05:40:32.972318 139998342932224 logging_writer.py:48] [25] global_step=25, grad_norm=0.342217, loss=6.907440
I0726 05:40:32.977969 140031612139328 submission.py:119] 25) loss = 6.907, grad_norm = 0.342
I0726 05:40:33.361068 139998351324928 logging_writer.py:48] [26] global_step=26, grad_norm=0.350339, loss=6.907432
I0726 05:40:33.367346 140031612139328 submission.py:119] 26) loss = 6.907, grad_norm = 0.350
I0726 05:40:33.760537 139998342932224 logging_writer.py:48] [27] global_step=27, grad_norm=0.349395, loss=6.907397
I0726 05:40:33.766834 140031612139328 submission.py:119] 27) loss = 6.907, grad_norm = 0.349
I0726 05:40:34.149315 139998351324928 logging_writer.py:48] [28] global_step=28, grad_norm=0.345764, loss=6.907401
I0726 05:40:34.154529 140031612139328 submission.py:119] 28) loss = 6.907, grad_norm = 0.346
I0726 05:40:34.541698 139998342932224 logging_writer.py:48] [29] global_step=29, grad_norm=0.346953, loss=6.907435
I0726 05:40:34.546484 140031612139328 submission.py:119] 29) loss = 6.907, grad_norm = 0.347
I0726 05:40:34.930463 139998351324928 logging_writer.py:48] [30] global_step=30, grad_norm=0.362521, loss=6.907229
I0726 05:40:34.934984 140031612139328 submission.py:119] 30) loss = 6.907, grad_norm = 0.363
I0726 05:40:35.321510 139998342932224 logging_writer.py:48] [31] global_step=31, grad_norm=0.351762, loss=6.907160
I0726 05:40:35.326404 140031612139328 submission.py:119] 31) loss = 6.907, grad_norm = 0.352
I0726 05:40:35.707015 139998351324928 logging_writer.py:48] [32] global_step=32, grad_norm=0.349629, loss=6.907156
I0726 05:40:35.712480 140031612139328 submission.py:119] 32) loss = 6.907, grad_norm = 0.350
I0726 05:40:36.094891 139998342932224 logging_writer.py:48] [33] global_step=33, grad_norm=0.354606, loss=6.906981
I0726 05:40:36.100370 140031612139328 submission.py:119] 33) loss = 6.907, grad_norm = 0.355
I0726 05:40:36.482340 139998351324928 logging_writer.py:48] [34] global_step=34, grad_norm=0.367913, loss=6.906877
I0726 05:40:36.487722 140031612139328 submission.py:119] 34) loss = 6.907, grad_norm = 0.368
I0726 05:40:36.875325 139998342932224 logging_writer.py:48] [35] global_step=35, grad_norm=0.353870, loss=6.906998
I0726 05:40:36.881172 140031612139328 submission.py:119] 35) loss = 6.907, grad_norm = 0.354
I0726 05:40:37.263075 139998351324928 logging_writer.py:48] [36] global_step=36, grad_norm=0.362489, loss=6.906903
I0726 05:40:37.268338 140031612139328 submission.py:119] 36) loss = 6.907, grad_norm = 0.362
I0726 05:40:37.658202 139998342932224 logging_writer.py:48] [37] global_step=37, grad_norm=0.352023, loss=6.906878
I0726 05:40:37.664162 140031612139328 submission.py:119] 37) loss = 6.907, grad_norm = 0.352
I0726 05:40:38.051545 139998351324928 logging_writer.py:48] [38] global_step=38, grad_norm=0.342798, loss=6.906685
I0726 05:40:38.056930 140031612139328 submission.py:119] 38) loss = 6.907, grad_norm = 0.343
I0726 05:40:38.441715 139998342932224 logging_writer.py:48] [39] global_step=39, grad_norm=0.366357, loss=6.906375
I0726 05:40:38.448246 140031612139328 submission.py:119] 39) loss = 6.906, grad_norm = 0.366
I0726 05:40:38.832589 139998351324928 logging_writer.py:48] [40] global_step=40, grad_norm=0.379001, loss=6.906249
I0726 05:40:38.837227 140031612139328 submission.py:119] 40) loss = 6.906, grad_norm = 0.379
I0726 05:40:39.231541 139998342932224 logging_writer.py:48] [41] global_step=41, grad_norm=0.381402, loss=6.906278
I0726 05:40:39.237500 140031612139328 submission.py:119] 41) loss = 6.906, grad_norm = 0.381
I0726 05:40:39.621571 139998351324928 logging_writer.py:48] [42] global_step=42, grad_norm=0.385261, loss=6.906078
I0726 05:40:39.626967 140031612139328 submission.py:119] 42) loss = 6.906, grad_norm = 0.385
I0726 05:40:40.013457 139998342932224 logging_writer.py:48] [43] global_step=43, grad_norm=0.361370, loss=6.906040
I0726 05:40:40.018419 140031612139328 submission.py:119] 43) loss = 6.906, grad_norm = 0.361
I0726 05:40:40.406342 139998351324928 logging_writer.py:48] [44] global_step=44, grad_norm=0.393425, loss=6.905916
I0726 05:40:40.412775 140031612139328 submission.py:119] 44) loss = 6.906, grad_norm = 0.393
I0726 05:40:40.857631 139998342932224 logging_writer.py:48] [45] global_step=45, grad_norm=0.368296, loss=6.906167
I0726 05:40:40.861984 140031612139328 submission.py:119] 45) loss = 6.906, grad_norm = 0.368
I0726 05:40:41.252846 139998351324928 logging_writer.py:48] [46] global_step=46, grad_norm=0.365841, loss=6.906388
I0726 05:40:41.259479 140031612139328 submission.py:119] 46) loss = 6.906, grad_norm = 0.366
I0726 05:40:41.647968 139998342932224 logging_writer.py:48] [47] global_step=47, grad_norm=0.399035, loss=6.905404
I0726 05:40:41.652367 140031612139328 submission.py:119] 47) loss = 6.905, grad_norm = 0.399
I0726 05:40:42.037308 139998351324928 logging_writer.py:48] [48] global_step=48, grad_norm=0.388488, loss=6.905225
I0726 05:40:42.042223 140031612139328 submission.py:119] 48) loss = 6.905, grad_norm = 0.388
I0726 05:40:42.535843 139998342932224 logging_writer.py:48] [49] global_step=49, grad_norm=0.402864, loss=6.905493
I0726 05:40:42.541054 140031612139328 submission.py:119] 49) loss = 6.905, grad_norm = 0.403
I0726 05:40:42.924427 139998351324928 logging_writer.py:48] [50] global_step=50, grad_norm=0.381971, loss=6.905897
I0726 05:40:42.930133 140031612139328 submission.py:119] 50) loss = 6.906, grad_norm = 0.382
I0726 05:40:43.309117 139998342932224 logging_writer.py:48] [51] global_step=51, grad_norm=0.385073, loss=6.904753
I0726 05:40:43.314741 140031612139328 submission.py:119] 51) loss = 6.905, grad_norm = 0.385
I0726 05:40:43.698770 139998351324928 logging_writer.py:48] [52] global_step=52, grad_norm=0.378202, loss=6.904827
I0726 05:40:43.703163 140031612139328 submission.py:119] 52) loss = 6.905, grad_norm = 0.378
I0726 05:40:44.091820 139998342932224 logging_writer.py:48] [53] global_step=53, grad_norm=0.404583, loss=6.904458
I0726 05:40:44.096904 140031612139328 submission.py:119] 53) loss = 6.904, grad_norm = 0.405
I0726 05:40:44.489741 139998351324928 logging_writer.py:48] [54] global_step=54, grad_norm=0.398553, loss=6.904603
I0726 05:40:44.496742 140031612139328 submission.py:119] 54) loss = 6.905, grad_norm = 0.399
I0726 05:40:44.878147 139998342932224 logging_writer.py:48] [55] global_step=55, grad_norm=0.403495, loss=6.904160
I0726 05:40:44.883466 140031612139328 submission.py:119] 55) loss = 6.904, grad_norm = 0.403
I0726 05:40:45.271794 139998351324928 logging_writer.py:48] [56] global_step=56, grad_norm=0.409103, loss=6.904491
I0726 05:40:45.277164 140031612139328 submission.py:119] 56) loss = 6.904, grad_norm = 0.409
I0726 05:40:45.663073 139998342932224 logging_writer.py:48] [57] global_step=57, grad_norm=0.395647, loss=6.904425
I0726 05:40:45.670375 140031612139328 submission.py:119] 57) loss = 6.904, grad_norm = 0.396
I0726 05:40:46.050259 139998351324928 logging_writer.py:48] [58] global_step=58, grad_norm=0.416957, loss=6.903376
I0726 05:40:46.056127 140031612139328 submission.py:119] 58) loss = 6.903, grad_norm = 0.417
I0726 05:40:46.438727 139998342932224 logging_writer.py:48] [59] global_step=59, grad_norm=0.409093, loss=6.903404
I0726 05:40:46.443624 140031612139328 submission.py:119] 59) loss = 6.903, grad_norm = 0.409
I0726 05:40:46.834172 139998351324928 logging_writer.py:48] [60] global_step=60, grad_norm=0.417126, loss=6.903044
I0726 05:40:46.840662 140031612139328 submission.py:119] 60) loss = 6.903, grad_norm = 0.417
I0726 05:40:47.227314 139998342932224 logging_writer.py:48] [61] global_step=61, grad_norm=0.436238, loss=6.902333
I0726 05:40:47.232994 140031612139328 submission.py:119] 61) loss = 6.902, grad_norm = 0.436
I0726 05:40:47.621949 139998351324928 logging_writer.py:48] [62] global_step=62, grad_norm=0.409159, loss=6.902831
I0726 05:40:47.627362 140031612139328 submission.py:119] 62) loss = 6.903, grad_norm = 0.409
I0726 05:40:48.020962 139998342932224 logging_writer.py:48] [63] global_step=63, grad_norm=0.415279, loss=6.902722
I0726 05:40:48.026779 140031612139328 submission.py:119] 63) loss = 6.903, grad_norm = 0.415
I0726 05:40:48.409737 139998351324928 logging_writer.py:48] [64] global_step=64, grad_norm=0.424076, loss=6.902338
I0726 05:40:48.414916 140031612139328 submission.py:119] 64) loss = 6.902, grad_norm = 0.424
I0726 05:40:48.798474 139998342932224 logging_writer.py:48] [65] global_step=65, grad_norm=0.413318, loss=6.901728
I0726 05:40:48.803143 140031612139328 submission.py:119] 65) loss = 6.902, grad_norm = 0.413
I0726 05:40:49.188634 139998351324928 logging_writer.py:48] [66] global_step=66, grad_norm=0.426560, loss=6.899706
I0726 05:40:49.195108 140031612139328 submission.py:119] 66) loss = 6.900, grad_norm = 0.427
I0726 05:40:49.581747 139998342932224 logging_writer.py:48] [67] global_step=67, grad_norm=0.421135, loss=6.901917
I0726 05:40:49.588050 140031612139328 submission.py:119] 67) loss = 6.902, grad_norm = 0.421
I0726 05:40:49.972781 139998351324928 logging_writer.py:48] [68] global_step=68, grad_norm=0.431548, loss=6.901656
I0726 05:40:49.979501 140031612139328 submission.py:119] 68) loss = 6.902, grad_norm = 0.432
I0726 05:40:50.364638 139998342932224 logging_writer.py:48] [69] global_step=69, grad_norm=0.417785, loss=6.902029
I0726 05:40:50.370291 140031612139328 submission.py:119] 69) loss = 6.902, grad_norm = 0.418
I0726 05:40:50.754451 139998351324928 logging_writer.py:48] [70] global_step=70, grad_norm=0.429111, loss=6.900919
I0726 05:40:50.759649 140031612139328 submission.py:119] 70) loss = 6.901, grad_norm = 0.429
I0726 05:40:51.146907 139998342932224 logging_writer.py:48] [71] global_step=71, grad_norm=0.417256, loss=6.901852
I0726 05:40:51.151934 140031612139328 submission.py:119] 71) loss = 6.902, grad_norm = 0.417
I0726 05:40:51.585829 139998351324928 logging_writer.py:48] [72] global_step=72, grad_norm=0.449664, loss=6.899893
I0726 05:40:51.591843 140031612139328 submission.py:119] 72) loss = 6.900, grad_norm = 0.450
I0726 05:40:52.016023 139998342932224 logging_writer.py:48] [73] global_step=73, grad_norm=0.431180, loss=6.900406
I0726 05:40:52.021680 140031612139328 submission.py:119] 73) loss = 6.900, grad_norm = 0.431
I0726 05:40:52.463675 139998351324928 logging_writer.py:48] [74] global_step=74, grad_norm=0.470994, loss=6.897459
I0726 05:40:52.468987 140031612139328 submission.py:119] 74) loss = 6.897, grad_norm = 0.471
I0726 05:40:52.856050 139998342932224 logging_writer.py:48] [75] global_step=75, grad_norm=0.440196, loss=6.897994
I0726 05:40:52.863215 140031612139328 submission.py:119] 75) loss = 6.898, grad_norm = 0.440
I0726 05:40:53.248351 139998351324928 logging_writer.py:48] [76] global_step=76, grad_norm=0.445154, loss=6.899373
I0726 05:40:53.253783 140031612139328 submission.py:119] 76) loss = 6.899, grad_norm = 0.445
I0726 05:40:53.705117 139998342932224 logging_writer.py:48] [77] global_step=77, grad_norm=0.429078, loss=6.899390
I0726 05:40:53.711220 140031612139328 submission.py:119] 77) loss = 6.899, grad_norm = 0.429
I0726 05:40:54.135886 139998351324928 logging_writer.py:48] [78] global_step=78, grad_norm=0.434547, loss=6.899168
I0726 05:40:54.141624 140031612139328 submission.py:119] 78) loss = 6.899, grad_norm = 0.435
I0726 05:40:54.525676 139998342932224 logging_writer.py:48] [79] global_step=79, grad_norm=0.454844, loss=6.898312
I0726 05:40:54.530965 140031612139328 submission.py:119] 79) loss = 6.898, grad_norm = 0.455
I0726 05:40:54.915623 139998351324928 logging_writer.py:48] [80] global_step=80, grad_norm=0.464832, loss=6.895289
I0726 05:40:54.921165 140031612139328 submission.py:119] 80) loss = 6.895, grad_norm = 0.465
I0726 05:40:55.355366 139998342932224 logging_writer.py:48] [81] global_step=81, grad_norm=0.459310, loss=6.894127
I0726 05:40:55.360831 140031612139328 submission.py:119] 81) loss = 6.894, grad_norm = 0.459
I0726 05:40:55.752737 139998351324928 logging_writer.py:48] [82] global_step=82, grad_norm=0.453574, loss=6.895366
I0726 05:40:55.759991 140031612139328 submission.py:119] 82) loss = 6.895, grad_norm = 0.454
I0726 05:40:56.148205 139998342932224 logging_writer.py:48] [83] global_step=83, grad_norm=0.428784, loss=6.895217
I0726 05:40:56.153486 140031612139328 submission.py:119] 83) loss = 6.895, grad_norm = 0.429
I0726 05:40:56.545013 139998351324928 logging_writer.py:48] [84] global_step=84, grad_norm=0.441090, loss=6.895911
I0726 05:40:56.551348 140031612139328 submission.py:119] 84) loss = 6.896, grad_norm = 0.441
I0726 05:40:56.933966 139998342932224 logging_writer.py:48] [85] global_step=85, grad_norm=0.447613, loss=6.893196
I0726 05:40:56.939408 140031612139328 submission.py:119] 85) loss = 6.893, grad_norm = 0.448
I0726 05:40:57.324438 139998351324928 logging_writer.py:48] [86] global_step=86, grad_norm=0.474726, loss=6.892115
I0726 05:40:57.330320 140031612139328 submission.py:119] 86) loss = 6.892, grad_norm = 0.475
I0726 05:40:57.716088 139998342932224 logging_writer.py:48] [87] global_step=87, grad_norm=0.449242, loss=6.894239
I0726 05:40:57.720397 140031612139328 submission.py:119] 87) loss = 6.894, grad_norm = 0.449
I0726 05:40:58.106155 139998351324928 logging_writer.py:48] [88] global_step=88, grad_norm=0.471401, loss=6.892200
I0726 05:40:58.110640 140031612139328 submission.py:119] 88) loss = 6.892, grad_norm = 0.471
I0726 05:40:58.500633 139998342932224 logging_writer.py:48] [89] global_step=89, grad_norm=0.444736, loss=6.892453
I0726 05:40:58.505398 140031612139328 submission.py:119] 89) loss = 6.892, grad_norm = 0.445
I0726 05:40:58.889719 139998351324928 logging_writer.py:48] [90] global_step=90, grad_norm=0.471280, loss=6.892966
I0726 05:40:58.894355 140031612139328 submission.py:119] 90) loss = 6.893, grad_norm = 0.471
I0726 05:40:59.277828 139998342932224 logging_writer.py:48] [91] global_step=91, grad_norm=0.456450, loss=6.895736
I0726 05:40:59.282781 140031612139328 submission.py:119] 91) loss = 6.896, grad_norm = 0.456
I0726 05:40:59.670741 139998351324928 logging_writer.py:48] [92] global_step=92, grad_norm=0.447003, loss=6.894483
I0726 05:40:59.677524 140031612139328 submission.py:119] 92) loss = 6.894, grad_norm = 0.447
I0726 05:41:00.139886 139998342932224 logging_writer.py:48] [93] global_step=93, grad_norm=0.442876, loss=6.893325
I0726 05:41:00.145533 140031612139328 submission.py:119] 93) loss = 6.893, grad_norm = 0.443
I0726 05:41:00.530457 139998351324928 logging_writer.py:48] [94] global_step=94, grad_norm=0.468736, loss=6.891662
I0726 05:41:00.534494 140031612139328 submission.py:119] 94) loss = 6.892, grad_norm = 0.469
I0726 05:41:00.918388 139998342932224 logging_writer.py:48] [95] global_step=95, grad_norm=0.465839, loss=6.891377
I0726 05:41:00.922690 140031612139328 submission.py:119] 95) loss = 6.891, grad_norm = 0.466
I0726 05:41:01.315197 139998351324928 logging_writer.py:48] [96] global_step=96, grad_norm=0.457575, loss=6.890326
I0726 05:41:01.321693 140031612139328 submission.py:119] 96) loss = 6.890, grad_norm = 0.458
I0726 05:41:01.831171 139998342932224 logging_writer.py:48] [97] global_step=97, grad_norm=0.455561, loss=6.894152
I0726 05:41:01.835585 140031612139328 submission.py:119] 97) loss = 6.894, grad_norm = 0.456
I0726 05:41:02.224998 139998351324928 logging_writer.py:48] [98] global_step=98, grad_norm=0.458885, loss=6.890528
I0726 05:41:02.229646 140031612139328 submission.py:119] 98) loss = 6.891, grad_norm = 0.459
I0726 05:41:02.626608 139998342932224 logging_writer.py:48] [99] global_step=99, grad_norm=0.439169, loss=6.889991
I0726 05:41:02.631618 140031612139328 submission.py:119] 99) loss = 6.890, grad_norm = 0.439
I0726 05:41:03.019844 139998351324928 logging_writer.py:48] [100] global_step=100, grad_norm=0.464441, loss=6.889781
I0726 05:41:03.024744 140031612139328 submission.py:119] 100) loss = 6.890, grad_norm = 0.464
I0726 05:43:44.225402 139998342932224 logging_writer.py:48] [500] global_step=500, grad_norm=0.576802, loss=6.678736
I0726 05:43:44.232014 140031612139328 submission.py:119] 500) loss = 6.679, grad_norm = 0.577
I0726 05:47:06.775126 139998351324928 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.077031, loss=6.436125
I0726 05:47:06.781662 140031612139328 submission.py:119] 1000) loss = 6.436, grad_norm = 1.077
I0726 05:47:23.142963 140031612139328 spec.py:320] Evaluating on the training split.
I0726 05:48:09.635171 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 05:49:06.954089 140031612139328 spec.py:348] Evaluating on the test split.
I0726 05:49:08.372102 140031612139328 submission_runner.py:364] Time since start: 786.58s, 	Step: 1042, 	{'train/accuracy': 0.03572265625, 'train/loss': 5.909124755859375, 'validation/accuracy': 0.0341, 'validation/loss': 5.937893125, 'validation/num_examples': 50000, 'test/accuracy': 0.0279, 'test/loss': 6.050512890625, 'test/num_examples': 10000, 'score': 484.9739508628845, 'total_duration': 786.58371758461, 'accumulated_submission_time': 484.9739508628845, 'accumulated_eval_time': 300.6933059692383, 'accumulated_logging_time': 0.03262829780578613}
I0726 05:49:08.391767 139987857164032 logging_writer.py:48] [1042] accumulated_eval_time=300.693306, accumulated_logging_time=0.032628, accumulated_submission_time=484.973951, global_step=1042, preemption_count=0, score=484.973951, test/accuracy=0.027900, test/loss=6.050513, test/num_examples=10000, total_duration=786.583718, train/accuracy=0.035723, train/loss=5.909125, validation/accuracy=0.034100, validation/loss=5.937893, validation/num_examples=50000
I0726 05:52:10.550891 139987865556736 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.913155, loss=6.317740
I0726 05:52:10.556216 140031612139328 submission.py:119] 1500) loss = 6.318, grad_norm = 0.913
I0726 05:55:21.699711 139987857164032 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.206637, loss=6.229183
I0726 05:55:21.707035 140031612139328 submission.py:119] 2000) loss = 6.229, grad_norm = 1.207
I0726 05:56:08.620468 140031612139328 spec.py:320] Evaluating on the training split.
I0726 05:56:53.488134 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 05:57:49.379996 140031612139328 spec.py:348] Evaluating on the test split.
I0726 05:57:50.761219 140031612139328 submission_runner.py:364] Time since start: 1308.97s, 	Step: 2121, 	{'train/accuracy': 0.0801953125, 'train/loss': 5.291973876953125, 'validation/accuracy': 0.07486, 'validation/loss': 5.33839875, 'validation/num_examples': 50000, 'test/accuracy': 0.0594, 'test/loss': 5.5062265625, 'test/num_examples': 10000, 'score': 903.6267750263214, 'total_duration': 1308.9728174209595, 'accumulated_submission_time': 903.6267750263214, 'accumulated_eval_time': 402.83408641815186, 'accumulated_logging_time': 0.6922385692596436}
I0726 05:57:50.780854 139987865556736 logging_writer.py:48] [2121] accumulated_eval_time=402.834086, accumulated_logging_time=0.692239, accumulated_submission_time=903.626775, global_step=2121, preemption_count=0, score=903.626775, test/accuracy=0.059400, test/loss=5.506227, test/num_examples=10000, total_duration=1308.972817, train/accuracy=0.080195, train/loss=5.291974, validation/accuracy=0.074860, validation/loss=5.338399, validation/num_examples=50000
I0726 06:00:25.953505 139987857164032 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.886798, loss=6.003251
I0726 06:00:25.958176 140031612139328 submission.py:119] 2500) loss = 6.003, grad_norm = 0.887
I0726 06:03:37.797662 139987865556736 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.688208, loss=5.768207
I0726 06:03:37.803687 140031612139328 submission.py:119] 3000) loss = 5.768, grad_norm = 1.688
I0726 06:04:50.788125 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:05:35.466222 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:06:20.492154 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:06:21.876291 140031612139328 submission_runner.py:364] Time since start: 1820.09s, 	Step: 3189, 	{'train/accuracy': 0.14802734375, 'train/loss': 4.642053527832031, 'validation/accuracy': 0.1374, 'validation/loss': 4.718795, 'validation/num_examples': 50000, 'test/accuracy': 0.1027, 'test/loss': 5.006610546875, 'test/num_examples': 10000, 'score': 1322.0230712890625, 'total_duration': 1820.0879447460175, 'accumulated_submission_time': 1322.0230712890625, 'accumulated_eval_time': 493.92252254486084, 'accumulated_logging_time': 1.4105026721954346}
I0726 06:06:21.894298 139987857164032 logging_writer.py:48] [3189] accumulated_eval_time=493.922523, accumulated_logging_time=1.410503, accumulated_submission_time=1322.023071, global_step=3189, preemption_count=0, score=1322.023071, test/accuracy=0.102700, test/loss=5.006611, test/num_examples=10000, total_duration=1820.087945, train/accuracy=0.148027, train/loss=4.642054, validation/accuracy=0.137400, validation/loss=4.718795, validation/num_examples=50000
I0726 06:08:25.006672 139987865556736 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.155408, loss=5.592483
I0726 06:08:25.012183 140031612139328 submission.py:119] 3500) loss = 5.592, grad_norm = 1.155
I0726 06:11:42.331805 139987857164032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.851262, loss=5.569706
I0726 06:11:42.341856 140031612139328 submission.py:119] 4000) loss = 5.570, grad_norm = 0.851
I0726 06:13:22.004185 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:14:06.677186 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:14:52.128861 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:14:53.514425 140031612139328 submission_runner.py:364] Time since start: 2331.72s, 	Step: 4262, 	{'train/accuracy': 0.2078515625, 'train/loss': 4.133241882324219, 'validation/accuracy': 0.19148, 'validation/loss': 4.240198125, 'validation/num_examples': 50000, 'test/accuracy': 0.1517, 'test/loss': 4.602304296875, 'test/num_examples': 10000, 'score': 1740.5823843479156, 'total_duration': 2331.723340511322, 'accumulated_submission_time': 1740.5823843479156, 'accumulated_eval_time': 585.4300391674042, 'accumulated_logging_time': 2.071247100830078}
I0726 06:14:53.538410 139987865556736 logging_writer.py:48] [4262] accumulated_eval_time=585.430039, accumulated_logging_time=2.071247, accumulated_submission_time=1740.582384, global_step=4262, preemption_count=0, score=1740.582384, test/accuracy=0.151700, test/loss=4.602304, test/num_examples=10000, total_duration=2331.723341, train/accuracy=0.207852, train/loss=4.133242, validation/accuracy=0.191480, validation/loss=4.240198, validation/num_examples=50000
I0726 06:16:26.625323 139987857164032 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.977183, loss=5.409571
I0726 06:16:26.630633 140031612139328 submission.py:119] 4500) loss = 5.410, grad_norm = 0.977
I0726 06:19:47.838398 139987865556736 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.820315, loss=5.467359
I0726 06:19:47.843027 140031612139328 submission.py:119] 5000) loss = 5.467, grad_norm = 0.820
I0726 06:21:53.861129 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:22:38.587202 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:23:22.962972 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:23:24.353338 140031612139328 submission_runner.py:364] Time since start: 2842.56s, 	Step: 5326, 	{'train/accuracy': 0.24365234375, 'train/loss': 3.868680114746094, 'validation/accuracy': 0.22212, 'validation/loss': 3.991595625, 'validation/num_examples': 50000, 'test/accuracy': 0.1756, 'test/loss': 4.385013671875, 'test/num_examples': 10000, 'score': 2159.3170506954193, 'total_duration': 2842.5647585392, 'accumulated_submission_time': 2159.3170506954193, 'accumulated_eval_time': 675.922089099884, 'accumulated_logging_time': 2.7789664268493652}
I0726 06:23:24.381041 139987857164032 logging_writer.py:48] [5326] accumulated_eval_time=675.922089, accumulated_logging_time=2.778966, accumulated_submission_time=2159.317051, global_step=5326, preemption_count=0, score=2159.317051, test/accuracy=0.175600, test/loss=4.385014, test/num_examples=10000, total_duration=2842.564759, train/accuracy=0.243652, train/loss=3.868680, validation/accuracy=0.222120, validation/loss=3.991596, validation/num_examples=50000
I0726 06:24:31.910036 139987865556736 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.770076, loss=5.489117
I0726 06:24:31.914736 140031612139328 submission.py:119] 5500) loss = 5.489, grad_norm = 0.770
I0726 06:27:48.342600 139987857164032 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.721719, loss=5.327785
I0726 06:27:48.347573 140031612139328 submission.py:119] 6000) loss = 5.328, grad_norm = 0.722
I0726 06:30:24.695450 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:31:10.615888 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:32:06.048031 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:32:07.436491 140031612139328 submission_runner.py:364] Time since start: 3365.65s, 	Step: 6390, 	{'train/accuracy': 0.30095703125, 'train/loss': 3.4463687133789063, 'validation/accuracy': 0.2742, 'validation/loss': 3.5967803125, 'validation/num_examples': 50000, 'test/accuracy': 0.2121, 'test/loss': 4.07743828125, 'test/num_examples': 10000, 'score': 2578.104582309723, 'total_duration': 3365.64697933197, 'accumulated_submission_time': 2578.104582309723, 'accumulated_eval_time': 778.6622202396393, 'accumulated_logging_time': 3.43684720993042}
I0726 06:32:07.455191 139987865556736 logging_writer.py:48] [6390] accumulated_eval_time=778.662220, accumulated_logging_time=3.436847, accumulated_submission_time=2578.104582, global_step=6390, preemption_count=0, score=2578.104582, test/accuracy=0.212100, test/loss=4.077438, test/num_examples=10000, total_duration=3365.646979, train/accuracy=0.300957, train/loss=3.446369, validation/accuracy=0.274200, validation/loss=3.596780, validation/num_examples=50000
I0726 06:32:50.627113 139987857164032 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.749036, loss=5.050464
I0726 06:32:50.632153 140031612139328 submission.py:119] 6500) loss = 5.050, grad_norm = 0.749
I0726 06:36:02.174901 139987865556736 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.780268, loss=4.743471
I0726 06:36:02.180897 140031612139328 submission.py:119] 7000) loss = 4.743, grad_norm = 0.780
I0726 06:39:07.972716 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:39:54.343984 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:40:41.259734 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:40:42.647971 140031612139328 submission_runner.py:364] Time since start: 3880.86s, 	Step: 7462, 	{'train/accuracy': 0.33123046875, 'train/loss': 3.3091390991210936, 'validation/accuracy': 0.30104, 'validation/loss': 3.455778125, 'validation/num_examples': 50000, 'test/accuracy': 0.2332, 'test/loss': 3.930358203125, 'test/num_examples': 10000, 'score': 2997.087502002716, 'total_duration': 3880.85946726799, 'accumulated_submission_time': 2997.087502002716, 'accumulated_eval_time': 873.3373675346375, 'accumulated_logging_time': 4.087952375411987}
I0726 06:40:42.664923 139987857164032 logging_writer.py:48] [7462] accumulated_eval_time=873.337368, accumulated_logging_time=4.087952, accumulated_submission_time=2997.087502, global_step=7462, preemption_count=0, score=2997.087502, test/accuracy=0.233200, test/loss=3.930358, test/num_examples=10000, total_duration=3880.859467, train/accuracy=0.331230, train/loss=3.309139, validation/accuracy=0.301040, validation/loss=3.455778, validation/num_examples=50000
I0726 06:40:58.640231 139987865556736 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.669751, loss=4.921760
I0726 06:40:58.643964 140031612139328 submission.py:119] 7500) loss = 4.922, grad_norm = 0.670
I0726 06:44:11.646974 139987857164032 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.865070, loss=5.017412
I0726 06:44:11.654875 140031612139328 submission.py:119] 8000) loss = 5.017, grad_norm = 0.865
I0726 06:47:29.639743 139987865556736 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.679922, loss=4.616867
I0726 06:47:29.646849 140031612139328 submission.py:119] 8500) loss = 4.617, grad_norm = 0.680
I0726 06:47:42.742291 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:48:27.753781 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:49:14.524027 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:49:15.915154 140031612139328 submission_runner.py:364] Time since start: 4394.13s, 	Step: 8534, 	{'train/accuracy': 0.3875390625, 'train/loss': 2.914888916015625, 'validation/accuracy': 0.35498, 'validation/loss': 3.085346875, 'validation/num_examples': 50000, 'test/accuracy': 0.2746, 'test/loss': 3.62704609375, 'test/num_examples': 10000, 'score': 3415.586760044098, 'total_duration': 4394.125241756439, 'accumulated_submission_time': 3415.586760044098, 'accumulated_eval_time': 966.50874376297, 'accumulated_logging_time': 4.770256996154785}
I0726 06:49:15.939180 139987857164032 logging_writer.py:48] [8534] accumulated_eval_time=966.508744, accumulated_logging_time=4.770257, accumulated_submission_time=3415.586760, global_step=8534, preemption_count=0, score=3415.586760, test/accuracy=0.274600, test/loss=3.627046, test/num_examples=10000, total_duration=4394.125242, train/accuracy=0.387539, train/loss=2.914889, validation/accuracy=0.354980, validation/loss=3.085347, validation/num_examples=50000
I0726 06:52:21.266918 139987865556736 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.751595, loss=4.501641
I0726 06:52:21.273422 140031612139328 submission.py:119] 9000) loss = 4.502, grad_norm = 0.752
I0726 06:55:33.841103 139987857164032 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.770039, loss=4.549005
I0726 06:55:33.846712 140031612139328 submission.py:119] 9500) loss = 4.549, grad_norm = 0.770
I0726 06:56:16.156961 140031612139328 spec.py:320] Evaluating on the training split.
I0726 06:57:01.353848 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 06:57:56.736641 140031612139328 spec.py:348] Evaluating on the test split.
I0726 06:57:58.119739 140031612139328 submission_runner.py:364] Time since start: 4916.33s, 	Step: 9608, 	{'train/accuracy': 0.41453125, 'train/loss': 2.7507052612304688, 'validation/accuracy': 0.3829, 'validation/loss': 2.9144675, 'validation/num_examples': 50000, 'test/accuracy': 0.3003, 'test/loss': 3.461908984375, 'test/num_examples': 10000, 'score': 3834.2679607868195, 'total_duration': 4916.3312911987305, 'accumulated_submission_time': 3834.2679607868195, 'accumulated_eval_time': 1068.4716975688934, 'accumulated_logging_time': 5.419613599777222}
I0726 06:57:58.137383 139987865556736 logging_writer.py:48] [9608] accumulated_eval_time=1068.471698, accumulated_logging_time=5.419614, accumulated_submission_time=3834.267961, global_step=9608, preemption_count=0, score=3834.267961, test/accuracy=0.300300, test/loss=3.461909, test/num_examples=10000, total_duration=4916.331291, train/accuracy=0.414531, train/loss=2.750705, validation/accuracy=0.382900, validation/loss=2.914468, validation/num_examples=50000
I0726 07:00:37.852640 139987857164032 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.789178, loss=4.348610
I0726 07:00:37.857236 140031612139328 submission.py:119] 10000) loss = 4.349, grad_norm = 0.789
I0726 07:03:50.499093 139987865556736 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.638218, loss=4.271163
I0726 07:03:50.504773 140031612139328 submission.py:119] 10500) loss = 4.271, grad_norm = 0.638
I0726 07:04:58.183454 140031612139328 spec.py:320] Evaluating on the training split.
I0726 07:05:45.290706 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 07:06:30.727674 140031612139328 spec.py:348] Evaluating on the test split.
I0726 07:06:32.108505 140031612139328 submission_runner.py:364] Time since start: 5430.32s, 	Step: 10675, 	{'train/accuracy': 0.44923828125, 'train/loss': 2.5749630737304687, 'validation/accuracy': 0.41306, 'validation/loss': 2.758755625, 'validation/num_examples': 50000, 'test/accuracy': 0.3178, 'test/loss': 3.3472265625, 'test/num_examples': 10000, 'score': 4252.750945329666, 'total_duration': 5430.320100784302, 'accumulated_submission_time': 4252.750945329666, 'accumulated_eval_time': 1162.396691083908, 'accumulated_logging_time': 6.113049507141113}
I0726 07:06:32.128918 139987857164032 logging_writer.py:48] [10675] accumulated_eval_time=1162.396691, accumulated_logging_time=6.113050, accumulated_submission_time=4252.750945, global_step=10675, preemption_count=0, score=4252.750945, test/accuracy=0.317800, test/loss=3.347227, test/num_examples=10000, total_duration=5430.320101, train/accuracy=0.449238, train/loss=2.574963, validation/accuracy=0.413060, validation/loss=2.758756, validation/num_examples=50000
I0726 07:08:42.001382 139987865556736 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.590646, loss=4.476415
I0726 07:08:42.008600 140031612139328 submission.py:119] 11000) loss = 4.476, grad_norm = 0.591
I0726 07:12:01.765791 139987857164032 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.553009, loss=4.356179
I0726 07:12:01.778985 140031612139328 submission.py:119] 11500) loss = 4.356, grad_norm = 0.553
I0726 07:13:32.458136 140031612139328 spec.py:320] Evaluating on the training split.
I0726 07:14:17.340272 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 07:15:02.826803 140031612139328 spec.py:348] Evaluating on the test split.
I0726 07:15:04.216490 140031612139328 submission_runner.py:364] Time since start: 5942.43s, 	Step: 11738, 	{'train/accuracy': 0.47880859375, 'train/loss': 2.432070770263672, 'validation/accuracy': 0.43836, 'validation/loss': 2.6369159375, 'validation/num_examples': 50000, 'test/accuracy': 0.3455, 'test/loss': 3.206315234375, 'test/num_examples': 10000, 'score': 4671.516818761826, 'total_duration': 5942.42809009552, 'accumulated_submission_time': 4671.516818761826, 'accumulated_eval_time': 1254.1550159454346, 'accumulated_logging_time': 6.814506530761719}
I0726 07:15:04.233413 139987865556736 logging_writer.py:48] [11738] accumulated_eval_time=1254.155016, accumulated_logging_time=6.814507, accumulated_submission_time=4671.516819, global_step=11738, preemption_count=0, score=4671.516819, test/accuracy=0.345500, test/loss=3.206315, test/num_examples=10000, total_duration=5942.428090, train/accuracy=0.478809, train/loss=2.432071, validation/accuracy=0.438360, validation/loss=2.636916, validation/num_examples=50000
I0726 07:16:46.468252 139987857164032 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.613581, loss=4.501694
I0726 07:16:46.474805 140031612139328 submission.py:119] 12000) loss = 4.502, grad_norm = 0.614
I0726 07:20:08.109394 139987865556736 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.633798, loss=4.529226
I0726 07:20:08.115304 140031612139328 submission.py:119] 12500) loss = 4.529, grad_norm = 0.634
I0726 07:22:04.548104 140031612139328 spec.py:320] Evaluating on the training split.
I0726 07:22:50.940442 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 07:23:37.095618 140031612139328 spec.py:348] Evaluating on the test split.
I0726 07:23:38.477854 140031612139328 submission_runner.py:364] Time since start: 6456.69s, 	Step: 12802, 	{'train/accuracy': 0.50921875, 'train/loss': 2.289734344482422, 'validation/accuracy': 0.4703, 'validation/loss': 2.4959384375, 'validation/num_examples': 50000, 'test/accuracy': 0.3683, 'test/loss': 3.0845208984375, 'test/num_examples': 10000, 'score': 5090.364213228226, 'total_duration': 6456.689470052719, 'accumulated_submission_time': 5090.364213228226, 'accumulated_eval_time': 1348.0850055217743, 'accumulated_logging_time': 7.425905227661133}
I0726 07:23:38.495639 139987857164032 logging_writer.py:48] [12802] accumulated_eval_time=1348.085006, accumulated_logging_time=7.425905, accumulated_submission_time=5090.364213, global_step=12802, preemption_count=0, score=5090.364213, test/accuracy=0.368300, test/loss=3.084521, test/num_examples=10000, total_duration=6456.689470, train/accuracy=0.509219, train/loss=2.289734, validation/accuracy=0.470300, validation/loss=2.495938, validation/num_examples=50000
I0726 07:24:56.529576 139987865556736 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.704184, loss=4.751629
I0726 07:24:56.535845 140031612139328 submission.py:119] 13000) loss = 4.752, grad_norm = 0.704
I0726 07:28:14.135029 139987857164032 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.599293, loss=4.364687
I0726 07:28:14.146523 140031612139328 submission.py:119] 13500) loss = 4.365, grad_norm = 0.599
I0726 07:30:38.686905 140031612139328 spec.py:320] Evaluating on the training split.
I0726 07:31:26.474420 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 07:32:21.481983 140031612139328 spec.py:348] Evaluating on the test split.
I0726 07:32:22.862276 140031612139328 submission_runner.py:364] Time since start: 6981.07s, 	Step: 13857, 	{'train/accuracy': 0.52626953125, 'train/loss': 2.163583984375, 'validation/accuracy': 0.48428, 'validation/loss': 2.37606453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3788, 'test/loss': 2.9476853515625, 'test/num_examples': 10000, 'score': 5509.010625123978, 'total_duration': 6981.073889017105, 'accumulated_submission_time': 5509.010625123978, 'accumulated_eval_time': 1452.2604293823242, 'accumulated_logging_time': 8.094439506530762}
I0726 07:32:22.880533 139987865556736 logging_writer.py:48] [13857] accumulated_eval_time=1452.260429, accumulated_logging_time=8.094440, accumulated_submission_time=5509.010625, global_step=13857, preemption_count=0, score=5509.010625, test/accuracy=0.378800, test/loss=2.947685, test/num_examples=10000, total_duration=6981.073889, train/accuracy=0.526270, train/loss=2.163584, validation/accuracy=0.484280, validation/loss=2.376065, validation/num_examples=50000
I0726 07:33:19.084637 140031612139328 spec.py:320] Evaluating on the training split.
I0726 07:34:04.820217 140031612139328 spec.py:332] Evaluating on the validation split.
I0726 07:34:50.248092 140031612139328 spec.py:348] Evaluating on the test split.
I0726 07:34:51.636819 140031612139328 submission_runner.py:364] Time since start: 7129.85s, 	Step: 14000, 	{'train/accuracy': 0.52353515625, 'train/loss': 2.20798095703125, 'validation/accuracy': 0.4812, 'validation/loss': 2.41778546875, 'validation/num_examples': 50000, 'test/accuracy': 0.3789, 'test/loss': 3.0177076171875, 'test/num_examples': 10000, 'score': 5564.438801527023, 'total_duration': 7129.848378419876, 'accumulated_submission_time': 5564.438801527023, 'accumulated_eval_time': 1544.8125689029694, 'accumulated_logging_time': 8.761152029037476}
I0726 07:34:51.652625 139987857164032 logging_writer.py:48] [14000] accumulated_eval_time=1544.812569, accumulated_logging_time=8.761152, accumulated_submission_time=5564.438802, global_step=14000, preemption_count=0, score=5564.438802, test/accuracy=0.378900, test/loss=3.017708, test/num_examples=10000, total_duration=7129.848378, train/accuracy=0.523535, train/loss=2.207981, validation/accuracy=0.481200, validation/loss=2.417785, validation/num_examples=50000
I0726 07:34:52.295114 139987865556736 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5564.438802
I0726 07:34:52.978923 140031612139328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0726 07:34:53.344640 140031612139328 submission_runner.py:530] Tuning trial 1/1
I0726 07:34:53.344844 140031612139328 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0726 07:34:53.345837 140031612139328 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00181640625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.0018, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 65.53112864494324, 'total_duration': 260.9958744049072, 'accumulated_submission_time': 65.53112864494324, 'accumulated_eval_time': 195.4641137123108, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1042, {'train/accuracy': 0.03572265625, 'train/loss': 5.909124755859375, 'validation/accuracy': 0.0341, 'validation/loss': 5.937893125, 'validation/num_examples': 50000, 'test/accuracy': 0.0279, 'test/loss': 6.050512890625, 'test/num_examples': 10000, 'score': 484.9739508628845, 'total_duration': 786.58371758461, 'accumulated_submission_time': 484.9739508628845, 'accumulated_eval_time': 300.6933059692383, 'accumulated_logging_time': 0.03262829780578613, 'global_step': 1042, 'preemption_count': 0}), (2121, {'train/accuracy': 0.0801953125, 'train/loss': 5.291973876953125, 'validation/accuracy': 0.07486, 'validation/loss': 5.33839875, 'validation/num_examples': 50000, 'test/accuracy': 0.0594, 'test/loss': 5.5062265625, 'test/num_examples': 10000, 'score': 903.6267750263214, 'total_duration': 1308.9728174209595, 'accumulated_submission_time': 903.6267750263214, 'accumulated_eval_time': 402.83408641815186, 'accumulated_logging_time': 0.6922385692596436, 'global_step': 2121, 'preemption_count': 0}), (3189, {'train/accuracy': 0.14802734375, 'train/loss': 4.642053527832031, 'validation/accuracy': 0.1374, 'validation/loss': 4.718795, 'validation/num_examples': 50000, 'test/accuracy': 0.1027, 'test/loss': 5.006610546875, 'test/num_examples': 10000, 'score': 1322.0230712890625, 'total_duration': 1820.0879447460175, 'accumulated_submission_time': 1322.0230712890625, 'accumulated_eval_time': 493.92252254486084, 'accumulated_logging_time': 1.4105026721954346, 'global_step': 3189, 'preemption_count': 0}), (4262, {'train/accuracy': 0.2078515625, 'train/loss': 4.133241882324219, 'validation/accuracy': 0.19148, 'validation/loss': 4.240198125, 'validation/num_examples': 50000, 'test/accuracy': 0.1517, 'test/loss': 4.602304296875, 'test/num_examples': 10000, 'score': 1740.5823843479156, 'total_duration': 2331.723340511322, 'accumulated_submission_time': 1740.5823843479156, 'accumulated_eval_time': 585.4300391674042, 'accumulated_logging_time': 2.071247100830078, 'global_step': 4262, 'preemption_count': 0}), (5326, {'train/accuracy': 0.24365234375, 'train/loss': 3.868680114746094, 'validation/accuracy': 0.22212, 'validation/loss': 3.991595625, 'validation/num_examples': 50000, 'test/accuracy': 0.1756, 'test/loss': 4.385013671875, 'test/num_examples': 10000, 'score': 2159.3170506954193, 'total_duration': 2842.5647585392, 'accumulated_submission_time': 2159.3170506954193, 'accumulated_eval_time': 675.922089099884, 'accumulated_logging_time': 2.7789664268493652, 'global_step': 5326, 'preemption_count': 0}), (6390, {'train/accuracy': 0.30095703125, 'train/loss': 3.4463687133789063, 'validation/accuracy': 0.2742, 'validation/loss': 3.5967803125, 'validation/num_examples': 50000, 'test/accuracy': 0.2121, 'test/loss': 4.07743828125, 'test/num_examples': 10000, 'score': 2578.104582309723, 'total_duration': 3365.64697933197, 'accumulated_submission_time': 2578.104582309723, 'accumulated_eval_time': 778.6622202396393, 'accumulated_logging_time': 3.43684720993042, 'global_step': 6390, 'preemption_count': 0}), (7462, {'train/accuracy': 0.33123046875, 'train/loss': 3.3091390991210936, 'validation/accuracy': 0.30104, 'validation/loss': 3.455778125, 'validation/num_examples': 50000, 'test/accuracy': 0.2332, 'test/loss': 3.930358203125, 'test/num_examples': 10000, 'score': 2997.087502002716, 'total_duration': 3880.85946726799, 'accumulated_submission_time': 2997.087502002716, 'accumulated_eval_time': 873.3373675346375, 'accumulated_logging_time': 4.087952375411987, 'global_step': 7462, 'preemption_count': 0}), (8534, {'train/accuracy': 0.3875390625, 'train/loss': 2.914888916015625, 'validation/accuracy': 0.35498, 'validation/loss': 3.085346875, 'validation/num_examples': 50000, 'test/accuracy': 0.2746, 'test/loss': 3.62704609375, 'test/num_examples': 10000, 'score': 3415.586760044098, 'total_duration': 4394.125241756439, 'accumulated_submission_time': 3415.586760044098, 'accumulated_eval_time': 966.50874376297, 'accumulated_logging_time': 4.770256996154785, 'global_step': 8534, 'preemption_count': 0}), (9608, {'train/accuracy': 0.41453125, 'train/loss': 2.7507052612304688, 'validation/accuracy': 0.3829, 'validation/loss': 2.9144675, 'validation/num_examples': 50000, 'test/accuracy': 0.3003, 'test/loss': 3.461908984375, 'test/num_examples': 10000, 'score': 3834.2679607868195, 'total_duration': 4916.3312911987305, 'accumulated_submission_time': 3834.2679607868195, 'accumulated_eval_time': 1068.4716975688934, 'accumulated_logging_time': 5.419613599777222, 'global_step': 9608, 'preemption_count': 0}), (10675, {'train/accuracy': 0.44923828125, 'train/loss': 2.5749630737304687, 'validation/accuracy': 0.41306, 'validation/loss': 2.758755625, 'validation/num_examples': 50000, 'test/accuracy': 0.3178, 'test/loss': 3.3472265625, 'test/num_examples': 10000, 'score': 4252.750945329666, 'total_duration': 5430.320100784302, 'accumulated_submission_time': 4252.750945329666, 'accumulated_eval_time': 1162.396691083908, 'accumulated_logging_time': 6.113049507141113, 'global_step': 10675, 'preemption_count': 0}), (11738, {'train/accuracy': 0.47880859375, 'train/loss': 2.432070770263672, 'validation/accuracy': 0.43836, 'validation/loss': 2.6369159375, 'validation/num_examples': 50000, 'test/accuracy': 0.3455, 'test/loss': 3.206315234375, 'test/num_examples': 10000, 'score': 4671.516818761826, 'total_duration': 5942.42809009552, 'accumulated_submission_time': 4671.516818761826, 'accumulated_eval_time': 1254.1550159454346, 'accumulated_logging_time': 6.814506530761719, 'global_step': 11738, 'preemption_count': 0}), (12802, {'train/accuracy': 0.50921875, 'train/loss': 2.289734344482422, 'validation/accuracy': 0.4703, 'validation/loss': 2.4959384375, 'validation/num_examples': 50000, 'test/accuracy': 0.3683, 'test/loss': 3.0845208984375, 'test/num_examples': 10000, 'score': 5090.364213228226, 'total_duration': 6456.689470052719, 'accumulated_submission_time': 5090.364213228226, 'accumulated_eval_time': 1348.0850055217743, 'accumulated_logging_time': 7.425905227661133, 'global_step': 12802, 'preemption_count': 0}), (13857, {'train/accuracy': 0.52626953125, 'train/loss': 2.163583984375, 'validation/accuracy': 0.48428, 'validation/loss': 2.37606453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3788, 'test/loss': 2.9476853515625, 'test/num_examples': 10000, 'score': 5509.010625123978, 'total_duration': 6981.073889017105, 'accumulated_submission_time': 5509.010625123978, 'accumulated_eval_time': 1452.2604293823242, 'accumulated_logging_time': 8.094439506530762, 'global_step': 13857, 'preemption_count': 0}), (14000, {'train/accuracy': 0.52353515625, 'train/loss': 2.20798095703125, 'validation/accuracy': 0.4812, 'validation/loss': 2.41778546875, 'validation/num_examples': 50000, 'test/accuracy': 0.3789, 'test/loss': 3.0177076171875, 'test/num_examples': 10000, 'score': 5564.438801527023, 'total_duration': 7129.848378419876, 'accumulated_submission_time': 5564.438801527023, 'accumulated_eval_time': 1544.8125689029694, 'accumulated_logging_time': 8.761152029037476, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0726 07:34:53.345951 140031612139328 submission_runner.py:533] Timing: 5564.438801527023
I0726 07:34:53.346001 140031612139328 submission_runner.py:535] Total number of evals: 15
I0726 07:34:53.346061 140031612139328 submission_runner.py:536] ====================
I0726 07:34:53.346185 140031612139328 submission_runner.py:604] Final imagenet_vit score: 5564.438801527023
