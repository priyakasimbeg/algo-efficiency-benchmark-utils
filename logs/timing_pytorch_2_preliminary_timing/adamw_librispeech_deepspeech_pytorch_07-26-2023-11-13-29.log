torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_timing/adamw --overwrite=True --save_checkpoints=False --max_global_steps=8000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --torch_compile=True 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_07-26-2023-11-13-29.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-26 11:13:40.333205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 11:13:40.333223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0726 11:13:55.162331 139826487465792 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0726 11:13:55.162361 140248730228544 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0726 11:13:55.163395 140218124355392 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0726 11:13:55.163549 140297781479232 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0726 11:13:55.163620 140715249198912 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0726 11:13:55.163777 139861273769792 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0726 11:13:55.164439 139904718640960 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0726 11:13:55.174300 139861273769792 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.174275 140667560675136 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0726 11:13:55.174540 140667560675136 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.174977 139904718640960 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.183208 139826487465792 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.183235 140248730228544 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.184159 140218124355392 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.184426 140297781479232 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.184477 140715249198912 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 11:13:55.697478 140667560675136 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/librispeech_deepspeech_pytorch.
W0726 11:13:55.740845 140297781479232 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.740864 140248730228544 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.741260 139826487465792 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.741198 140667560675136 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.741394 139904718640960 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.741607 140218124355392 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.741929 140715249198912 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 11:13:55.742052 139861273769792 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0726 11:13:55.748435 140667560675136 submission_runner.py:490] Using RNG seed 3407521296
I0726 11:13:55.750215 140667560675136 submission_runner.py:499] --- Tuning run 1/1 ---
I0726 11:13:55.750382 140667560675136 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/librispeech_deepspeech_pytorch/trial_1.
I0726 11:13:55.750661 140667560675136 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0726 11:13:55.751418 140667560675136 submission_runner.py:176] Initializing dataset.
I0726 11:13:55.751548 140667560675136 input_pipeline.py:20] Loading split = train-clean-100
I0726 11:13:55.809875 140667560675136 input_pipeline.py:20] Loading split = train-clean-360
I0726 11:13:56.231304 140667560675136 input_pipeline.py:20] Loading split = train-other-500
I0726 11:13:56.711720 140667560675136 submission_runner.py:183] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
W0726 11:14:04.828350 140297781479232 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 11:14:04.829939 140667560675136 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 11:14:04.829949 139826487465792 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 11:14:04.829989 140715249198912 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0726 11:14:04.830171 140667560675136 submission_runner.py:217] Initializing optimizer.
I0726 11:14:04.830986 140667560675136 submission_runner.py:224] Initializing metrics bundle.
I0726 11:14:04.831087 140667560675136 submission_runner.py:242] Initializing checkpoint and logger.
I0726 11:14:04.831798 140667560675136 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0726 11:14:04.831906 140667560675136 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
W0726 11:14:04.836035 140218124355392 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 11:14:04.836315 139904718640960 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 11:14:04.845379 139861273769792 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 11:14:04.845998 140248730228544 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0726 11:14:05.418890 140667560675136 submission_runner.py:263] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0726 11:14:05.419828 140667560675136 submission_runner.py:266] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0726 11:14:05.434562 140667560675136 submission_runner.py:276] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
I0726 11:14:40.156296 140641765934848 logging_writer.py:48] [0] global_step=0, grad_norm=23.733160, loss=33.211933
I0726 11:14:40.188848 140667560675136 submission.py:119] 0) loss = 33.212, grad_norm = 23.733
I0726 11:14:40.191004 140667560675136 spec.py:320] Evaluating on the training split.
I0726 11:14:40.192353 140667560675136 input_pipeline.py:20] Loading split = train-clean-100
I0726 11:14:40.230288 140667560675136 input_pipeline.py:20] Loading split = train-clean-360
I0726 11:14:40.785244 140667560675136 input_pipeline.py:20] Loading split = train-other-500
I0726 11:15:00.292700 140667560675136 spec.py:332] Evaluating on the validation split.
I0726 11:15:00.294151 140667560675136 input_pipeline.py:20] Loading split = dev-clean
I0726 11:15:00.298087 140667560675136 input_pipeline.py:20] Loading split = dev-other
I0726 11:15:14.935794 140667560675136 spec.py:348] Evaluating on the test split.
I0726 11:15:14.937322 140667560675136 input_pipeline.py:20] Loading split = test-clean
I0726 11:15:24.011337 140667560675136 submission_runner.py:364] Time since start: 78.58s, 	Step: 1, 	{'train/ctc_loss': 30.431378151489177, 'train/wer': 3.1459831434758305, 'validation/ctc_loss': 28.911197759694595, 'validation/wer': 2.7678462801139383, 'validation/num_examples': 5348, 'test/ctc_loss': 29.154184140912317, 'test/wer': 3.2043344910933724, 'test/num_examples': 2472, 'score': 34.75623631477356, 'total_duration': 78.5769419670105, 'accumulated_submission_time': 34.75623631477356, 'accumulated_eval_time': 43.82002592086792, 'accumulated_logging_time': 0}
I0726 11:15:24.038985 140627545716480 logging_writer.py:48] [1] accumulated_eval_time=43.820026, accumulated_logging_time=0, accumulated_submission_time=34.756236, global_step=1, preemption_count=0, score=34.756236, test/ctc_loss=29.154184, test/num_examples=2472, test/wer=3.204334, total_duration=78.576942, train/ctc_loss=30.431378, train/wer=3.145983, validation/ctc_loss=28.911198, validation/num_examples=5348, validation/wer=2.767846
I0726 11:15:24.156715 140667560675136 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.156823 140248730228544 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.156845 139861273769792 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.156869 140715249198912 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.156908 140218124355392 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.156970 140297781479232 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.157049 139904718640960 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:24.157491 139826487465792 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 11:15:25.146596 140627537323776 logging_writer.py:48] [1] global_step=1, grad_norm=22.450682, loss=32.593048
I0726 11:15:25.150670 140667560675136 submission.py:119] 1) loss = 32.593, grad_norm = 22.451
I0726 11:15:26.058685 140627545716480 logging_writer.py:48] [2] global_step=2, grad_norm=22.337036, loss=33.101521
I0726 11:15:26.062566 140667560675136 submission.py:119] 2) loss = 33.102, grad_norm = 22.337
I0726 11:15:26.973066 140627537323776 logging_writer.py:48] [3] global_step=3, grad_norm=22.961407, loss=33.161030
I0726 11:15:26.977048 140667560675136 submission.py:119] 3) loss = 33.161, grad_norm = 22.961
I0726 11:15:27.890949 140627545716480 logging_writer.py:48] [4] global_step=4, grad_norm=23.823170, loss=32.712444
I0726 11:15:27.894923 140667560675136 submission.py:119] 4) loss = 32.712, grad_norm = 23.823
I0726 11:15:28.824994 140627537323776 logging_writer.py:48] [5] global_step=5, grad_norm=25.265049, loss=32.936588
I0726 11:15:28.828791 140667560675136 submission.py:119] 5) loss = 32.937, grad_norm = 25.265
I0726 11:15:29.743032 140627545716480 logging_writer.py:48] [6] global_step=6, grad_norm=26.138832, loss=33.077190
I0726 11:15:29.746807 140667560675136 submission.py:119] 6) loss = 33.077, grad_norm = 26.139
I0726 11:15:30.653533 140627537323776 logging_writer.py:48] [7] global_step=7, grad_norm=27.345734, loss=32.080933
I0726 11:15:30.657196 140667560675136 submission.py:119] 7) loss = 32.081, grad_norm = 27.346
I0726 11:15:31.570935 140627545716480 logging_writer.py:48] [8] global_step=8, grad_norm=30.554586, loss=32.486385
I0726 11:15:31.574838 140667560675136 submission.py:119] 8) loss = 32.486, grad_norm = 30.555
I0726 11:15:32.480363 140627537323776 logging_writer.py:48] [9] global_step=9, grad_norm=29.666922, loss=32.139145
I0726 11:15:32.484245 140667560675136 submission.py:119] 9) loss = 32.139, grad_norm = 29.667
I0726 11:15:33.398417 140627545716480 logging_writer.py:48] [10] global_step=10, grad_norm=32.303692, loss=32.002945
I0726 11:15:33.402557 140667560675136 submission.py:119] 10) loss = 32.003, grad_norm = 32.304
I0726 11:15:34.314710 140627537323776 logging_writer.py:48] [11] global_step=11, grad_norm=35.963482, loss=32.154404
I0726 11:15:34.318683 140667560675136 submission.py:119] 11) loss = 32.154, grad_norm = 35.963
I0726 11:15:35.235655 140627545716480 logging_writer.py:48] [12] global_step=12, grad_norm=37.659527, loss=32.222042
I0726 11:15:35.239590 140667560675136 submission.py:119] 12) loss = 32.222, grad_norm = 37.660
I0726 11:15:36.157370 140627537323776 logging_writer.py:48] [13] global_step=13, grad_norm=39.444481, loss=31.693014
I0726 11:15:36.161341 140667560675136 submission.py:119] 13) loss = 31.693, grad_norm = 39.444
I0726 11:15:37.081416 140627545716480 logging_writer.py:48] [14] global_step=14, grad_norm=42.146275, loss=31.223280
I0726 11:15:37.085128 140667560675136 submission.py:119] 14) loss = 31.223, grad_norm = 42.146
I0726 11:15:38.001034 140627537323776 logging_writer.py:48] [15] global_step=15, grad_norm=39.788807, loss=30.311859
I0726 11:15:38.005036 140667560675136 submission.py:119] 15) loss = 30.312, grad_norm = 39.789
I0726 11:15:38.912135 140627545716480 logging_writer.py:48] [16] global_step=16, grad_norm=40.963612, loss=30.588657
I0726 11:15:38.915967 140667560675136 submission.py:119] 16) loss = 30.589, grad_norm = 40.964
I0726 11:15:39.831316 140627537323776 logging_writer.py:48] [17] global_step=17, grad_norm=39.442509, loss=30.210541
I0726 11:15:39.835585 140667560675136 submission.py:119] 17) loss = 30.211, grad_norm = 39.443
I0726 11:15:40.745833 140627545716480 logging_writer.py:48] [18] global_step=18, grad_norm=38.698135, loss=29.708157
I0726 11:15:40.749482 140667560675136 submission.py:119] 18) loss = 29.708, grad_norm = 38.698
I0726 11:15:41.658711 140627537323776 logging_writer.py:48] [19] global_step=19, grad_norm=38.198257, loss=29.079985
I0726 11:15:41.662793 140667560675136 submission.py:119] 19) loss = 29.080, grad_norm = 38.198
I0726 11:15:42.582468 140627545716480 logging_writer.py:48] [20] global_step=20, grad_norm=37.134922, loss=28.350830
I0726 11:15:42.586224 140667560675136 submission.py:119] 20) loss = 28.351, grad_norm = 37.135
I0726 11:15:43.500244 140627537323776 logging_writer.py:48] [21] global_step=21, grad_norm=38.504601, loss=28.023529
I0726 11:15:43.504242 140667560675136 submission.py:119] 21) loss = 28.024, grad_norm = 38.505
I0726 11:15:44.416831 140627545716480 logging_writer.py:48] [22] global_step=22, grad_norm=38.861698, loss=28.225468
I0726 11:15:44.420464 140667560675136 submission.py:119] 22) loss = 28.225, grad_norm = 38.862
I0726 11:15:45.346279 140627537323776 logging_writer.py:48] [23] global_step=23, grad_norm=37.346157, loss=27.133047
I0726 11:15:45.349971 140667560675136 submission.py:119] 23) loss = 27.133, grad_norm = 37.346
I0726 11:15:46.271094 140627545716480 logging_writer.py:48] [24] global_step=24, grad_norm=37.611778, loss=27.086420
I0726 11:15:46.274906 140667560675136 submission.py:119] 24) loss = 27.086, grad_norm = 37.612
I0726 11:15:47.196977 140627537323776 logging_writer.py:48] [25] global_step=25, grad_norm=38.949715, loss=26.690042
I0726 11:15:47.200664 140667560675136 submission.py:119] 25) loss = 26.690, grad_norm = 38.950
I0726 11:15:48.115806 140627545716480 logging_writer.py:48] [26] global_step=26, grad_norm=38.877224, loss=25.757093
I0726 11:15:48.119451 140667560675136 submission.py:119] 26) loss = 25.757, grad_norm = 38.877
I0726 11:15:49.042881 140627537323776 logging_writer.py:48] [27] global_step=27, grad_norm=38.273098, loss=25.441864
I0726 11:15:49.046468 140667560675136 submission.py:119] 27) loss = 25.442, grad_norm = 38.273
I0726 11:15:49.961970 140627545716480 logging_writer.py:48] [28] global_step=28, grad_norm=36.888664, loss=24.759272
I0726 11:15:49.966189 140667560675136 submission.py:119] 28) loss = 24.759, grad_norm = 36.889
I0726 11:15:50.886107 140627537323776 logging_writer.py:48] [29] global_step=29, grad_norm=36.001396, loss=23.841631
I0726 11:15:50.889979 140667560675136 submission.py:119] 29) loss = 23.842, grad_norm = 36.001
I0726 11:15:51.804280 140627545716480 logging_writer.py:48] [30] global_step=30, grad_norm=36.114021, loss=23.306293
I0726 11:15:51.808134 140667560675136 submission.py:119] 30) loss = 23.306, grad_norm = 36.114
I0726 11:15:52.717357 140627537323776 logging_writer.py:48] [31] global_step=31, grad_norm=36.354584, loss=22.817759
I0726 11:15:52.721333 140667560675136 submission.py:119] 31) loss = 22.818, grad_norm = 36.355
I0726 11:15:53.642884 140627545716480 logging_writer.py:48] [32] global_step=32, grad_norm=35.151825, loss=22.207420
I0726 11:15:53.646536 140667560675136 submission.py:119] 32) loss = 22.207, grad_norm = 35.152
I0726 11:15:54.564733 140627537323776 logging_writer.py:48] [33] global_step=33, grad_norm=34.713833, loss=21.892204
I0726 11:15:54.568433 140667560675136 submission.py:119] 33) loss = 21.892, grad_norm = 34.714
I0726 11:15:55.482538 140627545716480 logging_writer.py:48] [34] global_step=34, grad_norm=33.737316, loss=21.552525
I0726 11:15:55.486706 140667560675136 submission.py:119] 34) loss = 21.553, grad_norm = 33.737
I0726 11:15:56.402592 140627537323776 logging_writer.py:48] [35] global_step=35, grad_norm=33.965439, loss=20.596834
I0726 11:15:56.406458 140667560675136 submission.py:119] 35) loss = 20.597, grad_norm = 33.965
I0726 11:15:57.324243 140627545716480 logging_writer.py:48] [36] global_step=36, grad_norm=32.521385, loss=20.173407
I0726 11:15:57.328053 140667560675136 submission.py:119] 36) loss = 20.173, grad_norm = 32.521
I0726 11:15:58.239240 140627537323776 logging_writer.py:48] [37] global_step=37, grad_norm=30.969894, loss=19.235081
I0726 11:15:58.243309 140667560675136 submission.py:119] 37) loss = 19.235, grad_norm = 30.970
I0726 11:15:59.152140 140627545716480 logging_writer.py:48] [38] global_step=38, grad_norm=30.696135, loss=19.000528
I0726 11:15:59.156012 140667560675136 submission.py:119] 38) loss = 19.001, grad_norm = 30.696
I0726 11:16:00.064056 140627537323776 logging_writer.py:48] [39] global_step=39, grad_norm=30.651394, loss=18.404966
I0726 11:16:00.068097 140667560675136 submission.py:119] 39) loss = 18.405, grad_norm = 30.651
I0726 11:16:00.981955 140627545716480 logging_writer.py:48] [40] global_step=40, grad_norm=29.209711, loss=17.926323
I0726 11:16:00.985743 140667560675136 submission.py:119] 40) loss = 17.926, grad_norm = 29.210
I0726 11:16:01.916574 140627537323776 logging_writer.py:48] [41] global_step=41, grad_norm=28.951147, loss=17.115248
I0726 11:16:01.920557 140667560675136 submission.py:119] 41) loss = 17.115, grad_norm = 28.951
I0726 11:16:02.836919 140627545716480 logging_writer.py:48] [42] global_step=42, grad_norm=28.492970, loss=16.702482
I0726 11:16:02.840989 140667560675136 submission.py:119] 42) loss = 16.702, grad_norm = 28.493
I0726 11:16:03.753122 140627537323776 logging_writer.py:48] [43] global_step=43, grad_norm=26.385382, loss=16.399021
I0726 11:16:03.756815 140667560675136 submission.py:119] 43) loss = 16.399, grad_norm = 26.385
I0726 11:16:04.670917 140627545716480 logging_writer.py:48] [44] global_step=44, grad_norm=26.610746, loss=15.903447
I0726 11:16:04.674823 140667560675136 submission.py:119] 44) loss = 15.903, grad_norm = 26.611
I0726 11:16:05.584162 140627537323776 logging_writer.py:48] [45] global_step=45, grad_norm=25.282387, loss=15.126065
I0726 11:16:05.588401 140667560675136 submission.py:119] 45) loss = 15.126, grad_norm = 25.282
I0726 11:16:06.514067 140627545716480 logging_writer.py:48] [46] global_step=46, grad_norm=24.671181, loss=14.594281
I0726 11:16:06.518103 140667560675136 submission.py:119] 46) loss = 14.594, grad_norm = 24.671
I0726 11:16:07.429159 140627537323776 logging_writer.py:48] [47] global_step=47, grad_norm=23.855827, loss=14.125260
I0726 11:16:07.433349 140667560675136 submission.py:119] 47) loss = 14.125, grad_norm = 23.856
I0726 11:16:08.344502 140627545716480 logging_writer.py:48] [48] global_step=48, grad_norm=24.099512, loss=13.959086
I0726 11:16:08.348243 140667560675136 submission.py:119] 48) loss = 13.959, grad_norm = 24.100
I0726 11:16:09.255824 140627537323776 logging_writer.py:48] [49] global_step=49, grad_norm=22.602068, loss=13.389585
I0726 11:16:09.259737 140667560675136 submission.py:119] 49) loss = 13.390, grad_norm = 22.602
I0726 11:16:10.170636 140627545716480 logging_writer.py:48] [50] global_step=50, grad_norm=20.326061, loss=13.081140
I0726 11:16:10.174542 140667560675136 submission.py:119] 50) loss = 13.081, grad_norm = 20.326
I0726 11:16:11.088377 140627537323776 logging_writer.py:48] [51] global_step=51, grad_norm=19.617264, loss=12.440447
I0726 11:16:11.092183 140667560675136 submission.py:119] 51) loss = 12.440, grad_norm = 19.617
I0726 11:16:12.010536 140627545716480 logging_writer.py:48] [52] global_step=52, grad_norm=18.543140, loss=11.759102
I0726 11:16:12.014488 140667560675136 submission.py:119] 52) loss = 11.759, grad_norm = 18.543
I0726 11:16:12.924126 140627537323776 logging_writer.py:48] [53] global_step=53, grad_norm=17.208035, loss=11.840023
I0726 11:16:12.927785 140667560675136 submission.py:119] 53) loss = 11.840, grad_norm = 17.208
I0726 11:16:13.835830 140627545716480 logging_writer.py:48] [54] global_step=54, grad_norm=16.888987, loss=11.792995
I0726 11:16:13.839494 140667560675136 submission.py:119] 54) loss = 11.793, grad_norm = 16.889
I0726 11:16:14.746554 140627537323776 logging_writer.py:48] [55] global_step=55, grad_norm=15.995229, loss=11.508123
I0726 11:16:14.750633 140667560675136 submission.py:119] 55) loss = 11.508, grad_norm = 15.995
I0726 11:16:15.662215 140627545716480 logging_writer.py:48] [56] global_step=56, grad_norm=14.447363, loss=10.958987
I0726 11:16:15.666219 140667560675136 submission.py:119] 56) loss = 10.959, grad_norm = 14.447
I0726 11:16:16.585167 140627537323776 logging_writer.py:48] [57] global_step=57, grad_norm=13.697845, loss=10.957290
I0726 11:16:16.589225 140667560675136 submission.py:119] 57) loss = 10.957, grad_norm = 13.698
I0726 11:16:17.505555 140627545716480 logging_writer.py:48] [58] global_step=58, grad_norm=12.796585, loss=10.543635
I0726 11:16:17.509367 140667560675136 submission.py:119] 58) loss = 10.544, grad_norm = 12.797
I0726 11:16:18.419598 140627537323776 logging_writer.py:48] [59] global_step=59, grad_norm=11.976225, loss=10.566001
I0726 11:16:18.423637 140667560675136 submission.py:119] 59) loss = 10.566, grad_norm = 11.976
I0726 11:16:19.331635 140627545716480 logging_writer.py:48] [60] global_step=60, grad_norm=11.560631, loss=10.398591
I0726 11:16:19.335887 140667560675136 submission.py:119] 60) loss = 10.399, grad_norm = 11.561
I0726 11:16:20.245393 140627537323776 logging_writer.py:48] [61] global_step=61, grad_norm=10.592326, loss=10.077950
I0726 11:16:20.249700 140667560675136 submission.py:119] 61) loss = 10.078, grad_norm = 10.592
I0726 11:16:21.160337 140627545716480 logging_writer.py:48] [62] global_step=62, grad_norm=10.625711, loss=9.966645
I0726 11:16:21.164291 140667560675136 submission.py:119] 62) loss = 9.967, grad_norm = 10.626
I0726 11:16:22.075630 140627537323776 logging_writer.py:48] [63] global_step=63, grad_norm=10.791309, loss=10.060994
I0726 11:16:22.079439 140667560675136 submission.py:119] 63) loss = 10.061, grad_norm = 10.791
I0726 11:16:22.997985 140627545716480 logging_writer.py:48] [64] global_step=64, grad_norm=9.011702, loss=9.786550
I0726 11:16:23.001908 140667560675136 submission.py:119] 64) loss = 9.787, grad_norm = 9.012
I0726 11:16:23.919177 140627537323776 logging_writer.py:48] [65] global_step=65, grad_norm=8.994181, loss=9.545521
I0726 11:16:23.923058 140667560675136 submission.py:119] 65) loss = 9.546, grad_norm = 8.994
I0726 11:16:24.831709 140627545716480 logging_writer.py:48] [66] global_step=66, grad_norm=8.103859, loss=9.424374
I0726 11:16:24.835984 140667560675136 submission.py:119] 66) loss = 9.424, grad_norm = 8.104
I0726 11:16:25.748203 140627537323776 logging_writer.py:48] [67] global_step=67, grad_norm=8.307423, loss=9.304296
I0726 11:16:25.752275 140667560675136 submission.py:119] 67) loss = 9.304, grad_norm = 8.307
I0726 11:16:26.664372 140627545716480 logging_writer.py:48] [68] global_step=68, grad_norm=7.410960, loss=9.117580
I0726 11:16:26.668298 140667560675136 submission.py:119] 68) loss = 9.118, grad_norm = 7.411
I0726 11:16:27.581024 140627537323776 logging_writer.py:48] [69] global_step=69, grad_norm=7.885273, loss=9.417479
I0726 11:16:27.584871 140667560675136 submission.py:119] 69) loss = 9.417, grad_norm = 7.885
I0726 11:16:28.519116 140627545716480 logging_writer.py:48] [70] global_step=70, grad_norm=7.531543, loss=9.186724
I0726 11:16:28.523151 140667560675136 submission.py:119] 70) loss = 9.187, grad_norm = 7.532
I0726 11:16:29.436335 140627537323776 logging_writer.py:48] [71] global_step=71, grad_norm=7.075337, loss=9.107409
I0726 11:16:29.440564 140667560675136 submission.py:119] 71) loss = 9.107, grad_norm = 7.075
I0726 11:16:30.352274 140627545716480 logging_writer.py:48] [72] global_step=72, grad_norm=6.731636, loss=8.993060
I0726 11:16:30.356328 140667560675136 submission.py:119] 72) loss = 8.993, grad_norm = 6.732
I0726 11:16:31.264649 140627537323776 logging_writer.py:48] [73] global_step=73, grad_norm=6.944343, loss=8.797491
I0726 11:16:31.268601 140667560675136 submission.py:119] 73) loss = 8.797, grad_norm = 6.944
I0726 11:16:32.184535 140627545716480 logging_writer.py:48] [74] global_step=74, grad_norm=6.675250, loss=8.818057
I0726 11:16:32.188467 140667560675136 submission.py:119] 74) loss = 8.818, grad_norm = 6.675
I0726 11:16:33.109855 140627537323776 logging_writer.py:48] [75] global_step=75, grad_norm=6.331887, loss=8.725972
I0726 11:16:33.113551 140667560675136 submission.py:119] 75) loss = 8.726, grad_norm = 6.332
I0726 11:16:34.049084 140627545716480 logging_writer.py:48] [76] global_step=76, grad_norm=5.937688, loss=8.607470
I0726 11:16:34.053107 140667560675136 submission.py:119] 76) loss = 8.607, grad_norm = 5.938
I0726 11:16:34.971356 140627537323776 logging_writer.py:48] [77] global_step=77, grad_norm=5.757125, loss=8.531584
I0726 11:16:34.975047 140667560675136 submission.py:119] 77) loss = 8.532, grad_norm = 5.757
I0726 11:16:35.889515 140627545716480 logging_writer.py:48] [78] global_step=78, grad_norm=5.685494, loss=8.461056
I0726 11:16:35.893273 140667560675136 submission.py:119] 78) loss = 8.461, grad_norm = 5.685
I0726 11:16:36.806512 140627537323776 logging_writer.py:48] [79] global_step=79, grad_norm=5.873638, loss=8.439991
I0726 11:16:36.810703 140667560675136 submission.py:119] 79) loss = 8.440, grad_norm = 5.874
I0726 11:16:37.723735 140627545716480 logging_writer.py:48] [80] global_step=80, grad_norm=5.425517, loss=8.458839
I0726 11:16:37.727891 140667560675136 submission.py:119] 80) loss = 8.459, grad_norm = 5.426
I0726 11:16:38.637295 140627537323776 logging_writer.py:48] [81] global_step=81, grad_norm=6.094431, loss=8.425957
I0726 11:16:38.641074 140667560675136 submission.py:119] 81) loss = 8.426, grad_norm = 6.094
I0726 11:16:39.566380 140627545716480 logging_writer.py:48] [82] global_step=82, grad_norm=5.032259, loss=8.256403
I0726 11:16:39.570308 140667560675136 submission.py:119] 82) loss = 8.256, grad_norm = 5.032
I0726 11:16:40.474776 140627537323776 logging_writer.py:48] [83] global_step=83, grad_norm=5.217538, loss=8.344702
I0726 11:16:40.478528 140667560675136 submission.py:119] 83) loss = 8.345, grad_norm = 5.218
I0726 11:16:41.398099 140627545716480 logging_writer.py:48] [84] global_step=84, grad_norm=5.132270, loss=8.200712
I0726 11:16:41.402009 140667560675136 submission.py:119] 84) loss = 8.201, grad_norm = 5.132
I0726 11:16:42.314226 140627537323776 logging_writer.py:48] [85] global_step=85, grad_norm=4.899323, loss=8.136406
I0726 11:16:42.318453 140667560675136 submission.py:119] 85) loss = 8.136, grad_norm = 4.899
I0726 11:16:43.235138 140627545716480 logging_writer.py:48] [86] global_step=86, grad_norm=4.556487, loss=7.999382
I0726 11:16:43.239083 140667560675136 submission.py:119] 86) loss = 7.999, grad_norm = 4.556
I0726 11:16:44.146592 140627537323776 logging_writer.py:48] [87] global_step=87, grad_norm=4.772569, loss=7.953093
I0726 11:16:44.150471 140667560675136 submission.py:119] 87) loss = 7.953, grad_norm = 4.773
I0726 11:16:45.085604 140627545716480 logging_writer.py:48] [88] global_step=88, grad_norm=4.819997, loss=8.009373
I0726 11:16:45.089452 140667560675136 submission.py:119] 88) loss = 8.009, grad_norm = 4.820
I0726 11:16:46.003082 140627537323776 logging_writer.py:48] [89] global_step=89, grad_norm=4.668444, loss=7.972283
I0726 11:16:46.006959 140667560675136 submission.py:119] 89) loss = 7.972, grad_norm = 4.668
I0726 11:16:46.921014 140627545716480 logging_writer.py:48] [90] global_step=90, grad_norm=4.094674, loss=7.736663
I0726 11:16:46.924765 140667560675136 submission.py:119] 90) loss = 7.737, grad_norm = 4.095
I0726 11:16:47.833266 140627537323776 logging_writer.py:48] [91] global_step=91, grad_norm=4.422111, loss=7.935231
I0726 11:16:47.837412 140667560675136 submission.py:119] 91) loss = 7.935, grad_norm = 4.422
I0726 11:16:48.754267 140627545716480 logging_writer.py:48] [92] global_step=92, grad_norm=4.217075, loss=7.775083
I0726 11:16:48.758158 140667560675136 submission.py:119] 92) loss = 7.775, grad_norm = 4.217
I0726 11:16:49.671970 140627537323776 logging_writer.py:48] [93] global_step=93, grad_norm=4.300707, loss=7.766071
I0726 11:16:49.675549 140667560675136 submission.py:119] 93) loss = 7.766, grad_norm = 4.301
I0726 11:16:50.601687 140627545716480 logging_writer.py:48] [94] global_step=94, grad_norm=4.082162, loss=7.640006
I0726 11:16:50.605387 140667560675136 submission.py:119] 94) loss = 7.640, grad_norm = 4.082
I0726 11:16:51.519746 140627537323776 logging_writer.py:48] [95] global_step=95, grad_norm=3.791295, loss=7.537670
I0726 11:16:51.523596 140667560675136 submission.py:119] 95) loss = 7.538, grad_norm = 3.791
I0726 11:16:52.438835 140627545716480 logging_writer.py:48] [96] global_step=96, grad_norm=3.825155, loss=7.608899
I0726 11:16:52.443552 140667560675136 submission.py:119] 96) loss = 7.609, grad_norm = 3.825
I0726 11:16:53.350284 140627537323776 logging_writer.py:48] [97] global_step=97, grad_norm=3.542483, loss=7.543893
I0726 11:16:53.353941 140667560675136 submission.py:119] 97) loss = 7.544, grad_norm = 3.542
I0726 11:16:54.269591 140627545716480 logging_writer.py:48] [98] global_step=98, grad_norm=3.559043, loss=7.574147
I0726 11:16:54.273405 140667560675136 submission.py:119] 98) loss = 7.574, grad_norm = 3.559
I0726 11:16:55.178087 140627537323776 logging_writer.py:48] [99] global_step=99, grad_norm=3.441055, loss=7.503387
I0726 11:16:55.181974 140667560675136 submission.py:119] 99) loss = 7.503, grad_norm = 3.441
I0726 11:16:56.112216 140627545716480 logging_writer.py:48] [100] global_step=100, grad_norm=3.189443, loss=7.382458
I0726 11:16:56.115942 140667560675136 submission.py:119] 100) loss = 7.382, grad_norm = 3.189
I0726 11:22:57.407488 140627537323776 logging_writer.py:48] [500] global_step=500, grad_norm=1.360263, loss=5.753656
I0726 11:22:57.412019 140667560675136 submission.py:119] 500) loss = 5.754, grad_norm = 1.360
I0726 11:30:28.720339 140627545716480 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.925387, loss=4.353713
I0726 11:30:28.724621 140667560675136 submission.py:119] 1000) loss = 4.354, grad_norm = 2.925
I0726 11:38:00.950785 140627545716480 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.216077, loss=3.454521
I0726 11:38:00.958776 140667560675136 submission.py:119] 1500) loss = 3.455, grad_norm = 3.216
I0726 11:45:30.159246 140627537323776 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.173105, loss=2.862786
I0726 11:45:30.164400 140667560675136 submission.py:119] 2000) loss = 2.863, grad_norm = 3.173
I0726 11:53:00.990981 140627545716480 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.999633, loss=2.604490
I0726 11:53:00.998089 140667560675136 submission.py:119] 2500) loss = 2.604, grad_norm = 2.000
I0726 11:55:24.284856 140667560675136 spec.py:320] Evaluating on the training split.
I0726 11:55:35.791537 140667560675136 spec.py:332] Evaluating on the validation split.
I0726 11:55:46.372312 140667560675136 spec.py:348] Evaluating on the test split.
I0726 11:55:51.966455 140667560675136 submission_runner.py:364] Time since start: 2506.53s, 	Step: 2661, 	{'train/ctc_loss': 5.271449004721281, 'train/wer': 0.9079295130277161, 'validation/ctc_loss': 5.174701674578059, 'validation/wer': 0.8734900786945397, 'validation/num_examples': 5348, 'test/ctc_loss': 4.976610184528034, 'test/wer': 0.8652123575650479, 'test/num_examples': 2472, 'score': 2433.744001150131, 'total_duration': 2506.531156539917, 'accumulated_submission_time': 2433.744001150131, 'accumulated_eval_time': 71.50028395652771, 'accumulated_logging_time': 0.0988776683807373}
I0726 11:55:52.001450 140627545716480 logging_writer.py:48] [2661] accumulated_eval_time=71.500284, accumulated_logging_time=0.098878, accumulated_submission_time=2433.744001, global_step=2661, preemption_count=0, score=2433.744001, test/ctc_loss=4.976610, test/num_examples=2472, test/wer=0.865212, total_duration=2506.531157, train/ctc_loss=5.271449, train/wer=0.907930, validation/ctc_loss=5.174702, validation/num_examples=5348, validation/wer=0.873490
I0726 12:00:56.631280 140627537323776 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.364944, loss=2.481145
I0726 12:00:56.636161 140667560675136 submission.py:119] 3000) loss = 2.481, grad_norm = 3.365
I0726 12:08:25.666653 140627545716480 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.846007, loss=2.311039
I0726 12:08:25.673781 140667560675136 submission.py:119] 3500) loss = 2.311, grad_norm = 2.846
I0726 12:15:52.304384 140627537323776 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.657163, loss=2.157887
I0726 12:15:52.308639 140667560675136 submission.py:119] 4000) loss = 2.158, grad_norm = 2.657
I0726 12:23:20.428058 140627545716480 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.472941, loss=2.077758
I0726 12:23:20.435672 140667560675136 submission.py:119] 4500) loss = 2.078, grad_norm = 2.473
I0726 12:30:47.274575 140627537323776 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.450424, loss=2.098111
I0726 12:30:47.280016 140667560675136 submission.py:119] 5000) loss = 2.098, grad_norm = 3.450
I0726 12:35:52.594667 140667560675136 spec.py:320] Evaluating on the training split.
I0726 12:36:05.648584 140667560675136 spec.py:332] Evaluating on the validation split.
I0726 12:36:16.830317 140667560675136 spec.py:348] Evaluating on the test split.
I0726 12:36:22.573111 140667560675136 submission_runner.py:364] Time since start: 4937.14s, 	Step: 5341, 	{'train/ctc_loss': 0.8454207739771691, 'train/wer': 0.2691291157651375, 'validation/ctc_loss': 1.0963985112768737, 'validation/wer': 0.31015304398204024, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7398953158268275, 'test/wer': 0.23963601649300267, 'test/num_examples': 2472, 'score': 4832.948157072067, 'total_duration': 4937.137841463089, 'accumulated_submission_time': 4832.948157072067, 'accumulated_eval_time': 101.47764372825623, 'accumulated_logging_time': 0.31667351722717285}
I0726 12:36:22.608284 140627545716480 logging_writer.py:48] [5341] accumulated_eval_time=101.477644, accumulated_logging_time=0.316674, accumulated_submission_time=4832.948157, global_step=5341, preemption_count=0, score=4832.948157, test/ctc_loss=0.739895, test/num_examples=2472, test/wer=0.239636, total_duration=4937.137841, train/ctc_loss=0.845421, train/wer=0.269129, validation/ctc_loss=1.096399, validation/num_examples=5348, validation/wer=0.310153
I0726 12:38:45.630165 140627537323776 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.749934, loss=2.000897
I0726 12:38:45.634664 140667560675136 submission.py:119] 5500) loss = 2.001, grad_norm = 1.750
I0726 12:46:12.552677 140627545716480 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.331962, loss=1.996041
I0726 12:46:12.557162 140667560675136 submission.py:119] 6000) loss = 1.996, grad_norm = 2.332
I0726 12:53:41.552864 140627545716480 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.932268, loss=1.887709
I0726 12:53:41.559481 140667560675136 submission.py:119] 6500) loss = 1.888, grad_norm = 1.932
I0726 13:01:08.053601 140627537323776 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.046660, loss=1.820752
I0726 13:01:08.059170 140667560675136 submission.py:119] 7000) loss = 1.821, grad_norm = 4.047
I0726 13:08:36.451596 140627545716480 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.636130, loss=1.862973
I0726 13:08:36.458484 140667560675136 submission.py:119] 7500) loss = 1.863, grad_norm = 1.636
I0726 13:16:03.286003 140667560675136 spec.py:320] Evaluating on the training split.
I0726 13:16:16.501687 140667560675136 spec.py:332] Evaluating on the validation split.
I0726 13:16:27.396267 140667560675136 spec.py:348] Evaluating on the test split.
I0726 13:16:33.278722 140667560675136 submission_runner.py:364] Time since start: 7347.84s, 	Step: 8000, 	{'train/ctc_loss': 0.6190765545702546, 'train/wer': 0.20154563363924, 'validation/ctc_loss': 0.8582812970916215, 'validation/wer': 0.24573939072080336, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5415570661816892, 'test/wer': 0.177076351227835, 'test/num_examples': 2472, 'score': 7212.253179073334, 'total_duration': 7347.843254804611, 'accumulated_submission_time': 7212.253179073334, 'accumulated_eval_time': 131.46898698806763, 'accumulated_logging_time': 0.5506348609924316}
I0726 13:16:33.323417 140627545716480 logging_writer.py:48] [8000] accumulated_eval_time=131.468987, accumulated_logging_time=0.550635, accumulated_submission_time=7212.253179, global_step=8000, preemption_count=0, score=7212.253179, test/ctc_loss=0.541557, test/num_examples=2472, test/wer=0.177076, total_duration=7347.843255, train/ctc_loss=0.619077, train/wer=0.201546, validation/ctc_loss=0.858281, validation/num_examples=5348, validation/wer=0.245739
I0726 13:16:33.495933 140627537323776 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=7212.253179
I0726 13:16:33.896303 140667560675136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_8000.
I0726 13:16:34.003368 140667560675136 submission_runner.py:530] Tuning trial 1/1
I0726 13:16:34.003625 140667560675136 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0726 13:16:34.004187 140667560675136 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/ctc_loss': 30.431378151489177, 'train/wer': 3.1459831434758305, 'validation/ctc_loss': 28.911197759694595, 'validation/wer': 2.7678462801139383, 'validation/num_examples': 5348, 'test/ctc_loss': 29.154184140912317, 'test/wer': 3.2043344910933724, 'test/num_examples': 2472, 'score': 34.75623631477356, 'total_duration': 78.5769419670105, 'accumulated_submission_time': 34.75623631477356, 'accumulated_eval_time': 43.82002592086792, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2661, {'train/ctc_loss': 5.271449004721281, 'train/wer': 0.9079295130277161, 'validation/ctc_loss': 5.174701674578059, 'validation/wer': 0.8734900786945397, 'validation/num_examples': 5348, 'test/ctc_loss': 4.976610184528034, 'test/wer': 0.8652123575650479, 'test/num_examples': 2472, 'score': 2433.744001150131, 'total_duration': 2506.531156539917, 'accumulated_submission_time': 2433.744001150131, 'accumulated_eval_time': 71.50028395652771, 'accumulated_logging_time': 0.0988776683807373, 'global_step': 2661, 'preemption_count': 0}), (5341, {'train/ctc_loss': 0.8454207739771691, 'train/wer': 0.2691291157651375, 'validation/ctc_loss': 1.0963985112768737, 'validation/wer': 0.31015304398204024, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7398953158268275, 'test/wer': 0.23963601649300267, 'test/num_examples': 2472, 'score': 4832.948157072067, 'total_duration': 4937.137841463089, 'accumulated_submission_time': 4832.948157072067, 'accumulated_eval_time': 101.47764372825623, 'accumulated_logging_time': 0.31667351722717285, 'global_step': 5341, 'preemption_count': 0}), (8000, {'train/ctc_loss': 0.6190765545702546, 'train/wer': 0.20154563363924, 'validation/ctc_loss': 0.8582812970916215, 'validation/wer': 0.24573939072080336, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5415570661816892, 'test/wer': 0.177076351227835, 'test/num_examples': 2472, 'score': 7212.253179073334, 'total_duration': 7347.843254804611, 'accumulated_submission_time': 7212.253179073334, 'accumulated_eval_time': 131.46898698806763, 'accumulated_logging_time': 0.5506348609924316, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0726 13:16:34.004322 140667560675136 submission_runner.py:533] Timing: 7212.253179073334
I0726 13:16:34.004394 140667560675136 submission_runner.py:535] Total number of evals: 4
I0726 13:16:34.004465 140667560675136 submission_runner.py:536] ====================
I0726 13:16:34.004750 140667560675136 submission_runner.py:604] Final librispeech_deepspeech score: 7212.253179073334
