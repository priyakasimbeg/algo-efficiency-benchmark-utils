torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_timing/adamw --overwrite=True --save_checkpoints=False --max_global_steps=6000 --torch_compile=True 2>&1 | tee -a /logs/ogbg_pytorch_07-26-2023-07-58-31.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-26 07:58:41.299312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:58:41.299343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0726 07:58:55.670512 139908618880832 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0726 07:58:55.670546 140470846138176 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0726 07:58:55.670568 140490652682048 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0726 07:58:55.670586 140092666259264 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0726 07:58:55.671848 140549665371968 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0726 07:58:55.671876 140222670718784 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0726 07:58:55.672554 140483680073536 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0726 07:58:55.672574 139975043462976 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0726 07:58:55.672850 140483680073536 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.672909 139975043462976 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.681278 139908618880832 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.681302 140470846138176 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.681343 140092666259264 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.681336 140490652682048 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.682446 140222670718784 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:55.682416 140549665371968 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:58:57.544835 140222670718784 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/ogbg_pytorch.
W0726 07:58:57.582881 140483680073536 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:58:57.583827 139908618880832 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:58:57.583969 140490652682048 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:58:57.584729 140222670718784 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:58:57.585054 140092666259264 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:58:57.585555 140470846138176 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0726 07:58:57.589586 140222670718784 submission_runner.py:490] Using RNG seed 1459129178
I0726 07:58:57.591016 140222670718784 submission_runner.py:499] --- Tuning run 1/1 ---
I0726 07:58:57.591138 140222670718784 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/ogbg_pytorch/trial_1.
I0726 07:58:57.591390 140222670718784 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/ogbg_pytorch/trial_1/hparams.json.
I0726 07:58:57.592182 140222670718784 submission_runner.py:176] Initializing dataset.
I0726 07:58:57.592302 140222670718784 submission_runner.py:183] Initializing model.
W0726 07:58:57.615497 140549665371968 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:58:57.617806 139975043462976 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:59:02.129679 140222670718784 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129680 139975043462976 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129680 140490652682048 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129699 140470846138176 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129690 140483680073536 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129716 140549665371968 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129731 140092666259264 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:59:02.129793 139908618880832 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0726 07:59:02.129923 140222670718784 submission_runner.py:217] Initializing optimizer.
I0726 07:59:02.130788 140222670718784 submission_runner.py:224] Initializing metrics bundle.
I0726 07:59:02.130903 140222670718784 submission_runner.py:242] Initializing checkpoint and logger.
I0726 07:59:02.131712 140222670718784 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0726 07:59:02.131833 140222670718784 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0726 07:59:02.639817 140222670718784 submission_runner.py:263] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0726 07:59:02.640757 140222670718784 submission_runner.py:266] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0726 07:59:02.730385 140222670718784 submission_runner.py:276] Starting training loop.
I0726 07:59:03.245960 140222670718784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0726 07:59:03.252348 140222670718784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0726 07:59:03.354023 140222670718784 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0726 07:59:03.413666 140222670718784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0726 07:59:08.062132 140183856994048 logging_writer.py:48] [0] global_step=0, grad_norm=3.263613, loss=0.739712
I0726 07:59:08.076123 140222670718784 submission.py:119] 0) loss = 0.740, grad_norm = 3.264
I0726 07:59:08.078340 140222670718784 spec.py:320] Evaluating on the training split.
I0726 07:59:08.083564 140222670718784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0726 07:59:08.087720 140222670718784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0726 07:59:08.153373 140222670718784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0726 08:00:05.317939 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:00:05.321345 140222670718784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0726 08:00:05.325925 140222670718784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0726 08:00:05.388165 140222670718784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0726 08:00:50.122094 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:00:50.125304 140222670718784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0726 08:00:50.129855 140222670718784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0726 08:00:50.194960 140222670718784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0726 08:01:36.747461 140222670718784 submission_runner.py:364] Time since start: 154.02s, 	Step: 1, 	{'train/accuracy': 0.5493002090201586, 'train/loss': 0.7387837111254324, 'train/mean_average_precision': 0.02138932786922834, 'validation/accuracy': 0.5422068722512741, 'validation/loss': 0.736400377687226, 'validation/mean_average_precision': 0.025561914882982914, 'validation/num_examples': 43793, 'test/accuracy': 0.5404957629920659, 'test/loss': 0.7363124363206286, 'test/mean_average_precision': 0.027183486213567403, 'test/num_examples': 43793, 'score': 5.348168134689331, 'total_duration': 154.0172884464264, 'accumulated_submission_time': 5.348168134689331, 'accumulated_eval_time': 148.66873025894165, 'accumulated_logging_time': 0}
I0726 08:01:36.768584 140169956624128 logging_writer.py:48] [1] accumulated_eval_time=148.668730, accumulated_logging_time=0, accumulated_submission_time=5.348168, global_step=1, preemption_count=0, score=5.348168, test/accuracy=0.540496, test/loss=0.736312, test/mean_average_precision=0.027183, test/num_examples=43793, total_duration=154.017288, train/accuracy=0.549300, train/loss=0.738784, train/mean_average_precision=0.021389, validation/accuracy=0.542207, validation/loss=0.736400, validation/mean_average_precision=0.025562, validation/num_examples=43793
I0726 08:01:37.229855 140222670718784 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.234954 140483680073536 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.235141 140092666259264 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.235165 140549665371968 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.235230 140490652682048 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.235615 140470846138176 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.235630 139975043462976 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.235705 139908618880832 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 08:01:37.297119 140169965016832 logging_writer.py:48] [1] global_step=1, grad_norm=3.290825, loss=0.738325
I0726 08:01:37.301569 140222670718784 submission.py:119] 1) loss = 0.738, grad_norm = 3.291
I0726 08:01:37.679691 140169956624128 logging_writer.py:48] [2] global_step=2, grad_norm=3.315587, loss=0.737515
I0726 08:01:37.684424 140222670718784 submission.py:119] 2) loss = 0.738, grad_norm = 3.316
I0726 08:01:38.058438 140169965016832 logging_writer.py:48] [3] global_step=3, grad_norm=3.248861, loss=0.736616
I0726 08:01:38.063159 140222670718784 submission.py:119] 3) loss = 0.737, grad_norm = 3.249
I0726 08:01:38.429861 140169956624128 logging_writer.py:48] [4] global_step=4, grad_norm=3.251341, loss=0.733084
I0726 08:01:38.434756 140222670718784 submission.py:119] 4) loss = 0.733, grad_norm = 3.251
I0726 08:01:38.790877 140169965016832 logging_writer.py:48] [5] global_step=5, grad_norm=3.194616, loss=0.728679
I0726 08:01:38.795635 140222670718784 submission.py:119] 5) loss = 0.729, grad_norm = 3.195
I0726 08:01:39.137674 140169956624128 logging_writer.py:48] [6] global_step=6, grad_norm=3.144733, loss=0.723374
I0726 08:01:39.142118 140222670718784 submission.py:119] 6) loss = 0.723, grad_norm = 3.145
I0726 08:01:39.490021 140169965016832 logging_writer.py:48] [7] global_step=7, grad_norm=3.091266, loss=0.719417
I0726 08:01:39.495239 140222670718784 submission.py:119] 7) loss = 0.719, grad_norm = 3.091
I0726 08:01:39.841735 140169956624128 logging_writer.py:48] [8] global_step=8, grad_norm=3.028080, loss=0.712787
I0726 08:01:39.846382 140222670718784 submission.py:119] 8) loss = 0.713, grad_norm = 3.028
I0726 08:01:40.198371 140169965016832 logging_writer.py:48] [9] global_step=9, grad_norm=2.953448, loss=0.704395
I0726 08:01:40.203047 140222670718784 submission.py:119] 9) loss = 0.704, grad_norm = 2.953
I0726 08:01:40.567617 140169956624128 logging_writer.py:48] [10] global_step=10, grad_norm=2.838134, loss=0.698019
I0726 08:01:40.571889 140222670718784 submission.py:119] 10) loss = 0.698, grad_norm = 2.838
I0726 08:01:40.914149 140169965016832 logging_writer.py:48] [11] global_step=11, grad_norm=2.727081, loss=0.689683
I0726 08:01:40.918650 140222670718784 submission.py:119] 11) loss = 0.690, grad_norm = 2.727
I0726 08:01:41.247021 140169956624128 logging_writer.py:48] [12] global_step=12, grad_norm=2.619154, loss=0.680828
I0726 08:01:41.251790 140222670718784 submission.py:119] 12) loss = 0.681, grad_norm = 2.619
I0726 08:01:41.577121 140169965016832 logging_writer.py:48] [13] global_step=13, grad_norm=2.496758, loss=0.671571
I0726 08:01:41.581627 140222670718784 submission.py:119] 13) loss = 0.672, grad_norm = 2.497
I0726 08:01:41.901773 140169956624128 logging_writer.py:48] [14] global_step=14, grad_norm=2.368355, loss=0.663653
I0726 08:01:41.906634 140222670718784 submission.py:119] 14) loss = 0.664, grad_norm = 2.368
I0726 08:01:42.227776 140169965016832 logging_writer.py:48] [15] global_step=15, grad_norm=2.280356, loss=0.654385
I0726 08:01:42.232526 140222670718784 submission.py:119] 15) loss = 0.654, grad_norm = 2.280
I0726 08:01:42.552124 140169956624128 logging_writer.py:48] [16] global_step=16, grad_norm=2.159106, loss=0.647392
I0726 08:01:42.556675 140222670718784 submission.py:119] 16) loss = 0.647, grad_norm = 2.159
I0726 08:01:42.880481 140169965016832 logging_writer.py:48] [17] global_step=17, grad_norm=2.068147, loss=0.635330
I0726 08:01:42.885070 140222670718784 submission.py:119] 17) loss = 0.635, grad_norm = 2.068
I0726 08:01:43.209680 140169956624128 logging_writer.py:48] [18] global_step=18, grad_norm=2.092395, loss=0.628341
I0726 08:01:43.214379 140222670718784 submission.py:119] 18) loss = 0.628, grad_norm = 2.092
I0726 08:01:43.541416 140169965016832 logging_writer.py:48] [19] global_step=19, grad_norm=2.183830, loss=0.617340
I0726 08:01:43.545872 140222670718784 submission.py:119] 19) loss = 0.617, grad_norm = 2.184
I0726 08:01:43.873598 140169956624128 logging_writer.py:48] [20] global_step=20, grad_norm=2.088306, loss=0.606187
I0726 08:01:43.878084 140222670718784 submission.py:119] 20) loss = 0.606, grad_norm = 2.088
I0726 08:01:44.207756 140169965016832 logging_writer.py:48] [21] global_step=21, grad_norm=1.928564, loss=0.597127
I0726 08:01:44.212259 140222670718784 submission.py:119] 21) loss = 0.597, grad_norm = 1.929
I0726 08:01:44.539912 140169956624128 logging_writer.py:48] [22] global_step=22, grad_norm=1.839507, loss=0.587157
I0726 08:01:44.544760 140222670718784 submission.py:119] 22) loss = 0.587, grad_norm = 1.840
I0726 08:01:44.872233 140169965016832 logging_writer.py:48] [23] global_step=23, grad_norm=1.747747, loss=0.576614
I0726 08:01:44.876934 140222670718784 submission.py:119] 23) loss = 0.577, grad_norm = 1.748
I0726 08:01:45.202813 140169956624128 logging_writer.py:48] [24] global_step=24, grad_norm=1.619524, loss=0.568581
I0726 08:01:45.207314 140222670718784 submission.py:119] 24) loss = 0.569, grad_norm = 1.620
I0726 08:01:45.537561 140169965016832 logging_writer.py:48] [25] global_step=25, grad_norm=1.513378, loss=0.559196
I0726 08:01:45.542168 140222670718784 submission.py:119] 25) loss = 0.559, grad_norm = 1.513
I0726 08:01:45.867826 140169956624128 logging_writer.py:48] [26] global_step=26, grad_norm=1.398765, loss=0.549560
I0726 08:01:45.872401 140222670718784 submission.py:119] 26) loss = 0.550, grad_norm = 1.399
I0726 08:01:46.198830 140169965016832 logging_writer.py:48] [27] global_step=27, grad_norm=1.295958, loss=0.542669
I0726 08:01:46.203614 140222670718784 submission.py:119] 27) loss = 0.543, grad_norm = 1.296
I0726 08:01:46.529858 140169956624128 logging_writer.py:48] [28] global_step=28, grad_norm=1.193732, loss=0.535497
I0726 08:01:46.534500 140222670718784 submission.py:119] 28) loss = 0.535, grad_norm = 1.194
I0726 08:01:46.859095 140169965016832 logging_writer.py:48] [29] global_step=29, grad_norm=1.174360, loss=0.529229
I0726 08:01:46.863793 140222670718784 submission.py:119] 29) loss = 0.529, grad_norm = 1.174
I0726 08:01:47.188159 140169956624128 logging_writer.py:48] [30] global_step=30, grad_norm=1.120899, loss=0.522238
I0726 08:01:47.192685 140222670718784 submission.py:119] 30) loss = 0.522, grad_norm = 1.121
I0726 08:01:47.515840 140169965016832 logging_writer.py:48] [31] global_step=31, grad_norm=1.093283, loss=0.517728
I0726 08:01:47.520437 140222670718784 submission.py:119] 31) loss = 0.518, grad_norm = 1.093
I0726 08:01:47.847817 140169956624128 logging_writer.py:48] [32] global_step=32, grad_norm=1.027863, loss=0.509707
I0726 08:01:47.852324 140222670718784 submission.py:119] 32) loss = 0.510, grad_norm = 1.028
I0726 08:01:48.178071 140169965016832 logging_writer.py:48] [33] global_step=33, grad_norm=0.965017, loss=0.503608
I0726 08:01:48.182592 140222670718784 submission.py:119] 33) loss = 0.504, grad_norm = 0.965
I0726 08:01:48.512302 140169956624128 logging_writer.py:48] [34] global_step=34, grad_norm=0.970682, loss=0.497281
I0726 08:01:48.516798 140222670718784 submission.py:119] 34) loss = 0.497, grad_norm = 0.971
I0726 08:01:48.839265 140169965016832 logging_writer.py:48] [35] global_step=35, grad_norm=0.954220, loss=0.491819
I0726 08:01:48.844218 140222670718784 submission.py:119] 35) loss = 0.492, grad_norm = 0.954
I0726 08:01:49.165255 140169956624128 logging_writer.py:48] [36] global_step=36, grad_norm=0.907769, loss=0.485314
I0726 08:01:49.169643 140222670718784 submission.py:119] 36) loss = 0.485, grad_norm = 0.908
I0726 08:01:49.495538 140169965016832 logging_writer.py:48] [37] global_step=37, grad_norm=0.852546, loss=0.483407
I0726 08:01:49.500177 140222670718784 submission.py:119] 37) loss = 0.483, grad_norm = 0.853
I0726 08:01:49.823289 140169956624128 logging_writer.py:48] [38] global_step=38, grad_norm=0.832120, loss=0.477494
I0726 08:01:49.828011 140222670718784 submission.py:119] 38) loss = 0.477, grad_norm = 0.832
I0726 08:01:50.153418 140169965016832 logging_writer.py:48] [39] global_step=39, grad_norm=0.796755, loss=0.471853
I0726 08:01:50.158142 140222670718784 submission.py:119] 39) loss = 0.472, grad_norm = 0.797
I0726 08:01:50.491773 140169956624128 logging_writer.py:48] [40] global_step=40, grad_norm=0.781414, loss=0.467638
I0726 08:01:50.496170 140222670718784 submission.py:119] 40) loss = 0.468, grad_norm = 0.781
I0726 08:01:50.827111 140169965016832 logging_writer.py:48] [41] global_step=41, grad_norm=0.758279, loss=0.461635
I0726 08:01:50.831810 140222670718784 submission.py:119] 41) loss = 0.462, grad_norm = 0.758
I0726 08:01:51.160951 140169956624128 logging_writer.py:48] [42] global_step=42, grad_norm=0.721381, loss=0.460773
I0726 08:01:51.165414 140222670718784 submission.py:119] 42) loss = 0.461, grad_norm = 0.721
I0726 08:01:51.493047 140169965016832 logging_writer.py:48] [43] global_step=43, grad_norm=0.708003, loss=0.453806
I0726 08:01:51.497501 140222670718784 submission.py:119] 43) loss = 0.454, grad_norm = 0.708
I0726 08:01:51.819656 140169956624128 logging_writer.py:48] [44] global_step=44, grad_norm=0.663911, loss=0.451560
I0726 08:01:51.824682 140222670718784 submission.py:119] 44) loss = 0.452, grad_norm = 0.664
I0726 08:01:52.152549 140169965016832 logging_writer.py:48] [45] global_step=45, grad_norm=0.659505, loss=0.447063
I0726 08:01:52.157023 140222670718784 submission.py:119] 45) loss = 0.447, grad_norm = 0.660
I0726 08:01:52.483060 140169956624128 logging_writer.py:48] [46] global_step=46, grad_norm=0.630863, loss=0.444103
I0726 08:01:52.487468 140222670718784 submission.py:119] 46) loss = 0.444, grad_norm = 0.631
I0726 08:01:52.821702 140169965016832 logging_writer.py:48] [47] global_step=47, grad_norm=0.617355, loss=0.438839
I0726 08:01:52.826301 140222670718784 submission.py:119] 47) loss = 0.439, grad_norm = 0.617
I0726 08:01:53.157381 140169956624128 logging_writer.py:48] [48] global_step=48, grad_norm=0.607309, loss=0.434815
I0726 08:01:53.161777 140222670718784 submission.py:119] 48) loss = 0.435, grad_norm = 0.607
I0726 08:01:53.485413 140169965016832 logging_writer.py:48] [49] global_step=49, grad_norm=0.583128, loss=0.431688
I0726 08:01:53.489958 140222670718784 submission.py:119] 49) loss = 0.432, grad_norm = 0.583
I0726 08:01:53.816915 140169956624128 logging_writer.py:48] [50] global_step=50, grad_norm=0.568674, loss=0.429960
I0726 08:01:53.821336 140222670718784 submission.py:119] 50) loss = 0.430, grad_norm = 0.569
I0726 08:01:54.145935 140169965016832 logging_writer.py:48] [51] global_step=51, grad_norm=0.555285, loss=0.426951
I0726 08:01:54.150665 140222670718784 submission.py:119] 51) loss = 0.427, grad_norm = 0.555
I0726 08:01:54.483083 140169956624128 logging_writer.py:48] [52] global_step=52, grad_norm=0.548229, loss=0.422891
I0726 08:01:54.487809 140222670718784 submission.py:119] 52) loss = 0.423, grad_norm = 0.548
I0726 08:01:54.817206 140169965016832 logging_writer.py:48] [53] global_step=53, grad_norm=0.554906, loss=0.417574
I0726 08:01:54.821914 140222670718784 submission.py:119] 53) loss = 0.418, grad_norm = 0.555
I0726 08:01:55.159071 140169956624128 logging_writer.py:48] [54] global_step=54, grad_norm=0.532368, loss=0.415480
I0726 08:01:55.163496 140222670718784 submission.py:119] 54) loss = 0.415, grad_norm = 0.532
I0726 08:01:55.498395 140169965016832 logging_writer.py:48] [55] global_step=55, grad_norm=0.522846, loss=0.410144
I0726 08:01:55.503032 140222670718784 submission.py:119] 55) loss = 0.410, grad_norm = 0.523
I0726 08:01:55.829966 140169956624128 logging_writer.py:48] [56] global_step=56, grad_norm=0.505863, loss=0.411491
I0726 08:01:55.834505 140222670718784 submission.py:119] 56) loss = 0.411, grad_norm = 0.506
I0726 08:01:56.163709 140169965016832 logging_writer.py:48] [57] global_step=57, grad_norm=0.508565, loss=0.405731
I0726 08:01:56.168033 140222670718784 submission.py:119] 57) loss = 0.406, grad_norm = 0.509
I0726 08:01:56.500174 140169956624128 logging_writer.py:48] [58] global_step=58, grad_norm=0.494406, loss=0.403625
I0726 08:01:56.504623 140222670718784 submission.py:119] 58) loss = 0.404, grad_norm = 0.494
I0726 08:01:56.836620 140169965016832 logging_writer.py:48] [59] global_step=59, grad_norm=0.486348, loss=0.404095
I0726 08:01:56.841232 140222670718784 submission.py:119] 59) loss = 0.404, grad_norm = 0.486
I0726 08:01:57.171679 140169956624128 logging_writer.py:48] [60] global_step=60, grad_norm=0.481676, loss=0.398788
I0726 08:01:57.176226 140222670718784 submission.py:119] 60) loss = 0.399, grad_norm = 0.482
I0726 08:01:57.501530 140169965016832 logging_writer.py:48] [61] global_step=61, grad_norm=0.474658, loss=0.395061
I0726 08:01:57.506207 140222670718784 submission.py:119] 61) loss = 0.395, grad_norm = 0.475
I0726 08:01:57.829956 140169956624128 logging_writer.py:48] [62] global_step=62, grad_norm=0.467394, loss=0.393392
I0726 08:01:57.834353 140222670718784 submission.py:119] 62) loss = 0.393, grad_norm = 0.467
I0726 08:01:58.155944 140169965016832 logging_writer.py:48] [63] global_step=63, grad_norm=0.463883, loss=0.389489
I0726 08:01:58.160262 140222670718784 submission.py:119] 63) loss = 0.389, grad_norm = 0.464
I0726 08:01:58.486414 140169956624128 logging_writer.py:48] [64] global_step=64, grad_norm=0.459719, loss=0.387220
I0726 08:01:58.490676 140222670718784 submission.py:119] 64) loss = 0.387, grad_norm = 0.460
I0726 08:01:58.813896 140169965016832 logging_writer.py:48] [65] global_step=65, grad_norm=0.450090, loss=0.384959
I0726 08:01:58.818462 140222670718784 submission.py:119] 65) loss = 0.385, grad_norm = 0.450
I0726 08:01:59.142626 140169956624128 logging_writer.py:48] [66] global_step=66, grad_norm=0.442728, loss=0.384672
I0726 08:01:59.147292 140222670718784 submission.py:119] 66) loss = 0.385, grad_norm = 0.443
I0726 08:01:59.471527 140169965016832 logging_writer.py:48] [67] global_step=67, grad_norm=0.436882, loss=0.380743
I0726 08:01:59.476186 140222670718784 submission.py:119] 67) loss = 0.381, grad_norm = 0.437
I0726 08:01:59.800569 140169956624128 logging_writer.py:48] [68] global_step=68, grad_norm=0.429076, loss=0.380369
I0726 08:01:59.805460 140222670718784 submission.py:119] 68) loss = 0.380, grad_norm = 0.429
I0726 08:02:00.130898 140169965016832 logging_writer.py:48] [69] global_step=69, grad_norm=0.438411, loss=0.375163
I0726 08:02:00.135408 140222670718784 submission.py:119] 69) loss = 0.375, grad_norm = 0.438
I0726 08:02:00.460164 140169956624128 logging_writer.py:48] [70] global_step=70, grad_norm=0.426051, loss=0.373650
I0726 08:02:00.465107 140222670718784 submission.py:119] 70) loss = 0.374, grad_norm = 0.426
I0726 08:02:00.791684 140169965016832 logging_writer.py:48] [71] global_step=71, grad_norm=0.426052, loss=0.373638
I0726 08:02:00.796232 140222670718784 submission.py:119] 71) loss = 0.374, grad_norm = 0.426
I0726 08:02:01.117093 140169956624128 logging_writer.py:48] [72] global_step=72, grad_norm=0.431314, loss=0.371817
I0726 08:02:01.121638 140222670718784 submission.py:119] 72) loss = 0.372, grad_norm = 0.431
I0726 08:02:01.551367 140169965016832 logging_writer.py:48] [73] global_step=73, grad_norm=0.421532, loss=0.367524
I0726 08:02:01.556022 140222670718784 submission.py:119] 73) loss = 0.368, grad_norm = 0.422
I0726 08:02:01.896333 140169956624128 logging_writer.py:48] [74] global_step=74, grad_norm=0.418567, loss=0.366537
I0726 08:02:01.900756 140222670718784 submission.py:119] 74) loss = 0.367, grad_norm = 0.419
I0726 08:02:02.240264 140169965016832 logging_writer.py:48] [75] global_step=75, grad_norm=0.432892, loss=0.366981
I0726 08:02:02.245243 140222670718784 submission.py:119] 75) loss = 0.367, grad_norm = 0.433
I0726 08:02:02.585034 140169956624128 logging_writer.py:48] [76] global_step=76, grad_norm=0.431559, loss=0.363823
I0726 08:02:02.589538 140222670718784 submission.py:119] 76) loss = 0.364, grad_norm = 0.432
I0726 08:02:02.922310 140169965016832 logging_writer.py:48] [77] global_step=77, grad_norm=0.405643, loss=0.362664
I0726 08:02:02.926967 140222670718784 submission.py:119] 77) loss = 0.363, grad_norm = 0.406
I0726 08:02:03.268538 140169956624128 logging_writer.py:48] [78] global_step=78, grad_norm=0.401750, loss=0.360498
I0726 08:02:03.274319 140222670718784 submission.py:119] 78) loss = 0.360, grad_norm = 0.402
I0726 08:02:03.601918 140169965016832 logging_writer.py:48] [79] global_step=79, grad_norm=0.403618, loss=0.357373
I0726 08:02:03.607072 140222670718784 submission.py:119] 79) loss = 0.357, grad_norm = 0.404
I0726 08:02:03.937012 140169956624128 logging_writer.py:48] [80] global_step=80, grad_norm=0.399692, loss=0.356763
I0726 08:02:03.941690 140222670718784 submission.py:119] 80) loss = 0.357, grad_norm = 0.400
I0726 08:02:04.273786 140169965016832 logging_writer.py:48] [81] global_step=81, grad_norm=0.401880, loss=0.354744
I0726 08:02:04.279517 140222670718784 submission.py:119] 81) loss = 0.355, grad_norm = 0.402
I0726 08:02:04.612993 140169956624128 logging_writer.py:48] [82] global_step=82, grad_norm=0.404230, loss=0.357876
I0726 08:02:04.617919 140222670718784 submission.py:119] 82) loss = 0.358, grad_norm = 0.404
I0726 08:02:04.957003 140169965016832 logging_writer.py:48] [83] global_step=83, grad_norm=0.399603, loss=0.355584
I0726 08:02:04.961769 140222670718784 submission.py:119] 83) loss = 0.356, grad_norm = 0.400
I0726 08:02:05.293586 140169956624128 logging_writer.py:48] [84] global_step=84, grad_norm=0.405838, loss=0.351764
I0726 08:02:05.298908 140222670718784 submission.py:119] 84) loss = 0.352, grad_norm = 0.406
I0726 08:02:05.629168 140169965016832 logging_writer.py:48] [85] global_step=85, grad_norm=0.396689, loss=0.348387
I0726 08:02:05.634207 140222670718784 submission.py:119] 85) loss = 0.348, grad_norm = 0.397
I0726 08:02:05.960069 140169956624128 logging_writer.py:48] [86] global_step=86, grad_norm=0.400768, loss=0.347329
I0726 08:02:05.964652 140222670718784 submission.py:119] 86) loss = 0.347, grad_norm = 0.401
I0726 08:02:06.292995 140169965016832 logging_writer.py:48] [87] global_step=87, grad_norm=0.401275, loss=0.349616
I0726 08:02:06.297998 140222670718784 submission.py:119] 87) loss = 0.350, grad_norm = 0.401
I0726 08:02:06.631150 140169956624128 logging_writer.py:48] [88] global_step=88, grad_norm=0.406288, loss=0.347380
I0726 08:02:06.636244 140222670718784 submission.py:119] 88) loss = 0.347, grad_norm = 0.406
I0726 08:02:06.969747 140169965016832 logging_writer.py:48] [89] global_step=89, grad_norm=0.391355, loss=0.345699
I0726 08:02:06.974214 140222670718784 submission.py:119] 89) loss = 0.346, grad_norm = 0.391
I0726 08:02:07.298422 140169956624128 logging_writer.py:48] [90] global_step=90, grad_norm=0.406223, loss=0.342310
I0726 08:02:07.304459 140222670718784 submission.py:119] 90) loss = 0.342, grad_norm = 0.406
I0726 08:02:07.634717 140169965016832 logging_writer.py:48] [91] global_step=91, grad_norm=0.386875, loss=0.340330
I0726 08:02:07.639666 140222670718784 submission.py:119] 91) loss = 0.340, grad_norm = 0.387
I0726 08:02:07.968582 140169956624128 logging_writer.py:48] [92] global_step=92, grad_norm=0.407360, loss=0.338728
I0726 08:02:07.973798 140222670718784 submission.py:119] 92) loss = 0.339, grad_norm = 0.407
I0726 08:02:08.308899 140169965016832 logging_writer.py:48] [93] global_step=93, grad_norm=0.394731, loss=0.338833
I0726 08:02:08.314019 140222670718784 submission.py:119] 93) loss = 0.339, grad_norm = 0.395
I0726 08:02:08.644893 140169956624128 logging_writer.py:48] [94] global_step=94, grad_norm=0.386844, loss=0.335497
I0726 08:02:08.649985 140222670718784 submission.py:119] 94) loss = 0.335, grad_norm = 0.387
I0726 08:02:08.976077 140169965016832 logging_writer.py:48] [95] global_step=95, grad_norm=0.383649, loss=0.335694
I0726 08:02:08.981020 140222670718784 submission.py:119] 95) loss = 0.336, grad_norm = 0.384
I0726 08:02:09.317183 140169956624128 logging_writer.py:48] [96] global_step=96, grad_norm=0.383565, loss=0.331676
I0726 08:02:09.321879 140222670718784 submission.py:119] 96) loss = 0.332, grad_norm = 0.384
I0726 08:02:09.641721 140169965016832 logging_writer.py:48] [97] global_step=97, grad_norm=0.376424, loss=0.332165
I0726 08:02:09.646245 140222670718784 submission.py:119] 97) loss = 0.332, grad_norm = 0.376
I0726 08:02:09.963529 140169956624128 logging_writer.py:48] [98] global_step=98, grad_norm=0.375557, loss=0.332852
I0726 08:02:09.967983 140222670718784 submission.py:119] 98) loss = 0.333, grad_norm = 0.376
I0726 08:02:10.289965 140169965016832 logging_writer.py:48] [99] global_step=99, grad_norm=0.373703, loss=0.328621
I0726 08:02:10.294927 140222670718784 submission.py:119] 99) loss = 0.329, grad_norm = 0.374
I0726 08:02:10.618687 140169956624128 logging_writer.py:48] [100] global_step=100, grad_norm=0.380907, loss=0.329816
I0726 08:02:10.623671 140222670718784 submission.py:119] 100) loss = 0.330, grad_norm = 0.381
I0726 08:04:18.922739 140169965016832 logging_writer.py:48] [500] global_step=500, grad_norm=0.048851, loss=0.064790
I0726 08:04:18.928931 140222670718784 submission.py:119] 500) loss = 0.065, grad_norm = 0.049
I0726 08:05:36.957048 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:06:36.724076 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:06:40.091664 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:06:43.380037 140222670718784 submission_runner.py:364] Time since start: 460.65s, 	Step: 742, 	{'train/accuracy': 0.9866877774282068, 'train/loss': 0.056141177167774246, 'train/mean_average_precision': 0.03714810244085458, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06578205591914311, 'validation/mean_average_precision': 0.03915397386842283, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.068987819681501, 'test/mean_average_precision': 0.03877895837942359, 'test/num_examples': 43793, 'score': 245.17618989944458, 'total_duration': 460.6499147415161, 'accumulated_submission_time': 245.17618989944458, 'accumulated_eval_time': 215.09141492843628, 'accumulated_logging_time': 0.15670180320739746}
I0726 08:06:43.398146 140169956624128 logging_writer.py:48] [742] accumulated_eval_time=215.091415, accumulated_logging_time=0.156702, accumulated_submission_time=245.176190, global_step=742, preemption_count=0, score=245.176190, test/accuracy=0.983142, test/loss=0.068988, test/mean_average_precision=0.038779, test/num_examples=43793, total_duration=460.649915, train/accuracy=0.986688, train/loss=0.056141, train/mean_average_precision=0.037148, validation/accuracy=0.984118, validation/loss=0.065782, validation/mean_average_precision=0.039154, validation/num_examples=43793
I0726 08:08:07.783460 140169965016832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.029491, loss=0.055111
I0726 08:08:07.788687 140222670718784 submission.py:119] 1000) loss = 0.055, grad_norm = 0.029
I0726 08:10:43.406483 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:11:43.864634 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:11:47.958187 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:11:51.244948 140222670718784 submission_runner.py:364] Time since start: 768.51s, 	Step: 1477, 	{'train/accuracy': 0.987202818872333, 'train/loss': 0.04840578598824525, 'train/mean_average_precision': 0.08620832920493984, 'validation/accuracy': 0.9844910356115537, 'validation/loss': 0.05733859087429072, 'validation/mean_average_precision': 0.08898572282819889, 'validation/num_examples': 43793, 'test/accuracy': 0.9834950650724729, 'test/loss': 0.0605515139000564, 'test/mean_average_precision': 0.08722704370857667, 'test/num_examples': 43793, 'score': 484.8218414783478, 'total_duration': 768.5149104595184, 'accumulated_submission_time': 484.8218414783478, 'accumulated_eval_time': 282.9296817779541, 'accumulated_logging_time': 0.31697678565979004}
I0726 08:11:51.263192 140169956624128 logging_writer.py:48] [1477] accumulated_eval_time=282.929682, accumulated_logging_time=0.316977, accumulated_submission_time=484.821841, global_step=1477, preemption_count=0, score=484.821841, test/accuracy=0.983495, test/loss=0.060552, test/mean_average_precision=0.087227, test/num_examples=43793, total_duration=768.514910, train/accuracy=0.987203, train/loss=0.048406, train/mean_average_precision=0.086208, validation/accuracy=0.984491, validation/loss=0.057339, validation/mean_average_precision=0.088986, validation/num_examples=43793
I0726 08:11:59.355866 140169965016832 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.019148, loss=0.045438
I0726 08:11:59.361677 140222670718784 submission.py:119] 1500) loss = 0.045, grad_norm = 0.019
I0726 08:14:42.126616 140169956624128 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.009534, loss=0.052329
I0726 08:14:42.133699 140222670718784 submission.py:119] 2000) loss = 0.052, grad_norm = 0.010
I0726 08:15:51.294026 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:16:52.463985 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:16:57.071514 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:17:00.362351 140222670718784 submission_runner.py:364] Time since start: 1077.63s, 	Step: 2215, 	{'train/accuracy': 0.9873419041937593, 'train/loss': 0.0476181967950695, 'train/mean_average_precision': 0.09101455888163493, 'validation/accuracy': 0.9844865702638695, 'validation/loss': 0.05757653555188044, 'validation/mean_average_precision': 0.0927390042053077, 'validation/num_examples': 43793, 'test/accuracy': 0.9835169671960944, 'test/loss': 0.06077668852737281, 'test/mean_average_precision': 0.0915716992855218, 'test/num_examples': 43793, 'score': 724.4886541366577, 'total_duration': 1077.6322090625763, 'accumulated_submission_time': 724.4886541366577, 'accumulated_eval_time': 351.99768447875977, 'accumulated_logging_time': 0.4818556308746338}
I0726 08:17:00.381973 140169965016832 logging_writer.py:48] [2215] accumulated_eval_time=351.997684, accumulated_logging_time=0.481856, accumulated_submission_time=724.488654, global_step=2215, preemption_count=0, score=724.488654, test/accuracy=0.983517, test/loss=0.060777, test/mean_average_precision=0.091572, test/num_examples=43793, total_duration=1077.632209, train/accuracy=0.987342, train/loss=0.047618, train/mean_average_precision=0.091015, validation/accuracy=0.984487, validation/loss=0.057577, validation/mean_average_precision=0.092739, validation/num_examples=43793
I0726 08:18:32.466344 140169956624128 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.032115, loss=0.045993
I0726 08:18:32.472897 140222670718784 submission.py:119] 2500) loss = 0.046, grad_norm = 0.032
I0726 08:21:00.549344 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:22:01.557957 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:22:06.115384 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:22:09.532478 140222670718784 submission_runner.py:364] Time since start: 1386.80s, 	Step: 2963, 	{'train/accuracy': 0.9875850221723428, 'train/loss': 0.045149282481901996, 'train/mean_average_precision': 0.13088309434554918, 'validation/accuracy': 0.9848454218413957, 'validation/loss': 0.054666854109784224, 'validation/mean_average_precision': 0.12006185474876142, 'validation/num_examples': 43793, 'test/accuracy': 0.9838703495368333, 'test/loss': 0.057895223294256296, 'test/mean_average_precision': 0.12087795997604366, 'test/num_examples': 43793, 'score': 964.2850499153137, 'total_duration': 1386.8023934364319, 'accumulated_submission_time': 964.2850499153137, 'accumulated_eval_time': 420.9805860519409, 'accumulated_logging_time': 0.6520488262176514}
I0726 08:22:09.550605 140169965016832 logging_writer.py:48] [2963] accumulated_eval_time=420.980586, accumulated_logging_time=0.652049, accumulated_submission_time=964.285050, global_step=2963, preemption_count=0, score=964.285050, test/accuracy=0.983870, test/loss=0.057895, test/mean_average_precision=0.120878, test/num_examples=43793, total_duration=1386.802393, train/accuracy=0.987585, train/loss=0.045149, train/mean_average_precision=0.130883, validation/accuracy=0.984845, validation/loss=0.054667, validation/mean_average_precision=0.120062, validation/num_examples=43793
I0726 08:22:21.929898 140169956624128 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.025332, loss=0.047873
I0726 08:22:21.935666 140222670718784 submission.py:119] 3000) loss = 0.048, grad_norm = 0.025
I0726 08:25:04.865221 140169965016832 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.013531, loss=0.049081
I0726 08:25:04.871504 140222670718784 submission.py:119] 3500) loss = 0.049, grad_norm = 0.014
I0726 08:26:09.801712 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:27:16.534040 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:27:19.921943 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:27:23.201099 140222670718784 submission_runner.py:364] Time since start: 1700.47s, 	Step: 3698, 	{'train/accuracy': 0.9878556966157277, 'train/loss': 0.0431073405000822, 'train/mean_average_precision': 0.15946793665883469, 'validation/accuracy': 0.9849903426707812, 'validation/loss': 0.05287886610309919, 'validation/mean_average_precision': 0.14665069879947934, 'validation/num_examples': 43793, 'test/accuracy': 0.9839933383848616, 'test/loss': 0.05620302099781863, 'test/mean_average_precision': 0.14193413757173107, 'test/num_examples': 43793, 'score': 1204.1757230758667, 'total_duration': 1700.4709944725037, 'accumulated_submission_time': 1204.1757230758667, 'accumulated_eval_time': 494.37970519065857, 'accumulated_logging_time': 0.8168303966522217}
I0726 08:27:23.218475 140169956624128 logging_writer.py:48] [3698] accumulated_eval_time=494.379705, accumulated_logging_time=0.816830, accumulated_submission_time=1204.175723, global_step=3698, preemption_count=0, score=1204.175723, test/accuracy=0.983993, test/loss=0.056203, test/mean_average_precision=0.141934, test/num_examples=43793, total_duration=1700.470994, train/accuracy=0.987856, train/loss=0.043107, train/mean_average_precision=0.159468, validation/accuracy=0.984990, validation/loss=0.052879, validation/mean_average_precision=0.146651, validation/num_examples=43793
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0726 08:29:03.735646 140169965016832 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015852, loss=0.050687
I0726 08:29:03.742472 140222670718784 submission.py:119] 4000) loss = 0.051, grad_norm = 0.016
I0726 08:31:23.488561 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:32:29.826554 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:32:33.260612 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:32:36.596815 140222670718784 submission_runner.py:364] Time since start: 2013.87s, 	Step: 4424, 	{'train/accuracy': 0.9881288805807856, 'train/loss': 0.04150809552910168, 'train/mean_average_precision': 0.19003980762319125, 'validation/accuracy': 0.9852895209656193, 'validation/loss': 0.050762795555274104, 'validation/mean_average_precision': 0.16524744370463024, 'validation/num_examples': 43793, 'test/accuracy': 0.984369886433277, 'test/loss': 0.05358974818981054, 'test/mean_average_precision': 0.16253122969320905, 'test/num_examples': 43793, 'score': 1444.075429201126, 'total_duration': 2013.8665852546692, 'accumulated_submission_time': 1444.075429201126, 'accumulated_eval_time': 567.4875395298004, 'accumulated_logging_time': 0.9847531318664551}
I0726 08:32:36.614756 140175409555200 logging_writer.py:48] [4424] accumulated_eval_time=567.487540, accumulated_logging_time=0.984753, accumulated_submission_time=1444.075429, global_step=4424, preemption_count=0, score=1444.075429, test/accuracy=0.984370, test/loss=0.053590, test/mean_average_precision=0.162531, test/num_examples=43793, total_duration=2013.866585, train/accuracy=0.988129, train/loss=0.041508, train/mean_average_precision=0.190040, validation/accuracy=0.985290, validation/loss=0.050763, validation/mean_average_precision=0.165247, validation/num_examples=43793
I0726 08:33:01.898627 140175417947904 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.018683, loss=0.042019
I0726 08:33:01.904861 140222670718784 submission.py:119] 4500) loss = 0.042, grad_norm = 0.019
I0726 08:35:46.089967 140175409555200 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014754, loss=0.042077
I0726 08:35:46.096762 140222670718784 submission.py:119] 5000) loss = 0.042, grad_norm = 0.015
I0726 08:36:36.760583 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:37:47.562679 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:37:51.029760 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:37:54.472919 140222670718784 submission_runner.py:364] Time since start: 2331.74s, 	Step: 5156, 	{'train/accuracy': 0.9883935576214193, 'train/loss': 0.03995246386375205, 'train/mean_average_precision': 0.2202271043767101, 'validation/accuracy': 0.9854770655683535, 'validation/loss': 0.049827660413556144, 'validation/mean_average_precision': 0.1754619715966864, 'validation/num_examples': 43793, 'test/accuracy': 0.984575850634256, 'test/loss': 0.05282737922874199, 'test/mean_average_precision': 0.17223766138260976, 'test/num_examples': 43793, 'score': 1683.8408389091492, 'total_duration': 2331.7428493499756, 'accumulated_submission_time': 1683.8408389091492, 'accumulated_eval_time': 645.1996059417725, 'accumulated_logging_time': 1.162524700164795}
I0726 08:37:54.490121 140175417947904 logging_writer.py:48] [5156] accumulated_eval_time=645.199606, accumulated_logging_time=1.162525, accumulated_submission_time=1683.840839, global_step=5156, preemption_count=0, score=1683.840839, test/accuracy=0.984576, test/loss=0.052827, test/mean_average_precision=0.172238, test/num_examples=43793, total_duration=2331.742849, train/accuracy=0.988394, train/loss=0.039952, train/mean_average_precision=0.220227, validation/accuracy=0.985477, validation/loss=0.049828, validation/mean_average_precision=0.175462, validation/num_examples=43793
I0726 08:39:47.482624 140175409555200 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.015451, loss=0.044069
I0726 08:39:47.490444 140222670718784 submission.py:119] 5500) loss = 0.044, grad_norm = 0.015
I0726 08:41:54.713078 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:43:05.647431 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:43:08.912852 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:43:12.412060 140222670718784 submission_runner.py:364] Time since start: 2649.68s, 	Step: 5890, 	{'train/accuracy': 0.9887327381233565, 'train/loss': 0.03880680107114176, 'train/mean_average_precision': 0.23707551308081729, 'validation/accuracy': 0.9856321349152031, 'validation/loss': 0.04883205913622314, 'validation/mean_average_precision': 0.1907340062741693, 'validation/num_examples': 43793, 'test/accuracy': 0.9847266383314962, 'test/loss': 0.0516450660043661, 'test/mean_average_precision': 0.18259125273206125, 'test/num_examples': 43793, 'score': 1923.7067909240723, 'total_duration': 2649.681938648224, 'accumulated_submission_time': 1923.7067909240723, 'accumulated_eval_time': 722.8983590602875, 'accumulated_logging_time': 1.3216285705566406}
I0726 08:43:12.432057 140175417947904 logging_writer.py:48] [5890] accumulated_eval_time=722.898359, accumulated_logging_time=1.321629, accumulated_submission_time=1923.706791, global_step=5890, preemption_count=0, score=1923.706791, test/accuracy=0.984727, test/loss=0.051645, test/mean_average_precision=0.182591, test/num_examples=43793, total_duration=2649.681939, train/accuracy=0.988733, train/loss=0.038807, train/mean_average_precision=0.237076, validation/accuracy=0.985632, validation/loss=0.048832, validation/mean_average_precision=0.190734, validation/num_examples=43793
I0726 08:43:48.364578 140222670718784 spec.py:320] Evaluating on the training split.
I0726 08:44:54.068177 140222670718784 spec.py:332] Evaluating on the validation split.
I0726 08:44:59.915244 140222670718784 spec.py:348] Evaluating on the test split.
I0726 08:45:03.436310 140222670718784 submission_runner.py:364] Time since start: 2760.71s, 	Step: 6000, 	{'train/accuracy': 0.9882817256731266, 'train/loss': 0.03992723378436306, 'train/mean_average_precision': 0.2281514628771109, 'validation/accuracy': 0.9854129269379812, 'validation/loss': 0.05022449281769122, 'validation/mean_average_precision': 0.1918076502767154, 'validation/num_examples': 43793, 'test/accuracy': 0.9845543697053195, 'test/loss': 0.05293125307735367, 'test/mean_average_precision': 0.1893878964082617, 'test/num_examples': 43793, 'score': 1959.4575140476227, 'total_duration': 2760.706246614456, 'accumulated_submission_time': 1959.4575140476227, 'accumulated_eval_time': 797.9698567390442, 'accumulated_logging_time': 1.4906039237976074}
I0726 08:45:03.454809 140175409555200 logging_writer.py:48] [6000] accumulated_eval_time=797.969857, accumulated_logging_time=1.490604, accumulated_submission_time=1959.457514, global_step=6000, preemption_count=0, score=1959.457514, test/accuracy=0.984554, test/loss=0.052931, test/mean_average_precision=0.189388, test/num_examples=43793, total_duration=2760.706247, train/accuracy=0.988282, train/loss=0.039927, train/mean_average_precision=0.228151, validation/accuracy=0.985413, validation/loss=0.050224, validation/mean_average_precision=0.191808, validation/num_examples=43793
I0726 08:45:03.615174 140175417947904 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1959.457514
I0726 08:45:03.712898 140222670718784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0726 08:45:03.906719 140222670718784 submission_runner.py:530] Tuning trial 1/1
I0726 08:45:03.906969 140222670718784 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0726 08:45:03.908183 140222670718784 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5493002090201586, 'train/loss': 0.7387837111254324, 'train/mean_average_precision': 0.02138932786922834, 'validation/accuracy': 0.5422068722512741, 'validation/loss': 0.736400377687226, 'validation/mean_average_precision': 0.025561914882982914, 'validation/num_examples': 43793, 'test/accuracy': 0.5404957629920659, 'test/loss': 0.7363124363206286, 'test/mean_average_precision': 0.027183486213567403, 'test/num_examples': 43793, 'score': 5.348168134689331, 'total_duration': 154.0172884464264, 'accumulated_submission_time': 5.348168134689331, 'accumulated_eval_time': 148.66873025894165, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (742, {'train/accuracy': 0.9866877774282068, 'train/loss': 0.056141177167774246, 'train/mean_average_precision': 0.03714810244085458, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06578205591914311, 'validation/mean_average_precision': 0.03915397386842283, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.068987819681501, 'test/mean_average_precision': 0.03877895837942359, 'test/num_examples': 43793, 'score': 245.17618989944458, 'total_duration': 460.6499147415161, 'accumulated_submission_time': 245.17618989944458, 'accumulated_eval_time': 215.09141492843628, 'accumulated_logging_time': 0.15670180320739746, 'global_step': 742, 'preemption_count': 0}), (1477, {'train/accuracy': 0.987202818872333, 'train/loss': 0.04840578598824525, 'train/mean_average_precision': 0.08620832920493984, 'validation/accuracy': 0.9844910356115537, 'validation/loss': 0.05733859087429072, 'validation/mean_average_precision': 0.08898572282819889, 'validation/num_examples': 43793, 'test/accuracy': 0.9834950650724729, 'test/loss': 0.0605515139000564, 'test/mean_average_precision': 0.08722704370857667, 'test/num_examples': 43793, 'score': 484.8218414783478, 'total_duration': 768.5149104595184, 'accumulated_submission_time': 484.8218414783478, 'accumulated_eval_time': 282.9296817779541, 'accumulated_logging_time': 0.31697678565979004, 'global_step': 1477, 'preemption_count': 0}), (2215, {'train/accuracy': 0.9873419041937593, 'train/loss': 0.0476181967950695, 'train/mean_average_precision': 0.09101455888163493, 'validation/accuracy': 0.9844865702638695, 'validation/loss': 0.05757653555188044, 'validation/mean_average_precision': 0.0927390042053077, 'validation/num_examples': 43793, 'test/accuracy': 0.9835169671960944, 'test/loss': 0.06077668852737281, 'test/mean_average_precision': 0.0915716992855218, 'test/num_examples': 43793, 'score': 724.4886541366577, 'total_duration': 1077.6322090625763, 'accumulated_submission_time': 724.4886541366577, 'accumulated_eval_time': 351.99768447875977, 'accumulated_logging_time': 0.4818556308746338, 'global_step': 2215, 'preemption_count': 0}), (2963, {'train/accuracy': 0.9875850221723428, 'train/loss': 0.045149282481901996, 'train/mean_average_precision': 0.13088309434554918, 'validation/accuracy': 0.9848454218413957, 'validation/loss': 0.054666854109784224, 'validation/mean_average_precision': 0.12006185474876142, 'validation/num_examples': 43793, 'test/accuracy': 0.9838703495368333, 'test/loss': 0.057895223294256296, 'test/mean_average_precision': 0.12087795997604366, 'test/num_examples': 43793, 'score': 964.2850499153137, 'total_duration': 1386.8023934364319, 'accumulated_submission_time': 964.2850499153137, 'accumulated_eval_time': 420.9805860519409, 'accumulated_logging_time': 0.6520488262176514, 'global_step': 2963, 'preemption_count': 0}), (3698, {'train/accuracy': 0.9878556966157277, 'train/loss': 0.0431073405000822, 'train/mean_average_precision': 0.15946793665883469, 'validation/accuracy': 0.9849903426707812, 'validation/loss': 0.05287886610309919, 'validation/mean_average_precision': 0.14665069879947934, 'validation/num_examples': 43793, 'test/accuracy': 0.9839933383848616, 'test/loss': 0.05620302099781863, 'test/mean_average_precision': 0.14193413757173107, 'test/num_examples': 43793, 'score': 1204.1757230758667, 'total_duration': 1700.4709944725037, 'accumulated_submission_time': 1204.1757230758667, 'accumulated_eval_time': 494.37970519065857, 'accumulated_logging_time': 0.8168303966522217, 'global_step': 3698, 'preemption_count': 0}), (4424, {'train/accuracy': 0.9881288805807856, 'train/loss': 0.04150809552910168, 'train/mean_average_precision': 0.19003980762319125, 'validation/accuracy': 0.9852895209656193, 'validation/loss': 0.050762795555274104, 'validation/mean_average_precision': 0.16524744370463024, 'validation/num_examples': 43793, 'test/accuracy': 0.984369886433277, 'test/loss': 0.05358974818981054, 'test/mean_average_precision': 0.16253122969320905, 'test/num_examples': 43793, 'score': 1444.075429201126, 'total_duration': 2013.8665852546692, 'accumulated_submission_time': 1444.075429201126, 'accumulated_eval_time': 567.4875395298004, 'accumulated_logging_time': 0.9847531318664551, 'global_step': 4424, 'preemption_count': 0}), (5156, {'train/accuracy': 0.9883935576214193, 'train/loss': 0.03995246386375205, 'train/mean_average_precision': 0.2202271043767101, 'validation/accuracy': 0.9854770655683535, 'validation/loss': 0.049827660413556144, 'validation/mean_average_precision': 0.1754619715966864, 'validation/num_examples': 43793, 'test/accuracy': 0.984575850634256, 'test/loss': 0.05282737922874199, 'test/mean_average_precision': 0.17223766138260976, 'test/num_examples': 43793, 'score': 1683.8408389091492, 'total_duration': 2331.7428493499756, 'accumulated_submission_time': 1683.8408389091492, 'accumulated_eval_time': 645.1996059417725, 'accumulated_logging_time': 1.162524700164795, 'global_step': 5156, 'preemption_count': 0}), (5890, {'train/accuracy': 0.9887327381233565, 'train/loss': 0.03880680107114176, 'train/mean_average_precision': 0.23707551308081729, 'validation/accuracy': 0.9856321349152031, 'validation/loss': 0.04883205913622314, 'validation/mean_average_precision': 0.1907340062741693, 'validation/num_examples': 43793, 'test/accuracy': 0.9847266383314962, 'test/loss': 0.0516450660043661, 'test/mean_average_precision': 0.18259125273206125, 'test/num_examples': 43793, 'score': 1923.7067909240723, 'total_duration': 2649.681938648224, 'accumulated_submission_time': 1923.7067909240723, 'accumulated_eval_time': 722.8983590602875, 'accumulated_logging_time': 1.3216285705566406, 'global_step': 5890, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9882817256731266, 'train/loss': 0.03992723378436306, 'train/mean_average_precision': 0.2281514628771109, 'validation/accuracy': 0.9854129269379812, 'validation/loss': 0.05022449281769122, 'validation/mean_average_precision': 0.1918076502767154, 'validation/num_examples': 43793, 'test/accuracy': 0.9845543697053195, 'test/loss': 0.05293125307735367, 'test/mean_average_precision': 0.1893878964082617, 'test/num_examples': 43793, 'score': 1959.4575140476227, 'total_duration': 2760.706246614456, 'accumulated_submission_time': 1959.4575140476227, 'accumulated_eval_time': 797.9698567390442, 'accumulated_logging_time': 1.4906039237976074, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0726 08:45:03.908306 140222670718784 submission_runner.py:533] Timing: 1959.4575140476227
I0726 08:45:03.908369 140222670718784 submission_runner.py:535] Total number of evals: 10
I0726 08:45:03.908413 140222670718784 submission_runner.py:536] ====================
I0726 08:45:03.908560 140222670718784 submission_runner.py:604] Final ogbg score: 1959.4575140476227
