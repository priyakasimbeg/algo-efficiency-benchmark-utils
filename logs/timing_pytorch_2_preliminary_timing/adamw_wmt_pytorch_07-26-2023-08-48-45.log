torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_timing/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10000 --torch_compile=True 2>&1 | tee -a /logs/wmt_pytorch_07-26-2023-08-48-45.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-26 08:48:55.797443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 08:48:55.797461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0726 08:49:10.630442 140420801611584 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0726 08:49:10.630476 140147521369920 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0726 08:49:10.630507 139693155604288 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0726 08:49:10.630534 140588718593856 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0726 08:49:10.631405 140235789334336 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0726 08:49:10.631641 139940447758144 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0726 08:49:10.631839 140300418180928 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0726 08:49:10.632429 140266560898880 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0726 08:49:10.632744 140266560898880 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.641042 140147521369920 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.641071 140588718593856 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.641076 140420801611584 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.641155 139693155604288 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.641941 140235789334336 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.642219 139940447758144 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:10.642658 140300418180928 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 08:49:15.429501 140266560898880 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/wmt_pytorch.
W0726 08:49:15.461445 140147521369920 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.461788 139940447758144 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.461938 140420801611584 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.462377 139693155604288 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.462424 140588718593856 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.462948 140235789334336 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.464219 140300418180928 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 08:49:15.464724 140266560898880 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0726 08:49:15.469515 140266560898880 submission_runner.py:490] Using RNG seed 1971262831
I0726 08:49:15.471018 140266560898880 submission_runner.py:499] --- Tuning run 1/1 ---
I0726 08:49:15.471159 140266560898880 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/wmt_pytorch/trial_1.
I0726 08:49:15.471438 140266560898880 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/wmt_pytorch/trial_1/hparams.json.
I0726 08:49:15.472297 140266560898880 submission_runner.py:176] Initializing dataset.
I0726 08:49:15.472424 140266560898880 submission_runner.py:183] Initializing model.
W0726 08:49:18.989248 140147521369920 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989294 139693155604288 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989376 140235789334336 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989464 140300418180928 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989511 139940447758144 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989540 140420801611584 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989760 140588718593856 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 08:49:18.989893 140266560898880 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0726 08:49:18.990164 140266560898880 submission_runner.py:217] Initializing optimizer.
I0726 08:49:18.991394 140266560898880 submission_runner.py:224] Initializing metrics bundle.
I0726 08:49:18.991495 140266560898880 submission_runner.py:242] Initializing checkpoint and logger.
I0726 08:49:18.992277 140266560898880 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0726 08:49:18.992378 140266560898880 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0726 08:49:19.478640 140266560898880 submission_runner.py:263] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0726 08:49:19.481803 140266560898880 submission_runner.py:266] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/wmt_pytorch/trial_1/flags_0.json.
I0726 08:49:19.574281 140266560898880 submission_runner.py:276] Starting training loop.
I0726 08:49:19.588372 140266560898880 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0726 08:49:19.592831 140266560898880 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0726 08:49:19.671505 140266560898880 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0726 08:49:23.774811 140226873775872 logging_writer.py:48] [0] global_step=0, grad_norm=5.493969, loss=11.111410
I0726 08:49:23.788133 140266560898880 submission.py:119] 0) loss = 11.111, grad_norm = 5.494
I0726 08:49:23.789894 140266560898880 spec.py:320] Evaluating on the training split.
I0726 08:49:23.792628 140266560898880 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0726 08:49:23.795744 140266560898880 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0726 08:49:23.833321 140266560898880 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0726 08:49:28.168788 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 08:54:15.135064 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 08:54:15.138560 140266560898880 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0726 08:54:15.142549 140266560898880 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0726 08:54:15.178632 140266560898880 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0726 08:54:19.061482 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 08:58:59.704041 140266560898880 spec.py:348] Evaluating on the test split.
I0726 08:58:59.706831 140266560898880 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0726 08:58:59.710787 140266560898880 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0726 08:58:59.746554 140266560898880 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0726 08:59:03.709139 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:03:50.205526 140266560898880 submission_runner.py:364] Time since start: 870.63s, 	Step: 1, 	{'train/accuracy': 0.0006760085704137402, 'train/loss': 11.122050339722952, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.117800771224163, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.10932325257103, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.215680837631226, 'total_duration': 870.6317584514618, 'accumulated_submission_time': 4.215680837631226, 'accumulated_eval_time': 866.4155621528625, 'accumulated_logging_time': 0}
I0726 09:03:50.228935 140208233101056 logging_writer.py:48] [1] accumulated_eval_time=866.415562, accumulated_logging_time=0, accumulated_submission_time=4.215681, global_step=1, preemption_count=0, score=4.215681, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.109323, test/num_examples=3003, total_duration=870.631758, train/accuracy=0.000676, train/bleu=0.000000, train/loss=11.122050, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.117801, validation/num_examples=3000
I0726 09:03:50.534226 140266560898880 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534264 140588718593856 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534280 140300418180928 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534282 139693155604288 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534306 140235789334336 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534379 140420801611584 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534412 140147521369920 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:50.534543 139940447758144 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 09:03:51.321315 140208224708352 logging_writer.py:48] [1] global_step=1, grad_norm=5.457100, loss=11.110195
I0726 09:03:51.325358 140266560898880 submission.py:119] 1) loss = 11.110, grad_norm = 5.457
I0726 09:03:51.767763 140208233101056 logging_writer.py:48] [2] global_step=2, grad_norm=5.452594, loss=11.100530
I0726 09:03:51.771868 140266560898880 submission.py:119] 2) loss = 11.101, grad_norm = 5.453
I0726 09:03:52.210882 140208224708352 logging_writer.py:48] [3] global_step=3, grad_norm=5.373630, loss=11.078874
I0726 09:03:52.214984 140266560898880 submission.py:119] 3) loss = 11.079, grad_norm = 5.374
I0726 09:03:52.652005 140208233101056 logging_writer.py:48] [4] global_step=4, grad_norm=5.345009, loss=11.060637
I0726 09:03:52.656000 140266560898880 submission.py:119] 4) loss = 11.061, grad_norm = 5.345
I0726 09:03:53.099095 140208224708352 logging_writer.py:48] [5] global_step=5, grad_norm=5.255576, loss=11.028532
I0726 09:03:53.102518 140266560898880 submission.py:119] 5) loss = 11.029, grad_norm = 5.256
I0726 09:03:53.544599 140208233101056 logging_writer.py:48] [6] global_step=6, grad_norm=5.159221, loss=10.994461
I0726 09:03:53.548414 140266560898880 submission.py:119] 6) loss = 10.994, grad_norm = 5.159
I0726 09:03:53.987615 140208224708352 logging_writer.py:48] [7] global_step=7, grad_norm=5.030457, loss=10.946824
I0726 09:03:53.991019 140266560898880 submission.py:119] 7) loss = 10.947, grad_norm = 5.030
I0726 09:03:54.430255 140208233101056 logging_writer.py:48] [8] global_step=8, grad_norm=4.939111, loss=10.899155
I0726 09:03:54.434139 140266560898880 submission.py:119] 8) loss = 10.899, grad_norm = 4.939
I0726 09:03:54.873669 140208224708352 logging_writer.py:48] [9] global_step=9, grad_norm=4.797581, loss=10.838833
I0726 09:03:54.877655 140266560898880 submission.py:119] 9) loss = 10.839, grad_norm = 4.798
I0726 09:03:55.314698 140208233101056 logging_writer.py:48] [10] global_step=10, grad_norm=4.675301, loss=10.785762
I0726 09:03:55.318534 140266560898880 submission.py:119] 10) loss = 10.786, grad_norm = 4.675
I0726 09:03:55.760507 140208224708352 logging_writer.py:48] [11] global_step=11, grad_norm=4.477210, loss=10.722727
I0726 09:03:55.764328 140266560898880 submission.py:119] 11) loss = 10.723, grad_norm = 4.477
I0726 09:03:56.200290 140208233101056 logging_writer.py:48] [12] global_step=12, grad_norm=4.232706, loss=10.656803
I0726 09:03:56.203946 140266560898880 submission.py:119] 12) loss = 10.657, grad_norm = 4.233
I0726 09:03:56.641447 140208224708352 logging_writer.py:48] [13] global_step=13, grad_norm=4.120322, loss=10.566642
I0726 09:03:56.645363 140266560898880 submission.py:119] 13) loss = 10.567, grad_norm = 4.120
I0726 09:03:57.083623 140208233101056 logging_writer.py:48] [14] global_step=14, grad_norm=3.911807, loss=10.506099
I0726 09:03:57.087545 140266560898880 submission.py:119] 14) loss = 10.506, grad_norm = 3.912
I0726 09:03:57.525526 140208224708352 logging_writer.py:48] [15] global_step=15, grad_norm=3.697208, loss=10.434649
I0726 09:03:57.529506 140266560898880 submission.py:119] 15) loss = 10.435, grad_norm = 3.697
I0726 09:03:57.970137 140208233101056 logging_writer.py:48] [16] global_step=16, grad_norm=3.483238, loss=10.364313
I0726 09:03:57.974229 140266560898880 submission.py:119] 16) loss = 10.364, grad_norm = 3.483
I0726 09:03:58.417872 140208224708352 logging_writer.py:48] [17] global_step=17, grad_norm=3.289284, loss=10.285194
I0726 09:03:58.421497 140266560898880 submission.py:119] 17) loss = 10.285, grad_norm = 3.289
I0726 09:03:58.860310 140208233101056 logging_writer.py:48] [18] global_step=18, grad_norm=3.124394, loss=10.204949
I0726 09:03:58.863875 140266560898880 submission.py:119] 18) loss = 10.205, grad_norm = 3.124
I0726 09:03:59.303298 140208224708352 logging_writer.py:48] [19] global_step=19, grad_norm=2.907661, loss=10.163923
I0726 09:03:59.307151 140266560898880 submission.py:119] 19) loss = 10.164, grad_norm = 2.908
I0726 09:03:59.748202 140208233101056 logging_writer.py:48] [20] global_step=20, grad_norm=2.723349, loss=10.071249
I0726 09:03:59.751865 140266560898880 submission.py:119] 20) loss = 10.071, grad_norm = 2.723
I0726 09:04:00.188628 140208224708352 logging_writer.py:48] [21] global_step=21, grad_norm=2.539803, loss=10.015303
I0726 09:04:00.192197 140266560898880 submission.py:119] 21) loss = 10.015, grad_norm = 2.540
I0726 09:04:00.632198 140208233101056 logging_writer.py:48] [22] global_step=22, grad_norm=2.377682, loss=9.938626
I0726 09:04:00.636363 140266560898880 submission.py:119] 22) loss = 9.939, grad_norm = 2.378
I0726 09:04:01.074014 140208224708352 logging_writer.py:48] [23] global_step=23, grad_norm=2.218734, loss=9.882689
I0726 09:04:01.078023 140266560898880 submission.py:119] 23) loss = 9.883, grad_norm = 2.219
I0726 09:04:01.517089 140208233101056 logging_writer.py:48] [24] global_step=24, grad_norm=2.086026, loss=9.807080
I0726 09:04:01.520828 140266560898880 submission.py:119] 24) loss = 9.807, grad_norm = 2.086
I0726 09:04:01.963327 140208224708352 logging_writer.py:48] [25] global_step=25, grad_norm=1.925084, loss=9.767968
I0726 09:04:01.967975 140266560898880 submission.py:119] 25) loss = 9.768, grad_norm = 1.925
I0726 09:04:02.411140 140208233101056 logging_writer.py:48] [26] global_step=26, grad_norm=1.820743, loss=9.702605
I0726 09:04:02.414472 140266560898880 submission.py:119] 26) loss = 9.703, grad_norm = 1.821
I0726 09:04:02.851675 140208224708352 logging_writer.py:48] [27] global_step=27, grad_norm=1.697039, loss=9.655340
I0726 09:04:02.855262 140266560898880 submission.py:119] 27) loss = 9.655, grad_norm = 1.697
I0726 09:04:03.294210 140208233101056 logging_writer.py:48] [28] global_step=28, grad_norm=1.603041, loss=9.602024
I0726 09:04:03.297604 140266560898880 submission.py:119] 28) loss = 9.602, grad_norm = 1.603
I0726 09:04:03.732747 140208224708352 logging_writer.py:48] [29] global_step=29, grad_norm=1.516960, loss=9.530438
I0726 09:04:03.736505 140266560898880 submission.py:119] 29) loss = 9.530, grad_norm = 1.517
I0726 09:04:04.173902 140208233101056 logging_writer.py:48] [30] global_step=30, grad_norm=1.401132, loss=9.513826
I0726 09:04:04.177443 140266560898880 submission.py:119] 30) loss = 9.514, grad_norm = 1.401
I0726 09:04:04.616031 140208224708352 logging_writer.py:48] [31] global_step=31, grad_norm=1.336007, loss=9.459110
I0726 09:04:04.619353 140266560898880 submission.py:119] 31) loss = 9.459, grad_norm = 1.336
I0726 09:04:05.056505 140208233101056 logging_writer.py:48] [32] global_step=32, grad_norm=1.259228, loss=9.406299
I0726 09:04:05.059973 140266560898880 submission.py:119] 32) loss = 9.406, grad_norm = 1.259
I0726 09:04:05.498275 140208224708352 logging_writer.py:48] [33] global_step=33, grad_norm=1.171400, loss=9.387786
I0726 09:04:05.501727 140266560898880 submission.py:119] 33) loss = 9.388, grad_norm = 1.171
I0726 09:04:05.940960 140208233101056 logging_writer.py:48] [34] global_step=34, grad_norm=1.104365, loss=9.371765
I0726 09:04:05.944489 140266560898880 submission.py:119] 34) loss = 9.372, grad_norm = 1.104
I0726 09:04:06.381556 140208224708352 logging_writer.py:48] [35] global_step=35, grad_norm=1.054560, loss=9.320389
I0726 09:04:06.385202 140266560898880 submission.py:119] 35) loss = 9.320, grad_norm = 1.055
I0726 09:04:06.824179 140208233101056 logging_writer.py:48] [36] global_step=36, grad_norm=0.990382, loss=9.279085
I0726 09:04:06.827709 140266560898880 submission.py:119] 36) loss = 9.279, grad_norm = 0.990
I0726 09:04:07.264085 140208224708352 logging_writer.py:48] [37] global_step=37, grad_norm=0.915256, loss=9.283002
I0726 09:04:07.267502 140266560898880 submission.py:119] 37) loss = 9.283, grad_norm = 0.915
I0726 09:04:07.708390 140208233101056 logging_writer.py:48] [38] global_step=38, grad_norm=0.877622, loss=9.257265
I0726 09:04:07.711703 140266560898880 submission.py:119] 38) loss = 9.257, grad_norm = 0.878
I0726 09:04:08.150641 140208224708352 logging_writer.py:48] [39] global_step=39, grad_norm=0.841534, loss=9.209583
I0726 09:04:08.153899 140266560898880 submission.py:119] 39) loss = 9.210, grad_norm = 0.842
I0726 09:04:08.590981 140208233101056 logging_writer.py:48] [40] global_step=40, grad_norm=0.797751, loss=9.197304
I0726 09:04:08.594415 140266560898880 submission.py:119] 40) loss = 9.197, grad_norm = 0.798
I0726 09:04:09.036212 140208224708352 logging_writer.py:48] [41] global_step=41, grad_norm=0.764237, loss=9.151891
I0726 09:04:09.039534 140266560898880 submission.py:119] 41) loss = 9.152, grad_norm = 0.764
I0726 09:04:09.479784 140208233101056 logging_writer.py:48] [42] global_step=42, grad_norm=0.714615, loss=9.142965
I0726 09:04:09.483312 140266560898880 submission.py:119] 42) loss = 9.143, grad_norm = 0.715
I0726 09:04:09.921670 140208224708352 logging_writer.py:48] [43] global_step=43, grad_norm=0.692150, loss=9.113391
I0726 09:04:09.925135 140266560898880 submission.py:119] 43) loss = 9.113, grad_norm = 0.692
I0726 09:04:10.365063 140208233101056 logging_writer.py:48] [44] global_step=44, grad_norm=0.666694, loss=9.091310
I0726 09:04:10.368798 140266560898880 submission.py:119] 44) loss = 9.091, grad_norm = 0.667
I0726 09:04:10.811774 140208224708352 logging_writer.py:48] [45] global_step=45, grad_norm=0.635868, loss=9.045789
I0726 09:04:10.815113 140266560898880 submission.py:119] 45) loss = 9.046, grad_norm = 0.636
I0726 09:04:11.251703 140208233101056 logging_writer.py:48] [46] global_step=46, grad_norm=0.600909, loss=9.066776
I0726 09:04:11.255037 140266560898880 submission.py:119] 46) loss = 9.067, grad_norm = 0.601
I0726 09:04:11.693652 140208224708352 logging_writer.py:48] [47] global_step=47, grad_norm=0.584195, loss=9.022405
I0726 09:04:11.697735 140266560898880 submission.py:119] 47) loss = 9.022, grad_norm = 0.584
I0726 09:04:12.137827 140208233101056 logging_writer.py:48] [48] global_step=48, grad_norm=0.543195, loss=9.037872
I0726 09:04:12.141101 140266560898880 submission.py:119] 48) loss = 9.038, grad_norm = 0.543
I0726 09:04:12.579932 140208224708352 logging_writer.py:48] [49] global_step=49, grad_norm=0.525638, loss=8.992143
I0726 09:04:12.583265 140266560898880 submission.py:119] 49) loss = 8.992, grad_norm = 0.526
I0726 09:04:13.022209 140208233101056 logging_writer.py:48] [50] global_step=50, grad_norm=0.520380, loss=8.939292
I0726 09:04:13.025572 140266560898880 submission.py:119] 50) loss = 8.939, grad_norm = 0.520
I0726 09:04:13.463009 140208224708352 logging_writer.py:48] [51] global_step=51, grad_norm=0.489595, loss=8.962167
I0726 09:04:13.466359 140266560898880 submission.py:119] 51) loss = 8.962, grad_norm = 0.490
I0726 09:04:13.903990 140208233101056 logging_writer.py:48] [52] global_step=52, grad_norm=0.462217, loss=8.956822
I0726 09:04:13.907324 140266560898880 submission.py:119] 52) loss = 8.957, grad_norm = 0.462
I0726 09:04:14.346816 140208224708352 logging_writer.py:48] [53] global_step=53, grad_norm=0.455791, loss=8.938220
I0726 09:04:14.350152 140266560898880 submission.py:119] 53) loss = 8.938, grad_norm = 0.456
I0726 09:04:14.785254 140208233101056 logging_writer.py:48] [54] global_step=54, grad_norm=0.427260, loss=8.966868
I0726 09:04:14.788491 140266560898880 submission.py:119] 54) loss = 8.967, grad_norm = 0.427
I0726 09:04:15.227013 140208224708352 logging_writer.py:48] [55] global_step=55, grad_norm=0.410467, loss=8.916433
I0726 09:04:15.230190 140266560898880 submission.py:119] 55) loss = 8.916, grad_norm = 0.410
I0726 09:04:15.666306 140208233101056 logging_writer.py:48] [56] global_step=56, grad_norm=0.400227, loss=8.920515
I0726 09:04:15.669564 140266560898880 submission.py:119] 56) loss = 8.921, grad_norm = 0.400
I0726 09:04:16.106344 140208224708352 logging_writer.py:48] [57] global_step=57, grad_norm=0.392366, loss=8.889370
I0726 09:04:16.109736 140266560898880 submission.py:119] 57) loss = 8.889, grad_norm = 0.392
I0726 09:04:16.549708 140208233101056 logging_writer.py:48] [58] global_step=58, grad_norm=0.389254, loss=8.894380
I0726 09:04:16.553156 140266560898880 submission.py:119] 58) loss = 8.894, grad_norm = 0.389
I0726 09:04:16.988456 140208224708352 logging_writer.py:48] [59] global_step=59, grad_norm=0.361111, loss=8.893659
I0726 09:04:16.991950 140266560898880 submission.py:119] 59) loss = 8.894, grad_norm = 0.361
I0726 09:04:17.431633 140208233101056 logging_writer.py:48] [60] global_step=60, grad_norm=0.364095, loss=8.853874
I0726 09:04:17.435025 140266560898880 submission.py:119] 60) loss = 8.854, grad_norm = 0.364
I0726 09:04:17.876590 140208224708352 logging_writer.py:48] [61] global_step=61, grad_norm=0.353637, loss=8.887100
I0726 09:04:17.880099 140266560898880 submission.py:119] 61) loss = 8.887, grad_norm = 0.354
I0726 09:04:18.315303 140208233101056 logging_writer.py:48] [62] global_step=62, grad_norm=0.333120, loss=8.848016
I0726 09:04:18.318625 140266560898880 submission.py:119] 62) loss = 8.848, grad_norm = 0.333
I0726 09:04:18.757790 140208224708352 logging_writer.py:48] [63] global_step=63, grad_norm=0.330972, loss=8.850609
I0726 09:04:18.761199 140266560898880 submission.py:119] 63) loss = 8.851, grad_norm = 0.331
I0726 09:04:19.203891 140208233101056 logging_writer.py:48] [64] global_step=64, grad_norm=0.317416, loss=8.827234
I0726 09:04:19.207322 140266560898880 submission.py:119] 64) loss = 8.827, grad_norm = 0.317
I0726 09:04:19.643012 140208224708352 logging_writer.py:48] [65] global_step=65, grad_norm=0.317230, loss=8.800948
I0726 09:04:19.646566 140266560898880 submission.py:119] 65) loss = 8.801, grad_norm = 0.317
I0726 09:04:20.085903 140208233101056 logging_writer.py:48] [66] global_step=66, grad_norm=0.318564, loss=8.819183
I0726 09:04:20.089250 140266560898880 submission.py:119] 66) loss = 8.819, grad_norm = 0.319
I0726 09:04:20.526108 140208224708352 logging_writer.py:48] [67] global_step=67, grad_norm=0.302890, loss=8.773102
I0726 09:04:20.529637 140266560898880 submission.py:119] 67) loss = 8.773, grad_norm = 0.303
I0726 09:04:20.968987 140208233101056 logging_writer.py:48] [68] global_step=68, grad_norm=0.297799, loss=8.813720
I0726 09:04:20.972335 140266560898880 submission.py:119] 68) loss = 8.814, grad_norm = 0.298
I0726 09:04:21.411377 140208224708352 logging_writer.py:48] [69] global_step=69, grad_norm=0.284013, loss=8.766325
I0726 09:04:21.415148 140266560898880 submission.py:119] 69) loss = 8.766, grad_norm = 0.284
I0726 09:04:21.852002 140208233101056 logging_writer.py:48] [70] global_step=70, grad_norm=0.291215, loss=8.760069
I0726 09:04:21.855222 140266560898880 submission.py:119] 70) loss = 8.760, grad_norm = 0.291
I0726 09:04:22.295973 140208224708352 logging_writer.py:48] [71] global_step=71, grad_norm=0.274262, loss=8.775409
I0726 09:04:22.299415 140266560898880 submission.py:119] 71) loss = 8.775, grad_norm = 0.274
I0726 09:04:22.740804 140208233101056 logging_writer.py:48] [72] global_step=72, grad_norm=0.266603, loss=8.744416
I0726 09:04:22.744250 140266560898880 submission.py:119] 72) loss = 8.744, grad_norm = 0.267
I0726 09:04:23.179978 140208224708352 logging_writer.py:48] [73] global_step=73, grad_norm=0.282268, loss=8.740898
I0726 09:04:23.183302 140266560898880 submission.py:119] 73) loss = 8.741, grad_norm = 0.282
I0726 09:04:23.621326 140208233101056 logging_writer.py:48] [74] global_step=74, grad_norm=0.269629, loss=8.753098
I0726 09:04:23.624823 140266560898880 submission.py:119] 74) loss = 8.753, grad_norm = 0.270
I0726 09:04:24.062719 140208224708352 logging_writer.py:48] [75] global_step=75, grad_norm=0.265922, loss=8.726533
I0726 09:04:24.066382 140266560898880 submission.py:119] 75) loss = 8.727, grad_norm = 0.266
I0726 09:04:24.503200 140208233101056 logging_writer.py:48] [76] global_step=76, grad_norm=0.254690, loss=8.718683
I0726 09:04:24.506753 140266560898880 submission.py:119] 76) loss = 8.719, grad_norm = 0.255
I0726 09:04:24.943863 140208224708352 logging_writer.py:48] [77] global_step=77, grad_norm=0.251727, loss=8.720150
I0726 09:04:24.947281 140266560898880 submission.py:119] 77) loss = 8.720, grad_norm = 0.252
I0726 09:04:25.385393 140208233101056 logging_writer.py:48] [78] global_step=78, grad_norm=0.257926, loss=8.701589
I0726 09:04:25.389061 140266560898880 submission.py:119] 78) loss = 8.702, grad_norm = 0.258
I0726 09:04:25.831317 140208224708352 logging_writer.py:48] [79] global_step=79, grad_norm=0.248539, loss=8.714502
I0726 09:04:25.834545 140266560898880 submission.py:119] 79) loss = 8.715, grad_norm = 0.249
I0726 09:04:26.272385 140208233101056 logging_writer.py:48] [80] global_step=80, grad_norm=0.258230, loss=8.676395
I0726 09:04:26.275931 140266560898880 submission.py:119] 80) loss = 8.676, grad_norm = 0.258
I0726 09:04:26.713048 140208224708352 logging_writer.py:48] [81] global_step=81, grad_norm=0.245502, loss=8.705497
I0726 09:04:26.716350 140266560898880 submission.py:119] 81) loss = 8.705, grad_norm = 0.246
I0726 09:04:27.156865 140208233101056 logging_writer.py:48] [82] global_step=82, grad_norm=0.250510, loss=8.679811
I0726 09:04:27.160138 140266560898880 submission.py:119] 82) loss = 8.680, grad_norm = 0.251
I0726 09:04:27.599266 140208224708352 logging_writer.py:48] [83] global_step=83, grad_norm=0.252108, loss=8.679945
I0726 09:04:27.602681 140266560898880 submission.py:119] 83) loss = 8.680, grad_norm = 0.252
I0726 09:04:28.037791 140208233101056 logging_writer.py:48] [84] global_step=84, grad_norm=0.235558, loss=8.687902
I0726 09:04:28.041329 140266560898880 submission.py:119] 84) loss = 8.688, grad_norm = 0.236
I0726 09:04:28.483118 140208224708352 logging_writer.py:48] [85] global_step=85, grad_norm=0.239813, loss=8.651198
I0726 09:04:28.486392 140266560898880 submission.py:119] 85) loss = 8.651, grad_norm = 0.240
I0726 09:04:28.922087 140208233101056 logging_writer.py:48] [86] global_step=86, grad_norm=0.236416, loss=8.633716
I0726 09:04:28.925377 140266560898880 submission.py:119] 86) loss = 8.634, grad_norm = 0.236
I0726 09:04:29.364948 140208224708352 logging_writer.py:48] [87] global_step=87, grad_norm=0.242059, loss=8.627923
I0726 09:04:29.368257 140266560898880 submission.py:119] 87) loss = 8.628, grad_norm = 0.242
I0726 09:04:29.806325 140208233101056 logging_writer.py:48] [88] global_step=88, grad_norm=0.237136, loss=8.632863
I0726 09:04:29.809564 140266560898880 submission.py:119] 88) loss = 8.633, grad_norm = 0.237
I0726 09:04:30.243580 140208224708352 logging_writer.py:48] [89] global_step=89, grad_norm=0.250203, loss=8.638745
I0726 09:04:30.246995 140266560898880 submission.py:119] 89) loss = 8.639, grad_norm = 0.250
I0726 09:04:30.685431 140208233101056 logging_writer.py:48] [90] global_step=90, grad_norm=0.229773, loss=8.625948
I0726 09:04:30.688909 140266560898880 submission.py:119] 90) loss = 8.626, grad_norm = 0.230
I0726 09:04:31.125404 140208224708352 logging_writer.py:48] [91] global_step=91, grad_norm=0.245006, loss=8.599957
I0726 09:04:31.128874 140266560898880 submission.py:119] 91) loss = 8.600, grad_norm = 0.245
I0726 09:04:31.565749 140208233101056 logging_writer.py:48] [92] global_step=92, grad_norm=0.225404, loss=8.644407
I0726 09:04:31.569407 140266560898880 submission.py:119] 92) loss = 8.644, grad_norm = 0.225
I0726 09:04:32.010029 140208224708352 logging_writer.py:48] [93] global_step=93, grad_norm=0.229684, loss=8.599471
I0726 09:04:32.013323 140266560898880 submission.py:119] 93) loss = 8.599, grad_norm = 0.230
I0726 09:04:32.451586 140208233101056 logging_writer.py:48] [94] global_step=94, grad_norm=0.245605, loss=8.619070
I0726 09:04:32.455024 140266560898880 submission.py:119] 94) loss = 8.619, grad_norm = 0.246
I0726 09:04:32.896394 140208224708352 logging_writer.py:48] [95] global_step=95, grad_norm=0.227262, loss=8.613304
I0726 09:04:32.899744 140266560898880 submission.py:119] 95) loss = 8.613, grad_norm = 0.227
I0726 09:04:33.341482 140208233101056 logging_writer.py:48] [96] global_step=96, grad_norm=0.240808, loss=8.579593
I0726 09:04:33.344830 140266560898880 submission.py:119] 96) loss = 8.580, grad_norm = 0.241
I0726 09:04:33.784518 140208224708352 logging_writer.py:48] [97] global_step=97, grad_norm=0.262278, loss=8.611533
I0726 09:04:33.788291 140266560898880 submission.py:119] 97) loss = 8.612, grad_norm = 0.262
I0726 09:04:34.229696 140208233101056 logging_writer.py:48] [98] global_step=98, grad_norm=0.239979, loss=8.584543
I0726 09:04:34.233772 140266560898880 submission.py:119] 98) loss = 8.585, grad_norm = 0.240
I0726 09:04:34.675019 140208224708352 logging_writer.py:48] [99] global_step=99, grad_norm=0.244368, loss=8.622902
I0726 09:04:34.678802 140266560898880 submission.py:119] 99) loss = 8.623, grad_norm = 0.244
I0726 09:04:35.114418 140208233101056 logging_writer.py:48] [100] global_step=100, grad_norm=0.219779, loss=8.580141
I0726 09:04:35.118238 140266560898880 submission.py:119] 100) loss = 8.580, grad_norm = 0.220
I0726 09:07:27.993432 140208224708352 logging_writer.py:48] [500] global_step=500, grad_norm=0.424022, loss=6.874883
I0726 09:07:27.997475 140266560898880 submission.py:119] 500) loss = 6.875, grad_norm = 0.424
I0726 09:11:04.147890 140208233101056 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.196826, loss=5.997557
I0726 09:11:04.151725 140266560898880 submission.py:119] 1000) loss = 5.998, grad_norm = 1.197
I0726 09:14:39.933520 140208224708352 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.434555, loss=5.605472
I0726 09:14:39.937160 140266560898880 submission.py:119] 1500) loss = 5.605, grad_norm = 0.435
I0726 09:17:50.575109 140266560898880 spec.py:320] Evaluating on the training split.
I0726 09:17:54.409752 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:21:12.097836 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 09:21:15.855195 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:24:20.441929 140266560898880 spec.py:348] Evaluating on the test split.
I0726 09:24:24.271571 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:27:17.654376 140266560898880 submission_runner.py:364] Time since start: 2278.08s, 	Step: 1942, 	{'train/accuracy': 0.40308390230490765, 'train/loss': 4.001608466411584, 'train/bleu': 13.081155859788074, 'validation/accuracy': 0.38740995151950997, 'validation/loss': 4.126576235880522, 'validation/bleu': 8.8360725010858, 'validation/num_examples': 3000, 'test/accuracy': 0.3758875137993144, 'test/loss': 4.300234951484516, 'test/bleu': 7.211591635731238, 'test/num_examples': 3003, 'score': 843.5461502075195, 'total_duration': 2278.0806007385254, 'accumulated_submission_time': 843.5461502075195, 'accumulated_eval_time': 1433.4947364330292, 'accumulated_logging_time': 0.315352201461792}
I0726 09:27:17.673568 140208233101056 logging_writer.py:48] [1942] accumulated_eval_time=1433.494736, accumulated_logging_time=0.315352, accumulated_submission_time=843.546150, global_step=1942, preemption_count=0, score=843.546150, test/accuracy=0.375888, test/bleu=7.211592, test/loss=4.300235, test/num_examples=3003, total_duration=2278.080601, train/accuracy=0.403084, train/bleu=13.081156, train/loss=4.001608, validation/accuracy=0.387410, validation/bleu=8.836073, validation/loss=4.126576, validation/num_examples=3000
I0726 09:27:43.907552 140208224708352 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.524707, loss=4.853770
I0726 09:27:43.910923 140266560898880 submission.py:119] 2000) loss = 4.854, grad_norm = 0.525
I0726 09:31:20.423063 140208233101056 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.408919, loss=4.391135
I0726 09:31:20.427354 140266560898880 submission.py:119] 2500) loss = 4.391, grad_norm = 0.409
I0726 09:34:56.651598 140208224708352 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.445772, loss=4.076405
I0726 09:34:56.656011 140266560898880 submission.py:119] 3000) loss = 4.076, grad_norm = 0.446
I0726 09:38:33.022096 140208233101056 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.351682, loss=3.817672
I0726 09:38:33.025553 140266560898880 submission.py:119] 3500) loss = 3.818, grad_norm = 0.352
I0726 09:41:17.711722 140266560898880 spec.py:320] Evaluating on the training split.
I0726 09:41:21.534991 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:43:58.753360 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 09:44:02.502571 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:46:32.113743 140266560898880 spec.py:348] Evaluating on the test split.
I0726 09:46:35.947692 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 09:48:53.424618 140266560898880 submission_runner.py:364] Time since start: 3573.85s, 	Step: 3882, 	{'train/accuracy': 0.5318660483815703, 'train/loss': 2.7686377835393015, 'train/bleu': 23.89749938601213, 'validation/accuracy': 0.5391377664257108, 'validation/loss': 2.695039816617277, 'validation/bleu': 19.9699172322348, 'validation/num_examples': 3000, 'test/accuracy': 0.5382952762767997, 'test/loss': 2.7383417581779095, 'test/bleu': 18.500844951581144, 'test/num_examples': 3003, 'score': 1682.393075466156, 'total_duration': 3573.8508701324463, 'accumulated_submission_time': 1682.393075466156, 'accumulated_eval_time': 1889.2075538635254, 'accumulated_logging_time': 0.7031979560852051}
I0726 09:48:53.440645 140208224708352 logging_writer.py:48] [3882] accumulated_eval_time=1889.207554, accumulated_logging_time=0.703198, accumulated_submission_time=1682.393075, global_step=3882, preemption_count=0, score=1682.393075, test/accuracy=0.538295, test/bleu=18.500845, test/loss=2.738342, test/num_examples=3003, total_duration=3573.850870, train/accuracy=0.531866, train/bleu=23.897499, train/loss=2.768638, validation/accuracy=0.539138, validation/bleu=19.969917, validation/loss=2.695040, validation/num_examples=3000
I0726 09:49:45.421082 140208233101056 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.321131, loss=3.840247
I0726 09:49:45.425425 140266560898880 submission.py:119] 4000) loss = 3.840, grad_norm = 0.321
I0726 09:53:21.521321 140208224708352 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.260561, loss=3.600425
I0726 09:53:21.525205 140266560898880 submission.py:119] 4500) loss = 3.600, grad_norm = 0.261
I0726 09:56:57.801910 140208233101056 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.229271, loss=3.532525
I0726 09:56:57.805816 140266560898880 submission.py:119] 5000) loss = 3.533, grad_norm = 0.229
I0726 10:00:33.837605 140208224708352 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.359488, loss=3.527434
I0726 10:00:33.841230 140266560898880 submission.py:119] 5500) loss = 3.527, grad_norm = 0.359
I0726 10:02:53.838989 140266560898880 spec.py:320] Evaluating on the training split.
I0726 10:02:57.676611 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:05:20.716463 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 10:05:24.465997 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:07:46.621510 140266560898880 spec.py:348] Evaluating on the test split.
I0726 10:07:50.446884 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:10:06.457332 140266560898880 submission_runner.py:364] Time since start: 4846.88s, 	Step: 5825, 	{'train/accuracy': 0.5719957679662347, 'train/loss': 2.395366662495307, 'train/bleu': 26.779673756867034, 'validation/accuracy': 0.5858699830132298, 'validation/loss': 2.293998214529268, 'validation/bleu': 23.5296136772959, 'validation/num_examples': 3000, 'test/accuracy': 0.5876009528789727, 'test/loss': 2.285848803091046, 'test/bleu': 22.282172651922178, 'test/num_examples': 3003, 'score': 2521.628433227539, 'total_duration': 4846.883584260941, 'accumulated_submission_time': 2521.628433227539, 'accumulated_eval_time': 2321.8259060382843, 'accumulated_logging_time': 1.0415561199188232}
I0726 10:10:06.473282 140208233101056 logging_writer.py:48] [5825] accumulated_eval_time=2321.825906, accumulated_logging_time=1.041556, accumulated_submission_time=2521.628433, global_step=5825, preemption_count=0, score=2521.628433, test/accuracy=0.587601, test/bleu=22.282173, test/loss=2.285849, test/num_examples=3003, total_duration=4846.883584, train/accuracy=0.571996, train/bleu=26.779674, train/loss=2.395367, validation/accuracy=0.585870, validation/bleu=23.529614, validation/loss=2.293998, validation/num_examples=3000
I0726 10:11:23.173625 140208224708352 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.203971, loss=3.431272
I0726 10:11:23.177915 140266560898880 submission.py:119] 6000) loss = 3.431, grad_norm = 0.204
I0726 10:14:59.227802 140208233101056 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.217519, loss=3.393756
I0726 10:14:59.231316 140266560898880 submission.py:119] 6500) loss = 3.394, grad_norm = 0.218
I0726 10:18:35.299203 140208224708352 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.190214, loss=3.415402
I0726 10:18:35.302845 140266560898880 submission.py:119] 7000) loss = 3.415, grad_norm = 0.190
I0726 10:22:11.458161 140208233101056 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.181371, loss=3.326667
I0726 10:22:11.461632 140266560898880 submission.py:119] 7500) loss = 3.327, grad_norm = 0.181
I0726 10:24:06.694709 140266560898880 spec.py:320] Evaluating on the training split.
I0726 10:24:10.526479 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:26:38.036094 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 10:26:41.776116 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:29:01.756692 140266560898880 spec.py:348] Evaluating on the test split.
I0726 10:29:05.582920 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:31:19.472856 140266560898880 submission_runner.py:364] Time since start: 6119.90s, 	Step: 7768, 	{'train/accuracy': 0.594159618847881, 'train/loss': 2.1978686997059294, 'train/bleu': 28.113907119404942, 'validation/accuracy': 0.6083247572875724, 'validation/loss': 2.0978325981698926, 'validation/bleu': 24.738987152686633, 'validation/num_examples': 3000, 'test/accuracy': 0.6102376387194236, 'test/loss': 2.084252803439661, 'test/bleu': 23.222960223940422, 'test/num_examples': 3003, 'score': 3360.6802220344543, 'total_duration': 6119.899093389511, 'accumulated_submission_time': 3360.6802220344543, 'accumulated_eval_time': 2754.6039764881134, 'accumulated_logging_time': 1.376025676727295}
I0726 10:31:19.489141 140208224708352 logging_writer.py:48] [7768] accumulated_eval_time=2754.603976, accumulated_logging_time=1.376026, accumulated_submission_time=3360.680222, global_step=7768, preemption_count=0, score=3360.680222, test/accuracy=0.610238, test/bleu=23.222960, test/loss=2.084253, test/num_examples=3003, total_duration=6119.899093, train/accuracy=0.594160, train/bleu=28.113907, train/loss=2.197869, validation/accuracy=0.608325, validation/bleu=24.738987, validation/loss=2.097833, validation/num_examples=3000
I0726 10:33:00.738714 140208233101056 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.157619, loss=3.315750
I0726 10:33:00.742135 140266560898880 submission.py:119] 8000) loss = 3.316, grad_norm = 0.158
I0726 10:36:36.750820 140208224708352 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.155238, loss=3.236850
I0726 10:36:36.754961 140266560898880 submission.py:119] 8500) loss = 3.237, grad_norm = 0.155
I0726 10:40:12.610917 140208233101056 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.145384, loss=3.212114
I0726 10:40:12.615441 140266560898880 submission.py:119] 9000) loss = 3.212, grad_norm = 0.145
I0726 10:43:48.568406 140208224708352 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.136435, loss=3.177277
I0726 10:43:48.571800 140266560898880 submission.py:119] 9500) loss = 3.177, grad_norm = 0.136
I0726 10:45:19.741993 140266560898880 spec.py:320] Evaluating on the training split.
I0726 10:45:23.556877 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:48:03.572344 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 10:48:07.313637 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:50:30.181540 140266560898880 spec.py:348] Evaluating on the test split.
I0726 10:50:33.988837 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:52:47.178759 140266560898880 submission_runner.py:364] Time since start: 7407.60s, 	Step: 9712, 	{'train/accuracy': 0.611799719488694, 'train/loss': 2.0879840530462843, 'train/bleu': 29.38505281079433, 'validation/accuracy': 0.6261794646067624, 'validation/loss': 1.9681168708385512, 'validation/bleu': 25.879057686966974, 'validation/num_examples': 3000, 'test/accuracy': 0.6340131311370635, 'test/loss': 1.9221509862878392, 'test/bleu': 25.361778013340192, 'test/num_examples': 3003, 'score': 4199.828973770142, 'total_duration': 7407.6049263477325, 'accumulated_submission_time': 4199.828973770142, 'accumulated_eval_time': 3202.040596961975, 'accumulated_logging_time': 1.677593469619751}
I0726 10:52:47.196304 140208233101056 logging_writer.py:48] [9712] accumulated_eval_time=3202.040597, accumulated_logging_time=1.677593, accumulated_submission_time=4199.828974, global_step=9712, preemption_count=0, score=4199.828974, test/accuracy=0.634013, test/bleu=25.361778, test/loss=1.922151, test/num_examples=3003, total_duration=7407.604926, train/accuracy=0.611800, train/bleu=29.385053, train/loss=2.087984, validation/accuracy=0.626179, validation/bleu=25.879058, validation/loss=1.968117, validation/num_examples=3000
I0726 10:54:52.116296 140266560898880 spec.py:320] Evaluating on the training split.
I0726 10:54:55.962013 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:57:15.121523 140266560898880 spec.py:332] Evaluating on the validation split.
I0726 10:57:18.851157 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 10:59:38.524672 140266560898880 spec.py:348] Evaluating on the test split.
I0726 10:59:42.352812 140266560898880 workload.py:131] Translating evaluation dataset.
I0726 11:01:52.133379 140266560898880 submission_runner.py:364] Time since start: 7952.56s, 	Step: 10000, 	{'train/accuracy': 0.610444576986603, 'train/loss': 2.069033237176546, 'train/bleu': 28.966939851699383, 'validation/accuracy': 0.627010204461197, 'validation/loss': 1.9416649359586367, 'validation/bleu': 25.863807097535403, 'validation/num_examples': 3000, 'test/accuracy': 0.6333275230956946, 'test/loss': 1.8954990630991808, 'test/bleu': 25.00064649225441, 'test/num_examples': 3003, 'score': 4324.335209608078, 'total_duration': 7952.559632062912, 'accumulated_submission_time': 4324.335209608078, 'accumulated_eval_time': 3622.057601928711, 'accumulated_logging_time': 1.9922866821289062}
I0726 11:01:52.149401 140208224708352 logging_writer.py:48] [10000] accumulated_eval_time=3622.057602, accumulated_logging_time=1.992287, accumulated_submission_time=4324.335210, global_step=10000, preemption_count=0, score=4324.335210, test/accuracy=0.633328, test/bleu=25.000646, test/loss=1.895499, test/num_examples=3003, total_duration=7952.559632, train/accuracy=0.610445, train/bleu=28.966940, train/loss=2.069033, validation/accuracy=0.627010, validation/bleu=25.863807, validation/loss=1.941665, validation/num_examples=3000
I0726 11:01:52.484395 140208233101056 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4324.335210
I0726 11:01:54.760117 140266560898880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0726 11:01:54.782356 140266560898880 submission_runner.py:530] Tuning trial 1/1
I0726 11:01:54.782534 140266560898880 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0726 11:01:54.783200 140266560898880 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006760085704137402, 'train/loss': 11.122050339722952, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.117800771224163, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.10932325257103, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.215680837631226, 'total_duration': 870.6317584514618, 'accumulated_submission_time': 4.215680837631226, 'accumulated_eval_time': 866.4155621528625, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1942, {'train/accuracy': 0.40308390230490765, 'train/loss': 4.001608466411584, 'train/bleu': 13.081155859788074, 'validation/accuracy': 0.38740995151950997, 'validation/loss': 4.126576235880522, 'validation/bleu': 8.8360725010858, 'validation/num_examples': 3000, 'test/accuracy': 0.3758875137993144, 'test/loss': 4.300234951484516, 'test/bleu': 7.211591635731238, 'test/num_examples': 3003, 'score': 843.5461502075195, 'total_duration': 2278.0806007385254, 'accumulated_submission_time': 843.5461502075195, 'accumulated_eval_time': 1433.4947364330292, 'accumulated_logging_time': 0.315352201461792, 'global_step': 1942, 'preemption_count': 0}), (3882, {'train/accuracy': 0.5318660483815703, 'train/loss': 2.7686377835393015, 'train/bleu': 23.89749938601213, 'validation/accuracy': 0.5391377664257108, 'validation/loss': 2.695039816617277, 'validation/bleu': 19.9699172322348, 'validation/num_examples': 3000, 'test/accuracy': 0.5382952762767997, 'test/loss': 2.7383417581779095, 'test/bleu': 18.500844951581144, 'test/num_examples': 3003, 'score': 1682.393075466156, 'total_duration': 3573.8508701324463, 'accumulated_submission_time': 1682.393075466156, 'accumulated_eval_time': 1889.2075538635254, 'accumulated_logging_time': 0.7031979560852051, 'global_step': 3882, 'preemption_count': 0}), (5825, {'train/accuracy': 0.5719957679662347, 'train/loss': 2.395366662495307, 'train/bleu': 26.779673756867034, 'validation/accuracy': 0.5858699830132298, 'validation/loss': 2.293998214529268, 'validation/bleu': 23.5296136772959, 'validation/num_examples': 3000, 'test/accuracy': 0.5876009528789727, 'test/loss': 2.285848803091046, 'test/bleu': 22.282172651922178, 'test/num_examples': 3003, 'score': 2521.628433227539, 'total_duration': 4846.883584260941, 'accumulated_submission_time': 2521.628433227539, 'accumulated_eval_time': 2321.8259060382843, 'accumulated_logging_time': 1.0415561199188232, 'global_step': 5825, 'preemption_count': 0}), (7768, {'train/accuracy': 0.594159618847881, 'train/loss': 2.1978686997059294, 'train/bleu': 28.113907119404942, 'validation/accuracy': 0.6083247572875724, 'validation/loss': 2.0978325981698926, 'validation/bleu': 24.738987152686633, 'validation/num_examples': 3000, 'test/accuracy': 0.6102376387194236, 'test/loss': 2.084252803439661, 'test/bleu': 23.222960223940422, 'test/num_examples': 3003, 'score': 3360.6802220344543, 'total_duration': 6119.899093389511, 'accumulated_submission_time': 3360.6802220344543, 'accumulated_eval_time': 2754.6039764881134, 'accumulated_logging_time': 1.376025676727295, 'global_step': 7768, 'preemption_count': 0}), (9712, {'train/accuracy': 0.611799719488694, 'train/loss': 2.0879840530462843, 'train/bleu': 29.38505281079433, 'validation/accuracy': 0.6261794646067624, 'validation/loss': 1.9681168708385512, 'validation/bleu': 25.879057686966974, 'validation/num_examples': 3000, 'test/accuracy': 0.6340131311370635, 'test/loss': 1.9221509862878392, 'test/bleu': 25.361778013340192, 'test/num_examples': 3003, 'score': 4199.828973770142, 'total_duration': 7407.6049263477325, 'accumulated_submission_time': 4199.828973770142, 'accumulated_eval_time': 3202.040596961975, 'accumulated_logging_time': 1.677593469619751, 'global_step': 9712, 'preemption_count': 0}), (10000, {'train/accuracy': 0.610444576986603, 'train/loss': 2.069033237176546, 'train/bleu': 28.966939851699383, 'validation/accuracy': 0.627010204461197, 'validation/loss': 1.9416649359586367, 'validation/bleu': 25.863807097535403, 'validation/num_examples': 3000, 'test/accuracy': 0.6333275230956946, 'test/loss': 1.8954990630991808, 'test/bleu': 25.00064649225441, 'test/num_examples': 3003, 'score': 4324.335209608078, 'total_duration': 7952.559632062912, 'accumulated_submission_time': 4324.335209608078, 'accumulated_eval_time': 3622.057601928711, 'accumulated_logging_time': 1.9922866821289062, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0726 11:01:54.783315 140266560898880 submission_runner.py:533] Timing: 4324.335209608078
I0726 11:01:54.783378 140266560898880 submission_runner.py:535] Total number of evals: 7
I0726 11:01:54.783426 140266560898880 submission_runner.py:536] ====================
I0726 11:01:54.783510 140266560898880 submission_runner.py:604] Final wmt score: 4324.335209608078
