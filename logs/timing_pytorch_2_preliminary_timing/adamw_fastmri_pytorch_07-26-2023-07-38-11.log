torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_timing/adamw --overwrite=True --save_checkpoints=False --max_global_steps=2714 --torch_compile=True 2>&1 | tee -a /logs/fastmri_pytorch_07-26-2023-07-38-11.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-26 07:38:21.435018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-26 07:38:21.435027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0726 07:38:35.874413 139680448735040 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0726 07:38:35.874442 139758057981760 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0726 07:38:35.874459 140129492617024 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0726 07:38:35.875531 140698602706752 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0726 07:38:35.875708 140380607911744 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0726 07:38:36.851486 139904039765824 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0726 07:38:36.851480 139813689005888 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0726 07:38:36.857182 140312776001344 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0726 07:38:36.857471 140312776001344 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.862055 139904039765824 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.862102 139813689005888 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.866403 139680448735040 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.866437 139758057981760 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.866496 140129492617024 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.866499 140698602706752 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:36.866574 140380607911744 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0726 07:38:37.200637 140312776001344 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/fastmri_pytorch.
W0726 07:38:37.241565 139758057981760 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.241561 139680448735040 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.241575 140312776001344 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.241599 140698602706752 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.243169 140380607911744 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.243589 139904039765824 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.243973 139813689005888 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0726 07:38:37.244006 140129492617024 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0726 07:38:37.249443 140312776001344 submission_runner.py:490] Using RNG seed 2849044300
I0726 07:38:37.251787 140312776001344 submission_runner.py:499] --- Tuning run 1/1 ---
I0726 07:38:37.252052 140312776001344 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/fastmri_pytorch/trial_1.
I0726 07:38:37.252633 140312776001344 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/fastmri_pytorch/trial_1/hparams.json.
I0726 07:38:37.253881 140312776001344 submission_runner.py:176] Initializing dataset.
I0726 07:38:37.254098 140312776001344 submission_runner.py:183] Initializing model.
W0726 07:38:41.813827 139680448735040 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.813827 140380607911744 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.813844 139904039765824 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.813833 140698602706752 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.813843 139758057981760 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.813848 139813689005888 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.813970 140129492617024 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0726 07:38:41.814021 140312776001344 submission_runner.py:200] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0726 07:38:41.814264 140312776001344 submission_runner.py:217] Initializing optimizer.
I0726 07:38:41.815115 140312776001344 submission_runner.py:224] Initializing metrics bundle.
I0726 07:38:41.815209 140312776001344 submission_runner.py:242] Initializing checkpoint and logger.
I0726 07:38:41.816210 140312776001344 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0726 07:38:41.816318 140312776001344 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0726 07:38:42.280422 140312776001344 submission_runner.py:263] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0726 07:38:42.282738 140312776001344 submission_runner.py:266] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/fastmri_pytorch/trial_1/flags_0.json.
I0726 07:38:42.379692 140312776001344 submission_runner.py:276] Starting training loop.
I0726 07:39:34.832393 140271165626112 logging_writer.py:48] [0] global_step=0, grad_norm=3.210877, loss=0.936457
I0726 07:39:34.846432 140312776001344 submission.py:119] 0) loss = 0.936, grad_norm = 3.211
I0726 07:39:34.848091 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:41:07.375077 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:42:08.605275 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:43:09.783480 140312776001344 submission_runner.py:364] Time since start: 267.40s, 	Step: 1, 	{'train/ssim': 0.2609733854021345, 'train/loss': 0.9489507675170898, 'validation/ssim': 0.25269056158114095, 'validation/loss': 0.958190196675401, 'validation/num_examples': 3554, 'test/ssim': 0.27520883193198475, 'test/loss': 0.9587258490994136, 'test/num_examples': 3581, 'score': 52.46823334693909, 'total_duration': 267.404408454895, 'accumulated_submission_time': 52.46823334693909, 'accumulated_eval_time': 214.93551349639893, 'accumulated_logging_time': 0}
I0726 07:43:09.807423 140255084660480 logging_writer.py:48] [1] accumulated_eval_time=214.935513, accumulated_logging_time=0, accumulated_submission_time=52.468233, global_step=1, preemption_count=0, score=52.468233, test/loss=0.958726, test/num_examples=3581, test/ssim=0.275209, total_duration=267.404408, train/loss=0.948951, train/ssim=0.260973, validation/loss=0.958190, validation/num_examples=3554, validation/ssim=0.252691
I0726 07:43:09.920140 140698602706752 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920167 140380607911744 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920167 139680448735040 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920165 139904039765824 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920336 140312776001344 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920295 139813689005888 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920307 139758057981760 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:09.920337 140129492617024 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0726 07:43:10.100410 140255076267776 logging_writer.py:48] [1] global_step=1, grad_norm=3.354265, loss=0.897666
I0726 07:43:10.104591 140312776001344 submission.py:119] 1) loss = 0.898, grad_norm = 3.354
I0726 07:43:10.174727 140255084660480 logging_writer.py:48] [2] global_step=2, grad_norm=3.034237, loss=0.946120
I0726 07:43:10.178556 140312776001344 submission.py:119] 2) loss = 0.946, grad_norm = 3.034
I0726 07:43:10.252642 140255076267776 logging_writer.py:48] [3] global_step=3, grad_norm=3.460410, loss=0.910307
I0726 07:43:10.259500 140312776001344 submission.py:119] 3) loss = 0.910, grad_norm = 3.460
I0726 07:43:10.343203 140255084660480 logging_writer.py:48] [4] global_step=4, grad_norm=3.389930, loss=0.906197
I0726 07:43:10.348588 140312776001344 submission.py:119] 4) loss = 0.906, grad_norm = 3.390
I0726 07:43:10.427693 140255076267776 logging_writer.py:48] [5] global_step=5, grad_norm=3.004043, loss=0.909235
I0726 07:43:10.432491 140312776001344 submission.py:119] 5) loss = 0.909, grad_norm = 3.004
I0726 07:43:10.512510 140255084660480 logging_writer.py:48] [6] global_step=6, grad_norm=2.967687, loss=0.972982
I0726 07:43:10.518016 140312776001344 submission.py:119] 6) loss = 0.973, grad_norm = 2.968
I0726 07:43:10.594680 140255076267776 logging_writer.py:48] [7] global_step=7, grad_norm=3.219552, loss=0.928891
I0726 07:43:10.598680 140312776001344 submission.py:119] 7) loss = 0.929, grad_norm = 3.220
I0726 07:43:10.673802 140255084660480 logging_writer.py:48] [8] global_step=8, grad_norm=3.252723, loss=0.919824
I0726 07:43:10.680005 140312776001344 submission.py:119] 8) loss = 0.920, grad_norm = 3.253
I0726 07:43:10.753710 140255076267776 logging_writer.py:48] [9] global_step=9, grad_norm=3.269434, loss=0.904831
I0726 07:43:10.759752 140312776001344 submission.py:119] 9) loss = 0.905, grad_norm = 3.269
I0726 07:43:10.834533 140255084660480 logging_writer.py:48] [10] global_step=10, grad_norm=2.807770, loss=0.917089
I0726 07:43:10.839858 140312776001344 submission.py:119] 10) loss = 0.917, grad_norm = 2.808
I0726 07:43:10.915737 140255076267776 logging_writer.py:48] [11] global_step=11, grad_norm=2.999932, loss=0.941843
I0726 07:43:10.921350 140312776001344 submission.py:119] 11) loss = 0.942, grad_norm = 3.000
I0726 07:43:10.993969 140255084660480 logging_writer.py:48] [12] global_step=12, grad_norm=3.216309, loss=0.870567
I0726 07:43:11.000067 140312776001344 submission.py:119] 12) loss = 0.871, grad_norm = 3.216
I0726 07:43:11.079910 140255076267776 logging_writer.py:48] [13] global_step=13, grad_norm=3.323327, loss=0.865906
I0726 07:43:11.085416 140312776001344 submission.py:119] 13) loss = 0.866, grad_norm = 3.323
I0726 07:43:11.156165 140255084660480 logging_writer.py:48] [14] global_step=14, grad_norm=3.017853, loss=0.850667
I0726 07:43:11.162361 140312776001344 submission.py:119] 14) loss = 0.851, grad_norm = 3.018
I0726 07:43:11.364531 140255076267776 logging_writer.py:48] [15] global_step=15, grad_norm=3.047967, loss=0.865061
I0726 07:43:11.368310 140312776001344 submission.py:119] 15) loss = 0.865, grad_norm = 3.048
I0726 07:43:11.618368 140255084660480 logging_writer.py:48] [16] global_step=16, grad_norm=2.748650, loss=0.815278
I0726 07:43:11.622199 140312776001344 submission.py:119] 16) loss = 0.815, grad_norm = 2.749
I0726 07:43:11.870863 140255076267776 logging_writer.py:48] [17] global_step=17, grad_norm=2.981287, loss=0.845263
I0726 07:43:11.876635 140312776001344 submission.py:119] 17) loss = 0.845, grad_norm = 2.981
I0726 07:43:12.136710 140255084660480 logging_writer.py:48] [18] global_step=18, grad_norm=2.658365, loss=0.805464
I0726 07:43:12.142817 140312776001344 submission.py:119] 18) loss = 0.805, grad_norm = 2.658
I0726 07:43:12.413543 140255076267776 logging_writer.py:48] [19] global_step=19, grad_norm=2.723203, loss=0.849073
I0726 07:43:12.419596 140312776001344 submission.py:119] 19) loss = 0.849, grad_norm = 2.723
I0726 07:43:12.712339 140255084660480 logging_writer.py:48] [20] global_step=20, grad_norm=2.563492, loss=0.798684
I0726 07:43:12.717707 140312776001344 submission.py:119] 20) loss = 0.799, grad_norm = 2.563
I0726 07:43:12.989802 140255076267776 logging_writer.py:48] [21] global_step=21, grad_norm=2.487808, loss=0.827602
I0726 07:43:12.997904 140312776001344 submission.py:119] 21) loss = 0.828, grad_norm = 2.488
I0726 07:43:13.288763 140255084660480 logging_writer.py:48] [22] global_step=22, grad_norm=2.471664, loss=0.773618
I0726 07:43:13.293937 140312776001344 submission.py:119] 22) loss = 0.774, grad_norm = 2.472
I0726 07:43:13.546427 140255076267776 logging_writer.py:48] [23] global_step=23, grad_norm=2.573598, loss=0.737316
I0726 07:43:13.551698 140312776001344 submission.py:119] 23) loss = 0.737, grad_norm = 2.574
I0726 07:43:13.780617 140255084660480 logging_writer.py:48] [24] global_step=24, grad_norm=2.346416, loss=0.726624
I0726 07:43:13.788628 140312776001344 submission.py:119] 24) loss = 0.727, grad_norm = 2.346
I0726 07:43:14.019930 140255076267776 logging_writer.py:48] [25] global_step=25, grad_norm=2.218334, loss=0.771742
I0726 07:43:14.027902 140312776001344 submission.py:119] 25) loss = 0.772, grad_norm = 2.218
I0726 07:43:14.337318 140255084660480 logging_writer.py:48] [26] global_step=26, grad_norm=2.307311, loss=0.727582
I0726 07:43:14.343359 140312776001344 submission.py:119] 26) loss = 0.728, grad_norm = 2.307
I0726 07:43:14.609726 140255076267776 logging_writer.py:48] [27] global_step=27, grad_norm=2.266237, loss=0.689874
I0726 07:43:14.616878 140312776001344 submission.py:119] 27) loss = 0.690, grad_norm = 2.266
I0726 07:43:14.872565 140255084660480 logging_writer.py:48] [28] global_step=28, grad_norm=2.138625, loss=0.658008
I0726 07:43:14.876131 140312776001344 submission.py:119] 28) loss = 0.658, grad_norm = 2.139
I0726 07:43:15.161784 140255076267776 logging_writer.py:48] [29] global_step=29, grad_norm=1.951136, loss=0.691440
I0726 07:43:15.166746 140312776001344 submission.py:119] 29) loss = 0.691, grad_norm = 1.951
I0726 07:43:15.423673 140255084660480 logging_writer.py:48] [30] global_step=30, grad_norm=1.899294, loss=0.678104
I0726 07:43:15.427882 140312776001344 submission.py:119] 30) loss = 0.678, grad_norm = 1.899
I0726 07:43:15.710970 140255076267776 logging_writer.py:48] [31] global_step=31, grad_norm=1.809683, loss=0.658656
I0726 07:43:15.714778 140312776001344 submission.py:119] 31) loss = 0.659, grad_norm = 1.810
I0726 07:43:15.934866 140255084660480 logging_writer.py:48] [32] global_step=32, grad_norm=1.796060, loss=0.645031
I0726 07:43:15.938272 140312776001344 submission.py:119] 32) loss = 0.645, grad_norm = 1.796
I0726 07:43:16.254141 140255076267776 logging_writer.py:48] [33] global_step=33, grad_norm=1.660902, loss=0.672149
I0726 07:43:16.257946 140312776001344 submission.py:119] 33) loss = 0.672, grad_norm = 1.661
I0726 07:43:16.512624 140255084660480 logging_writer.py:48] [34] global_step=34, grad_norm=1.583791, loss=0.721287
I0726 07:43:16.518666 140312776001344 submission.py:119] 34) loss = 0.721, grad_norm = 1.584
I0726 07:43:16.785233 140255076267776 logging_writer.py:48] [35] global_step=35, grad_norm=1.639798, loss=0.617525
I0726 07:43:16.791835 140312776001344 submission.py:119] 35) loss = 0.618, grad_norm = 1.640
I0726 07:43:17.005448 140255084660480 logging_writer.py:48] [36] global_step=36, grad_norm=1.494481, loss=0.662010
I0726 07:43:17.011743 140312776001344 submission.py:119] 36) loss = 0.662, grad_norm = 1.494
I0726 07:43:17.291918 140255076267776 logging_writer.py:48] [37] global_step=37, grad_norm=1.464852, loss=0.599980
I0726 07:43:17.298145 140312776001344 submission.py:119] 37) loss = 0.600, grad_norm = 1.465
I0726 07:43:17.576746 140255084660480 logging_writer.py:48] [38] global_step=38, grad_norm=1.480635, loss=0.637522
I0726 07:43:17.583020 140312776001344 submission.py:119] 38) loss = 0.638, grad_norm = 1.481
I0726 07:43:17.826939 140255076267776 logging_writer.py:48] [39] global_step=39, grad_norm=1.446611, loss=0.542961
I0726 07:43:17.833416 140312776001344 submission.py:119] 39) loss = 0.543, grad_norm = 1.447
I0726 07:43:18.091285 140255084660480 logging_writer.py:48] [40] global_step=40, grad_norm=1.413270, loss=0.629542
I0726 07:43:18.097900 140312776001344 submission.py:119] 40) loss = 0.630, grad_norm = 1.413
I0726 07:43:18.362619 140255076267776 logging_writer.py:48] [41] global_step=41, grad_norm=1.409547, loss=0.579472
I0726 07:43:18.368274 140312776001344 submission.py:119] 41) loss = 0.579, grad_norm = 1.410
I0726 07:43:18.641676 140255084660480 logging_writer.py:48] [42] global_step=42, grad_norm=1.349891, loss=0.616955
I0726 07:43:18.647136 140312776001344 submission.py:119] 42) loss = 0.617, grad_norm = 1.350
I0726 07:43:18.959387 140255076267776 logging_writer.py:48] [43] global_step=43, grad_norm=1.383981, loss=0.589103
I0726 07:43:18.964889 140312776001344 submission.py:119] 43) loss = 0.589, grad_norm = 1.384
I0726 07:43:19.219176 140255084660480 logging_writer.py:48] [44] global_step=44, grad_norm=1.323670, loss=0.597567
I0726 07:43:19.222774 140312776001344 submission.py:119] 44) loss = 0.598, grad_norm = 1.324
I0726 07:43:19.501914 140255076267776 logging_writer.py:48] [45] global_step=45, grad_norm=1.327420, loss=0.588383
I0726 07:43:19.505453 140312776001344 submission.py:119] 45) loss = 0.588, grad_norm = 1.327
I0726 07:43:19.763360 140255084660480 logging_writer.py:48] [46] global_step=46, grad_norm=1.286327, loss=0.584928
I0726 07:43:19.767228 140312776001344 submission.py:119] 46) loss = 0.585, grad_norm = 1.286
I0726 07:43:20.016285 140255076267776 logging_writer.py:48] [47] global_step=47, grad_norm=1.292080, loss=0.620898
I0726 07:43:20.019918 140312776001344 submission.py:119] 47) loss = 0.621, grad_norm = 1.292
I0726 07:43:20.276673 140255084660480 logging_writer.py:48] [48] global_step=48, grad_norm=1.252793, loss=0.536716
I0726 07:43:20.280440 140312776001344 submission.py:119] 48) loss = 0.537, grad_norm = 1.253
I0726 07:43:20.569774 140255076267776 logging_writer.py:48] [49] global_step=49, grad_norm=1.219736, loss=0.558782
I0726 07:43:20.573962 140312776001344 submission.py:119] 49) loss = 0.559, grad_norm = 1.220
I0726 07:43:20.793678 140255084660480 logging_writer.py:48] [50] global_step=50, grad_norm=1.238920, loss=0.529163
I0726 07:43:20.799155 140312776001344 submission.py:119] 50) loss = 0.529, grad_norm = 1.239
I0726 07:43:21.094745 140255076267776 logging_writer.py:48] [51] global_step=51, grad_norm=1.181185, loss=0.596807
I0726 07:43:21.099706 140312776001344 submission.py:119] 51) loss = 0.597, grad_norm = 1.181
I0726 07:43:21.430114 140255084660480 logging_writer.py:48] [52] global_step=52, grad_norm=1.221815, loss=0.451591
I0726 07:43:21.435210 140312776001344 submission.py:119] 52) loss = 0.452, grad_norm = 1.222
I0726 07:43:21.725678 140255076267776 logging_writer.py:48] [53] global_step=53, grad_norm=1.168490, loss=0.524115
I0726 07:43:21.729274 140312776001344 submission.py:119] 53) loss = 0.524, grad_norm = 1.168
I0726 07:43:21.927745 140255084660480 logging_writer.py:48] [54] global_step=54, grad_norm=1.099782, loss=0.577303
I0726 07:43:21.935146 140312776001344 submission.py:119] 54) loss = 0.577, grad_norm = 1.100
I0726 07:43:22.203618 140255076267776 logging_writer.py:48] [55] global_step=55, grad_norm=1.095141, loss=0.537607
I0726 07:43:22.208908 140312776001344 submission.py:119] 55) loss = 0.538, grad_norm = 1.095
I0726 07:43:22.433630 140255084660480 logging_writer.py:48] [56] global_step=56, grad_norm=1.063768, loss=0.574041
I0726 07:43:22.439283 140312776001344 submission.py:119] 56) loss = 0.574, grad_norm = 1.064
I0726 07:43:22.683985 140255076267776 logging_writer.py:48] [57] global_step=57, grad_norm=1.053747, loss=0.503421
I0726 07:43:22.689366 140312776001344 submission.py:119] 57) loss = 0.503, grad_norm = 1.054
I0726 07:43:23.007886 140255084660480 logging_writer.py:48] [58] global_step=58, grad_norm=1.042796, loss=0.461370
I0726 07:43:23.014698 140312776001344 submission.py:119] 58) loss = 0.461, grad_norm = 1.043
I0726 07:43:23.258239 140255076267776 logging_writer.py:48] [59] global_step=59, grad_norm=1.077848, loss=0.478985
I0726 07:43:23.263246 140312776001344 submission.py:119] 59) loss = 0.479, grad_norm = 1.078
I0726 07:43:23.502547 140255084660480 logging_writer.py:48] [60] global_step=60, grad_norm=1.023454, loss=0.531115
I0726 07:43:23.508115 140312776001344 submission.py:119] 60) loss = 0.531, grad_norm = 1.023
I0726 07:43:23.795082 140255076267776 logging_writer.py:48] [61] global_step=61, grad_norm=0.971714, loss=0.478285
I0726 07:43:23.800261 140312776001344 submission.py:119] 61) loss = 0.478, grad_norm = 0.972
I0726 07:43:24.093913 140255084660480 logging_writer.py:48] [62] global_step=62, grad_norm=0.920595, loss=0.533144
I0726 07:43:24.100229 140312776001344 submission.py:119] 62) loss = 0.533, grad_norm = 0.921
I0726 07:43:24.326216 140255076267776 logging_writer.py:48] [63] global_step=63, grad_norm=0.955445, loss=0.448518
I0726 07:43:24.329907 140312776001344 submission.py:119] 63) loss = 0.449, grad_norm = 0.955
I0726 07:43:24.597929 140255084660480 logging_writer.py:48] [64] global_step=64, grad_norm=0.950673, loss=0.412362
I0726 07:43:24.601695 140312776001344 submission.py:119] 64) loss = 0.412, grad_norm = 0.951
I0726 07:43:24.916162 140255076267776 logging_writer.py:48] [65] global_step=65, grad_norm=0.906636, loss=0.489669
I0726 07:43:24.919976 140312776001344 submission.py:119] 65) loss = 0.490, grad_norm = 0.907
I0726 07:43:25.159147 140255084660480 logging_writer.py:48] [66] global_step=66, grad_norm=0.862942, loss=0.485015
I0726 07:43:25.164858 140312776001344 submission.py:119] 66) loss = 0.485, grad_norm = 0.863
I0726 07:43:25.415006 140255076267776 logging_writer.py:48] [67] global_step=67, grad_norm=0.896881, loss=0.411925
I0726 07:43:25.420801 140312776001344 submission.py:119] 67) loss = 0.412, grad_norm = 0.897
I0726 07:43:25.690867 140255084660480 logging_writer.py:48] [68] global_step=68, grad_norm=0.889861, loss=0.455486
I0726 07:43:25.696773 140312776001344 submission.py:119] 68) loss = 0.455, grad_norm = 0.890
I0726 07:43:25.989810 140255076267776 logging_writer.py:48] [69] global_step=69, grad_norm=0.793682, loss=0.427053
I0726 07:43:25.996055 140312776001344 submission.py:119] 69) loss = 0.427, grad_norm = 0.794
I0726 07:43:26.273772 140255084660480 logging_writer.py:48] [70] global_step=70, grad_norm=0.802918, loss=0.417600
I0726 07:43:26.279979 140312776001344 submission.py:119] 70) loss = 0.418, grad_norm = 0.803
I0726 07:43:26.494426 140255076267776 logging_writer.py:48] [71] global_step=71, grad_norm=0.773816, loss=0.498581
I0726 07:43:26.497966 140312776001344 submission.py:119] 71) loss = 0.499, grad_norm = 0.774
I0726 07:43:26.829803 140255084660480 logging_writer.py:48] [72] global_step=72, grad_norm=0.763483, loss=0.451909
I0726 07:43:26.833406 140312776001344 submission.py:119] 72) loss = 0.452, grad_norm = 0.763
I0726 07:43:27.060569 140255076267776 logging_writer.py:48] [73] global_step=73, grad_norm=0.738000, loss=0.359006
I0726 07:43:27.065657 140312776001344 submission.py:119] 73) loss = 0.359, grad_norm = 0.738
I0726 07:43:27.329593 140255084660480 logging_writer.py:48] [74] global_step=74, grad_norm=0.770512, loss=0.351031
I0726 07:43:27.337576 140312776001344 submission.py:119] 74) loss = 0.351, grad_norm = 0.771
I0726 07:43:27.600718 140255076267776 logging_writer.py:48] [75] global_step=75, grad_norm=0.720363, loss=0.407799
I0726 07:43:27.606998 140312776001344 submission.py:119] 75) loss = 0.408, grad_norm = 0.720
I0726 07:43:27.911037 140255084660480 logging_writer.py:48] [76] global_step=76, grad_norm=0.669565, loss=0.406894
I0726 07:43:27.916383 140312776001344 submission.py:119] 76) loss = 0.407, grad_norm = 0.670
I0726 07:43:28.181674 140255076267776 logging_writer.py:48] [77] global_step=77, grad_norm=0.670350, loss=0.364262
I0726 07:43:28.186572 140312776001344 submission.py:119] 77) loss = 0.364, grad_norm = 0.670
I0726 07:43:28.445732 140255084660480 logging_writer.py:48] [78] global_step=78, grad_norm=0.615095, loss=0.351848
I0726 07:43:28.452080 140312776001344 submission.py:119] 78) loss = 0.352, grad_norm = 0.615
I0726 07:43:28.767310 140255076267776 logging_writer.py:48] [79] global_step=79, grad_norm=0.565992, loss=0.334708
I0726 07:43:28.773505 140312776001344 submission.py:119] 79) loss = 0.335, grad_norm = 0.566
I0726 07:43:29.088002 140255084660480 logging_writer.py:48] [80] global_step=80, grad_norm=0.585352, loss=0.299704
I0726 07:43:29.093050 140312776001344 submission.py:119] 80) loss = 0.300, grad_norm = 0.585
I0726 07:43:29.294353 140255076267776 logging_writer.py:48] [81] global_step=81, grad_norm=0.588145, loss=0.376701
I0726 07:43:29.297975 140312776001344 submission.py:119] 81) loss = 0.377, grad_norm = 0.588
I0726 07:43:29.568165 140255084660480 logging_writer.py:48] [82] global_step=82, grad_norm=0.555778, loss=0.282920
I0726 07:43:29.573055 140312776001344 submission.py:119] 82) loss = 0.283, grad_norm = 0.556
I0726 07:43:29.821146 140255076267776 logging_writer.py:48] [83] global_step=83, grad_norm=0.565119, loss=0.426687
I0726 07:43:29.826851 140312776001344 submission.py:119] 83) loss = 0.427, grad_norm = 0.565
I0726 07:43:30.119260 140255084660480 logging_writer.py:48] [84] global_step=84, grad_norm=0.509297, loss=0.312507
I0726 07:43:30.124897 140312776001344 submission.py:119] 84) loss = 0.313, grad_norm = 0.509
I0726 07:43:30.361711 140255076267776 logging_writer.py:48] [85] global_step=85, grad_norm=0.491511, loss=0.427451
I0726 07:43:30.366449 140312776001344 submission.py:119] 85) loss = 0.427, grad_norm = 0.492
I0726 07:43:30.640941 140255084660480 logging_writer.py:48] [86] global_step=86, grad_norm=0.477419, loss=0.352650
I0726 07:43:30.646726 140312776001344 submission.py:119] 86) loss = 0.353, grad_norm = 0.477
I0726 07:43:30.929267 140255076267776 logging_writer.py:48] [87] global_step=87, grad_norm=0.448022, loss=0.337173
I0726 07:43:30.935493 140312776001344 submission.py:119] 87) loss = 0.337, grad_norm = 0.448
I0726 07:43:31.255330 140255084660480 logging_writer.py:48] [88] global_step=88, grad_norm=0.427747, loss=0.346431
I0726 07:43:31.262191 140312776001344 submission.py:119] 88) loss = 0.346, grad_norm = 0.428
I0726 07:43:31.505940 140255076267776 logging_writer.py:48] [89] global_step=89, grad_norm=0.400781, loss=0.297341
I0726 07:43:31.509457 140312776001344 submission.py:119] 89) loss = 0.297, grad_norm = 0.401
I0726 07:43:31.757549 140255084660480 logging_writer.py:48] [90] global_step=90, grad_norm=0.372158, loss=0.310284
I0726 07:43:31.766811 140312776001344 submission.py:119] 90) loss = 0.310, grad_norm = 0.372
I0726 07:43:32.039350 140255076267776 logging_writer.py:48] [91] global_step=91, grad_norm=0.442709, loss=0.415061
I0726 07:43:32.045504 140312776001344 submission.py:119] 91) loss = 0.415, grad_norm = 0.443
I0726 07:43:32.292903 140255084660480 logging_writer.py:48] [92] global_step=92, grad_norm=0.371536, loss=0.340196
I0726 07:43:32.298940 140312776001344 submission.py:119] 92) loss = 0.340, grad_norm = 0.372
I0726 07:43:32.578190 140255076267776 logging_writer.py:48] [93] global_step=93, grad_norm=0.398568, loss=0.283740
I0726 07:43:32.583030 140312776001344 submission.py:119] 93) loss = 0.284, grad_norm = 0.399
I0726 07:43:32.882467 140255084660480 logging_writer.py:48] [94] global_step=94, grad_norm=0.320664, loss=0.260704
I0726 07:43:32.885876 140312776001344 submission.py:119] 94) loss = 0.261, grad_norm = 0.321
I0726 07:43:33.103136 140255076267776 logging_writer.py:48] [95] global_step=95, grad_norm=0.301293, loss=0.269666
I0726 07:43:33.106829 140312776001344 submission.py:119] 95) loss = 0.270, grad_norm = 0.301
I0726 07:43:33.414509 140255084660480 logging_writer.py:48] [96] global_step=96, grad_norm=0.307414, loss=0.302065
I0726 07:43:33.418615 140312776001344 submission.py:119] 96) loss = 0.302, grad_norm = 0.307
I0726 07:43:33.645421 140255076267776 logging_writer.py:48] [97] global_step=97, grad_norm=0.263674, loss=0.280130
I0726 07:43:33.650680 140312776001344 submission.py:119] 97) loss = 0.280, grad_norm = 0.264
I0726 07:43:33.942492 140255084660480 logging_writer.py:48] [98] global_step=98, grad_norm=0.299563, loss=0.383782
I0726 07:43:33.948648 140312776001344 submission.py:119] 98) loss = 0.384, grad_norm = 0.300
I0726 07:43:34.210597 140255076267776 logging_writer.py:48] [99] global_step=99, grad_norm=0.246208, loss=0.284311
I0726 07:43:34.216145 140312776001344 submission.py:119] 99) loss = 0.284, grad_norm = 0.246
I0726 07:43:34.565281 140255084660480 logging_writer.py:48] [100] global_step=100, grad_norm=0.256755, loss=0.287509
I0726 07:43:34.573059 140312776001344 submission.py:119] 100) loss = 0.288, grad_norm = 0.257
I0726 07:44:29.988135 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:44:32.169178 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:44:34.419905 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:44:36.661862 140312776001344 submission_runner.py:364] Time since start: 354.28s, 	Step: 305, 	{'train/ssim': 0.7040531294686454, 'train/loss': 0.30308846064976286, 'validation/ssim': 0.682002439482801, 'validation/loss': 0.32520894837902714, 'validation/num_examples': 3554, 'test/ssim': 0.700410409867181, 'test/loss': 0.3270385026332554, 'test/num_examples': 3581, 'score': 132.3388864994049, 'total_duration': 354.2827491760254, 'accumulated_submission_time': 132.3388864994049, 'accumulated_eval_time': 221.60978031158447, 'accumulated_logging_time': 0.11320376396179199}
I0726 07:44:36.686030 140255076267776 logging_writer.py:48] [305] accumulated_eval_time=221.609780, accumulated_logging_time=0.113204, accumulated_submission_time=132.338886, global_step=305, preemption_count=0, score=132.338886, test/loss=0.327039, test/num_examples=3581, test/ssim=0.700410, total_duration=354.282749, train/loss=0.303088, train/ssim=0.704053, validation/loss=0.325209, validation/num_examples=3554, validation/ssim=0.682002
I0726 07:45:45.956769 140255084660480 logging_writer.py:48] [500] global_step=500, grad_norm=0.517026, loss=0.275588
I0726 07:45:45.962641 140312776001344 submission.py:119] 500) loss = 0.276, grad_norm = 0.517
I0726 07:45:57.022207 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:45:59.170676 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:46:01.401468 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:46:03.580911 140312776001344 submission_runner.py:364] Time since start: 441.20s, 	Step: 531, 	{'train/ssim': 0.7178893089294434, 'train/loss': 0.2933213710784912, 'validation/ssim': 0.6965254406346723, 'validation/loss': 0.3150418377620287, 'validation/num_examples': 3554, 'test/ssim': 0.7137800577265428, 'test/loss': 0.3175052577841385, 'test/num_examples': 3581, 'score': 212.30554151535034, 'total_duration': 441.20182728767395, 'accumulated_submission_time': 212.30554151535034, 'accumulated_eval_time': 228.16843557357788, 'accumulated_logging_time': 0.3292527198791504}
I0726 07:46:03.598287 140255076267776 logging_writer.py:48] [531] accumulated_eval_time=228.168436, accumulated_logging_time=0.329253, accumulated_submission_time=212.305542, global_step=531, preemption_count=0, score=212.305542, test/loss=0.317505, test/num_examples=3581, test/ssim=0.713780, total_duration=441.201827, train/loss=0.293321, train/ssim=0.717889, validation/loss=0.315042, validation/num_examples=3554, validation/ssim=0.696525
I0726 07:47:23.726876 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:47:25.862117 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:47:28.084034 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:47:30.241686 140312776001344 submission_runner.py:364] Time since start: 527.86s, 	Step: 755, 	{'train/ssim': 0.7255093029567173, 'train/loss': 0.2862475599561419, 'validation/ssim': 0.7040063517779263, 'validation/loss': 0.3077185807364941, 'validation/num_examples': 3554, 'test/ssim': 0.7211897700101229, 'test/loss': 0.30994623861177045, 'test/num_examples': 3581, 'score': 292.06615948677063, 'total_duration': 527.8620045185089, 'accumulated_submission_time': 292.06615948677063, 'accumulated_eval_time': 234.68275451660156, 'accumulated_logging_time': 0.5401449203491211}
I0726 07:47:30.266366 140255084660480 logging_writer.py:48] [755] accumulated_eval_time=234.682755, accumulated_logging_time=0.540145, accumulated_submission_time=292.066159, global_step=755, preemption_count=0, score=292.066159, test/loss=0.309946, test/num_examples=3581, test/ssim=0.721190, total_duration=527.862005, train/loss=0.286248, train/ssim=0.725509, validation/loss=0.307719, validation/num_examples=3554, validation/ssim=0.704006
I0726 07:48:50.450767 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:48:52.673032 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:48:54.943295 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:48:57.223225 140312776001344 submission_runner.py:364] Time since start: 614.84s, 	Step: 984, 	{'train/ssim': 0.7264759199959892, 'train/loss': 0.28200505461011616, 'validation/ssim': 0.7050920700047482, 'validation/loss': 0.3033274842167276, 'validation/num_examples': 3554, 'test/ssim': 0.7224768090268081, 'test/loss': 0.30537683422490225, 'test/num_examples': 3581, 'score': 371.88724637031555, 'total_duration': 614.8441746234894, 'accumulated_submission_time': 371.88724637031555, 'accumulated_eval_time': 241.4559919834137, 'accumulated_logging_time': 0.7450041770935059}
I0726 07:48:57.241785 140255076267776 logging_writer.py:48] [984] accumulated_eval_time=241.455992, accumulated_logging_time=0.745004, accumulated_submission_time=371.887246, global_step=984, preemption_count=0, score=371.887246, test/loss=0.305377, test/num_examples=3581, test/ssim=0.722477, total_duration=614.844175, train/loss=0.282005, train/ssim=0.726476, validation/loss=0.303327, validation/num_examples=3554, validation/ssim=0.705092
I0726 07:48:59.386638 140255084660480 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.179854, loss=0.221581
I0726 07:48:59.391277 140312776001344 submission.py:119] 1000) loss = 0.222, grad_norm = 0.180
I0726 07:50:17.273956 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:50:19.350793 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:50:21.466049 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:50:23.529483 140312776001344 submission_runner.py:364] Time since start: 701.15s, 	Step: 1293, 	{'train/ssim': 0.7273302759443011, 'train/loss': 0.2792449678693499, 'validation/ssim': 0.706587620243036, 'validation/loss': 0.30003440225801914, 'validation/num_examples': 3554, 'test/ssim': 0.7236674462440659, 'test/loss': 0.3020779019259634, 'test/num_examples': 3581, 'score': 451.5780141353607, 'total_duration': 701.1504554748535, 'accumulated_submission_time': 451.5780141353607, 'accumulated_eval_time': 247.71161150932312, 'accumulated_logging_time': 0.9604082107543945}
I0726 07:50:23.545376 140255076267776 logging_writer.py:48] [1293] accumulated_eval_time=247.711612, accumulated_logging_time=0.960408, accumulated_submission_time=451.578014, global_step=1293, preemption_count=0, score=451.578014, test/loss=0.302078, test/num_examples=3581, test/ssim=0.723667, total_duration=701.150455, train/loss=0.279245, train/ssim=0.727330, validation/loss=0.300034, validation/num_examples=3554, validation/ssim=0.706588
I0726 07:51:16.467109 140255084660480 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.221606, loss=0.251832
I0726 07:51:16.471260 140312776001344 submission.py:119] 1500) loss = 0.252, grad_norm = 0.222
I0726 07:51:43.643012 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:51:45.754248 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:51:47.860424 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:51:49.921274 140312776001344 submission_runner.py:364] Time since start: 787.54s, 	Step: 1604, 	{'train/ssim': 0.7317947660173688, 'train/loss': 0.27880862780979704, 'validation/ssim': 0.710348237681134, 'validation/loss': 0.29935236787554514, 'validation/num_examples': 3554, 'test/ssim': 0.7273431909513753, 'test/loss': 0.30151363778666923, 'test/num_examples': 3581, 'score': 531.3330700397491, 'total_duration': 787.5422267913818, 'accumulated_submission_time': 531.3330700397491, 'accumulated_eval_time': 253.9899458885193, 'accumulated_logging_time': 1.177818775177002}
I0726 07:51:49.937033 140255076267776 logging_writer.py:48] [1604] accumulated_eval_time=253.989946, accumulated_logging_time=1.177819, accumulated_submission_time=531.333070, global_step=1604, preemption_count=0, score=531.333070, test/loss=0.301514, test/num_examples=3581, test/ssim=0.727343, total_duration=787.542227, train/loss=0.278809, train/ssim=0.731795, validation/loss=0.299352, validation/num_examples=3554, validation/ssim=0.710348
I0726 07:53:10.075746 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:53:12.173848 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:53:14.302851 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:53:16.370122 140312776001344 submission_runner.py:364] Time since start: 873.99s, 	Step: 1912, 	{'train/ssim': 0.7362490381513324, 'train/loss': 0.27486489500318256, 'validation/ssim': 0.7143311508599466, 'validation/loss': 0.29613457520355585, 'validation/num_examples': 3554, 'test/ssim': 0.7315390554096272, 'test/loss': 0.29796729238297615, 'test/num_examples': 3581, 'score': 611.1320860385895, 'total_duration': 873.9910571575165, 'accumulated_submission_time': 611.1320860385895, 'accumulated_eval_time': 260.28436279296875, 'accumulated_logging_time': 1.3948426246643066}
I0726 07:53:16.386507 140255084660480 logging_writer.py:48] [1912] accumulated_eval_time=260.284363, accumulated_logging_time=1.394843, accumulated_submission_time=611.132086, global_step=1912, preemption_count=0, score=611.132086, test/loss=0.297967, test/num_examples=3581, test/ssim=0.731539, total_duration=873.991057, train/loss=0.274865, train/ssim=0.736249, validation/loss=0.296135, validation/num_examples=3554, validation/ssim=0.714331
I0726 07:53:38.047794 140255076267776 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.119761, loss=0.276447
I0726 07:53:38.051393 140312776001344 submission.py:119] 2000) loss = 0.276, grad_norm = 0.120
I0726 07:54:36.437565 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:54:38.527238 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:54:40.661271 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:54:42.724565 140312776001344 submission_runner.py:364] Time since start: 960.35s, 	Step: 2218, 	{'train/ssim': 0.7369805744716099, 'train/loss': 0.27353179454803467, 'validation/ssim': 0.7142517398969471, 'validation/loss': 0.29481536401545794, 'validation/num_examples': 3554, 'test/ssim': 0.7314184508953505, 'test/loss': 0.29671437578539517, 'test/num_examples': 3581, 'score': 690.8473498821259, 'total_duration': 960.3455398082733, 'accumulated_submission_time': 690.8473498821259, 'accumulated_eval_time': 266.5713903903961, 'accumulated_logging_time': 1.60772705078125}
I0726 07:54:42.740355 140255084660480 logging_writer.py:48] [2218] accumulated_eval_time=266.571390, accumulated_logging_time=1.607727, accumulated_submission_time=690.847350, global_step=2218, preemption_count=0, score=690.847350, test/loss=0.296714, test/num_examples=3581, test/ssim=0.731418, total_duration=960.345540, train/loss=0.273532, train/ssim=0.736981, validation/loss=0.294815, validation/num_examples=3554, validation/ssim=0.714252
I0726 07:55:55.459746 140255076267776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.141076, loss=0.278782
I0726 07:55:55.464051 140312776001344 submission.py:119] 2500) loss = 0.279, grad_norm = 0.141
I0726 07:56:02.989141 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:56:05.070157 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:56:07.190887 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:56:09.258760 140312776001344 submission_runner.py:364] Time since start: 1046.88s, 	Step: 2529, 	{'train/ssim': 0.7382005964006696, 'train/loss': 0.27155189854758127, 'validation/ssim': 0.7157216670476927, 'validation/loss': 0.2930098637208955, 'validation/num_examples': 3554, 'test/ssim': 0.7328535014442544, 'test/loss': 0.2947554557691636, 'test/num_examples': 3581, 'score': 770.7324087619781, 'total_duration': 1046.8797283172607, 'accumulated_submission_time': 770.7324087619781, 'accumulated_eval_time': 272.84098291397095, 'accumulated_logging_time': 1.8463234901428223}
I0726 07:56:09.274031 140255084660480 logging_writer.py:48] [2529] accumulated_eval_time=272.840983, accumulated_logging_time=1.846323, accumulated_submission_time=770.732409, global_step=2529, preemption_count=0, score=770.732409, test/loss=0.294755, test/num_examples=3581, test/ssim=0.732854, total_duration=1046.879728, train/loss=0.271552, train/ssim=0.738201, validation/loss=0.293010, validation/num_examples=3554, validation/ssim=0.715722
I0726 07:56:55.604763 140312776001344 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0726 07:56:57.664953 140312776001344 spec.py:332] Evaluating on the validation split.
I0726 07:56:59.739576 140312776001344 spec.py:348] Evaluating on the test split.
I0726 07:57:01.780630 140312776001344 submission_runner.py:364] Time since start: 1099.40s, 	Step: 2714, 	{'train/ssim': 0.7376838411603656, 'train/loss': 0.2715043681008475, 'validation/ssim': 0.7156584680113957, 'validation/loss': 0.29278413324994723, 'validation/num_examples': 3554, 'test/ssim': 0.7328014144748325, 'test/loss': 0.29454516485662174, 'test/num_examples': 3581, 'score': 816.7901341915131, 'total_duration': 1099.4015855789185, 'accumulated_submission_time': 816.7901341915131, 'accumulated_eval_time': 279.01687264442444, 'accumulated_logging_time': 2.051671266555786}
I0726 07:57:01.795938 140255076267776 logging_writer.py:48] [2714] accumulated_eval_time=279.016873, accumulated_logging_time=2.051671, accumulated_submission_time=816.790134, global_step=2714, preemption_count=0, score=816.790134, test/loss=0.294545, test/num_examples=3581, test/ssim=0.732801, total_duration=1099.401586, train/loss=0.271504, train/ssim=0.737684, validation/loss=0.292784, validation/num_examples=3554, validation/ssim=0.715658
I0726 07:57:02.049597 140255084660480 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=816.790134
I0726 07:57:02.217989 140312776001344 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_timing/adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0726 07:57:02.901736 140312776001344 submission_runner.py:530] Tuning trial 1/1
I0726 07:57:02.901974 140312776001344 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0726 07:57:02.907750 140312776001344 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/ssim': 0.2609733854021345, 'train/loss': 0.9489507675170898, 'validation/ssim': 0.25269056158114095, 'validation/loss': 0.958190196675401, 'validation/num_examples': 3554, 'test/ssim': 0.27520883193198475, 'test/loss': 0.9587258490994136, 'test/num_examples': 3581, 'score': 52.46823334693909, 'total_duration': 267.404408454895, 'accumulated_submission_time': 52.46823334693909, 'accumulated_eval_time': 214.93551349639893, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (305, {'train/ssim': 0.7040531294686454, 'train/loss': 0.30308846064976286, 'validation/ssim': 0.682002439482801, 'validation/loss': 0.32520894837902714, 'validation/num_examples': 3554, 'test/ssim': 0.700410409867181, 'test/loss': 0.3270385026332554, 'test/num_examples': 3581, 'score': 132.3388864994049, 'total_duration': 354.2827491760254, 'accumulated_submission_time': 132.3388864994049, 'accumulated_eval_time': 221.60978031158447, 'accumulated_logging_time': 0.11320376396179199, 'global_step': 305, 'preemption_count': 0}), (531, {'train/ssim': 0.7178893089294434, 'train/loss': 0.2933213710784912, 'validation/ssim': 0.6965254406346723, 'validation/loss': 0.3150418377620287, 'validation/num_examples': 3554, 'test/ssim': 0.7137800577265428, 'test/loss': 0.3175052577841385, 'test/num_examples': 3581, 'score': 212.30554151535034, 'total_duration': 441.20182728767395, 'accumulated_submission_time': 212.30554151535034, 'accumulated_eval_time': 228.16843557357788, 'accumulated_logging_time': 0.3292527198791504, 'global_step': 531, 'preemption_count': 0}), (755, {'train/ssim': 0.7255093029567173, 'train/loss': 0.2862475599561419, 'validation/ssim': 0.7040063517779263, 'validation/loss': 0.3077185807364941, 'validation/num_examples': 3554, 'test/ssim': 0.7211897700101229, 'test/loss': 0.30994623861177045, 'test/num_examples': 3581, 'score': 292.06615948677063, 'total_duration': 527.8620045185089, 'accumulated_submission_time': 292.06615948677063, 'accumulated_eval_time': 234.68275451660156, 'accumulated_logging_time': 0.5401449203491211, 'global_step': 755, 'preemption_count': 0}), (984, {'train/ssim': 0.7264759199959892, 'train/loss': 0.28200505461011616, 'validation/ssim': 0.7050920700047482, 'validation/loss': 0.3033274842167276, 'validation/num_examples': 3554, 'test/ssim': 0.7224768090268081, 'test/loss': 0.30537683422490225, 'test/num_examples': 3581, 'score': 371.88724637031555, 'total_duration': 614.8441746234894, 'accumulated_submission_time': 371.88724637031555, 'accumulated_eval_time': 241.4559919834137, 'accumulated_logging_time': 0.7450041770935059, 'global_step': 984, 'preemption_count': 0}), (1293, {'train/ssim': 0.7273302759443011, 'train/loss': 0.2792449678693499, 'validation/ssim': 0.706587620243036, 'validation/loss': 0.30003440225801914, 'validation/num_examples': 3554, 'test/ssim': 0.7236674462440659, 'test/loss': 0.3020779019259634, 'test/num_examples': 3581, 'score': 451.5780141353607, 'total_duration': 701.1504554748535, 'accumulated_submission_time': 451.5780141353607, 'accumulated_eval_time': 247.71161150932312, 'accumulated_logging_time': 0.9604082107543945, 'global_step': 1293, 'preemption_count': 0}), (1604, {'train/ssim': 0.7317947660173688, 'train/loss': 0.27880862780979704, 'validation/ssim': 0.710348237681134, 'validation/loss': 0.29935236787554514, 'validation/num_examples': 3554, 'test/ssim': 0.7273431909513753, 'test/loss': 0.30151363778666923, 'test/num_examples': 3581, 'score': 531.3330700397491, 'total_duration': 787.5422267913818, 'accumulated_submission_time': 531.3330700397491, 'accumulated_eval_time': 253.9899458885193, 'accumulated_logging_time': 1.177818775177002, 'global_step': 1604, 'preemption_count': 0}), (1912, {'train/ssim': 0.7362490381513324, 'train/loss': 0.27486489500318256, 'validation/ssim': 0.7143311508599466, 'validation/loss': 0.29613457520355585, 'validation/num_examples': 3554, 'test/ssim': 0.7315390554096272, 'test/loss': 0.29796729238297615, 'test/num_examples': 3581, 'score': 611.1320860385895, 'total_duration': 873.9910571575165, 'accumulated_submission_time': 611.1320860385895, 'accumulated_eval_time': 260.28436279296875, 'accumulated_logging_time': 1.3948426246643066, 'global_step': 1912, 'preemption_count': 0}), (2218, {'train/ssim': 0.7369805744716099, 'train/loss': 0.27353179454803467, 'validation/ssim': 0.7142517398969471, 'validation/loss': 0.29481536401545794, 'validation/num_examples': 3554, 'test/ssim': 0.7314184508953505, 'test/loss': 0.29671437578539517, 'test/num_examples': 3581, 'score': 690.8473498821259, 'total_duration': 960.3455398082733, 'accumulated_submission_time': 690.8473498821259, 'accumulated_eval_time': 266.5713903903961, 'accumulated_logging_time': 1.60772705078125, 'global_step': 2218, 'preemption_count': 0}), (2529, {'train/ssim': 0.7382005964006696, 'train/loss': 0.27155189854758127, 'validation/ssim': 0.7157216670476927, 'validation/loss': 0.2930098637208955, 'validation/num_examples': 3554, 'test/ssim': 0.7328535014442544, 'test/loss': 0.2947554557691636, 'test/num_examples': 3581, 'score': 770.7324087619781, 'total_duration': 1046.8797283172607, 'accumulated_submission_time': 770.7324087619781, 'accumulated_eval_time': 272.84098291397095, 'accumulated_logging_time': 1.8463234901428223, 'global_step': 2529, 'preemption_count': 0}), (2714, {'train/ssim': 0.7376838411603656, 'train/loss': 0.2715043681008475, 'validation/ssim': 0.7156584680113957, 'validation/loss': 0.29278413324994723, 'validation/num_examples': 3554, 'test/ssim': 0.7328014144748325, 'test/loss': 0.29454516485662174, 'test/num_examples': 3581, 'score': 816.7901341915131, 'total_duration': 1099.4015855789185, 'accumulated_submission_time': 816.7901341915131, 'accumulated_eval_time': 279.01687264442444, 'accumulated_logging_time': 2.051671266555786, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0726 07:57:02.907926 140312776001344 submission_runner.py:533] Timing: 816.7901341915131
I0726 07:57:02.907978 140312776001344 submission_runner.py:535] Total number of evals: 11
I0726 07:57:02.908020 140312776001344 submission_runner.py:536] ====================
I0726 07:57:02.908152 140312776001344 submission_runner.py:604] Final fastmri score: 816.7901341915131
