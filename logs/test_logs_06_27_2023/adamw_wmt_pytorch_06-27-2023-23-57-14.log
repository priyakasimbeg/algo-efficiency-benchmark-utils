torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 2>&1 | tee -a /logs/wmt_pytorch_06-27-2023-23-57-14.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-27 23:57:19.318504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.318505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.318505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.318505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.318504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.375243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.411962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 23:57:19.411962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 23:57:32.105179 139968325564224 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0627 23:57:32.105267 139653380708160 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0627 23:57:32.106203 140162622699328 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0627 23:57:32.106319 140365909583680 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0627 23:57:32.106522 140469073581888 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0627 23:57:32.106770 140713533396800 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0627 23:57:33.088131 140439590360896 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0627 23:57:33.090845 139819457648448 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0627 23:57:33.091181 139819457648448 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.098972 140439590360896 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.099770 139653380708160 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.099793 140469073581888 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.099859 139968325564224 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.099828 140713533396800 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.099896 140365909583680 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:33.099943 140162622699328 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 23:57:37.010253 139819457648448 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/wmt_pytorch.
W0627 23:57:37.038551 139819457648448 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 23:57:37.040176 140162622699328 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 23:57:37.041000 140439590360896 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 23:57:37.041172 139968325564224 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 23:57:37.043789 139819457648448 submission_runner.py:547] Using RNG seed 48478768
I0627 23:57:37.045331 139819457648448 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 23:57:37.045454 139819457648448 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/wmt_pytorch/trial_1.
I0627 23:57:37.045704 139819457648448 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/wmt_pytorch/trial_1/hparams.json.
I0627 23:57:37.046543 139819457648448 submission_runner.py:249] Initializing dataset.
I0627 23:57:37.046673 139819457648448 submission_runner.py:256] Initializing model.
W0627 23:57:37.062740 140365909583680 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 23:57:37.266438 140469073581888 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 23:57:37.266790 139653380708160 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 23:57:37.279947 140713533396800 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 23:57:40.749513 139819457648448 submission_runner.py:268] Initializing optimizer.
I0627 23:57:40.750969 139819457648448 submission_runner.py:275] Initializing metrics bundle.
I0627 23:57:40.751072 139819457648448 submission_runner.py:292] Initializing checkpoint and logger.
I0627 23:57:40.751784 139819457648448 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 23:57:40.751881 139819457648448 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 23:57:41.355050 139819457648448 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0627 23:57:41.355927 139819457648448 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/wmt_pytorch/trial_1/flags_0.json.
I0627 23:57:41.400891 139819457648448 submission_runner.py:328] Starting training loop.
I0627 23:57:41.414447 139819457648448 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:57:41.419038 139819457648448 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0627 23:57:41.476390 139819457648448 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:57:45.370876 139780666939136 logging_writer.py:48] [0] global_step=0, grad_norm=5.392676, loss=11.121677
I0627 23:57:45.380951 139819457648448 submission.py:119] 0) loss = 11.122, grad_norm = 5.393
I0627 23:57:45.382239 139819457648448 spec.py:298] Evaluating on the training split.
I0627 23:57:45.384727 139819457648448 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:57:45.387488 139819457648448 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0627 23:57:45.423734 139819457648448 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:57:49.647177 139819457648448 workload.py:131] Translating evaluation dataset.
I0628 00:02:34.728169 139819457648448 spec.py:310] Evaluating on the validation split.
I0628 00:02:34.731061 139819457648448 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0628 00:02:34.734636 139819457648448 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0628 00:02:34.770665 139819457648448 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0628 00:02:38.652652 139819457648448 workload.py:131] Translating evaluation dataset.
I0628 00:07:17.940180 139819457648448 spec.py:326] Evaluating on the test split.
I0628 00:07:17.942998 139819457648448 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0628 00:07:17.946470 139819457648448 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0628 00:07:17.982097 139819457648448 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0628 00:07:21.948629 139819457648448 workload.py:131] Translating evaluation dataset.
I0628 00:12:06.929378 139819457648448 submission_runner.py:424] Time since start: 865.53s, 	Step: 1, 	{'train/accuracy': 0.0006405270622683809, 'train/loss': 11.141377047399002, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.140237721788942, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.125461913892279, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.9805333614349365, 'total_duration': 865.5290422439575, 'accumulated_submission_time': 3.9805333614349365, 'accumulated_eval_time': 861.547016620636, 'accumulated_logging_time': 0}
I0628 00:12:06.938004 139761590040320 logging_writer.py:48] [1] accumulated_eval_time=861.547017, accumulated_logging_time=0, accumulated_submission_time=3.980533, global_step=1, preemption_count=0, score=3.980533, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.125462, test/num_examples=3003, total_duration=865.529042, train/accuracy=0.000641, train/bleu=0.000000, train/loss=11.141377, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.140238, validation/num_examples=3000
I0628 00:12:06.957188 140469073581888 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957192 140713533396800 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957195 139968325564224 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957230 140162622699328 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957196 140439590360896 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957203 140365909583680 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957239 139653380708160 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:06.957539 139819457648448 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0628 00:12:07.383515 139761581647616 logging_writer.py:48] [1] global_step=1, grad_norm=5.386283, loss=11.114626
I0628 00:12:07.387419 139819457648448 submission.py:119] 1) loss = 11.115, grad_norm = 5.386
I0628 00:12:07.828794 139761590040320 logging_writer.py:48] [2] global_step=2, grad_norm=5.361952, loss=11.112307
I0628 00:12:07.832596 139819457648448 submission.py:119] 2) loss = 11.112, grad_norm = 5.362
I0628 00:12:08.269080 139761581647616 logging_writer.py:48] [3] global_step=3, grad_norm=5.258652, loss=11.085407
I0628 00:12:08.272604 139819457648448 submission.py:119] 3) loss = 11.085, grad_norm = 5.259
I0628 00:12:08.709330 139761590040320 logging_writer.py:48] [4] global_step=4, grad_norm=5.183880, loss=11.071752
I0628 00:12:08.712843 139819457648448 submission.py:119] 4) loss = 11.072, grad_norm = 5.184
I0628 00:12:09.150627 139761581647616 logging_writer.py:48] [5] global_step=5, grad_norm=5.149819, loss=11.040085
I0628 00:12:09.154320 139819457648448 submission.py:119] 5) loss = 11.040, grad_norm = 5.150
I0628 00:12:09.589354 139761590040320 logging_writer.py:48] [6] global_step=6, grad_norm=5.171541, loss=10.990687
I0628 00:12:09.592843 139819457648448 submission.py:119] 6) loss = 10.991, grad_norm = 5.172
I0628 00:12:10.030836 139761581647616 logging_writer.py:48] [7] global_step=7, grad_norm=4.993814, loss=10.965516
I0628 00:12:10.034414 139819457648448 submission.py:119] 7) loss = 10.966, grad_norm = 4.994
I0628 00:12:10.472015 139761590040320 logging_writer.py:48] [8] global_step=8, grad_norm=4.847979, loss=10.910401
I0628 00:12:10.475782 139819457648448 submission.py:119] 8) loss = 10.910, grad_norm = 4.848
I0628 00:12:10.910366 139761581647616 logging_writer.py:48] [9] global_step=9, grad_norm=4.715663, loss=10.852593
I0628 00:12:10.913924 139819457648448 submission.py:119] 9) loss = 10.853, grad_norm = 4.716
I0628 00:12:10.915282 139819457648448 spec.py:298] Evaluating on the training split.
I0628 00:12:14.717843 139819457648448 workload.py:131] Translating evaluation dataset.
I0628 00:16:59.810901 139819457648448 spec.py:310] Evaluating on the validation split.
I0628 00:17:03.568501 139819457648448 workload.py:131] Translating evaluation dataset.
I0628 00:21:42.892543 139819457648448 spec.py:326] Evaluating on the test split.
I0628 00:21:46.688061 139819457648448 workload.py:131] Translating evaluation dataset.
I0628 00:26:31.770093 139819457648448 submission_runner.py:424] Time since start: 1730.37s, 	Step: 10, 	{'train/accuracy': 0.0006407542592995183, 'train/loss': 10.786626686270695, 'train/bleu': 4.488677851427821e-11, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 10.780650580897943, 'validation/bleu': 4.657784967143944e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 10.782140055778282, 'test/bleu': 4.3425723436154384e-10, 'test/num_examples': 3003, 'score': 7.943788051605225, 'total_duration': 1730.3697504997253, 'accumulated_submission_time': 7.943788051605225, 'accumulated_eval_time': 1722.4017431735992, 'accumulated_logging_time': 0.018958568572998047}
I0628 00:26:31.778158 139761590040320 logging_writer.py:48] [10] accumulated_eval_time=1722.401743, accumulated_logging_time=0.018959, accumulated_submission_time=7.943788, global_step=10, preemption_count=0, score=7.943788, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.782140, test/num_examples=3003, total_duration=1730.369750, train/accuracy=0.000641, train/bleu=0.000000, train/loss=10.786627, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.780651, validation/num_examples=3000
I0628 00:26:31.794396 139761581647616 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=7.943788
I0628 00:26:34.328508 139819457648448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/wmt_pytorch/trial_1/checkpoint_10.
I0628 00:26:34.351353 139819457648448 submission_runner.py:587] Tuning trial 1/1
I0628 00:26:34.351555 139819457648448 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0628 00:26:34.352168 139819457648448 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006405270622683809, 'train/loss': 11.141377047399002, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.140237721788942, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.125461913892279, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.9805333614349365, 'total_duration': 865.5290422439575, 'accumulated_submission_time': 3.9805333614349365, 'accumulated_eval_time': 861.547016620636, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.0006407542592995183, 'train/loss': 10.786626686270695, 'train/bleu': 4.488677851427821e-11, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 10.780650580897943, 'validation/bleu': 4.657784967143944e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 10.782140055778282, 'test/bleu': 4.3425723436154384e-10, 'test/num_examples': 3003, 'score': 7.943788051605225, 'total_duration': 1730.3697504997253, 'accumulated_submission_time': 7.943788051605225, 'accumulated_eval_time': 1722.4017431735992, 'accumulated_logging_time': 0.018958568572998047, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0628 00:26:34.352363 139819457648448 submission_runner.py:590] Timing: 7.943788051605225
I0628 00:26:34.352426 139819457648448 submission_runner.py:591] ====================
I0628 00:26:34.352541 139819457648448 submission_runner.py:659] Final wmt score: 7.943788051605225
