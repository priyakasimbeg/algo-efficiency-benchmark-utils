python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_06-27-2023-01-08-01.log
2023-06-27 01:08:03.601164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 01:08:16.971354 139908915062592 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/librispeech_deepspeech_jax because --overwrite was set.
I0627 01:08:16.972640 139908915062592 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/librispeech_deepspeech_jax.
I0627 01:08:17.869751 139908915062592 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0627 01:08:17.870374 139908915062592 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0627 01:08:17.870498 139908915062592 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0627 01:08:17.875042 139908915062592 submission_runner.py:547] Using RNG seed 1046305692
I0627 01:08:20.039527 139908915062592 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 01:08:20.039721 139908915062592 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1.
I0627 01:08:20.039944 139908915062592 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0627 01:08:20.220233 139908915062592 submission_runner.py:249] Initializing dataset.
I0627 01:08:20.220427 139908915062592 submission_runner.py:256] Initializing model.
I0627 01:08:22.589242 139908915062592 submission_runner.py:268] Initializing optimizer.
I0627 01:08:23.273717 139908915062592 submission_runner.py:275] Initializing metrics bundle.
I0627 01:08:23.273906 139908915062592 submission_runner.py:292] Initializing checkpoint and logger.
I0627 01:08:23.274767 139908915062592 checkpoints.py:915] Found no checkpoint files in /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0627 01:08:23.275013 139908915062592 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 01:08:23.275091 139908915062592 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 01:08:24.140472 139908915062592 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0627 01:08:24.141405 139908915062592 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0627 01:08:24.148679 139908915062592 submission_runner.py:328] Starting training loop.
I0627 01:08:24.437796 139908915062592 input_pipeline.py:20] Loading split = train-clean-100
I0627 01:08:24.479100 139908915062592 input_pipeline.py:20] Loading split = train-clean-360
I0627 01:08:24.609388 139908915062592 input_pipeline.py:20] Loading split = train-other-500
2023-06-27 01:09:16.970841: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-06-27 01:09:18.623172: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0627 01:09:23.587408 139743615026944 logging_writer.py:48] [0] global_step=0, grad_norm=30.95747947692871, loss=33.125431060791016
I0627 01:09:23.610736 139908915062592 spec.py:298] Evaluating on the training split.
I0627 01:09:23.886573 139908915062592 input_pipeline.py:20] Loading split = train-clean-100
I0627 01:09:23.921878 139908915062592 input_pipeline.py:20] Loading split = train-clean-360
I0627 01:09:24.294312 139908915062592 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0627 01:11:24.825608 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 01:11:25.019787 139908915062592 input_pipeline.py:20] Loading split = dev-clean
I0627 01:11:25.025156 139908915062592 input_pipeline.py:20] Loading split = dev-other
I0627 01:12:33.427148 139908915062592 spec.py:326] Evaluating on the test split.
I0627 01:12:33.640980 139908915062592 input_pipeline.py:20] Loading split = test-clean
I0627 01:13:16.979169 139908915062592 submission_runner.py:424] Time since start: 292.83s, 	Step: 1, 	{'train/ctc_loss': Array(31.92686, dtype=float32), 'train/wer': 3.9299398205026757, 'validation/ctc_loss': Array(31.03651, dtype=float32), 'validation/wer': 3.5862188733128155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.115553, dtype=float32), 'test/wer': 3.8258688278187396, 'test/num_examples': 2472, 'score': 59.461833477020264, 'total_duration': 292.82825660705566, 'accumulated_submission_time': 59.461833477020264, 'accumulated_eval_time': 233.36624383926392, 'accumulated_logging_time': 0}
I0627 01:13:16.992661 139741139818240 logging_writer.py:48] [1] accumulated_eval_time=233.366244, accumulated_logging_time=0, accumulated_submission_time=59.461833, global_step=1, preemption_count=0, score=59.461833, test/ctc_loss=31.11555290222168, test/num_examples=2472, test/wer=3.825869, total_duration=292.828257, train/ctc_loss=31.926860809326172, train/wer=3.929940, validation/ctc_loss=31.036510467529297, validation/num_examples=5348, validation/wer=3.586219
I0627 01:14:42.466796 139750061266688 logging_writer.py:48] [100] global_step=100, grad_norm=3.3374269008636475, loss=7.464563369750977
I0627 01:15:58.866047 139750069659392 logging_writer.py:48] [200] global_step=200, grad_norm=1.0713376998901367, loss=6.0814208984375
I0627 01:17:14.826368 139750061266688 logging_writer.py:48] [300] global_step=300, grad_norm=1.0078216791152954, loss=5.86672830581665
I0627 01:18:32.563107 139750069659392 logging_writer.py:48] [400] global_step=400, grad_norm=0.8811098337173462, loss=5.799566268920898
I0627 01:19:48.885125 139750061266688 logging_writer.py:48] [500] global_step=500, grad_norm=1.1670314073562622, loss=5.753033638000488
I0627 01:21:04.761644 139750069659392 logging_writer.py:48] [600] global_step=600, grad_norm=0.5779500007629395, loss=5.670572280883789
I0627 01:22:22.256313 139750061266688 logging_writer.py:48] [700] global_step=700, grad_norm=0.8548862338066101, loss=5.512627601623535
I0627 01:23:40.063799 139750069659392 logging_writer.py:48] [800] global_step=800, grad_norm=1.12074613571167, loss=5.280066967010498
I0627 01:25:01.244959 139750061266688 logging_writer.py:48] [900] global_step=900, grad_norm=1.3781808614730835, loss=4.941941738128662
I0627 01:26:24.375781 139750069659392 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.581260323524475, loss=4.548788070678711
I0627 01:27:45.877836 139750758590208 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.361737608909607, loss=4.22285795211792
I0627 01:29:01.719532 139750750197504 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.827716112136841, loss=4.000070095062256
I0627 01:30:18.377620 139750758590208 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.863112211227417, loss=3.7576792240142822
I0627 01:31:34.817426 139750750197504 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.497626304626465, loss=3.6396939754486084
I0627 01:32:50.070144 139750758590208 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4387524127960205, loss=3.413245439529419
I0627 01:34:07.318995 139750750197504 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.1746463775634766, loss=3.3596818447113037
I0627 01:35:23.934065 139750758590208 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.4291625022888184, loss=3.222254514694214
I0627 01:36:40.366317 139750750197504 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.241290330886841, loss=3.1291213035583496
I0627 01:37:59.124611 139750758590208 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.5083212852478027, loss=3.020019769668579
I0627 01:39:18.205571 139750750197504 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.1985251903533936, loss=3.0312933921813965
I0627 01:40:40.083969 139750758590208 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.4826252460479736, loss=2.838026523590088
I0627 01:41:54.784308 139750750197504 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.167314291000366, loss=2.8114192485809326
I0627 01:43:09.737849 139750758590208 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.6951963901519775, loss=2.7657299041748047
I0627 01:44:26.105551 139750750197504 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.581164836883545, loss=2.712380886077881
I0627 01:45:43.169376 139750758590208 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.2249677181243896, loss=2.7525720596313477
I0627 01:46:59.928406 139750750197504 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.795452117919922, loss=2.6532328128814697
I0627 01:48:40.811498 139750758590208 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.0782220363616943, loss=2.584378480911255
I0627 01:50:06.634161 139750750197504 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.2155330181121826, loss=2.53665828704834
I0627 01:51:21.393475 139750758590208 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.3912644386291504, loss=2.5506961345672607
I0627 01:52:37.061679 139750750197504 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.7738070487976074, loss=2.4450151920318604
I0627 01:53:18.050440 139908915062592 spec.py:298] Evaluating on the training split.
I0627 01:54:05.124438 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 01:54:53.132661 139908915062592 spec.py:326] Evaluating on the test split.
I0627 01:55:17.706151 139908915062592 submission_runner.py:424] Time since start: 2813.55s, 	Step: 3051, 	{'train/ctc_loss': Array(3.3917294, dtype=float32), 'train/wer': 0.6823597866161356, 'validation/ctc_loss': Array(3.607257, dtype=float32), 'validation/wer': 0.7091240629432025, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.230003, dtype=float32), 'test/wer': 0.663193386549672, 'test/num_examples': 2472, 'score': 2460.470052719116, 'total_duration': 2813.5549714565277, 'accumulated_submission_time': 2460.470052719116, 'accumulated_eval_time': 353.01953053474426, 'accumulated_logging_time': 0.026746749877929688}
I0627 01:55:17.719882 139750758590208 logging_writer.py:48] [3051] accumulated_eval_time=353.019531, accumulated_logging_time=0.026747, accumulated_submission_time=2460.470053, global_step=3051, preemption_count=0, score=2460.470053, test/ctc_loss=3.2300031185150146, test/num_examples=2472, test/wer=0.663193, total_duration=2813.554971, train/ctc_loss=3.3917293548583984, train/wer=0.682360, validation/ctc_loss=3.6072568893432617, validation/num_examples=5348, validation/wer=0.709124
I0627 01:55:59.426836 139750758590208 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.8594746589660645, loss=2.393022060394287
I0627 01:57:13.919678 139750750197504 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.8016774654388428, loss=2.3757736682891846
I0627 01:58:29.047942 139750758590208 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.345571756362915, loss=2.3103878498077393
I0627 01:59:45.459218 139750750197504 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.16074538230896, loss=2.3340916633605957
I0627 02:01:01.558730 139750758590208 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.929673910140991, loss=2.288271903991699
I0627 02:02:26.313253 139750750197504 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.7135825157165527, loss=2.2500574588775635
I0627 02:03:51.165451 139750758590208 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.919365406036377, loss=2.296503782272339
I0627 02:05:15.663235 139750750197504 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.501211643218994, loss=2.311034679412842
I0627 02:06:39.582206 139750758590208 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.25100564956665, loss=2.3776965141296387
I0627 02:08:03.374004 139750750197504 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.7691688537597656, loss=2.2612197399139404
I0627 02:09:25.809200 139750758590208 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.3322432041168213, loss=2.224024534225464
I0627 02:10:45.537575 139750758590208 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.559565544128418, loss=2.049072742462158
I0627 02:12:00.173181 139750750197504 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.723471164703369, loss=2.1033504009246826
I0627 02:13:16.125968 139750758590208 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.9182418584823608, loss=2.1400279998779297
I0627 02:14:32.333364 139750750197504 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.9767835140228271, loss=2.1615688800811768
I0627 02:15:52.174010 139750758590208 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.6478328704833984, loss=2.0643906593322754
I0627 02:17:15.824521 139750750197504 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.5350255966186523, loss=2.08357310295105
I0627 02:18:41.353595 139750758590208 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.562268018722534, loss=2.1207549571990967
I0627 02:20:06.446441 139750750197504 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.95674991607666, loss=2.0107054710388184
I0627 02:21:31.189010 139750758590208 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.570664644241333, loss=2.0028045177459717
I0627 02:22:53.227811 139750750197504 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.6200575828552246, loss=2.104281425476074
I0627 02:24:14.311619 139750103230208 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.791149377822876, loss=2.038846969604492
I0627 02:25:29.034555 139750094837504 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.5473003387451172, loss=1.9626413583755493
I0627 02:26:43.494048 139750103230208 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.304245710372925, loss=2.0083086490631104
I0627 02:28:01.171008 139750094837504 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.574951648712158, loss=1.9716790914535522
I0627 02:29:22.705881 139750103230208 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.1696746349334717, loss=1.9737125635147095
I0627 02:30:46.674664 139750094837504 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.847738742828369, loss=1.9580591917037964
I0627 02:32:09.676501 139750103230208 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.0785207748413086, loss=1.930473804473877
I0627 02:33:35.229196 139750094837504 logging_writer.py:48] [5900] global_step=5900, grad_norm=4.5151801109313965, loss=1.9371927976608276
I0627 02:34:59.577142 139750103230208 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.822726845741272, loss=2.022369384765625
I0627 02:35:18.344317 139908915062592 spec.py:298] Evaluating on the training split.
I0627 02:36:11.594429 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 02:37:02.239789 139908915062592 spec.py:326] Evaluating on the test split.
I0627 02:37:27.428658 139908915062592 submission_runner.py:424] Time since start: 5343.28s, 	Step: 6023, 	{'train/ctc_loss': Array(0.7311462, dtype=float32), 'train/wer': 0.242585834909304, 'validation/ctc_loss': Array(1.1313654, dtype=float32), 'validation/wer': 0.30937105037192836, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.76451725, dtype=float32), 'test/wer': 0.24585135986025633, 'test/num_examples': 2472, 'score': 4861.044587612152, 'total_duration': 5343.27553486824, 'accumulated_submission_time': 4861.044587612152, 'accumulated_eval_time': 482.09951543807983, 'accumulated_logging_time': 0.05035591125488281}
I0627 02:37:27.445848 139749519550208 logging_writer.py:48] [6023] accumulated_eval_time=482.099515, accumulated_logging_time=0.050356, accumulated_submission_time=4861.044588, global_step=6023, preemption_count=0, score=4861.044588, test/ctc_loss=0.7645172476768494, test/num_examples=2472, test/wer=0.245851, total_duration=5343.275535, train/ctc_loss=0.7311462163925171, train/wer=0.242586, validation/ctc_loss=1.131365418434143, validation/num_examples=5348, validation/wer=0.309371
I0627 02:38:26.622334 139749511157504 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.3019859790802, loss=1.9302960634231567
I0627 02:39:44.909625 139750758590208 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.7118725776672363, loss=1.948818564414978
I0627 02:40:59.571182 139750750197504 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.9804257154464722, loss=2.0095367431640625
I0627 02:42:13.902374 139750758590208 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.5496764183044434, loss=1.9418991804122925
I0627 02:43:29.187924 139750750197504 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.0476763248443604, loss=1.984702229499817
I0627 02:44:49.215570 139750758590208 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.311953067779541, loss=1.9021730422973633
I0627 02:46:10.171198 139750750197504 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.919610619544983, loss=1.8593332767486572
I0627 02:47:33.108178 139750758590208 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.8894412517547607, loss=1.9286996126174927
I0627 02:48:56.770337 139750750197504 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.457531452178955, loss=1.9866893291473389
I0627 02:50:19.275645 139750758590208 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.2745299339294434, loss=1.8980631828308105
I0627 02:51:45.371752 139750750197504 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.389711380004883, loss=1.8796279430389404
I0627 02:53:08.758838 139750758590208 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.3959131240844727, loss=1.864296793937683
I0627 02:54:27.989665 139749519550208 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.9560545682907104, loss=1.8958760499954224
I0627 02:55:43.522817 139749511157504 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.8832967281341553, loss=1.87593674659729
I0627 02:56:59.124407 139749519550208 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.7254208326339722, loss=1.8320382833480835
I0627 02:58:15.154552 139749511157504 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.5034408569335938, loss=1.8846813440322876
I0627 02:59:38.140171 139749519550208 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1310477256774902, loss=1.8590978384017944
I0627 03:01:04.144289 139749511157504 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.6485488414764404, loss=1.9122412204742432
I0627 03:02:26.394449 139749519550208 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.9485297203063965, loss=1.872603178024292
I0627 03:03:50.528431 139749511157504 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.5629173517227173, loss=1.8185278177261353
I0627 03:05:14.228605 139749519550208 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.0668299198150635, loss=1.8524564504623413
I0627 03:06:38.204638 139749511157504 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.883371591567993, loss=1.792450189590454
I0627 03:08:00.274535 139749519550208 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.2892539501190186, loss=1.8000826835632324
I0627 03:09:14.979145 139749511157504 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.922912836074829, loss=1.8360658884048462
I0627 03:10:30.264039 139749519550208 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.8325326442718506, loss=1.7645606994628906
I0627 03:11:46.123293 139749511157504 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.028839588165283, loss=1.7499321699142456
I0627 03:13:07.562468 139749519550208 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.041926383972168, loss=1.8100886344909668
I0627 03:14:30.917397 139749511157504 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.0312588214874268, loss=1.7119531631469727
I0627 03:15:57.588146 139749519550208 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.1592297554016113, loss=1.7069361209869385
I0627 03:17:22.579429 139749511157504 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.4187936782836914, loss=1.7481584548950195
I0627 03:17:27.688655 139908915062592 spec.py:298] Evaluating on the training split.
I0627 03:18:22.407504 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 03:19:12.832972 139908915062592 spec.py:326] Evaluating on the test split.
I0627 03:19:39.589246 139908915062592 submission_runner.py:424] Time since start: 7875.44s, 	Step: 9007, 	{'train/ctc_loss': Array(0.45195788, dtype=float32), 'train/wer': 0.15927384284516977, 'validation/ctc_loss': Array(0.8551614, dtype=float32), 'validation/wer': 0.24386149408098487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5277823, dtype=float32), 'test/wer': 0.17122661629394917, 'test/num_examples': 2472, 'score': 7261.233250141144, 'total_duration': 7875.435887813568, 'accumulated_submission_time': 7261.233250141144, 'accumulated_eval_time': 613.9955124855042, 'accumulated_logging_time': 0.08127784729003906}
I0627 03:19:39.607254 139749519550208 logging_writer.py:48] [9007] accumulated_eval_time=613.995512, accumulated_logging_time=0.081278, accumulated_submission_time=7261.233250, global_step=9007, preemption_count=0, score=7261.233250, test/ctc_loss=0.5277823209762573, test/num_examples=2472, test/wer=0.171227, total_duration=7875.435888, train/ctc_loss=0.4519578814506531, train/wer=0.159274, validation/ctc_loss=0.8551614284515381, validation/num_examples=5348, validation/wer=0.243861
I0627 03:20:49.634331 139749511157504 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.521190643310547, loss=1.9028748273849487
I0627 03:22:05.880559 139749519550208 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.20747709274292, loss=1.8470312356948853
I0627 03:23:24.618010 139749519550208 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.252894401550293, loss=1.77849280834198
I0627 03:24:40.962824 139749511157504 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.8154497146606445, loss=1.8165253400802612
I0627 03:25:55.613901 139749519550208 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.691387414932251, loss=1.765195608139038
I0627 03:27:10.243831 139749511157504 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.4656622409820557, loss=1.7692112922668457
I0627 03:28:26.036370 139749519550208 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.599248170852661, loss=1.7664542198181152
I0627 03:29:49.286417 139749511157504 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.263138771057129, loss=1.8897427320480347
I0627 03:31:14.608016 139749519550208 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.076371908187866, loss=1.7861605882644653
I0627 03:32:39.489183 139749511157504 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.2786412239074707, loss=1.7460488080978394
I0627 03:34:04.538902 139749519550208 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.150862455368042, loss=1.7391129732131958
I0627 03:35:26.236095 139749511157504 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.159282684326172, loss=1.7225515842437744
I0627 03:36:56.349864 139750758590208 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.5960137844085693, loss=1.728506326675415
I0627 03:38:11.599829 139750750197504 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.2547571659088135, loss=1.6800884008407593
I0627 03:39:27.348912 139750758590208 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.7769107818603516, loss=1.722409963607788
I0627 03:40:41.764036 139750750197504 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.9266812801361084, loss=1.8140931129455566
I0627 03:41:56.294202 139750758590208 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.633195400238037, loss=1.7206803560256958
I0627 03:43:18.009147 139750750197504 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.1805965900421143, loss=1.7693177461624146
I0627 03:44:41.481655 139750758590208 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.652371883392334, loss=1.79876708984375
I0627 03:46:07.476695 139750750197504 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.4479405879974365, loss=1.7195427417755127
I0627 03:47:32.115739 139750758590208 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.3128421306610107, loss=1.711524486541748
I0627 03:48:58.761946 139750750197504 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.0981898307800293, loss=1.734606146812439
I0627 03:50:25.512284 139750758590208 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.007300853729248, loss=1.6641745567321777
I0627 03:51:46.101563 139749519550208 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.8623719215393066, loss=1.6912651062011719
I0627 03:53:00.979740 139749511157504 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.180838108062744, loss=1.7986234426498413
I0627 03:54:17.266593 139749519550208 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.4258172512054443, loss=1.7333489656448364
I0627 03:55:32.173270 139749511157504 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.6115052700042725, loss=1.6415631771087646
I0627 03:56:51.620348 139749519550208 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.323162078857422, loss=1.707640290260315
I0627 03:58:17.354262 139749511157504 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.5039939880371094, loss=1.7079079151153564
I0627 03:59:40.081179 139908915062592 spec.py:298] Evaluating on the training split.
I0627 04:00:34.645287 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 04:01:24.697729 139908915062592 spec.py:326] Evaluating on the test split.
I0627 04:01:50.278291 139908915062592 submission_runner.py:424] Time since start: 10406.13s, 	Step: 11996, 	{'train/ctc_loss': Array(0.40960434, dtype=float32), 'train/wer': 0.13970195519902817, 'validation/ctc_loss': Array(0.7591697, dtype=float32), 'validation/wer': 0.21767696745747667, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45681867, dtype=float32), 'test/wer': 0.15010257347713932, 'test/num_examples': 2472, 'score': 9661.651376962662, 'total_duration': 10406.125103712082, 'accumulated_submission_time': 9661.651376962662, 'accumulated_eval_time': 744.1882276535034, 'accumulated_logging_time': 0.11491918563842773}
I0627 04:01:50.296161 139749519550208 logging_writer.py:48] [11996] accumulated_eval_time=744.188228, accumulated_logging_time=0.114919, accumulated_submission_time=9661.651377, global_step=11996, preemption_count=0, score=9661.651377, test/ctc_loss=0.45681867003440857, test/num_examples=2472, test/wer=0.150103, total_duration=10406.125104, train/ctc_loss=0.40960434079170227, train/wer=0.139702, validation/ctc_loss=0.7591696977615356, validation/num_examples=5348, validation/wer=0.217677
I0627 04:01:54.226344 139749511157504 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.189194679260254, loss=1.7470142841339111
I0627 04:03:08.655690 139749519550208 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.2120556831359863, loss=1.7344470024108887
I0627 04:04:23.186682 139749511157504 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.3025732040405273, loss=1.6894086599349976
I0627 04:05:41.140615 139749519550208 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.380931377410889, loss=1.7552769184112549
I0627 04:07:05.959516 139750758590208 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.9563095569610596, loss=1.7141616344451904
I0627 04:08:21.186964 139750750197504 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.3354101181030273, loss=1.730322241783142
I0627 04:09:35.856298 139750758590208 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.119081497192383, loss=1.6925039291381836
I0627 04:10:52.016808 139750750197504 logging_writer.py:48] [12700] global_step=12700, grad_norm=5.683786869049072, loss=1.6883766651153564
I0627 04:12:11.876248 139750758590208 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.7201201915740967, loss=1.7317506074905396
I0627 04:13:36.483702 139750750197504 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.3562581539154053, loss=1.648146629333496
I0627 04:15:01.699432 139750758590208 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.6291120052337646, loss=1.6551415920257568
I0627 04:16:28.968225 139750750197504 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.5892624855041504, loss=1.6966511011123657
I0627 04:17:55.018419 139750758590208 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.5898900032043457, loss=1.7495001554489136
I0627 04:19:19.834647 139750750197504 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.547555446624756, loss=1.7392117977142334
I0627 04:20:47.097715 139750758590208 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.706442356109619, loss=1.7467604875564575
I0627 04:22:01.419862 139750750197504 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.411449670791626, loss=1.6487669944763184
I0627 04:23:16.066900 139750758590208 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.1893997192382812, loss=1.6443952322006226
I0627 04:24:31.000568 139750750197504 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.321255683898926, loss=1.603173851966858
I0627 04:25:52.645438 139750758590208 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.522157907485962, loss=1.634247064590454
I0627 04:27:18.350347 139750750197504 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.224534273147583, loss=1.7122689485549927
I0627 04:28:41.865823 139750758590208 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.781461715698242, loss=1.653156042098999
I0627 04:30:04.559875 139750750197504 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.0969016551971436, loss=1.6233713626861572
I0627 04:31:30.538394 139750758590208 logging_writer.py:48] [14200] global_step=14200, grad_norm=8.12517261505127, loss=1.6462565660476685
I0627 04:32:55.488842 139750750197504 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.0289902687072754, loss=1.582899570465088
I0627 04:34:23.165789 139750758590208 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.362250328063965, loss=1.5690181255340576
I0627 04:35:43.676851 139750758590208 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.8576923608779907, loss=1.6314600706100464
I0627 04:37:00.182875 139750750197504 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.075774908065796, loss=1.5511095523834229
I0627 04:38:15.881151 139750758590208 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.4531800746917725, loss=1.6578837633132935
I0627 04:39:30.951626 139750750197504 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.507667303085327, loss=1.6255556344985962
I0627 04:40:50.679103 139750758590208 logging_writer.py:48] [14900] global_step=14900, grad_norm=4.0364580154418945, loss=1.6122318506240845
I0627 04:41:51.008594 139908915062592 spec.py:298] Evaluating on the training split.
I0627 04:42:44.521631 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 04:43:35.567142 139908915062592 spec.py:326] Evaluating on the test split.
I0627 04:44:01.339132 139908915062592 submission_runner.py:424] Time since start: 12937.19s, 	Step: 14973, 	{'train/ctc_loss': Array(0.3597357, dtype=float32), 'train/wer': 0.12554632629541848, 'validation/ctc_loss': Array(0.7016525, dtype=float32), 'validation/wer': 0.19881523217783095, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41626787, dtype=float32), 'test/wer': 0.1350110698108992, 'test/num_examples': 2472, 'score': 12062.307854890823, 'total_duration': 12937.185357570648, 'accumulated_submission_time': 12062.307854890823, 'accumulated_eval_time': 874.5137565135956, 'accumulated_logging_time': 0.148972749710083}
I0627 04:44:01.367864 139750758590208 logging_writer.py:48] [14973] accumulated_eval_time=874.513757, accumulated_logging_time=0.148973, accumulated_submission_time=12062.307855, global_step=14973, preemption_count=0, score=12062.307855, test/ctc_loss=0.41626787185668945, test/num_examples=2472, test/wer=0.135011, total_duration=12937.185358, train/ctc_loss=0.3597356975078583, train/wer=0.125546, validation/ctc_loss=0.7016525268554688, validation/num_examples=5348, validation/wer=0.198815
I0627 04:44:22.474708 139750750197504 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.9607746601104736, loss=1.5988349914550781
I0627 04:45:38.254012 139750758590208 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.3317859172821045, loss=1.643584132194519
I0627 04:46:54.166804 139750750197504 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.942556858062744, loss=1.6378707885742188
I0627 04:48:09.610706 139750758590208 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.3429856300354004, loss=1.5904133319854736
I0627 04:49:33.818696 139750750197504 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.043151617050171, loss=1.594667673110962
I0627 04:50:57.241410 139750758590208 logging_writer.py:48] [15500] global_step=15500, grad_norm=5.32246208190918, loss=1.5533391237258911
I0627 04:52:11.871083 139750750197504 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.6217334270477295, loss=1.6089701652526855
I0627 04:53:27.206139 139750758590208 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.365455389022827, loss=1.646803379058838
I0627 04:54:43.246496 139750750197504 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.3951597213745117, loss=1.6162714958190918
I0627 04:56:03.981907 139750758590208 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.4042587280273438, loss=1.5229833126068115
I0627 04:57:26.729344 139908915062592 spec.py:298] Evaluating on the training split.
I0627 04:58:19.918948 139908915062592 spec.py:310] Evaluating on the validation split.
I0627 04:59:10.806897 139908915062592 spec.py:326] Evaluating on the test split.
I0627 04:59:36.361539 139908915062592 submission_runner.py:424] Time since start: 13872.21s, 	Step: 16000, 	{'train/ctc_loss': Array(0.38618892, dtype=float32), 'train/wer': 0.1288069948946658, 'validation/ctc_loss': Array(0.70406944, dtype=float32), 'validation/wer': 0.19939410896390702, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4142691, dtype=float32), 'test/wer': 0.1330205350070075, 'test/num_examples': 2472, 'score': 12867.639452695847, 'total_duration': 13872.208798646927, 'accumulated_submission_time': 12867.639452695847, 'accumulated_eval_time': 1004.1419911384583, 'accumulated_logging_time': 0.1930561065673828}
I0627 04:59:36.379119 139750758590208 logging_writer.py:48] [16000] accumulated_eval_time=1004.141991, accumulated_logging_time=0.193056, accumulated_submission_time=12867.639453, global_step=16000, preemption_count=0, score=12867.639453, test/ctc_loss=0.4142690896987915, test/num_examples=2472, test/wer=0.133021, total_duration=13872.208799, train/ctc_loss=0.38618892431259155, train/wer=0.128807, validation/ctc_loss=0.7040694355964661, validation/num_examples=5348, validation/wer=0.199394
I0627 04:59:36.399019 139750750197504 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12867.639453
I0627 04:59:36.615542 139908915062592 checkpoints.py:490] Saving checkpoint at step: 16000
I0627 04:59:37.574943 139908915062592 checkpoints.py:422] Saved checkpoint at /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0627 04:59:37.594743 139908915062592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0627 04:59:38.847918 139908915062592 submission_runner.py:587] Tuning trial 1/1
I0627 04:59:38.848166 139908915062592 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 04:59:38.853608 139908915062592 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.92686, dtype=float32), 'train/wer': 3.9299398205026757, 'validation/ctc_loss': Array(31.03651, dtype=float32), 'validation/wer': 3.5862188733128155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.115553, dtype=float32), 'test/wer': 3.8258688278187396, 'test/num_examples': 2472, 'score': 59.461833477020264, 'total_duration': 292.82825660705566, 'accumulated_submission_time': 59.461833477020264, 'accumulated_eval_time': 233.36624383926392, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3051, {'train/ctc_loss': Array(3.3917294, dtype=float32), 'train/wer': 0.6823597866161356, 'validation/ctc_loss': Array(3.607257, dtype=float32), 'validation/wer': 0.7091240629432025, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.230003, dtype=float32), 'test/wer': 0.663193386549672, 'test/num_examples': 2472, 'score': 2460.470052719116, 'total_duration': 2813.5549714565277, 'accumulated_submission_time': 2460.470052719116, 'accumulated_eval_time': 353.01953053474426, 'accumulated_logging_time': 0.026746749877929688, 'global_step': 3051, 'preemption_count': 0}), (6023, {'train/ctc_loss': Array(0.7311462, dtype=float32), 'train/wer': 0.242585834909304, 'validation/ctc_loss': Array(1.1313654, dtype=float32), 'validation/wer': 0.30937105037192836, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.76451725, dtype=float32), 'test/wer': 0.24585135986025633, 'test/num_examples': 2472, 'score': 4861.044587612152, 'total_duration': 5343.27553486824, 'accumulated_submission_time': 4861.044587612152, 'accumulated_eval_time': 482.09951543807983, 'accumulated_logging_time': 0.05035591125488281, 'global_step': 6023, 'preemption_count': 0}), (9007, {'train/ctc_loss': Array(0.45195788, dtype=float32), 'train/wer': 0.15927384284516977, 'validation/ctc_loss': Array(0.8551614, dtype=float32), 'validation/wer': 0.24386149408098487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5277823, dtype=float32), 'test/wer': 0.17122661629394917, 'test/num_examples': 2472, 'score': 7261.233250141144, 'total_duration': 7875.435887813568, 'accumulated_submission_time': 7261.233250141144, 'accumulated_eval_time': 613.9955124855042, 'accumulated_logging_time': 0.08127784729003906, 'global_step': 9007, 'preemption_count': 0}), (11996, {'train/ctc_loss': Array(0.40960434, dtype=float32), 'train/wer': 0.13970195519902817, 'validation/ctc_loss': Array(0.7591697, dtype=float32), 'validation/wer': 0.21767696745747667, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45681867, dtype=float32), 'test/wer': 0.15010257347713932, 'test/num_examples': 2472, 'score': 9661.651376962662, 'total_duration': 10406.125103712082, 'accumulated_submission_time': 9661.651376962662, 'accumulated_eval_time': 744.1882276535034, 'accumulated_logging_time': 0.11491918563842773, 'global_step': 11996, 'preemption_count': 0}), (14973, {'train/ctc_loss': Array(0.3597357, dtype=float32), 'train/wer': 0.12554632629541848, 'validation/ctc_loss': Array(0.7016525, dtype=float32), 'validation/wer': 0.19881523217783095, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41626787, dtype=float32), 'test/wer': 0.1350110698108992, 'test/num_examples': 2472, 'score': 12062.307854890823, 'total_duration': 12937.185357570648, 'accumulated_submission_time': 12062.307854890823, 'accumulated_eval_time': 874.5137565135956, 'accumulated_logging_time': 0.148972749710083, 'global_step': 14973, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.38618892, dtype=float32), 'train/wer': 0.1288069948946658, 'validation/ctc_loss': Array(0.70406944, dtype=float32), 'validation/wer': 0.19939410896390702, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4142691, dtype=float32), 'test/wer': 0.1330205350070075, 'test/num_examples': 2472, 'score': 12867.639452695847, 'total_duration': 13872.208798646927, 'accumulated_submission_time': 12867.639452695847, 'accumulated_eval_time': 1004.1419911384583, 'accumulated_logging_time': 0.1930561065673828, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0627 04:59:38.853769 139908915062592 submission_runner.py:590] Timing: 12867.639452695847
I0627 04:59:38.853839 139908915062592 submission_runner.py:591] ====================
I0627 04:59:38.854870 139908915062592 submission_runner.py:659] Final librispeech_deepspeech score: 12867.639452695847
