python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_06-27-2023-18-38-11.log
2023-06-27 18:38:13.861990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 18:38:29.226609 140332482959168 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/librispeech_conformer_jax.
I0627 18:38:30.113845 140332482959168 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0627 18:38:30.114559 140332482959168 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0627 18:38:30.114690 140332482959168 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0627 18:38:30.119668 140332482959168 submission_runner.py:547] Using RNG seed 1003927733
I0627 18:38:32.487496 140332482959168 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 18:38:32.487698 140332482959168 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1.
I0627 18:38:32.487972 140332482959168 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1/hparams.json.
I0627 18:38:32.673763 140332482959168 submission_runner.py:249] Initializing dataset.
I0627 18:38:32.673970 140332482959168 submission_runner.py:256] Initializing model.
I0627 18:38:37.454869 140332482959168 submission_runner.py:268] Initializing optimizer.
I0627 18:38:38.699618 140332482959168 submission_runner.py:275] Initializing metrics bundle.
I0627 18:38:38.699820 140332482959168 submission_runner.py:292] Initializing checkpoint and logger.
I0627 18:38:38.701112 140332482959168 checkpoints.py:915] Found no checkpoint files in /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0627 18:38:38.701400 140332482959168 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 18:38:38.701467 140332482959168 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 18:38:39.633795 140332482959168 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0627 18:38:39.634811 140332482959168 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0627 18:38:39.643123 140332482959168 submission_runner.py:328] Starting training loop.
I0627 18:38:39.942739 140332482959168 input_pipeline.py:20] Loading split = train-clean-100
I0627 18:38:39.980575 140332482959168 input_pipeline.py:20] Loading split = train-clean-360
I0627 18:38:40.413354 140332482959168 input_pipeline.py:20] Loading split = train-other-500
2023-06-27 18:39:52.303681: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-06-27 18:39:55.038009: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0627 18:39:57.040335 140158020081408 logging_writer.py:48] [0] global_step=0, grad_norm=51.741127014160156, loss=31.291975021362305
I0627 18:39:57.065742 140332482959168 spec.py:298] Evaluating on the training split.
I0627 18:39:57.239108 140332482959168 input_pipeline.py:20] Loading split = train-clean-100
I0627 18:39:57.273614 140332482959168 input_pipeline.py:20] Loading split = train-clean-360
I0627 18:39:57.703924 140332482959168 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0627 18:41:15.717677 140332482959168 spec.py:310] Evaluating on the validation split.
I0627 18:41:15.835295 140332482959168 input_pipeline.py:20] Loading split = dev-clean
I0627 18:41:15.841763 140332482959168 input_pipeline.py:20] Loading split = dev-other
I0627 18:42:09.494607 140332482959168 spec.py:326] Evaluating on the test split.
I0627 18:42:09.625893 140332482959168 input_pipeline.py:20] Loading split = test-clean
I0627 18:42:45.277488 140332482959168 submission_runner.py:424] Time since start: 245.63s, 	Step: 1, 	{'train/ctc_loss': Array(31.32539, dtype=float32), 'train/wer': 1.3832122108156508, 'validation/ctc_loss': Array(30.593378, dtype=float32), 'validation/wer': 1.5677719997298576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.663218, dtype=float32), 'test/wer': 1.5799971563788515, 'test/num_examples': 2472, 'score': 77.42240357398987, 'total_duration': 245.63205575942993, 'accumulated_submission_time': 77.42240357398987, 'accumulated_eval_time': 168.20946860313416, 'accumulated_logging_time': 0}
I0627 18:42:45.292179 140152785590016 logging_writer.py:48] [1] accumulated_eval_time=168.209469, accumulated_logging_time=0, accumulated_submission_time=77.422404, global_step=1, preemption_count=0, score=77.422404, test/ctc_loss=30.663217544555664, test/num_examples=2472, test/wer=1.579997, total_duration=245.632056, train/ctc_loss=31.325389862060547, train/wer=1.383212, validation/ctc_loss=30.5933780670166, validation/num_examples=5348, validation/wer=1.567772
I0627 18:43:13.176626 140332482959168 spec.py:298] Evaluating on the training split.
I0627 18:44:00.361971 140332482959168 spec.py:310] Evaluating on the validation split.
I0627 18:44:47.852633 140332482959168 spec.py:326] Evaluating on the test split.
I0627 18:45:11.023953 140332482959168 submission_runner.py:424] Time since start: 391.38s, 	Step: 10, 	{'train/ctc_loss': Array(29.268047, dtype=float32), 'train/wer': 1.0098710634178494, 'validation/ctc_loss': Array(29.030697, dtype=float32), 'validation/wer': 1.3136257947495875, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.087538, dtype=float32), 'test/wer': 1.3144029411167306, 'test/num_examples': 2472, 'score': 105.29171991348267, 'total_duration': 391.37794613838196, 'accumulated_submission_time': 105.29171991348267, 'accumulated_eval_time': 286.0539472103119, 'accumulated_logging_time': 0.02912163734436035}
I0627 18:45:11.042901 140159767783168 logging_writer.py:48] [10] accumulated_eval_time=286.053947, accumulated_logging_time=0.029122, accumulated_submission_time=105.291720, global_step=10, preemption_count=0, score=105.291720, test/ctc_loss=29.08753776550293, test/num_examples=2472, test/wer=1.314403, total_duration=391.377946, train/ctc_loss=29.268047332763672, train/wer=1.009871, validation/ctc_loss=29.030696868896484, validation/num_examples=5348, validation/wer=1.313626
I0627 18:45:11.059750 140159759390464 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=105.291720
I0627 18:45:11.554172 140332482959168 checkpoints.py:490] Saving checkpoint at step: 10
I0627 18:45:13.041048 140332482959168 checkpoints.py:422] Saved checkpoint at /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1/checkpoint_10
I0627 18:45:13.074036 140332482959168 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/librispeech_conformer_jax/trial_1/checkpoint_10.
I0627 18:45:14.205095 140332482959168 submission_runner.py:587] Tuning trial 1/1
I0627 18:45:14.205334 140332482959168 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 18:45:14.210124 140332482959168 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.32539, dtype=float32), 'train/wer': 1.3832122108156508, 'validation/ctc_loss': Array(30.593378, dtype=float32), 'validation/wer': 1.5677719997298576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.663218, dtype=float32), 'test/wer': 1.5799971563788515, 'test/num_examples': 2472, 'score': 77.42240357398987, 'total_duration': 245.63205575942993, 'accumulated_submission_time': 77.42240357398987, 'accumulated_eval_time': 168.20946860313416, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/ctc_loss': Array(29.268047, dtype=float32), 'train/wer': 1.0098710634178494, 'validation/ctc_loss': Array(29.030697, dtype=float32), 'validation/wer': 1.3136257947495875, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.087538, dtype=float32), 'test/wer': 1.3144029411167306, 'test/num_examples': 2472, 'score': 105.29171991348267, 'total_duration': 391.37794613838196, 'accumulated_submission_time': 105.29171991348267, 'accumulated_eval_time': 286.0539472103119, 'accumulated_logging_time': 0.02912163734436035, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0627 18:45:14.210284 140332482959168 submission_runner.py:590] Timing: 105.29171991348267
I0627 18:45:14.210348 140332482959168 submission_runner.py:591] ====================
I0627 18:45:14.210895 140332482959168 submission_runner.py:659] Final librispeech_conformer score: 105.29171991348267
