torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-27-2023-22-08-46.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-27 22:08:51.573113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:08:51.573113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 22:09:04.613205 140306022487872 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0627 22:09:04.613247 140656956577600 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0627 22:09:04.613260 139702844618560 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0627 22:09:04.613292 140503511045952 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0627 22:09:05.586086 140427619710784 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0627 22:09:05.586354 140156517959488 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0627 22:09:05.586575 139771071416128 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0627 22:09:05.594711 140281529718592 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0627 22:09:05.595120 140281529718592 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.596790 140427619710784 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.596928 140156517959488 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.597140 139771071416128 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.603542 140306022487872 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.603565 140656956577600 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.603586 139702844618560 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:09:05.603610 140503511045952 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0627 22:09:06.364756 140281529718592 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/imagenet_vit_pytorch because --overwrite was set.
I0627 22:09:06.368566 140281529718592 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/imagenet_vit_pytorch.
W0627 22:09:06.391834 140156517959488 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.392945 139702844618560 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.393607 140656956577600 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.394332 140427619710784 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.394897 139771071416128 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.395752 140281529718592 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.395893 140503511045952 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:09:06.396085 140306022487872 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 22:09:06.400795 140281529718592 submission_runner.py:547] Using RNG seed 1660974863
I0627 22:09:06.402295 140281529718592 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 22:09:06.402412 140281529718592 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1.
I0627 22:09:06.402657 140281529718592 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0627 22:09:06.403453 140281529718592 submission_runner.py:249] Initializing dataset.
I0627 22:09:12.986983 140281529718592 submission_runner.py:256] Initializing model.
I0627 22:09:17.592842 140281529718592 submission_runner.py:268] Initializing optimizer.
I0627 22:09:17.594537 140281529718592 submission_runner.py:275] Initializing metrics bundle.
I0627 22:09:17.594642 140281529718592 submission_runner.py:292] Initializing checkpoint and logger.
I0627 22:09:18.302665 140281529718592 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0627 22:09:18.304306 140281529718592 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0627 22:09:18.351760 140281529718592 submission_runner.py:328] Starting training loop.
I0627 22:09:25.693085 140253675374336 logging_writer.py:48] [0] global_step=0, grad_norm=0.335680, loss=6.907756
I0627 22:09:25.709939 140281529718592 submission.py:119] 0) loss = 6.908, grad_norm = 0.336
I0627 22:09:25.855742 140281529718592 spec.py:298] Evaluating on the training split.
I0627 22:10:27.016964 140281529718592 spec.py:310] Evaluating on the validation split.
I0627 22:11:11.350610 140281529718592 spec.py:326] Evaluating on the test split.
I0627 22:11:11.364494 140281529718592 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0627 22:11:11.370241 140281529718592 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0627 22:11:11.432215 140281529718592 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0627 22:11:25.546988 140281529718592 submission_runner.py:424] Time since start: 127.20s, 	Step: 1, 	{'train/accuracy': 0.0019140625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00196, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0021, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 7.502917289733887, 'total_duration': 127.19568347930908, 'accumulated_submission_time': 7.502917289733887, 'accumulated_eval_time': 119.69120287895203, 'accumulated_logging_time': 0}
I0627 22:11:25.555167 140248700942080 logging_writer.py:48] [1] accumulated_eval_time=119.691203, accumulated_logging_time=0, accumulated_submission_time=7.502917, global_step=1, preemption_count=0, score=7.502917, test/accuracy=0.002100, test/loss=6.907755, test/num_examples=10000, total_duration=127.195683, train/accuracy=0.001914, train/loss=6.907756, validation/accuracy=0.001960, validation/loss=6.907756, validation/num_examples=50000
I0627 22:11:25.577152 140281529718592 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.577169 140503511045952 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.577349 139702844618560 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.577496 140156517959488 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.578487 140427619710784 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.578529 140656956577600 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.578635 139771071416128 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:25.578713 140306022487872 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:11:26.145097 140248692549376 logging_writer.py:48] [1] global_step=1, grad_norm=0.350571, loss=6.907756
I0627 22:11:26.148758 140281529718592 submission.py:119] 1) loss = 6.908, grad_norm = 0.351
I0627 22:11:26.536293 140248700942080 logging_writer.py:48] [2] global_step=2, grad_norm=0.352258, loss=6.907755
I0627 22:11:26.541697 140281529718592 submission.py:119] 2) loss = 6.908, grad_norm = 0.352
I0627 22:11:26.931718 140248692549376 logging_writer.py:48] [3] global_step=3, grad_norm=0.345748, loss=6.907752
I0627 22:11:26.937337 140281529718592 submission.py:119] 3) loss = 6.908, grad_norm = 0.346
I0627 22:11:27.334833 140248700942080 logging_writer.py:48] [4] global_step=4, grad_norm=0.341500, loss=6.907755
I0627 22:11:27.339637 140281529718592 submission.py:119] 4) loss = 6.908, grad_norm = 0.342
I0627 22:11:27.736057 140248692549376 logging_writer.py:48] [5] global_step=5, grad_norm=0.340603, loss=6.907756
I0627 22:11:27.743080 140281529718592 submission.py:119] 5) loss = 6.908, grad_norm = 0.341
I0627 22:11:28.137555 140248700942080 logging_writer.py:48] [6] global_step=6, grad_norm=0.354451, loss=6.907739
I0627 22:11:28.142406 140281529718592 submission.py:119] 6) loss = 6.908, grad_norm = 0.354
I0627 22:11:28.549440 140248692549376 logging_writer.py:48] [7] global_step=7, grad_norm=0.359918, loss=6.907744
I0627 22:11:28.555754 140281529718592 submission.py:119] 7) loss = 6.908, grad_norm = 0.360
I0627 22:11:28.951293 140248700942080 logging_writer.py:48] [8] global_step=8, grad_norm=0.346623, loss=6.907734
I0627 22:11:28.957088 140281529718592 submission.py:119] 8) loss = 6.908, grad_norm = 0.347
I0627 22:11:29.351715 140248692549376 logging_writer.py:48] [9] global_step=9, grad_norm=0.354403, loss=6.907739
I0627 22:11:29.356238 140281529718592 submission.py:119] 9) loss = 6.908, grad_norm = 0.354
I0627 22:11:29.358812 140281529718592 spec.py:298] Evaluating on the training split.
I0627 22:12:15.348410 140281529718592 spec.py:310] Evaluating on the validation split.
I0627 22:13:02.706648 140281529718592 spec.py:326] Evaluating on the test split.
I0627 22:13:04.163429 140281529718592 submission_runner.py:424] Time since start: 225.81s, 	Step: 10, 	{'train/accuracy': 0.0029296875, 'train/loss': 6.907685546875, 'validation/accuracy': 0.00246, 'validation/loss': 6.90769375, 'validation/num_examples': 50000, 'test/accuracy': 0.0027, 'test/loss': 6.90770234375, 'test/num_examples': 10000, 'score': 11.284852266311646, 'total_duration': 225.81225991249084, 'accumulated_submission_time': 11.284852266311646, 'accumulated_eval_time': 214.4959659576416, 'accumulated_logging_time': 0.0220186710357666}
I0627 22:13:04.171156 140240471713536 logging_writer.py:48] [10] accumulated_eval_time=214.495966, accumulated_logging_time=0.022019, accumulated_submission_time=11.284852, global_step=10, preemption_count=0, score=11.284852, test/accuracy=0.002700, test/loss=6.907702, test/num_examples=10000, total_duration=225.812260, train/accuracy=0.002930, train/loss=6.907686, validation/accuracy=0.002460, validation/loss=6.907694, validation/num_examples=50000
I0627 22:13:04.188118 140240480106240 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=11.284852
I0627 22:13:04.741858 140281529718592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/imagenet_vit_pytorch/trial_1/checkpoint_10.
I0627 22:13:05.039977 140281529718592 submission_runner.py:587] Tuning trial 1/1
I0627 22:13:05.040203 140281529718592 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 22:13:05.040544 140281529718592 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0019140625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00196, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0021, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 7.502917289733887, 'total_duration': 127.19568347930908, 'accumulated_submission_time': 7.502917289733887, 'accumulated_eval_time': 119.69120287895203, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.0029296875, 'train/loss': 6.907685546875, 'validation/accuracy': 0.00246, 'validation/loss': 6.90769375, 'validation/num_examples': 50000, 'test/accuracy': 0.0027, 'test/loss': 6.90770234375, 'test/num_examples': 10000, 'score': 11.284852266311646, 'total_duration': 225.81225991249084, 'accumulated_submission_time': 11.284852266311646, 'accumulated_eval_time': 214.4959659576416, 'accumulated_logging_time': 0.0220186710357666, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0627 22:13:05.040658 140281529718592 submission_runner.py:590] Timing: 11.284852266311646
I0627 22:13:05.040717 140281529718592 submission_runner.py:591] ====================
I0627 22:13:05.040853 140281529718592 submission_runner.py:659] Final imagenet_vit score: 11.284852266311646
