I0309 23:25:00.992125 139667344451392 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/wmt_jax.
I0309 23:25:01.043680 139667344451392 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0309 23:25:02.014793 139667344451392 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0309 23:25:02.015362 139667344451392 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0309 23:25:02.018069 139667344451392 submission_runner.py:481] Using RNG seed 2264256066
I0309 23:25:03.362062 139667344451392 submission_runner.py:490] --- Tuning run 1/1 ---
I0309 23:25:03.362284 139667344451392 submission_runner.py:495] Creating tuning directory at /experiment_runs/timing/wmt_jax/trial_1.
I0309 23:25:03.362461 139667344451392 logger_utils.py:84] Saving hparams to /experiment_runs/timing/wmt_jax/trial_1/hparams.json.
I0309 23:25:03.483675 139667344451392 submission_runner.py:226] Initializing dataset.
I0309 23:25:03.490272 139667344451392 dataset_info.py:539] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0309 23:25:03.492777 139667344451392 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0309 23:25:03.492895 139667344451392 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0309 23:25:03.560696 139667344451392 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0309 23:25:05.310222 139667344451392 submission_runner.py:233] Initializing model.
I0309 23:25:16.946410 139667344451392 submission_runner.py:243] Initializing optimizer.
I0309 23:25:17.807708 139667344451392 submission_runner.py:250] Initializing metrics bundle.
I0309 23:25:17.807893 139667344451392 submission_runner.py:265] Initializing checkpoint and logger.
I0309 23:25:17.808739 139667344451392 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/wmt_jax/trial_1 with prefix checkpoint_
I0309 23:25:18.555376 139667344451392 submission_runner.py:286] Saving meta data to /experiment_runs/timing/wmt_jax/trial_1/meta_data_0.json.
I0309 23:25:18.556340 139667344451392 submission_runner.py:289] Saving flags to /experiment_runs/timing/wmt_jax/trial_1/flags_0.json.
I0309 23:25:18.558929 139667344451392 submission_runner.py:299] Starting training loop.
I0309 23:25:53.093055 139491100583680 logging_writer.py:48] [0] global_step=0, grad_norm=5.099231719970703, loss=10.976582527160645
I0309 23:25:53.106194 139667344451392 spec.py:298] Evaluating on the training split.
I0309 23:25:53.108777 139667344451392 dataset_info.py:539] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0309 23:25:53.110991 139667344451392 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0309 23:25:53.111089 139667344451392 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0309 23:25:53.139873 139667344451392 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0309 23:26:01.162761 139667344451392 workload.py:179] Translating evaluation dataset.
I0309 23:31:05.262500 139667344451392 spec.py:310] Evaluating on the validation split.
I0309 23:31:05.265325 139667344451392 dataset_info.py:539] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0309 23:31:05.267861 139667344451392 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0309 23:31:05.267965 139667344451392 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0309 23:31:05.296572 139667344451392 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0309 23:31:12.998399 139667344451392 workload.py:179] Translating evaluation dataset.
I0309 23:36:11.051646 139667344451392 spec.py:326] Evaluating on the test split.
I0309 23:36:11.054956 139667344451392 dataset_info.py:539] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0309 23:36:11.058222 139667344451392 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0309 23:36:11.058362 139667344451392 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0309 23:36:11.088062 139667344451392 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0309 23:36:13.926725 139667344451392 workload.py:179] Translating evaluation dataset.
I0309 23:41:05.855111 139667344451392 submission_runner.py:359] Time since start: 34.55s, 	Step: 1, 	{'train/accuracy': 0.0005486090667545795, 'train/loss': 11.013607025146484, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.025561332702637, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.03548812866211, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0309 23:41:05.862692 139480127301376 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=34.351663, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.035488, test/num_examples=3003, total_duration=34.547214, train/accuracy=0.000549, train/bleu=0.000000, train/loss=11.013607, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.025561, validation/num_examples=3000
I0309 23:41:06.964314 139667344451392 checkpoints.py:356] Saving checkpoint at step: 1
I0309 23:41:11.121278 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_1
I0309 23:41:11.125730 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_1.
I0309 23:41:49.114897 139480118908672 logging_writer.py:48] [100] global_step=100, grad_norm=0.1423218697309494, loss=8.192728042602539
I0309 23:42:27.111665 139479692109568 logging_writer.py:48] [200] global_step=200, grad_norm=0.42168816924095154, loss=7.314687728881836
I0309 23:43:05.080481 139480118908672 logging_writer.py:48] [300] global_step=300, grad_norm=0.3393026292324066, loss=6.657696723937988
I0309 23:43:43.116838 139479692109568 logging_writer.py:48] [400] global_step=400, grad_norm=0.47561824321746826, loss=6.063244342803955
I0309 23:44:21.159335 139480118908672 logging_writer.py:48] [500] global_step=500, grad_norm=0.39626988768577576, loss=5.68543815612793
I0309 23:44:59.238323 139479692109568 logging_writer.py:48] [600] global_step=600, grad_norm=0.5083514451980591, loss=5.371035575866699
I0309 23:45:37.335170 139480118908672 logging_writer.py:48] [700] global_step=700, grad_norm=0.6572881937026978, loss=5.1195969581604
I0309 23:46:15.488665 139479692109568 logging_writer.py:48] [800] global_step=800, grad_norm=0.588981568813324, loss=4.862363815307617
I0309 23:46:53.527358 139480118908672 logging_writer.py:48] [900] global_step=900, grad_norm=0.547045111656189, loss=4.6148786544799805
I0309 23:47:31.598680 139479692109568 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4652865529060364, loss=4.277057647705078
I0309 23:48:09.665003 139480118908672 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.49810150265693665, loss=4.069201469421387
I0309 23:48:47.733313 139479692109568 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.49766016006469727, loss=3.8676562309265137
I0309 23:49:25.773489 139480118908672 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5774275064468384, loss=3.810802698135376
I0309 23:50:03.808880 139479692109568 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.47722750902175903, loss=3.6510767936706543
I0309 23:50:41.949596 139480118908672 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.47664937376976013, loss=3.4014554023742676
I0309 23:51:20.006323 139479692109568 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4623175859451294, loss=3.3608853816986084
I0309 23:51:58.074159 139480118908672 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.4106070101261139, loss=3.2490413188934326
I0309 23:52:36.159029 139479692109568 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.444482684135437, loss=3.2543845176696777
I0309 23:53:14.256453 139480118908672 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.31168898940086365, loss=3.198105573654175
I0309 23:53:52.331038 139479692109568 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3695931136608124, loss=3.093230962753296
I0309 23:54:30.375290 139480118908672 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.3629981577396393, loss=3.008986234664917
I0309 23:55:08.442530 139479692109568 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.34733736515045166, loss=3.0033020973205566
I0309 23:55:11.204303 139667344451392 spec.py:298] Evaluating on the training split.
I0309 23:55:14.193351 139667344451392 workload.py:179] Translating evaluation dataset.
I0309 23:58:06.740311 139667344451392 spec.py:310] Evaluating on the validation split.
I0309 23:58:09.383553 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:00:40.818088 139667344451392 spec.py:326] Evaluating on the test split.
I0310 00:00:43.506922 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:03:10.688562 139667344451392 submission_runner.py:359] Time since start: 1792.65s, 	Step: 2209, 	{'train/accuracy': 0.5148157477378845, 'train/loss': 2.8249032497406006, 'train/bleu': 22.7281047693171, 'validation/accuracy': 0.5195719599723816, 'validation/loss': 2.79937481880188, 'validation/bleu': 18.72165831519777, 'validation/num_examples': 3000, 'test/accuracy': 0.5158329010009766, 'test/loss': 2.856799602508545, 'test/bleu': 17.019463688338895, 'test/num_examples': 3003}
I0310 00:03:10.696525 139480118908672 logging_writer.py:48] [2209] global_step=2209, preemption_count=0, score=870.787711, test/accuracy=0.515833, test/bleu=17.019464, test/loss=2.856800, test/num_examples=3003, total_duration=1792.645325, train/accuracy=0.514816, train/bleu=22.728105, train/loss=2.824903, validation/accuracy=0.519572, validation/bleu=18.721658, validation/loss=2.799375, validation/num_examples=3000
I0310 00:03:11.641552 139667344451392 checkpoints.py:356] Saving checkpoint at step: 2209
I0310 00:03:14.918279 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_2209
I0310 00:03:14.922641 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_2209.
I0310 00:03:49.825180 139479692109568 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.27119168639183044, loss=2.8713455200195312
I0310 00:04:27.830533 139479683716864 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2822715640068054, loss=2.8996615409851074
I0310 00:05:05.889261 139479692109568 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3475717008113861, loss=2.887910842895508
I0310 00:05:43.969567 139479683716864 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.22660766541957855, loss=2.721160411834717
I0310 00:06:22.061373 139479692109568 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.2554948031902313, loss=2.70837664604187
I0310 00:07:00.092191 139479683716864 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.22343537211418152, loss=2.6494903564453125
I0310 00:07:38.197645 139479692109568 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.3201967179775238, loss=2.650952100753784
I0310 00:08:16.264370 139479683716864 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.19549578428268433, loss=2.518693208694458
I0310 00:08:54.329510 139479692109568 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.23450592160224915, loss=2.5573325157165527
I0310 00:09:32.385205 139479683716864 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.21287688612937927, loss=2.6037657260894775
I0310 00:10:10.492253 139479692109568 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.221567302942276, loss=2.481872081756592
I0310 00:10:48.552295 139479683716864 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.19007138907909393, loss=2.546384811401367
I0310 00:11:26.615270 139479692109568 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.1881968379020691, loss=2.4480206966400146
I0310 00:12:04.705898 139479683716864 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.19532889127731323, loss=2.5311973094940186
I0310 00:12:42.812426 139479692109568 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.1727464199066162, loss=2.4684903621673584
I0310 00:13:20.846501 139479683716864 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.17370785772800446, loss=2.420926809310913
I0310 00:13:58.926885 139479692109568 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.19652098417282104, loss=2.36785888671875
I0310 00:14:37.013435 139479683716864 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.17165249586105347, loss=2.503910541534424
I0310 00:15:15.080054 139479692109568 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.16685929894447327, loss=2.4092957973480225
I0310 00:15:53.184371 139479683716864 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.1862952560186386, loss=2.3898725509643555
I0310 00:16:31.237178 139479692109568 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.1670427918434143, loss=2.2708773612976074
I0310 00:17:09.350437 139479683716864 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.16975189745426178, loss=2.305635452270508
I0310 00:17:15.143135 139667344451392 spec.py:298] Evaluating on the training split.
I0310 00:17:18.122550 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:20:13.893331 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 00:20:16.530074 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:22:39.196682 139667344451392 spec.py:326] Evaluating on the test split.
I0310 00:22:41.886988 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:24:57.663451 139667344451392 submission_runner.py:359] Time since start: 3116.58s, 	Step: 4417, 	{'train/accuracy': 0.5784480571746826, 'train/loss': 2.2423603534698486, 'train/bleu': 27.17698925930642, 'validation/accuracy': 0.5891805291175842, 'validation/loss': 2.1514506340026855, 'validation/bleu': 23.289699184267153, 'validation/num_examples': 3000, 'test/accuracy': 0.5924699306488037, 'test/loss': 2.1272497177124023, 'test/bleu': 22.156263772275942, 'test/num_examples': 3003}
I0310 00:24:57.671271 139479692109568 logging_writer.py:48] [4417] global_step=4417, preemption_count=0, score=1707.830520, test/accuracy=0.592470, test/bleu=22.156264, test/loss=2.127250, test/num_examples=3003, total_duration=3116.584163, train/accuracy=0.578448, train/bleu=27.176989, train/loss=2.242360, validation/accuracy=0.589181, validation/bleu=23.289699, validation/loss=2.151451, validation/num_examples=3000
I0310 00:24:58.609790 139667344451392 checkpoints.py:356] Saving checkpoint at step: 4417
I0310 00:25:01.903715 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_4417
I0310 00:25:01.908274 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_4417.
I0310 00:25:33.828299 139479683716864 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.16403092443943024, loss=2.3461415767669678
I0310 00:26:11.861968 139479675324160 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.15952174365520477, loss=2.24401593208313
I0310 00:26:49.951270 139479683716864 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.16684627532958984, loss=2.2108118534088135
I0310 00:27:27.977366 139479675324160 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1688452810049057, loss=2.292768955230713
I0310 00:28:05.994408 139479683716864 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.18112294375896454, loss=2.2826955318450928
I0310 00:28:44.077389 139479675324160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.17513969540596008, loss=2.2629590034484863
I0310 00:29:22.156414 139479683716864 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.17330241203308105, loss=2.1882569789886475
I0310 00:30:00.224449 139479675324160 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.1446918100118637, loss=2.2495670318603516
I0310 00:30:38.341380 139479683716864 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.15493768453598022, loss=2.2646470069885254
I0310 00:31:16.343697 139479675324160 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.14510925114154816, loss=2.1718926429748535
I0310 00:31:54.384342 139479683716864 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.1719433069229126, loss=2.3396620750427246
I0310 00:32:32.487418 139479675324160 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.16726335883140564, loss=2.2120749950408936
I0310 00:33:10.550395 139479683716864 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.18482747673988342, loss=2.329242706298828
I0310 00:33:48.574985 139479675324160 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.1525256186723709, loss=2.1413187980651855
I0310 00:34:26.634834 139479683716864 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.15975823998451233, loss=2.2192046642303467
I0310 00:35:04.719458 139479675324160 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.15048913657665253, loss=2.128103494644165
I0310 00:35:42.738377 139479683716864 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.14368848502635956, loss=2.1424612998962402
I0310 00:36:20.829734 139479675324160 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.153009295463562, loss=2.102982759475708
I0310 00:36:58.915439 139479683716864 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.18958650529384613, loss=2.129026412963867
I0310 00:37:37.016983 139479675324160 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.1590881496667862, loss=2.108508586883545
I0310 00:38:15.112448 139479683716864 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1437206268310547, loss=2.204637050628662
I0310 00:38:53.131718 139479675324160 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.13766856491565704, loss=2.138099431991577
I0310 00:39:01.983159 139667344451392 spec.py:298] Evaluating on the training split.
I0310 00:39:04.980597 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:41:40.373833 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 00:41:43.014466 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:43:55.557754 139667344451392 spec.py:326] Evaluating on the test split.
I0310 00:43:58.239622 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 00:46:09.434833 139667344451392 submission_runner.py:359] Time since start: 4423.42s, 	Step: 6625, 	{'train/accuracy': 0.6107933521270752, 'train/loss': 1.9620240926742554, 'train/bleu': 29.187282307733632, 'validation/accuracy': 0.617053747177124, 'validation/loss': 1.932531476020813, 'validation/bleu': 25.380176309014352, 'validation/num_examples': 3000, 'test/accuracy': 0.6198826432228088, 'test/loss': 1.8891469240188599, 'test/bleu': 23.87154884445195, 'test/num_examples': 3003}
I0310 00:46:09.443173 139479683716864 logging_writer.py:48] [6625] global_step=6625, preemption_count=0, score=2544.720065, test/accuracy=0.619883, test/bleu=23.871549, test/loss=1.889147, test/num_examples=3003, total_duration=4423.424188, train/accuracy=0.610793, train/bleu=29.187282, train/loss=1.962024, validation/accuracy=0.617054, validation/bleu=25.380176, validation/loss=1.932531, validation/num_examples=3000
I0310 00:46:10.394100 139667344451392 checkpoints.py:356] Saving checkpoint at step: 6625
I0310 00:46:13.679648 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_6625
I0310 00:46:13.684113 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_6625.
I0310 00:46:42.550923 139479675324160 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1496083289384842, loss=2.092970371246338
I0310 00:47:20.581555 139479666931456 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.19201073050498962, loss=2.2228193283081055
I0310 00:47:58.649497 139479675324160 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.1734744906425476, loss=2.1315958499908447
I0310 00:48:36.714991 139479666931456 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.15561728179454803, loss=2.1146349906921387
I0310 00:49:14.802606 139479675324160 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.20079118013381958, loss=2.2356626987457275
I0310 00:49:52.894472 139479666931456 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.15593309700489044, loss=2.1314971446990967
I0310 00:50:30.991193 139479675324160 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1768457591533661, loss=2.0347936153411865
I0310 00:51:09.073216 139479666931456 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.16723497211933136, loss=2.1047723293304443
I0310 00:51:47.228137 139479675324160 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.22721166908740997, loss=2.0023341178894043
I0310 00:52:25.374183 139479666931456 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.15438778698444366, loss=2.0845484733581543
I0310 00:53:03.460622 139479675324160 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.15481004118919373, loss=2.1266634464263916
I0310 00:53:41.573252 139479666931456 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.17516252398490906, loss=2.112961530685425
I0310 00:54:19.713266 139479675324160 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.18591763079166412, loss=2.0537450313568115
I0310 00:54:57.798082 139479666931456 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.21807006001472473, loss=2.0721611976623535
I0310 00:55:35.932031 139479675324160 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.15328681468963623, loss=2.199352264404297
I0310 00:56:13.998327 139479666931456 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17496973276138306, loss=2.0920958518981934
I0310 00:56:52.073027 139479675324160 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.1702239215373993, loss=2.0405378341674805
I0310 00:57:30.165556 139479666931456 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.15416471660137177, loss=2.061809539794922
I0310 00:58:08.288584 139479675324160 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.17347198724746704, loss=2.0290050506591797
I0310 00:58:46.341310 139479666931456 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.14683987200260162, loss=1.9657317399978638
I0310 00:59:24.463145 139479675324160 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.2601570785045624, loss=2.0243775844573975
I0310 01:00:02.570266 139479666931456 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.14619944989681244, loss=1.9898179769515991
I0310 01:00:13.723176 139667344451392 spec.py:298] Evaluating on the training split.
I0310 01:00:16.716039 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:03:24.495633 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 01:03:27.140393 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:05:47.442382 139667344451392 spec.py:326] Evaluating on the test split.
I0310 01:05:50.144690 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:08:05.239729 139667344451392 submission_runner.py:359] Time since start: 5695.16s, 	Step: 8831, 	{'train/accuracy': 0.6131600737571716, 'train/loss': 1.950272798538208, 'train/bleu': 29.461344004550806, 'validation/accuracy': 0.6304075717926025, 'validation/loss': 1.8245033025741577, 'validation/bleu': 26.1477890754307, 'validation/num_examples': 3000, 'test/accuracy': 0.6371855139732361, 'test/loss': 1.7693054676055908, 'test/bleu': 25.083469679290186, 'test/num_examples': 3003}
I0310 01:08:05.249296 139479675324160 logging_writer.py:48] [8831] global_step=8831, preemption_count=0, score=3381.259720, test/accuracy=0.637186, test/bleu=25.083470, test/loss=1.769305, test/num_examples=3003, total_duration=5695.164191, train/accuracy=0.613160, train/bleu=29.461344, train/loss=1.950273, validation/accuracy=0.630408, validation/bleu=26.147789, validation/loss=1.824503, validation/num_examples=3000
I0310 01:08:06.593006 139667344451392 checkpoints.py:356] Saving checkpoint at step: 8831
I0310 01:08:10.948308 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_8831
I0310 01:08:10.951832 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_8831.
I0310 01:08:37.526226 139479666931456 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.14730480313301086, loss=2.082526922225952
I0310 01:09:15.536916 139479658538752 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.17160122096538544, loss=2.087958574295044
I0310 01:09:53.568776 139479666931456 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.16634559631347656, loss=1.9777334928512573
I0310 01:10:31.646854 139479658538752 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1885291188955307, loss=2.024373769760132
I0310 01:11:09.671168 139479666931456 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.23079293966293335, loss=1.984999656677246
I0310 01:11:47.784312 139479658538752 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.2049158662557602, loss=1.9805972576141357
I0310 01:12:25.859464 139479666931456 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.14914895594120026, loss=1.9667367935180664
I0310 01:13:03.985440 139479658538752 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.16484327614307404, loss=2.1150081157684326
I0310 01:13:42.060677 139479666931456 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15196050703525543, loss=1.98756742477417
I0310 01:14:20.125244 139479658538752 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.18830440938472748, loss=2.0036895275115967
I0310 01:14:58.237042 139479666931456 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16827397048473358, loss=1.9890596866607666
I0310 01:15:36.405039 139479658538752 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1950850635766983, loss=1.9594870805740356
I0310 01:16:14.448199 139479666931456 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.17241321504116058, loss=1.9955495595932007
I0310 01:16:52.535329 139479658538752 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.13905954360961914, loss=1.9430341720581055
I0310 01:17:30.612416 139479666931456 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.14979080855846405, loss=2.0614118576049805
I0310 01:18:08.713256 139479658538752 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.19560126960277557, loss=1.9943879842758179
I0310 01:18:46.831497 139479666931456 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.23462189733982086, loss=2.0398826599121094
I0310 01:19:24.902132 139479658538752 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.1679953634738922, loss=1.9841457605361938
I0310 01:20:02.989424 139479666931456 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.17843623459339142, loss=2.0657594203948975
I0310 01:20:40.988171 139479658538752 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1501864641904831, loss=2.095444440841675
I0310 01:21:19.000630 139479666931456 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1662716120481491, loss=2.0744450092315674
I0310 01:21:57.042979 139479658538752 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.26330870389938354, loss=1.953921914100647
I0310 01:22:11.225434 139667344451392 spec.py:298] Evaluating on the training split.
I0310 01:22:14.200878 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:25:01.170582 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 01:25:03.810867 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:27:28.699908 139667344451392 spec.py:326] Evaluating on the test split.
I0310 01:27:31.387451 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:29:39.133676 139667344451392 submission_runner.py:359] Time since start: 7012.67s, 	Step: 11039, 	{'train/accuracy': 0.6168193221092224, 'train/loss': 1.9161125421524048, 'train/bleu': 30.29042642640719, 'validation/accuracy': 0.6386653780937195, 'validation/loss': 1.7650939226150513, 'validation/bleu': 26.871443771245396, 'validation/num_examples': 3000, 'test/accuracy': 0.6444599628448486, 'test/loss': 1.7024526596069336, 'test/bleu': 25.6739794667213, 'test/num_examples': 3003}
I0310 01:29:39.142100 139479666931456 logging_writer.py:48] [11039] global_step=11039, preemption_count=0, score=4218.180895, test/accuracy=0.644460, test/bleu=25.673979, test/loss=1.702453, test/num_examples=3003, total_duration=7012.666456, train/accuracy=0.616819, train/bleu=30.290426, train/loss=1.916113, validation/accuracy=0.638665, validation/bleu=26.871444, validation/loss=1.765094, validation/num_examples=3000
I0310 01:29:40.175318 139667344451392 checkpoints.py:356] Saving checkpoint at step: 11039
I0310 01:29:44.370993 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_11039
I0310 01:29:44.375430 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_11039.
I0310 01:30:07.920068 139479658538752 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.15939708054065704, loss=1.9225130081176758
I0310 01:30:45.855522 139479650146048 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.16905243694782257, loss=1.967073678970337
I0310 01:31:23.850589 139479658538752 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1804351508617401, loss=1.9759382009506226
I0310 01:32:01.915419 139479650146048 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.21262963116168976, loss=1.9835363626480103
I0310 01:32:39.998502 139479658538752 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.15510907769203186, loss=1.9912840127944946
I0310 01:33:18.098170 139479650146048 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.17648132145404816, loss=1.9052953720092773
I0310 01:33:56.132490 139479658538752 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.153736874461174, loss=2.030651092529297
I0310 01:34:34.230323 139479650146048 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.15775826573371887, loss=1.8658699989318848
I0310 01:35:12.291600 139479658538752 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.15706351399421692, loss=1.944530725479126
I0310 01:35:50.375400 139479650146048 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.16092193126678467, loss=1.9859331846237183
I0310 01:36:28.514418 139479658538752 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.15802474319934845, loss=1.9310053586959839
I0310 01:37:06.557660 139479650146048 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.16177256405353546, loss=1.9576841592788696
I0310 01:37:44.626127 139479658538752 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2423354983329773, loss=1.987661600112915
I0310 01:38:22.721315 139479650146048 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3914682865142822, loss=1.9024955034255981
I0310 01:39:00.865119 139479658538752 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.17396439611911774, loss=1.9284862279891968
I0310 01:39:38.945641 139479650146048 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.1552543193101883, loss=1.955238938331604
I0310 01:40:17.021534 139479658538752 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.17842738330364227, loss=1.9038788080215454
I0310 01:40:55.079284 139479650146048 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.16675814986228943, loss=2.0121777057647705
I0310 01:41:33.176158 139479658538752 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.15605385601520538, loss=1.8820127248764038
I0310 01:42:11.267316 139479650146048 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2060099095106125, loss=1.933701515197754
I0310 01:42:49.387062 139479658538752 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2047346532344818, loss=1.8698859214782715
I0310 01:43:27.467691 139479650146048 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.18233703076839447, loss=1.9456512928009033
I0310 01:43:44.678320 139667344451392 spec.py:298] Evaluating on the training split.
I0310 01:43:47.666455 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:46:56.108141 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 01:46:58.749992 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:49:21.617420 139667344451392 spec.py:326] Evaluating on the test split.
I0310 01:49:24.297905 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 01:51:35.162163 139667344451392 submission_runner.py:359] Time since start: 8306.12s, 	Step: 13247, 	{'train/accuracy': 0.6304225921630859, 'train/loss': 1.7965344190597534, 'train/bleu': 30.365646850061317, 'validation/accuracy': 0.6435133814811707, 'validation/loss': 1.7158416509628296, 'validation/bleu': 26.9875655178886, 'validation/num_examples': 3000, 'test/accuracy': 0.6513044238090515, 'test/loss': 1.6521775722503662, 'test/bleu': 26.157470880858774, 'test/num_examples': 3003}
I0310 01:51:35.172178 139479658538752 logging_writer.py:48] [13247] global_step=13247, preemption_count=0, score=5055.267154, test/accuracy=0.651304, test/bleu=26.157471, test/loss=1.652178, test/num_examples=3003, total_duration=8306.119352, train/accuracy=0.630423, train/bleu=30.365647, train/loss=1.796534, validation/accuracy=0.643513, validation/bleu=26.987566, validation/loss=1.715842, validation/num_examples=3000
I0310 01:51:36.135812 139667344451392 checkpoints.py:356] Saving checkpoint at step: 13247
I0310 01:51:39.790372 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_13247
I0310 01:51:39.794887 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_13247.
I0310 01:52:00.289201 139479650146048 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.30062615871429443, loss=1.8990397453308105
I0310 01:52:38.278490 139479641753344 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.15942341089248657, loss=2.015540838241577
I0310 01:53:16.291177 139479650146048 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16694441437721252, loss=1.9349411725997925
I0310 01:53:54.290012 139479641753344 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.22272922098636627, loss=1.9870127439498901
I0310 01:54:32.356955 139479650146048 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.15518537163734436, loss=1.9387288093566895
I0310 01:55:10.430760 139479641753344 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.18567609786987305, loss=2.0277740955352783
I0310 01:55:48.495218 139479650146048 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.20272547006607056, loss=1.9237432479858398
I0310 01:56:26.548156 139479641753344 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.15611745417118073, loss=1.9657530784606934
I0310 01:57:04.691746 139479650146048 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.16784627735614777, loss=1.82970130443573
I0310 01:57:42.762588 139479641753344 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.19853922724723816, loss=1.9421725273132324
I0310 01:58:20.793457 139479650146048 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1761256456375122, loss=1.8740003108978271
I0310 01:58:58.842938 139479641753344 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.16029870510101318, loss=1.9541469812393188
I0310 01:59:36.936028 139479650146048 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.19364966452121735, loss=1.897984266281128
I0310 02:00:14.959983 139479641753344 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.17582516372203827, loss=1.9222878217697144
I0310 02:00:53.007377 139479650146048 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.18841701745986938, loss=1.9091135263442993
I0310 02:01:31.119314 139479641753344 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.20532813668251038, loss=1.9524009227752686
I0310 02:02:09.265723 139479650146048 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.17725636065006256, loss=1.8558214902877808
I0310 02:02:47.310281 139479641753344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.20436173677444458, loss=1.8349908590316772
I0310 02:03:25.475975 139479650146048 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.19844958186149597, loss=1.902063250541687
I0310 02:04:03.562735 139479641753344 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.15884873270988464, loss=2.014270067214966
I0310 02:04:41.630625 139479650146048 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2618855834007263, loss=1.9566845893859863
I0310 02:05:19.725501 139479641753344 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3230130970478058, loss=1.937645435333252
I0310 02:05:39.963765 139667344451392 spec.py:298] Evaluating on the training split.
I0310 02:05:42.955595 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:09:48.810770 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 02:09:51.446310 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:13:25.828222 139667344451392 spec.py:326] Evaluating on the test split.
I0310 02:13:28.519629 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:16:30.171847 139667344451392 submission_runner.py:359] Time since start: 9621.40s, 	Step: 15455, 	{'train/accuracy': 0.6287755966186523, 'train/loss': 1.8220529556274414, 'train/bleu': 30.291904551234712, 'validation/accuracy': 0.6475927233695984, 'validation/loss': 1.6832470893859863, 'validation/bleu': 27.30646174819221, 'validation/num_examples': 3000, 'test/accuracy': 0.6590901613235474, 'test/loss': 1.608644723892212, 'test/bleu': 26.967221303543717, 'test/num_examples': 3003}
I0310 02:16:30.180755 139479650146048 logging_writer.py:48] [15455] global_step=15455, preemption_count=0, score=5892.196576, test/accuracy=0.659090, test/bleu=26.967221, test/loss=1.608645, test/num_examples=3003, total_duration=9621.404798, train/accuracy=0.628776, train/bleu=30.291905, train/loss=1.822053, validation/accuracy=0.647593, validation/bleu=27.306462, validation/loss=1.683247, validation/num_examples=3000
I0310 02:16:31.111336 139667344451392 checkpoints.py:356] Saving checkpoint at step: 15455
I0310 02:16:34.402923 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_15455
I0310 02:16:34.407489 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_15455.
I0310 02:16:51.875868 139479641753344 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.22498925030231476, loss=1.8812769651412964
I0310 02:17:29.856063 139479633360640 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.18205714225769043, loss=1.9809226989746094
I0310 02:18:07.772519 139479641753344 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.20572637021541595, loss=1.925330400466919
I0310 02:18:45.829875 139479633360640 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.18017952144145966, loss=1.8772672414779663
I0310 02:19:23.855723 139479641753344 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.19010072946548462, loss=1.939731478691101
I0310 02:20:01.896965 139479633360640 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.19487909972667694, loss=1.8429287672042847
I0310 02:20:39.919178 139479641753344 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.31841471791267395, loss=1.9161022901535034
I0310 02:21:18.010383 139479633360640 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.18740370869636536, loss=1.8201013803482056
I0310 02:21:56.101006 139479641753344 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1805838793516159, loss=1.8841139078140259
I0310 02:22:34.139030 139479633360640 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.19695566594600677, loss=1.867567777633667
I0310 02:23:12.196182 139479641753344 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.186894491314888, loss=1.8863180875778198
I0310 02:23:50.263672 139479633360640 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.20863892138004303, loss=1.7589493989944458
I0310 02:24:28.316768 139479641753344 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.22353233397006989, loss=1.8092035055160522
I0310 02:25:06.393638 139479633360640 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1869240403175354, loss=1.8211076259613037
I0310 02:25:44.442878 139479641753344 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.1985320746898651, loss=1.8588217496871948
I0310 02:26:22.531723 139479633360640 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.19661593437194824, loss=1.988646149635315
I0310 02:27:00.614172 139479641753344 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.17416715621948242, loss=1.8344132900238037
I0310 02:27:38.694127 139479633360640 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2257480025291443, loss=1.797616720199585
I0310 02:28:16.742635 139479641753344 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.18084385991096497, loss=1.9448356628417969
I0310 02:28:54.787295 139479633360640 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2573910057544708, loss=1.8233747482299805
I0310 02:29:32.821832 139479641753344 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.20135954022407532, loss=1.8143576383590698
I0310 02:30:10.833964 139479633360640 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.16519701480865479, loss=1.8856512308120728
I0310 02:30:34.499720 139667344451392 spec.py:298] Evaluating on the training split.
I0310 02:30:37.482318 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:34:48.086420 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 02:34:50.721883 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:38:34.084874 139667344451392 spec.py:326] Evaluating on the test split.
I0310 02:38:36.768741 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:41:31.225721 139667344451392 submission_runner.py:359] Time since start: 11115.94s, 	Step: 17664, 	{'train/accuracy': 0.6305258274078369, 'train/loss': 1.8074754476547241, 'train/bleu': 30.055514840002132, 'validation/accuracy': 0.6495269536972046, 'validation/loss': 1.66240656375885, 'validation/bleu': 27.26310202235998, 'validation/num_examples': 3000, 'test/accuracy': 0.6598454713821411, 'test/loss': 1.592898964881897, 'test/bleu': 26.630011270921496, 'test/num_examples': 3003}
I0310 02:41:31.234769 139479641753344 logging_writer.py:48] [17664] global_step=17664, preemption_count=0, score=6729.210386, test/accuracy=0.659845, test/bleu=26.630011, test/loss=1.592899, test/num_examples=3003, total_duration=11115.940737, train/accuracy=0.630526, train/bleu=30.055515, train/loss=1.807475, validation/accuracy=0.649527, validation/bleu=27.263102, validation/loss=1.662407, validation/num_examples=3000
I0310 02:41:32.163944 139667344451392 checkpoints.py:356] Saving checkpoint at step: 17664
I0310 02:41:35.689285 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_17664
I0310 02:41:35.693887 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_17664.
I0310 02:41:49.755868 139479633360640 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.18928225338459015, loss=1.8482006788253784
I0310 02:42:27.681791 139479624967936 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.18781821429729462, loss=1.8857492208480835
I0310 02:43:05.689474 139479633360640 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.23111729323863983, loss=1.82609224319458
I0310 02:43:43.695173 139479624967936 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.24304300546646118, loss=1.8140668869018555
I0310 02:44:21.816295 139479633360640 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.29200056195259094, loss=1.9229047298431396
I0310 02:44:59.947062 139479624967936 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.214302197098732, loss=1.8938376903533936
I0310 02:45:38.041152 139479633360640 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.20451702177524567, loss=1.8733552694320679
I0310 02:46:16.135587 139479624967936 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.1698937863111496, loss=1.8539962768554688
I0310 02:46:54.193217 139479633360640 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.22988936305046082, loss=1.9566484689712524
I0310 02:47:32.288696 139479624967936 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.22302407026290894, loss=1.8844914436340332
I0310 02:48:10.374159 139479633360640 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19180257618427277, loss=1.839398741722107
I0310 02:48:48.451933 139479624967936 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.24401770532131195, loss=1.8252347707748413
I0310 02:49:26.532799 139479633360640 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.24865032732486725, loss=1.868769645690918
I0310 02:50:04.574436 139479624967936 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.190878227353096, loss=1.824250340461731
I0310 02:50:42.639300 139479633360640 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.17430908977985382, loss=1.8566162586212158
I0310 02:51:20.667714 139479624967936 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.1751941740512848, loss=1.8507994413375854
I0310 02:51:58.739105 139479633360640 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.17446251213550568, loss=1.8580338954925537
I0310 02:52:36.806878 139479624967936 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.16933320462703705, loss=1.9056206941604614
I0310 02:53:14.826851 139479633360640 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.45295462012290955, loss=1.8262879848480225
I0310 02:53:52.923317 139479624967936 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.16121117770671844, loss=1.8677476644515991
I0310 02:54:30.979409 139479633360640 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.1743735820055008, loss=1.8538480997085571
I0310 02:55:09.009392 139479624967936 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17779122292995453, loss=1.7845772504806519
I0310 02:55:35.736934 139667344451392 spec.py:298] Evaluating on the training split.
I0310 02:55:38.719213 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 02:59:30.938343 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 02:59:33.567287 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:02:12.671649 139667344451392 spec.py:326] Evaluating on the test split.
I0310 03:02:15.351664 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:04:44.386454 139667344451392 submission_runner.py:359] Time since start: 12617.18s, 	Step: 19872, 	{'train/accuracy': 0.6410760879516602, 'train/loss': 1.721678376197815, 'train/bleu': 31.255892306340634, 'validation/accuracy': 0.652415931224823, 'validation/loss': 1.6448966264724731, 'validation/bleu': 27.757413057725437, 'validation/num_examples': 3000, 'test/accuracy': 0.664702832698822, 'test/loss': 1.5712642669677734, 'test/bleu': 27.1376884327326, 'test/num_examples': 3003}
I0310 03:04:44.396302 139479633360640 logging_writer.py:48] [19872] global_step=19872, preemption_count=0, score=7566.076242, test/accuracy=0.664703, test/bleu=27.137688, test/loss=1.571264, test/num_examples=3003, total_duration=12617.177949, train/accuracy=0.641076, train/bleu=31.255892, train/loss=1.721678, validation/accuracy=0.652416, validation/bleu=27.757413, validation/loss=1.644897, validation/num_examples=3000
I0310 03:04:45.329656 139667344451392 checkpoints.py:356] Saving checkpoint at step: 19872
I0310 03:04:48.717586 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_19872
I0310 03:04:48.722068 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_19872.
I0310 03:04:59.740588 139479624967936 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.17484118044376373, loss=1.8797733783721924
I0310 03:05:37.752206 139479616575232 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.2049746960401535, loss=1.8800510168075562
I0310 03:06:15.782108 139479624967936 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.19176192581653595, loss=1.8684237003326416
I0310 03:06:53.817060 139479616575232 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.20920324325561523, loss=1.8082727193832397
I0310 03:07:31.895407 139479624967936 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.18654654920101166, loss=1.9134095907211304
I0310 03:08:09.932919 139479616575232 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.2686507999897003, loss=1.9079610109329224
I0310 03:08:48.029564 139479624967936 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.17741459608078003, loss=1.8301094770431519
I0310 03:09:26.092558 139479616575232 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.1696384996175766, loss=1.8271188735961914
I0310 03:10:04.180152 139479624967936 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.2093215435743332, loss=1.808049201965332
I0310 03:10:42.232404 139479616575232 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.18496696650981903, loss=1.8990482091903687
I0310 03:11:20.263409 139479624967936 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.17302438616752625, loss=1.789592981338501
I0310 03:11:58.359365 139479616575232 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.17071396112442017, loss=1.8004957437515259
I0310 03:12:36.450165 139479624967936 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.1768915057182312, loss=1.8086246252059937
I0310 03:13:14.537705 139479616575232 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.21333234012126923, loss=1.8600798845291138
I0310 03:13:52.642000 139479624967936 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.1752590835094452, loss=1.789581537246704
I0310 03:14:30.698621 139479616575232 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.31972795724868774, loss=1.864768624305725
I0310 03:15:08.757616 139479624967936 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.187361478805542, loss=1.9068694114685059
I0310 03:15:46.812901 139479616575232 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3393724262714386, loss=1.852764368057251
I0310 03:16:24.933329 139479624967936 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.2002977430820465, loss=1.7605572938919067
I0310 03:17:03.009953 139479616575232 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.1806807965040207, loss=1.77841317653656
I0310 03:17:41.054367 139479624967936 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.17636631429195404, loss=1.8993892669677734
I0310 03:18:19.110459 139479616575232 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2046177238225937, loss=1.8589226007461548
I0310 03:18:48.873765 139667344451392 spec.py:298] Evaluating on the training split.
I0310 03:18:51.863090 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:23:22.403351 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 03:23:25.032976 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:27:11.951487 139667344451392 spec.py:326] Evaluating on the test split.
I0310 03:27:14.642000 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:31:21.462506 139667344451392 submission_runner.py:359] Time since start: 14010.31s, 	Step: 22080, 	{'train/accuracy': 0.6375857591629028, 'train/loss': 1.762131690979004, 'train/bleu': 30.757607968921217, 'validation/accuracy': 0.6544246077537537, 'validation/loss': 1.6277469396591187, 'validation/bleu': 27.58514496549623, 'validation/num_examples': 3000, 'test/accuracy': 0.6679217219352722, 'test/loss': 1.5480490922927856, 'test/bleu': 27.55242671673115, 'test/num_examples': 3003}
I0310 03:31:21.472069 139479624967936 logging_writer.py:48] [22080] global_step=22080, preemption_count=0, score=8403.004111, test/accuracy=0.667922, test/bleu=27.552427, test/loss=1.548049, test/num_examples=3003, total_duration=14010.314790, train/accuracy=0.637586, train/bleu=30.757608, train/loss=1.762132, validation/accuracy=0.654425, validation/bleu=27.585145, validation/loss=1.627747, validation/num_examples=3000
I0310 03:31:22.397145 139667344451392 checkpoints.py:356] Saving checkpoint at step: 22080
I0310 03:31:26.379185 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_22080
I0310 03:31:26.383620 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_22080.
I0310 03:31:34.356410 139479616575232 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1993018537759781, loss=1.9103224277496338
I0310 03:32:12.308361 139479608182528 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.18760420382022858, loss=1.8969080448150635
I0310 03:32:50.273871 139479616575232 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.1818738877773285, loss=1.8254444599151611
I0310 03:33:28.392275 139479608182528 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.19655323028564453, loss=1.839571237564087
I0310 03:34:06.456916 139479616575232 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.2293921411037445, loss=1.8503152132034302
I0310 03:34:44.492366 139479608182528 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.18302227556705475, loss=1.8219218254089355
I0310 03:35:22.546619 139479616575232 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.16191156208515167, loss=1.8287848234176636
I0310 03:36:00.632722 139479608182528 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.1909862458705902, loss=1.8528167009353638
I0310 03:36:38.662153 139479616575232 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.18508271872997284, loss=1.8388670682907104
I0310 03:37:16.738879 139479608182528 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.17926985025405884, loss=1.9224920272827148
I0310 03:37:54.796635 139479616575232 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.18580025434494019, loss=1.7629119157791138
I0310 03:38:32.832417 139479608182528 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.21508924663066864, loss=1.905547857284546
I0310 03:39:10.906482 139479616575232 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.19575783610343933, loss=1.8903967142105103
I0310 03:39:48.939993 139479608182528 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.18681597709655762, loss=1.8182662725448608
I0310 03:40:26.984237 139479616575232 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.18234242498874664, loss=1.8375531435012817
I0310 03:41:04.993988 139479608182528 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.4707883894443512, loss=1.9067604541778564
I0310 03:41:43.065611 139479616575232 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.20149239897727966, loss=1.8312488794326782
I0310 03:42:21.141295 139479608182528 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.18293526768684387, loss=1.8156424760818481
I0310 03:42:59.204266 139479616575232 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.30644235014915466, loss=1.8502882719039917
I0310 03:43:37.275162 139479608182528 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.24539552628993988, loss=1.8262200355529785
I0310 03:44:15.321604 139479616575232 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.20147722959518433, loss=1.7698423862457275
I0310 03:44:53.395566 139479608182528 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.18215931951999664, loss=1.8391064405441284
I0310 03:45:26.648555 139667344451392 spec.py:298] Evaluating on the training split.
I0310 03:45:29.640332 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:49:29.093189 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 03:49:31.730880 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:51:59.059021 139667344451392 spec.py:326] Evaluating on the test split.
I0310 03:52:01.743593 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 03:54:07.287250 139667344451392 submission_runner.py:359] Time since start: 15608.09s, 	Step: 24289, 	{'train/accuracy': 0.6386384963989258, 'train/loss': 1.756717562675476, 'train/bleu': 31.09430183229091, 'validation/accuracy': 0.6561108827590942, 'validation/loss': 1.6155592203140259, 'validation/bleu': 27.922035610183844, 'validation/num_examples': 3000, 'test/accuracy': 0.6677008867263794, 'test/loss': 1.5392003059387207, 'test/bleu': 27.59171743878872, 'test/num_examples': 3003}
I0310 03:54:07.297104 139479616575232 logging_writer.py:48] [24289] global_step=24289, preemption_count=0, score=9239.943795, test/accuracy=0.667701, test/bleu=27.591717, test/loss=1.539200, test/num_examples=3003, total_duration=15608.089581, train/accuracy=0.638638, train/bleu=31.094302, train/loss=1.756718, validation/accuracy=0.656111, validation/bleu=27.922036, validation/loss=1.615559, validation/num_examples=3000
I0310 03:54:08.228228 139667344451392 checkpoints.py:356] Saving checkpoint at step: 24289
I0310 03:54:12.688265 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_24289
I0310 03:54:12.692709 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_24289.
I0310 03:54:17.269527 139479608182528 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.1849699169397354, loss=1.9044450521469116
I0310 03:54:55.248121 139479599789824 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.24826736748218536, loss=1.7772464752197266
I0310 03:55:33.282958 139479608182528 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.20439513027668, loss=1.8588422536849976
I0310 03:56:11.346962 139479599789824 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.17179323732852936, loss=1.8527348041534424
I0310 03:56:49.403319 139479608182528 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.1996234655380249, loss=1.9059566259384155
I0310 03:57:27.481808 139479599789824 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.19037321209907532, loss=1.7597124576568604
I0310 03:58:05.556475 139479608182528 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1832789033651352, loss=1.8226606845855713
I0310 03:58:43.602277 139479599789824 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.23917771875858307, loss=1.7253190279006958
I0310 03:59:21.689083 139479608182528 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.22936607897281647, loss=1.8320637941360474
I0310 03:59:59.700733 139479599789824 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.20738036930561066, loss=1.8303722143173218
I0310 04:00:37.740124 139479608182528 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.38091009855270386, loss=1.9638079404830933
I0310 04:01:15.772271 139479599789824 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.1716514527797699, loss=1.8006069660186768
I0310 04:01:53.788074 139479608182528 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.215897336602211, loss=1.7591005563735962
I0310 04:02:31.829051 139479599789824 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.26848486065864563, loss=1.806871771812439
I0310 04:03:09.895766 139479608182528 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.20864148437976837, loss=1.8666754961013794
I0310 04:03:47.941030 139479599789824 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.18872952461242676, loss=1.774167776107788
I0310 04:04:26.006408 139479608182528 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.18188010156154633, loss=1.8520864248275757
I0310 04:05:04.131555 139479599789824 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.17937913537025452, loss=1.7390644550323486
I0310 04:05:42.216777 139479608182528 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.17357860505580902, loss=1.812613844871521
I0310 04:06:20.326025 139479599789824 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.18753206729888916, loss=1.8334147930145264
I0310 04:06:58.389398 139479608182528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.1738542765378952, loss=1.8127193450927734
I0310 04:07:36.454690 139479599789824 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2818405330181122, loss=1.7980377674102783
I0310 04:08:12.728719 139667344451392 spec.py:298] Evaluating on the training split.
I0310 04:08:15.710970 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 04:12:02.102993 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 04:12:04.728592 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 04:15:35.487718 139667344451392 spec.py:326] Evaluating on the test split.
I0310 04:15:38.186528 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 04:19:40.324203 139667344451392 submission_runner.py:359] Time since start: 16974.17s, 	Step: 26497, 	{'train/accuracy': 0.6424170732498169, 'train/loss': 1.7030401229858398, 'train/bleu': 31.409216344746813, 'validation/accuracy': 0.659694254398346, 'validation/loss': 1.601055383682251, 'validation/bleu': 28.27352770993427, 'validation/num_examples': 3000, 'test/accuracy': 0.6693510413169861, 'test/loss': 1.5284686088562012, 'test/bleu': 27.382106408728536, 'test/num_examples': 3003}
I0310 04:19:40.336018 139479608182528 logging_writer.py:48] [26497] global_step=26497, preemption_count=0, score=10076.857559, test/accuracy=0.669351, test/bleu=27.382106, test/loss=1.528469, test/num_examples=3003, total_duration=16974.169753, train/accuracy=0.642417, train/bleu=31.409216, train/loss=1.703040, validation/accuracy=0.659694, validation/bleu=28.273528, validation/loss=1.601055, validation/num_examples=3000
I0310 04:19:41.670665 139667344451392 checkpoints.py:356] Saving checkpoint at step: 26497
I0310 04:19:46.419974 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_26497
I0310 04:19:46.423557 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_26497.
I0310 04:19:47.951660 139479599789824 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1996234804391861, loss=1.8181989192962646
I0310 04:20:25.897059 139479591397120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.22267156839370728, loss=1.767493486404419
I0310 04:21:03.859706 139479599789824 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.1842990517616272, loss=1.843441128730774
I0310 04:21:41.860903 139479591397120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.20112428069114685, loss=1.7817997932434082
I0310 04:22:19.974194 139479599789824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.18509630858898163, loss=1.795432448387146
I0310 04:22:58.000495 139479591397120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2142951339483261, loss=1.8574953079223633
I0310 04:23:36.035426 139479599789824 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.18931396305561066, loss=1.876261830329895
I0310 04:24:14.042873 139479591397120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2044534534215927, loss=1.8154929876327515
I0310 04:24:52.143096 139479599789824 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.19573982059955597, loss=1.8223873376846313
I0310 04:25:30.214126 139479591397120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.20029373466968536, loss=1.7815171480178833
I0310 04:26:08.283779 139479599789824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.18675528466701508, loss=1.8543338775634766
I0310 04:26:46.333470 139479591397120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.1761551797389984, loss=1.7403696775436401
I0310 04:27:24.403264 139479599789824 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.1812659204006195, loss=1.7769187688827515
I0310 04:28:02.541734 139479591397120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.17756807804107666, loss=1.7852411270141602
I0310 04:28:40.655560 139479599789824 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.17577087879180908, loss=1.8067920207977295
I0310 04:29:18.712709 139479591397120 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.1926441192626953, loss=1.8143386840820312
I0310 04:29:56.812572 139479599789824 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.16761457920074463, loss=1.872910976409912
I0310 04:30:34.962454 139479591397120 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.180254727602005, loss=1.8897687196731567
I0310 04:31:13.001527 139479599789824 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.19093956053256989, loss=1.6787735223770142
I0310 04:31:51.030493 139479591397120 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.20998866856098175, loss=1.7881768941879272
I0310 04:32:29.109728 139479599789824 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.19953523576259613, loss=1.8759795427322388
I0310 04:33:07.149196 139479591397120 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.17382508516311646, loss=1.8418066501617432
I0310 04:33:45.238512 139479599789824 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.25832292437553406, loss=1.8242477178573608
I0310 04:33:46.474316 139667344451392 spec.py:298] Evaluating on the training split.
I0310 04:33:49.465759 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 04:37:35.623305 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 04:37:38.258872 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 04:40:18.696905 139667344451392 spec.py:326] Evaluating on the test split.
I0310 04:40:21.384798 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 04:42:54.117598 139667344451392 submission_runner.py:359] Time since start: 18507.92s, 	Step: 28705, 	{'train/accuracy': 0.6411365866661072, 'train/loss': 1.7290245294570923, 'train/bleu': 30.99537003611994, 'validation/accuracy': 0.6603266000747681, 'validation/loss': 1.5942227840423584, 'validation/bleu': 28.163309539775355, 'validation/num_examples': 3000, 'test/accuracy': 0.6741386651992798, 'test/loss': 1.5118300914764404, 'test/bleu': 27.834865371067284, 'test/num_examples': 3003}
I0310 04:42:54.127800 139479591397120 logging_writer.py:48] [28705] global_step=28705, preemption_count=0, score=10913.350633, test/accuracy=0.674139, test/bleu=27.834865, test/loss=1.511830, test/num_examples=3003, total_duration=18507.915346, train/accuracy=0.641137, train/bleu=30.995370, train/loss=1.729025, validation/accuracy=0.660327, validation/bleu=28.163310, validation/loss=1.594223, validation/num_examples=3000
I0310 04:42:55.098815 139667344451392 checkpoints.py:356] Saving checkpoint at step: 28705
I0310 04:42:58.498080 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_28705
I0310 04:42:58.502620 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_28705.
I0310 04:43:34.938779 139479599789824 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.18876329064369202, loss=1.7616111040115356
I0310 04:44:12.920498 139479583004416 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.178894504904747, loss=1.8044732809066772
I0310 04:44:50.988396 139479599789824 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.17979049682617188, loss=1.8249354362487793
I0310 04:45:29.045532 139479583004416 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.22974605858325958, loss=1.8477939367294312
I0310 04:46:07.090117 139479599789824 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.17658890783786774, loss=1.8128597736358643
I0310 04:46:45.176524 139479583004416 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1766684651374817, loss=1.9216824769973755
I0310 04:47:23.241556 139479599789824 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.22002683579921722, loss=1.8207911252975464
I0310 04:48:01.357487 139479583004416 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.20505490899085999, loss=1.8018819093704224
I0310 04:48:39.439559 139479599789824 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19810707867145538, loss=1.689161777496338
I0310 04:49:17.577139 139479583004416 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.22356414794921875, loss=1.801159381866455
I0310 04:49:55.628821 139479599789824 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.17962436378002167, loss=1.8172234296798706
I0310 04:50:33.804365 139479583004416 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.24250859022140503, loss=1.7519862651824951
I0310 04:51:11.918428 139479599789824 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.18504172563552856, loss=1.8607484102249146
I0310 04:51:49.993918 139479583004416 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.18620631098747253, loss=1.8266066312789917
I0310 04:52:28.051062 139479599789824 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.19025322794914246, loss=1.7517857551574707
I0310 04:53:06.150473 139479583004416 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.1984308511018753, loss=1.8389720916748047
I0310 04:53:44.241161 139479599789824 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.18190763890743256, loss=1.7536183595657349
I0310 04:54:22.354287 139479583004416 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.21632888913154602, loss=1.7768532037734985
I0310 04:55:00.449870 139479599789824 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.23484215140342712, loss=1.7982854843139648
I0310 04:55:38.589089 139479583004416 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1979459822177887, loss=1.7856543064117432
I0310 04:56:16.713536 139479599789824 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.254147469997406, loss=1.7357120513916016
I0310 04:56:54.844989 139479583004416 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.20153580605983734, loss=1.826265573501587
I0310 04:56:58.746665 139667344451392 spec.py:298] Evaluating on the training split.
I0310 04:57:01.765670 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:00:31.275697 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 05:00:33.936972 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:03:15.430221 139667344451392 spec.py:326] Evaluating on the test split.
I0310 05:03:18.141504 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:05:37.321281 139667344451392 submission_runner.py:359] Time since start: 19900.19s, 	Step: 30912, 	{'train/accuracy': 0.6416804790496826, 'train/loss': 1.728795051574707, 'train/bleu': 31.032973717829783, 'validation/accuracy': 0.6616780757904053, 'validation/loss': 1.5820846557617188, 'validation/bleu': 27.926792805953077, 'validation/num_examples': 3000, 'test/accuracy': 0.6728487610816956, 'test/loss': 1.5049326419830322, 'test/bleu': 27.49777381514415, 'test/num_examples': 3003}
I0310 05:05:37.332581 139479599789824 logging_writer.py:48] [30912] global_step=30912, preemption_count=0, score=11750.201052, test/accuracy=0.672849, test/bleu=27.497774, test/loss=1.504933, test/num_examples=3003, total_duration=19900.187679, train/accuracy=0.641680, train/bleu=31.032974, train/loss=1.728795, validation/accuracy=0.661678, validation/bleu=27.926793, validation/loss=1.582085, validation/num_examples=3000
I0310 05:05:38.286293 139667344451392 checkpoints.py:356] Saving checkpoint at step: 30912
I0310 05:05:41.663387 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_30912
I0310 05:05:41.667888 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_30912.
I0310 05:06:15.429585 139479583004416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.20138710737228394, loss=1.7920900583267212
I0310 05:06:53.439408 139479574611712 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.23277926445007324, loss=1.7831515073776245
I0310 05:07:31.485133 139479583004416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.202534019947052, loss=1.829246997833252
I0310 05:08:09.550782 139479574611712 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.2351146936416626, loss=1.8197336196899414
I0310 05:08:47.630310 139479583004416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.1849725991487503, loss=1.7405985593795776
I0310 05:09:25.752752 139479574611712 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.19535022974014282, loss=1.7373127937316895
I0310 05:10:03.850523 139479583004416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1777942031621933, loss=1.7821862697601318
I0310 05:10:41.891519 139479574611712 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.20407816767692566, loss=1.8598536252975464
I0310 05:11:20.012416 139479583004416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.1912432610988617, loss=1.8237359523773193
I0310 05:11:58.115922 139479574611712 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.2821473777294159, loss=1.7676812410354614
I0310 05:12:36.176122 139479583004416 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.19903819262981415, loss=1.8262840509414673
I0310 05:13:14.308072 139479574611712 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.25758785009384155, loss=1.853078007698059
I0310 05:13:52.353540 139479583004416 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.1789398342370987, loss=1.8151215314865112
I0310 05:14:30.476298 139479574611712 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.2147093415260315, loss=1.738511562347412
I0310 05:15:08.572618 139479583004416 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.18449006974697113, loss=1.8359532356262207
I0310 05:15:46.679584 139479574611712 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.18778257071971893, loss=1.7118871212005615
I0310 05:16:24.815976 139479583004416 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.20403283834457397, loss=1.7791639566421509
I0310 05:17:02.902198 139479574611712 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.18593338131904602, loss=1.826774001121521
I0310 05:17:41.010556 139479583004416 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.1962611973285675, loss=1.867411494255066
I0310 05:18:19.064733 139479574611712 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.218424454331398, loss=1.7755441665649414
I0310 05:18:57.180920 139479583004416 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.23556937277317047, loss=1.7670481204986572
I0310 05:19:35.256070 139479574611712 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.2034389078617096, loss=1.8257066011428833
I0310 05:19:41.795745 139667344451392 spec.py:298] Evaluating on the training split.
I0310 05:19:44.785060 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:24:04.587177 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 05:24:07.230989 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:27:43.363585 139667344451392 spec.py:326] Evaluating on the test split.
I0310 05:27:46.050080 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:31:58.619443 139667344451392 submission_runner.py:359] Time since start: 21263.24s, 	Step: 33119, 	{'train/accuracy': 0.6491636633872986, 'train/loss': 1.6753838062286377, 'train/bleu': 31.83454204698703, 'validation/accuracy': 0.6607977747917175, 'validation/loss': 1.5729471445083618, 'validation/bleu': 27.79212884005354, 'validation/num_examples': 3000, 'test/accuracy': 0.6751263737678528, 'test/loss': 1.4910290241241455, 'test/bleu': 27.75899427976285, 'test/num_examples': 3003}
I0310 05:31:58.630151 139479583004416 logging_writer.py:48] [33119] global_step=33119, preemption_count=0, score=12586.969907, test/accuracy=0.675126, test/bleu=27.758994, test/loss=1.491029, test/num_examples=3003, total_duration=21263.236770, train/accuracy=0.649164, train/bleu=31.834542, train/loss=1.675384, validation/accuracy=0.660798, validation/bleu=27.792129, validation/loss=1.572947, validation/num_examples=3000
I0310 05:31:59.595775 139667344451392 checkpoints.py:356] Saving checkpoint at step: 33119
I0310 05:32:02.937799 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_33119
I0310 05:32:02.942364 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_33119.
I0310 05:32:34.059349 139479574611712 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.178897425532341, loss=1.8446252346038818
I0310 05:33:12.059960 139479566219008 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.20108507573604584, loss=1.7668039798736572
I0310 05:33:50.096141 139479574611712 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2816338539123535, loss=1.7627192735671997
I0310 05:34:28.215981 139479566219008 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.2840016484260559, loss=1.7664738893508911
I0310 05:35:06.331432 139479574611712 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.26140356063842773, loss=1.8265986442565918
I0310 05:35:44.435602 139479566219008 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.20531563460826874, loss=1.756632924079895
I0310 05:36:22.544967 139479574611712 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.1821282058954239, loss=1.7690544128417969
I0310 05:37:00.647248 139479566219008 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.2017539143562317, loss=1.7516003847122192
I0310 05:37:38.768599 139479574611712 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1885799914598465, loss=1.7851577997207642
I0310 05:38:16.844216 139479566219008 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.2389504760503769, loss=1.7833826541900635
I0310 05:38:54.896869 139479574611712 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.21368177235126495, loss=1.787666916847229
I0310 05:39:32.983259 139479566219008 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.1714928150177002, loss=1.7231152057647705
I0310 05:40:11.073565 139479574611712 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.18377408385276794, loss=1.7999314069747925
I0310 05:40:49.131727 139479566219008 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1746233105659485, loss=1.6662535667419434
I0310 05:41:27.204826 139479574611712 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.1906599998474121, loss=1.7348668575286865
I0310 05:42:05.303955 139479566219008 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.21365682780742645, loss=1.8001121282577515
I0310 05:42:43.359769 139479574611712 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.19189095497131348, loss=1.733771562576294
I0310 05:43:21.466694 139479566219008 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.24012970924377441, loss=1.807213306427002
I0310 05:43:59.584105 139479574611712 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.20177622139453888, loss=1.7063606977462769
I0310 05:44:37.673689 139479566219008 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.19217607378959656, loss=1.766945242881775
I0310 05:45:15.755393 139479574611712 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.2063985913991928, loss=1.768813967704773
I0310 05:45:53.944175 139479566219008 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.1838454306125641, loss=1.7796207666397095
I0310 05:46:03.190616 139667344451392 spec.py:298] Evaluating on the training split.
I0310 05:46:06.182498 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:50:10.048331 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 05:50:12.694698 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:53:13.046826 139667344451392 spec.py:326] Evaluating on the test split.
I0310 05:53:15.753805 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 05:55:43.980132 139667344451392 submission_runner.py:359] Time since start: 22844.63s, 	Step: 35326, 	{'train/accuracy': 0.6468344330787659, 'train/loss': 1.6949365139007568, 'train/bleu': 31.926462409987625, 'validation/accuracy': 0.665162205696106, 'validation/loss': 1.5592386722564697, 'validation/bleu': 28.441987746962315, 'validation/num_examples': 3000, 'test/accuracy': 0.676416277885437, 'test/loss': 1.483656406402588, 'test/bleu': 28.214819397498445, 'test/num_examples': 3003}
I0310 05:55:43.992454 139479574611712 logging_writer.py:48] [35326] global_step=35326, preemption_count=0, score=13424.017351, test/accuracy=0.676416, test/bleu=28.214819, test/loss=1.483656, test/num_examples=3003, total_duration=22844.631625, train/accuracy=0.646834, train/bleu=31.926462, train/loss=1.694937, validation/accuracy=0.665162, validation/bleu=28.441988, validation/loss=1.559239, validation/num_examples=3000
I0310 05:55:45.324131 139667344451392 checkpoints.py:356] Saving checkpoint at step: 35326
I0310 05:55:49.829818 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_35326
I0310 05:55:49.833464 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_35326.
I0310 05:56:18.299392 139479566219008 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.3561955392360687, loss=1.7500333786010742
I0310 05:56:56.241451 139479557826304 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.32724109292030334, loss=1.766160488128662
I0310 05:57:34.265626 139479566219008 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.19732312858104706, loss=1.7265775203704834
I0310 05:58:12.363034 139479557826304 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2079894095659256, loss=1.797599196434021
I0310 05:58:50.376713 139479566219008 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.1882873773574829, loss=1.7160873413085938
I0310 05:59:28.426263 139479557826304 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.18141765892505646, loss=1.7435648441314697
I0310 06:00:06.407182 139479566219008 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.193571075797081, loss=1.7621519565582275
I0310 06:00:44.521598 139479557826304 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.18271426856517792, loss=1.7228080034255981
I0310 06:01:22.625795 139479566219008 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.223492830991745, loss=1.814292311668396
I0310 06:02:00.735976 139479557826304 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2173823118209839, loss=1.726519227027893
I0310 06:02:38.869462 139479566219008 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.19311456382274628, loss=1.818360447883606
I0310 06:03:16.970370 139479557826304 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.18217754364013672, loss=1.7427600622177124
I0310 06:03:55.036459 139479566219008 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.18943367898464203, loss=1.7074410915374756
I0310 06:04:33.072503 139479557826304 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.16932794451713562, loss=1.784840703010559
I0310 06:05:11.090584 139479566219008 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.24745824933052063, loss=1.7558451890945435
I0310 06:05:49.118966 139479557826304 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.18894940614700317, loss=1.6890543699264526
I0310 06:06:27.254858 139479566219008 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.21399450302124023, loss=1.7808730602264404
I0310 06:07:05.318649 139479557826304 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.17407451570034027, loss=1.7731229066848755
I0310 06:07:43.436399 139479566219008 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.1882093846797943, loss=1.7495746612548828
I0310 06:08:21.534683 139479557826304 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1971527338027954, loss=1.767092227935791
I0310 06:08:59.609323 139479566219008 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.18705929815769196, loss=1.7834982872009277
I0310 06:09:37.733165 139479557826304 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.19185234606266022, loss=1.7095613479614258
I0310 06:09:49.981327 139667344451392 spec.py:298] Evaluating on the training split.
I0310 06:09:52.968849 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 06:14:18.579509 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 06:14:21.214093 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 06:16:50.358070 139667344451392 spec.py:326] Evaluating on the test split.
I0310 06:16:53.056941 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 06:19:06.869517 139667344451392 submission_runner.py:359] Time since start: 24271.42s, 	Step: 37534, 	{'train/accuracy': 0.6703068614006042, 'train/loss': 1.5395845174789429, 'train/bleu': 33.470978948812636, 'validation/accuracy': 0.665075421333313, 'validation/loss': 1.558985948562622, 'validation/bleu': 28.791192688013695, 'validation/num_examples': 3000, 'test/accuracy': 0.6781477332115173, 'test/loss': 1.4762883186340332, 'test/bleu': 27.935895725376845, 'test/num_examples': 3003}
I0310 06:19:06.880112 139479566219008 logging_writer.py:48] [37534] global_step=37534, preemption_count=0, score=14260.883837, test/accuracy=0.678148, test/bleu=27.935896, test/loss=1.476288, test/num_examples=3003, total_duration=24271.422354, train/accuracy=0.670307, train/bleu=33.470979, train/loss=1.539585, validation/accuracy=0.665075, validation/bleu=28.791193, validation/loss=1.558986, validation/num_examples=3000
I0310 06:19:07.853903 139667344451392 checkpoints.py:356] Saving checkpoint at step: 37534
I0310 06:19:12.316258 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_37534
I0310 06:19:12.320698 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_37534.
I0310 06:19:37.750109 139479557826304 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.20222309231758118, loss=1.7774471044540405
I0310 06:20:15.691339 139479549433600 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.18736951053142548, loss=1.7676711082458496
I0310 06:20:53.746081 139479557826304 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.20892922580242157, loss=1.7238938808441162
I0310 06:21:31.828470 139479549433600 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2041451632976532, loss=1.8011610507965088
I0310 06:22:09.847680 139479557826304 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.23959757387638092, loss=1.753968358039856
I0310 06:22:47.875689 139479549433600 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.21798761188983917, loss=1.800123929977417
I0310 06:23:25.976588 139479557826304 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.19648797810077667, loss=1.7733392715454102
I0310 06:24:04.052458 139479549433600 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.19209468364715576, loss=1.7258330583572388
I0310 06:24:42.128876 139479557826304 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.20784468948841095, loss=1.717089056968689
I0310 06:25:20.155719 139479549433600 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.19193696975708008, loss=1.7201931476593018
I0310 06:25:58.215690 139479557826304 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.1975642740726471, loss=1.7356319427490234
I0310 06:26:36.346387 139479549433600 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.19676652550697327, loss=1.800216794013977
I0310 06:27:14.410155 139479557826304 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.26184746623039246, loss=1.7310504913330078
I0310 06:27:52.444802 139479549433600 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.17628240585327148, loss=1.7493535280227661
I0310 06:28:30.484625 139479557826304 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.21272651851177216, loss=1.7403095960617065
I0310 06:29:08.490811 139479549433600 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.19129055738449097, loss=1.6841663122177124
I0310 06:29:46.600781 139479557826304 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.19771261513233185, loss=1.662793517112732
I0310 06:30:24.664900 139479549433600 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.34889131784439087, loss=1.8180490732192993
I0310 06:31:02.766130 139479557826304 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.19017063081264496, loss=1.6779446601867676
I0310 06:31:40.847650 139479549433600 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.19171786308288574, loss=1.7306572198867798
I0310 06:32:18.885427 139479557826304 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.17805509269237518, loss=1.6515837907791138
I0310 06:32:56.925201 139479549433600 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2061128467321396, loss=1.7058454751968384
I0310 06:33:12.646251 139667344451392 spec.py:298] Evaluating on the training split.
I0310 06:33:15.643344 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 06:37:21.933320 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 06:37:24.600563 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 06:40:02.941878 139667344451392 spec.py:326] Evaluating on the test split.
I0310 06:40:05.639319 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 06:42:50.403820 139667344451392 submission_runner.py:359] Time since start: 25674.09s, 	Step: 39743, 	{'train/accuracy': 0.6500890851020813, 'train/loss': 1.6639429330825806, 'train/bleu': 31.350653435801895, 'validation/accuracy': 0.6670345067977905, 'validation/loss': 1.5461325645446777, 'validation/bleu': 28.754831161977457, 'validation/num_examples': 3000, 'test/accuracy': 0.6785311698913574, 'test/loss': 1.4651907682418823, 'test/bleu': 28.171057878563253, 'test/num_examples': 3003}
I0310 06:42:50.416227 139479557826304 logging_writer.py:48] [39743] global_step=39743, preemption_count=0, score=15097.930700, test/accuracy=0.678531, test/bleu=28.171058, test/loss=1.465191, test/num_examples=3003, total_duration=25674.087282, train/accuracy=0.650089, train/bleu=31.350653, train/loss=1.663943, validation/accuracy=0.667035, validation/bleu=28.754831, validation/loss=1.546133, validation/num_examples=3000
I0310 06:42:51.716911 139667344451392 checkpoints.py:356] Saving checkpoint at step: 39743
I0310 06:42:56.182169 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_39743
I0310 06:42:56.185983 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_39743.
I0310 06:43:18.209950 139479549433600 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.21529097855091095, loss=1.7765253782272339
I0310 06:43:56.196723 139479541040896 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.28721436858177185, loss=1.7467390298843384
I0310 06:44:34.239970 139479549433600 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.19220367074012756, loss=1.6688259840011597
I0310 06:45:12.316760 139479541040896 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.18459074199199677, loss=1.7152262926101685
I0310 06:45:50.407490 139479549433600 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19952136278152466, loss=1.7327852249145508
I0310 06:46:28.459298 139479541040896 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2664467692375183, loss=1.7524100542068481
I0310 06:47:06.539896 139479549433600 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.1869116723537445, loss=1.7011661529541016
I0310 06:47:44.633148 139479541040896 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.2040741741657257, loss=1.7638777494430542
I0310 06:48:22.742430 139479549433600 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.18138925731182098, loss=1.7192602157592773
I0310 06:49:00.809594 139479541040896 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1264796257019043, loss=1.7914855480194092
I0310 06:49:38.875180 139479549433600 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.17761698365211487, loss=1.7332768440246582
I0310 06:50:16.962337 139479541040896 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.1910512000322342, loss=1.7439152002334595
I0310 06:50:54.973865 139479549433600 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.20390284061431885, loss=1.7126870155334473
I0310 06:51:33.092689 139479541040896 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.2143901288509369, loss=1.7649568319320679
I0310 06:52:11.195795 139479549433600 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.18776321411132812, loss=1.6508145332336426
I0310 06:52:49.270491 139479541040896 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.1816355288028717, loss=1.745904564857483
I0310 06:53:27.355610 139479549433600 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2013384848833084, loss=1.7123398780822754
I0310 06:54:05.407744 139479541040896 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1810556948184967, loss=1.6829997301101685
I0310 06:54:43.485748 139479549433600 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.198217511177063, loss=1.7103006839752197
I0310 06:55:21.578237 139479541040896 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.2183759212493896, loss=1.7558656930923462
I0310 06:55:59.681535 139479549433600 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.19743812084197998, loss=1.6640123128890991
I0310 06:56:37.721339 139479541040896 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.22321319580078125, loss=1.6681513786315918
I0310 06:56:56.508345 139667344451392 spec.py:298] Evaluating on the training split.
I0310 06:56:59.509678 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:01:20.401085 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 07:01:23.039991 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:03:46.291711 139667344451392 spec.py:326] Evaluating on the test split.
I0310 07:03:48.981209 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:06:08.678943 139667344451392 submission_runner.py:359] Time since start: 27097.95s, 	Step: 41951, 	{'train/accuracy': 0.6513440608978271, 'train/loss': 1.6586329936981201, 'train/bleu': 32.34427413706207, 'validation/accuracy': 0.6689563393592834, 'validation/loss': 1.5376447439193726, 'validation/bleu': 28.49650263359707, 'validation/num_examples': 3000, 'test/accuracy': 0.6801928877830505, 'test/loss': 1.4536679983139038, 'test/bleu': 28.41930312809594, 'test/num_examples': 3003}
I0310 07:06:08.690200 139479549433600 logging_writer.py:48] [41951] global_step=41951, preemption_count=0, score=15934.899314, test/accuracy=0.680193, test/bleu=28.419303, test/loss=1.453668, test/num_examples=3003, total_duration=27097.949345, train/accuracy=0.651344, train/bleu=32.344274, train/loss=1.658633, validation/accuracy=0.668956, validation/bleu=28.496503, validation/loss=1.537645, validation/num_examples=3000
I0310 07:06:09.702152 139667344451392 checkpoints.py:356] Saving checkpoint at step: 41951
I0310 07:06:13.063146 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_41951
I0310 07:06:13.067664 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_41951.
I0310 07:06:32.031378 139479541040896 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.231586754322052, loss=1.6983649730682373
I0310 07:07:09.982672 139479532648192 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.20227932929992676, loss=1.6891599893569946
I0310 07:07:47.932372 139479541040896 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1870490312576294, loss=1.7207648754119873
I0310 07:08:26.035780 139479532648192 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1873995065689087, loss=1.7649760246276855
I0310 07:09:04.098887 139479541040896 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.20713992416858673, loss=1.663752794265747
I0310 07:09:42.135308 139479532648192 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1923264116048813, loss=1.738651156425476
I0310 07:10:20.261220 139479541040896 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2085798680782318, loss=1.7771278619766235
I0310 07:10:58.387750 139479532648192 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.18482990562915802, loss=1.7511218786239624
I0310 07:11:36.419363 139479541040896 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.20769640803337097, loss=1.8172175884246826
I0310 07:12:14.526957 139479532648192 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.22995230555534363, loss=1.692812204360962
I0310 07:12:52.602260 139479541040896 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.2210972160100937, loss=1.7681533098220825
I0310 07:13:30.703638 139479532648192 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.234229177236557, loss=1.6110891103744507
I0310 07:14:08.813055 139479541040896 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.24913276731967926, loss=1.724426031112671
I0310 07:14:46.905723 139479532648192 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.18261872231960297, loss=1.6713227033615112
I0310 07:15:24.970217 139479541040896 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.19595453143119812, loss=1.710079312324524
I0310 07:16:03.050170 139479532648192 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.20870453119277954, loss=1.6937166452407837
I0310 07:16:41.106535 139479541040896 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.20649446547031403, loss=1.7460824251174927
I0310 07:17:19.217494 139479532648192 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.20350678265094757, loss=1.644723653793335
I0310 07:17:57.302660 139479541040896 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1972464919090271, loss=1.724989414215088
I0310 07:18:35.377844 139479532648192 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.1977018266916275, loss=1.7444337606430054
I0310 07:19:13.427540 139479541040896 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1975160390138626, loss=1.6896809339523315
I0310 07:19:51.527470 139479532648192 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.23727786540985107, loss=1.7688921689987183
I0310 07:20:13.298743 139667344451392 spec.py:298] Evaluating on the training split.
I0310 07:20:16.293886 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:24:37.285778 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 07:24:39.940706 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:27:55.279918 139667344451392 spec.py:326] Evaluating on the test split.
I0310 07:27:57.985947 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:30:58.065630 139667344451392 submission_runner.py:359] Time since start: 28494.74s, 	Step: 44159, 	{'train/accuracy': 0.6668686866760254, 'train/loss': 1.5389398336410522, 'train/bleu': 33.027143920282136, 'validation/accuracy': 0.6703822612762451, 'validation/loss': 1.525027871131897, 'validation/bleu': 29.012150640453754, 'validation/num_examples': 3000, 'test/accuracy': 0.6822962164878845, 'test/loss': 1.4443830251693726, 'test/bleu': 28.651560987598042, 'test/num_examples': 3003}
I0310 07:30:58.076948 139479541040896 logging_writer.py:48] [44159] global_step=44159, preemption_count=0, score=16771.819239, test/accuracy=0.682296, test/bleu=28.651561, test/loss=1.444383, test/num_examples=3003, total_duration=28494.739745, train/accuracy=0.666869, train/bleu=33.027144, train/loss=1.538940, validation/accuracy=0.670382, validation/bleu=29.012151, validation/loss=1.525028, validation/num_examples=3000
I0310 07:30:59.066009 139667344451392 checkpoints.py:356] Saving checkpoint at step: 44159
I0310 07:31:02.436057 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_44159
I0310 07:31:02.440627 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_44159.
I0310 07:31:18.376703 139479532648192 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2087497115135193, loss=1.7856725454330444
I0310 07:31:56.374545 139479524255488 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.21689631044864655, loss=1.7454959154129028
I0310 07:32:34.456295 139479532648192 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.19846779108047485, loss=1.7402496337890625
I0310 07:33:12.504796 139479524255488 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19344107806682587, loss=1.7072101831436157
I0310 07:33:50.599609 139479532648192 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2034965604543686, loss=1.7128407955169678
I0310 07:34:28.706966 139479524255488 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.19472692906856537, loss=1.699472188949585
I0310 07:35:06.746110 139479532648192 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.20393405854701996, loss=1.689742922782898
I0310 07:35:44.809896 139479524255488 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.20934396982192993, loss=1.7774983644485474
I0310 07:36:22.920836 139479532648192 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.22090332210063934, loss=1.7960864305496216
I0310 07:37:01.050135 139479524255488 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.18022873997688293, loss=1.7257373332977295
I0310 07:37:39.089746 139479532648192 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.21544116735458374, loss=1.6786190271377563
I0310 07:38:17.177789 139479524255488 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.1854007989168167, loss=1.7211880683898926
I0310 07:38:55.285587 139479532648192 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2094726413488388, loss=1.7280722856521606
I0310 07:39:33.389117 139479524255488 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.19078654050827026, loss=1.705776333808899
I0310 07:40:11.409860 139479532648192 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.22478438913822174, loss=1.77605402469635
I0310 07:40:49.516726 139479524255488 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.2149333953857422, loss=1.657774806022644
I0310 07:41:27.623966 139479532648192 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.22669707238674164, loss=1.692116379737854
I0310 07:42:05.704256 139479524255488 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.21272653341293335, loss=1.6920198202133179
I0310 07:42:43.814165 139479532648192 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.21795061230659485, loss=1.7420786619186401
I0310 07:43:21.888558 139479524255488 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.1952998787164688, loss=1.7699451446533203
I0310 07:43:59.992983 139479532648192 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.17775413393974304, loss=1.6537319421768188
I0310 07:44:38.099414 139479524255488 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19319701194763184, loss=1.733760952949524
I0310 07:45:02.585680 139667344451392 spec.py:298] Evaluating on the training split.
I0310 07:45:05.581555 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:49:13.833263 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 07:49:16.461278 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:53:07.432126 139667344451392 spec.py:326] Evaluating on the test split.
I0310 07:53:10.126304 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 07:57:05.685963 139667344451392 submission_runner.py:359] Time since start: 29984.03s, 	Step: 46366, 	{'train/accuracy': 0.65520840883255, 'train/loss': 1.6176486015319824, 'train/bleu': 32.32295884980679, 'validation/accuracy': 0.6708162426948547, 'validation/loss': 1.5153177976608276, 'validation/bleu': 28.940572733548493, 'validation/num_examples': 3000, 'test/accuracy': 0.6833188533782959, 'test/loss': 1.433402419090271, 'test/bleu': 28.795042940423215, 'test/num_examples': 3003}
I0310 07:57:05.696764 139479532648192 logging_writer.py:48] [46366] global_step=46366, preemption_count=0, score=17608.827064, test/accuracy=0.683319, test/bleu=28.795043, test/loss=1.433402, test/num_examples=3003, total_duration=29984.026712, train/accuracy=0.655208, train/bleu=32.322959, train/loss=1.617649, validation/accuracy=0.670816, validation/bleu=28.940573, validation/loss=1.515318, validation/num_examples=3000
I0310 07:57:06.679066 139667344451392 checkpoints.py:356] Saving checkpoint at step: 46366
I0310 07:57:10.053174 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_46366
I0310 07:57:10.057569 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_46366.
I0310 07:57:23.376252 139479524255488 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.19476063549518585, loss=1.7442145347595215
I0310 07:58:01.316646 139479515862784 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.19030022621154785, loss=1.7936269044876099
I0310 07:58:39.364161 139479524255488 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.19059541821479797, loss=1.5819145441055298
I0310 07:59:17.394968 139479515862784 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.18304046988487244, loss=1.6933387517929077
I0310 07:59:55.426551 139479524255488 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.21309657394886017, loss=1.7216075658798218
I0310 08:00:33.580391 139479515862784 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.22964391112327576, loss=1.699281930923462
I0310 08:01:11.644783 139479524255488 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.2200387567281723, loss=1.6283042430877686
I0310 08:01:49.807397 139479515862784 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2018882930278778, loss=1.6989035606384277
I0310 08:02:27.977241 139479524255488 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2107037454843521, loss=1.7028028964996338
I0310 08:03:06.033913 139479515862784 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.211929589509964, loss=1.6969531774520874
I0310 08:03:44.115144 139479524255488 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.2017872929573059, loss=1.677330493927002
I0310 08:04:22.198111 139479515862784 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.20402026176452637, loss=1.6712652444839478
I0310 08:05:00.293489 139479524255488 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.19517092406749725, loss=1.7018951177597046
I0310 08:05:38.402991 139479515862784 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19370253384113312, loss=1.651538610458374
I0310 08:06:16.466583 139479524255488 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.21359744668006897, loss=1.681278109550476
I0310 08:06:54.562571 139479515862784 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.20618842542171478, loss=1.67409086227417
I0310 08:07:32.644139 139479524255488 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2053440362215042, loss=1.7215641736984253
I0310 08:08:10.707848 139479515862784 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.17839299142360687, loss=1.7561225891113281
I0310 08:08:48.749875 139479524255488 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.21373625099658966, loss=1.7548699378967285
I0310 08:09:26.762381 139479515862784 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.22076871991157532, loss=1.7045159339904785
I0310 08:10:04.844373 139479524255488 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.18408000469207764, loss=1.7202932834625244
I0310 08:10:42.931608 139479515862784 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.21598589420318604, loss=1.6776413917541504
I0310 08:11:10.434972 139667344451392 spec.py:298] Evaluating on the training split.
I0310 08:11:13.416913 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 08:15:09.157856 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 08:15:11.785243 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 08:17:43.416102 139667344451392 spec.py:326] Evaluating on the test split.
I0310 08:17:46.089647 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 08:20:05.132438 139667344451392 submission_runner.py:359] Time since start: 31551.88s, 	Step: 48574, 	{'train/accuracy': 0.6518847346305847, 'train/loss': 1.6468784809112549, 'train/bleu': 32.42530928243256, 'validation/accuracy': 0.6714361906051636, 'validation/loss': 1.5131075382232666, 'validation/bleu': 29.234529956586847, 'validation/num_examples': 3000, 'test/accuracy': 0.6860728859901428, 'test/loss': 1.4255199432373047, 'test/bleu': 28.647253701992746, 'test/num_examples': 3003}
I0310 08:20:05.143890 139479524255488 logging_writer.py:48] [48574] global_step=48574, preemption_count=0, score=18446.102585, test/accuracy=0.686073, test/bleu=28.647254, test/loss=1.425520, test/num_examples=3003, total_duration=31551.876004, train/accuracy=0.651885, train/bleu=32.425309, train/loss=1.646878, validation/accuracy=0.671436, validation/bleu=29.234530, validation/loss=1.513108, validation/num_examples=3000
I0310 08:20:06.128489 139667344451392 checkpoints.py:356] Saving checkpoint at step: 48574
I0310 08:20:09.645223 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_48574
I0310 08:20:09.649759 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_48574.
I0310 08:20:19.918758 139479515862784 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.19771137833595276, loss=1.7312240600585938
I0310 08:20:57.928262 139479507470080 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.198745459318161, loss=1.7370564937591553
I0310 08:21:35.943266 139479515862784 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.1864158660173416, loss=1.7008510828018188
I0310 08:22:14.032636 139479507470080 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.19017551839351654, loss=1.7535356283187866
I0310 08:22:52.150108 139479515862784 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2114255726337433, loss=1.661548376083374
I0310 08:23:30.198104 139479507470080 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.19044210016727448, loss=1.6990779638290405
I0310 08:24:08.265780 139479515862784 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19979338347911835, loss=1.64790678024292
I0310 08:24:46.350343 139479507470080 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.18763135373592377, loss=1.7913281917572021
I0310 08:25:24.418018 139479515862784 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1989601105451584, loss=1.6394846439361572
I0310 08:26:02.503787 139479507470080 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2060917764902115, loss=1.7475955486297607
I0310 08:26:40.537019 139479515862784 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.20903760194778442, loss=1.7007087469100952
I0310 08:27:18.615767 139479507470080 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1911526918411255, loss=1.6702284812927246
I0310 08:27:56.701774 139479515862784 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.18086078763008118, loss=1.606723666191101
I0310 08:28:34.817390 139479507470080 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.18755362927913666, loss=1.666158676147461
I0310 08:29:12.929295 139479515862784 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.1943701058626175, loss=1.653595209121704
I0310 08:29:51.005616 139479507470080 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1904025822877884, loss=1.7387160062789917
I0310 08:30:29.136044 139479515862784 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.20387664437294006, loss=1.6902416944503784
I0310 08:31:07.254735 139479507470080 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.1926029920578003, loss=1.7657220363616943
I0310 08:31:45.321255 139479515862784 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.19042915105819702, loss=1.7086238861083984
I0310 08:32:23.395173 139479507470080 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.20180214941501617, loss=1.7578924894332886
I0310 08:33:01.426150 139479515862784 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.19488009810447693, loss=1.719082236289978
I0310 08:33:39.497489 139479507470080 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.21771949529647827, loss=1.7219946384429932
I0310 08:34:09.712544 139667344451392 spec.py:298] Evaluating on the training split.
I0310 08:34:12.704138 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 08:38:11.311342 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 08:38:13.948867 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 08:41:43.607285 139667344451392 spec.py:326] Evaluating on the test split.
I0310 08:41:46.299428 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 08:44:47.852665 139667344451392 submission_runner.py:359] Time since start: 32931.15s, 	Step: 50781, 	{'train/accuracy': 0.6651281714439392, 'train/loss': 1.554115891456604, 'train/bleu': 33.010174543235756, 'validation/accuracy': 0.6737052202224731, 'validation/loss': 1.5002410411834717, 'validation/bleu': 29.20247955242532, 'validation/num_examples': 3000, 'test/accuracy': 0.6858288645744324, 'test/loss': 1.4174050092697144, 'test/bleu': 28.728659102374237, 'test/num_examples': 3003}
I0310 08:44:47.864414 139479515862784 logging_writer.py:48] [50781] global_step=50781, preemption_count=0, score=19282.854325, test/accuracy=0.685829, test/bleu=28.728659, test/loss=1.417405, test/num_examples=3003, total_duration=32931.153572, train/accuracy=0.665128, train/bleu=33.010175, train/loss=1.554116, validation/accuracy=0.673705, validation/bleu=29.202480, validation/loss=1.500241, validation/num_examples=3000
I0310 08:44:48.843161 139667344451392 checkpoints.py:356] Saving checkpoint at step: 50781
I0310 08:44:53.081731 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_50781
I0310 08:44:53.086297 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_50781.
I0310 08:45:00.690609 139479507470080 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.20586271584033966, loss=1.611302375793457
I0310 08:45:38.646240 139479499077376 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.21039171516895294, loss=1.6874455213546753
I0310 08:46:16.681129 139479507470080 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.19771338999271393, loss=1.677100658416748
I0310 08:46:54.746044 139479499077376 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.20028838515281677, loss=1.759927749633789
I0310 08:47:32.823617 139479507470080 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1792699545621872, loss=1.726121425628662
I0310 08:48:10.899905 139479499077376 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.1895170360803604, loss=1.682535171508789
I0310 08:48:48.999927 139479507470080 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.1946215182542801, loss=1.6621440649032593
I0310 08:49:27.083984 139479499077376 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2176990807056427, loss=1.5911409854888916
I0310 08:50:05.201895 139479507470080 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.19810445606708527, loss=1.6992425918579102
I0310 08:50:43.273504 139479499077376 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.20932698249816895, loss=1.741614818572998
I0310 08:51:21.336228 139479507470080 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.23743334412574768, loss=1.7895101308822632
I0310 08:51:59.499952 139479499077376 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.20606893301010132, loss=1.6470367908477783
I0310 08:52:37.583203 139479507470080 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2278720587491989, loss=1.6960680484771729
I0310 08:53:15.693082 139479499077376 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.20819804072380066, loss=1.7378809452056885
I0310 08:53:53.817141 139479507470080 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.17736537754535675, loss=1.7226630449295044
I0310 08:54:31.983097 139479499077376 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.19926238059997559, loss=1.6808547973632812
I0310 08:55:10.062172 139479507470080 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1905972957611084, loss=1.758891224861145
I0310 08:55:48.180640 139479499077376 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19955545663833618, loss=1.7119501829147339
I0310 08:56:26.296932 139479507470080 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.20871573686599731, loss=1.7043050527572632
I0310 08:57:04.367449 139479499077376 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.20730511844158173, loss=1.6995073556900024
I0310 08:57:42.547081 139479507470080 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.19514432549476624, loss=1.6176975965499878
I0310 08:58:20.568080 139479499077376 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.19723589718341827, loss=1.689857840538025
I0310 08:58:53.461857 139667344451392 spec.py:298] Evaluating on the training split.
I0310 08:58:56.463247 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:03:22.893705 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 09:03:25.534579 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:07:03.300477 139667344451392 spec.py:326] Evaluating on the test split.
I0310 09:07:05.981110 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:10:41.381834 139667344451392 submission_runner.py:359] Time since start: 34414.90s, 	Step: 52988, 	{'train/accuracy': 0.6589625477790833, 'train/loss': 1.6013875007629395, 'train/bleu': 32.350600623363505, 'validation/accuracy': 0.6752427220344543, 'validation/loss': 1.4943979978561401, 'validation/bleu': 29.424971805114474, 'validation/num_examples': 3000, 'test/accuracy': 0.6882807612419128, 'test/loss': 1.4021090269088745, 'test/bleu': 28.97845816840377, 'test/num_examples': 3003}
I0310 09:10:41.394997 139479507470080 logging_writer.py:48] [52988] global_step=52988, preemption_count=0, score=20120.000185, test/accuracy=0.688281, test/bleu=28.978458, test/loss=1.402109, test/num_examples=3003, total_duration=34414.902884, train/accuracy=0.658963, train/bleu=32.350601, train/loss=1.601388, validation/accuracy=0.675243, validation/bleu=29.424972, validation/loss=1.494398, validation/num_examples=3000
I0310 09:10:42.686462 139667344451392 checkpoints.py:356] Saving checkpoint at step: 52988
I0310 09:10:46.649015 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_52988
I0310 09:10:46.652522 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_52988.
I0310 09:10:51.592269 139479499077376 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.1895192265510559, loss=1.6079579591751099
I0310 09:11:29.562077 139479490684672 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.20287726819515228, loss=1.7019627094268799
I0310 09:12:07.552667 139479499077376 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.19492869079113007, loss=1.6847716569900513
I0310 09:12:45.594643 139479490684672 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.19827215373516083, loss=1.6018905639648438
I0310 09:13:23.675456 139479499077376 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.2227543145418167, loss=1.6620391607284546
I0310 09:14:01.770955 139479490684672 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20631030201911926, loss=1.6961703300476074
I0310 09:14:39.891047 139479499077376 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.20284956693649292, loss=1.7256921529769897
I0310 09:15:17.983674 139479490684672 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.20786981284618378, loss=1.6709548234939575
I0310 09:15:56.030669 139479499077376 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.21367618441581726, loss=1.7693549394607544
I0310 09:16:34.132224 139479490684672 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.20489369332790375, loss=1.6852104663848877
I0310 09:17:12.181205 139479499077376 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.19220365583896637, loss=1.6840651035308838
I0310 09:17:50.279078 139479490684672 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.208525612950325, loss=1.6592577695846558
I0310 09:18:28.345397 139479499077376 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.19539587199687958, loss=1.664106011390686
I0310 09:19:06.463207 139479490684672 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.20914629101753235, loss=1.6937813758850098
I0310 09:19:44.492204 139479499077376 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19601571559906006, loss=1.5634900331497192
I0310 09:20:22.560798 139479490684672 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2378707230091095, loss=1.5992043018341064
I0310 09:21:00.588276 139479499077376 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.20711880922317505, loss=1.7673367261886597
I0310 09:21:38.658738 139479490684672 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.19818614423274994, loss=1.6679353713989258
I0310 09:22:16.734559 139479499077376 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2326396107673645, loss=1.7174321413040161
I0310 09:22:54.803144 139479490684672 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.18920600414276123, loss=1.6732484102249146
I0310 09:23:32.895983 139479499077376 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2040429711341858, loss=1.5872424840927124
I0310 09:24:11.046200 139479490684672 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.18502914905548096, loss=1.6477649211883545
I0310 09:24:46.988421 139667344451392 spec.py:298] Evaluating on the training split.
I0310 09:24:49.968626 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:29:04.548075 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 09:29:07.192364 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:32:07.245885 139667344451392 spec.py:326] Evaluating on the test split.
I0310 09:32:09.938945 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:34:43.934626 139667344451392 submission_runner.py:359] Time since start: 35968.43s, 	Step: 55196, 	{'train/accuracy': 0.6581787467002869, 'train/loss': 1.6115548610687256, 'train/bleu': 32.77269833835701, 'validation/accuracy': 0.6750071048736572, 'validation/loss': 1.4914934635162354, 'validation/bleu': 29.233946453709738, 'validation/num_examples': 3000, 'test/accuracy': 0.6896287202835083, 'test/loss': 1.4000343084335327, 'test/bleu': 29.13969687460392, 'test/num_examples': 3003}
I0310 09:34:43.946582 139479499077376 logging_writer.py:48] [55196] global_step=55196, preemption_count=0, score=20956.616995, test/accuracy=0.689629, test/bleu=29.139697, test/loss=1.400034, test/num_examples=3003, total_duration=35968.429433, train/accuracy=0.658179, train/bleu=32.772698, train/loss=1.611555, validation/accuracy=0.675007, validation/bleu=29.233946, validation/loss=1.491493, validation/num_examples=3000
I0310 09:34:44.952939 139667344451392 checkpoints.py:356] Saving checkpoint at step: 55196
I0310 09:34:48.319879 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_55196
I0310 09:34:48.324347 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_55196.
I0310 09:34:50.243442 139479490684672 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1954634189605713, loss=1.5968643426895142
I0310 09:35:28.192020 139479482291968 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1982785314321518, loss=1.7118587493896484
I0310 09:36:06.176457 139479490684672 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.19307932257652283, loss=1.6809707880020142
I0310 09:36:44.221184 139479482291968 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.23268680274486542, loss=1.6558587551116943
I0310 09:37:22.310609 139479490684672 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.20182675123214722, loss=1.629038691520691
I0310 09:38:00.466221 139479482291968 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19763822853565216, loss=1.6283178329467773
I0310 09:38:38.485018 139479490684672 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.19081291556358337, loss=1.6522961854934692
I0310 09:39:16.511583 139479482291968 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.19561247527599335, loss=1.6274125576019287
I0310 09:39:54.583373 139479490684672 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.19146840274333954, loss=1.5760035514831543
I0310 09:40:32.683520 139479482291968 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.23106393218040466, loss=1.7065826654434204
I0310 09:41:10.804615 139479490684672 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.20624209940433502, loss=1.656003475189209
I0310 09:41:48.908425 139479482291968 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.20691071450710297, loss=1.7014849185943604
I0310 09:42:27.009731 139479490684672 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.20500271022319794, loss=1.6308873891830444
I0310 09:43:05.096415 139479482291968 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2045392543077469, loss=1.6552693843841553
I0310 09:43:43.157823 139479490684672 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.20641259849071503, loss=1.712170124053955
I0310 09:44:21.261443 139479482291968 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.19283679127693176, loss=1.6579546928405762
I0310 09:44:59.367581 139479490684672 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1914888173341751, loss=1.6274139881134033
I0310 09:45:37.521561 139479482291968 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.19804653525352478, loss=1.7017848491668701
I0310 09:46:15.623673 139479490684672 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21998311579227448, loss=1.626214861869812
I0310 09:46:53.718590 139479482291968 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2048758864402771, loss=1.686495304107666
I0310 09:47:31.821778 139479490684672 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1939418613910675, loss=1.6053165197372437
I0310 09:48:09.970447 139479482291968 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.18910320103168488, loss=1.709528923034668
I0310 09:48:48.044742 139479490684672 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.22300273180007935, loss=1.6610254049301147
I0310 09:48:48.528146 139667344451392 spec.py:298] Evaluating on the training split.
I0310 09:48:51.533854 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:52:23.631854 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 09:52:26.257478 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:55:22.062589 139667344451392 spec.py:326] Evaluating on the test split.
I0310 09:55:24.758085 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 09:58:44.977622 139667344451392 submission_runner.py:359] Time since start: 37409.97s, 	Step: 57403, 	{'train/accuracy': 0.6688748002052307, 'train/loss': 1.529470443725586, 'train/bleu': 32.8679719597202, 'validation/accuracy': 0.6767057776451111, 'validation/loss': 1.4770402908325195, 'validation/bleu': 29.576737294651515, 'validation/num_examples': 3000, 'test/accuracy': 0.6911161541938782, 'test/loss': 1.3892191648483276, 'test/bleu': 29.367747552592764, 'test/num_examples': 3003}
I0310 09:58:44.989729 139479482291968 logging_writer.py:48] [57403] global_step=57403, preemption_count=0, score=21793.714373, test/accuracy=0.691116, test/bleu=29.367748, test/loss=1.389219, test/num_examples=3003, total_duration=37409.969158, train/accuracy=0.668875, train/bleu=32.867972, train/loss=1.529470, validation/accuracy=0.676706, validation/bleu=29.576737, validation/loss=1.477040, validation/num_examples=3000
I0310 09:58:45.965645 139667344451392 checkpoints.py:356] Saving checkpoint at step: 57403
I0310 09:58:49.340005 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_57403
I0310 09:58:49.344505 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_57403.
I0310 09:59:26.560110 139479490684672 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1919122189283371, loss=1.5955318212509155
I0310 10:00:04.621169 139479473899264 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19337347149848938, loss=1.6670373678207397
I0310 10:00:42.694047 139479490684672 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3981859087944031, loss=1.7974708080291748
I0310 10:01:20.795361 139479473899264 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.21466650068759918, loss=1.689976692199707
I0310 10:01:58.932654 139479490684672 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2147701233625412, loss=1.5542470216751099
I0310 10:02:36.966788 139479473899264 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.18973076343536377, loss=1.6309903860092163
I0310 10:03:15.068538 139479490684672 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.19488643109798431, loss=1.664108157157898
I0310 10:03:53.155904 139479473899264 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19206742942333221, loss=1.6373660564422607
I0310 10:04:31.204982 139479490684672 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.18961027264595032, loss=1.6403923034667969
I0310 10:05:09.328199 139479473899264 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.30346983671188354, loss=1.6648149490356445
I0310 10:05:47.380485 139479490684672 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.18409284949302673, loss=1.6363362073898315
I0310 10:06:25.460446 139479473899264 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.19196712970733643, loss=1.6515750885009766
I0310 10:07:03.506283 139479490684672 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2136007398366928, loss=1.708077311515808
I0310 10:07:41.602534 139479473899264 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.42359986901283264, loss=1.6694982051849365
I0310 10:08:19.685040 139479490684672 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.19909685850143433, loss=1.5881954431533813
I0310 10:08:57.757326 139479473899264 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.19575148820877075, loss=1.6341348886489868
I0310 10:09:35.858776 139479490684672 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.1886698454618454, loss=1.6003549098968506
I0310 10:10:13.968402 139479473899264 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.19214613735675812, loss=1.5784211158752441
I0310 10:10:52.070960 139479490684672 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.1908116191625595, loss=1.6578959226608276
I0310 10:11:30.207749 139479473899264 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.20167487859725952, loss=1.6376897096633911
I0310 10:12:08.312352 139479490684672 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.19377504289150238, loss=1.6742790937423706
I0310 10:12:46.483975 139479473899264 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.19497409462928772, loss=1.6413365602493286
I0310 10:12:49.619622 139667344451392 spec.py:298] Evaluating on the training split.
I0310 10:12:52.606805 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 10:17:05.600571 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 10:17:08.245220 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 10:21:13.069193 139667344451392 spec.py:326] Evaluating on the test split.
I0310 10:21:15.751788 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 10:25:22.845373 139667344451392 submission_runner.py:359] Time since start: 38851.06s, 	Step: 59610, 	{'train/accuracy': 0.6687506437301636, 'train/loss': 1.5389645099639893, 'train/bleu': 33.386464967585454, 'validation/accuracy': 0.6782432794570923, 'validation/loss': 1.4693187475204468, 'validation/bleu': 29.260053902552222, 'validation/num_examples': 3000, 'test/accuracy': 0.6937307715415955, 'test/loss': 1.3724411725997925, 'test/bleu': 29.472963709040275, 'test/num_examples': 3003}
I0310 10:25:22.860044 139479490684672 logging_writer.py:48] [59610] global_step=59610, preemption_count=0, score=22630.779332, test/accuracy=0.693731, test/bleu=29.472964, test/loss=1.372441, test/num_examples=3003, total_duration=38851.060637, train/accuracy=0.668751, train/bleu=33.386465, train/loss=1.538965, validation/accuracy=0.678243, validation/bleu=29.260054, validation/loss=1.469319, validation/num_examples=3000
I0310 10:25:24.171147 139667344451392 checkpoints.py:356] Saving checkpoint at step: 59610
I0310 10:25:29.086311 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_59610
I0310 10:25:29.089879 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_59610.
I0310 10:26:03.622260 139479473899264 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.19555000960826874, loss=1.6137937307357788
I0310 10:26:41.590854 139479465506560 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.19859449565410614, loss=1.634692668914795
I0310 10:27:19.635669 139479473899264 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19770143926143646, loss=1.5824605226516724
I0310 10:27:57.664501 139479465506560 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.21066802740097046, loss=1.5961381196975708
I0310 10:28:35.718259 139479473899264 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.22022292017936707, loss=1.7073924541473389
I0310 10:29:13.821707 139479465506560 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.18708789348602295, loss=1.5891672372817993
I0310 10:29:51.916046 139479473899264 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.21059651672840118, loss=1.6201597452163696
I0310 10:30:30.012019 139479465506560 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20729193091392517, loss=1.610865592956543
I0310 10:31:08.100840 139479473899264 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.18692730367183685, loss=1.6731501817703247
I0310 10:31:46.232381 139479465506560 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1954905241727829, loss=1.648542881011963
I0310 10:32:24.359230 139479473899264 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.19139985740184784, loss=1.557924509048462
I0310 10:33:02.482376 139479465506560 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.23498056828975677, loss=1.6900147199630737
I0310 10:33:40.574970 139479473899264 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2097012847661972, loss=1.69170081615448
I0310 10:34:18.706717 139479465506560 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.19940593838691711, loss=1.642987608909607
I0310 10:34:56.779639 139479473899264 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.20191164314746857, loss=1.6427637338638306
I0310 10:35:34.853288 139479465506560 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.18970739841461182, loss=1.6667169332504272
I0310 10:36:12.917732 139479473899264 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.20190374553203583, loss=1.6744128465652466
I0310 10:36:50.943933 139479465506560 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.19511227309703827, loss=1.6660798788070679
I0310 10:37:29.080028 139479473899264 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.1879018396139145, loss=1.6089876890182495
I0310 10:38:07.157344 139479465506560 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.20636717975139618, loss=1.5912374258041382
I0310 10:38:45.273550 139479473899264 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.20023512840270996, loss=1.6315144300460815
I0310 10:39:23.330482 139479465506560 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.20224057137966156, loss=1.6925173997879028
I0310 10:39:29.120801 139667344451392 spec.py:298] Evaluating on the training split.
I0310 10:39:32.123666 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 10:43:42.081188 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 10:43:44.719083 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 10:46:16.103471 139667344451392 spec.py:326] Evaluating on the test split.
I0310 10:46:18.784613 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 10:48:31.926851 139667344451392 submission_runner.py:359] Time since start: 40450.56s, 	Step: 61817, 	{'train/accuracy': 0.6634488701820374, 'train/loss': 1.5767509937286377, 'train/bleu': 32.65168481297114, 'validation/accuracy': 0.6803635358810425, 'validation/loss': 1.4587160348892212, 'validation/bleu': 29.661452102767296, 'validation/num_examples': 3000, 'test/accuracy': 0.6965312957763672, 'test/loss': 1.3633774518966675, 'test/bleu': 29.656919652276386, 'test/num_examples': 3003}
I0310 10:48:31.941237 139479473899264 logging_writer.py:48] [61817] global_step=61817, preemption_count=0, score=23467.180179, test/accuracy=0.696531, test/bleu=29.656920, test/loss=1.363377, test/num_examples=3003, total_duration=40450.561834, train/accuracy=0.663449, train/bleu=32.651685, train/loss=1.576751, validation/accuracy=0.680364, validation/bleu=29.661452, validation/loss=1.458716, validation/num_examples=3000
I0310 10:48:32.956864 139667344451392 checkpoints.py:356] Saving checkpoint at step: 61817
I0310 10:48:36.317635 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_61817
I0310 10:48:36.322954 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_61817.
I0310 10:49:08.216389 139479465506560 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2036214917898178, loss=1.6070947647094727
I0310 10:49:46.138367 139479457113856 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.1870952695608139, loss=1.698148488998413
I0310 10:50:24.181469 139479465506560 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2049897462129593, loss=1.594415545463562
I0310 10:51:02.204967 139479457113856 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.21610742807388306, loss=1.6295527219772339
I0310 10:51:40.324696 139479465506560 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.19088374078273773, loss=1.5856987237930298
I0310 10:52:18.366081 139479457113856 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.19700874388217926, loss=1.6529651880264282
I0310 10:52:56.476862 139479465506560 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.21458348631858826, loss=1.6454980373382568
I0310 10:53:34.636526 139479457113856 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.19583868980407715, loss=1.6307499408721924
I0310 10:54:12.755321 139479465506560 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.20433591306209564, loss=1.7367902994155884
I0310 10:54:50.883763 139479457113856 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.22268860042095184, loss=1.7207911014556885
I0310 10:55:28.994744 139479465506560 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.20733466744422913, loss=1.6830376386642456
I0310 10:56:07.125546 139479457113856 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.21471811830997467, loss=1.6148326396942139
I0310 10:56:45.223729 139479465506560 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.19355754554271698, loss=1.5846930742263794
I0310 10:57:23.239450 139479457113856 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.20631398260593414, loss=1.6466844081878662
I0310 10:58:01.368307 139479465506560 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.18941789865493774, loss=1.6109685897827148
I0310 10:58:39.485147 139479457113856 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2170172780752182, loss=1.6900898218154907
I0310 10:59:17.524900 139479465506560 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.20375332236289978, loss=1.6275454759597778
I0310 10:59:55.581926 139479457113856 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.20648281276226044, loss=1.677385687828064
I0310 11:00:33.653394 139479465506560 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.19379614293575287, loss=1.6171969175338745
I0310 11:01:11.704787 139479457113856 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.22244778275489807, loss=1.651855707168579
I0310 11:01:49.777481 139479465506560 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.207085981965065, loss=1.65337073802948
I0310 11:02:27.848504 139479457113856 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.19901058077812195, loss=1.6340625286102295
I0310 11:02:36.675252 139667344451392 spec.py:298] Evaluating on the training split.
I0310 11:02:39.660983 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:07:10.299459 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 11:07:12.934407 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:10:22.900028 139667344451392 spec.py:326] Evaluating on the test split.
I0310 11:10:25.597532 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:13:33.281218 139667344451392 submission_runner.py:359] Time since start: 41838.12s, 	Step: 64025, 	{'train/accuracy': 0.6724666953086853, 'train/loss': 1.5077913999557495, 'train/bleu': 33.49897729036829, 'validation/accuracy': 0.6825953722000122, 'validation/loss': 1.4473845958709717, 'validation/bleu': 29.688873026658328, 'validation/num_examples': 3000, 'test/accuracy': 0.6963453888893127, 'test/loss': 1.3594682216644287, 'test/bleu': 29.641632742723417, 'test/num_examples': 3003}
I0310 11:13:33.293605 139479465506560 logging_writer.py:48] [64025] global_step=64025, preemption_count=0, score=24304.398917, test/accuracy=0.696345, test/bleu=29.641633, test/loss=1.359468, test/num_examples=3003, total_duration=41838.116276, train/accuracy=0.672467, train/bleu=33.498977, train/loss=1.507791, validation/accuracy=0.682595, validation/bleu=29.688873, validation/loss=1.447385, validation/num_examples=3000
I0310 11:13:34.269965 139667344451392 checkpoints.py:356] Saving checkpoint at step: 64025
I0310 11:13:38.171660 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_64025
I0310 11:13:38.176096 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_64025.
I0310 11:14:07.012959 139479457113856 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.20881743729114532, loss=1.6017146110534668
I0310 11:14:44.933568 139479448721152 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2193276733160019, loss=1.7157262563705444
I0310 11:15:22.919891 139479457113856 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19830955564975739, loss=1.6019617319107056
I0310 11:16:00.970416 139479448721152 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.21947382390499115, loss=1.5590782165527344
I0310 11:16:38.993381 139479457113856 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.18866360187530518, loss=1.6422415971755981
I0310 11:17:17.018366 139479448721152 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.24334914982318878, loss=1.6737573146820068
I0310 11:17:55.107952 139479457113856 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.20743778347969055, loss=1.6556485891342163
I0310 11:18:33.182139 139479448721152 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.1940370351076126, loss=1.5774519443511963
I0310 11:19:11.220053 139479457113856 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20216654241085052, loss=1.6265568733215332
I0310 11:19:49.317827 139479448721152 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19424642622470856, loss=1.5760400295257568
I0310 11:20:27.382841 139479457113856 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2016879767179489, loss=1.5907161235809326
I0310 11:21:05.519915 139479448721152 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.19868658483028412, loss=1.6091934442520142
I0310 11:21:43.634263 139479457113856 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.20360010862350464, loss=1.6250710487365723
I0310 11:22:21.691019 139479448721152 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.18696770071983337, loss=1.5994219779968262
I0310 11:22:59.723083 139479457113856 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.20654410123825073, loss=1.5939602851867676
I0310 11:23:37.770664 139479448721152 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.21284279227256775, loss=1.659179449081421
I0310 11:24:15.868679 139479457113856 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20420661568641663, loss=1.5502387285232544
I0310 11:24:53.917792 139479448721152 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.2055789828300476, loss=1.5869168043136597
I0310 11:25:32.013098 139479457113856 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.20443087816238403, loss=1.6587228775024414
I0310 11:26:10.119404 139479448721152 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.21778661012649536, loss=1.6730202436447144
I0310 11:26:48.189272 139479457113856 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.20438791811466217, loss=1.585087537765503
I0310 11:27:26.267815 139479448721152 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.21211113035678864, loss=1.5941141843795776
I0310 11:27:38.559630 139667344451392 spec.py:298] Evaluating on the training split.
I0310 11:27:41.559265 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:31:57.994946 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 11:32:00.637885 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:35:31.535746 139667344451392 spec.py:326] Evaluating on the test split.
I0310 11:35:34.241638 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:39:22.078073 139667344451392 submission_runner.py:359] Time since start: 43340.00s, 	Step: 66234, 	{'train/accuracy': 0.671855092048645, 'train/loss': 1.5220634937286377, 'train/bleu': 33.673221140985994, 'validation/accuracy': 0.6824465990066528, 'validation/loss': 1.4436591863632202, 'validation/bleu': 29.791277737131875, 'validation/num_examples': 3000, 'test/accuracy': 0.6987972855567932, 'test/loss': 1.3430780172348022, 'test/bleu': 29.83415922910861, 'test/num_examples': 3003}
I0310 11:39:22.090644 139479457113856 logging_writer.py:48] [66234] global_step=66234, preemption_count=0, score=25141.714159, test/accuracy=0.698797, test/bleu=29.834159, test/loss=1.343078, test/num_examples=3003, total_duration=43340.000637, train/accuracy=0.671855, train/bleu=33.673221, train/loss=1.522063, validation/accuracy=0.682447, validation/bleu=29.791278, validation/loss=1.443659, validation/num_examples=3000
I0310 11:39:23.063419 139667344451392 checkpoints.py:356] Saving checkpoint at step: 66234
I0310 11:39:27.532835 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_66234
I0310 11:39:27.537294 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_66234.
I0310 11:39:52.981247 139479448721152 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.20793122053146362, loss=1.5895450115203857
I0310 11:40:30.946269 139479440328448 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.18937189877033234, loss=1.5001111030578613
I0310 11:41:08.925270 139479448721152 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.1914893388748169, loss=1.5645850896835327
I0310 11:41:46.966048 139479440328448 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2014625370502472, loss=1.6456289291381836
I0310 11:42:25.016950 139479448721152 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.18982574343681335, loss=1.6191848516464233
I0310 11:43:03.126678 139479440328448 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.23487761616706848, loss=1.5468471050262451
I0310 11:43:41.153913 139479448721152 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.20340901613235474, loss=1.5650546550750732
I0310 11:44:19.257216 139479440328448 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.20363830029964447, loss=1.6436539888381958
I0310 11:44:57.311667 139479448721152 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.2119627594947815, loss=1.6044623851776123
I0310 11:45:35.366073 139479440328448 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.19924429059028625, loss=1.5730106830596924
I0310 11:46:13.452950 139479448721152 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.22751888632774353, loss=1.6013318300247192
I0310 11:46:51.528602 139479440328448 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.21396026015281677, loss=1.6060447692871094
I0310 11:47:29.606672 139479448721152 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.20105013251304626, loss=1.5560946464538574
I0310 11:48:07.632394 139479440328448 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2061813324689865, loss=1.581115961074829
I0310 11:48:45.708777 139479448721152 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.195649653673172, loss=1.5541945695877075
I0310 11:49:23.793139 139479440328448 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.20427939295768738, loss=1.6187782287597656
I0310 11:50:01.860308 139479448721152 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.21581503748893738, loss=1.577185869216919
I0310 11:50:39.932019 139479440328448 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.19807599484920502, loss=1.5671114921569824
I0310 11:51:18.028267 139479448721152 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.23114880919456482, loss=1.6175211668014526
I0310 11:51:56.125334 139479440328448 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.21094900369644165, loss=1.6368017196655273
I0310 11:52:34.193771 139479448721152 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.19685311615467072, loss=1.6358659267425537
I0310 11:53:12.245351 139479440328448 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2161247879266739, loss=1.5244215726852417
I0310 11:53:27.591281 139667344451392 spec.py:298] Evaluating on the training split.
I0310 11:53:30.589586 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:57:18.540529 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 11:57:21.191689 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 11:59:51.107537 139667344451392 spec.py:326] Evaluating on the test split.
I0310 11:59:53.781967 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:02:05.959974 139667344451392 submission_runner.py:359] Time since start: 44889.03s, 	Step: 68442, 	{'train/accuracy': 0.6690624356269836, 'train/loss': 1.536400556564331, 'train/bleu': 33.65930010309209, 'validation/accuracy': 0.6832773089408875, 'validation/loss': 1.4335070848464966, 'validation/bleu': 29.61420765126098, 'validation/num_examples': 3000, 'test/accuracy': 0.6984370350837708, 'test/loss': 1.3391729593276978, 'test/bleu': 29.46285816369212, 'test/num_examples': 3003}
I0310 12:02:05.972708 139479448721152 logging_writer.py:48] [68442] global_step=68442, preemption_count=0, score=25978.660173, test/accuracy=0.698437, test/bleu=29.462858, test/loss=1.339173, test/num_examples=3003, total_duration=44889.032288, train/accuracy=0.669062, train/bleu=33.659300, train/loss=1.536401, validation/accuracy=0.683277, validation/bleu=29.614208, validation/loss=1.433507, validation/num_examples=3000
I0310 12:02:06.947484 139667344451392 checkpoints.py:356] Saving checkpoint at step: 68442
I0310 12:02:10.646256 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_68442
I0310 12:02:10.650794 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_68442.
I0310 12:02:32.996040 139479440328448 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.19885781407356262, loss=1.5781883001327515
I0310 12:03:10.923889 139479431935744 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2018817663192749, loss=1.5601019859313965
I0310 12:03:49.000273 139479440328448 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.1952970176935196, loss=1.6146994829177856
I0310 12:04:27.057354 139479431935744 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.19580182433128357, loss=1.6024854183197021
I0310 12:05:05.177605 139479440328448 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.20564302802085876, loss=1.6035913228988647
I0310 12:05:43.254578 139479431935744 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2159462422132492, loss=1.5710983276367188
I0310 12:06:21.273894 139479440328448 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.21856023371219635, loss=1.5764293670654297
I0310 12:06:59.404883 139479431935744 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.1930924355983734, loss=1.571769118309021
I0310 12:07:37.439440 139479440328448 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.20072734355926514, loss=1.5439523458480835
I0310 12:08:15.479909 139479431935744 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.19516028463840485, loss=1.5649737119674683
I0310 12:08:53.547739 139479440328448 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.3820825815200806, loss=1.7157975435256958
I0310 12:09:31.622454 139479431935744 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.20855042338371277, loss=1.5580904483795166
I0310 12:10:09.697282 139479440328448 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.22524864971637726, loss=1.5580642223358154
I0310 12:10:47.769212 139479431935744 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.20812658965587616, loss=1.5713545083999634
I0310 12:11:25.873445 139479440328448 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2088274359703064, loss=1.6925193071365356
I0310 12:12:03.963989 139479431935744 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.19938303530216217, loss=1.5612962245941162
I0310 12:12:42.114750 139479440328448 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.22278712689876556, loss=1.613180160522461
I0310 12:13:20.197381 139479431935744 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2154979705810547, loss=1.5524030923843384
I0310 12:13:58.277533 139479440328448 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.19734591245651245, loss=1.5611498355865479
I0310 12:14:36.317555 139479431935744 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.20109601318836212, loss=1.6256060600280762
I0310 12:15:14.406171 139479440328448 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.21479645371437073, loss=1.6427409648895264
I0310 12:15:52.458464 139479431935744 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.21124038100242615, loss=1.613984227180481
I0310 12:16:10.782422 139667344451392 spec.py:298] Evaluating on the training split.
I0310 12:16:13.771546 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:20:32.772280 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 12:20:35.407991 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:23:36.659575 139667344451392 spec.py:326] Evaluating on the test split.
I0310 12:23:39.360397 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:26:45.921887 139667344451392 submission_runner.py:359] Time since start: 46252.22s, 	Step: 70650, 	{'train/accuracy': 0.6790371537208557, 'train/loss': 1.468093752861023, 'train/bleu': 34.16825540146199, 'validation/accuracy': 0.6862903237342834, 'validation/loss': 1.421138048171997, 'validation/bleu': 30.222751241209593, 'validation/num_examples': 3000, 'test/accuracy': 0.7009819746017456, 'test/loss': 1.3293074369430542, 'test/bleu': 29.841257767057396, 'test/num_examples': 3003}
I0310 12:26:45.935245 139479440328448 logging_writer.py:48] [70650] global_step=70650, preemption_count=0, score=26815.657141, test/accuracy=0.700982, test/bleu=29.841258, test/loss=1.329307, test/num_examples=3003, total_duration=46252.223451, train/accuracy=0.679037, train/bleu=34.168255, train/loss=1.468094, validation/accuracy=0.686290, validation/bleu=30.222751, validation/loss=1.421138, validation/num_examples=3000
I0310 12:26:46.921271 139667344451392 checkpoints.py:356] Saving checkpoint at step: 70650
I0310 12:26:50.269604 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_70650
I0310 12:26:50.274054 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_70650.
I0310 12:27:09.623513 139479431935744 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.20407846570014954, loss=1.5602701902389526
I0310 12:27:47.607006 139479423543040 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.20461687445640564, loss=1.5975186824798584
I0310 12:28:25.660237 139479431935744 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.21900597214698792, loss=1.5970381498336792
I0310 12:29:03.723893 139479423543040 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.2572324573993683, loss=1.6882941722869873
I0310 12:29:41.763003 139479431935744 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2016940861940384, loss=1.6032538414001465
I0310 12:30:19.866995 139479423543040 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.21755412220954895, loss=1.5925161838531494
I0310 12:30:57.872018 139479431935744 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.20167602598667145, loss=1.4903225898742676
I0310 12:31:35.984527 139479423543040 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.20002563297748566, loss=1.5757478475570679
I0310 12:32:13.961006 139479431935744 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2082921862602234, loss=1.557751178741455
I0310 12:32:52.070360 139479423543040 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.20519232749938965, loss=1.5892727375030518
I0310 12:33:30.131804 139479431935744 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.22552978992462158, loss=1.5445871353149414
I0310 12:34:08.186252 139479423543040 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.2004571408033371, loss=1.623389720916748
I0310 12:34:46.258402 139479431935744 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.21185234189033508, loss=1.5772095918655396
I0310 12:35:24.347409 139479423543040 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.21637943387031555, loss=1.639960765838623
I0310 12:36:02.455909 139479431935744 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.20128442347049713, loss=1.5356504917144775
I0310 12:36:40.545066 139479423543040 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.21468240022659302, loss=1.6033592224121094
I0310 12:37:18.586523 139479431935744 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.202006995677948, loss=1.5283795595169067
I0310 12:37:56.660535 139479423543040 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.21695555746555328, loss=1.5419780015945435
I0310 12:38:34.712957 139479431935744 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2056710124015808, loss=1.6252354383468628
I0310 12:39:12.792050 139479423543040 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.19034141302108765, loss=1.591941237449646
I0310 12:39:50.834535 139479431935744 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2041938155889511, loss=1.5657594203948975
I0310 12:40:28.918293 139479423543040 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.22450676560401917, loss=1.6662338972091675
I0310 12:40:50.328250 139667344451392 spec.py:298] Evaluating on the training split.
I0310 12:40:53.325474 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:45:15.856245 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 12:45:18.500920 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:48:39.696597 139667344451392 spec.py:326] Evaluating on the test split.
I0310 12:48:42.405603 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 12:51:58.874105 139667344451392 submission_runner.py:359] Time since start: 47731.77s, 	Step: 72858, 	{'train/accuracy': 0.6763147115707397, 'train/loss': 1.483642816543579, 'train/bleu': 33.30129068983058, 'validation/accuracy': 0.6865134835243225, 'validation/loss': 1.414038896560669, 'validation/bleu': 30.19502093341028, 'validation/num_examples': 3000, 'test/accuracy': 0.7019231915473938, 'test/loss': 1.3191272020339966, 'test/bleu': 30.02672152046085, 'test/num_examples': 3003}
I0310 12:51:58.887775 139479431935744 logging_writer.py:48] [72858] global_step=72858, preemption_count=0, score=27652.425802, test/accuracy=0.701923, test/bleu=30.026722, test/loss=1.319127, test/num_examples=3003, total_duration=47731.769279, train/accuracy=0.676315, train/bleu=33.301291, train/loss=1.483643, validation/accuracy=0.686513, validation/bleu=30.195021, validation/loss=1.414039, validation/num_examples=3000
I0310 12:51:59.868777 139667344451392 checkpoints.py:356] Saving checkpoint at step: 72858
I0310 12:52:03.199835 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_72858
I0310 12:52:03.204423 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_72858.
I0310 12:52:19.508896 139479423543040 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.20597463846206665, loss=1.5258285999298096
I0310 12:52:57.478590 139479415150336 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.219620481133461, loss=1.539686679840088
I0310 12:53:35.467169 139479423543040 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2173670083284378, loss=1.574249267578125
I0310 12:54:13.527042 139479415150336 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.19872364401817322, loss=1.568434715270996
I0310 12:54:51.560409 139479423543040 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.21405677497386932, loss=1.4862112998962402
I0310 12:55:29.651188 139479415150336 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2023923397064209, loss=1.5412406921386719
I0310 12:56:07.731367 139479423543040 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.19670267403125763, loss=1.5222340822219849
I0310 12:56:45.755975 139479415150336 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.21580199897289276, loss=1.579122543334961
I0310 12:57:23.841425 139479423543040 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2025754302740097, loss=1.4947400093078613
I0310 12:58:01.893274 139479415150336 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.2072814553976059, loss=1.5292437076568604
I0310 12:58:39.974011 139479423543040 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.21877935528755188, loss=1.5990532636642456
I0310 12:59:18.056608 139479415150336 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.21573194861412048, loss=1.6020045280456543
I0310 12:59:56.108902 139479423543040 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20444345474243164, loss=1.5112563371658325
I0310 13:00:34.148678 139479415150336 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.22613094747066498, loss=1.5905505418777466
I0310 13:01:12.276719 139479423543040 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.21228255331516266, loss=1.5763862133026123
I0310 13:01:50.352231 139479415150336 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2061087191104889, loss=1.5779709815979004
I0310 13:02:28.392041 139479423543040 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.2212657630443573, loss=1.5879210233688354
I0310 13:03:06.462831 139479415150336 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.21041597425937653, loss=1.527886986732483
I0310 13:03:44.512270 139479423543040 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.19907164573669434, loss=1.508642315864563
I0310 13:04:22.591319 139479415150336 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.21646159887313843, loss=1.5979245901107788
I0310 13:05:00.750136 139479423543040 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.20616883039474487, loss=1.5786720514297485
I0310 13:05:38.851732 139479415150336 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.21383526921272278, loss=1.5751044750213623
I0310 13:06:03.298136 139667344451392 spec.py:298] Evaluating on the training split.
I0310 13:06:06.291002 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:10:03.586362 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 13:10:06.228745 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:13:13.757465 139667344451392 spec.py:326] Evaluating on the test split.
I0310 13:13:16.460487 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:16:12.112587 139667344451392 submission_runner.py:359] Time since start: 49244.74s, 	Step: 75066, 	{'train/accuracy': 0.6912358999252319, 'train/loss': 1.4028245210647583, 'train/bleu': 35.08809718531821, 'validation/accuracy': 0.687889814376831, 'validation/loss': 1.4114909172058105, 'validation/bleu': 30.30036838579653, 'validation/num_examples': 3000, 'test/accuracy': 0.7035965323448181, 'test/loss': 1.3124353885650635, 'test/bleu': 30.03725170320686, 'test/num_examples': 3003}
I0310 13:16:12.126600 139479423543040 logging_writer.py:48] [75066] global_step=75066, preemption_count=0, score=28489.409359, test/accuracy=0.703597, test/bleu=30.037252, test/loss=1.312435, test/num_examples=3003, total_duration=49244.739167, train/accuracy=0.691236, train/bleu=35.088097, train/loss=1.402825, validation/accuracy=0.687890, validation/bleu=30.300368, validation/loss=1.411491, validation/num_examples=3000
I0310 13:16:13.101958 139667344451392 checkpoints.py:356] Saving checkpoint at step: 75066
I0310 13:16:16.451689 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_75066
I0310 13:16:16.456201 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_75066.
I0310 13:16:29.724031 139479415150336 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2156946361064911, loss=1.5186595916748047
I0310 13:17:07.691134 139479406757632 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20456957817077637, loss=1.5672649145126343
I0310 13:17:45.683796 139479415150336 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.22768127918243408, loss=1.5714370012283325
I0310 13:18:23.674139 139479406757632 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.21009279787540436, loss=1.548309087753296
I0310 13:19:01.748232 139479415150336 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.2154376059770584, loss=1.5363456010818481
I0310 13:19:39.792372 139479406757632 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2166188508272171, loss=1.5176986455917358
I0310 13:20:17.869199 139479415150336 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.21087683737277985, loss=1.524146556854248
I0310 13:20:55.944224 139479406757632 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.22127822041511536, loss=1.6152081489562988
I0310 13:21:33.982490 139479415150336 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.20407547056674957, loss=1.5795775651931763
I0310 13:22:11.993610 139479406757632 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20789042115211487, loss=1.614966630935669
I0310 13:22:50.040613 139479415150336 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.22364650666713715, loss=1.535348653793335
I0310 13:23:28.114502 139479406757632 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21628431975841522, loss=1.51786470413208
I0310 13:24:06.256805 139479415150336 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.21378450095653534, loss=1.4952386617660522
I0310 13:24:44.371176 139479406757632 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.21334382891654968, loss=1.593874216079712
I0310 13:25:22.411982 139479415150336 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2034241259098053, loss=1.4996399879455566
I0310 13:26:00.498544 139479406757632 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.21868063509464264, loss=1.4296092987060547
I0310 13:26:38.572993 139479415150336 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.21361958980560303, loss=1.5538512468338013
I0310 13:27:16.687411 139479406757632 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.2064504325389862, loss=1.6538392305374146
I0310 13:27:54.686884 139479415150336 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.20819289982318878, loss=1.5126707553863525
I0310 13:28:32.807783 139479406757632 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.20566080510616302, loss=1.50929594039917
I0310 13:29:10.931421 139479415150336 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.20726603269577026, loss=1.5739318132400513
I0310 13:29:48.959427 139479406757632 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.20875748991966248, loss=1.4856886863708496
I0310 13:30:16.479774 139667344451392 spec.py:298] Evaluating on the training split.
I0310 13:30:19.480606 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:34:37.649693 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 13:34:40.280307 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:37:50.933287 139667344451392 spec.py:326] Evaluating on the test split.
I0310 13:37:53.623029 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:41:20.633715 139667344451392 submission_runner.py:359] Time since start: 50697.92s, 	Step: 77274, 	{'train/accuracy': 0.6815696358680725, 'train/loss': 1.4528909921646118, 'train/bleu': 34.45791832931259, 'validation/accuracy': 0.6889933347702026, 'validation/loss': 1.4010437726974487, 'validation/bleu': 30.275694084382916, 'validation/num_examples': 3000, 'test/accuracy': 0.7051304578781128, 'test/loss': 1.3022544384002686, 'test/bleu': 30.322258281412612, 'test/num_examples': 3003}
I0310 13:41:20.648206 139479415150336 logging_writer.py:48] [77274] global_step=77274, preemption_count=0, score=29326.320791, test/accuracy=0.705130, test/bleu=30.322258, test/loss=1.302254, test/num_examples=3003, total_duration=50697.920795, train/accuracy=0.681570, train/bleu=34.457918, train/loss=1.452891, validation/accuracy=0.688993, validation/bleu=30.275694, validation/loss=1.401044, validation/num_examples=3000
I0310 13:41:21.626693 139667344451392 checkpoints.py:356] Saving checkpoint at step: 77274
I0310 13:41:25.277010 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_77274
I0310 13:41:25.281497 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_77274.
I0310 13:41:35.509901 139479406757632 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21102933585643768, loss=1.4798833131790161
I0310 13:42:13.505624 139479398364928 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.21205393970012665, loss=1.517091155052185
I0310 13:42:51.458825 139479406757632 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2120257467031479, loss=1.5044622421264648
I0310 13:43:29.516808 139479398364928 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.21438805758953094, loss=1.5834720134735107
I0310 13:44:07.591132 139479406757632 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.22687014937400818, loss=1.521188735961914
I0310 13:44:45.656710 139479398364928 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.21250759065151215, loss=1.52729332447052
I0310 13:45:23.746680 139479406757632 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2072536051273346, loss=1.5843228101730347
I0310 13:46:01.833595 139479398364928 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.23739981651306152, loss=1.6257867813110352
I0310 13:46:39.872417 139479406757632 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2138998806476593, loss=1.5279746055603027
I0310 13:47:17.953833 139479398364928 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.20633864402770996, loss=1.5291101932525635
I0310 13:47:56.039638 139479406757632 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3231316804885864, loss=1.5580363273620605
I0310 13:48:34.151086 139479398364928 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20824527740478516, loss=1.483497977256775
I0310 13:49:12.210706 139479406757632 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2007376104593277, loss=1.525736689567566
I0310 13:49:50.328874 139479398364928 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2117210477590561, loss=1.5785317420959473
I0310 13:50:28.380711 139479406757632 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2052191197872162, loss=1.512086033821106
I0310 13:51:06.468091 139479398364928 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.21505793929100037, loss=1.534461498260498
I0310 13:51:44.561611 139479406757632 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.21848970651626587, loss=1.5705386400222778
I0310 13:52:22.621320 139479398364928 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21087299287319183, loss=1.5433365106582642
I0310 13:53:00.736739 139479406757632 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.21519413590431213, loss=1.5033162832260132
I0310 13:53:38.787889 139479398364928 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.19559089839458466, loss=1.466328501701355
I0310 13:54:16.867293 139479406757632 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20959003269672394, loss=1.5223517417907715
I0310 13:54:54.890020 139479398364928 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.22597384452819824, loss=1.5600042343139648
I0310 13:55:25.433684 139667344451392 spec.py:298] Evaluating on the training split.
I0310 13:55:28.427415 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 13:59:18.460044 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 13:59:21.085250 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:01:57.733654 139667344451392 spec.py:326] Evaluating on the test split.
I0310 14:02:00.415273 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:04:37.018337 139667344451392 submission_runner.py:359] Time since start: 52206.87s, 	Step: 79482, 	{'train/accuracy': 0.6819186806678772, 'train/loss': 1.452987551689148, 'train/bleu': 34.34596542901429, 'validation/accuracy': 0.6894768476486206, 'validation/loss': 1.397220253944397, 'validation/bleu': 30.495073244464894, 'validation/num_examples': 3000, 'test/accuracy': 0.7063273787498474, 'test/loss': 1.2936499118804932, 'test/bleu': 30.514199400740736, 'test/num_examples': 3003}
I0310 14:04:37.032041 139479406757632 logging_writer.py:48] [79482] global_step=79482, preemption_count=0, score=30163.202346, test/accuracy=0.706327, test/bleu=30.514199, test/loss=1.293650, test/num_examples=3003, total_duration=52206.874719, train/accuracy=0.681919, train/bleu=34.345965, train/loss=1.452988, validation/accuracy=0.689477, validation/bleu=30.495073, validation/loss=1.397220, validation/num_examples=3000
I0310 14:04:38.011507 139667344451392 checkpoints.py:356] Saving checkpoint at step: 79482
I0310 14:04:42.439209 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_79482
I0310 14:04:42.443718 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_79482.
I0310 14:04:49.655579 139479398364928 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.21525010466575623, loss=1.6276023387908936
I0310 14:05:27.565750 139479389972224 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.20501740276813507, loss=1.5206029415130615
I0310 14:06:05.554112 139479398364928 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.22789083421230316, loss=1.5908221006393433
I0310 14:06:43.603984 139479389972224 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.22341874241828918, loss=1.4748879671096802
I0310 14:07:21.708790 139479398364928 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2096572369337082, loss=1.4663840532302856
I0310 14:07:59.814542 139479389972224 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.21047522127628326, loss=1.5239791870117188
I0310 14:08:37.911208 139479398364928 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.20971202850341797, loss=1.4848521947860718
I0310 14:09:16.020988 139479389972224 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.21866118907928467, loss=1.5186086893081665
I0310 14:09:54.073537 139479398364928 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.22151662409305573, loss=1.591966986656189
I0310 14:10:32.199280 139479389972224 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.23935478925704956, loss=1.5797592401504517
I0310 14:11:10.227858 139479398364928 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.2295844703912735, loss=1.4755994081497192
I0310 14:11:48.318745 139479389972224 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.22955358028411865, loss=1.4946801662445068
I0310 14:12:26.503402 139479398364928 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.24371680617332458, loss=1.5152052640914917
I0310 14:13:04.591013 139479389972224 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.2236354798078537, loss=1.5370514392852783
I0310 14:13:42.696401 139479398364928 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20644912123680115, loss=1.512260913848877
I0310 14:14:20.774171 139479389972224 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.20936734974384308, loss=1.4835503101348877
I0310 14:14:58.846771 139479398364928 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.22396406531333923, loss=1.5642026662826538
I0310 14:15:36.954255 139479389972224 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.21241357922554016, loss=1.4704906940460205
I0310 14:16:14.964221 139479398364928 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3687390387058258, loss=1.561893343925476
I0310 14:16:53.137602 139479389972224 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.22659453749656677, loss=1.5269004106521606
I0310 14:17:31.250771 139479398364928 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2138390690088272, loss=1.5384665727615356
I0310 14:18:09.320511 139479389972224 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.23138895630836487, loss=1.5589439868927002
I0310 14:18:42.569476 139667344451392 spec.py:298] Evaluating on the training split.
I0310 14:18:45.564284 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:22:32.794356 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 14:22:35.423816 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:25:27.603732 139667344451392 spec.py:326] Evaluating on the test split.
I0310 14:25:30.287800 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:28:11.510204 139667344451392 submission_runner.py:359] Time since start: 53604.01s, 	Step: 81689, 	{'train/accuracy': 0.6911970973014832, 'train/loss': 1.3932617902755737, 'train/bleu': 34.999901150795445, 'validation/accuracy': 0.6922791004180908, 'validation/loss': 1.3901021480560303, 'validation/bleu': 30.82339503225242, 'validation/num_examples': 3000, 'test/accuracy': 0.7073732018470764, 'test/loss': 1.2911477088928223, 'test/bleu': 30.56165665897835, 'test/num_examples': 3003}
I0310 14:28:11.524156 139479398364928 logging_writer.py:48] [81689] global_step=81689, preemption_count=0, score=31000.127916, test/accuracy=0.707373, test/bleu=30.561657, test/loss=1.291148, test/num_examples=3003, total_duration=53604.010496, train/accuracy=0.691197, train/bleu=34.999901, train/loss=1.393262, validation/accuracy=0.692279, validation/bleu=30.823395, validation/loss=1.390102, validation/num_examples=3000
I0310 14:28:12.504572 139667344451392 checkpoints.py:356] Saving checkpoint at step: 81689
I0310 14:28:16.628384 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_81689
I0310 14:28:16.632859 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_81689.
I0310 14:28:21.204336 139479389972224 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.21335947513580322, loss=1.4974344968795776
I0310 14:28:59.121099 139479381579520 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.2218552827835083, loss=1.5773929357528687
I0310 14:29:37.139467 139479389972224 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.2158278524875641, loss=1.4729228019714355
I0310 14:30:15.217539 139479381579520 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.21886879205703735, loss=1.6037747859954834
I0310 14:30:53.281198 139479389972224 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.22218669950962067, loss=1.5465826988220215
I0310 14:31:31.371834 139479381579520 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.21771079301834106, loss=1.5471382141113281
I0310 14:32:09.491313 139479389972224 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.21934637427330017, loss=1.5665233135223389
I0310 14:32:47.580690 139479381579520 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20731182396411896, loss=1.5366847515106201
I0310 14:33:25.706999 139479389972224 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2242959588766098, loss=1.5425851345062256
I0310 14:34:03.753863 139479381579520 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.229910746216774, loss=1.6053035259246826
I0310 14:34:41.899233 139479389972224 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.22941944003105164, loss=1.557895541191101
I0310 14:35:19.900946 139479381579520 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.20901475846767426, loss=1.5373976230621338
I0310 14:35:57.985346 139479389972224 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.21332484483718872, loss=1.5287189483642578
I0310 14:36:36.124297 139479381579520 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.21439525485038757, loss=1.4638457298278809
I0310 14:37:14.176664 139479389972224 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.22014379501342773, loss=1.5001381635665894
I0310 14:37:52.273805 139479381579520 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.2178918421268463, loss=1.503940463066101
I0310 14:38:30.323274 139479389972224 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.21183058619499207, loss=1.5712916851043701
I0310 14:39:08.451924 139479381579520 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.21130479872226715, loss=1.4369001388549805
I0310 14:39:46.576614 139479389972224 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.21403862535953522, loss=1.4942727088928223
I0310 14:40:24.676605 139479381579520 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.21691633760929108, loss=1.5089184045791626
I0310 14:41:02.748654 139479389972224 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2176739126443863, loss=1.5056744813919067
I0310 14:41:40.816683 139479381579520 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2381005436182022, loss=1.5674810409545898
I0310 14:42:16.726947 139667344451392 spec.py:298] Evaluating on the training split.
I0310 14:42:19.719265 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:46:50.068877 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 14:46:52.707513 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:50:32.808672 139667344451392 spec.py:326] Evaluating on the test split.
I0310 14:50:35.499951 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 14:53:58.358034 139667344451392 submission_runner.py:359] Time since start: 55018.17s, 	Step: 83896, 	{'train/accuracy': 0.6862708926200867, 'train/loss': 1.4239469766616821, 'train/bleu': 34.7863066432078, 'validation/accuracy': 0.6918079257011414, 'validation/loss': 1.386249303817749, 'validation/bleu': 30.8209711897384, 'validation/num_examples': 3000, 'test/accuracy': 0.7076404690742493, 'test/loss': 1.2858471870422363, 'test/bleu': 30.474917143723186, 'test/num_examples': 3003}
I0310 14:53:58.371803 139479389972224 logging_writer.py:48] [83896] global_step=83896, preemption_count=0, score=31837.088873, test/accuracy=0.707640, test/bleu=30.474917, test/loss=1.285847, test/num_examples=3003, total_duration=55018.167948, train/accuracy=0.686271, train/bleu=34.786307, train/loss=1.423947, validation/accuracy=0.691808, validation/bleu=30.820971, validation/loss=1.386249, validation/num_examples=3000
I0310 14:53:59.357471 139667344451392 checkpoints.py:356] Saving checkpoint at step: 83896
I0310 14:54:02.714612 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_83896
I0310 14:54:02.719040 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_83896.
I0310 14:54:04.633814 139479381579520 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2209867238998413, loss=1.5256794691085815
I0310 14:54:42.522243 139479373186816 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21947932243347168, loss=1.476277470588684
I0310 14:55:20.484858 139479381579520 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.22073954343795776, loss=1.6358345746994019
I0310 14:55:58.502142 139479373186816 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.2088908553123474, loss=1.4705514907836914
I0310 14:56:36.550468 139479381579520 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.21180851757526398, loss=1.503458023071289
I0310 14:57:14.603455 139479373186816 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.21969757974147797, loss=1.5645437240600586
I0310 14:57:52.662602 139479381579520 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.22366531193256378, loss=1.5514219999313354
I0310 14:58:30.749765 139479373186816 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.20256942510604858, loss=1.46234929561615
I0310 14:59:08.803369 139479381579520 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.21961799263954163, loss=1.4921503067016602
I0310 14:59:46.900942 139479373186816 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.2142515629529953, loss=1.5267106294631958
I0310 15:00:24.944459 139479381579520 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.22560270130634308, loss=1.5218685865402222
I0310 15:01:02.984077 139479373186816 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.21325838565826416, loss=1.4914896488189697
I0310 15:01:41.058667 139479381579520 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2122371792793274, loss=1.4532299041748047
I0310 15:02:19.150058 139479373186816 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.20874948799610138, loss=1.40170419216156
I0310 15:02:57.202230 139479381579520 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.22695761919021606, loss=1.489974856376648
I0310 15:03:35.219446 139479373186816 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.2176898717880249, loss=1.5135728120803833
I0310 15:04:13.335219 139479381579520 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.21354101598262787, loss=1.5504993200302124
I0310 15:04:51.408587 139479373186816 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.21533943712711334, loss=1.4828931093215942
I0310 15:05:29.486944 139479381579520 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.23417410254478455, loss=1.5452851057052612
I0310 15:06:07.560445 139479373186816 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2174891233444214, loss=1.557713508605957
I0310 15:06:45.682985 139479381579520 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.22117407619953156, loss=1.5182336568832397
I0310 15:07:23.781901 139479373186816 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.21450203657150269, loss=1.4879604578018188
I0310 15:08:01.892147 139479381579520 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.22605885565280914, loss=1.439375877380371
I0310 15:08:02.743479 139667344451392 spec.py:298] Evaluating on the training split.
I0310 15:08:05.738334 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 15:11:49.138767 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 15:11:51.785352 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 15:14:55.736079 139667344451392 spec.py:326] Evaluating on the test split.
I0310 15:14:58.435129 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 15:17:55.126660 139667344451392 submission_runner.py:359] Time since start: 56564.18s, 	Step: 86104, 	{'train/accuracy': 0.6841979026794434, 'train/loss': 1.4375219345092773, 'train/bleu': 34.81450445414224, 'validation/accuracy': 0.6919814944267273, 'validation/loss': 1.3840076923370361, 'validation/bleu': 30.78276959542512, 'validation/num_examples': 3000, 'test/accuracy': 0.7082447409629822, 'test/loss': 1.2816535234451294, 'test/bleu': 30.401666794917436, 'test/num_examples': 3003}
I0310 15:17:55.140647 139479373186816 logging_writer.py:48] [86104] global_step=86104, preemption_count=0, score=32673.890524, test/accuracy=0.708245, test/bleu=30.401667, test/loss=1.281654, test/num_examples=3003, total_duration=56564.184512, train/accuracy=0.684198, train/bleu=34.814504, train/loss=1.437522, validation/accuracy=0.691981, validation/bleu=30.782770, validation/loss=1.384008, validation/num_examples=3000
I0310 15:17:56.127062 139667344451392 checkpoints.py:356] Saving checkpoint at step: 86104
I0310 15:17:59.528846 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_86104
I0310 15:17:59.533256 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_86104.
I0310 15:18:36.320534 139479381579520 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.21008794009685516, loss=1.4722574949264526
I0310 15:19:14.325982 139479364794112 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.22440651059150696, loss=1.5356364250183105
I0310 15:19:52.401118 139479381579520 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21451999247074127, loss=1.5075236558914185
I0310 15:20:30.507603 139479364794112 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.22669993340969086, loss=1.4988092184066772
I0310 15:21:08.582799 139479381579520 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.2118390053510666, loss=1.5393009185791016
I0310 15:21:46.627284 139479364794112 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.20497369766235352, loss=1.4879236221313477
I0310 15:22:24.760214 139479381579520 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2564535439014435, loss=1.4731194972991943
I0310 15:23:02.839466 139479364794112 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2176826447248459, loss=1.4737498760223389
I0310 15:23:40.890534 139479381579520 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2175556868314743, loss=1.5031453371047974
I0310 15:24:18.941212 139479364794112 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.2270808070898056, loss=1.4653581380844116
I0310 15:24:57.047709 139479381579520 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.22618569433689117, loss=1.5121378898620605
I0310 15:25:35.149183 139479364794112 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.21272139251232147, loss=1.467935562133789
I0310 15:26:13.280287 139479381579520 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.24004651606082916, loss=1.5630335807800293
I0310 15:26:51.382331 139479364794112 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2285311371088028, loss=1.4734933376312256
I0310 15:27:29.509897 139479381579520 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.22557680308818817, loss=1.556311845779419
I0310 15:28:07.607228 139479364794112 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.22239075601100922, loss=1.4648960828781128
I0310 15:28:45.711037 139479381579520 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.20778372883796692, loss=1.4749796390533447
I0310 15:29:23.798384 139479364794112 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.20967678725719452, loss=1.422549843788147
I0310 15:30:01.848567 139479381579520 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.23553712666034698, loss=1.4878933429718018
I0310 15:30:39.939803 139479364794112 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.2176969349384308, loss=1.4432061910629272
I0310 15:31:18.013444 139479381579520 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21670684218406677, loss=1.4179482460021973
I0310 15:31:56.048092 139479364794112 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.22545403242111206, loss=1.5216552019119263
I0310 15:31:59.571959 139667344451392 spec.py:298] Evaluating on the training split.
I0310 15:32:02.562108 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 15:36:01.172452 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 15:36:03.809610 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 15:39:28.724593 139667344451392 spec.py:326] Evaluating on the test split.
I0310 15:39:31.409168 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 15:42:54.540759 139667344451392 submission_runner.py:359] Time since start: 58001.01s, 	Step: 88311, 	{'train/accuracy': 0.6953146457672119, 'train/loss': 1.375197410583496, 'train/bleu': 35.31733295340084, 'validation/accuracy': 0.6934322118759155, 'validation/loss': 1.379611849784851, 'validation/bleu': 30.891172567849278, 'validation/num_examples': 3000, 'test/accuracy': 0.7099413275718689, 'test/loss': 1.2765907049179077, 'test/bleu': 30.705119576444037, 'test/num_examples': 3003}
I0310 15:42:54.555083 139479381579520 logging_writer.py:48] [88311] global_step=88311, preemption_count=0, score=33510.785958, test/accuracy=0.709941, test/bleu=30.705120, test/loss=1.276591, test/num_examples=3003, total_duration=58001.012995, train/accuracy=0.695315, train/bleu=35.317333, train/loss=1.375197, validation/accuracy=0.693432, validation/bleu=30.891173, validation/loss=1.379612, validation/num_examples=3000
I0310 15:42:55.537670 139667344451392 checkpoints.py:356] Saving checkpoint at step: 88311
I0310 15:42:58.891374 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_88311
I0310 15:42:58.895882 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_88311.
I0310 15:43:33.036919 139479364794112 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.2314574122428894, loss=1.5325965881347656
I0310 15:44:10.983635 139479356401408 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.22669696807861328, loss=1.5269423723220825
I0310 15:44:48.984654 139479364794112 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.22203537821769714, loss=1.4848980903625488
I0310 15:45:27.002099 139479356401408 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.22283542156219482, loss=1.5204862356185913
I0310 15:46:05.048551 139479364794112 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.21934132277965546, loss=1.5060665607452393
I0310 15:46:43.118617 139479356401408 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.22087134420871735, loss=1.4227027893066406
I0310 15:47:21.177110 139479364794112 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21737931668758392, loss=1.5348774194717407
I0310 15:47:59.247703 139479356401408 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.20623502135276794, loss=1.451728105545044
I0310 15:48:37.320262 139479364794112 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2231886386871338, loss=1.5837304592132568
I0310 15:49:15.347627 139479356401408 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.229170560836792, loss=1.516800045967102
I0310 15:49:53.465435 139479364794112 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.27442070841789246, loss=1.5310676097869873
I0310 15:50:31.514590 139479356401408 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.23225566744804382, loss=1.4071451425552368
I0310 15:51:09.591448 139479364794112 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.21558913588523865, loss=1.4853615760803223
I0310 15:51:47.648127 139479356401408 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.2124512493610382, loss=1.409874677658081
I0310 15:52:25.710637 139479364794112 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.23377123475074768, loss=1.498612642288208
I0310 15:53:03.848582 139479356401408 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2232726365327835, loss=1.4506299495697021
I0310 15:53:41.921628 139479364794112 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.22366388142108917, loss=1.5431265830993652
I0310 15:54:19.959415 139479356401408 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.21787983179092407, loss=1.507056713104248
I0310 15:54:58.024888 139479364794112 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.21750108897686005, loss=1.4758753776550293
I0310 15:55:36.089318 139479356401408 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2278696447610855, loss=1.5239101648330688
I0310 15:56:14.199390 139479364794112 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.22187985479831696, loss=1.516488790512085
I0310 15:56:52.263939 139479356401408 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.20877020061016083, loss=1.4036105871200562
I0310 15:56:59.213227 139667344451392 spec.py:298] Evaluating on the training split.
I0310 15:57:02.208517 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:01:04.019283 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 16:01:06.649495 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:03:51.699009 139667344451392 spec.py:326] Evaluating on the test split.
I0310 16:03:54.384736 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:06:57.075802 139667344451392 submission_runner.py:359] Time since start: 59500.65s, 	Step: 90520, 	{'train/accuracy': 0.6905471086502075, 'train/loss': 1.4070128202438354, 'train/bleu': 35.380318617067445, 'validation/accuracy': 0.6930602192878723, 'validation/loss': 1.3752175569534302, 'validation/bleu': 30.929527900417934, 'validation/num_examples': 3000, 'test/accuracy': 0.7106385827064514, 'test/loss': 1.2709410190582275, 'test/bleu': 30.842665916511915, 'test/num_examples': 3003}
I0310 16:06:57.090443 139479364794112 logging_writer.py:48] [90520] global_step=90520, preemption_count=0, score=34347.894517, test/accuracy=0.710639, test/bleu=30.842666, test/loss=1.270941, test/num_examples=3003, total_duration=59500.654242, train/accuracy=0.690547, train/bleu=35.380319, train/loss=1.407013, validation/accuracy=0.693060, validation/bleu=30.929528, validation/loss=1.375218, validation/num_examples=3000
I0310 16:06:58.063827 139667344451392 checkpoints.py:356] Saving checkpoint at step: 90520
I0310 16:07:01.460862 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_90520
I0310 16:07:01.465490 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_90520.
I0310 16:07:32.224647 139479356401408 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.21235142648220062, loss=1.5211445093154907
I0310 16:08:10.200997 139479348008704 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.220596581697464, loss=1.4817659854888916
I0310 16:08:48.243194 139479356401408 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.21649634838104248, loss=1.4760425090789795
I0310 16:09:26.227375 139479348008704 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.2098187506198883, loss=1.4447681903839111
I0310 16:10:04.302553 139479356401408 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.22353066504001617, loss=1.5090577602386475
I0310 16:10:42.323469 139479348008704 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.22064581513404846, loss=1.5277806520462036
I0310 16:11:20.345077 139479356401408 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.22469794750213623, loss=1.4853706359863281
I0310 16:11:58.370631 139479348008704 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.22400078177452087, loss=1.4351110458374023
I0310 16:12:36.374675 139479356401408 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.23695701360702515, loss=1.496313214302063
I0310 16:13:14.506346 139479348008704 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.22804996371269226, loss=1.525377631187439
I0310 16:13:52.587097 139479356401408 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.21585120260715485, loss=1.5120407342910767
I0310 16:14:30.659461 139479348008704 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.22342924773693085, loss=1.494206190109253
I0310 16:15:08.737842 139479356401408 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.21770334243774414, loss=1.4656621217727661
I0310 16:15:46.868933 139479348008704 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2207319438457489, loss=1.4584912061691284
I0310 16:16:24.950198 139479356401408 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22572363913059235, loss=1.460015892982483
I0310 16:17:03.016718 139479348008704 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.24350324273109436, loss=1.5090497732162476
I0310 16:17:41.077212 139479356401408 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.22398871183395386, loss=1.5180906057357788
I0310 16:18:19.147119 139479348008704 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.20929355919361115, loss=1.4274228811264038
I0310 16:18:57.189427 139479356401408 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.22761380672454834, loss=1.472233772277832
I0310 16:19:35.194652 139479348008704 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22655801475048065, loss=1.509950041770935
I0310 16:20:13.200546 139479356401408 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.23844105005264282, loss=1.574737548828125
I0310 16:20:51.334558 139479348008704 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.20447230339050293, loss=1.4711579084396362
I0310 16:21:01.713693 139667344451392 spec.py:298] Evaluating on the training split.
I0310 16:21:04.709701 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:25:08.006034 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 16:25:10.653840 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:27:58.099614 139667344451392 spec.py:326] Evaluating on the test split.
I0310 16:28:00.807126 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:30:44.328778 139667344451392 submission_runner.py:359] Time since start: 60943.15s, 	Step: 92729, 	{'train/accuracy': 0.6902154088020325, 'train/loss': 1.4038432836532593, 'train/bleu': 35.216317839623656, 'validation/accuracy': 0.6935561895370483, 'validation/loss': 1.372991919517517, 'validation/bleu': 31.04262787770375, 'validation/num_examples': 3000, 'test/accuracy': 0.7105223536491394, 'test/loss': 1.2690119743347168, 'test/bleu': 30.753902241190726, 'test/num_examples': 3003}
I0310 16:30:44.343008 139479356401408 logging_writer.py:48] [92729] global_step=92729, preemption_count=0, score=35185.066706, test/accuracy=0.710522, test/bleu=30.753902, test/loss=1.269012, test/num_examples=3003, total_duration=60943.154727, train/accuracy=0.690215, train/bleu=35.216318, train/loss=1.403843, validation/accuracy=0.693556, validation/bleu=31.042628, validation/loss=1.372992, validation/num_examples=3000
I0310 16:30:45.325164 139667344451392 checkpoints.py:356] Saving checkpoint at step: 92729
I0310 16:30:49.358118 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_92729
I0310 16:30:49.362509 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_92729.
I0310 16:31:16.701433 139479348008704 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21290796995162964, loss=1.4684484004974365
I0310 16:31:54.711449 139479339616000 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2264353185892105, loss=1.561541199684143
I0310 16:32:32.809348 139479348008704 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21873001754283905, loss=1.509364366531372
I0310 16:33:10.849892 139479339616000 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.22195914387702942, loss=1.4272212982177734
I0310 16:33:48.905454 139479348008704 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.22728511691093445, loss=1.5070298910140991
I0310 16:34:27.001032 139479339616000 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.21469047665596008, loss=1.505441427230835
I0310 16:35:05.093950 139479348008704 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.22599928081035614, loss=1.539589285850525
I0310 16:35:43.153694 139479339616000 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.2185775339603424, loss=1.5161129236221313
I0310 16:36:21.164377 139479348008704 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.22103241086006165, loss=1.4806219339370728
I0310 16:36:59.301079 139479339616000 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.22758246958255768, loss=1.557880163192749
I0310 16:37:37.368633 139479348008704 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2171902358531952, loss=1.3781206607818604
I0310 16:38:15.441436 139479339616000 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.22141888737678528, loss=1.5262058973312378
I0310 16:38:53.518312 139479348008704 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.2099587470293045, loss=1.408787488937378
I0310 16:39:31.544499 139479339616000 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.21146050095558167, loss=1.5024683475494385
I0310 16:40:09.635970 139479348008704 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2271178662776947, loss=1.6097906827926636
I0310 16:40:47.721774 139479339616000 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.2098877876996994, loss=1.4499565362930298
I0310 16:41:25.775933 139479348008704 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.23125410079956055, loss=1.4420896768569946
I0310 16:42:03.890340 139479339616000 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.2201755940914154, loss=1.5217117071151733
I0310 16:42:41.979342 139479348008704 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.21813362836837769, loss=1.529363751411438
I0310 16:43:20.087516 139479339616000 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2113536298274994, loss=1.4542070627212524
I0310 16:43:58.178843 139479348008704 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.23097845911979675, loss=1.5131853818893433
I0310 16:44:36.198663 139479339616000 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21717128157615662, loss=1.4054791927337646
I0310 16:44:49.637538 139667344451392 spec.py:298] Evaluating on the training split.
I0310 16:44:52.629326 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:48:48.227376 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 16:48:50.870215 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:51:48.364225 139667344451392 spec.py:326] Evaluating on the test split.
I0310 16:51:51.078269 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 16:54:45.750220 139667344451392 submission_runner.py:359] Time since start: 62371.08s, 	Step: 94937, 	{'train/accuracy': 0.6949642300605774, 'train/loss': 1.379796028137207, 'train/bleu': 34.983656612319805, 'validation/accuracy': 0.6936801671981812, 'validation/loss': 1.3716943264007568, 'validation/bleu': 31.027450445434546, 'validation/num_examples': 3000, 'test/accuracy': 0.7109174728393555, 'test/loss': 1.2673640251159668, 'test/bleu': 30.865486531611324, 'test/num_examples': 3003}
I0310 16:54:45.764703 139479348008704 logging_writer.py:48] [94937] global_step=94937, preemption_count=0, score=36022.228355, test/accuracy=0.710917, test/bleu=30.865487, test/loss=1.267364, test/num_examples=3003, total_duration=62371.078566, train/accuracy=0.694964, train/bleu=34.983657, train/loss=1.379796, validation/accuracy=0.693680, validation/bleu=31.027450, validation/loss=1.371694, validation/num_examples=3000
I0310 16:54:46.759906 139667344451392 checkpoints.py:356] Saving checkpoint at step: 94937
I0310 16:54:51.296482 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_94937
I0310 16:54:51.300989 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_94937.
I0310 16:55:15.570195 139479339616000 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.21247325837612152, loss=1.4871069192886353
I0310 16:55:53.497835 139479331223296 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22737565636634827, loss=1.467262625694275
I0310 16:56:31.507161 139479339616000 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.21604396402835846, loss=1.4324257373809814
I0310 16:57:09.535734 139479331223296 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21119308471679688, loss=1.4118421077728271
I0310 16:57:47.627285 139479339616000 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.21913139522075653, loss=1.4302316904067993
I0310 16:58:25.716262 139479331223296 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.2158910036087036, loss=1.4792606830596924
I0310 16:59:03.794733 139479339616000 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22465886175632477, loss=1.4803063869476318
I0310 16:59:41.896834 139479331223296 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.22231413424015045, loss=1.4634400606155396
I0310 17:00:19.976866 139479339616000 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21536952257156372, loss=1.4410351514816284
I0310 17:00:58.093946 139479331223296 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.20680634677410126, loss=1.4528886079788208
I0310 17:01:36.183495 139479339616000 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.21763506531715393, loss=1.4477218389511108
I0310 17:02:14.227912 139479331223296 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.22048629820346832, loss=1.4754760265350342
I0310 17:02:52.291340 139479339616000 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2153293341398239, loss=1.4580028057098389
I0310 17:03:30.383143 139479331223296 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.21926255524158478, loss=1.477003574371338
I0310 17:04:08.466131 139479339616000 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.21626372635364532, loss=1.4388959407806396
I0310 17:04:46.601097 139479331223296 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.2202349305152893, loss=1.4754904508590698
I0310 17:05:24.677951 139479339616000 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.21438324451446533, loss=1.440014123916626
I0310 17:06:02.724952 139479331223296 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.21951356530189514, loss=1.517041802406311
I0310 17:06:40.815778 139479339616000 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.22385558485984802, loss=1.5307494401931763
I0310 17:07:18.890159 139479331223296 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.21297793090343475, loss=1.4119027853012085
I0310 17:07:56.971220 139479339616000 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.23989251255989075, loss=1.5214349031448364
I0310 17:08:35.111325 139479331223296 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.22167770564556122, loss=1.5507023334503174
I0310 17:08:51.586226 139667344451392 spec.py:298] Evaluating on the training split.
I0310 17:08:54.574798 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 17:12:56.715916 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 17:12:59.348923 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 17:16:12.646201 139667344451392 spec.py:326] Evaluating on the test split.
I0310 17:16:15.326378 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 17:19:03.221345 139667344451392 submission_runner.py:359] Time since start: 63813.03s, 	Step: 97145, 	{'train/accuracy': 0.6938560605049133, 'train/loss': 1.382908821105957, 'train/bleu': 35.045964375192526, 'validation/accuracy': 0.6935809850692749, 'validation/loss': 1.3716565370559692, 'validation/bleu': 30.942928503907456, 'validation/num_examples': 3000, 'test/accuracy': 0.7108477354049683, 'test/loss': 1.2668497562408447, 'test/bleu': 30.946916357119132, 'test/num_examples': 3003}
I0310 17:19:03.236301 139479339616000 logging_writer.py:48] [97145] global_step=97145, preemption_count=0, score=36859.164889, test/accuracy=0.710848, test/bleu=30.946916, test/loss=1.266850, test/num_examples=3003, total_duration=63813.027219, train/accuracy=0.693856, train/bleu=35.045964, train/loss=1.382909, validation/accuracy=0.693581, validation/bleu=30.942929, validation/loss=1.371657, validation/num_examples=3000
I0310 17:19:04.229509 139667344451392 checkpoints.py:356] Saving checkpoint at step: 97145
I0310 17:19:07.877429 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_97145
I0310 17:19:07.881892 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_97145.
I0310 17:19:29.090588 139479331223296 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.20861712098121643, loss=1.408154010772705
I0310 17:20:07.073071 139479322830592 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.22019736468791962, loss=1.4655107259750366
I0310 17:20:45.068734 139479331223296 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.20805497467517853, loss=1.4610137939453125
I0310 17:21:23.096193 139479322830592 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21589630842208862, loss=1.4688467979431152
I0310 17:22:01.183758 139479331223296 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21332837641239166, loss=1.4593889713287354
I0310 17:22:39.246753 139479322830592 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21161894500255585, loss=1.464388370513916
I0310 17:23:17.346787 139479331223296 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.23247841000556946, loss=1.4792786836624146
I0310 17:23:55.434839 139479322830592 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.20992453396320343, loss=1.4467012882232666
I0310 17:24:33.502256 139479331223296 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21983303129673004, loss=1.492193341255188
I0310 17:25:11.593200 139479322830592 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.21937686204910278, loss=1.4329171180725098
I0310 17:25:49.696549 139479331223296 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2119598090648651, loss=1.5217103958129883
I0310 17:26:27.776553 139479322830592 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.21132409572601318, loss=1.5378170013427734
I0310 17:27:05.853990 139479331223296 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21655085682868958, loss=1.4267669916152954
I0310 17:27:43.843667 139479322830592 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21586215496063232, loss=1.4405933618545532
I0310 17:28:21.936745 139479331223296 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21304839849472046, loss=1.4159502983093262
I0310 17:29:00.000066 139479322830592 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.21409152448177338, loss=1.3757834434509277
I0310 17:29:38.100742 139479331223296 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.22300393879413605, loss=1.401786208152771
I0310 17:30:16.205137 139479322830592 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.2494116723537445, loss=1.5638338327407837
I0310 17:30:54.302859 139479331223296 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.21966677904129028, loss=1.475921392440796
I0310 17:31:32.410633 139479322830592 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21593469381332397, loss=1.4800901412963867
I0310 17:32:10.477535 139479331223296 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.22574397921562195, loss=1.4344185590744019
I0310 17:32:48.571356 139479322830592 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.22637204825878143, loss=1.4963866472244263
I0310 17:33:08.087003 139667344451392 spec.py:298] Evaluating on the training split.
I0310 17:33:11.090024 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 17:36:59.608570 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 17:37:02.254083 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 17:40:00.717227 139667344451392 spec.py:326] Evaluating on the test split.
I0310 17:40:03.409469 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 17:42:50.370594 139667344451392 submission_runner.py:359] Time since start: 65269.53s, 	Step: 99353, 	{'train/accuracy': 0.6920814514160156, 'train/loss': 1.3939239978790283, 'train/bleu': 35.17504075512664, 'validation/accuracy': 0.693630576133728, 'validation/loss': 1.3709577322006226, 'validation/bleu': 31.077740131730607, 'validation/num_examples': 3000, 'test/accuracy': 0.7110103964805603, 'test/loss': 1.2664778232574463, 'test/bleu': 30.9330050039182, 'test/num_examples': 3003}
I0310 17:42:50.386023 139479331223296 logging_writer.py:48] [99353] global_step=99353, preemption_count=0, score=37696.107449, test/accuracy=0.711010, test/bleu=30.933005, test/loss=1.266478, test/num_examples=3003, total_duration=65269.528007, train/accuracy=0.692081, train/bleu=35.175041, train/loss=1.393924, validation/accuracy=0.693631, validation/bleu=31.077740, validation/loss=1.370958, validation/num_examples=3000
I0310 17:42:51.377270 139667344451392 checkpoints.py:356] Saving checkpoint at step: 99353
I0310 17:42:54.755687 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_99353
I0310 17:42:54.760107 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_99353.
I0310 17:43:12.945302 139479322830592 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.2119152843952179, loss=1.471827745437622
I0310 17:43:50.882754 139479314437888 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.21298976242542267, loss=1.4218363761901855
I0310 17:44:28.929702 139479322830592 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.2176496386528015, loss=1.423712134361267
I0310 17:45:06.974018 139479314437888 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.21156230568885803, loss=1.4670296907424927
I0310 17:45:45.031575 139479322830592 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.21377211809158325, loss=1.4815962314605713
I0310 17:46:23.083636 139479314437888 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.2181973159313202, loss=1.4589675664901733
I0310 17:47:01.152628 139479322830592 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.21914921700954437, loss=1.4638431072235107
I0310 17:47:39.181991 139479314437888 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.21312367916107178, loss=1.4620451927185059
I0310 17:48:17.177727 139479322830592 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.23378098011016846, loss=1.47184157371521
I0310 17:48:55.269828 139479314437888 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.22231699526309967, loss=1.4604527950286865
I0310 17:49:33.298891 139479322830592 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.21319074928760529, loss=1.4740357398986816
I0310 17:50:11.332732 139479314437888 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.21367694437503815, loss=1.4654808044433594
I0310 17:50:49.374619 139479322830592 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.2125023454427719, loss=1.4382002353668213
I0310 17:51:27.462216 139479314437888 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.2121703028678894, loss=1.463202714920044
I0310 17:52:05.529729 139479322830592 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.21851694583892822, loss=1.482573390007019
I0310 17:52:43.550635 139479314437888 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.2269591987133026, loss=1.4603779315948486
I0310 17:53:21.611150 139479322830592 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.2299121767282486, loss=1.4779049158096313
I0310 17:53:59.723364 139479314437888 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.22026236355304718, loss=1.4426851272583008
I0310 17:54:37.817339 139479322830592 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.2256852090358734, loss=1.4988433122634888
I0310 17:55:15.903227 139479314437888 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.211605042219162, loss=1.5277960300445557
I0310 17:55:53.958638 139479322830592 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.20838183164596558, loss=1.4794855117797852
I0310 17:56:32.042418 139479314437888 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.21223323047161102, loss=1.4896020889282227
I0310 17:56:54.950942 139667344451392 spec.py:298] Evaluating on the training split.
I0310 17:56:57.947313 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:00:48.503773 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 18:00:51.132902 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:03:49.962365 139667344451392 spec.py:326] Evaluating on the test split.
I0310 18:03:52.648602 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:06:39.917226 139667344451392 submission_runner.py:359] Time since start: 66696.39s, 	Step: 101562, 	{'train/accuracy': 0.6914293169975281, 'train/loss': 1.3962477445602417, 'train/bleu': 35.14922664243305, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 18:06:39.932231 139479322830592 logging_writer.py:48] [101562] global_step=101562, preemption_count=0, score=38533.217177, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=66696.391966, train/accuracy=0.691429, train/bleu=35.149227, train/loss=1.396248, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 18:06:40.913359 139667344451392 checkpoints.py:356] Saving checkpoint at step: 101562
I0310 18:06:44.260324 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_101562
I0310 18:06:44.264746 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_101562.
I0310 18:06:59.116882 139479314437888 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22264224290847778, loss=1.4901752471923828
I0310 18:07:37.053848 139479306045184 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.2236565500497818, loss=1.446027159690857
I0310 18:08:15.036936 139479314437888 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.20323745906352997, loss=1.442355751991272
I0310 18:08:53.107024 139479306045184 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22482407093048096, loss=1.5156441926956177
I0310 18:09:31.220163 139479314437888 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.219294011592865, loss=1.5068415403366089
I0310 18:10:09.244459 139479306045184 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.21223175525665283, loss=1.4466054439544678
I0310 18:10:47.328390 139479314437888 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.21545830368995667, loss=1.451206088066101
I0310 18:11:25.423253 139479306045184 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22626148164272308, loss=1.4678351879119873
I0310 18:12:03.495512 139479314437888 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21044591069221497, loss=1.3899818658828735
I0310 18:12:41.540118 139479306045184 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21008248627185822, loss=1.4118343591690063
I0310 18:13:19.652184 139479314437888 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.22027255594730377, loss=1.4412777423858643
I0310 18:13:57.756820 139479306045184 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.22778238356113434, loss=1.5192891359329224
I0310 18:14:35.797846 139479314437888 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.22688071429729462, loss=1.447719693183899
I0310 18:15:13.866921 139479306045184 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.21979880332946777, loss=1.462921142578125
I0310 18:15:51.960322 139479314437888 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2238769233226776, loss=1.4628018140792847
I0310 18:16:29.984516 139479306045184 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.2180885523557663, loss=1.4943766593933105
I0310 18:17:08.043229 139479314437888 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.21378184854984283, loss=1.4459338188171387
I0310 18:17:46.143932 139479306045184 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.21851977705955505, loss=1.5433237552642822
I0310 18:18:24.187732 139479314437888 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.22177058458328247, loss=1.4976823329925537
I0310 18:19:02.236822 139479306045184 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.23478636145591736, loss=1.4968605041503906
I0310 18:19:40.308456 139479314437888 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.2224234789609909, loss=1.5017895698547363
I0310 18:20:18.371462 139479306045184 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.22152821719646454, loss=1.426941990852356
I0310 18:20:44.334014 139667344451392 spec.py:298] Evaluating on the training split.
I0310 18:20:47.335798 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:24:40.773561 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 18:24:43.417428 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:27:42.089495 139667344451392 spec.py:326] Evaluating on the test split.
I0310 18:27:44.773351 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:30:31.676346 139667344451392 submission_runner.py:359] Time since start: 68125.78s, 	Step: 103770, 	{'train/accuracy': 0.6912432312965393, 'train/loss': 1.4025146961212158, 'train/bleu': 35.32366220521299, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 18:30:31.691589 139479314437888 logging_writer.py:48] [103770] global_step=103770, preemption_count=0, score=39369.777000, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=68125.775046, train/accuracy=0.691243, train/bleu=35.323662, train/loss=1.402515, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 18:30:32.670468 139667344451392 checkpoints.py:356] Saving checkpoint at step: 103770
I0310 18:30:36.008668 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_103770
I0310 18:30:36.013136 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_103770.
I0310 18:30:47.783710 139479306045184 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.21734021604061127, loss=1.4344936609268188
I0310 18:31:25.756969 139479297652480 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.21565769612789154, loss=1.5303089618682861
I0310 18:32:03.801126 139479306045184 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.22474485635757446, loss=1.4886659383773804
I0310 18:32:41.784706 139479297652480 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.21058839559555054, loss=1.435518741607666
I0310 18:33:19.846082 139479306045184 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.2266714870929718, loss=1.5248475074768066
I0310 18:33:57.870568 139479297652480 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.21672789752483368, loss=1.4727966785430908
I0310 18:34:35.948499 139479306045184 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.23000791668891907, loss=1.517418384552002
I0310 18:35:14.047903 139479297652480 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.20788684487342834, loss=1.44548499584198
I0310 18:35:52.185797 139479306045184 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.21736769378185272, loss=1.5090607404708862
I0310 18:36:30.284929 139479297652480 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.21450193226337433, loss=1.5087686777114868
I0310 18:37:08.359675 139479306045184 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.20987644791603088, loss=1.4411113262176514
I0310 18:37:46.445164 139479297652480 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.22205796837806702, loss=1.4547133445739746
I0310 18:38:24.469552 139479306045184 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22107873857021332, loss=1.467708945274353
I0310 18:39:02.521967 139479297652480 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.2159430831670761, loss=1.4375507831573486
I0310 18:39:40.588273 139479306045184 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.21850942075252533, loss=1.5340194702148438
I0310 18:40:18.619784 139479297652480 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.20713818073272705, loss=1.4024778604507446
I0310 18:40:56.622900 139479306045184 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.2026146948337555, loss=1.4103938341140747
I0310 18:41:34.662791 139479297652480 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.23016269505023956, loss=1.4508178234100342
I0310 18:42:12.779732 139479306045184 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.22369757294654846, loss=1.5048823356628418
I0310 18:42:50.849429 139479297652480 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.21339121460914612, loss=1.4951744079589844
I0310 18:43:28.937428 139479306045184 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.22244930267333984, loss=1.4476168155670166
I0310 18:44:06.999663 139479297652480 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.22008293867111206, loss=1.4678698778152466
I0310 18:44:36.044245 139667344451392 spec.py:298] Evaluating on the training split.
I0310 18:44:39.041312 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:48:27.068488 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 18:48:29.712023 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:51:28.674378 139667344451392 spec.py:326] Evaluating on the test split.
I0310 18:51:31.359238 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 18:54:18.559011 139667344451392 submission_runner.py:359] Time since start: 69557.49s, 	Step: 105978, 	{'train/accuracy': 0.692474901676178, 'train/loss': 1.393912672996521, 'train/bleu': 35.4926919263436, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 18:54:18.574431 139479306045184 logging_writer.py:48] [105978] global_step=105978, preemption_count=0, score=40206.726496, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=69557.485275, train/accuracy=0.692475, train/bleu=35.492692, train/loss=1.393913, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 18:54:19.553968 139667344451392 checkpoints.py:356] Saving checkpoint at step: 105978
I0310 18:54:23.242229 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_105978
I0310 18:54:23.246711 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_105978.
I0310 18:54:31.995227 139479297652480 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.21772752702236176, loss=1.4949049949645996
I0310 18:55:09.931301 139479289259776 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.22263149917125702, loss=1.5255510807037354
I0310 18:55:47.915244 139479297652480 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.2165377140045166, loss=1.4390627145767212
I0310 18:56:25.932479 139479289259776 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.22022324800491333, loss=1.4811075925827026
I0310 18:57:03.978046 139479297652480 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.22001679241657257, loss=1.5299103260040283
I0310 18:57:42.006127 139479289259776 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.2159428596496582, loss=1.450801134109497
I0310 18:58:20.093683 139479297652480 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.2064354419708252, loss=1.4209332466125488
I0310 18:58:58.153177 139479289259776 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.21843022108078003, loss=1.4972296953201294
I0310 18:59:36.196469 139479297652480 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22570617496967316, loss=1.5371848344802856
I0310 19:00:14.337318 139479289259776 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.21847093105316162, loss=1.4086227416992188
I0310 19:00:52.398750 139479297652480 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.2150273621082306, loss=1.4504903554916382
I0310 19:01:30.528515 139479289259776 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.21968711912631989, loss=1.4647636413574219
I0310 19:02:08.609378 139479297652480 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.21200533211231232, loss=1.4198803901672363
I0310 19:02:46.682475 139479289259776 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.22523486614227295, loss=1.5040901899337769
I0310 19:03:24.742904 139479297652480 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.2260214388370514, loss=1.5169249773025513
I0310 19:04:02.805100 139479289259776 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.2172767072916031, loss=1.4474376440048218
I0310 19:04:40.854303 139479297652480 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.2171081155538559, loss=1.4683324098587036
I0310 19:05:18.886202 139479289259776 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.2154383659362793, loss=1.3945870399475098
I0310 19:05:56.949685 139479297652480 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.2149013876914978, loss=1.4812289476394653
I0310 19:06:35.004988 139479289259776 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.21718524396419525, loss=1.4286658763885498
I0310 19:07:13.062590 139479297652480 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.21928487718105316, loss=1.4608614444732666
I0310 19:07:51.125866 139479289259776 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22584907710552216, loss=1.495849370956421
I0310 19:08:23.600728 139667344451392 spec.py:298] Evaluating on the training split.
I0310 19:08:26.590071 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 19:12:20.761688 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 19:12:23.387680 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 19:15:22.224532 139667344451392 spec.py:326] Evaluating on the test split.
I0310 19:15:24.908054 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 19:18:11.965842 139667344451392 submission_runner.py:359] Time since start: 70985.04s, 	Step: 108187, 	{'train/accuracy': 0.6928708553314209, 'train/loss': 1.3952828645706177, 'train/bleu': 35.49152637640315, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 19:18:11.981601 139479297652480 logging_writer.py:48] [108187] global_step=108187, preemption_count=0, score=41043.974183, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=70985.041757, train/accuracy=0.692871, train/bleu=35.491526, train/loss=1.395283, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 19:18:12.962698 139667344451392 checkpoints.py:356] Saving checkpoint at step: 108187
I0310 19:18:17.394821 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_108187
I0310 19:18:17.399269 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_108187.
I0310 19:18:22.741745 139479289259776 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.20975486934185028, loss=1.396180510520935
I0310 19:19:00.701581 139479280867072 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.22182025015354156, loss=1.5060884952545166
I0310 19:19:38.678726 139479289259776 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.21893778443336487, loss=1.416558861732483
I0310 19:20:16.694921 139479280867072 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.22046124935150146, loss=1.4247504472732544
I0310 19:20:54.806786 139479289259776 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.2075449675321579, loss=1.4335832595825195
I0310 19:21:32.907427 139479280867072 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.21426726877689362, loss=1.505831003189087
I0310 19:22:10.956982 139479289259776 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22302235662937164, loss=1.4184849262237549
I0310 19:22:49.012588 139479280867072 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.21539871394634247, loss=1.4880733489990234
I0310 19:23:27.110170 139479289259776 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.21023714542388916, loss=1.4406225681304932
I0310 19:24:05.171795 139479280867072 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.21218924224376678, loss=1.4717899560928345
I0310 19:24:43.229794 139479289259776 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.2139240801334381, loss=1.4347494840621948
I0310 19:25:21.326871 139479280867072 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.21529601514339447, loss=1.4042575359344482
I0310 19:25:59.407411 139479289259776 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.24613827466964722, loss=1.4511688947677612
I0310 19:26:37.511207 139479280867072 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.21394072473049164, loss=1.4228482246398926
I0310 19:27:15.603382 139479289259776 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.2285921424627304, loss=1.4294406175613403
I0310 19:27:53.712667 139479280867072 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.21092616021633148, loss=1.4467206001281738
I0310 19:28:31.793440 139479289259776 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.21494153141975403, loss=1.466500997543335
I0310 19:29:09.886193 139479280867072 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.21979522705078125, loss=1.4658176898956299
I0310 19:29:47.955592 139479289259776 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.21602652966976166, loss=1.4524790048599243
I0310 19:30:26.011660 139479280867072 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.21748781204223633, loss=1.4660866260528564
I0310 19:31:04.104782 139479289259776 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.2249421924352646, loss=1.4429028034210205
I0310 19:31:42.237021 139479280867072 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.22163133323192596, loss=1.407678484916687
I0310 19:32:17.404277 139667344451392 spec.py:298] Evaluating on the training split.
I0310 19:32:20.393992 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 19:36:24.244848 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 19:36:26.874001 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 19:39:25.622301 139667344451392 spec.py:326] Evaluating on the test split.
I0310 19:39:28.308191 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 19:42:15.502281 139667344451392 submission_runner.py:359] Time since start: 72418.85s, 	Step: 110394, 	{'train/accuracy': 0.6902603507041931, 'train/loss': 1.403262972831726, 'train/bleu': 35.05110832232195, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 19:42:15.518016 139479289259776 logging_writer.py:48] [110394] global_step=110394, preemption_count=0, score=41880.599387, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=72418.845282, train/accuracy=0.690260, train/bleu=35.051108, train/loss=1.403263, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 19:42:16.499680 139667344451392 checkpoints.py:356] Saving checkpoint at step: 110394
I0310 19:42:20.493501 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_110394
I0310 19:42:20.497926 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_110394.
I0310 19:42:23.167717 139479280867072 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.21390455961227417, loss=1.4316328763961792
I0310 19:43:01.066011 139479272474368 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.2226681411266327, loss=1.5192102193832397
I0310 19:43:39.048059 139479280867072 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.22146885097026825, loss=1.4419690370559692
I0310 19:44:17.079189 139479272474368 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.21926313638687134, loss=1.5269312858581543
I0310 19:44:55.157688 139479280867072 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.21172980964183807, loss=1.4836636781692505
I0310 19:45:33.213876 139479272474368 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.207405686378479, loss=1.456499695777893
I0310 19:46:11.306377 139479280867072 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.2307944893836975, loss=1.5088056325912476
I0310 19:46:49.384154 139479272474368 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.21483755111694336, loss=1.4135960340499878
I0310 19:47:27.466513 139479280867072 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.23727625608444214, loss=1.4241535663604736
I0310 19:48:05.460473 139479272474368 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.21013781428337097, loss=1.5128077268600464
I0310 19:48:43.546588 139479280867072 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.23294073343276978, loss=1.4756908416748047
I0310 19:49:21.666699 139479272474368 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.22433893382549286, loss=1.4666664600372314
I0310 19:49:59.721572 139479280867072 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.20738928020000458, loss=1.4257267713546753
I0310 19:50:37.744752 139479272474368 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.22181659936904907, loss=1.441389560699463
I0310 19:51:15.816003 139479280867072 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.2323913872241974, loss=1.57197904586792
I0310 19:51:53.931210 139479272474368 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.21413123607635498, loss=1.5429645776748657
I0310 19:52:32.007449 139479280867072 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.21426591277122498, loss=1.461182713508606
I0310 19:53:10.033843 139479272474368 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.2096979320049286, loss=1.438576340675354
I0310 19:53:48.138076 139479280867072 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.22407832741737366, loss=1.4879769086837769
I0310 19:54:26.195984 139479272474368 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.21431396901607513, loss=1.4963611364364624
I0310 19:55:04.321469 139479280867072 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.21255719661712646, loss=1.4514472484588623
I0310 19:55:42.379203 139479272474368 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.22171244025230408, loss=1.4812920093536377
I0310 19:56:20.409933 139479280867072 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.2140081226825714, loss=1.3947443962097168
I0310 19:56:20.883841 139667344451392 spec.py:298] Evaluating on the training split.
I0310 19:56:23.873753 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:00:08.798644 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 20:00:11.417452 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:03:10.021564 139667344451392 spec.py:326] Evaluating on the test split.
I0310 20:03:12.702712 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:05:59.733919 139667344451392 submission_runner.py:359] Time since start: 73862.32s, 	Step: 112603, 	{'train/accuracy': 0.691085159778595, 'train/loss': 1.4016913175582886, 'train/bleu': 35.272948290954716, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 20:05:59.749809 139479272474368 logging_writer.py:48] [112603] global_step=112603, preemption_count=0, score=42717.771101, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=73862.324879, train/accuracy=0.691085, train/bleu=35.272948, train/loss=1.401691, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 20:06:00.725130 139667344451392 checkpoints.py:356] Saving checkpoint at step: 112603
I0310 20:06:04.018820 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_112603
I0310 20:06:04.023450 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_112603.
I0310 20:06:41.196578 139479280867072 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.22221386432647705, loss=1.441293716430664
I0310 20:07:19.212154 139479264081664 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.21624456346035004, loss=1.4782898426055908
I0310 20:07:57.224744 139479280867072 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.22343240678310394, loss=1.502780556678772
I0310 20:08:35.290477 139479264081664 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.2235804945230484, loss=1.5263415575027466
I0310 20:09:13.369269 139479280867072 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.2285054624080658, loss=1.4550572633743286
I0310 20:09:51.398053 139479264081664 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.21241186559200287, loss=1.4203059673309326
I0310 20:10:29.472621 139479280867072 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.22468850016593933, loss=1.4675202369689941
I0310 20:11:07.597573 139479264081664 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.21577224135398865, loss=1.4788521528244019
I0310 20:11:45.677390 139479280867072 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.2266353815793991, loss=1.417344331741333
I0310 20:12:23.732159 139479264081664 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.22199100255966187, loss=1.4619182348251343
I0310 20:13:01.856791 139479280867072 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.21937249600887299, loss=1.5265350341796875
I0310 20:13:39.921364 139479264081664 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.2093188762664795, loss=1.3900083303451538
I0310 20:14:18.068786 139479280867072 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.21124008297920227, loss=1.4489420652389526
I0310 20:14:56.175474 139479264081664 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.21466386318206787, loss=1.4638950824737549
I0310 20:15:34.290299 139479280867072 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.21871346235275269, loss=1.4574326276779175
I0310 20:16:12.411294 139479264081664 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.22065693140029907, loss=1.435889720916748
I0310 20:16:50.552807 139479280867072 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.22368811070919037, loss=1.5051982402801514
I0310 20:17:28.615430 139479264081664 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.22721731662750244, loss=1.5024409294128418
I0310 20:18:06.729521 139479280867072 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.21287958323955536, loss=1.4752120971679688
I0310 20:18:44.792546 139479264081664 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.21341049671173096, loss=1.4377788305282593
I0310 20:19:22.888959 139479280867072 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.21616409718990326, loss=1.5033458471298218
I0310 20:20:00.989429 139479264081664 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.21962209045886993, loss=1.4983832836151123
I0310 20:20:04.130598 139667344451392 spec.py:298] Evaluating on the training split.
I0310 20:20:07.142757 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:24:12.774375 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 20:24:15.417941 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:27:14.343348 139667344451392 spec.py:326] Evaluating on the test split.
I0310 20:27:17.030885 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:30:04.171545 139667344451392 submission_runner.py:359] Time since start: 75285.57s, 	Step: 114810, 	{'train/accuracy': 0.692069947719574, 'train/loss': 1.3982443809509277, 'train/bleu': 35.4151978832511, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 20:30:04.187999 139479280867072 logging_writer.py:48] [114810] global_step=114810, preemption_count=0, score=43554.310712, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=75285.571611, train/accuracy=0.692070, train/bleu=35.415198, train/loss=1.398244, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 20:30:05.170501 139667344451392 checkpoints.py:356] Saving checkpoint at step: 114810
I0310 20:30:08.526354 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_114810
I0310 20:30:08.530915 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_114810.
I0310 20:30:43.059974 139479264081664 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.21325643360614777, loss=1.4069774150848389
I0310 20:31:21.043429 139479255688960 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.22659678757190704, loss=1.4915382862091064
I0310 20:31:59.102180 139479264081664 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.22824598848819733, loss=1.5392147302627563
I0310 20:32:37.118367 139479255688960 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.21481643617153168, loss=1.4287478923797607
I0310 20:33:15.169838 139479264081664 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.2210177630186081, loss=1.4611221551895142
I0310 20:33:53.231423 139479255688960 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.2139495611190796, loss=1.4476919174194336
I0310 20:34:31.319343 139479264081664 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.22174444794654846, loss=1.3932020664215088
I0310 20:35:09.396758 139479255688960 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.2180098444223404, loss=1.5158591270446777
I0310 20:35:47.514820 139479264081664 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.20581607520580292, loss=1.4420133829116821
I0310 20:36:25.584342 139479255688960 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.2121826857328415, loss=1.4687681198120117
I0310 20:37:03.702947 139479264081664 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.2260696142911911, loss=1.480768084526062
I0310 20:37:41.801069 139479255688960 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.2258438616991043, loss=1.4306412935256958
I0310 20:38:19.850931 139479264081664 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.21879634261131287, loss=1.4157631397247314
I0310 20:38:57.901238 139479255688960 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.22868692874908447, loss=1.407789707183838
I0310 20:39:35.969009 139479264081664 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.22559823095798492, loss=1.4898148775100708
I0310 20:40:14.053946 139479255688960 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.21056349575519562, loss=1.427287220954895
I0310 20:40:52.125397 139479264081664 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.22524291276931763, loss=1.509122371673584
I0310 20:41:30.214667 139479255688960 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.21777954697608948, loss=1.4496355056762695
I0310 20:42:08.385393 139479264081664 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.21836398541927338, loss=1.4012789726257324
I0310 20:42:46.493758 139479255688960 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.21533559262752533, loss=1.4778861999511719
I0310 20:43:24.629040 139479264081664 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.2127949297428131, loss=1.4318641424179077
I0310 20:44:02.686990 139479255688960 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.20890207588672638, loss=1.443336844444275
I0310 20:44:08.907418 139667344451392 spec.py:298] Evaluating on the training split.
I0310 20:44:11.904837 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:48:10.495521 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 20:48:13.134125 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:51:12.261243 139667344451392 spec.py:326] Evaluating on the test split.
I0310 20:51:14.954658 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 20:54:02.216060 139667344451392 submission_runner.py:359] Time since start: 76730.35s, 	Step: 117018, 	{'train/accuracy': 0.6904824376106262, 'train/loss': 1.3978420495986938, 'train/bleu': 35.15842067106161, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 20:54:02.233546 139479264081664 logging_writer.py:48] [117018] global_step=117018, preemption_count=0, score=44391.388609, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=76730.348434, train/accuracy=0.690482, train/bleu=35.158421, train/loss=1.397842, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 20:54:03.211184 139667344451392 checkpoints.py:356] Saving checkpoint at step: 117018
I0310 20:54:06.549391 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_117018
I0310 20:54:06.553795 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_117018.
I0310 20:54:38.032063 139479255688960 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.22367583215236664, loss=1.525938868522644
I0310 20:55:16.039741 139479247296256 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.21128995716571808, loss=1.413343071937561
I0310 20:55:54.020879 139479255688960 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.23792892694473267, loss=1.4516297578811646
I0310 20:56:32.087450 139479247296256 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.22093695402145386, loss=1.5449246168136597
I0310 20:57:10.208117 139479255688960 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.21548523008823395, loss=1.5204813480377197
I0310 20:57:48.303357 139479247296256 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.2146887332201004, loss=1.4133068323135376
I0310 20:58:26.398466 139479255688960 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.20977742969989777, loss=1.4534010887145996
I0310 20:59:04.561178 139479247296256 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.2264108508825302, loss=1.5159566402435303
I0310 20:59:42.620015 139479255688960 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.21003423631191254, loss=1.4802032709121704
I0310 21:00:20.700504 139479247296256 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.2130616009235382, loss=1.4399678707122803
I0310 21:00:58.796299 139479255688960 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.22443000972270966, loss=1.4405879974365234
I0310 21:01:36.846910 139479247296256 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.22449080646038055, loss=1.4369980096817017
I0310 21:02:14.976984 139479255688960 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.21526610851287842, loss=1.4404528141021729
I0310 21:02:53.083343 139479247296256 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.21596598625183105, loss=1.4439700841903687
I0310 21:03:31.174256 139479255688960 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.2119666337966919, loss=1.4148043394088745
I0310 21:04:09.255050 139479247296256 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.22431236505508423, loss=1.5219056606292725
I0310 21:04:47.318392 139479255688960 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.22292204201221466, loss=1.5504143238067627
I0310 21:05:25.431751 139479247296256 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.23406361043453217, loss=1.5236163139343262
I0310 21:06:03.453182 139479255688960 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.22458288073539734, loss=1.4450548887252808
I0310 21:06:41.506560 139479247296256 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.21728667616844177, loss=1.4733097553253174
I0310 21:07:19.649767 139479255688960 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.22028902173042297, loss=1.457476019859314
I0310 21:07:57.769339 139479247296256 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.21429359912872314, loss=1.495837688446045
I0310 21:08:06.619179 139667344451392 spec.py:298] Evaluating on the training split.
I0310 21:08:09.614117 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 21:12:04.182064 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 21:12:06.818379 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 21:15:05.768507 139667344451392 spec.py:326] Evaluating on the test split.
I0310 21:15:08.453459 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 21:17:55.625447 139667344451392 submission_runner.py:359] Time since start: 78168.06s, 	Step: 119225, 	{'train/accuracy': 0.6914687156677246, 'train/loss': 1.3962147235870361, 'train/bleu': 35.12967524929205, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 21:17:55.641563 139479255688960 logging_writer.py:48] [119225] global_step=119225, preemption_count=0, score=45228.243197, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=78168.060177, train/accuracy=0.691469, train/bleu=35.129675, train/loss=1.396215, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 21:17:56.615518 139667344451392 checkpoints.py:356] Saving checkpoint at step: 119225
I0310 21:18:00.033164 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_119225
I0310 21:18:00.037576 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_119225.
I0310 21:18:28.851733 139479247296256 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.2184206247329712, loss=1.4542089700698853
I0310 21:19:06.830421 139479238903552 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.22739115357398987, loss=1.5341664552688599
I0310 21:19:44.902856 139479247296256 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.2084440439939499, loss=1.4648511409759521
I0310 21:20:22.913789 139479238903552 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.22362956404685974, loss=1.5039030313491821
I0310 21:21:01.020826 139479247296256 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.2141992598772049, loss=1.4448456764221191
I0310 21:21:39.089348 139479238903552 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.2366054207086563, loss=1.4878408908843994
I0310 21:22:17.158129 139479247296256 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.22332517802715302, loss=1.516928791999817
I0310 21:22:55.197542 139479238903552 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.23452185094356537, loss=1.5247067213058472
I0310 21:23:33.267800 139479247296256 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.21284717321395874, loss=1.406667947769165
I0310 21:24:11.369807 139479238903552 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.22027114033699036, loss=1.4608161449432373
I0310 21:24:49.414416 139479247296256 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.22269657254219055, loss=1.5235872268676758
I0310 21:25:27.497045 139479238903552 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.22362925112247467, loss=1.4592689275741577
I0310 21:26:05.605179 139479247296256 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.2179066240787506, loss=1.4288581609725952
I0310 21:26:43.672959 139479238903552 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.21112510561943054, loss=1.5706901550292969
I0310 21:27:21.709058 139479247296256 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.2181851714849472, loss=1.390365719795227
I0310 21:27:59.781509 139479238903552 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.2151971310377121, loss=1.3583707809448242
I0310 21:28:37.877394 139479247296256 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.217299222946167, loss=1.44967782497406
I0310 21:29:15.966310 139479238903552 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.21579594910144806, loss=1.5372648239135742
I0310 21:29:54.092305 139479247296256 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.21882638335227966, loss=1.4732686281204224
I0310 21:30:32.217121 139479238903552 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.21489068865776062, loss=1.4217172861099243
I0310 21:31:10.336708 139479247296256 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.22176283597946167, loss=1.5319453477859497
I0310 21:31:48.387962 139479238903552 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.2097451090812683, loss=1.4118432998657227
I0310 21:32:00.245698 139667344451392 spec.py:298] Evaluating on the training split.
I0310 21:32:03.245027 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 21:36:04.461138 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 21:36:07.110337 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 21:39:05.979650 139667344451392 spec.py:326] Evaluating on the test split.
I0310 21:39:08.666206 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 21:41:55.596274 139667344451392 submission_runner.py:359] Time since start: 79601.69s, 	Step: 121433, 	{'train/accuracy': 0.6949947476387024, 'train/loss': 1.3745415210723877, 'train/bleu': 34.940446582224986, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 21:41:55.613566 139479247296256 logging_writer.py:48] [121433] global_step=121433, preemption_count=0, score=46064.926969, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=79601.686713, train/accuracy=0.694995, train/bleu=34.940447, train/loss=1.374542, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 21:41:56.585464 139667344451392 checkpoints.py:356] Saving checkpoint at step: 121433
I0310 21:42:00.622781 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_121433
I0310 21:42:00.627299 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_121433.
I0310 21:42:26.446472 139479238903552 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.22084274888038635, loss=1.4812265634536743
I0310 21:43:04.442101 139479230510848 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.20878858864307404, loss=1.4502302408218384
I0310 21:43:42.472747 139479238903552 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.21564272046089172, loss=1.410565972328186
I0310 21:44:20.504161 139479230510848 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23024152219295502, loss=1.4752365350723267
I0310 21:44:58.622215 139479238903552 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.22867882251739502, loss=1.5358409881591797
I0310 21:45:36.646406 139479230510848 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.22407802939414978, loss=1.4776784181594849
I0310 21:46:14.780670 139479238903552 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.22268448770046234, loss=1.4384453296661377
I0310 21:46:52.859320 139479230510848 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.21850383281707764, loss=1.4445427656173706
I0310 21:47:30.912698 139479238903552 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.2070264220237732, loss=1.4636285305023193
I0310 21:48:08.976734 139479230510848 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.22812248766422272, loss=1.4074945449829102
I0310 21:48:47.024349 139479238903552 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.21500304341316223, loss=1.4789748191833496
I0310 21:49:25.140244 139479230510848 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.21940942108631134, loss=1.4878429174423218
I0310 21:50:03.211941 139479238903552 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.21717000007629395, loss=1.5351488590240479
I0310 21:50:41.321443 139479230510848 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.22244364023208618, loss=1.4464203119277954
I0310 21:51:19.402385 139479238903552 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.2196750044822693, loss=1.4888347387313843
I0310 21:51:57.512378 139479230510848 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.2174862176179886, loss=1.4384714365005493
I0310 21:52:35.568938 139479238903552 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.2181120067834854, loss=1.4161266088485718
I0310 21:53:13.704441 139479230510848 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.2116696685552597, loss=1.4835115671157837
I0310 21:53:51.745458 139479238903552 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.21906310319900513, loss=1.4546011686325073
I0310 21:54:29.854960 139479230510848 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.2211911529302597, loss=1.4764560461044312
I0310 21:55:07.999680 139479238903552 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.21827805042266846, loss=1.4801679849624634
I0310 21:55:46.105027 139479230510848 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.21068860590457916, loss=1.38387131690979
I0310 21:56:00.661195 139667344451392 spec.py:298] Evaluating on the training split.
I0310 21:56:03.658454 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:00:03.973803 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 22:00:06.602389 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:03:05.494688 139667344451392 spec.py:326] Evaluating on the test split.
I0310 22:03:08.190964 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:05:55.512484 139667344451392 submission_runner.py:359] Time since start: 81042.10s, 	Step: 123640, 	{'train/accuracy': 0.6892856955528259, 'train/loss': 1.4049540758132935, 'train/bleu': 34.878482318010406, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 22:05:55.529275 139479238903552 logging_writer.py:48] [123640] global_step=123640, preemption_count=0, score=46901.775324, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=81042.102229, train/accuracy=0.689286, train/bleu=34.878482, train/loss=1.404954, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 22:05:56.506156 139667344451392 checkpoints.py:356] Saving checkpoint at step: 123640
I0310 22:06:00.977272 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_123640
I0310 22:06:00.981715 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_123640.
I0310 22:06:24.099651 139479230510848 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.21612931787967682, loss=1.5994774103164673
I0310 22:07:02.070169 139479222118144 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.20960189402103424, loss=1.409895420074463
I0310 22:07:40.141597 139479230510848 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.2327347695827484, loss=1.4719057083129883
I0310 22:08:18.192328 139479222118144 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.22344787418842316, loss=1.4891523122787476
I0310 22:08:56.274767 139479230510848 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.2107638120651245, loss=1.4101276397705078
I0310 22:09:34.295877 139479222118144 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.22552436590194702, loss=1.5199050903320312
I0310 22:10:12.335623 139479230510848 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.22296994924545288, loss=1.514486312866211
I0310 22:10:50.329036 139479222118144 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.2098231464624405, loss=1.416605830192566
I0310 22:11:28.427941 139479230510848 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.22117100656032562, loss=1.3845715522766113
I0310 22:12:06.526514 139479222118144 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.21588928997516632, loss=1.4697946310043335
I0310 22:12:44.613515 139479230510848 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.21930783987045288, loss=1.435836911201477
I0310 22:13:22.675825 139479222118144 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.2129896730184555, loss=1.4765100479125977
I0310 22:14:00.700403 139479230510848 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.21231241524219513, loss=1.4362179040908813
I0310 22:14:38.706635 139479222118144 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.21791361272335052, loss=1.4645671844482422
I0310 22:15:16.796568 139479230510848 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.21463321149349213, loss=1.487143635749817
I0310 22:15:54.841533 139479222118144 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.22099831700325012, loss=1.4379841089248657
I0310 22:16:32.846675 139479230510848 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.20951436460018158, loss=1.496960997581482
I0310 22:17:10.897905 139479222118144 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.2182510495185852, loss=1.4851016998291016
I0310 22:17:48.907150 139479230510848 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.21271416544914246, loss=1.4374287128448486
I0310 22:18:26.957850 139479222118144 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.22613734006881714, loss=1.4336469173431396
I0310 22:19:04.995970 139479230510848 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.2122388780117035, loss=1.5057610273361206
I0310 22:19:43.066519 139479222118144 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.21090519428253174, loss=1.4443349838256836
I0310 22:20:01.052577 139667344451392 spec.py:298] Evaluating on the training split.
I0310 22:20:04.046130 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:24:05.369124 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 22:24:08.000380 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:27:06.830929 139667344451392 spec.py:326] Evaluating on the test split.
I0310 22:27:09.514901 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:29:56.638705 139667344451392 submission_runner.py:359] Time since start: 82482.49s, 	Step: 125849, 	{'train/accuracy': 0.6911211013793945, 'train/loss': 1.3947985172271729, 'train/bleu': 35.112504179241746, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 22:29:56.656973 139479230510848 logging_writer.py:48] [125849] global_step=125849, preemption_count=0, score=47738.752504, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=82482.493609, train/accuracy=0.691121, train/bleu=35.112504, train/loss=1.394799, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 22:29:57.626879 139667344451392 checkpoints.py:356] Saving checkpoint at step: 125849
I0310 22:30:01.151818 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_125849
I0310 22:30:01.156439 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_125849.
I0310 22:30:20.872392 139479222118144 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.22303113341331482, loss=1.4875649213790894
I0310 22:30:58.800165 139479213725440 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.22722777724266052, loss=1.531771183013916
I0310 22:31:36.850590 139479222118144 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.21019244194030762, loss=1.5382869243621826
I0310 22:32:14.872925 139479213725440 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.2176443338394165, loss=1.4828588962554932
I0310 22:32:52.986162 139479222118144 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.2077130824327469, loss=1.4443920850753784
I0310 22:33:31.076602 139479213725440 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.22317974269390106, loss=1.4502019882202148
I0310 22:34:09.134953 139479222118144 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.2207222729921341, loss=1.5498855113983154
I0310 22:34:47.215617 139479213725440 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.2206188440322876, loss=1.508565068244934
I0310 22:35:25.259262 139479222118144 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.2147480994462967, loss=1.456127405166626
I0310 22:36:03.290060 139479213725440 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.21316730976104736, loss=1.4110254049301147
I0310 22:36:41.370843 139479222118144 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.21270357072353363, loss=1.4657139778137207
I0310 22:37:19.438650 139479213725440 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.22128230333328247, loss=1.4435770511627197
I0310 22:37:57.525630 139479222118144 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.22248414158821106, loss=1.4565019607543945
I0310 22:38:35.605774 139479213725440 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.21626900136470795, loss=1.4456828832626343
I0310 22:39:13.686626 139479222118144 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.22805601358413696, loss=1.4545620679855347
I0310 22:39:51.766402 139479213725440 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.2160084992647171, loss=1.4654239416122437
I0310 22:40:29.851898 139479222118144 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.21299615502357483, loss=1.434551477432251
I0310 22:41:07.908644 139479213725440 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.20878666639328003, loss=1.4429855346679688
I0310 22:41:45.968950 139479222118144 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.22886121273040771, loss=1.4790191650390625
I0310 22:42:24.058340 139479213725440 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.21173454821109772, loss=1.4468069076538086
I0310 22:43:02.111344 139479222118144 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.21548053622245789, loss=1.4454381465911865
I0310 22:43:40.182917 139479213725440 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.21895433962345123, loss=1.507946252822876
I0310 22:44:01.212587 139667344451392 spec.py:298] Evaluating on the training split.
I0310 22:44:04.201256 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:48:01.157240 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 22:48:03.788450 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:51:02.640133 139667344451392 spec.py:326] Evaluating on the test split.
I0310 22:51:05.351860 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 22:53:52.553758 139667344451392 submission_runner.py:359] Time since start: 83922.65s, 	Step: 128057, 	{'train/accuracy': 0.6901925206184387, 'train/loss': 1.4033451080322266, 'train/bleu': 35.01325996777261, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 22:53:52.571778 139479222118144 logging_writer.py:48] [128057] global_step=128057, preemption_count=0, score=48575.545371, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=83922.653616, train/accuracy=0.690193, train/bleu=35.013260, train/loss=1.403345, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 22:53:53.549800 139667344451392 checkpoints.py:356] Saving checkpoint at step: 128057
I0310 22:53:56.890953 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_128057
I0310 22:53:56.895419 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_128057.
I0310 22:54:13.598651 139479213725440 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.2088129222393036, loss=1.4299954175949097
I0310 22:54:51.581018 139479205332736 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.2254505306482315, loss=1.5283430814743042
I0310 22:55:29.577128 139479213725440 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.21450236439704895, loss=1.407699465751648
I0310 22:56:07.634925 139479205332736 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.21660128235816956, loss=1.4441862106323242
I0310 22:56:45.737090 139479213725440 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.2144600749015808, loss=1.5404011011123657
I0310 22:57:23.769971 139479205332736 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.23019997775554657, loss=1.4703747034072876
I0310 22:58:01.845841 139479213725440 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.21682368218898773, loss=1.4197516441345215
I0310 22:58:39.888898 139479205332736 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.2397628128528595, loss=1.5598218441009521
I0310 22:59:17.965246 139479213725440 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.2089872658252716, loss=1.3778153657913208
I0310 22:59:56.049154 139479205332736 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.22265423834323883, loss=1.478379726409912
I0310 23:00:34.101588 139479213725440 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.22558312118053436, loss=1.5678800344467163
I0310 23:01:12.144756 139479205332736 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.22247198224067688, loss=1.4692332744598389
I0310 23:01:50.211477 139479213725440 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.2160521000623703, loss=1.435379147529602
I0310 23:02:28.259069 139479205332736 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.20771612226963043, loss=1.4018871784210205
I0310 23:03:06.326427 139479213725440 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.23001894354820251, loss=1.5050327777862549
I0310 23:03:44.418367 139479205332736 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.21915173530578613, loss=1.5539003610610962
I0310 23:04:22.513962 139479213725440 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.21214014291763306, loss=1.494454264640808
I0310 23:05:00.607252 139479205332736 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.22436225414276123, loss=1.4520200490951538
I0310 23:05:38.641035 139479213725440 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.21987859904766083, loss=1.4687765836715698
I0310 23:06:16.714486 139479205332736 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.21145382523536682, loss=1.4778566360473633
I0310 23:06:54.791096 139479213725440 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.3509376347064972, loss=1.4107407331466675
I0310 23:07:32.811796 139479205332736 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.21793752908706665, loss=1.43634033203125
I0310 23:07:57.230944 139667344451392 spec.py:298] Evaluating on the training split.
I0310 23:08:00.238446 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:11:57.897940 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 23:12:00.531371 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:14:59.369601 139667344451392 spec.py:326] Evaluating on the test split.
I0310 23:15:02.055541 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:17:49.405113 139667344451392 submission_runner.py:359] Time since start: 85358.67s, 	Step: 130266, 	{'train/accuracy': 0.6882218718528748, 'train/loss': 1.4139481782913208, 'train/bleu': 35.16435582508785, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 23:17:49.423009 139479213725440 logging_writer.py:48] [130266] global_step=130266, preemption_count=0, score=49412.530432, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=85358.671979, train/accuracy=0.688222, train/bleu=35.164356, train/loss=1.413948, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 23:17:50.397695 139667344451392 checkpoints.py:356] Saving checkpoint at step: 130266
I0310 23:17:53.763909 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_130266
I0310 23:17:53.768417 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_130266.
I0310 23:18:07.051966 139479205332736 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.21756744384765625, loss=1.4587732553482056
I0310 23:18:44.984978 139479196940032 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.22824198007583618, loss=1.494756817817688
I0310 23:19:22.995042 139479205332736 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.2206798642873764, loss=1.5160387754440308
I0310 23:20:00.989385 139479196940032 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.218958780169487, loss=1.5315884351730347
I0310 23:20:39.000520 139479205332736 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.22958962619304657, loss=1.4275404214859009
I0310 23:21:17.054582 139479196940032 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.22519250214099884, loss=1.5160999298095703
I0310 23:21:55.131815 139479205332736 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.21323108673095703, loss=1.5077584981918335
I0310 23:22:33.128775 139479196940032 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.21646906435489655, loss=1.4918320178985596
I0310 23:23:11.180600 139479205332736 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.22475475072860718, loss=1.4235891103744507
I0310 23:23:49.182132 139479196940032 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.2227093130350113, loss=1.4324272871017456
I0310 23:24:27.224941 139479205332736 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.22099505364894867, loss=1.392120122909546
I0310 23:25:05.270598 139479196940032 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.22229281067848206, loss=1.436693787574768
I0310 23:25:43.320096 139479205332736 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.21386657655239105, loss=1.4687846899032593
I0310 23:26:21.394863 139479196940032 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.22501444816589355, loss=1.4425548315048218
I0310 23:26:59.466118 139479205332736 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.20801742374897003, loss=1.4576362371444702
I0310 23:27:37.607322 139479196940032 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.22107449173927307, loss=1.500855803489685
I0310 23:28:15.659237 139479205332736 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.21819710731506348, loss=1.5313231945037842
I0310 23:28:53.739580 139479196940032 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.219629168510437, loss=1.4179326295852661
I0310 23:29:31.774064 139479205332736 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.22297021746635437, loss=1.439909815788269
I0310 23:30:09.889499 139479196940032 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.206671342253685, loss=1.401203989982605
I0310 23:30:47.985839 139479205332736 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.2092369645833969, loss=1.421942949295044
I0310 23:31:26.100362 139479196940032 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.24413786828517914, loss=1.468109369277954
I0310 23:31:53.977842 139667344451392 spec.py:298] Evaluating on the training split.
I0310 23:31:56.969874 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:35:46.735043 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 23:35:49.363925 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:38:48.032346 139667344451392 spec.py:326] Evaluating on the test split.
I0310 23:38:50.713032 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:41:37.766105 139667344451392 submission_runner.py:359] Time since start: 86795.42s, 	Step: 132475, 	{'train/accuracy': 0.6899522542953491, 'train/loss': 1.4076088666915894, 'train/bleu': 35.27276915829082, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 23:41:37.783518 139479205332736 logging_writer.py:48] [132475] global_step=132475, preemption_count=0, score=50249.644494, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=86795.418860, train/accuracy=0.689952, train/bleu=35.272769, train/loss=1.407609, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 23:41:38.767432 139667344451392 checkpoints.py:356] Saving checkpoint at step: 132475
I0310 23:41:42.115376 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_132475
I0310 23:41:42.119813 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_132475.
I0310 23:41:51.978600 139479196940032 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.21748565137386322, loss=1.4893947839736938
I0310 23:42:29.961243 139479213725440 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.22475796937942505, loss=1.4832582473754883
I0310 23:43:08.000176 139479196940032 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.20941784977912903, loss=1.4584753513336182
I0310 23:43:46.032602 139479213725440 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.21743230521678925, loss=1.4723975658416748
I0310 23:44:24.074541 139479196940032 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.21529044210910797, loss=1.4668854475021362
I0310 23:45:02.122385 139479213725440 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.22444161772727966, loss=1.5127770900726318
I0310 23:45:40.175976 139479196940032 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.2165602296590805, loss=1.4448587894439697
I0310 23:46:18.269725 139479213725440 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.21815742552280426, loss=1.4005258083343506
I0310 23:46:56.394801 139479196940032 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.21053647994995117, loss=1.4956839084625244
I0310 23:47:08.282387 139667344451392 spec.py:298] Evaluating on the training split.
I0310 23:47:11.281347 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:51:00.180882 139667344451392 spec.py:310] Evaluating on the validation split.
I0310 23:51:02.820747 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:54:01.589168 139667344451392 spec.py:326] Evaluating on the test split.
I0310 23:54:04.274337 139667344451392 workload.py:179] Translating evaluation dataset.
I0310 23:56:51.284456 139667344451392 submission_runner.py:359] Time since start: 87709.72s, 	Step: 133333, 	{'train/accuracy': 0.6923307776451111, 'train/loss': 1.3933942317962646, 'train/bleu': 35.31184536436698, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003}
I0310 23:56:51.300532 139479213725440 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=50574.614697, test/accuracy=0.711057, test/bleu=30.920585, test/loss=1.266456, test/num_examples=3003, total_duration=87709.723424, train/accuracy=0.692331, train/bleu=35.311845, train/loss=1.393394, validation/accuracy=0.693618, validation/bleu=31.108706, validation/loss=1.370983, validation/num_examples=3000
I0310 23:56:52.270016 139667344451392 checkpoints.py:356] Saving checkpoint at step: 133333
I0310 23:56:55.999336 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333
I0310 23:56:56.003711 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333.
I0310 23:56:56.021864 139479196940032 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=50574.614697
I0310 23:56:56.598414 139667344451392 checkpoints.py:356] Saving checkpoint at step: 133333
I0310 23:57:03.465560 139667344451392 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333
I0310 23:57:03.470025 139667344451392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333.
I0310 23:57:03.499341 139667344451392 submission_runner.py:520] Tuning trial 1/1
I0310 23:57:03.499475 139667344451392 submission_runner.py:521] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, beta1=0.9326607383586145, beta2=0.9955159689799007, warmup_steps=1999, weight_decay=0.08121616522670176, label_smoothing=0.0)
I0310 23:57:03.503468 139667344451392 submission_runner.py:522] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005486090667545795, 'train/loss': 11.013607025146484, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.025561332702637, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.03548812866211, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 34.3516628742218, 'total_duration': 34.54721426963806, 'global_step': 1, 'preemption_count': 0}), (2209, {'train/accuracy': 0.5148157477378845, 'train/loss': 2.8249032497406006, 'train/bleu': 22.7281047693171, 'validation/accuracy': 0.5195719599723816, 'validation/loss': 2.79937481880188, 'validation/bleu': 18.72165831519777, 'validation/num_examples': 3000, 'test/accuracy': 0.5158329010009766, 'test/loss': 2.856799602508545, 'test/bleu': 17.019463688338895, 'test/num_examples': 3003, 'score': 870.7877106666565, 'total_duration': 1792.6453251838684, 'global_step': 2209, 'preemption_count': 0}), (4417, {'train/accuracy': 0.5784480571746826, 'train/loss': 2.2423603534698486, 'train/bleu': 27.17698925930642, 'validation/accuracy': 0.5891805291175842, 'validation/loss': 2.1514506340026855, 'validation/bleu': 23.289699184267153, 'validation/num_examples': 3000, 'test/accuracy': 0.5924699306488037, 'test/loss': 2.1272497177124023, 'test/bleu': 22.156263772275942, 'test/num_examples': 3003, 'score': 1707.8305203914642, 'total_duration': 3116.584163427353, 'global_step': 4417, 'preemption_count': 0}), (6625, {'train/accuracy': 0.6107933521270752, 'train/loss': 1.9620240926742554, 'train/bleu': 29.187282307733632, 'validation/accuracy': 0.617053747177124, 'validation/loss': 1.932531476020813, 'validation/bleu': 25.380176309014352, 'validation/num_examples': 3000, 'test/accuracy': 0.6198826432228088, 'test/loss': 1.8891469240188599, 'test/bleu': 23.87154884445195, 'test/num_examples': 3003, 'score': 2544.7200648784637, 'total_duration': 4423.424187660217, 'global_step': 6625, 'preemption_count': 0}), (8831, {'train/accuracy': 0.6131600737571716, 'train/loss': 1.950272798538208, 'train/bleu': 29.461344004550806, 'validation/accuracy': 0.6304075717926025, 'validation/loss': 1.8245033025741577, 'validation/bleu': 26.1477890754307, 'validation/num_examples': 3000, 'test/accuracy': 0.6371855139732361, 'test/loss': 1.7693054676055908, 'test/bleu': 25.083469679290186, 'test/num_examples': 3003, 'score': 3381.25972032547, 'total_duration': 5695.164191007614, 'global_step': 8831, 'preemption_count': 0}), (11039, {'train/accuracy': 0.6168193221092224, 'train/loss': 1.9161125421524048, 'train/bleu': 30.29042642640719, 'validation/accuracy': 0.6386653780937195, 'validation/loss': 1.7650939226150513, 'validation/bleu': 26.871443771245396, 'validation/num_examples': 3000, 'test/accuracy': 0.6444599628448486, 'test/loss': 1.7024526596069336, 'test/bleu': 25.6739794667213, 'test/num_examples': 3003, 'score': 4218.180894613266, 'total_duration': 7012.666455745697, 'global_step': 11039, 'preemption_count': 0}), (13247, {'train/accuracy': 0.6304225921630859, 'train/loss': 1.7965344190597534, 'train/bleu': 30.365646850061317, 'validation/accuracy': 0.6435133814811707, 'validation/loss': 1.7158416509628296, 'validation/bleu': 26.9875655178886, 'validation/num_examples': 3000, 'test/accuracy': 0.6513044238090515, 'test/loss': 1.6521775722503662, 'test/bleu': 26.157470880858774, 'test/num_examples': 3003, 'score': 5055.267154216766, 'total_duration': 8306.119352340698, 'global_step': 13247, 'preemption_count': 0}), (15455, {'train/accuracy': 0.6287755966186523, 'train/loss': 1.8220529556274414, 'train/bleu': 30.291904551234712, 'validation/accuracy': 0.6475927233695984, 'validation/loss': 1.6832470893859863, 'validation/bleu': 27.30646174819221, 'validation/num_examples': 3000, 'test/accuracy': 0.6590901613235474, 'test/loss': 1.608644723892212, 'test/bleu': 26.967221303543717, 'test/num_examples': 3003, 'score': 5892.196576118469, 'total_duration': 9621.404797792435, 'global_step': 15455, 'preemption_count': 0}), (17664, {'train/accuracy': 0.6305258274078369, 'train/loss': 1.8074754476547241, 'train/bleu': 30.055514840002132, 'validation/accuracy': 0.6495269536972046, 'validation/loss': 1.66240656375885, 'validation/bleu': 27.26310202235998, 'validation/num_examples': 3000, 'test/accuracy': 0.6598454713821411, 'test/loss': 1.592898964881897, 'test/bleu': 26.630011270921496, 'test/num_examples': 3003, 'score': 6729.210385560989, 'total_duration': 11115.940736532211, 'global_step': 17664, 'preemption_count': 0}), (19872, {'train/accuracy': 0.6410760879516602, 'train/loss': 1.721678376197815, 'train/bleu': 31.255892306340634, 'validation/accuracy': 0.652415931224823, 'validation/loss': 1.6448966264724731, 'validation/bleu': 27.757413057725437, 'validation/num_examples': 3000, 'test/accuracy': 0.664702832698822, 'test/loss': 1.5712642669677734, 'test/bleu': 27.1376884327326, 'test/num_examples': 3003, 'score': 7566.076242208481, 'total_duration': 12617.177949428558, 'global_step': 19872, 'preemption_count': 0}), (22080, {'train/accuracy': 0.6375857591629028, 'train/loss': 1.762131690979004, 'train/bleu': 30.757607968921217, 'validation/accuracy': 0.6544246077537537, 'validation/loss': 1.6277469396591187, 'validation/bleu': 27.58514496549623, 'validation/num_examples': 3000, 'test/accuracy': 0.6679217219352722, 'test/loss': 1.5480490922927856, 'test/bleu': 27.55242671673115, 'test/num_examples': 3003, 'score': 8403.004111289978, 'total_duration': 14010.314790010452, 'global_step': 22080, 'preemption_count': 0}), (24289, {'train/accuracy': 0.6386384963989258, 'train/loss': 1.756717562675476, 'train/bleu': 31.09430183229091, 'validation/accuracy': 0.6561108827590942, 'validation/loss': 1.6155592203140259, 'validation/bleu': 27.922035610183844, 'validation/num_examples': 3000, 'test/accuracy': 0.6677008867263794, 'test/loss': 1.5392003059387207, 'test/bleu': 27.59171743878872, 'test/num_examples': 3003, 'score': 9239.943794965744, 'total_duration': 15608.089581251144, 'global_step': 24289, 'preemption_count': 0}), (26497, {'train/accuracy': 0.6424170732498169, 'train/loss': 1.7030401229858398, 'train/bleu': 31.409216344746813, 'validation/accuracy': 0.659694254398346, 'validation/loss': 1.601055383682251, 'validation/bleu': 28.27352770993427, 'validation/num_examples': 3000, 'test/accuracy': 0.6693510413169861, 'test/loss': 1.5284686088562012, 'test/bleu': 27.382106408728536, 'test/num_examples': 3003, 'score': 10076.857558965683, 'total_duration': 16974.169753313065, 'global_step': 26497, 'preemption_count': 0}), (28705, {'train/accuracy': 0.6411365866661072, 'train/loss': 1.7290245294570923, 'train/bleu': 30.99537003611994, 'validation/accuracy': 0.6603266000747681, 'validation/loss': 1.5942227840423584, 'validation/bleu': 28.163309539775355, 'validation/num_examples': 3000, 'test/accuracy': 0.6741386651992798, 'test/loss': 1.5118300914764404, 'test/bleu': 27.834865371067284, 'test/num_examples': 3003, 'score': 10913.350633382797, 'total_duration': 18507.915345668793, 'global_step': 28705, 'preemption_count': 0}), (30912, {'train/accuracy': 0.6416804790496826, 'train/loss': 1.728795051574707, 'train/bleu': 31.032973717829783, 'validation/accuracy': 0.6616780757904053, 'validation/loss': 1.5820846557617188, 'validation/bleu': 27.926792805953077, 'validation/num_examples': 3000, 'test/accuracy': 0.6728487610816956, 'test/loss': 1.5049326419830322, 'test/bleu': 27.49777381514415, 'test/num_examples': 3003, 'score': 11750.201052188873, 'total_duration': 19900.187679052353, 'global_step': 30912, 'preemption_count': 0}), (33119, {'train/accuracy': 0.6491636633872986, 'train/loss': 1.6753838062286377, 'train/bleu': 31.83454204698703, 'validation/accuracy': 0.6607977747917175, 'validation/loss': 1.5729471445083618, 'validation/bleu': 27.79212884005354, 'validation/num_examples': 3000, 'test/accuracy': 0.6751263737678528, 'test/loss': 1.4910290241241455, 'test/bleu': 27.75899427976285, 'test/num_examples': 3003, 'score': 12586.969906568527, 'total_duration': 21263.236769914627, 'global_step': 33119, 'preemption_count': 0}), (35326, {'train/accuracy': 0.6468344330787659, 'train/loss': 1.6949365139007568, 'train/bleu': 31.926462409987625, 'validation/accuracy': 0.665162205696106, 'validation/loss': 1.5592386722564697, 'validation/bleu': 28.441987746962315, 'validation/num_examples': 3000, 'test/accuracy': 0.676416277885437, 'test/loss': 1.483656406402588, 'test/bleu': 28.214819397498445, 'test/num_examples': 3003, 'score': 13424.017350912094, 'total_duration': 22844.63162469864, 'global_step': 35326, 'preemption_count': 0}), (37534, {'train/accuracy': 0.6703068614006042, 'train/loss': 1.5395845174789429, 'train/bleu': 33.470978948812636, 'validation/accuracy': 0.665075421333313, 'validation/loss': 1.558985948562622, 'validation/bleu': 28.791192688013695, 'validation/num_examples': 3000, 'test/accuracy': 0.6781477332115173, 'test/loss': 1.4762883186340332, 'test/bleu': 27.935895725376845, 'test/num_examples': 3003, 'score': 14260.883837223053, 'total_duration': 24271.422353744507, 'global_step': 37534, 'preemption_count': 0}), (39743, {'train/accuracy': 0.6500890851020813, 'train/loss': 1.6639429330825806, 'train/bleu': 31.350653435801895, 'validation/accuracy': 0.6670345067977905, 'validation/loss': 1.5461325645446777, 'validation/bleu': 28.754831161977457, 'validation/num_examples': 3000, 'test/accuracy': 0.6785311698913574, 'test/loss': 1.4651907682418823, 'test/bleu': 28.171057878563253, 'test/num_examples': 3003, 'score': 15097.930699825287, 'total_duration': 25674.087282419205, 'global_step': 39743, 'preemption_count': 0}), (41951, {'train/accuracy': 0.6513440608978271, 'train/loss': 1.6586329936981201, 'train/bleu': 32.34427413706207, 'validation/accuracy': 0.6689563393592834, 'validation/loss': 1.5376447439193726, 'validation/bleu': 28.49650263359707, 'validation/num_examples': 3000, 'test/accuracy': 0.6801928877830505, 'test/loss': 1.4536679983139038, 'test/bleu': 28.41930312809594, 'test/num_examples': 3003, 'score': 15934.899314165115, 'total_duration': 27097.949345350266, 'global_step': 41951, 'preemption_count': 0}), (44159, {'train/accuracy': 0.6668686866760254, 'train/loss': 1.5389398336410522, 'train/bleu': 33.027143920282136, 'validation/accuracy': 0.6703822612762451, 'validation/loss': 1.525027871131897, 'validation/bleu': 29.012150640453754, 'validation/num_examples': 3000, 'test/accuracy': 0.6822962164878845, 'test/loss': 1.4443830251693726, 'test/bleu': 28.651560987598042, 'test/num_examples': 3003, 'score': 16771.819239377975, 'total_duration': 28494.739745140076, 'global_step': 44159, 'preemption_count': 0}), (46366, {'train/accuracy': 0.65520840883255, 'train/loss': 1.6176486015319824, 'train/bleu': 32.32295884980679, 'validation/accuracy': 0.6708162426948547, 'validation/loss': 1.5153177976608276, 'validation/bleu': 28.940572733548493, 'validation/num_examples': 3000, 'test/accuracy': 0.6833188533782959, 'test/loss': 1.433402419090271, 'test/bleu': 28.795042940423215, 'test/num_examples': 3003, 'score': 17608.827063560486, 'total_duration': 29984.026712179184, 'global_step': 46366, 'preemption_count': 0}), (48574, {'train/accuracy': 0.6518847346305847, 'train/loss': 1.6468784809112549, 'train/bleu': 32.42530928243256, 'validation/accuracy': 0.6714361906051636, 'validation/loss': 1.5131075382232666, 'validation/bleu': 29.234529956586847, 'validation/num_examples': 3000, 'test/accuracy': 0.6860728859901428, 'test/loss': 1.4255199432373047, 'test/bleu': 28.647253701992746, 'test/num_examples': 3003, 'score': 18446.102584838867, 'total_duration': 31551.8760035038, 'global_step': 48574, 'preemption_count': 0}), (50781, {'train/accuracy': 0.6651281714439392, 'train/loss': 1.554115891456604, 'train/bleu': 33.010174543235756, 'validation/accuracy': 0.6737052202224731, 'validation/loss': 1.5002410411834717, 'validation/bleu': 29.20247955242532, 'validation/num_examples': 3000, 'test/accuracy': 0.6858288645744324, 'test/loss': 1.4174050092697144, 'test/bleu': 28.728659102374237, 'test/num_examples': 3003, 'score': 19282.854324817657, 'total_duration': 32931.15357232094, 'global_step': 50781, 'preemption_count': 0}), (52988, {'train/accuracy': 0.6589625477790833, 'train/loss': 1.6013875007629395, 'train/bleu': 32.350600623363505, 'validation/accuracy': 0.6752427220344543, 'validation/loss': 1.4943979978561401, 'validation/bleu': 29.424971805114474, 'validation/num_examples': 3000, 'test/accuracy': 0.6882807612419128, 'test/loss': 1.4021090269088745, 'test/bleu': 28.97845816840377, 'test/num_examples': 3003, 'score': 20120.000185489655, 'total_duration': 34414.90288352966, 'global_step': 52988, 'preemption_count': 0}), (55196, {'train/accuracy': 0.6581787467002869, 'train/loss': 1.6115548610687256, 'train/bleu': 32.77269833835701, 'validation/accuracy': 0.6750071048736572, 'validation/loss': 1.4914934635162354, 'validation/bleu': 29.233946453709738, 'validation/num_examples': 3000, 'test/accuracy': 0.6896287202835083, 'test/loss': 1.4000343084335327, 'test/bleu': 29.13969687460392, 'test/num_examples': 3003, 'score': 20956.61699461937, 'total_duration': 35968.42943286896, 'global_step': 55196, 'preemption_count': 0}), (57403, {'train/accuracy': 0.6688748002052307, 'train/loss': 1.529470443725586, 'train/bleu': 32.8679719597202, 'validation/accuracy': 0.6767057776451111, 'validation/loss': 1.4770402908325195, 'validation/bleu': 29.576737294651515, 'validation/num_examples': 3000, 'test/accuracy': 0.6911161541938782, 'test/loss': 1.3892191648483276, 'test/bleu': 29.367747552592764, 'test/num_examples': 3003, 'score': 21793.714372634888, 'total_duration': 37409.96915793419, 'global_step': 57403, 'preemption_count': 0}), (59610, {'train/accuracy': 0.6687506437301636, 'train/loss': 1.5389645099639893, 'train/bleu': 33.386464967585454, 'validation/accuracy': 0.6782432794570923, 'validation/loss': 1.4693187475204468, 'validation/bleu': 29.260053902552222, 'validation/num_examples': 3000, 'test/accuracy': 0.6937307715415955, 'test/loss': 1.3724411725997925, 'test/bleu': 29.472963709040275, 'test/num_examples': 3003, 'score': 22630.77933239937, 'total_duration': 38851.06063723564, 'global_step': 59610, 'preemption_count': 0}), (61817, {'train/accuracy': 0.6634488701820374, 'train/loss': 1.5767509937286377, 'train/bleu': 32.65168481297114, 'validation/accuracy': 0.6803635358810425, 'validation/loss': 1.4587160348892212, 'validation/bleu': 29.661452102767296, 'validation/num_examples': 3000, 'test/accuracy': 0.6965312957763672, 'test/loss': 1.3633774518966675, 'test/bleu': 29.656919652276386, 'test/num_examples': 3003, 'score': 23467.18017911911, 'total_duration': 40450.56183433533, 'global_step': 61817, 'preemption_count': 0}), (64025, {'train/accuracy': 0.6724666953086853, 'train/loss': 1.5077913999557495, 'train/bleu': 33.49897729036829, 'validation/accuracy': 0.6825953722000122, 'validation/loss': 1.4473845958709717, 'validation/bleu': 29.688873026658328, 'validation/num_examples': 3000, 'test/accuracy': 0.6963453888893127, 'test/loss': 1.3594682216644287, 'test/bleu': 29.641632742723417, 'test/num_examples': 3003, 'score': 24304.39891719818, 'total_duration': 41838.11627602577, 'global_step': 64025, 'preemption_count': 0}), (66234, {'train/accuracy': 0.671855092048645, 'train/loss': 1.5220634937286377, 'train/bleu': 33.673221140985994, 'validation/accuracy': 0.6824465990066528, 'validation/loss': 1.4436591863632202, 'validation/bleu': 29.791277737131875, 'validation/num_examples': 3000, 'test/accuracy': 0.6987972855567932, 'test/loss': 1.3430780172348022, 'test/bleu': 29.83415922910861, 'test/num_examples': 3003, 'score': 25141.71415925026, 'total_duration': 43340.00063729286, 'global_step': 66234, 'preemption_count': 0}), (68442, {'train/accuracy': 0.6690624356269836, 'train/loss': 1.536400556564331, 'train/bleu': 33.65930010309209, 'validation/accuracy': 0.6832773089408875, 'validation/loss': 1.4335070848464966, 'validation/bleu': 29.61420765126098, 'validation/num_examples': 3000, 'test/accuracy': 0.6984370350837708, 'test/loss': 1.3391729593276978, 'test/bleu': 29.46285816369212, 'test/num_examples': 3003, 'score': 25978.6601729393, 'total_duration': 44889.03228831291, 'global_step': 68442, 'preemption_count': 0}), (70650, {'train/accuracy': 0.6790371537208557, 'train/loss': 1.468093752861023, 'train/bleu': 34.16825540146199, 'validation/accuracy': 0.6862903237342834, 'validation/loss': 1.421138048171997, 'validation/bleu': 30.222751241209593, 'validation/num_examples': 3000, 'test/accuracy': 0.7009819746017456, 'test/loss': 1.3293074369430542, 'test/bleu': 29.841257767057396, 'test/num_examples': 3003, 'score': 26815.657141447067, 'total_duration': 46252.22345113754, 'global_step': 70650, 'preemption_count': 0}), (72858, {'train/accuracy': 0.6763147115707397, 'train/loss': 1.483642816543579, 'train/bleu': 33.30129068983058, 'validation/accuracy': 0.6865134835243225, 'validation/loss': 1.414038896560669, 'validation/bleu': 30.19502093341028, 'validation/num_examples': 3000, 'test/accuracy': 0.7019231915473938, 'test/loss': 1.3191272020339966, 'test/bleu': 30.02672152046085, 'test/num_examples': 3003, 'score': 27652.425801992416, 'total_duration': 47731.76927900314, 'global_step': 72858, 'preemption_count': 0}), (75066, {'train/accuracy': 0.6912358999252319, 'train/loss': 1.4028245210647583, 'train/bleu': 35.08809718531821, 'validation/accuracy': 0.687889814376831, 'validation/loss': 1.4114909172058105, 'validation/bleu': 30.30036838579653, 'validation/num_examples': 3000, 'test/accuracy': 0.7035965323448181, 'test/loss': 1.3124353885650635, 'test/bleu': 30.03725170320686, 'test/num_examples': 3003, 'score': 28489.40935921669, 'total_duration': 49244.73916745186, 'global_step': 75066, 'preemption_count': 0}), (77274, {'train/accuracy': 0.6815696358680725, 'train/loss': 1.4528909921646118, 'train/bleu': 34.45791832931259, 'validation/accuracy': 0.6889933347702026, 'validation/loss': 1.4010437726974487, 'validation/bleu': 30.275694084382916, 'validation/num_examples': 3000, 'test/accuracy': 0.7051304578781128, 'test/loss': 1.3022544384002686, 'test/bleu': 30.322258281412612, 'test/num_examples': 3003, 'score': 29326.32079076767, 'total_duration': 50697.920795440674, 'global_step': 77274, 'preemption_count': 0}), (79482, {'train/accuracy': 0.6819186806678772, 'train/loss': 1.452987551689148, 'train/bleu': 34.34596542901429, 'validation/accuracy': 0.6894768476486206, 'validation/loss': 1.397220253944397, 'validation/bleu': 30.495073244464894, 'validation/num_examples': 3000, 'test/accuracy': 0.7063273787498474, 'test/loss': 1.2936499118804932, 'test/bleu': 30.514199400740736, 'test/num_examples': 3003, 'score': 30163.202345609665, 'total_duration': 52206.874719142914, 'global_step': 79482, 'preemption_count': 0}), (81689, {'train/accuracy': 0.6911970973014832, 'train/loss': 1.3932617902755737, 'train/bleu': 34.999901150795445, 'validation/accuracy': 0.6922791004180908, 'validation/loss': 1.3901021480560303, 'validation/bleu': 30.82339503225242, 'validation/num_examples': 3000, 'test/accuracy': 0.7073732018470764, 'test/loss': 1.2911477088928223, 'test/bleu': 30.56165665897835, 'test/num_examples': 3003, 'score': 31000.127915859222, 'total_duration': 53604.01049613953, 'global_step': 81689, 'preemption_count': 0}), (83896, {'train/accuracy': 0.6862708926200867, 'train/loss': 1.4239469766616821, 'train/bleu': 34.7863066432078, 'validation/accuracy': 0.6918079257011414, 'validation/loss': 1.386249303817749, 'validation/bleu': 30.8209711897384, 'validation/num_examples': 3000, 'test/accuracy': 0.7076404690742493, 'test/loss': 1.2858471870422363, 'test/bleu': 30.474917143723186, 'test/num_examples': 3003, 'score': 31837.088873147964, 'total_duration': 55018.16794848442, 'global_step': 83896, 'preemption_count': 0}), (86104, {'train/accuracy': 0.6841979026794434, 'train/loss': 1.4375219345092773, 'train/bleu': 34.81450445414224, 'validation/accuracy': 0.6919814944267273, 'validation/loss': 1.3840076923370361, 'validation/bleu': 30.78276959542512, 'validation/num_examples': 3000, 'test/accuracy': 0.7082447409629822, 'test/loss': 1.2816535234451294, 'test/bleu': 30.401666794917436, 'test/num_examples': 3003, 'score': 32673.890523910522, 'total_duration': 56564.18451213837, 'global_step': 86104, 'preemption_count': 0}), (88311, {'train/accuracy': 0.6953146457672119, 'train/loss': 1.375197410583496, 'train/bleu': 35.31733295340084, 'validation/accuracy': 0.6934322118759155, 'validation/loss': 1.379611849784851, 'validation/bleu': 30.891172567849278, 'validation/num_examples': 3000, 'test/accuracy': 0.7099413275718689, 'test/loss': 1.2765907049179077, 'test/bleu': 30.705119576444037, 'test/num_examples': 3003, 'score': 33510.785957574844, 'total_duration': 58001.012995004654, 'global_step': 88311, 'preemption_count': 0}), (90520, {'train/accuracy': 0.6905471086502075, 'train/loss': 1.4070128202438354, 'train/bleu': 35.380318617067445, 'validation/accuracy': 0.6930602192878723, 'validation/loss': 1.3752175569534302, 'validation/bleu': 30.929527900417934, 'validation/num_examples': 3000, 'test/accuracy': 0.7106385827064514, 'test/loss': 1.2709410190582275, 'test/bleu': 30.842665916511915, 'test/num_examples': 3003, 'score': 34347.89451670647, 'total_duration': 59500.65424180031, 'global_step': 90520, 'preemption_count': 0}), (92729, {'train/accuracy': 0.6902154088020325, 'train/loss': 1.4038432836532593, 'train/bleu': 35.216317839623656, 'validation/accuracy': 0.6935561895370483, 'validation/loss': 1.372991919517517, 'validation/bleu': 31.04262787770375, 'validation/num_examples': 3000, 'test/accuracy': 0.7105223536491394, 'test/loss': 1.2690119743347168, 'test/bleu': 30.753902241190726, 'test/num_examples': 3003, 'score': 35185.066705942154, 'total_duration': 60943.15472650528, 'global_step': 92729, 'preemption_count': 0}), (94937, {'train/accuracy': 0.6949642300605774, 'train/loss': 1.379796028137207, 'train/bleu': 34.983656612319805, 'validation/accuracy': 0.6936801671981812, 'validation/loss': 1.3716943264007568, 'validation/bleu': 31.027450445434546, 'validation/num_examples': 3000, 'test/accuracy': 0.7109174728393555, 'test/loss': 1.2673640251159668, 'test/bleu': 30.865486531611324, 'test/num_examples': 3003, 'score': 36022.228355407715, 'total_duration': 62371.07856583595, 'global_step': 94937, 'preemption_count': 0}), (97145, {'train/accuracy': 0.6938560605049133, 'train/loss': 1.382908821105957, 'train/bleu': 35.045964375192526, 'validation/accuracy': 0.6935809850692749, 'validation/loss': 1.3716565370559692, 'validation/bleu': 30.942928503907456, 'validation/num_examples': 3000, 'test/accuracy': 0.7108477354049683, 'test/loss': 1.2668497562408447, 'test/bleu': 30.946916357119132, 'test/num_examples': 3003, 'score': 36859.16488933563, 'total_duration': 63813.02721905708, 'global_step': 97145, 'preemption_count': 0}), (99353, {'train/accuracy': 0.6920814514160156, 'train/loss': 1.3939239978790283, 'train/bleu': 35.17504075512664, 'validation/accuracy': 0.693630576133728, 'validation/loss': 1.3709577322006226, 'validation/bleu': 31.077740131730607, 'validation/num_examples': 3000, 'test/accuracy': 0.7110103964805603, 'test/loss': 1.2664778232574463, 'test/bleu': 30.9330050039182, 'test/num_examples': 3003, 'score': 37696.1074488163, 'total_duration': 65269.52800679207, 'global_step': 99353, 'preemption_count': 0}), (101562, {'train/accuracy': 0.6914293169975281, 'train/loss': 1.3962477445602417, 'train/bleu': 35.14922664243305, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 38533.21717739105, 'total_duration': 66696.39196562767, 'global_step': 101562, 'preemption_count': 0}), (103770, {'train/accuracy': 0.6912432312965393, 'train/loss': 1.4025146961212158, 'train/bleu': 35.32366220521299, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 39369.77699995041, 'total_duration': 68125.77504611015, 'global_step': 103770, 'preemption_count': 0}), (105978, {'train/accuracy': 0.692474901676178, 'train/loss': 1.393912672996521, 'train/bleu': 35.4926919263436, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 40206.72649550438, 'total_duration': 69557.48527479172, 'global_step': 105978, 'preemption_count': 0}), (108187, {'train/accuracy': 0.6928708553314209, 'train/loss': 1.3952828645706177, 'train/bleu': 35.49152637640315, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 41043.97418260574, 'total_duration': 70985.0417573452, 'global_step': 108187, 'preemption_count': 0}), (110394, {'train/accuracy': 0.6902603507041931, 'train/loss': 1.403262972831726, 'train/bleu': 35.05110832232195, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 41880.59938669205, 'total_duration': 72418.84528160095, 'global_step': 110394, 'preemption_count': 0}), (112603, {'train/accuracy': 0.691085159778595, 'train/loss': 1.4016913175582886, 'train/bleu': 35.272948290954716, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 42717.77110147476, 'total_duration': 73862.32487869263, 'global_step': 112603, 'preemption_count': 0}), (114810, {'train/accuracy': 0.692069947719574, 'train/loss': 1.3982443809509277, 'train/bleu': 35.4151978832511, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 43554.31071162224, 'total_duration': 75285.57161140442, 'global_step': 114810, 'preemption_count': 0}), (117018, {'train/accuracy': 0.6904824376106262, 'train/loss': 1.3978420495986938, 'train/bleu': 35.15842067106161, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 44391.388608932495, 'total_duration': 76730.34843444824, 'global_step': 117018, 'preemption_count': 0}), (119225, {'train/accuracy': 0.6914687156677246, 'train/loss': 1.3962147235870361, 'train/bleu': 35.12967524929205, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 45228.243196964264, 'total_duration': 78168.06017708778, 'global_step': 119225, 'preemption_count': 0}), (121433, {'train/accuracy': 0.6949947476387024, 'train/loss': 1.3745415210723877, 'train/bleu': 34.940446582224986, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 46064.926968574524, 'total_duration': 79601.68671345711, 'global_step': 121433, 'preemption_count': 0}), (123640, {'train/accuracy': 0.6892856955528259, 'train/loss': 1.4049540758132935, 'train/bleu': 34.878482318010406, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 46901.775324344635, 'total_duration': 81042.10222911835, 'global_step': 123640, 'preemption_count': 0}), (125849, {'train/accuracy': 0.6911211013793945, 'train/loss': 1.3947985172271729, 'train/bleu': 35.112504179241746, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 47738.752504110336, 'total_duration': 82482.49360871315, 'global_step': 125849, 'preemption_count': 0}), (128057, {'train/accuracy': 0.6901925206184387, 'train/loss': 1.4033451080322266, 'train/bleu': 35.01325996777261, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 48575.545370817184, 'total_duration': 83922.65361571312, 'global_step': 128057, 'preemption_count': 0}), (130266, {'train/accuracy': 0.6882218718528748, 'train/loss': 1.4139481782913208, 'train/bleu': 35.16435582508785, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 49412.53043174744, 'total_duration': 85358.6719789505, 'global_step': 130266, 'preemption_count': 0}), (132475, {'train/accuracy': 0.6899522542953491, 'train/loss': 1.4076088666915894, 'train/bleu': 35.27276915829082, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 50249.64449429512, 'total_duration': 86795.41885972023, 'global_step': 132475, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6923307776451111, 'train/loss': 1.3933942317962646, 'train/bleu': 35.31184536436698, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.3709828853607178, 'validation/bleu': 31.108705962677252, 'validation/num_examples': 3000, 'test/accuracy': 0.7110568881034851, 'test/loss': 1.2664564847946167, 'test/bleu': 30.92058506500475, 'test/num_examples': 3003, 'score': 50574.614696741104, 'total_duration': 87709.72342443466, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0310 23:57:03.503665 139667344451392 submission_runner.py:523] Timing: 50574.614696741104
I0310 23:57:03.503749 139667344451392 submission_runner.py:524] ====================
I0310 23:57:03.503891 139667344451392 submission_runner.py:583] Final wmt score: 50574.614696741104
