python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_04-29-2023-13-08-31.log
I0429 13:08:52.472892 140112961300288 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax.
I0429 13:08:52.552534 140112961300288 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 13:08:53.407254 140112961300288 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0429 13:08:53.407885 140112961300288 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 13:08:53.411843 140112961300288 submission_runner.py:538] Using RNG seed 2500006991
I0429 13:08:56.050696 140112961300288 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 13:08:56.050900 140112961300288 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1.
I0429 13:08:56.051146 140112961300288 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1/hparams.json.
I0429 13:08:56.175732 140112961300288 submission_runner.py:241] Initializing dataset.
I0429 13:08:56.175982 140112961300288 submission_runner.py:248] Initializing model.
I0429 13:09:02.120980 140112961300288 submission_runner.py:258] Initializing optimizer.
I0429 13:09:02.939964 140112961300288 submission_runner.py:265] Initializing metrics bundle.
I0429 13:09:02.940160 140112961300288 submission_runner.py:282] Initializing checkpoint and logger.
I0429 13:09:02.941018 140112961300288 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0429 13:09:02.941287 140112961300288 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 13:09:02.941354 140112961300288 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 13:09:03.690466 140112961300288 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0429 13:09:03.691394 140112961300288 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1/flags_0.json.
I0429 13:09:03.698183 140112961300288 submission_runner.py:318] Starting training loop.
I0429 13:09:03.891626 140112961300288 input_pipeline.py:20] Loading split = train-clean-100
I0429 13:09:03.923042 140112961300288 input_pipeline.py:20] Loading split = train-clean-360
I0429 13:09:04.247355 140112961300288 input_pipeline.py:20] Loading split = train-other-500
2023-04-29 13:10:21.905225: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-29 13:10:22.163924: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0429 13:10:24.152135 139936846051072 logging_writer.py:48] [0] global_step=0, grad_norm=105.5168228149414, loss=31.3052978515625
I0429 13:10:24.181666 140112961300288 spec.py:298] Evaluating on the training split.
I0429 13:10:24.286406 140112961300288 input_pipeline.py:20] Loading split = train-clean-100
I0429 13:10:24.315044 140112961300288 input_pipeline.py:20] Loading split = train-clean-360
I0429 13:10:24.600769 140112961300288 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0429 13:11:16.564353 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 13:11:16.629068 140112961300288 input_pipeline.py:20] Loading split = dev-clean
I0429 13:11:16.633886 140112961300288 input_pipeline.py:20] Loading split = dev-other
I0429 13:11:59.029857 140112961300288 spec.py:326] Evaluating on the test split.
I0429 13:11:59.095998 140112961300288 input_pipeline.py:20] Loading split = test-clean
I0429 13:12:28.931226 140112961300288 submission_runner.py:415] Time since start: 205.23s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(30.676682, dtype=float32), 'train/wer': 1.1878891364554485, 'validation/ctc_loss': DeviceArray(29.399063, dtype=float32), 'validation/wer': 1.3489662225395325, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.454649, dtype=float32), 'test/wer': 1.3627445006398147, 'test/num_examples': 2472, 'score': 80.4832980632782, 'total_duration': 205.23145198822021, 'accumulated_submission_time': 80.4832980632782, 'accumulated_eval_time': 124.74799418449402, 'accumulated_logging_time': 0}
I0429 13:12:28.955646 139933490607872 logging_writer.py:48] [1] accumulated_eval_time=124.747994, accumulated_logging_time=0, accumulated_submission_time=80.483298, global_step=1, preemption_count=0, score=80.483298, test/ctc_loss=29.454648971557617, test/num_examples=2472, test/wer=1.362745, total_duration=205.231452, train/ctc_loss=30.676681518554688, train/wer=1.187889, validation/ctc_loss=29.399063110351562, validation/num_examples=5348, validation/wer=1.348966
I0429 13:14:18.101115 139937776051968 logging_writer.py:48] [100] global_step=100, grad_norm=65.00130462646484, loss=16.93662452697754
I0429 13:15:33.862807 139937784444672 logging_writer.py:48] [200] global_step=200, grad_norm=20.19190788269043, loss=10.216423988342285
I0429 13:16:49.644150 139937776051968 logging_writer.py:48] [300] global_step=300, grad_norm=10.42761516571045, loss=7.258882999420166
I0429 13:18:05.404468 139937784444672 logging_writer.py:48] [400] global_step=400, grad_norm=0.9915550351142883, loss=6.196399688720703
I0429 13:19:21.240164 139937776051968 logging_writer.py:48] [500] global_step=500, grad_norm=2.948254346847534, loss=5.969305992126465
I0429 13:20:37.052831 139937784444672 logging_writer.py:48] [600] global_step=600, grad_norm=0.5620943307876587, loss=5.880702495574951
I0429 13:21:52.919854 139937776051968 logging_writer.py:48] [700] global_step=700, grad_norm=2.317457914352417, loss=5.844999313354492
I0429 13:23:08.838222 139937784444672 logging_writer.py:48] [800] global_step=800, grad_norm=2.3066492080688477, loss=5.824455261230469
I0429 13:24:24.760913 139937776051968 logging_writer.py:48] [900] global_step=900, grad_norm=1.898998737335205, loss=5.817814350128174
I0429 13:25:40.711838 139937784444672 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.27622705698013306, loss=5.810911655426025
I0429 13:26:59.937113 139938926581504 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.36280959844589233, loss=5.802700996398926
I0429 13:28:16.075486 139938918188800 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4869961440563202, loss=5.78028678894043
I0429 13:29:31.911739 139938926581504 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.705293893814087, loss=5.807028293609619
I0429 13:30:47.742072 139938918188800 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7035846710205078, loss=5.791754245758057
I0429 13:32:03.595779 139938926581504 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9745005369186401, loss=5.795588493347168
I0429 13:33:19.515100 139938918188800 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5778385996818542, loss=5.791810989379883
I0429 13:34:35.559113 139938926581504 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.3407772183418274, loss=5.800775051116943
I0429 13:35:51.472115 139938918188800 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.733305811882019, loss=5.788067817687988
I0429 13:37:07.436021 139938926581504 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6836584806442261, loss=5.800527095794678
I0429 13:38:23.310428 139938918188800 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.4878026247024536, loss=5.780227184295654
I0429 13:39:42.807703 139938926581504 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4547402858734131, loss=5.677432060241699
I0429 13:40:58.669007 139938918188800 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.7226331233978271, loss=5.607292652130127
I0429 13:42:14.542297 139938926581504 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.4128119945526123, loss=5.548257350921631
I0429 13:43:30.395623 139938918188800 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5969879627227783, loss=5.514796733856201
I0429 13:44:46.389116 139938926581504 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5498712658882141, loss=5.534456729888916
I0429 13:46:02.339424 139938918188800 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8197226524353027, loss=5.542491912841797
I0429 13:47:18.302860 139938926581504 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0689302682876587, loss=5.501464366912842
I0429 13:48:34.201613 139938918188800 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.3692500591278076, loss=5.519251823425293
I0429 13:49:50.143696 139938926581504 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0805020332336426, loss=5.500378608703613
I0429 13:51:06.332753 139938918188800 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.863288402557373, loss=5.491876602172852
I0429 13:52:25.746797 139939581941504 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7722516655921936, loss=5.485838413238525
I0429 13:52:29.396591 140112961300288 spec.py:298] Evaluating on the training split.
I0429 13:52:57.466294 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 13:53:33.279526 140112961300288 spec.py:326] Evaluating on the test split.
I0429 13:53:51.293357 140112961300288 submission_runner.py:415] Time since start: 2687.59s, 	Step: 3106, 	{'train/ctc_loss': DeviceArray(6.237166, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.3672566, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.2878737, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2480.870813369751, 'total_duration': 2687.5915632247925, 'accumulated_submission_time': 2480.870813369751, 'accumulated_eval_time': 206.64121508598328, 'accumulated_logging_time': 0.0369570255279541}
I0429 13:53:51.315643 139939653621504 logging_writer.py:48] [3106] accumulated_eval_time=206.641215, accumulated_logging_time=0.036957, accumulated_submission_time=2480.870813, global_step=3106, preemption_count=0, score=2480.870813, test/ctc_loss=6.2878737449646, test/num_examples=2472, test/wer=0.899580, total_duration=2687.591563, train/ctc_loss=6.237165927886963, train/wer=0.944636, validation/ctc_loss=6.3672566413879395, validation/num_examples=5348, validation/wer=0.895995
I0429 13:55:03.285773 139939645228800 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0400235652923584, loss=5.473663806915283
I0429 13:56:19.101991 139939653621504 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9242509603500366, loss=5.487395763397217
I0429 13:57:34.880024 139939645228800 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.169667363166809, loss=5.504669666290283
I0429 13:58:50.676011 139939653621504 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.5812268257141113, loss=5.50458288192749
I0429 14:00:06.416627 139939645228800 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.5141820907592773, loss=5.505943298339844
I0429 14:01:22.186608 139939653621504 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.191382884979248, loss=5.455716609954834
I0429 14:02:37.989220 139939645228800 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.0487475395202637, loss=5.4970855712890625
I0429 14:03:53.934103 139939653621504 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.7240676879882812, loss=5.4528985023498535
I0429 14:05:09.696524 139939645228800 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.3536359071731567, loss=5.490879058837891
I0429 14:06:25.355021 139939653621504 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8235083222389221, loss=5.443617820739746
I0429 14:07:44.552654 139939653621504 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.4013895988464355, loss=5.502588272094727
I0429 14:09:00.193443 139939645228800 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.46256685256958, loss=5.468637943267822
I0429 14:10:15.820779 139939653621504 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.2302249670028687, loss=5.464062690734863
I0429 14:11:31.532455 139939645228800 logging_writer.py:48] [4500] global_step=4500, grad_norm=8.4921236038208, loss=5.444075107574463
I0429 14:12:47.272731 139939653621504 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.3197799921035767, loss=5.43218994140625
I0429 14:14:03.023730 139939645228800 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1854126453399658, loss=5.4557905197143555
I0429 14:15:19.035317 139939653621504 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0984934568405151, loss=5.455304145812988
I0429 14:16:34.776843 139939645228800 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.4449050426483154, loss=5.522019863128662
I0429 14:17:50.504621 139939653621504 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.3012709617614746, loss=5.453972816467285
I0429 14:19:06.223169 139939645228800 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.872809648513794, loss=5.4718780517578125
I0429 14:20:25.507263 139939653621504 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9625365138053894, loss=5.460639476776123
I0429 14:21:41.141611 139939645228800 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.0510258674621582, loss=5.481089115142822
I0429 14:22:56.750634 139939653621504 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7195591330528259, loss=5.456821918487549
I0429 14:24:12.367071 139939645228800 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8666066527366638, loss=5.473720073699951
I0429 14:25:27.917858 139939653621504 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6784060001373291, loss=5.4392571449279785
I0429 14:26:43.884557 139939645228800 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.547585666179657, loss=5.4333176612854
I0429 14:27:59.501953 139939653621504 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.6871042251586914, loss=5.425063133239746
I0429 14:29:15.017603 139939645228800 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.0865780115127563, loss=5.445448875427246
I0429 14:30:30.493868 139939653621504 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9520923495292664, loss=5.439263820648193
I0429 14:31:45.994229 139939645228800 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.5808227062225342, loss=5.4231390953063965
I0429 14:33:04.999690 139938998261504 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7086508274078369, loss=5.43965482711792
I0429 14:33:51.566008 140112961300288 spec.py:298] Evaluating on the training split.
I0429 14:34:19.064312 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 14:34:54.338616 140112961300288 spec.py:326] Evaluating on the test split.
I0429 14:35:12.450989 140112961300288 submission_runner.py:415] Time since start: 5168.75s, 	Step: 6263, 	{'train/ctc_loss': DeviceArray(5.4997864, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(5.4405603, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.410326, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4881.062111616135, 'total_duration': 5168.749376773834, 'accumulated_submission_time': 4881.062111616135, 'accumulated_eval_time': 287.52284574508667, 'accumulated_logging_time': 0.07428860664367676}
I0429 14:35:12.472811 139938998261504 logging_writer.py:48] [6263] accumulated_eval_time=287.522846, accumulated_logging_time=0.074289, accumulated_submission_time=4881.062112, global_step=6263, preemption_count=0, score=4881.062112, test/ctc_loss=5.41032600402832, test/num_examples=2472, test/wer=0.899580, total_duration=5168.749377, train/ctc_loss=5.499786376953125, train/wer=0.942722, validation/ctc_loss=5.440560340881348, validation/num_examples=5348, validation/wer=0.895995
I0429 14:35:41.151538 139938989868800 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.4442405700683594, loss=5.420505046844482
I0429 14:36:56.623585 139938998261504 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.3812190294265747, loss=5.415552616119385
I0429 14:38:12.058127 139938989868800 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3861666917800903, loss=5.443202495574951
I0429 14:39:27.771286 139938998261504 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.0483942031860352, loss=5.442596435546875
I0429 14:40:43.234696 139938989868800 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.9165396690368652, loss=5.4491472244262695
I0429 14:41:58.659345 139938998261504 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.393359899520874, loss=5.424156665802002
I0429 14:43:14.147392 139938989868800 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8658260703086853, loss=5.45404577255249
I0429 14:44:29.571967 139938998261504 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5342962145805359, loss=5.436954021453857
I0429 14:45:44.963412 139938989868800 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.022216796875, loss=5.440877437591553
I0429 14:47:00.304446 139938998261504 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.8252334594726562, loss=5.409095287322998
I0429 14:48:19.342292 139939653621504 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.3560271263122559, loss=5.425788879394531
I0429 14:49:34.885414 139939645228800 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.876945972442627, loss=5.43555212020874
I0429 14:50:50.151848 139939653621504 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8776358366012573, loss=5.430051326751709
I0429 14:52:05.472349 139939645228800 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.0236318111419678, loss=5.405448913574219
I0429 14:53:20.745006 139939653621504 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.058048725128174, loss=5.432309627532959
I0429 14:54:36.093676 139939645228800 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7472898960113525, loss=5.410605430603027
I0429 14:55:51.349471 139939653621504 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.4589704275131226, loss=5.44686222076416
I0429 14:57:06.609906 139939645228800 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.1469712257385254, loss=5.445923805236816
I0429 14:58:21.864068 139939653621504 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.5426852703094482, loss=5.412529945373535
I0429 14:59:37.098129 139939645228800 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.735303282737732, loss=5.453272819519043
I0429 15:00:56.105656 139939653621504 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5801330804824829, loss=5.456755638122559
I0429 15:02:11.340069 139939645228800 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8141514658927917, loss=5.419439792633057
I0429 15:03:26.581755 139939653621504 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.37044358253479, loss=5.42687463760376
I0429 15:04:41.984352 139939645228800 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.3993890285491943, loss=5.402987003326416
I0429 15:05:57.304921 139939653621504 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.4636244773864746, loss=5.433895587921143
I0429 15:07:12.567427 139939645228800 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8130870461463928, loss=5.390927314758301
I0429 15:08:27.890050 139939653621504 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.3169612884521484, loss=5.4282097816467285
I0429 15:09:43.142189 139939645228800 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7408877611160278, loss=5.415733814239502
I0429 15:10:58.361417 139939653621504 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7004249095916748, loss=5.422824859619141
I0429 15:12:13.918739 139939645228800 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.1231416463851929, loss=5.415042400360107
I0429 15:13:34.844416 139938998261504 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.5881333351135254, loss=5.451180458068848
I0429 15:14:50.017953 139938989868800 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.3730289936065674, loss=5.4246015548706055
I0429 15:15:13.129840 140112961300288 spec.py:298] Evaluating on the training split.
I0429 15:15:40.920269 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 15:16:16.649001 140112961300288 spec.py:326] Evaluating on the test split.
I0429 15:16:34.765984 140112961300288 submission_runner.py:415] Time since start: 7651.06s, 	Step: 9432, 	{'train/ctc_loss': DeviceArray(7.637304, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(7.107837, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(7.181365, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7281.658621788025, 'total_duration': 7651.064416408539, 'accumulated_submission_time': 7281.658621788025, 'accumulated_eval_time': 369.1556887626648, 'accumulated_logging_time': 0.11151313781738281}
I0429 15:16:34.788861 139939807221504 logging_writer.py:48] [9432] accumulated_eval_time=369.155689, accumulated_logging_time=0.111513, accumulated_submission_time=7281.658622, global_step=9432, preemption_count=0, score=7281.658622, test/ctc_loss=7.181365013122559, test/num_examples=2472, test/wer=0.899580, total_duration=7651.064416, train/ctc_loss=7.637303829193115, train/wer=0.943324, validation/ctc_loss=7.107837200164795, validation/num_examples=5348, validation/wer=0.895995
I0429 15:17:26.665023 139939798828800 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9956234097480774, loss=5.453327655792236
I0429 15:18:41.843242 139939807221504 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7732990980148315, loss=5.423336505889893
I0429 15:19:57.110093 139939798828800 logging_writer.py:48] [9700] global_step=9700, grad_norm=6.3283915519714355, loss=5.43471622467041
I0429 15:21:12.313232 139939807221504 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6862087845802307, loss=5.424928188323975
I0429 15:22:27.494396 139939798828800 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7701398730278015, loss=5.432074546813965
I0429 15:23:42.768279 139939807221504 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8537577390670776, loss=5.4215497970581055
I0429 15:24:58.261622 139939798828800 logging_writer.py:48] [10100] global_step=10100, grad_norm=6.563364028930664, loss=5.425165176391602
I0429 15:26:13.482164 139939807221504 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5441315174102783, loss=5.423024654388428
I0429 15:27:32.287115 139939807221504 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5628790259361267, loss=5.395160675048828
I0429 15:28:47.480118 139939798828800 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4381608963012695, loss=5.434011459350586
I0429 15:30:02.705461 139939807221504 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9033288359642029, loss=5.400026798248291
I0429 15:31:17.873698 139939798828800 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.032126784324646, loss=5.394901275634766
I0429 15:32:33.010430 139939807221504 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8197773694992065, loss=5.4009246826171875
I0429 15:33:48.247926 139939798828800 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.2081820964813232, loss=5.428707599639893
I0429 15:35:03.545679 139939807221504 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.612610936164856, loss=5.403078079223633
I0429 15:36:18.985502 139939798828800 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.2867136001586914, loss=5.376216411590576
I0429 15:37:34.216502 139939807221504 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.020403504371643, loss=5.395091533660889
I0429 15:38:52.123863 139939798828800 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.8394795656204224, loss=5.377748966217041
I0429 15:40:10.360363 139939807221504 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.463482856750488, loss=5.405030727386475
I0429 15:41:30.466486 139939807221504 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9633434414863586, loss=5.369680881500244
I0429 15:42:45.581060 139939798828800 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9054731726646423, loss=5.382132053375244
I0429 15:44:00.713312 139939807221504 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.363824725151062, loss=5.3956522941589355
I0429 15:45:16.003550 139939798828800 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.068374514579773, loss=5.384702205657959
I0429 15:46:31.193132 139939807221504 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.2180912494659424, loss=5.36248254776001
I0429 15:47:46.557223 139939798828800 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8941073417663574, loss=5.361109733581543
I0429 15:49:01.716720 139939807221504 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.59633469581604, loss=5.380481243133545
I0429 15:50:16.909243 139939798828800 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.9993021488189697, loss=5.3597493171691895
I0429 15:51:32.041547 139939807221504 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6012970209121704, loss=5.3579182624816895
I0429 15:52:49.016405 139939798828800 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0502337217330933, loss=5.390366554260254
I0429 15:54:09.482540 139939807221504 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6064636707305908, loss=5.378422260284424
I0429 15:55:24.587309 139939798828800 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.0782474279403687, loss=5.335916042327881
I0429 15:56:34.886455 140112961300288 spec.py:298] Evaluating on the training split.
I0429 15:57:02.686792 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 15:57:38.402609 140112961300288 spec.py:326] Evaluating on the test split.
I0429 15:57:56.651769 140112961300288 submission_runner.py:415] Time since start: 10132.95s, 	Step: 12595, 	{'train/ctc_loss': DeviceArray(6.6180515, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(6.300693, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.2707157, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9681.698583364487, 'total_duration': 10132.950209379196, 'accumulated_submission_time': 9681.698583364487, 'accumulated_eval_time': 450.91771149635315, 'accumulated_logging_time': 0.14880800247192383}
I0429 15:57:56.674885 139940237301504 logging_writer.py:48] [12595] accumulated_eval_time=450.917711, accumulated_logging_time=0.148808, accumulated_submission_time=9681.698583, global_step=12595, preemption_count=0, score=9681.698583, test/ctc_loss=6.270715713500977, test/num_examples=2472, test/wer=0.899580, total_duration=10132.950209, train/ctc_loss=6.618051528930664, train/wer=0.943700, validation/ctc_loss=6.300693035125732, validation/num_examples=5348, validation/wer=0.895995
I0429 15:58:01.261994 139940228908800 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.007763624191284, loss=5.3689866065979
I0429 15:59:16.634102 139940237301504 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7350175380706787, loss=5.350306987762451
I0429 16:00:31.894151 139940228908800 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.6078565120697021, loss=5.368218898773193
I0429 16:01:46.985414 139940237301504 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.1756646633148193, loss=5.328611373901367
I0429 16:03:02.105929 139940228908800 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7871870398521423, loss=5.3752875328063965
I0429 16:04:17.212068 139940237301504 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.4590799808502197, loss=5.369609355926514
I0429 16:05:32.254680 139940228908800 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6564030647277832, loss=5.331938743591309
I0429 16:06:47.381560 139940237301504 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7250230312347412, loss=5.400796413421631
I0429 16:08:06.442936 139940237301504 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9170219302177429, loss=5.373763561248779
I0429 16:09:21.462337 139940228908800 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9068820476531982, loss=5.341822624206543
I0429 16:10:36.809726 139940237301504 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0868148803710938, loss=5.329049110412598
I0429 16:11:51.918771 139940228908800 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7389238476753235, loss=5.326929092407227
I0429 16:13:07.065467 139940237301504 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9289475083351135, loss=5.347065448760986
I0429 16:14:22.238802 139940228908800 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7856210470199585, loss=5.347164154052734
I0429 16:15:37.332009 139940237301504 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0184930562973022, loss=5.344224452972412
I0429 16:16:52.512823 139940228908800 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8059598207473755, loss=5.321779727935791
I0429 16:18:07.651405 139940237301504 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8544753789901733, loss=5.3232741355896
I0429 16:19:23.761650 139940228908800 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8578490018844604, loss=5.3161444664001465
I0429 16:20:42.528560 139940237301504 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6947932839393616, loss=5.318545818328857
I0429 16:22:01.899966 139940237301504 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6755985021591187, loss=5.263525009155273
I0429 16:23:17.078052 139940228908800 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.045186758041382, loss=5.28430700302124
I0429 16:24:32.192611 139940237301504 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7094564437866211, loss=5.283457279205322
I0429 16:25:47.297262 139940228908800 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.5204871892929077, loss=5.290542125701904
I0429 16:27:02.366135 139940237301504 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.6611883640289307, loss=5.286595821380615
I0429 16:28:17.447211 139940228908800 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.1564829349517822, loss=5.25668478012085
I0429 16:29:32.524775 139940237301504 logging_writer.py:48] [15100] global_step=15100, grad_norm=5.9039835929870605, loss=5.274171829223633
I0429 16:30:47.648006 139940228908800 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1250267028808594, loss=5.263241291046143
I0429 16:32:03.516191 139940237301504 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.7928541898727417, loss=5.271467685699463
I0429 16:33:21.649152 139940228908800 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.9698307514190674, loss=5.2510576248168945
I0429 16:34:42.900857 139939909621504 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.6240787506103516, loss=5.193544387817383
I0429 16:35:58.000239 139939901228800 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.997105360031128, loss=5.214498996734619
I0429 16:37:13.069215 139939909621504 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0766184329986572, loss=5.186681747436523
I0429 16:37:57.211447 140112961300288 spec.py:298] Evaluating on the training split.
I0429 16:38:25.565250 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 16:39:01.606211 140112961300288 spec.py:326] Evaluating on the test split.
I0429 16:39:20.119810 140112961300288 submission_runner.py:415] Time since start: 12616.42s, 	Step: 15760, 	{'train/ctc_loss': DeviceArray(5.5548, dtype=float32), 'train/wer': 0.9402791971993303, 'validation/ctc_loss': DeviceArray(5.4461484, dtype=float32), 'validation/wer': 0.8954162606489209, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.3874784, dtype=float32), 'test/wer': 0.8987467755367335, 'test/num_examples': 2472, 'score': 12082.176491260529, 'total_duration': 12616.418354988098, 'accumulated_submission_time': 12082.176491260529, 'accumulated_eval_time': 533.8228733539581, 'accumulated_logging_time': 0.18804693222045898}
I0429 16:39:20.143206 139939479541504 logging_writer.py:48] [15760] accumulated_eval_time=533.822873, accumulated_logging_time=0.188047, accumulated_submission_time=12082.176491, global_step=15760, preemption_count=0, score=12082.176491, test/ctc_loss=5.387478351593018, test/num_examples=2472, test/wer=0.898747, total_duration=12616.418355, train/ctc_loss=5.554800033569336, train/wer=0.940279, validation/ctc_loss=5.44614839553833, validation/num_examples=5348, validation/wer=0.895416
I0429 16:39:50.916664 139939471148800 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.2238993644714355, loss=5.151102542877197
I0429 16:41:05.990697 139939479541504 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.4842069149017334, loss=5.170234203338623
I0429 16:42:21.068823 139939471148800 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.8368741273880005, loss=5.105535507202148
I0429 16:43:36.125071 139939479541504 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.5537277460098267, loss=5.114091873168945
I0429 16:44:51.225523 139939471148800 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.095616340637207, loss=5.101879119873047
I0429 16:46:06.595804 139939479541504 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.1771067380905151, loss=5.033088207244873
I0429 16:47:22.690713 139939471148800 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.2911200523376465, loss=5.023423194885254
I0429 16:48:45.301474 139939479541504 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.5116195678710938, loss=5.006608963012695
I0429 16:50:00.360580 139939471148800 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1754471063613892, loss=4.96836519241333
I0429 16:51:15.515580 139939479541504 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.4486405849456787, loss=4.970415115356445
I0429 16:52:30.697992 139939471148800 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.3347564935684204, loss=4.929971218109131
I0429 16:53:45.804442 139939479541504 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.3088253736495972, loss=4.9369025230407715
I0429 16:55:00.903455 139939471148800 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3388491868972778, loss=4.853069305419922
I0429 16:56:16.050029 139939479541504 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.8895143270492554, loss=4.865617752075195
I0429 16:57:31.477850 139939471148800 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.2411847114562988, loss=4.836441516876221
I0429 16:58:51.385081 139939479541504 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.4783201217651367, loss=4.8091936111450195
I0429 17:00:11.813694 139939471148800 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.7033824920654297, loss=4.747971057891846
I0429 17:01:32.105794 139939479541504 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.503867268562317, loss=4.7488226890563965
I0429 17:02:51.095258 139938824181504 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.8043029308319092, loss=4.734791278839111
I0429 17:04:06.259922 139938815788800 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8387830257415771, loss=4.686448097229004
I0429 17:05:21.334854 139938824181504 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.1893415451049805, loss=4.636295795440674
I0429 17:06:36.473674 139938815788800 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.906544804573059, loss=4.61812162399292
I0429 17:07:51.524140 139938824181504 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.5154695510864258, loss=4.587329387664795
I0429 17:09:07.011659 139938815788800 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.5378652811050415, loss=4.633486747741699
I0429 17:10:26.899286 139938824181504 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.29480242729187, loss=4.515498638153076
I0429 17:11:47.824889 139938815788800 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.8736079931259155, loss=4.463296890258789
I0429 17:13:08.945287 139938824181504 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.7702343463897705, loss=4.4411516189575195
I0429 17:14:31.068287 139938815788800 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.5717803239822388, loss=4.3840155601501465
I0429 17:15:51.431165 139938824181504 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.3680551052093506, loss=4.31947135925293
I0429 17:17:06.633130 139938815788800 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.392865777015686, loss=4.239779949188232
I0429 17:18:21.787103 139938824181504 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.6975703239440918, loss=4.187934875488281
I0429 17:19:20.186840 140112961300288 spec.py:298] Evaluating on the training split.
I0429 17:19:53.225930 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 17:20:31.047572 140112961300288 spec.py:326] Evaluating on the test split.
I0429 17:20:50.504199 140112961300288 submission_runner.py:415] Time since start: 15106.80s, 	Step: 18879, 	{'train/ctc_loss': DeviceArray(3.9327354, dtype=float32), 'train/wer': 0.7977764855882338, 'validation/ctc_loss': DeviceArray(4.0797787, dtype=float32), 'validation/wer': 0.7776437785217416, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3.8621793, dtype=float32), 'test/wer': 0.760242114029208, 'test/num_examples': 2472, 'score': 14482.161615610123, 'total_duration': 15106.802252531052, 'accumulated_submission_time': 14482.161615610123, 'accumulated_eval_time': 624.1365456581116, 'accumulated_logging_time': 0.2270827293395996}
I0429 17:20:50.529979 139940237301504 logging_writer.py:48] [18879] accumulated_eval_time=624.136546, accumulated_logging_time=0.227083, accumulated_submission_time=14482.161616, global_step=18879, preemption_count=0, score=14482.161616, test/ctc_loss=3.8621792793273926, test/num_examples=2472, test/wer=0.760242, total_duration=15106.802253, train/ctc_loss=3.9327354431152344, train/wer=0.797776, validation/ctc_loss=4.079778671264648, validation/num_examples=5348, validation/wer=0.777644
I0429 17:21:07.106896 139940228908800 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.1795125007629395, loss=4.085800647735596
I0429 17:22:22.576010 139940237301504 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.387526512145996, loss=3.9794154167175293
I0429 17:23:37.742986 139940228908800 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5644010305404663, loss=3.953794240951538
I0429 17:24:52.935851 139940237301504 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.3937617540359497, loss=3.866703510284424
I0429 17:26:08.201631 139940228908800 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4481232166290283, loss=3.7265563011169434
I0429 17:27:23.457484 139940237301504 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.2983497381210327, loss=3.689012289047241
I0429 17:28:39.408860 139940228908800 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.4268532991409302, loss=3.6642889976501465
I0429 17:30:01.746263 139940237301504 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.3538494110107422, loss=3.5702860355377197
I0429 17:31:17.039739 139940228908800 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.632938265800476, loss=3.5930354595184326
I0429 17:32:32.340090 139940237301504 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.5347169637680054, loss=3.4894628524780273
I0429 17:33:47.918647 139940228908800 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.667141079902649, loss=3.4382972717285156
I0429 17:35:02.219899 140112961300288 spec.py:298] Evaluating on the training split.
I0429 17:35:39.765527 140112961300288 spec.py:310] Evaluating on the validation split.
I0429 17:36:18.837871 140112961300288 spec.py:326] Evaluating on the test split.
I0429 17:36:39.300621 140112961300288 submission_runner.py:415] Time since start: 16055.60s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(2.4854267, dtype=float32), 'train/wer': 0.5949418246017059, 'validation/ctc_loss': DeviceArray(2.7889202, dtype=float32), 'validation/wer': 0.6144680604733282, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.4881477, dtype=float32), 'test/wer': 0.5742489793431235, 'test/num_examples': 2472, 'score': 15333.81836938858, 'total_duration': 16055.599267721176, 'accumulated_submission_time': 15333.81836938858, 'accumulated_eval_time': 721.2141869068146, 'accumulated_logging_time': 0.2702369689941406}
I0429 17:36:39.325896 139940237301504 logging_writer.py:48] [20000] accumulated_eval_time=721.214187, accumulated_logging_time=0.270237, accumulated_submission_time=15333.818369, global_step=20000, preemption_count=0, score=15333.818369, test/ctc_loss=2.488147735595703, test/num_examples=2472, test/wer=0.574249, total_duration=16055.599268, train/ctc_loss=2.485426664352417, train/wer=0.594942, validation/ctc_loss=2.7889201641082764, validation/num_examples=5348, validation/wer=0.614468
I0429 17:36:39.351250 139940228908800 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15333.818369
I0429 17:36:39.671031 140112961300288 checkpoints.py:356] Saving checkpoint at step: 20000
I0429 17:36:41.205105 140112961300288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1/checkpoint_20000
I0429 17:36:41.236876 140112961300288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_lamb/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0429 17:36:42.594414 140112961300288 submission_runner.py:578] Tuning trial 1/1
I0429 17:36:42.594664 140112961300288 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 17:36:42.602196 140112961300288 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(30.676682, dtype=float32), 'train/wer': 1.1878891364554485, 'validation/ctc_loss': DeviceArray(29.399063, dtype=float32), 'validation/wer': 1.3489662225395325, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.454649, dtype=float32), 'test/wer': 1.3627445006398147, 'test/num_examples': 2472, 'score': 80.4832980632782, 'total_duration': 205.23145198822021, 'accumulated_submission_time': 80.4832980632782, 'accumulated_eval_time': 124.74799418449402, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3106, {'train/ctc_loss': DeviceArray(6.237166, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.3672566, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.2878737, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2480.870813369751, 'total_duration': 2687.5915632247925, 'accumulated_submission_time': 2480.870813369751, 'accumulated_eval_time': 206.64121508598328, 'accumulated_logging_time': 0.0369570255279541, 'global_step': 3106, 'preemption_count': 0}), (6263, {'train/ctc_loss': DeviceArray(5.4997864, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(5.4405603, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.410326, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4881.062111616135, 'total_duration': 5168.749376773834, 'accumulated_submission_time': 4881.062111616135, 'accumulated_eval_time': 287.52284574508667, 'accumulated_logging_time': 0.07428860664367676, 'global_step': 6263, 'preemption_count': 0}), (9432, {'train/ctc_loss': DeviceArray(7.637304, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(7.107837, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(7.181365, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7281.658621788025, 'total_duration': 7651.064416408539, 'accumulated_submission_time': 7281.658621788025, 'accumulated_eval_time': 369.1556887626648, 'accumulated_logging_time': 0.11151313781738281, 'global_step': 9432, 'preemption_count': 0}), (12595, {'train/ctc_loss': DeviceArray(6.6180515, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(6.300693, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.2707157, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9681.698583364487, 'total_duration': 10132.950209379196, 'accumulated_submission_time': 9681.698583364487, 'accumulated_eval_time': 450.91771149635315, 'accumulated_logging_time': 0.14880800247192383, 'global_step': 12595, 'preemption_count': 0}), (15760, {'train/ctc_loss': DeviceArray(5.5548, dtype=float32), 'train/wer': 0.9402791971993303, 'validation/ctc_loss': DeviceArray(5.4461484, dtype=float32), 'validation/wer': 0.8954162606489209, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.3874784, dtype=float32), 'test/wer': 0.8987467755367335, 'test/num_examples': 2472, 'score': 12082.176491260529, 'total_duration': 12616.418354988098, 'accumulated_submission_time': 12082.176491260529, 'accumulated_eval_time': 533.8228733539581, 'accumulated_logging_time': 0.18804693222045898, 'global_step': 15760, 'preemption_count': 0}), (18879, {'train/ctc_loss': DeviceArray(3.9327354, dtype=float32), 'train/wer': 0.7977764855882338, 'validation/ctc_loss': DeviceArray(4.0797787, dtype=float32), 'validation/wer': 0.7776437785217416, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3.8621793, dtype=float32), 'test/wer': 0.760242114029208, 'test/num_examples': 2472, 'score': 14482.161615610123, 'total_duration': 15106.802252531052, 'accumulated_submission_time': 14482.161615610123, 'accumulated_eval_time': 624.1365456581116, 'accumulated_logging_time': 0.2270827293395996, 'global_step': 18879, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(2.4854267, dtype=float32), 'train/wer': 0.5949418246017059, 'validation/ctc_loss': DeviceArray(2.7889202, dtype=float32), 'validation/wer': 0.6144680604733282, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.4881477, dtype=float32), 'test/wer': 0.5742489793431235, 'test/num_examples': 2472, 'score': 15333.81836938858, 'total_duration': 16055.599267721176, 'accumulated_submission_time': 15333.81836938858, 'accumulated_eval_time': 721.2141869068146, 'accumulated_logging_time': 0.2702369689941406, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0429 17:36:42.602375 140112961300288 submission_runner.py:581] Timing: 15333.81836938858
I0429 17:36:42.602438 140112961300288 submission_runner.py:582] ====================
I0429 17:36:42.603114 140112961300288 submission_runner.py:645] Final librispeech_conformer score: 15333.81836938858
