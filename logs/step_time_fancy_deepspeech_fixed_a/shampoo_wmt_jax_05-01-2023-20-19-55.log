python3 submission_runner.py --framework=jax --workload=wmt --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_shampoo --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_jax_05-01-2023-20-19-55.log
I0501 20:20:18.282370 140436552230720 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_shampoo/wmt_jax.
I0501 20:20:18.387550 140436552230720 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 20:20:19.222381 140436552230720 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0501 20:20:19.223113 140436552230720 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 20:20:19.228289 140436552230720 submission_runner.py:538] Using RNG seed 864443694
I0501 20:20:21.876493 140436552230720 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 20:20:21.876718 140436552230720 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1.
I0501 20:20:21.876903 140436552230720 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1/hparams.json.
I0501 20:20:22.010231 140436552230720 submission_runner.py:241] Initializing dataset.
I0501 20:20:22.021231 140436552230720 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0501 20:20:22.024291 140436552230720 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 20:20:22.024409 140436552230720 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 20:20:22.146251 140436552230720 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0501 20:20:24.394197 140436552230720 submission_runner.py:248] Initializing model.
I0501 20:20:37.981703 140436552230720 submission_runner.py:258] Initializing optimizer.
I0501 20:20:49.041820 140436552230720 submission_runner.py:265] Initializing metrics bundle.
I0501 20:20:49.042051 140436552230720 submission_runner.py:282] Initializing checkpoint and logger.
I0501 20:20:49.043128 140436552230720 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1 with prefix checkpoint_
I0501 20:20:49.043429 140436552230720 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0501 20:20:49.043495 140436552230720 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0501 20:20:49.838286 140436552230720 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1/meta_data_0.json.
I0501 20:20:49.839242 140436552230720 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1/flags_0.json.
I0501 20:20:49.843493 140436552230720 submission_runner.py:318] Starting training loop.
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:812: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:813: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:814: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
I0501 20:24:43.645486 140260143003392 logging_writer.py:48] [0] global_step=0, grad_norm=5.656487464904785, loss=11.090275764465332
I0501 20:24:43.730779 140436552230720 spec.py:298] Evaluating on the training split.
I0501 20:24:43.736881 140436552230720 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0501 20:24:43.740330 140436552230720 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 20:24:43.740454 140436552230720 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 20:24:43.775344 140436552230720 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0501 20:24:52.747815 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 20:29:52.897685 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 20:29:52.901077 140436552230720 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0501 20:29:52.904096 140436552230720 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 20:29:52.904203 140436552230720 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 20:29:52.936597 140436552230720 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0501 20:30:01.100260 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 20:34:54.386281 140436552230720 spec.py:326] Evaluating on the test split.
I0501 20:34:54.389182 140436552230720 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0501 20:34:54.392272 140436552230720 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 20:34:54.392376 140436552230720 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 20:34:54.422679 140436552230720 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0501 20:35:02.072174 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 20:39:47.186654 140436552230720 submission_runner.py:415] Time since start: 1137.34s, 	Step: 1, 	{'train/accuracy': 0.0005380898946896195, 'train/loss': 11.089571952819824, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.082098960876465, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.077061653137207, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 233.88706994056702, 'total_duration': 1137.3430449962616, 'accumulated_submission_time': 233.88706994056702, 'accumulated_eval_time': 903.4557967185974, 'accumulated_logging_time': 0}
I0501 20:39:47.206177 140248741742336 logging_writer.py:48] [1] accumulated_eval_time=903.455797, accumulated_logging_time=0, accumulated_submission_time=233.887070, global_step=1, preemption_count=0, score=233.887070, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.077062, test/num_examples=3003, total_duration=1137.343045, train/accuracy=0.000538, train/bleu=0.000000, train/loss=11.089572, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.082099, validation/num_examples=3000
I0501 20:41:06.885232 140248750135040 logging_writer.py:48] [100] global_step=100, grad_norm=0.5946314334869385, loss=7.949650764465332
I0501 20:42:26.445664 140248741742336 logging_writer.py:48] [200] global_step=200, grad_norm=0.7947147488594055, loss=7.26105260848999
I0501 20:43:46.508095 140248750135040 logging_writer.py:48] [300] global_step=300, grad_norm=0.8904976844787598, loss=6.675222873687744
I0501 20:45:07.523952 140248741742336 logging_writer.py:48] [400] global_step=400, grad_norm=0.5314610004425049, loss=6.278421401977539
I0501 20:46:28.710473 140248750135040 logging_writer.py:48] [500] global_step=500, grad_norm=0.7372065186500549, loss=5.893978595733643
I0501 20:47:50.708552 140248741742336 logging_writer.py:48] [600] global_step=600, grad_norm=1.0937665700912476, loss=5.5517449378967285
I0501 20:49:12.709151 140248750135040 logging_writer.py:48] [700] global_step=700, grad_norm=0.9201748967170715, loss=5.3949360847473145
I0501 20:50:34.684415 140248741742336 logging_writer.py:48] [800] global_step=800, grad_norm=0.9881066083908081, loss=5.148166656494141
I0501 20:51:57.517365 140248750135040 logging_writer.py:48] [900] global_step=900, grad_norm=0.7961152791976929, loss=4.974158763885498
I0501 20:53:20.010333 140248741742336 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7570740580558777, loss=4.889341831207275
I0501 20:53:47.523762 140436552230720 spec.py:298] Evaluating on the training split.
I0501 20:53:50.384281 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 20:57:37.361509 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 20:57:40.060339 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:00:55.431396 140436552230720 spec.py:326] Evaluating on the test split.
I0501 21:00:58.180029 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:04:25.517156 140436552230720 submission_runner.py:415] Time since start: 2615.67s, 	Step: 1037, 	{'train/accuracy': 0.4286072850227356, 'train/loss': 3.877760410308838, 'train/bleu': 15.332419828227241, 'validation/accuracy': 0.41554349660873413, 'validation/loss': 3.9914638996124268, 'validation/bleu': 10.340105662885252, 'validation/num_examples': 3000, 'test/accuracy': 0.4012666344642639, 'test/loss': 4.168376445770264, 'test/bleu': 8.787092960464516, 'test/num_examples': 3003, 'score': 1074.1816589832306, 'total_duration': 2615.6735429763794, 'accumulated_submission_time': 1074.1816589832306, 'accumulated_eval_time': 1541.449119091034, 'accumulated_logging_time': 0.029551267623901367}
I0501 21:04:25.527182 140248750135040 logging_writer.py:48] [1037] accumulated_eval_time=1541.449119, accumulated_logging_time=0.029551, accumulated_submission_time=1074.181659, global_step=1037, preemption_count=0, score=1074.181659, test/accuracy=0.401267, test/bleu=8.787093, test/loss=4.168376, test/num_examples=3003, total_duration=2615.673543, train/accuracy=0.428607, train/bleu=15.332420, train/loss=3.877760, validation/accuracy=0.415543, validation/bleu=10.340106, validation/loss=3.991464, validation/num_examples=3000
I0501 21:05:21.763028 140248741742336 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7561678886413574, loss=4.660907745361328
I0501 21:06:44.262130 140248750135040 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6508409380912781, loss=4.482088088989258
I0501 21:08:06.699342 140248741742336 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8193346261978149, loss=4.464588165283203
I0501 21:09:29.188691 140248750135040 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7566961050033569, loss=4.405999660491943
I0501 21:10:51.680625 140248741742336 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6667650938034058, loss=4.283012866973877
I0501 21:12:14.545764 140248750135040 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7253258228302002, loss=4.197757244110107
I0501 21:13:37.044483 140248741742336 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.694277286529541, loss=4.141340255737305
I0501 21:14:59.606355 140248750135040 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8252307176589966, loss=4.01829719543457
I0501 21:16:22.260743 140248741742336 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6714679598808289, loss=4.0272016525268555
I0501 21:17:45.069743 140248750135040 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6781554818153381, loss=3.985037326812744
I0501 21:18:25.903765 140436552230720 spec.py:298] Evaluating on the training split.
I0501 21:18:28.744829 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:21:44.384156 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 21:21:47.078257 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:24:42.643725 140436552230720 spec.py:326] Evaluating on the test split.
I0501 21:24:45.402683 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:27:22.153877 140436552230720 submission_runner.py:415] Time since start: 3992.31s, 	Step: 2053, 	{'train/accuracy': 0.5220142602920532, 'train/loss': 2.9639554023742676, 'train/bleu': 22.598551297326058, 'validation/accuracy': 0.5216178297996521, 'validation/loss': 2.963824987411499, 'validation/bleu': 18.935765327579553, 'validation/num_examples': 3000, 'test/accuracy': 0.520562469959259, 'test/loss': 3.0248167514801025, 'test/bleu': 17.413539091918206, 'test/num_examples': 3003, 'score': 1914.5362091064453, 'total_duration': 3992.310295343399, 'accumulated_submission_time': 1914.5362091064453, 'accumulated_eval_time': 2077.699175596237, 'accumulated_logging_time': 0.04913663864135742}
I0501 21:27:22.163773 140248741742336 logging_writer.py:48] [2053] accumulated_eval_time=2077.699176, accumulated_logging_time=0.049137, accumulated_submission_time=1914.536209, global_step=2053, preemption_count=0, score=1914.536209, test/accuracy=0.520562, test/bleu=17.413539, test/loss=3.024817, test/num_examples=3003, total_duration=3992.310295, train/accuracy=0.522014, train/bleu=22.598551, train/loss=2.963955, validation/accuracy=0.521618, validation/bleu=18.935765, validation/loss=2.963825, validation/num_examples=3000
I0501 21:28:04.260016 140248750135040 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6922791600227356, loss=3.8438661098480225
I0501 21:29:26.495971 140248741742336 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5712568163871765, loss=3.8945446014404297
I0501 21:30:48.874379 140248750135040 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5904141068458557, loss=3.8539328575134277
I0501 21:32:11.414764 140248741742336 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5437206029891968, loss=3.887098789215088
I0501 21:33:33.757978 140248750135040 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6829515099525452, loss=3.940427303314209
I0501 21:34:56.261148 140248741742336 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6187863945960999, loss=3.750321865081787
I0501 21:36:18.508710 140248750135040 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5569173097610474, loss=3.7150516510009766
I0501 21:37:40.902440 140248741742336 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6093986630439758, loss=3.735011100769043
I0501 21:39:02.968893 140248750135040 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.49084770679473877, loss=3.7449557781219482
I0501 21:40:25.670050 140248741742336 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.576183021068573, loss=3.761408805847168
I0501 21:41:22.708992 140436552230720 spec.py:298] Evaluating on the training split.
I0501 21:41:25.548616 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:44:16.335275 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 21:44:19.031649 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:46:45.303832 140436552230720 spec.py:326] Evaluating on the test split.
I0501 21:46:48.046976 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 21:49:17.185331 140436552230720 submission_runner.py:415] Time since start: 5307.34s, 	Step: 3073, 	{'train/accuracy': 0.5486227869987488, 'train/loss': 2.6782023906707764, 'train/bleu': 25.19036209894087, 'validation/accuracy': 0.5519336462020874, 'validation/loss': 2.6405537128448486, 'validation/bleu': 21.15530481817793, 'validation/num_examples': 3000, 'test/accuracy': 0.5547731518745422, 'test/loss': 2.656911849975586, 'test/bleu': 19.868803720207474, 'test/num_examples': 3003, 'score': 2755.0597808361053, 'total_duration': 5307.341752290726, 'accumulated_submission_time': 2755.0597808361053, 'accumulated_eval_time': 2552.1754615306854, 'accumulated_logging_time': 0.06812906265258789}
I0501 21:49:17.195445 140248750135040 logging_writer.py:48] [3073] accumulated_eval_time=2552.175462, accumulated_logging_time=0.068129, accumulated_submission_time=2755.059781, global_step=3073, preemption_count=0, score=2755.059781, test/accuracy=0.554773, test/bleu=19.868804, test/loss=2.656912, test/num_examples=3003, total_duration=5307.341752, train/accuracy=0.548623, train/bleu=25.190362, train/loss=2.678202, validation/accuracy=0.551934, validation/bleu=21.155305, validation/loss=2.640554, validation/num_examples=3000
I0501 21:49:42.610415 140248741742336 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.501554548740387, loss=3.776456594467163
I0501 21:51:04.587275 140248750135040 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5327154397964478, loss=3.6319191455841064
I0501 21:52:27.017480 140248741742336 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6278525590896606, loss=3.679453134536743
I0501 21:53:49.297088 140248750135040 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.535496711730957, loss=3.676342487335205
I0501 21:55:11.403271 140248741742336 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.556052029132843, loss=3.564011812210083
I0501 21:56:33.837064 140248750135040 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5423636436462402, loss=3.6773931980133057
I0501 21:57:55.791757 140248741742336 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4230983853340149, loss=3.668379306793213
I0501 21:59:18.649290 140248750135040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.4444063901901245, loss=3.6627891063690186
I0501 22:00:40.997596 140248741742336 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.570786714553833, loss=3.6206858158111572
I0501 22:02:03.435759 140248750135040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3948173522949219, loss=3.649623394012451
I0501 22:03:17.784703 140436552230720 spec.py:298] Evaluating on the training split.
I0501 22:03:20.614679 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:05:56.898820 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 22:05:59.592462 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:08:23.345839 140436552230720 spec.py:326] Evaluating on the test split.
I0501 22:08:26.099339 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:10:48.618736 140436552230720 submission_runner.py:415] Time since start: 6598.78s, 	Step: 4094, 	{'train/accuracy': 0.5580021739006042, 'train/loss': 2.549273729324341, 'train/bleu': 24.162500821741105, 'validation/accuracy': 0.5605633854866028, 'validation/loss': 2.4987900257110596, 'validation/bleu': 20.8212681038752, 'validation/num_examples': 3000, 'test/accuracy': 0.5628609657287598, 'test/loss': 2.530064821243286, 'test/bleu': 19.556879281092037, 'test/num_examples': 3003, 'score': 3595.6270706653595, 'total_duration': 6598.7751541137695, 'accumulated_submission_time': 3595.6270706653595, 'accumulated_eval_time': 3003.009439229965, 'accumulated_logging_time': 0.08755159378051758}
I0501 22:10:48.628763 140248741742336 logging_writer.py:48] [4094] accumulated_eval_time=3003.009439, accumulated_logging_time=0.087552, accumulated_submission_time=3595.627071, global_step=4094, preemption_count=0, score=3595.627071, test/accuracy=0.562861, test/bleu=19.556879, test/loss=2.530065, test/num_examples=3003, total_duration=6598.775154, train/accuracy=0.558002, train/bleu=24.162501, train/loss=2.549274, validation/accuracy=0.560563, validation/bleu=20.821268, validation/loss=2.498790, validation/num_examples=3000
I0501 22:10:57.052355 140248750135040 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.41971492767333984, loss=3.6035635471343994
I0501 22:12:19.626153 140248741742336 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4080806374549866, loss=3.663060188293457
I0501 22:13:42.255404 140248750135040 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5956413745880127, loss=3.571275234222412
I0501 22:15:05.428102 140248741742336 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3347955644130707, loss=3.5575568675994873
I0501 22:16:28.714827 140248750135040 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3413652181625366, loss=3.4617488384246826
I0501 22:17:51.657423 140248741742336 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3287261128425598, loss=3.5177690982818604
I0501 22:19:14.881077 140248750135040 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3222678005695343, loss=3.4533002376556396
I0501 22:20:39.681267 140248741742336 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.322318971157074, loss=3.497755765914917
I0501 22:22:02.197135 140248750135040 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.31782057881355286, loss=3.4269323348999023
I0501 22:23:24.746960 140248741742336 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.25442808866500854, loss=3.427081346511841
I0501 22:24:47.099077 140248750135040 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.27697667479515076, loss=3.4792888164520264
I0501 22:24:48.989015 140436552230720 spec.py:298] Evaluating on the training split.
I0501 22:24:51.824118 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:27:21.998522 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 22:27:24.684961 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:29:48.413708 140436552230720 spec.py:326] Evaluating on the test split.
I0501 22:29:51.185019 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:32:08.988960 140436552230720 submission_runner.py:415] Time since start: 7879.15s, 	Step: 5104, 	{'train/accuracy': 0.5718808770179749, 'train/loss': 2.421102523803711, 'train/bleu': 26.805479974118015, 'validation/accuracy': 0.5828322172164917, 'validation/loss': 2.3273799419403076, 'validation/bleu': 23.17840162777327, 'validation/num_examples': 3000, 'test/accuracy': 0.585300087928772, 'test/loss': 2.3284077644348145, 'test/bleu': 22.035251867653237, 'test/num_examples': 3003, 'score': 4435.9636306762695, 'total_duration': 7879.145348072052, 'accumulated_submission_time': 4435.9636306762695, 'accumulated_eval_time': 3443.009294986725, 'accumulated_logging_time': 0.10796952247619629}
I0501 22:32:09.000878 140248741742336 logging_writer.py:48] [5104] accumulated_eval_time=3443.009295, accumulated_logging_time=0.107970, accumulated_submission_time=4435.963631, global_step=5104, preemption_count=0, score=4435.963631, test/accuracy=0.585300, test/bleu=22.035252, test/loss=2.328408, test/num_examples=3003, total_duration=7879.145348, train/accuracy=0.571881, train/bleu=26.805480, train/loss=2.421103, validation/accuracy=0.582832, validation/bleu=23.178402, validation/loss=2.327380, validation/num_examples=3000
I0501 22:33:31.713961 140248750135040 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.2654317319393158, loss=3.454263687133789
I0501 22:34:54.681114 140248741742336 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.23796667158603668, loss=3.3949503898620605
I0501 22:36:17.006739 140248750135040 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.22475910186767578, loss=3.4237818717956543
I0501 22:37:39.290383 140248741742336 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.22819361090660095, loss=3.465481758117676
I0501 22:39:01.710912 140248750135040 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.22394533455371857, loss=3.437263011932373
I0501 22:40:24.054853 140248741742336 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.23895026743412018, loss=3.4121625423431396
I0501 22:41:46.638117 140248750135040 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.22425910830497742, loss=3.4548239707946777
I0501 22:43:08.671270 140248741742336 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.21116241812705994, loss=3.325524091720581
I0501 22:44:30.909738 140248750135040 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.20558425784111023, loss=3.3429248332977295
I0501 22:45:53.311532 140248741742336 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.19534313678741455, loss=3.3172028064727783
I0501 22:46:09.720758 140436552230720 spec.py:298] Evaluating on the training split.
I0501 22:46:12.558793 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:48:54.387967 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 22:48:57.081112 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:51:22.253101 140436552230720 spec.py:326] Evaluating on the test split.
I0501 22:51:25.005648 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 22:53:47.931131 140436552230720 submission_runner.py:415] Time since start: 9178.09s, 	Step: 6121, 	{'train/accuracy': 0.5826478004455566, 'train/loss': 2.327449321746826, 'train/bleu': 27.730322997937115, 'validation/accuracy': 0.5972647666931152, 'validation/loss': 2.206331491470337, 'validation/bleu': 24.365539973130133, 'validation/num_examples': 3000, 'test/accuracy': 0.6010226011276245, 'test/loss': 2.189636468887329, 'test/bleu': 23.402790312153087, 'test/num_examples': 3003, 'score': 5276.660764455795, 'total_duration': 9178.087550878525, 'accumulated_submission_time': 5276.660764455795, 'accumulated_eval_time': 3901.219612121582, 'accumulated_logging_time': 0.12955284118652344}
I0501 22:53:47.941505 140248750135040 logging_writer.py:48] [6121] accumulated_eval_time=3901.219612, accumulated_logging_time=0.129553, accumulated_submission_time=5276.660764, global_step=6121, preemption_count=0, score=5276.660764, test/accuracy=0.601023, test/bleu=23.402790, test/loss=2.189636, test/num_examples=3003, total_duration=9178.087551, train/accuracy=0.582648, train/bleu=27.730323, train/loss=2.327449, validation/accuracy=0.597265, validation/bleu=24.365540, validation/loss=2.206331, validation/num_examples=3000
I0501 22:54:53.525888 140248741742336 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.20551635324954987, loss=3.35918927192688
I0501 22:56:15.867236 140248750135040 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.19871489703655243, loss=3.3636698722839355
I0501 22:57:38.223698 140248741742336 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.23789666593074799, loss=3.435680866241455
I0501 22:59:00.570351 140248750135040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.18258622288703918, loss=3.368802547454834
I0501 23:00:22.597192 140248741742336 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.18833701312541962, loss=3.386380672454834
I0501 23:01:44.749077 140248750135040 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1677718460559845, loss=3.3868448734283447
I0501 23:03:07.040973 140248741742336 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.16892613470554352, loss=3.3764684200286865
I0501 23:04:29.596575 140248750135040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.1726948767900467, loss=3.293128728866577
I0501 23:05:52.112418 140248741742336 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.1961478292942047, loss=3.337726593017578
I0501 23:07:14.185308 140248750135040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.15835776925086975, loss=3.441464900970459
I0501 23:07:48.450818 140436552230720 spec.py:298] Evaluating on the training split.
I0501 23:07:51.302862 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:10:38.714151 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 23:10:41.402589 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:13:02.097731 140436552230720 spec.py:326] Evaluating on the test split.
I0501 23:13:04.847538 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:15:25.350418 140436552230720 submission_runner.py:415] Time since start: 10475.51s, 	Step: 7143, 	{'train/accuracy': 0.6033188700675964, 'train/loss': 2.1551942825317383, 'train/bleu': 28.678011788561815, 'validation/accuracy': 0.6109781861305237, 'validation/loss': 2.0925533771514893, 'validation/bleu': 24.913357089278065, 'validation/num_examples': 3000, 'test/accuracy': 0.6146069765090942, 'test/loss': 2.0688061714172363, 'test/bleu': 23.766795831856736, 'test/num_examples': 3003, 'score': 6117.148396492004, 'total_duration': 10475.506830453873, 'accumulated_submission_time': 6117.148396492004, 'accumulated_eval_time': 4358.119149446487, 'accumulated_logging_time': 0.14905619621276855}
I0501 23:15:25.360901 140248741742336 logging_writer.py:48] [7143] accumulated_eval_time=4358.119149, accumulated_logging_time=0.149056, accumulated_submission_time=6117.148396, global_step=7143, preemption_count=0, score=6117.148396, test/accuracy=0.614607, test/bleu=23.766796, test/loss=2.068806, test/num_examples=3003, total_duration=10475.506830, train/accuracy=0.603319, train/bleu=28.678012, train/loss=2.155194, validation/accuracy=0.610978, validation/bleu=24.913357, validation/loss=2.092553, validation/num_examples=3000
I0501 23:16:13.226934 140248750135040 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.1543690711259842, loss=3.290011405944824
I0501 23:17:35.784985 140248741742336 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1773989200592041, loss=3.242807388305664
I0501 23:18:57.998265 140248750135040 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.14490164816379547, loss=3.3069188594818115
I0501 23:20:20.290567 140248741742336 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.22327043116092682, loss=3.257817268371582
I0501 23:21:42.504814 140248750135040 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.14241816103458405, loss=3.2222700119018555
I0501 23:23:04.541898 140248741742336 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.17122550308704376, loss=3.2112648487091064
I0501 23:24:26.805303 140248750135040 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1332012116909027, loss=3.2614972591400146
I0501 23:25:48.992202 140248741742336 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.1545722335577011, loss=3.253591775894165
I0501 23:27:11.445149 140248750135040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.14879797399044037, loss=3.2425355911254883
I0501 23:28:33.431612 140248741742336 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.14707957208156586, loss=3.1769845485687256
I0501 23:29:25.532580 140436552230720 spec.py:298] Evaluating on the training split.
I0501 23:29:28.350839 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:32:00.502961 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 23:32:03.201927 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:34:25.936101 140436552230720 spec.py:326] Evaluating on the test split.
I0501 23:34:28.685428 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:36:47.197216 140436552230720 submission_runner.py:415] Time since start: 11757.35s, 	Step: 8165, 	{'train/accuracy': 0.6079455018043518, 'train/loss': 2.1137988567352295, 'train/bleu': 29.102572485043527, 'validation/accuracy': 0.6220629811286926, 'validation/loss': 2.0048046112060547, 'validation/bleu': 25.608917026473012, 'validation/num_examples': 3000, 'test/accuracy': 0.6288071870803833, 'test/loss': 1.9664326906204224, 'test/bleu': 24.880094802498775, 'test/num_examples': 3003, 'score': 6957.298423290253, 'total_duration': 11757.353631019592, 'accumulated_submission_time': 6957.298423290253, 'accumulated_eval_time': 4799.783720493317, 'accumulated_logging_time': 0.16863179206848145}
I0501 23:36:47.207958 140248750135040 logging_writer.py:48] [8165] accumulated_eval_time=4799.783720, accumulated_logging_time=0.168632, accumulated_submission_time=6957.298423, global_step=8165, preemption_count=0, score=6957.298423, test/accuracy=0.628807, test/bleu=24.880095, test/loss=1.966433, test/num_examples=3003, total_duration=11757.353631, train/accuracy=0.607946, train/bleu=29.102572, train/loss=2.113799, validation/accuracy=0.622063, validation/bleu=25.608917, validation/loss=2.004805, validation/num_examples=3000
I0501 23:37:17.327693 140248741742336 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1496521681547165, loss=3.2169909477233887
I0501 23:38:39.372287 140248750135040 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.1700688749551773, loss=3.1490318775177
I0501 23:40:01.210709 140248741742336 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.17415927350521088, loss=3.2420239448547363
I0501 23:41:23.608248 140248750135040 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.12556268274784088, loss=3.244872570037842
I0501 23:42:45.435334 140248741742336 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.13592633605003357, loss=3.2378642559051514
I0501 23:44:07.346888 140248750135040 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.13850301504135132, loss=3.1996889114379883
I0501 23:45:29.664592 140248741742336 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1349620819091797, loss=3.2172229290008545
I0501 23:46:51.652958 140248750135040 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.12304309010505676, loss=3.2032124996185303
I0501 23:48:13.866708 140248741742336 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.13178235292434692, loss=3.246030569076538
I0501 23:49:36.176047 140248750135040 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.1369532197713852, loss=3.2274961471557617
I0501 23:50:47.791564 140436552230720 spec.py:298] Evaluating on the training split.
I0501 23:50:50.617247 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:53:55.325569 140436552230720 spec.py:310] Evaluating on the validation split.
I0501 23:53:58.022886 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:56:16.113585 140436552230720 spec.py:326] Evaluating on the test split.
I0501 23:56:18.857136 140436552230720 workload.py:179] Translating evaluation dataset.
I0501 23:58:27.657845 140436552230720 submission_runner.py:415] Time since start: 13057.81s, 	Step: 9190, 	{'train/accuracy': 0.613373875617981, 'train/loss': 2.0594215393066406, 'train/bleu': 30.181708885599242, 'validation/accuracy': 0.6311390995979309, 'validation/loss': 1.9345043897628784, 'validation/bleu': 26.33774681595562, 'validation/num_examples': 3000, 'test/accuracy': 0.6353495121002197, 'test/loss': 1.8932571411132812, 'test/bleu': 25.243706467131346, 'test/num_examples': 3003, 'score': 7797.860035657883, 'total_duration': 13057.814243078232, 'accumulated_submission_time': 7797.860035657883, 'accumulated_eval_time': 5259.649926662445, 'accumulated_logging_time': 0.18874382972717285}
I0501 23:58:27.669868 140248741742336 logging_writer.py:48] [9190] accumulated_eval_time=5259.649927, accumulated_logging_time=0.188744, accumulated_submission_time=7797.860036, global_step=9190, preemption_count=0, score=7797.860036, test/accuracy=0.635350, test/bleu=25.243706, test/loss=1.893257, test/num_examples=3003, total_duration=13057.814243, train/accuracy=0.613374, train/bleu=30.181709, train/loss=2.059422, validation/accuracy=0.631139, validation/bleu=26.337747, validation/loss=1.934504, validation/num_examples=3000
I0501 23:58:38.346618 140248750135040 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.13472583889961243, loss=3.1627562046051025
I0502 00:00:00.580686 140248741742336 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.126482754945755, loss=3.1286795139312744
I0502 00:01:22.317971 140248750135040 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.11499755084514618, loss=3.1106386184692383
I0502 00:02:44.003345 140248741742336 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.13090112805366516, loss=3.184844493865967
I0502 00:04:07.964394 140248750135040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.11808568984270096, loss=3.148777723312378
I0502 00:05:32.686295 140248741742336 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1677839457988739, loss=3.129666566848755
I0502 00:06:56.746032 140248750135040 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1278807669878006, loss=3.1244399547576904
I0502 00:08:18.611670 140248741742336 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.15547789633274078, loss=3.10204815864563
I0502 00:09:40.423127 140248750135040 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.12017825245857239, loss=3.125194787979126
I0502 00:11:01.840184 140248741742336 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.15471309423446655, loss=3.146672010421753
I0502 00:12:24.157195 140248750135040 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.16700440645217896, loss=3.1279425621032715
I0502 00:12:27.900512 140436552230720 spec.py:298] Evaluating on the training split.
I0502 00:12:30.716422 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:15:09.214328 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 00:15:11.914587 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:17:29.761818 140436552230720 spec.py:326] Evaluating on the test split.
I0502 00:17:32.523322 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:19:42.491513 140436552230720 submission_runner.py:415] Time since start: 14332.65s, 	Step: 10207, 	{'train/accuracy': 0.6204060316085815, 'train/loss': 2.0129036903381348, 'train/bleu': 30.050341042619326, 'validation/accuracy': 0.63567715883255, 'validation/loss': 1.8904883861541748, 'validation/bleu': 26.70517044330013, 'validation/num_examples': 3000, 'test/accuracy': 0.6432514190673828, 'test/loss': 1.8357809782028198, 'test/bleu': 25.772184592359746, 'test/num_examples': 3003, 'score': 8638.06702876091, 'total_duration': 14332.647903442383, 'accumulated_submission_time': 8638.06702876091, 'accumulated_eval_time': 5694.240840673447, 'accumulated_logging_time': 0.2105567455291748}
I0502 00:19:42.504415 140248741742336 logging_writer.py:48] [10207] accumulated_eval_time=5694.240841, accumulated_logging_time=0.210557, accumulated_submission_time=8638.067029, global_step=10207, preemption_count=0, score=8638.067029, test/accuracy=0.643251, test/bleu=25.772185, test/loss=1.835781, test/num_examples=3003, total_duration=14332.647903, train/accuracy=0.620406, train/bleu=30.050341, train/loss=2.012904, validation/accuracy=0.635677, validation/bleu=26.705170, validation/loss=1.890488, validation/num_examples=3000
I0502 00:21:02.774643 140248750135040 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.14279019832611084, loss=3.1729276180267334
I0502 00:22:24.465650 140248741742336 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.11602079123258591, loss=3.1607065200805664
I0502 00:23:46.171481 140248750135040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.10974255949258804, loss=3.2441446781158447
I0502 00:25:07.953544 140248741742336 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.11244896799325943, loss=3.1955862045288086
I0502 00:26:29.430792 140248750135040 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.11692829430103302, loss=3.1136505603790283
I0502 00:27:51.716161 140248741742336 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1328737884759903, loss=3.060354471206665
I0502 00:29:13.574390 140248750135040 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.12241259217262268, loss=3.1090614795684814
I0502 00:30:35.686872 140248741742336 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.12812189757823944, loss=3.172006368637085
I0502 00:31:57.461489 140248750135040 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.12156201899051666, loss=3.0428473949432373
I0502 00:33:19.471354 140248741742336 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.12425398081541061, loss=3.096951961517334
I0502 00:33:43.130958 140436552230720 spec.py:298] Evaluating on the training split.
I0502 00:33:45.964775 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:36:33.398983 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 00:36:36.090338 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:38:54.526063 140436552230720 spec.py:326] Evaluating on the test split.
I0502 00:38:57.283376 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:41:11.150045 140436552230720 submission_runner.py:415] Time since start: 15621.31s, 	Step: 11232, 	{'train/accuracy': 0.6238304972648621, 'train/loss': 1.9838541746139526, 'train/bleu': 30.377893559880768, 'validation/accuracy': 0.64218670129776, 'validation/loss': 1.8433747291564941, 'validation/bleu': 27.25695483547751, 'validation/num_examples': 3000, 'test/accuracy': 0.6502237319946289, 'test/loss': 1.793707251548767, 'test/bleu': 26.31296724638282, 'test/num_examples': 3003, 'score': 9478.669534683228, 'total_duration': 15621.306453466415, 'accumulated_submission_time': 9478.669534683228, 'accumulated_eval_time': 6142.259861469269, 'accumulated_logging_time': 0.23442292213439941}
I0502 00:41:11.160917 140248750135040 logging_writer.py:48] [11232] accumulated_eval_time=6142.259861, accumulated_logging_time=0.234423, accumulated_submission_time=9478.669535, global_step=11232, preemption_count=0, score=9478.669535, test/accuracy=0.650224, test/bleu=26.312967, test/loss=1.793707, test/num_examples=3003, total_duration=15621.306453, train/accuracy=0.623830, train/bleu=30.377894, train/loss=1.983854, validation/accuracy=0.642187, validation/bleu=27.256955, validation/loss=1.843375, validation/num_examples=3000
I0502 00:42:09.799247 140248741742336 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.14128920435905457, loss=3.1195783615112305
I0502 00:43:31.247829 140248750135040 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.11925499886274338, loss=3.0678865909576416
I0502 00:44:53.120436 140248741742336 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.12142632901668549, loss=3.101003885269165
I0502 00:46:14.853690 140248750135040 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.13120777904987335, loss=3.1276237964630127
I0502 00:47:36.586706 140248741742336 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.12155786156654358, loss=3.087761878967285
I0502 00:48:58.550302 140248750135040 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.11257267743349075, loss=3.081041097640991
I0502 00:50:22.703905 140248741742336 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.11305858939886093, loss=3.0553011894226074
I0502 00:51:46.890522 140248750135040 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.12104498594999313, loss=3.1349704265594482
I0502 00:53:10.808321 140248741742336 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.12305732816457748, loss=3.0059409141540527
I0502 00:54:33.029443 140248750135040 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.15843531489372253, loss=3.076296806335449
I0502 00:55:11.184879 140436552230720 spec.py:298] Evaluating on the training split.
I0502 00:55:14.009646 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 00:57:57.186171 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 00:57:59.882702 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:00:15.808803 140436552230720 spec.py:326] Evaluating on the test split.
I0502 01:00:18.564636 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:02:31.895548 140436552230720 submission_runner.py:415] Time since start: 16902.05s, 	Step: 12250, 	{'train/accuracy': 0.6259514093399048, 'train/loss': 1.9570560455322266, 'train/bleu': 30.603223435387637, 'validation/accuracy': 0.6466503739356995, 'validation/loss': 1.8029392957687378, 'validation/bleu': 27.442786014379962, 'validation/num_examples': 3000, 'test/accuracy': 0.6548370122909546, 'test/loss': 1.7495146989822388, 'test/bleu': 26.642132497654636, 'test/num_examples': 3003, 'score': 10318.669472932816, 'total_duration': 16902.051966667175, 'accumulated_submission_time': 10318.669472932816, 'accumulated_eval_time': 6582.970474720001, 'accumulated_logging_time': 0.2554666996002197}
I0502 01:02:31.906953 140248741742336 logging_writer.py:48] [12250] accumulated_eval_time=6582.970475, accumulated_logging_time=0.255467, accumulated_submission_time=10318.669473, global_step=12250, preemption_count=0, score=10318.669473, test/accuracy=0.654837, test/bleu=26.642132, test/loss=1.749515, test/num_examples=3003, total_duration=16902.051967, train/accuracy=0.625951, train/bleu=30.603223, train/loss=1.957056, validation/accuracy=0.646650, validation/bleu=27.442786, validation/loss=1.802939, validation/num_examples=3000
I0502 01:03:15.215417 140248750135040 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1297117918729782, loss=2.9884331226348877
I0502 01:04:36.666290 140248741742336 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.11015760153532028, loss=2.957871437072754
I0502 01:05:58.533598 140248750135040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.13043443858623505, loss=3.074730634689331
I0502 01:07:19.988601 140248741742336 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.10842917114496231, loss=3.0709221363067627
I0502 01:08:41.667061 140248750135040 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1288778930902481, loss=3.119892120361328
I0502 01:10:03.287779 140248741742336 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.14641693234443665, loss=3.067215919494629
I0502 01:11:25.007373 140248750135040 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.16410623490810394, loss=2.9848363399505615
I0502 01:12:46.896103 140248741742336 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.14145633578300476, loss=3.022449493408203
I0502 01:14:08.497182 140248750135040 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.11866376549005508, loss=3.032750129699707
I0502 01:15:30.390160 140248741742336 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.23865912854671478, loss=3.0817346572875977
I0502 01:16:35.629919 140436552230720 spec.py:298] Evaluating on the training split.
I0502 01:16:38.486969 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:19:06.661132 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 01:19:09.363841 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:21:24.499476 140436552230720 spec.py:326] Evaluating on the test split.
I0502 01:21:27.256460 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:23:40.471160 140436552230720 submission_runner.py:415] Time since start: 18170.63s, 	Step: 13281, 	{'train/accuracy': 0.6463851928710938, 'train/loss': 1.819798469543457, 'train/bleu': 31.44374216680972, 'validation/accuracy': 0.6527507305145264, 'validation/loss': 1.7747780084609985, 'validation/bleu': 27.695191142016704, 'validation/num_examples': 3000, 'test/accuracy': 0.6617279648780823, 'test/loss': 1.7163119316101074, 'test/bleu': 27.016740393754286, 'test/num_examples': 3003, 'score': 11162.370233297348, 'total_duration': 18170.62757897377, 'accumulated_submission_time': 11162.370233297348, 'accumulated_eval_time': 7007.811661243439, 'accumulated_logging_time': 0.27629780769348145}
I0502 01:23:40.482949 140248750135040 logging_writer.py:48] [13281] accumulated_eval_time=7007.811661, accumulated_logging_time=0.276298, accumulated_submission_time=11162.370233, global_step=13281, preemption_count=0, score=11162.370233, test/accuracy=0.661728, test/bleu=27.016740, test/loss=1.716312, test/num_examples=3003, total_duration=18170.627579, train/accuracy=0.646385, train/bleu=31.443742, train/loss=1.819798, validation/accuracy=0.652751, validation/bleu=27.695191, validation/loss=1.774778, validation/num_examples=3000
I0502 01:23:56.822763 140248741742336 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.11935216188430786, loss=3.019422769546509
I0502 01:25:18.686044 140248750135040 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.14067383110523224, loss=3.006509304046631
I0502 01:26:40.426447 140248741742336 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.11405915021896362, loss=2.9821081161499023
I0502 01:28:02.156523 140248750135040 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.11082665622234344, loss=2.9910593032836914
I0502 01:29:23.895315 140248741742336 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.15252746641635895, loss=3.008565902709961
I0502 01:30:45.248167 140248750135040 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.14816904067993164, loss=2.979163646697998
I0502 01:32:07.130616 140248741742336 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.115488201379776, loss=3.014559507369995
I0502 01:33:28.737767 140248750135040 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.12997868657112122, loss=2.9205081462860107
I0502 01:34:50.165198 140248741742336 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.10533392429351807, loss=2.97727108001709
I0502 01:36:12.083152 140248750135040 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.15758763253688812, loss=3.0730173587799072
I0502 01:37:33.739691 140248741742336 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.11751966923475266, loss=2.883232593536377
I0502 01:37:40.825883 140436552230720 spec.py:298] Evaluating on the training split.
I0502 01:37:43.651980 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:40:14.666821 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 01:40:17.353788 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:42:38.093677 140436552230720 spec.py:326] Evaluating on the test split.
I0502 01:42:40.843721 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 01:44:59.597692 140436552230720 submission_runner.py:415] Time since start: 19449.75s, 	Step: 14312, 	{'train/accuracy': 0.6412822008132935, 'train/loss': 1.8483413457870483, 'train/bleu': 31.961002254504177, 'validation/accuracy': 0.654932975769043, 'validation/loss': 1.7449880838394165, 'validation/bleu': 28.046102323872198, 'validation/num_examples': 3000, 'test/accuracy': 0.6644704341888428, 'test/loss': 1.6859990358352661, 'test/bleu': 27.265351841547304, 'test/num_examples': 3003, 'score': 12002.690684318542, 'total_duration': 19449.754088163376, 'accumulated_submission_time': 12002.690684318542, 'accumulated_eval_time': 7446.583389997482, 'accumulated_logging_time': 0.29757213592529297}
I0502 01:44:59.611716 140248750135040 logging_writer.py:48] [14312] accumulated_eval_time=7446.583390, accumulated_logging_time=0.297572, accumulated_submission_time=12002.690684, global_step=14312, preemption_count=0, score=12002.690684, test/accuracy=0.664470, test/bleu=27.265352, test/loss=1.685999, test/num_examples=3003, total_duration=19449.754088, train/accuracy=0.641282, train/bleu=31.961002, train/loss=1.848341, validation/accuracy=0.654933, validation/bleu=28.046102, validation/loss=1.744988, validation/num_examples=3000
I0502 01:46:16.385146 140248741742336 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.14215528964996338, loss=3.1194491386413574
I0502 01:47:40.187993 140248750135040 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1058708056807518, loss=2.9497759342193604
I0502 01:49:01.857596 140248741742336 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.13375669717788696, loss=2.953789710998535
I0502 01:50:23.547620 140248750135040 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.10883767157793045, loss=3.036538600921631
I0502 01:51:45.311725 140248741742336 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.19112339615821838, loss=2.9197440147399902
I0502 01:53:07.031755 140248750135040 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.17919549345970154, loss=3.0391604900360107
I0502 01:54:28.626183 140248741742336 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.1345607340335846, loss=2.9261772632598877
I0502 01:55:50.114180 140248750135040 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.13947124779224396, loss=3.0108935832977295
I0502 01:57:12.140432 140248741742336 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.12699583172798157, loss=2.978191375732422
I0502 01:58:33.684400 140248750135040 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.142476424574852, loss=2.9637973308563232
I0502 01:59:00.173185 140436552230720 spec.py:298] Evaluating on the training split.
I0502 01:59:02.999268 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:01:42.925296 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 02:01:45.608525 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:04:06.260291 140436552230720 spec.py:326] Evaluating on the test split.
I0502 02:04:09.007950 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:06:28.697069 140436552230720 submission_runner.py:415] Time since start: 20738.85s, 	Step: 15337, 	{'train/accuracy': 0.6444819569587708, 'train/loss': 1.8201416730880737, 'train/bleu': 31.633940494876686, 'validation/accuracy': 0.6596818566322327, 'validation/loss': 1.7197084426879883, 'validation/bleu': 28.28168647422798, 'validation/num_examples': 3000, 'test/accuracy': 0.6672477126121521, 'test/loss': 1.6618252992630005, 'test/bleu': 27.321427057041674, 'test/num_examples': 3003, 'score': 12843.22847032547, 'total_duration': 20738.85347366333, 'accumulated_submission_time': 12843.22847032547, 'accumulated_eval_time': 7895.107216358185, 'accumulated_logging_time': 0.3217735290527344}
I0502 02:06:28.708587 140248741742336 logging_writer.py:48] [15337] accumulated_eval_time=7895.107216, accumulated_logging_time=0.321774, accumulated_submission_time=12843.228470, global_step=15337, preemption_count=0, score=12843.228470, test/accuracy=0.667248, test/bleu=27.321427, test/loss=1.661825, test/num_examples=3003, total_duration=20738.853474, train/accuracy=0.644482, train/bleu=31.633940, train/loss=1.820142, validation/accuracy=0.659682, validation/bleu=28.281686, validation/loss=1.719708, validation/num_examples=3000
I0502 02:07:23.899520 140248750135040 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1440989077091217, loss=2.9940450191497803
I0502 02:08:45.156376 140248741742336 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.12257721275091171, loss=3.0390594005584717
I0502 02:10:07.144511 140248750135040 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.16449211537837982, loss=2.9945454597473145
I0502 02:11:29.204496 140248741742336 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.12574928998947144, loss=2.9761102199554443
I0502 02:12:50.961973 140248750135040 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.14455124735832214, loss=2.9939565658569336
I0502 02:14:12.779950 140248741742336 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1356956511735916, loss=3.0087075233459473
I0502 02:15:34.447766 140248750135040 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1152152493596077, loss=2.9407382011413574
I0502 02:16:55.946558 140248741742336 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.12282466143369675, loss=2.9752156734466553
I0502 02:18:17.887003 140248750135040 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.12914134562015533, loss=2.8264012336730957
I0502 02:19:39.592879 140248741742336 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.12412983924150467, loss=2.9917609691619873
I0502 02:20:29.434924 140436552230720 spec.py:298] Evaluating on the training split.
I0502 02:20:32.257078 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:23:10.936582 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 02:23:13.631197 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:25:36.054826 140436552230720 spec.py:326] Evaluating on the test split.
I0502 02:25:38.815885 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:27:57.422636 140436552230720 submission_runner.py:415] Time since start: 22027.58s, 	Step: 16362, 	{'train/accuracy': 0.6455411314964294, 'train/loss': 1.8104206323623657, 'train/bleu': 31.56069293376034, 'validation/accuracy': 0.6595578193664551, 'validation/loss': 1.7050437927246094, 'validation/bleu': 28.19938953313257, 'validation/num_examples': 3000, 'test/accuracy': 0.6703852415084839, 'test/loss': 1.642431616783142, 'test/bleu': 27.84028160999479, 'test/num_examples': 3003, 'score': 13683.932832241058, 'total_duration': 22027.579048395157, 'accumulated_submission_time': 13683.932832241058, 'accumulated_eval_time': 8343.094858884811, 'accumulated_logging_time': 0.3425710201263428}
I0502 02:27:57.434302 140248750135040 logging_writer.py:48] [16362] accumulated_eval_time=8343.094859, accumulated_logging_time=0.342571, accumulated_submission_time=13683.932832, global_step=16362, preemption_count=0, score=13683.932832, test/accuracy=0.670385, test/bleu=27.840282, test/loss=1.642432, test/num_examples=3003, total_duration=22027.579048, train/accuracy=0.645541, train/bleu=31.560693, train/loss=1.810421, validation/accuracy=0.659558, validation/bleu=28.199390, validation/loss=1.705044, validation/num_examples=3000
I0502 02:28:29.364500 140248741742336 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.11687608063220978, loss=3.0165364742279053
I0502 02:29:50.998548 140248750135040 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.15199999511241913, loss=3.011040449142456
I0502 02:31:12.659748 140248741742336 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.11396731436252594, loss=2.939406394958496
I0502 02:32:34.571827 140248750135040 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.13338764011859894, loss=2.96014404296875
I0502 02:33:57.680824 140248741742336 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1251402646303177, loss=3.0210764408111572
I0502 02:35:21.985329 140248750135040 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.11633377522230148, loss=2.960536003112793
I0502 02:36:45.889478 140248741742336 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.14263220131397247, loss=2.9782934188842773
I0502 02:38:07.468292 140248750135040 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.1187596470117569, loss=2.95871901512146
I0502 02:39:29.362172 140248741742336 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.13280664384365082, loss=2.9832801818847656
I0502 02:40:50.848392 140248750135040 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.12547780573368073, loss=2.953996419906616
I0502 02:41:57.756615 140436552230720 spec.py:298] Evaluating on the training split.
I0502 02:42:00.578317 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:45:07.550615 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 02:45:10.239760 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:47:39.422160 140436552230720 spec.py:326] Evaluating on the test split.
I0502 02:47:42.155971 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 02:49:57.577356 140436552230720 submission_runner.py:415] Time since start: 23347.73s, 	Step: 17383, 	{'train/accuracy': 0.6453712582588196, 'train/loss': 1.8171662092208862, 'train/bleu': 31.888677919737358, 'validation/accuracy': 0.6620252728462219, 'validation/loss': 1.6891862154006958, 'validation/bleu': 28.722034158817706, 'validation/num_examples': 3000, 'test/accuracy': 0.6736157536506653, 'test/loss': 1.621958613395691, 'test/bleu': 27.927163257432593, 'test/num_examples': 3003, 'score': 14524.231023550034, 'total_duration': 23347.733773946762, 'accumulated_submission_time': 14524.231023550034, 'accumulated_eval_time': 8822.915543556213, 'accumulated_logging_time': 0.3646862506866455}
I0502 02:49:57.589422 140248741742336 logging_writer.py:48] [17383] accumulated_eval_time=8822.915544, accumulated_logging_time=0.364686, accumulated_submission_time=14524.231024, global_step=17383, preemption_count=0, score=14524.231024, test/accuracy=0.673616, test/bleu=27.927163, test/loss=1.621959, test/num_examples=3003, total_duration=23347.733774, train/accuracy=0.645371, train/bleu=31.888678, train/loss=1.817166, validation/accuracy=0.662025, validation/bleu=28.722034, validation/loss=1.689186, validation/num_examples=3000
I0502 02:50:12.638812 140248750135040 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.13792869448661804, loss=2.9597558975219727
I0502 02:51:33.951331 140248741742336 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.12901565432548523, loss=2.879537343978882
I0502 02:52:55.456252 140248750135040 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.158669114112854, loss=2.9769251346588135
I0502 02:54:17.250171 140248741742336 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.15237940847873688, loss=2.8995680809020996
I0502 02:55:38.897327 140248750135040 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.13529247045516968, loss=2.9138691425323486
I0502 02:57:00.843741 140248741742336 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1274084895849228, loss=2.948882579803467
I0502 02:58:22.404754 140248750135040 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.11983205378055573, loss=2.980121612548828
I0502 02:59:43.935484 140248741742336 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.12718524038791656, loss=3.0107219219207764
I0502 03:01:05.641667 140248750135040 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.11641251295804977, loss=2.9032883644104004
I0502 03:02:27.123984 140248741742336 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1292586326599121, loss=2.966064214706421
I0502 03:03:49.378899 140248750135040 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.20261389017105103, loss=2.8902857303619385
I0502 03:03:58.095651 140436552230720 spec.py:298] Evaluating on the training split.
I0502 03:04:00.931244 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:06:36.644646 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 03:06:39.337160 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:08:56.852309 140436552230720 spec.py:326] Evaluating on the test split.
I0502 03:08:59.610739 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:11:21.520488 140436552230720 submission_runner.py:415] Time since start: 24631.68s, 	Step: 18415, 	{'train/accuracy': 0.6463190317153931, 'train/loss': 1.803583025932312, 'train/bleu': 31.931010198157065, 'validation/accuracy': 0.6646538972854614, 'validation/loss': 1.6775872707366943, 'validation/bleu': 28.58263903102196, 'validation/num_examples': 3000, 'test/accuracy': 0.6769043207168579, 'test/loss': 1.6065776348114014, 'test/bleu': 28.37172575436843, 'test/num_examples': 3003, 'score': 15364.714918136597, 'total_duration': 24631.6768989563, 'accumulated_submission_time': 15364.714918136597, 'accumulated_eval_time': 9266.340315580368, 'accumulated_logging_time': 0.3863637447357178}
I0502 03:11:21.533052 140248741742336 logging_writer.py:48] [18415] accumulated_eval_time=9266.340316, accumulated_logging_time=0.386364, accumulated_submission_time=15364.714918, global_step=18415, preemption_count=0, score=15364.714918, test/accuracy=0.676904, test/bleu=28.371726, test/loss=1.606578, test/num_examples=3003, total_duration=24631.676899, train/accuracy=0.646319, train/bleu=31.931010, train/loss=1.803583, validation/accuracy=0.664654, validation/bleu=28.582639, validation/loss=1.677587, validation/num_examples=3000
I0502 03:12:34.540360 140248750135040 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.11691944301128387, loss=2.9030189514160156
I0502 03:13:56.724418 140248741742336 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1867908090353012, loss=2.951458692550659
I0502 03:15:20.706739 140248750135040 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.20786337554454803, loss=2.9537174701690674
I0502 03:16:44.841876 140248741742336 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.16084682941436768, loss=2.8843281269073486
I0502 03:18:08.911010 140248750135040 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.13846732676029205, loss=2.9166388511657715
I0502 03:19:30.444012 140248741742336 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.12533144652843475, loss=2.8388116359710693
I0502 03:20:51.845973 140248750135040 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.16781796514987946, loss=2.960721731185913
I0502 03:22:13.638489 140248741742336 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.16492685675621033, loss=2.9010515213012695
I0502 03:23:35.424106 140248750135040 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.22215911746025085, loss=2.8728187084198
I0502 03:24:57.210933 140248741742336 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.16010035574436188, loss=3.0304760932922363
I0502 03:25:21.598642 140436552230720 spec.py:298] Evaluating on the training split.
I0502 03:25:24.425958 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:28:38.154494 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 03:28:40.843419 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:31:02.651448 140436552230720 spec.py:326] Evaluating on the test split.
I0502 03:31:05.396145 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:33:28.713973 140436552230720 submission_runner.py:415] Time since start: 25958.87s, 	Step: 19434, 	{'train/accuracy': 0.6643738150596619, 'train/loss': 1.6876049041748047, 'train/bleu': 32.598339202612244, 'validation/accuracy': 0.6663773655891418, 'validation/loss': 1.665796160697937, 'validation/bleu': 28.63351755860351, 'validation/num_examples': 3000, 'test/accuracy': 0.6779733896255493, 'test/loss': 1.5963714122772217, 'test/bleu': 28.152856968904302, 'test/num_examples': 3003, 'score': 16204.756236076355, 'total_duration': 25958.870386123657, 'accumulated_submission_time': 16204.756236076355, 'accumulated_eval_time': 9753.455584287643, 'accumulated_logging_time': 0.40939855575561523}
I0502 03:33:28.726348 140248750135040 logging_writer.py:48] [19434] accumulated_eval_time=9753.455584, accumulated_logging_time=0.409399, accumulated_submission_time=16204.756236, global_step=19434, preemption_count=0, score=16204.756236, test/accuracy=0.677973, test/bleu=28.152857, test/loss=1.596371, test/num_examples=3003, total_duration=25958.870386, train/accuracy=0.664374, train/bleu=32.598339, train/loss=1.687605, validation/accuracy=0.666377, validation/bleu=28.633518, validation/loss=1.665796, validation/num_examples=3000
I0502 03:34:25.978705 140248741742336 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.14023765921592712, loss=2.8982186317443848
I0502 03:35:47.658170 140248750135040 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1301397979259491, loss=2.9346835613250732
I0502 03:37:09.565492 140248741742336 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.16327470541000366, loss=2.902679204940796
I0502 03:38:32.653246 140248750135040 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.13912099599838257, loss=2.910642385482788
I0502 03:39:56.866952 140248741742336 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.12788309156894684, loss=2.94470477104187
I0502 03:41:15.757555 140436552230720 spec.py:298] Evaluating on the training split.
I0502 03:41:18.579723 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:44:24.190976 140436552230720 spec.py:310] Evaluating on the validation split.
I0502 03:44:26.876643 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:46:42.202711 140436552230720 spec.py:326] Evaluating on the test split.
I0502 03:46:44.946621 140436552230720 workload.py:179] Translating evaluation dataset.
I0502 03:48:55.071809 140436552230720 submission_runner.py:415] Time since start: 26885.23s, 	Step: 20000, 	{'train/accuracy': 0.6597866415977478, 'train/loss': 1.711543321609497, 'train/bleu': 32.87959245110847, 'validation/accuracy': 0.6663773655891418, 'validation/loss': 1.6547476053237915, 'validation/bleu': 29.06044440314696, 'validation/num_examples': 3000, 'test/accuracy': 0.6775899529457092, 'test/loss': 1.5866987705230713, 'test/bleu': 28.806290431273165, 'test/num_examples': 3003, 'score': 16671.76833677292, 'total_duration': 26885.228228330612, 'accumulated_submission_time': 16671.76833677292, 'accumulated_eval_time': 10212.769785881042, 'accumulated_logging_time': 0.4328148365020752}
I0502 03:48:55.084122 140248750135040 logging_writer.py:48] [20000] accumulated_eval_time=10212.769786, accumulated_logging_time=0.432815, accumulated_submission_time=16671.768337, global_step=20000, preemption_count=0, score=16671.768337, test/accuracy=0.677590, test/bleu=28.806290, test/loss=1.586699, test/num_examples=3003, total_duration=26885.228228, train/accuracy=0.659787, train/bleu=32.879592, train/loss=1.711543, validation/accuracy=0.666377, validation/bleu=29.060444, validation/loss=1.654748, validation/num_examples=3000
I0502 03:48:55.102987 140248741742336 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=16671.768337
I0502 03:48:57.910625 140436552230720 checkpoints.py:356] Saving checkpoint at step: 20000
I0502 03:49:06.450885 140436552230720 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1/checkpoint_20000
I0502 03:49:06.460244 140436552230720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_shampoo/wmt_jax/trial_1/checkpoint_20000.
I0502 03:49:06.641950 140436552230720 submission_runner.py:578] Tuning trial 1/1
I0502 03:49:06.642180 140436552230720 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0502 03:49:06.650354 140436552230720 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005380898946896195, 'train/loss': 11.089571952819824, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.082098960876465, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.077061653137207, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 233.88706994056702, 'total_duration': 1137.3430449962616, 'accumulated_submission_time': 233.88706994056702, 'accumulated_eval_time': 903.4557967185974, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1037, {'train/accuracy': 0.4286072850227356, 'train/loss': 3.877760410308838, 'train/bleu': 15.332419828227241, 'validation/accuracy': 0.41554349660873413, 'validation/loss': 3.9914638996124268, 'validation/bleu': 10.340105662885252, 'validation/num_examples': 3000, 'test/accuracy': 0.4012666344642639, 'test/loss': 4.168376445770264, 'test/bleu': 8.787092960464516, 'test/num_examples': 3003, 'score': 1074.1816589832306, 'total_duration': 2615.6735429763794, 'accumulated_submission_time': 1074.1816589832306, 'accumulated_eval_time': 1541.449119091034, 'accumulated_logging_time': 0.029551267623901367, 'global_step': 1037, 'preemption_count': 0}), (2053, {'train/accuracy': 0.5220142602920532, 'train/loss': 2.9639554023742676, 'train/bleu': 22.598551297326058, 'validation/accuracy': 0.5216178297996521, 'validation/loss': 2.963824987411499, 'validation/bleu': 18.935765327579553, 'validation/num_examples': 3000, 'test/accuracy': 0.520562469959259, 'test/loss': 3.0248167514801025, 'test/bleu': 17.413539091918206, 'test/num_examples': 3003, 'score': 1914.5362091064453, 'total_duration': 3992.310295343399, 'accumulated_submission_time': 1914.5362091064453, 'accumulated_eval_time': 2077.699175596237, 'accumulated_logging_time': 0.04913663864135742, 'global_step': 2053, 'preemption_count': 0}), (3073, {'train/accuracy': 0.5486227869987488, 'train/loss': 2.6782023906707764, 'train/bleu': 25.19036209894087, 'validation/accuracy': 0.5519336462020874, 'validation/loss': 2.6405537128448486, 'validation/bleu': 21.15530481817793, 'validation/num_examples': 3000, 'test/accuracy': 0.5547731518745422, 'test/loss': 2.656911849975586, 'test/bleu': 19.868803720207474, 'test/num_examples': 3003, 'score': 2755.0597808361053, 'total_duration': 5307.341752290726, 'accumulated_submission_time': 2755.0597808361053, 'accumulated_eval_time': 2552.1754615306854, 'accumulated_logging_time': 0.06812906265258789, 'global_step': 3073, 'preemption_count': 0}), (4094, {'train/accuracy': 0.5580021739006042, 'train/loss': 2.549273729324341, 'train/bleu': 24.162500821741105, 'validation/accuracy': 0.5605633854866028, 'validation/loss': 2.4987900257110596, 'validation/bleu': 20.8212681038752, 'validation/num_examples': 3000, 'test/accuracy': 0.5628609657287598, 'test/loss': 2.530064821243286, 'test/bleu': 19.556879281092037, 'test/num_examples': 3003, 'score': 3595.6270706653595, 'total_duration': 6598.7751541137695, 'accumulated_submission_time': 3595.6270706653595, 'accumulated_eval_time': 3003.009439229965, 'accumulated_logging_time': 0.08755159378051758, 'global_step': 4094, 'preemption_count': 0}), (5104, {'train/accuracy': 0.5718808770179749, 'train/loss': 2.421102523803711, 'train/bleu': 26.805479974118015, 'validation/accuracy': 0.5828322172164917, 'validation/loss': 2.3273799419403076, 'validation/bleu': 23.17840162777327, 'validation/num_examples': 3000, 'test/accuracy': 0.585300087928772, 'test/loss': 2.3284077644348145, 'test/bleu': 22.035251867653237, 'test/num_examples': 3003, 'score': 4435.9636306762695, 'total_duration': 7879.145348072052, 'accumulated_submission_time': 4435.9636306762695, 'accumulated_eval_time': 3443.009294986725, 'accumulated_logging_time': 0.10796952247619629, 'global_step': 5104, 'preemption_count': 0}), (6121, {'train/accuracy': 0.5826478004455566, 'train/loss': 2.327449321746826, 'train/bleu': 27.730322997937115, 'validation/accuracy': 0.5972647666931152, 'validation/loss': 2.206331491470337, 'validation/bleu': 24.365539973130133, 'validation/num_examples': 3000, 'test/accuracy': 0.6010226011276245, 'test/loss': 2.189636468887329, 'test/bleu': 23.402790312153087, 'test/num_examples': 3003, 'score': 5276.660764455795, 'total_duration': 9178.087550878525, 'accumulated_submission_time': 5276.660764455795, 'accumulated_eval_time': 3901.219612121582, 'accumulated_logging_time': 0.12955284118652344, 'global_step': 6121, 'preemption_count': 0}), (7143, {'train/accuracy': 0.6033188700675964, 'train/loss': 2.1551942825317383, 'train/bleu': 28.678011788561815, 'validation/accuracy': 0.6109781861305237, 'validation/loss': 2.0925533771514893, 'validation/bleu': 24.913357089278065, 'validation/num_examples': 3000, 'test/accuracy': 0.6146069765090942, 'test/loss': 2.0688061714172363, 'test/bleu': 23.766795831856736, 'test/num_examples': 3003, 'score': 6117.148396492004, 'total_duration': 10475.506830453873, 'accumulated_submission_time': 6117.148396492004, 'accumulated_eval_time': 4358.119149446487, 'accumulated_logging_time': 0.14905619621276855, 'global_step': 7143, 'preemption_count': 0}), (8165, {'train/accuracy': 0.6079455018043518, 'train/loss': 2.1137988567352295, 'train/bleu': 29.102572485043527, 'validation/accuracy': 0.6220629811286926, 'validation/loss': 2.0048046112060547, 'validation/bleu': 25.608917026473012, 'validation/num_examples': 3000, 'test/accuracy': 0.6288071870803833, 'test/loss': 1.9664326906204224, 'test/bleu': 24.880094802498775, 'test/num_examples': 3003, 'score': 6957.298423290253, 'total_duration': 11757.353631019592, 'accumulated_submission_time': 6957.298423290253, 'accumulated_eval_time': 4799.783720493317, 'accumulated_logging_time': 0.16863179206848145, 'global_step': 8165, 'preemption_count': 0}), (9190, {'train/accuracy': 0.613373875617981, 'train/loss': 2.0594215393066406, 'train/bleu': 30.181708885599242, 'validation/accuracy': 0.6311390995979309, 'validation/loss': 1.9345043897628784, 'validation/bleu': 26.33774681595562, 'validation/num_examples': 3000, 'test/accuracy': 0.6353495121002197, 'test/loss': 1.8932571411132812, 'test/bleu': 25.243706467131346, 'test/num_examples': 3003, 'score': 7797.860035657883, 'total_duration': 13057.814243078232, 'accumulated_submission_time': 7797.860035657883, 'accumulated_eval_time': 5259.649926662445, 'accumulated_logging_time': 0.18874382972717285, 'global_step': 9190, 'preemption_count': 0}), (10207, {'train/accuracy': 0.6204060316085815, 'train/loss': 2.0129036903381348, 'train/bleu': 30.050341042619326, 'validation/accuracy': 0.63567715883255, 'validation/loss': 1.8904883861541748, 'validation/bleu': 26.70517044330013, 'validation/num_examples': 3000, 'test/accuracy': 0.6432514190673828, 'test/loss': 1.8357809782028198, 'test/bleu': 25.772184592359746, 'test/num_examples': 3003, 'score': 8638.06702876091, 'total_duration': 14332.647903442383, 'accumulated_submission_time': 8638.06702876091, 'accumulated_eval_time': 5694.240840673447, 'accumulated_logging_time': 0.2105567455291748, 'global_step': 10207, 'preemption_count': 0}), (11232, {'train/accuracy': 0.6238304972648621, 'train/loss': 1.9838541746139526, 'train/bleu': 30.377893559880768, 'validation/accuracy': 0.64218670129776, 'validation/loss': 1.8433747291564941, 'validation/bleu': 27.25695483547751, 'validation/num_examples': 3000, 'test/accuracy': 0.6502237319946289, 'test/loss': 1.793707251548767, 'test/bleu': 26.31296724638282, 'test/num_examples': 3003, 'score': 9478.669534683228, 'total_duration': 15621.306453466415, 'accumulated_submission_time': 9478.669534683228, 'accumulated_eval_time': 6142.259861469269, 'accumulated_logging_time': 0.23442292213439941, 'global_step': 11232, 'preemption_count': 0}), (12250, {'train/accuracy': 0.6259514093399048, 'train/loss': 1.9570560455322266, 'train/bleu': 30.603223435387637, 'validation/accuracy': 0.6466503739356995, 'validation/loss': 1.8029392957687378, 'validation/bleu': 27.442786014379962, 'validation/num_examples': 3000, 'test/accuracy': 0.6548370122909546, 'test/loss': 1.7495146989822388, 'test/bleu': 26.642132497654636, 'test/num_examples': 3003, 'score': 10318.669472932816, 'total_duration': 16902.051966667175, 'accumulated_submission_time': 10318.669472932816, 'accumulated_eval_time': 6582.970474720001, 'accumulated_logging_time': 0.2554666996002197, 'global_step': 12250, 'preemption_count': 0}), (13281, {'train/accuracy': 0.6463851928710938, 'train/loss': 1.819798469543457, 'train/bleu': 31.44374216680972, 'validation/accuracy': 0.6527507305145264, 'validation/loss': 1.7747780084609985, 'validation/bleu': 27.695191142016704, 'validation/num_examples': 3000, 'test/accuracy': 0.6617279648780823, 'test/loss': 1.7163119316101074, 'test/bleu': 27.016740393754286, 'test/num_examples': 3003, 'score': 11162.370233297348, 'total_duration': 18170.62757897377, 'accumulated_submission_time': 11162.370233297348, 'accumulated_eval_time': 7007.811661243439, 'accumulated_logging_time': 0.27629780769348145, 'global_step': 13281, 'preemption_count': 0}), (14312, {'train/accuracy': 0.6412822008132935, 'train/loss': 1.8483413457870483, 'train/bleu': 31.961002254504177, 'validation/accuracy': 0.654932975769043, 'validation/loss': 1.7449880838394165, 'validation/bleu': 28.046102323872198, 'validation/num_examples': 3000, 'test/accuracy': 0.6644704341888428, 'test/loss': 1.6859990358352661, 'test/bleu': 27.265351841547304, 'test/num_examples': 3003, 'score': 12002.690684318542, 'total_duration': 19449.754088163376, 'accumulated_submission_time': 12002.690684318542, 'accumulated_eval_time': 7446.583389997482, 'accumulated_logging_time': 0.29757213592529297, 'global_step': 14312, 'preemption_count': 0}), (15337, {'train/accuracy': 0.6444819569587708, 'train/loss': 1.8201416730880737, 'train/bleu': 31.633940494876686, 'validation/accuracy': 0.6596818566322327, 'validation/loss': 1.7197084426879883, 'validation/bleu': 28.28168647422798, 'validation/num_examples': 3000, 'test/accuracy': 0.6672477126121521, 'test/loss': 1.6618252992630005, 'test/bleu': 27.321427057041674, 'test/num_examples': 3003, 'score': 12843.22847032547, 'total_duration': 20738.85347366333, 'accumulated_submission_time': 12843.22847032547, 'accumulated_eval_time': 7895.107216358185, 'accumulated_logging_time': 0.3217735290527344, 'global_step': 15337, 'preemption_count': 0}), (16362, {'train/accuracy': 0.6455411314964294, 'train/loss': 1.8104206323623657, 'train/bleu': 31.56069293376034, 'validation/accuracy': 0.6595578193664551, 'validation/loss': 1.7050437927246094, 'validation/bleu': 28.19938953313257, 'validation/num_examples': 3000, 'test/accuracy': 0.6703852415084839, 'test/loss': 1.642431616783142, 'test/bleu': 27.84028160999479, 'test/num_examples': 3003, 'score': 13683.932832241058, 'total_duration': 22027.579048395157, 'accumulated_submission_time': 13683.932832241058, 'accumulated_eval_time': 8343.094858884811, 'accumulated_logging_time': 0.3425710201263428, 'global_step': 16362, 'preemption_count': 0}), (17383, {'train/accuracy': 0.6453712582588196, 'train/loss': 1.8171662092208862, 'train/bleu': 31.888677919737358, 'validation/accuracy': 0.6620252728462219, 'validation/loss': 1.6891862154006958, 'validation/bleu': 28.722034158817706, 'validation/num_examples': 3000, 'test/accuracy': 0.6736157536506653, 'test/loss': 1.621958613395691, 'test/bleu': 27.927163257432593, 'test/num_examples': 3003, 'score': 14524.231023550034, 'total_duration': 23347.733773946762, 'accumulated_submission_time': 14524.231023550034, 'accumulated_eval_time': 8822.915543556213, 'accumulated_logging_time': 0.3646862506866455, 'global_step': 17383, 'preemption_count': 0}), (18415, {'train/accuracy': 0.6463190317153931, 'train/loss': 1.803583025932312, 'train/bleu': 31.931010198157065, 'validation/accuracy': 0.6646538972854614, 'validation/loss': 1.6775872707366943, 'validation/bleu': 28.58263903102196, 'validation/num_examples': 3000, 'test/accuracy': 0.6769043207168579, 'test/loss': 1.6065776348114014, 'test/bleu': 28.37172575436843, 'test/num_examples': 3003, 'score': 15364.714918136597, 'total_duration': 24631.6768989563, 'accumulated_submission_time': 15364.714918136597, 'accumulated_eval_time': 9266.340315580368, 'accumulated_logging_time': 0.3863637447357178, 'global_step': 18415, 'preemption_count': 0}), (19434, {'train/accuracy': 0.6643738150596619, 'train/loss': 1.6876049041748047, 'train/bleu': 32.598339202612244, 'validation/accuracy': 0.6663773655891418, 'validation/loss': 1.665796160697937, 'validation/bleu': 28.63351755860351, 'validation/num_examples': 3000, 'test/accuracy': 0.6779733896255493, 'test/loss': 1.5963714122772217, 'test/bleu': 28.152856968904302, 'test/num_examples': 3003, 'score': 16204.756236076355, 'total_duration': 25958.870386123657, 'accumulated_submission_time': 16204.756236076355, 'accumulated_eval_time': 9753.455584287643, 'accumulated_logging_time': 0.40939855575561523, 'global_step': 19434, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6597866415977478, 'train/loss': 1.711543321609497, 'train/bleu': 32.87959245110847, 'validation/accuracy': 0.6663773655891418, 'validation/loss': 1.6547476053237915, 'validation/bleu': 29.06044440314696, 'validation/num_examples': 3000, 'test/accuracy': 0.6775899529457092, 'test/loss': 1.5866987705230713, 'test/bleu': 28.806290431273165, 'test/num_examples': 3003, 'score': 16671.76833677292, 'total_duration': 26885.228228330612, 'accumulated_submission_time': 16671.76833677292, 'accumulated_eval_time': 10212.769785881042, 'accumulated_logging_time': 0.4328148365020752, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0502 03:49:06.650545 140436552230720 submission_runner.py:581] Timing: 16671.76833677292
I0502 03:49:06.650596 140436552230720 submission_runner.py:582] ====================
I0502 03:49:06.650739 140436552230720 submission_runner.py:645] Final wmt score: 16671.76833677292
