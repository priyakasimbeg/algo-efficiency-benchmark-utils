python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_04-29-2023-02-29-15.log
I0429 02:29:36.514917 140172950476608 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_lamb/ogbg_jax.
I0429 02:29:36.583252 140172950476608 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 02:29:37.495627 140172950476608 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0429 02:29:37.496417 140172950476608 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 02:29:37.500124 140172950476608 submission_runner.py:538] Using RNG seed 482485050
I0429 02:29:40.208456 140172950476608 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 02:29:40.208660 140172950476608 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1.
I0429 02:29:40.208851 140172950476608 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1/hparams.json.
I0429 02:29:40.333051 140172950476608 submission_runner.py:241] Initializing dataset.
I0429 02:29:40.567728 140172950476608 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:29:40.573606 140172950476608 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0429 02:29:40.803150 140172950476608 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:29:40.858975 140172950476608 submission_runner.py:248] Initializing model.
I0429 02:29:48.129891 140172950476608 submission_runner.py:258] Initializing optimizer.
I0429 02:29:48.523223 140172950476608 submission_runner.py:265] Initializing metrics bundle.
I0429 02:29:48.523409 140172950476608 submission_runner.py:282] Initializing checkpoint and logger.
I0429 02:29:48.524387 140172950476608 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1 with prefix checkpoint_
I0429 02:29:48.524621 140172950476608 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 02:29:48.524682 140172950476608 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 02:29:49.385499 140172950476608 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1/meta_data_0.json.
I0429 02:29:49.386452 140172950476608 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1/flags_0.json.
I0429 02:29:49.392664 140172950476608 submission_runner.py:318] Starting training loop.
I0429 02:30:15.659206 139996707157760 logging_writer.py:48] [0] global_step=0, grad_norm=2.907931327819824, loss=0.7302298545837402
I0429 02:30:15.671770 140172950476608 spec.py:298] Evaluating on the training split.
I0429 02:30:15.679649 140172950476608 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:30:15.683102 140172950476608 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0429 02:30:15.743587 140172950476608 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:31:47.195096 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 02:31:47.197907 140172950476608 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:31:47.201508 140172950476608 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0429 02:31:47.255213 140172950476608 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:32:49.425527 140172950476608 spec.py:326] Evaluating on the test split.
I0429 02:32:49.428427 140172950476608 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:32:49.432102 140172950476608 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0429 02:32:49.485128 140172950476608 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0429 02:33:53.221678 140172950476608 submission_runner.py:415] Time since start: 243.83s, 	Step: 1, 	{'train/accuracy': 0.5278680920600891, 'train/loss': 0.7312097549438477, 'train/mean_average_precision': 0.021113802074181197, 'validation/accuracy': 0.5310146808624268, 'validation/loss': 0.7295722961425781, 'validation/mean_average_precision': 0.026691026564612353, 'validation/num_examples': 43793, 'test/accuracy': 0.5329578518867493, 'test/loss': 0.7286767959594727, 'test/mean_average_precision': 0.027852736853321666, 'test/num_examples': 43793, 'score': 26.278934001922607, 'total_duration': 243.82895708084106, 'accumulated_submission_time': 26.278934001922607, 'accumulated_eval_time': 217.5498549938202, 'accumulated_logging_time': 0}
I0429 02:33:53.238286 139987017860864 logging_writer.py:48] [1] accumulated_eval_time=217.549855, accumulated_logging_time=0, accumulated_submission_time=26.278934, global_step=1, preemption_count=0, score=26.278934, test/accuracy=0.532958, test/loss=0.728677, test/mean_average_precision=0.027853, test/num_examples=43793, total_duration=243.828957, train/accuracy=0.527868, train/loss=0.731210, train/mean_average_precision=0.021114, validation/accuracy=0.531015, validation/loss=0.729572, validation/mean_average_precision=0.026691, validation/num_examples=43793
I0429 02:34:17.594528 139987026253568 logging_writer.py:48] [100] global_step=100, grad_norm=1.522974967956543, loss=0.5893471837043762
I0429 02:34:41.894315 139987017860864 logging_writer.py:48] [200] global_step=200, grad_norm=0.696686327457428, loss=0.4655761420726776
I0429 02:35:06.229884 139987026253568 logging_writer.py:48] [300] global_step=300, grad_norm=0.46338310837745667, loss=0.37717196345329285
I0429 02:35:30.370117 139987017860864 logging_writer.py:48] [400] global_step=400, grad_norm=0.46761906147003174, loss=0.3093545436859131
I0429 02:35:55.014404 139987026253568 logging_writer.py:48] [500] global_step=500, grad_norm=0.3676620423793793, loss=0.23703007400035858
I0429 02:36:19.370498 139987017860864 logging_writer.py:48] [600] global_step=600, grad_norm=0.23297394812107086, loss=0.16433076560497284
I0429 02:36:43.529677 139987026253568 logging_writer.py:48] [700] global_step=700, grad_norm=0.1368389129638672, loss=0.11089899390935898
I0429 02:37:07.670681 139987017860864 logging_writer.py:48] [800] global_step=800, grad_norm=0.16391904652118683, loss=0.06817621737718582
I0429 02:37:31.782743 139987026253568 logging_writer.py:48] [900] global_step=900, grad_norm=0.207073375582695, loss=0.05944613367319107
I0429 02:37:53.300240 140172950476608 spec.py:298] Evaluating on the training split.
I0429 02:39:06.973957 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 02:39:09.581904 140172950476608 spec.py:326] Evaluating on the test split.
I0429 02:39:12.164983 140172950476608 submission_runner.py:415] Time since start: 562.77s, 	Step: 990, 	{'train/accuracy': 0.9868677258491516, 'train/loss': 0.05501144006848335, 'train/mean_average_precision': 0.04865692826348793, 'validation/accuracy': 0.984278678894043, 'validation/loss': 0.06431061029434204, 'validation/mean_average_precision': 0.04847660524505075, 'validation/num_examples': 43793, 'test/accuracy': 0.983282744884491, 'test/loss': 0.06743130832910538, 'test/mean_average_precision': 0.049513291200891145, 'test/num_examples': 43793, 'score': 266.32204389572144, 'total_duration': 562.772260427475, 'accumulated_submission_time': 266.32204389572144, 'accumulated_eval_time': 296.41456294059753, 'accumulated_logging_time': 0.027406692504882812}
I0429 02:39:12.173197 139987017860864 logging_writer.py:48] [990] accumulated_eval_time=296.414563, accumulated_logging_time=0.027407, accumulated_submission_time=266.322044, global_step=990, preemption_count=0, score=266.322044, test/accuracy=0.983283, test/loss=0.067431, test/mean_average_precision=0.049513, test/num_examples=43793, total_duration=562.772260, train/accuracy=0.986868, train/loss=0.055011, train/mean_average_precision=0.048657, validation/accuracy=0.984279, validation/loss=0.064311, validation/mean_average_precision=0.048477, validation/num_examples=43793
I0429 02:39:14.909705 139987026253568 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.11896848678588867, loss=0.05351633578538895
I0429 02:39:39.151812 139987017860864 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.21520380675792694, loss=0.057631056755781174
I0429 02:40:03.796088 139987026253568 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.10914856195449829, loss=0.04875712841749191
I0429 02:40:28.060207 139987017860864 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.09372801333665848, loss=0.054152511060237885
I0429 02:40:52.521435 139987026253568 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.21971666812896729, loss=0.054856449365615845
I0429 02:41:16.956583 139987017860864 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.09732817113399506, loss=0.04823461174964905
I0429 02:41:41.066438 139987026253568 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.17214429378509521, loss=0.0515081062912941
I0429 02:42:05.213265 139987017860864 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2835662364959717, loss=0.04712771996855736
I0429 02:42:29.493312 139987026253568 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.11168337613344193, loss=0.05355667322874069
I0429 02:42:53.835504 139987017860864 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.08373568207025528, loss=0.0440978966653347
I0429 02:43:12.370596 140172950476608 spec.py:298] Evaluating on the training split.
I0429 02:44:26.612454 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 02:44:29.234434 140172950476608 spec.py:326] Evaluating on the test split.
I0429 02:44:31.821199 140172950476608 submission_runner.py:415] Time since start: 882.43s, 	Step: 1977, 	{'train/accuracy': 0.9872841238975525, 'train/loss': 0.0481090173125267, 'train/mean_average_precision': 0.09437767865881272, 'validation/accuracy': 0.9845737814903259, 'validation/loss': 0.05802327021956444, 'validation/mean_average_precision': 0.09303609997152006, 'validation/num_examples': 43793, 'test/accuracy': 0.9835758805274963, 'test/loss': 0.061414893716573715, 'test/mean_average_precision': 0.09217892943166313, 'test/num_examples': 43793, 'score': 506.5017046928406, 'total_duration': 882.4284811019897, 'accumulated_submission_time': 506.5017046928406, 'accumulated_eval_time': 375.86513471603394, 'accumulated_logging_time': 0.045046091079711914}
I0429 02:44:31.829165 139987026253568 logging_writer.py:48] [1977] accumulated_eval_time=375.865135, accumulated_logging_time=0.045046, accumulated_submission_time=506.501705, global_step=1977, preemption_count=0, score=506.501705, test/accuracy=0.983576, test/loss=0.061415, test/mean_average_precision=0.092179, test/num_examples=43793, total_duration=882.428481, train/accuracy=0.987284, train/loss=0.048109, train/mean_average_precision=0.094378, validation/accuracy=0.984574, validation/loss=0.058023, validation/mean_average_precision=0.093036, validation/num_examples=43793
I0429 02:44:37.712226 139987017860864 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.1961348056793213, loss=0.04785573109984398
I0429 02:45:02.315169 139987026253568 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.1694795787334442, loss=0.048602744936943054
I0429 02:45:26.605688 139987017860864 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.21965278685092926, loss=0.05001632124185562
I0429 02:45:51.093933 139987026253568 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.20879004895687103, loss=0.0475299209356308
I0429 02:46:15.741478 139987017860864 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.12409437447786331, loss=0.04970189556479454
I0429 02:46:40.919242 139987026253568 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.09627839922904968, loss=0.04817807674407959
I0429 02:47:05.847888 139987017860864 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.1136239618062973, loss=0.04459575563669205
I0429 02:47:30.171213 139987026253568 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1205737441778183, loss=0.04700898006558418
I0429 02:47:55.030092 139987017860864 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.12994731962680817, loss=0.04544480890035629
I0429 02:48:19.869812 139987026253568 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.15557262301445007, loss=0.046153705567121506
I0429 02:48:32.055690 140172950476608 spec.py:298] Evaluating on the training split.
I0429 02:49:46.233711 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 02:49:48.810565 140172950476608 spec.py:326] Evaluating on the test split.
I0429 02:49:51.338837 140172950476608 submission_runner.py:415] Time since start: 1201.95s, 	Step: 2950, 	{'train/accuracy': 0.9875606298446655, 'train/loss': 0.04541366919875145, 'train/mean_average_precision': 0.1324003887249699, 'validation/accuracy': 0.9848458170890808, 'validation/loss': 0.05530287325382233, 'validation/mean_average_precision': 0.12952113067833662, 'validation/num_examples': 43793, 'test/accuracy': 0.9838648438453674, 'test/loss': 0.05865506827831268, 'test/mean_average_precision': 0.12569594545095475, 'test/num_examples': 43793, 'score': 746.7105331420898, 'total_duration': 1201.946100473404, 'accumulated_submission_time': 746.7105331420898, 'accumulated_eval_time': 455.148232460022, 'accumulated_logging_time': 0.06241250038146973}
I0429 02:49:51.346537 139987017860864 logging_writer.py:48] [2950] accumulated_eval_time=455.148232, accumulated_logging_time=0.062413, accumulated_submission_time=746.710533, global_step=2950, preemption_count=0, score=746.710533, test/accuracy=0.983865, test/loss=0.058655, test/mean_average_precision=0.125696, test/num_examples=43793, total_duration=1201.946100, train/accuracy=0.987561, train/loss=0.045414, train/mean_average_precision=0.132400, validation/accuracy=0.984846, validation/loss=0.055303, validation/mean_average_precision=0.129521, validation/num_examples=43793
I0429 02:50:04.146736 139987026253568 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.1093030720949173, loss=0.04562867432832718
I0429 02:50:29.064992 139987017860864 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.08465754240751266, loss=0.04230062663555145
I0429 02:50:53.788331 139987026253568 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.16238221526145935, loss=0.046749722212553024
I0429 02:51:18.667939 139987017860864 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.08733434975147247, loss=0.04510744661092758
I0429 02:51:43.662877 139987026253568 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.08551684021949768, loss=0.04728492721915245
I0429 02:52:08.337092 139987017860864 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.08407887816429138, loss=0.047188565135002136
I0429 02:52:33.005647 139987026253568 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11133045703172684, loss=0.04538284242153168
I0429 02:52:57.443511 139987017860864 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0920315608382225, loss=0.04238511621952057
I0429 02:53:21.958435 139987026253568 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0468229204416275, loss=0.046540770679712296
I0429 02:53:46.398957 139987017860864 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.04385347291827202, loss=0.040565259754657745
I0429 02:53:51.434832 140172950476608 spec.py:298] Evaluating on the training split.
I0429 02:55:06.225135 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 02:55:08.822109 140172950476608 spec.py:326] Evaluating on the test split.
I0429 02:55:11.353572 140172950476608 submission_runner.py:415] Time since start: 1521.96s, 	Step: 3921, 	{'train/accuracy': 0.9882091879844666, 'train/loss': 0.04135122895240784, 'train/mean_average_precision': 0.17064978769129563, 'validation/accuracy': 0.9853544235229492, 'validation/loss': 0.05084559693932533, 'validation/mean_average_precision': 0.16089689963686712, 'validation/num_examples': 43793, 'test/accuracy': 0.9843711256980896, 'test/loss': 0.05361801013350487, 'test/mean_average_precision': 0.16189840707544728, 'test/num_examples': 43793, 'score': 986.7797102928162, 'total_duration': 1521.9608376026154, 'accumulated_submission_time': 986.7797102928162, 'accumulated_eval_time': 535.0669198036194, 'accumulated_logging_time': 0.08080935478210449}
I0429 02:55:11.362063 139987026253568 logging_writer.py:48] [3921] accumulated_eval_time=535.066920, accumulated_logging_time=0.080809, accumulated_submission_time=986.779710, global_step=3921, preemption_count=0, score=986.779710, test/accuracy=0.984371, test/loss=0.053618, test/mean_average_precision=0.161898, test/num_examples=43793, total_duration=1521.960838, train/accuracy=0.988209, train/loss=0.041351, train/mean_average_precision=0.170650, validation/accuracy=0.985354, validation/loss=0.050846, validation/mean_average_precision=0.160897, validation/num_examples=43793
I0429 02:55:30.722541 139987017860864 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.07520397752523422, loss=0.04926251620054245
I0429 02:55:55.046946 139987026253568 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.08202267438173294, loss=0.04156089574098587
I0429 02:56:19.659149 139987017860864 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.08690688759088516, loss=0.04266895726323128
I0429 02:56:43.976612 139987026253568 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05478659272193909, loss=0.03786260262131691
I0429 02:57:08.534283 139987017860864 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.06627525389194489, loss=0.03931901603937149
I0429 02:57:33.036148 139987026253568 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.06500165909528732, loss=0.04445599392056465
I0429 02:57:57.562337 139987017860864 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.04874967038631439, loss=0.044916655868291855
I0429 02:58:22.301661 139987026253568 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.042781930416822433, loss=0.04797138646245003
I0429 02:58:46.809450 139987017860864 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.05029282346367836, loss=0.045663971453905106
I0429 02:59:11.011790 139987026253568 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.09552547335624695, loss=0.03847616910934448
I0429 02:59:11.513710 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:00:26.315430 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:00:28.931757 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:00:31.452522 140172950476608 submission_runner.py:415] Time since start: 1842.06s, 	Step: 4903, 	{'train/accuracy': 0.9885743260383606, 'train/loss': 0.03940508887171745, 'train/mean_average_precision': 0.2247011062738582, 'validation/accuracy': 0.9855732321739197, 'validation/loss': 0.04935610294342041, 'validation/mean_average_precision': 0.18179570877159723, 'validation/num_examples': 43793, 'test/accuracy': 0.9846507906913757, 'test/loss': 0.05191381648182869, 'test/mean_average_precision': 0.18282126396147053, 'test/num_examples': 43793, 'score': 1226.9133729934692, 'total_duration': 1842.0598032474518, 'accumulated_submission_time': 1226.9133729934692, 'accumulated_eval_time': 615.0056929588318, 'accumulated_logging_time': 0.09881138801574707}
I0429 03:00:31.465448 139987017860864 logging_writer.py:48] [4903] accumulated_eval_time=615.005693, accumulated_logging_time=0.098811, accumulated_submission_time=1226.913373, global_step=4903, preemption_count=0, score=1226.913373, test/accuracy=0.984651, test/loss=0.051914, test/mean_average_precision=0.182821, test/num_examples=43793, total_duration=1842.059803, train/accuracy=0.988574, train/loss=0.039405, train/mean_average_precision=0.224701, validation/accuracy=0.985573, validation/loss=0.049356, validation/mean_average_precision=0.181796, validation/num_examples=43793
I0429 03:00:55.016047 139987026253568 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.03640544414520264, loss=0.041953228414058685
I0429 03:01:19.457388 139987017860864 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.03676770627498627, loss=0.03959018737077713
I0429 03:01:43.999761 139987026253568 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.041476454585790634, loss=0.0440128929913044
I0429 03:02:08.576710 139987017860864 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.04921203851699829, loss=0.03500291705131531
I0429 03:02:33.041662 139987026253568 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05555472522974014, loss=0.04592079296708107
I0429 03:02:57.598759 139987017860864 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0770195946097374, loss=0.04120779037475586
I0429 03:03:22.001744 139987026253568 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0656440407037735, loss=0.04341030493378639
I0429 03:03:46.292196 139987017860864 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.06778211146593094, loss=0.04384975880384445
I0429 03:04:10.930380 139987026253568 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0638086125254631, loss=0.03905037045478821
I0429 03:04:31.599535 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:05:46.744446 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:05:49.364552 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:05:51.908447 140172950476608 submission_runner.py:415] Time since start: 2162.52s, 	Step: 5887, 	{'train/accuracy': 0.9888923168182373, 'train/loss': 0.03793647512793541, 'train/mean_average_precision': 0.24021499381146638, 'validation/accuracy': 0.985870361328125, 'validation/loss': 0.04774964973330498, 'validation/mean_average_precision': 0.20105228667646197, 'validation/num_examples': 43793, 'test/accuracy': 0.984968364238739, 'test/loss': 0.050444621592760086, 'test/mean_average_precision': 0.20695311065874858, 'test/num_examples': 43793, 'score': 1467.0294451713562, 'total_duration': 2162.5157079696655, 'accumulated_submission_time': 1467.0294451713562, 'accumulated_eval_time': 695.3145515918732, 'accumulated_logging_time': 0.12133955955505371}
I0429 03:05:51.916545 139987017860864 logging_writer.py:48] [5887] accumulated_eval_time=695.314552, accumulated_logging_time=0.121340, accumulated_submission_time=1467.029445, global_step=5887, preemption_count=0, score=1467.029445, test/accuracy=0.984968, test/loss=0.050445, test/mean_average_precision=0.206953, test/num_examples=43793, total_duration=2162.515708, train/accuracy=0.988892, train/loss=0.037936, train/mean_average_precision=0.240215, validation/accuracy=0.985870, validation/loss=0.047750, validation/mean_average_precision=0.201052, validation/num_examples=43793
I0429 03:05:55.292031 139987026253568 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.06912346929311752, loss=0.0417131744325161
I0429 03:06:19.554990 139987017860864 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.057503778487443924, loss=0.043063435703516006
I0429 03:06:43.579606 139987026253568 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.043200600892305374, loss=0.03880681097507477
I0429 03:07:08.038190 139987017860864 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.036445435136556625, loss=0.043384112417697906
I0429 03:07:32.035980 139987026253568 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.06066909804940224, loss=0.041043225675821304
I0429 03:07:55.995739 139987017860864 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.059111855924129486, loss=0.04022713378071785
I0429 03:08:20.151000 139987026253568 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.07916929572820663, loss=0.04115213081240654
I0429 03:08:44.064848 139987017860864 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.059904515743255615, loss=0.0382547564804554
I0429 03:09:08.373221 139987026253568 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.048267725855112076, loss=0.04637633264064789
I0429 03:09:32.419522 139987017860864 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.06413911283016205, loss=0.03970910981297493
I0429 03:09:52.015809 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:11:06.469858 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:11:09.106137 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:11:11.628676 140172950476608 submission_runner.py:415] Time since start: 2482.24s, 	Step: 6882, 	{'train/accuracy': 0.9892810583114624, 'train/loss': 0.03675183653831482, 'train/mean_average_precision': 0.27069982396324865, 'validation/accuracy': 0.986020565032959, 'validation/loss': 0.0471947155892849, 'validation/mean_average_precision': 0.21357351199037353, 'validation/num_examples': 43793, 'test/accuracy': 0.9851014614105225, 'test/loss': 0.049878187477588654, 'test/mean_average_precision': 0.21224782266716583, 'test/num_examples': 43793, 'score': 1707.110920906067, 'total_duration': 2482.2359206676483, 'accumulated_submission_time': 1707.110920906067, 'accumulated_eval_time': 774.9273490905762, 'accumulated_logging_time': 0.1388404369354248}
I0429 03:11:11.636800 139987026253568 logging_writer.py:48] [6882] accumulated_eval_time=774.927349, accumulated_logging_time=0.138840, accumulated_submission_time=1707.110921, global_step=6882, preemption_count=0, score=1707.110921, test/accuracy=0.985101, test/loss=0.049878, test/mean_average_precision=0.212248, test/num_examples=43793, total_duration=2482.235921, train/accuracy=0.989281, train/loss=0.036752, train/mean_average_precision=0.270700, validation/accuracy=0.986021, validation/loss=0.047195, validation/mean_average_precision=0.213574, validation/num_examples=43793
I0429 03:11:16.313726 139987017860864 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.04359490051865578, loss=0.0388946533203125
I0429 03:11:40.578666 139987026253568 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.057477761059999466, loss=0.04086681455373764
I0429 03:12:04.634479 139987017860864 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.06506102532148361, loss=0.04129225015640259
I0429 03:12:28.780183 139987026253568 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.037870295345783234, loss=0.037364814430475235
I0429 03:12:52.786404 139987017860864 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03736566752195358, loss=0.03916795179247856
I0429 03:13:16.768551 139987026253568 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.03912215307354927, loss=0.037181366235017776
I0429 03:13:40.836713 139987017860864 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.034917041659355164, loss=0.03889458253979683
I0429 03:14:05.311502 139987026253568 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.027041343972086906, loss=0.035833343863487244
I0429 03:14:29.894299 139987017860864 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03997759521007538, loss=0.03676782175898552
I0429 03:14:54.527260 139987026253568 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.049727462232112885, loss=0.04167599976062775
I0429 03:15:11.873937 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:16:24.670861 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:16:27.269750 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:16:29.760298 140172950476608 submission_runner.py:415] Time since start: 2800.37s, 	Step: 7872, 	{'train/accuracy': 0.989453136920929, 'train/loss': 0.03616655245423317, 'train/mean_average_precision': 0.2963019092425617, 'validation/accuracy': 0.9860765933990479, 'validation/loss': 0.046551741659641266, 'validation/mean_average_precision': 0.2232731575831293, 'validation/num_examples': 43793, 'test/accuracy': 0.9851848483085632, 'test/loss': 0.04915203899145126, 'test/mean_average_precision': 0.22569891955659133, 'test/num_examples': 43793, 'score': 1947.3306016921997, 'total_duration': 2800.3675775527954, 'accumulated_submission_time': 1947.3306016921997, 'accumulated_eval_time': 852.8136835098267, 'accumulated_logging_time': 0.1560688018798828}
I0429 03:16:29.768566 139987017860864 logging_writer.py:48] [7872] accumulated_eval_time=852.813684, accumulated_logging_time=0.156069, accumulated_submission_time=1947.330602, global_step=7872, preemption_count=0, score=1947.330602, test/accuracy=0.985185, test/loss=0.049152, test/mean_average_precision=0.225699, test/num_examples=43793, total_duration=2800.367578, train/accuracy=0.989453, train/loss=0.036167, train/mean_average_precision=0.296302, validation/accuracy=0.986077, validation/loss=0.046552, validation/mean_average_precision=0.223273, validation/num_examples=43793
I0429 03:16:36.740718 139987026253568 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.05588303878903389, loss=0.03921020030975342
I0429 03:17:00.646370 139987017860864 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03463049605488777, loss=0.039495017379522324
I0429 03:17:24.486805 139987026253568 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.04586295038461685, loss=0.035315074026584625
I0429 03:17:48.649653 139987017860864 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.04950733110308647, loss=0.04204622656106949
I0429 03:18:12.843093 139987026253568 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.041815727949142456, loss=0.03909245878458023
I0429 03:18:36.906842 139987017860864 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.031080124899744987, loss=0.03767940402030945
I0429 03:19:01.435109 139987026253568 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.04153681918978691, loss=0.034738682210445404
I0429 03:19:25.867914 139987017860864 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.05433353781700134, loss=0.03982105851173401
I0429 03:19:50.338599 139987026253568 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.05219525843858719, loss=0.03739941492676735
I0429 03:20:14.717882 139987017860864 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.03406916931271553, loss=0.03487219288945198
I0429 03:20:29.830932 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:21:44.726481 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:21:47.322128 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:21:49.828542 140172950476608 submission_runner.py:415] Time since start: 3120.44s, 	Step: 8863, 	{'train/accuracy': 0.9900183081626892, 'train/loss': 0.03367453068494797, 'train/mean_average_precision': 0.3330292076850068, 'validation/accuracy': 0.9864626526832581, 'validation/loss': 0.045540791004896164, 'validation/mean_average_precision': 0.24379242915874577, 'validation/num_examples': 43793, 'test/accuracy': 0.9854853749275208, 'test/loss': 0.04827471077442169, 'test/mean_average_precision': 0.23654541157130224, 'test/num_examples': 43793, 'score': 2187.3739976882935, 'total_duration': 3120.4358048439026, 'accumulated_submission_time': 2187.3739976882935, 'accumulated_eval_time': 932.8112485408783, 'accumulated_logging_time': 0.17505478858947754}
I0429 03:21:49.836799 139987026253568 logging_writer.py:48] [8863] accumulated_eval_time=932.811249, accumulated_logging_time=0.175055, accumulated_submission_time=2187.373998, global_step=8863, preemption_count=0, score=2187.373998, test/accuracy=0.985485, test/loss=0.048275, test/mean_average_precision=0.236545, test/num_examples=43793, total_duration=3120.435805, train/accuracy=0.990018, train/loss=0.033675, train/mean_average_precision=0.333029, validation/accuracy=0.986463, validation/loss=0.045541, validation/mean_average_precision=0.243792, validation/num_examples=43793
I0429 03:21:58.955713 139987017860864 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.04086697846651077, loss=0.03949182108044624
I0429 03:22:23.488588 139987026253568 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.038396384567022324, loss=0.03710572421550751
I0429 03:22:48.138911 139987017860864 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0407445915043354, loss=0.03957540541887283
I0429 03:23:12.437529 139987026253568 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.036881137639284134, loss=0.037930481135845184
I0429 03:23:36.853972 139987017860864 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.04134432598948479, loss=0.0362369567155838
I0429 03:24:01.463437 139987026253568 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.052989210933446884, loss=0.03746872395277023
I0429 03:24:25.850538 139987017860864 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.04239941015839577, loss=0.03918565809726715
I0429 03:24:49.967877 139987026253568 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03180404379963875, loss=0.033968236297369
I0429 03:25:14.112760 139987017860864 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.041238948702812195, loss=0.03861505910754204
I0429 03:25:38.448434 139987026253568 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.051197025924921036, loss=0.0400468073785305
I0429 03:25:49.960367 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:27:01.352740 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:27:03.976033 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:27:06.537661 140172950476608 submission_runner.py:415] Time since start: 3437.14s, 	Step: 9848, 	{'train/accuracy': 0.9900870323181152, 'train/loss': 0.03335465118288994, 'train/mean_average_precision': 0.34751768170174374, 'validation/accuracy': 0.9864577651023865, 'validation/loss': 0.04586412012577057, 'validation/mean_average_precision': 0.24318124381761178, 'validation/num_examples': 43793, 'test/accuracy': 0.9855639338493347, 'test/loss': 0.04882890731096268, 'test/mean_average_precision': 0.23628665282344039, 'test/num_examples': 43793, 'score': 2427.4786863327026, 'total_duration': 3437.144919872284, 'accumulated_submission_time': 2427.4786863327026, 'accumulated_eval_time': 1009.38849568367, 'accumulated_logging_time': 0.19356536865234375}
I0429 03:27:06.545849 139987017860864 logging_writer.py:48] [9848] accumulated_eval_time=1009.388496, accumulated_logging_time=0.193565, accumulated_submission_time=2427.478686, global_step=9848, preemption_count=0, score=2427.478686, test/accuracy=0.985564, test/loss=0.048829, test/mean_average_precision=0.236287, test/num_examples=43793, total_duration=3437.144920, train/accuracy=0.990087, train/loss=0.033355, train/mean_average_precision=0.347518, validation/accuracy=0.986458, validation/loss=0.045864, validation/mean_average_precision=0.243181, validation/num_examples=43793
I0429 03:27:19.351177 139987026253568 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.031798310577869415, loss=0.036042891442775726
I0429 03:27:43.694034 139987017860864 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.04298299178481102, loss=0.03594384342432022
I0429 03:28:07.904724 139987026253568 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.041025660932064056, loss=0.03850744664669037
I0429 03:28:32.151191 139987017860864 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.04330156370997429, loss=0.035906124860048294
I0429 03:28:56.314949 139987026253568 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04482957348227501, loss=0.03903994709253311
I0429 03:29:20.355531 139987017860864 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.029560839757323265, loss=0.03373702988028526
I0429 03:29:44.770663 139987026253568 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.05912571772933006, loss=0.03688015788793564
I0429 03:30:09.098950 139987017860864 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.042244475334882736, loss=0.04039681330323219
I0429 03:30:33.328144 139987026253568 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.048933591693639755, loss=0.03520890325307846
I0429 03:30:57.695748 139987017860864 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03629238158464432, loss=0.03654284030199051
I0429 03:31:06.728555 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:32:19.361888 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:32:21.979244 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:32:24.533324 140172950476608 submission_runner.py:415] Time since start: 3755.14s, 	Step: 10838, 	{'train/accuracy': 0.990231990814209, 'train/loss': 0.03267614170908928, 'train/mean_average_precision': 0.36813371967460284, 'validation/accuracy': 0.9865121841430664, 'validation/loss': 0.04526502639055252, 'validation/mean_average_precision': 0.2458374552266383, 'validation/num_examples': 43793, 'test/accuracy': 0.9855837225914001, 'test/loss': 0.048091139644384384, 'test/mean_average_precision': 0.24256714821646597, 'test/num_examples': 43793, 'score': 2667.6439604759216, 'total_duration': 3755.1405866146088, 'accumulated_submission_time': 2667.6439604759216, 'accumulated_eval_time': 1087.1932127475739, 'accumulated_logging_time': 0.21080613136291504}
I0429 03:32:24.541714 139987026253568 logging_writer.py:48] [10838] accumulated_eval_time=1087.193213, accumulated_logging_time=0.210806, accumulated_submission_time=2667.643960, global_step=10838, preemption_count=0, score=2667.643960, test/accuracy=0.985584, test/loss=0.048091, test/mean_average_precision=0.242567, test/num_examples=43793, total_duration=3755.140587, train/accuracy=0.990232, train/loss=0.032676, train/mean_average_precision=0.368134, validation/accuracy=0.986512, validation/loss=0.045265, validation/mean_average_precision=0.245837, validation/num_examples=43793
I0429 03:32:39.888386 139987017860864 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03703795000910759, loss=0.03600964695215225
I0429 03:33:04.318596 139987026253568 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0464167483150959, loss=0.03559127822518349
I0429 03:33:28.624123 139987017860864 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.032969675958156586, loss=0.03352928161621094
I0429 03:33:53.348135 139987026253568 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.053078796714544296, loss=0.03640706464648247
I0429 03:34:18.173397 139987017860864 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.038439176976680756, loss=0.035633888095617294
I0429 03:34:42.768737 139987026253568 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04895637929439545, loss=0.03571674972772598
I0429 03:35:07.461190 139987017860864 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.040128111839294434, loss=0.03686915710568428
I0429 03:35:32.117560 139987026253568 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.035411983728408813, loss=0.03473521023988724
I0429 03:35:56.723057 139987017860864 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03869767114520073, loss=0.03414696082472801
I0429 03:36:21.071422 139987026253568 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03361956775188446, loss=0.03527083620429039
I0429 03:36:24.709840 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:37:36.646220 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:37:39.266093 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:37:41.815935 140172950476608 submission_runner.py:415] Time since start: 4072.42s, 	Step: 11816, 	{'train/accuracy': 0.9905137419700623, 'train/loss': 0.031792037189006805, 'train/mean_average_precision': 0.38081064340277687, 'validation/accuracy': 0.9865572452545166, 'validation/loss': 0.04492660239338875, 'validation/mean_average_precision': 0.24758165382411443, 'validation/num_examples': 43793, 'test/accuracy': 0.9856886267662048, 'test/loss': 0.04772837832570076, 'test/mean_average_precision': 0.2440532633419902, 'test/num_examples': 43793, 'score': 2907.794383764267, 'total_duration': 4072.4231424331665, 'accumulated_submission_time': 2907.794383764267, 'accumulated_eval_time': 1164.2992033958435, 'accumulated_logging_time': 0.22856760025024414}
I0429 03:37:41.824512 139987017860864 logging_writer.py:48] [11816] accumulated_eval_time=1164.299203, accumulated_logging_time=0.228568, accumulated_submission_time=2907.794384, global_step=11816, preemption_count=0, score=2907.794384, test/accuracy=0.985689, test/loss=0.047728, test/mean_average_precision=0.244053, test/num_examples=43793, total_duration=4072.423142, train/accuracy=0.990514, train/loss=0.031792, train/mean_average_precision=0.380811, validation/accuracy=0.986557, validation/loss=0.044927, validation/mean_average_precision=0.247582, validation/num_examples=43793
I0429 03:38:02.521354 139987026253568 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.031157437711954117, loss=0.03236773610115051
I0429 03:38:26.303688 140172950476608 spec.py:298] Evaluating on the training split.
I0429 03:39:37.919076 140172950476608 spec.py:310] Evaluating on the validation split.
I0429 03:39:40.550716 140172950476608 spec.py:326] Evaluating on the test split.
I0429 03:39:43.129936 140172950476608 submission_runner.py:415] Time since start: 4193.74s, 	Step: 12000, 	{'train/accuracy': 0.9903889298439026, 'train/loss': 0.03184239938855171, 'train/mean_average_precision': 0.39451836069622204, 'validation/accuracy': 0.9865917563438416, 'validation/loss': 0.04576314985752106, 'validation/mean_average_precision': 0.2503830124131846, 'validation/num_examples': 43793, 'test/accuracy': 0.9856553673744202, 'test/loss': 0.048723235726356506, 'test/mean_average_precision': 0.24551746734124166, 'test/num_examples': 43793, 'score': 2952.262408733368, 'total_duration': 4193.737215042114, 'accumulated_submission_time': 2952.262408733368, 'accumulated_eval_time': 1241.1254153251648, 'accumulated_logging_time': 0.24651074409484863}
I0429 03:39:43.138577 139987017860864 logging_writer.py:48] [12000] accumulated_eval_time=1241.125415, accumulated_logging_time=0.246511, accumulated_submission_time=2952.262409, global_step=12000, preemption_count=0, score=2952.262409, test/accuracy=0.985655, test/loss=0.048723, test/mean_average_precision=0.245517, test/num_examples=43793, total_duration=4193.737215, train/accuracy=0.990389, train/loss=0.031842, train/mean_average_precision=0.394518, validation/accuracy=0.986592, validation/loss=0.045763, validation/mean_average_precision=0.250383, validation/num_examples=43793
I0429 03:39:43.154221 139987026253568 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2952.262409
I0429 03:39:43.180832 140172950476608 checkpoints.py:356] Saving checkpoint at step: 12000
I0429 03:39:43.273594 140172950476608 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1/checkpoint_12000
I0429 03:39:43.274159 140172950476608 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_lamb/ogbg_jax/trial_1/checkpoint_12000.
I0429 03:39:43.436901 140172950476608 submission_runner.py:578] Tuning trial 1/1
I0429 03:39:43.437141 140172950476608 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 03:39:43.438556 140172950476608 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5278680920600891, 'train/loss': 0.7312097549438477, 'train/mean_average_precision': 0.021113802074181197, 'validation/accuracy': 0.5310146808624268, 'validation/loss': 0.7295722961425781, 'validation/mean_average_precision': 0.026691026564612353, 'validation/num_examples': 43793, 'test/accuracy': 0.5329578518867493, 'test/loss': 0.7286767959594727, 'test/mean_average_precision': 0.027852736853321666, 'test/num_examples': 43793, 'score': 26.278934001922607, 'total_duration': 243.82895708084106, 'accumulated_submission_time': 26.278934001922607, 'accumulated_eval_time': 217.5498549938202, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (990, {'train/accuracy': 0.9868677258491516, 'train/loss': 0.05501144006848335, 'train/mean_average_precision': 0.04865692826348793, 'validation/accuracy': 0.984278678894043, 'validation/loss': 0.06431061029434204, 'validation/mean_average_precision': 0.04847660524505075, 'validation/num_examples': 43793, 'test/accuracy': 0.983282744884491, 'test/loss': 0.06743130832910538, 'test/mean_average_precision': 0.049513291200891145, 'test/num_examples': 43793, 'score': 266.32204389572144, 'total_duration': 562.772260427475, 'accumulated_submission_time': 266.32204389572144, 'accumulated_eval_time': 296.41456294059753, 'accumulated_logging_time': 0.027406692504882812, 'global_step': 990, 'preemption_count': 0}), (1977, {'train/accuracy': 0.9872841238975525, 'train/loss': 0.0481090173125267, 'train/mean_average_precision': 0.09437767865881272, 'validation/accuracy': 0.9845737814903259, 'validation/loss': 0.05802327021956444, 'validation/mean_average_precision': 0.09303609997152006, 'validation/num_examples': 43793, 'test/accuracy': 0.9835758805274963, 'test/loss': 0.061414893716573715, 'test/mean_average_precision': 0.09217892943166313, 'test/num_examples': 43793, 'score': 506.5017046928406, 'total_duration': 882.4284811019897, 'accumulated_submission_time': 506.5017046928406, 'accumulated_eval_time': 375.86513471603394, 'accumulated_logging_time': 0.045046091079711914, 'global_step': 1977, 'preemption_count': 0}), (2950, {'train/accuracy': 0.9875606298446655, 'train/loss': 0.04541366919875145, 'train/mean_average_precision': 0.1324003887249699, 'validation/accuracy': 0.9848458170890808, 'validation/loss': 0.05530287325382233, 'validation/mean_average_precision': 0.12952113067833662, 'validation/num_examples': 43793, 'test/accuracy': 0.9838648438453674, 'test/loss': 0.05865506827831268, 'test/mean_average_precision': 0.12569594545095475, 'test/num_examples': 43793, 'score': 746.7105331420898, 'total_duration': 1201.946100473404, 'accumulated_submission_time': 746.7105331420898, 'accumulated_eval_time': 455.148232460022, 'accumulated_logging_time': 0.06241250038146973, 'global_step': 2950, 'preemption_count': 0}), (3921, {'train/accuracy': 0.9882091879844666, 'train/loss': 0.04135122895240784, 'train/mean_average_precision': 0.17064978769129563, 'validation/accuracy': 0.9853544235229492, 'validation/loss': 0.05084559693932533, 'validation/mean_average_precision': 0.16089689963686712, 'validation/num_examples': 43793, 'test/accuracy': 0.9843711256980896, 'test/loss': 0.05361801013350487, 'test/mean_average_precision': 0.16189840707544728, 'test/num_examples': 43793, 'score': 986.7797102928162, 'total_duration': 1521.9608376026154, 'accumulated_submission_time': 986.7797102928162, 'accumulated_eval_time': 535.0669198036194, 'accumulated_logging_time': 0.08080935478210449, 'global_step': 3921, 'preemption_count': 0}), (4903, {'train/accuracy': 0.9885743260383606, 'train/loss': 0.03940508887171745, 'train/mean_average_precision': 0.2247011062738582, 'validation/accuracy': 0.9855732321739197, 'validation/loss': 0.04935610294342041, 'validation/mean_average_precision': 0.18179570877159723, 'validation/num_examples': 43793, 'test/accuracy': 0.9846507906913757, 'test/loss': 0.05191381648182869, 'test/mean_average_precision': 0.18282126396147053, 'test/num_examples': 43793, 'score': 1226.9133729934692, 'total_duration': 1842.0598032474518, 'accumulated_submission_time': 1226.9133729934692, 'accumulated_eval_time': 615.0056929588318, 'accumulated_logging_time': 0.09881138801574707, 'global_step': 4903, 'preemption_count': 0}), (5887, {'train/accuracy': 0.9888923168182373, 'train/loss': 0.03793647512793541, 'train/mean_average_precision': 0.24021499381146638, 'validation/accuracy': 0.985870361328125, 'validation/loss': 0.04774964973330498, 'validation/mean_average_precision': 0.20105228667646197, 'validation/num_examples': 43793, 'test/accuracy': 0.984968364238739, 'test/loss': 0.050444621592760086, 'test/mean_average_precision': 0.20695311065874858, 'test/num_examples': 43793, 'score': 1467.0294451713562, 'total_duration': 2162.5157079696655, 'accumulated_submission_time': 1467.0294451713562, 'accumulated_eval_time': 695.3145515918732, 'accumulated_logging_time': 0.12133955955505371, 'global_step': 5887, 'preemption_count': 0}), (6882, {'train/accuracy': 0.9892810583114624, 'train/loss': 0.03675183653831482, 'train/mean_average_precision': 0.27069982396324865, 'validation/accuracy': 0.986020565032959, 'validation/loss': 0.0471947155892849, 'validation/mean_average_precision': 0.21357351199037353, 'validation/num_examples': 43793, 'test/accuracy': 0.9851014614105225, 'test/loss': 0.049878187477588654, 'test/mean_average_precision': 0.21224782266716583, 'test/num_examples': 43793, 'score': 1707.110920906067, 'total_duration': 2482.2359206676483, 'accumulated_submission_time': 1707.110920906067, 'accumulated_eval_time': 774.9273490905762, 'accumulated_logging_time': 0.1388404369354248, 'global_step': 6882, 'preemption_count': 0}), (7872, {'train/accuracy': 0.989453136920929, 'train/loss': 0.03616655245423317, 'train/mean_average_precision': 0.2963019092425617, 'validation/accuracy': 0.9860765933990479, 'validation/loss': 0.046551741659641266, 'validation/mean_average_precision': 0.2232731575831293, 'validation/num_examples': 43793, 'test/accuracy': 0.9851848483085632, 'test/loss': 0.04915203899145126, 'test/mean_average_precision': 0.22569891955659133, 'test/num_examples': 43793, 'score': 1947.3306016921997, 'total_duration': 2800.3675775527954, 'accumulated_submission_time': 1947.3306016921997, 'accumulated_eval_time': 852.8136835098267, 'accumulated_logging_time': 0.1560688018798828, 'global_step': 7872, 'preemption_count': 0}), (8863, {'train/accuracy': 0.9900183081626892, 'train/loss': 0.03367453068494797, 'train/mean_average_precision': 0.3330292076850068, 'validation/accuracy': 0.9864626526832581, 'validation/loss': 0.045540791004896164, 'validation/mean_average_precision': 0.24379242915874577, 'validation/num_examples': 43793, 'test/accuracy': 0.9854853749275208, 'test/loss': 0.04827471077442169, 'test/mean_average_precision': 0.23654541157130224, 'test/num_examples': 43793, 'score': 2187.3739976882935, 'total_duration': 3120.4358048439026, 'accumulated_submission_time': 2187.3739976882935, 'accumulated_eval_time': 932.8112485408783, 'accumulated_logging_time': 0.17505478858947754, 'global_step': 8863, 'preemption_count': 0}), (9848, {'train/accuracy': 0.9900870323181152, 'train/loss': 0.03335465118288994, 'train/mean_average_precision': 0.34751768170174374, 'validation/accuracy': 0.9864577651023865, 'validation/loss': 0.04586412012577057, 'validation/mean_average_precision': 0.24318124381761178, 'validation/num_examples': 43793, 'test/accuracy': 0.9855639338493347, 'test/loss': 0.04882890731096268, 'test/mean_average_precision': 0.23628665282344039, 'test/num_examples': 43793, 'score': 2427.4786863327026, 'total_duration': 3437.144919872284, 'accumulated_submission_time': 2427.4786863327026, 'accumulated_eval_time': 1009.38849568367, 'accumulated_logging_time': 0.19356536865234375, 'global_step': 9848, 'preemption_count': 0}), (10838, {'train/accuracy': 0.990231990814209, 'train/loss': 0.03267614170908928, 'train/mean_average_precision': 0.36813371967460284, 'validation/accuracy': 0.9865121841430664, 'validation/loss': 0.04526502639055252, 'validation/mean_average_precision': 0.2458374552266383, 'validation/num_examples': 43793, 'test/accuracy': 0.9855837225914001, 'test/loss': 0.048091139644384384, 'test/mean_average_precision': 0.24256714821646597, 'test/num_examples': 43793, 'score': 2667.6439604759216, 'total_duration': 3755.1405866146088, 'accumulated_submission_time': 2667.6439604759216, 'accumulated_eval_time': 1087.1932127475739, 'accumulated_logging_time': 0.21080613136291504, 'global_step': 10838, 'preemption_count': 0}), (11816, {'train/accuracy': 0.9905137419700623, 'train/loss': 0.031792037189006805, 'train/mean_average_precision': 0.38081064340277687, 'validation/accuracy': 0.9865572452545166, 'validation/loss': 0.04492660239338875, 'validation/mean_average_precision': 0.24758165382411443, 'validation/num_examples': 43793, 'test/accuracy': 0.9856886267662048, 'test/loss': 0.04772837832570076, 'test/mean_average_precision': 0.2440532633419902, 'test/num_examples': 43793, 'score': 2907.794383764267, 'total_duration': 4072.4231424331665, 'accumulated_submission_time': 2907.794383764267, 'accumulated_eval_time': 1164.2992033958435, 'accumulated_logging_time': 0.22856760025024414, 'global_step': 11816, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9903889298439026, 'train/loss': 0.03184239938855171, 'train/mean_average_precision': 0.39451836069622204, 'validation/accuracy': 0.9865917563438416, 'validation/loss': 0.04576314985752106, 'validation/mean_average_precision': 0.2503830124131846, 'validation/num_examples': 43793, 'test/accuracy': 0.9856553673744202, 'test/loss': 0.048723235726356506, 'test/mean_average_precision': 0.24551746734124166, 'test/num_examples': 43793, 'score': 2952.262408733368, 'total_duration': 4193.737215042114, 'accumulated_submission_time': 2952.262408733368, 'accumulated_eval_time': 1241.1254153251648, 'accumulated_logging_time': 0.24651074409484863, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0429 03:39:43.438724 140172950476608 submission_runner.py:581] Timing: 2952.262408733368
I0429 03:39:43.438769 140172950476608 submission_runner.py:582] ====================
I0429 03:39:43.438880 140172950476608 submission_runner.py:645] Final ogbg score: 2952.262408733368
