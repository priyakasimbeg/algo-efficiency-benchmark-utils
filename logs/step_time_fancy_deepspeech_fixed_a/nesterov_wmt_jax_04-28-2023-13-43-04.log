python3 submission_runner.py --framework=jax --workload=wmt --submission_path=baselines/nesterov/jax/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_nesterov --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_jax_04-28-2023-13-43-04.log
I0428 13:43:26.849473 140088188094272 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_nesterov/wmt_jax.
I0428 13:43:26.919924 140088188094272 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 13:43:27.761139 140088188094272 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0428 13:43:27.761935 140088188094272 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 13:43:27.766306 140088188094272 submission_runner.py:538] Using RNG seed 1045714493
I0428 13:43:30.394246 140088188094272 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 13:43:30.394459 140088188094272 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1.
I0428 13:43:30.394651 140088188094272 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1/hparams.json.
I0428 13:43:30.517910 140088188094272 submission_runner.py:241] Initializing dataset.
I0428 13:43:30.526734 140088188094272 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 13:43:30.540823 140088188094272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 13:43:30.540961 140088188094272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 13:43:30.658716 140088188094272 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 13:43:32.579682 140088188094272 submission_runner.py:248] Initializing model.
I0428 13:43:44.708073 140088188094272 submission_runner.py:258] Initializing optimizer.
I0428 13:43:45.295098 140088188094272 submission_runner.py:265] Initializing metrics bundle.
I0428 13:43:45.295284 140088188094272 submission_runner.py:282] Initializing checkpoint and logger.
I0428 13:43:45.296192 140088188094272 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1 with prefix checkpoint_
I0428 13:43:45.296455 140088188094272 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 13:43:45.296516 140088188094272 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 13:43:46.188166 140088188094272 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1/meta_data_0.json.
I0428 13:43:46.189231 140088188094272 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1/flags_0.json.
I0428 13:43:46.193279 140088188094272 submission_runner.py:318] Starting training loop.
I0428 13:44:16.832778 139911873160960 logging_writer.py:48] [0] global_step=0, grad_norm=4.657358169555664, loss=11.040104866027832
I0428 13:44:16.845708 140088188094272 spec.py:298] Evaluating on the training split.
I0428 13:44:16.848253 140088188094272 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 13:44:16.850881 140088188094272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 13:44:16.851047 140088188094272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 13:44:16.881649 140088188094272 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0428 13:44:25.139045 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 13:49:18.982281 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 13:49:18.986247 140088188094272 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 13:49:18.989915 140088188094272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 13:49:18.990054 140088188094272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 13:49:19.020250 140088188094272 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 13:49:26.496446 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 13:54:13.284088 140088188094272 spec.py:326] Evaluating on the test split.
I0428 13:54:13.286525 140088188094272 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 13:54:13.289110 140088188094272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 13:54:13.289222 140088188094272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 13:54:13.319924 140088188094272 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0428 13:54:20.169468 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 13:59:01.351143 140088188094272 submission_runner.py:415] Time since start: 915.16s, 	Step: 1, 	{'train/accuracy': 0.0005968915647827089, 'train/loss': 11.008682250976562, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.031411170959473, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.011260986328125, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.652264833450317, 'total_duration': 915.1577713489532, 'accumulated_submission_time': 30.652264833450317, 'accumulated_eval_time': 884.5053508281708, 'accumulated_logging_time': 0}
I0428 13:59:01.370025 139900715828992 logging_writer.py:48] [1] accumulated_eval_time=884.505351, accumulated_logging_time=0, accumulated_submission_time=30.652265, global_step=1, preemption_count=0, score=30.652265, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.011261, test/num_examples=3003, total_duration=915.157771, train/accuracy=0.000597, train/bleu=0.000000, train/loss=11.008682, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.031411, validation/num_examples=3000
I0428 13:59:37.166555 139900724221696 logging_writer.py:48] [100] global_step=100, grad_norm=0.16602855920791626, loss=9.056611061096191
I0428 14:00:12.947244 139900715828992 logging_writer.py:48] [200] global_step=200, grad_norm=0.12489040940999985, loss=8.919934272766113
I0428 14:00:48.817556 139900724221696 logging_writer.py:48] [300] global_step=300, grad_norm=0.1285015046596527, loss=8.779584884643555
I0428 14:01:24.693404 139900715828992 logging_writer.py:48] [400] global_step=400, grad_norm=0.18059797585010529, loss=8.66421127319336
I0428 14:02:00.545794 139900724221696 logging_writer.py:48] [500] global_step=500, grad_norm=0.5571776628494263, loss=8.465163230895996
I0428 14:02:36.381376 139900715828992 logging_writer.py:48] [600] global_step=600, grad_norm=1.5578593015670776, loss=8.307367324829102
I0428 14:03:12.202304 139900724221696 logging_writer.py:48] [700] global_step=700, grad_norm=0.583650529384613, loss=8.14757251739502
I0428 14:03:48.027723 139900715828992 logging_writer.py:48] [800] global_step=800, grad_norm=0.768536388874054, loss=8.025734901428223
I0428 14:04:23.841761 139900724221696 logging_writer.py:48] [900] global_step=900, grad_norm=0.6554908156394958, loss=7.926802158355713
I0428 14:04:59.699326 139900715828992 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7180324196815491, loss=7.861419677734375
I0428 14:05:35.536107 139900724221696 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5550933480262756, loss=7.7268829345703125
I0428 14:06:11.399760 139900715828992 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.719404935836792, loss=7.63730525970459
I0428 14:06:47.287662 139900724221696 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6959388852119446, loss=7.614664077758789
I0428 14:07:23.158607 139900715828992 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6931189894676208, loss=7.488893985748291
I0428 14:07:59.003080 139900724221696 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8355914950370789, loss=7.4515581130981445
I0428 14:08:34.870739 139900715828992 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7168281078338623, loss=7.375406265258789
I0428 14:09:10.764433 139900724221696 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9124073386192322, loss=7.299575328826904
I0428 14:09:46.601197 139900715828992 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7883011698722839, loss=7.1686906814575195
I0428 14:10:22.473439 139900724221696 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.848242461681366, loss=7.122432231903076
I0428 14:10:58.335231 139900715828992 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9092828035354614, loss=7.085926532745361
I0428 14:11:34.166038 139900724221696 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.792026698589325, loss=6.949679851531982
I0428 14:12:10.002182 139900715828992 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.717156708240509, loss=6.932007789611816
I0428 14:12:45.860854 139900724221696 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6534447073936462, loss=6.791719913482666
I0428 14:13:01.711998 140088188094272 spec.py:298] Evaluating on the training split.
I0428 14:13:04.712835 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 14:17:36.833776 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 14:17:39.490753 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 14:22:08.308644 140088188094272 spec.py:326] Evaluating on the test split.
I0428 14:22:11.036146 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 14:26:50.668074 140088188094272 submission_runner.py:415] Time since start: 2584.47s, 	Step: 2346, 	{'train/accuracy': 0.3049754202365875, 'train/loss': 5.55180025100708, 'train/bleu': 6.278616102473807, 'validation/accuracy': 0.2819555699825287, 'validation/loss': 5.813656806945801, 'validation/bleu': 3.5052125931010347, 'validation/num_examples': 3000, 'test/accuracy': 0.2622857391834259, 'test/loss': 6.10167932510376, 'test/bleu': 2.619755183998621, 'test/num_examples': 3003, 'score': 870.9549169540405, 'total_duration': 2584.4747166633606, 'accumulated_submission_time': 870.9549169540405, 'accumulated_eval_time': 1713.4613864421844, 'accumulated_logging_time': 0.02757120132446289}
I0428 14:26:50.676971 139900732614400 logging_writer.py:48] [2346] accumulated_eval_time=1713.461386, accumulated_logging_time=0.027571, accumulated_submission_time=870.954917, global_step=2346, preemption_count=0, score=870.954917, test/accuracy=0.262286, test/bleu=2.619755, test/loss=6.101679, test/num_examples=3003, total_duration=2584.474717, train/accuracy=0.304975, train/bleu=6.278616, train/loss=5.551800, validation/accuracy=0.281956, validation/bleu=3.505213, validation/loss=5.813657, validation/num_examples=3000
I0428 14:27:10.408203 139900741007104 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9129140377044678, loss=6.837949275970459
I0428 14:27:46.274867 139900732614400 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6419569253921509, loss=6.755893707275391
I0428 14:28:22.161593 139900741007104 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9515591263771057, loss=6.678528308868408
I0428 14:28:57.996581 139900732614400 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6833342909812927, loss=6.64222526550293
I0428 14:29:33.815979 139900741007104 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6774167418479919, loss=6.594300270080566
I0428 14:30:09.643745 139900732614400 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5980185866355896, loss=6.524947643280029
I0428 14:30:45.482937 139900741007104 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7907387614250183, loss=6.45305871963501
I0428 14:31:21.306954 139900732614400 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.807360827922821, loss=6.399468421936035
I0428 14:31:57.146170 139900741007104 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6497536897659302, loss=6.299741744995117
I0428 14:32:32.990398 139900732614400 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7841693758964539, loss=6.2655229568481445
I0428 14:33:08.828851 139900741007104 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.663861095905304, loss=6.266549110412598
I0428 14:33:44.661413 139900732614400 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6355364918708801, loss=6.134181499481201
I0428 14:34:20.528618 139900741007104 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5901913046836853, loss=6.148494720458984
I0428 14:34:56.399864 139900732614400 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7279039025306702, loss=6.029318332672119
I0428 14:35:32.237067 139900741007104 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7172038555145264, loss=6.061103343963623
I0428 14:36:08.075754 139900732614400 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5733051896095276, loss=5.987088680267334
I0428 14:36:43.951615 139900741007104 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8373372554779053, loss=5.968045234680176
I0428 14:37:19.797625 139900732614400 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5522570610046387, loss=5.862324237823486
I0428 14:37:55.667818 139900741007104 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6578179001808167, loss=5.812407970428467
I0428 14:38:31.550109 139900732614400 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.867609977722168, loss=5.752222537994385
I0428 14:39:07.380815 139900741007104 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6228039860725403, loss=5.7318644523620605
I0428 14:39:43.242635 139900732614400 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.582341194152832, loss=5.570957183837891
I0428 14:40:19.118523 139900741007104 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5717870593070984, loss=5.6394243240356445
I0428 14:40:50.710625 140088188094272 spec.py:298] Evaluating on the training split.
I0428 14:40:53.709516 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 14:43:57.779624 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 14:44:00.443772 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 14:46:48.379477 140088188094272 spec.py:326] Evaluating on the test split.
I0428 14:46:51.094834 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 14:49:29.922229 140088188094272 submission_runner.py:415] Time since start: 3943.73s, 	Step: 4690, 	{'train/accuracy': 0.45951518416404724, 'train/loss': 3.8847429752349854, 'train/bleu': 17.64822150160748, 'validation/accuracy': 0.4485127329826355, 'validation/loss': 3.968470573425293, 'validation/bleu': 13.042222086072792, 'validation/num_examples': 3000, 'test/accuracy': 0.4397187829017639, 'test/loss': 4.111972808837891, 'test/bleu': 11.601581408927313, 'test/num_examples': 3003, 'score': 1710.9479629993439, 'total_duration': 3943.7288670539856, 'accumulated_submission_time': 1710.9479629993439, 'accumulated_eval_time': 2232.6729652881622, 'accumulated_logging_time': 0.04538154602050781}
I0428 14:49:29.932872 139900732614400 logging_writer.py:48] [4690] accumulated_eval_time=2232.672965, accumulated_logging_time=0.045382, accumulated_submission_time=1710.947963, global_step=4690, preemption_count=0, score=1710.947963, test/accuracy=0.439719, test/bleu=11.601581, test/loss=4.111973, test/num_examples=3003, total_duration=3943.728867, train/accuracy=0.459515, train/bleu=17.648222, train/loss=3.884743, validation/accuracy=0.448513, validation/bleu=13.042222, validation/loss=3.968471, validation/num_examples=3000
I0428 14:49:33.879306 139900741007104 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6095002293586731, loss=5.557967662811279
I0428 14:50:09.668454 139900732614400 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5494917631149292, loss=5.490204811096191
I0428 14:50:45.522508 139900741007104 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.547760009765625, loss=5.498784065246582
I0428 14:51:21.372964 139900732614400 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.539059579372406, loss=5.469799518585205
I0428 14:51:57.200739 139900741007104 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5497651100158691, loss=5.3806562423706055
I0428 14:52:33.043294 139900732614400 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5607271194458008, loss=5.438522815704346
I0428 14:53:08.884679 139900741007104 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6483418941497803, loss=5.328557014465332
I0428 14:53:44.718255 139900732614400 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.49400413036346436, loss=5.380989074707031
I0428 14:54:20.551934 139900741007104 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5336867570877075, loss=5.343876838684082
I0428 14:54:56.383585 139900732614400 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6048120260238647, loss=5.316056251525879
I0428 14:55:32.211499 139900741007104 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.48690083622932434, loss=5.320964813232422
I0428 14:56:08.048228 139900732614400 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5497424006462097, loss=5.204623222351074
I0428 14:56:43.885506 139900741007104 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4861924648284912, loss=5.272374153137207
I0428 14:57:19.739600 139900732614400 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5318835377693176, loss=5.18217658996582
I0428 14:57:55.594337 139900741007104 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.506507933139801, loss=5.115418910980225
I0428 14:58:31.404189 139900732614400 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.45207494497299194, loss=5.117866516113281
I0428 14:59:07.245612 139900741007104 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4651639759540558, loss=5.094756126403809
I0428 14:59:43.086717 139900732614400 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.44586965441703796, loss=5.181424140930176
I0428 15:00:18.959993 139900741007104 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.47657790780067444, loss=5.064119338989258
I0428 15:00:54.845976 139900732614400 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.45090925693511963, loss=5.027101993560791
I0428 15:01:30.660100 139900741007104 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4700717329978943, loss=5.128161430358887
I0428 15:02:06.464226 139900732614400 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.44292715191841125, loss=5.068649768829346
I0428 15:02:42.304745 139900741007104 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4393834173679352, loss=5.017090797424316
I0428 15:03:18.105408 139900732614400 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4795078933238983, loss=5.078934669494629
I0428 15:03:29.997713 140088188094272 spec.py:298] Evaluating on the training split.
I0428 15:03:32.997482 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:05:59.765514 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 15:06:02.435937 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:08:30.976311 140088188094272 spec.py:326] Evaluating on the test split.
I0428 15:08:33.680913 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:10:51.664832 140088188094272 submission_runner.py:415] Time since start: 5225.47s, 	Step: 7035, 	{'train/accuracy': 0.5399675369262695, 'train/loss': 3.135305643081665, 'train/bleu': 23.07175898486892, 'validation/accuracy': 0.5325290560722351, 'validation/loss': 3.1738529205322266, 'validation/bleu': 19.19782104630395, 'validation/num_examples': 3000, 'test/accuracy': 0.5293591618537903, 'test/loss': 3.2300455570220947, 'test/bleu': 17.779616726279002, 'test/num_examples': 3003, 'score': 2550.969848871231, 'total_duration': 5225.471460580826, 'accumulated_submission_time': 2550.969848871231, 'accumulated_eval_time': 2674.3400292396545, 'accumulated_logging_time': 0.06692123413085938}
I0428 15:10:51.673772 139900741007104 logging_writer.py:48] [7035] accumulated_eval_time=2674.340029, accumulated_logging_time=0.066921, accumulated_submission_time=2550.969849, global_step=7035, preemption_count=0, score=2550.969849, test/accuracy=0.529359, test/bleu=17.779617, test/loss=3.230046, test/num_examples=3003, total_duration=5225.471461, train/accuracy=0.539968, train/bleu=23.071759, train/loss=3.135306, validation/accuracy=0.532529, validation/bleu=19.197821, validation/loss=3.173853, validation/num_examples=3000
I0428 15:11:15.289126 139900732614400 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.445205420255661, loss=5.014587879180908
I0428 15:11:51.090384 139900741007104 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4102095365524292, loss=5.004655361175537
I0428 15:12:26.958952 139900732614400 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4284118413925171, loss=4.951016426086426
I0428 15:13:02.795812 139900741007104 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.38296446204185486, loss=5.009814739227295
I0428 15:13:38.608610 139900732614400 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.42630892992019653, loss=4.88068151473999
I0428 15:14:14.467108 139900741007104 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.40867921710014343, loss=5.0659098625183105
I0428 15:14:50.342015 139900732614400 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.41367894411087036, loss=4.933841705322266
I0428 15:15:26.197470 139900741007104 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3953580856323242, loss=4.892753601074219
I0428 15:16:02.068350 139900732614400 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3936346769332886, loss=4.898203372955322
I0428 15:16:37.935168 139900741007104 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.40074121952056885, loss=4.959802150726318
I0428 15:17:13.807939 139900732614400 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.40521398186683655, loss=4.891237735748291
I0428 15:17:49.657793 139900741007104 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.38150322437286377, loss=4.871952056884766
I0428 15:18:25.528895 139900732614400 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4165493845939636, loss=4.784308910369873
I0428 15:19:01.361386 139900741007104 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3869190514087677, loss=4.776302337646484
I0428 15:19:37.162996 139900732614400 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4015517830848694, loss=4.821935653686523
I0428 15:20:12.976225 139900741007104 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3809870779514313, loss=4.843444347381592
I0428 15:20:48.778942 139900732614400 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3811858892440796, loss=4.808484077453613
I0428 15:21:24.629698 139900741007104 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3709837794303894, loss=4.811060428619385
I0428 15:22:00.453259 139900732614400 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3771144449710846, loss=4.80277681350708
I0428 15:22:36.308755 139900741007104 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3769465684890747, loss=4.847626209259033
I0428 15:23:12.158003 139900732614400 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3643207550048828, loss=4.751632213592529
I0428 15:23:48.036145 139900741007104 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3701700270175934, loss=4.89367151260376
I0428 15:24:23.854707 139900732614400 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3665221333503723, loss=4.74065637588501
I0428 15:24:51.865614 140088188094272 spec.py:298] Evaluating on the training split.
I0428 15:24:54.857499 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:27:28.691183 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 15:27:31.343070 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:29:54.175530 140088188094272 spec.py:326] Evaluating on the test split.
I0428 15:29:56.881858 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:32:20.438373 140088188094272 submission_runner.py:415] Time since start: 6514.25s, 	Step: 9380, 	{'train/accuracy': 0.5656463503837585, 'train/loss': 2.864206075668335, 'train/bleu': 26.554353269540474, 'validation/accuracy': 0.5702595114707947, 'validation/loss': 2.7922470569610596, 'validation/bleu': 22.196821487464653, 'validation/num_examples': 3000, 'test/accuracy': 0.5710417628288269, 'test/loss': 2.809493064880371, 'test/bleu': 20.853969490707758, 'test/num_examples': 3003, 'score': 3391.1207592487335, 'total_duration': 6514.245016336441, 'accumulated_submission_time': 3391.1207592487335, 'accumulated_eval_time': 3122.9127571582794, 'accumulated_logging_time': 0.08474254608154297}
I0428 15:32:20.447338 139900741007104 logging_writer.py:48] [9380] accumulated_eval_time=3122.912757, accumulated_logging_time=0.084743, accumulated_submission_time=3391.120759, global_step=9380, preemption_count=0, score=3391.120759, test/accuracy=0.571042, test/bleu=20.853969, test/loss=2.809493, test/num_examples=3003, total_duration=6514.245016, train/accuracy=0.565646, train/bleu=26.554353, train/loss=2.864206, validation/accuracy=0.570260, validation/bleu=22.196821, validation/loss=2.792247, validation/num_examples=3000
I0428 15:32:27.961068 139900732614400 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3648877739906311, loss=4.708643436431885
I0428 15:33:03.749163 139900741007104 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.37416207790374756, loss=4.734046459197998
I0428 15:33:39.586764 139900732614400 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.36754101514816284, loss=4.6722941398620605
I0428 15:34:15.443581 139900741007104 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3733479082584381, loss=4.746830463409424
I0428 15:34:51.269469 139900732614400 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.37988704442977905, loss=4.707470893859863
I0428 15:35:27.096941 139900741007104 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3723711669445038, loss=4.667440414428711
I0428 15:36:02.928316 139900732614400 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3483724892139435, loss=4.619357109069824
I0428 15:36:38.726499 139900741007104 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.351938933134079, loss=4.681743621826172
I0428 15:37:14.570401 139900732614400 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3739323914051056, loss=4.667160511016846
I0428 15:37:50.393264 139900741007104 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3642491400241852, loss=4.696465015411377
I0428 15:38:26.212053 139900732614400 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.35700616240501404, loss=4.6747941970825195
I0428 15:39:02.057551 139900741007104 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.35224708914756775, loss=4.697094440460205
I0428 15:39:37.871884 139900732614400 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.34989088773727417, loss=4.678521633148193
I0428 15:40:13.763299 139900741007104 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3554542362689972, loss=4.68358850479126
I0428 15:40:49.572320 139900732614400 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.37053731083869934, loss=4.73445987701416
I0428 15:41:25.415457 139900741007104 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.3660287857055664, loss=4.636263370513916
I0428 15:42:01.251260 139900732614400 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3480681777000427, loss=4.654295921325684
I0428 15:42:37.091635 139900741007104 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3485772907733917, loss=4.644951343536377
I0428 15:43:12.889574 139900732614400 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3519861698150635, loss=4.656643390655518
I0428 15:43:48.749060 139900741007104 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.35037583112716675, loss=4.648382186889648
I0428 15:44:24.586864 139900732614400 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.34639453887939453, loss=4.635635852813721
I0428 15:45:00.458291 139900741007104 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.34700509905815125, loss=4.71217155456543
I0428 15:45:36.318668 139900732614400 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.33108073472976685, loss=4.5596442222595215
I0428 15:46:12.145153 139900741007104 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3353690207004547, loss=4.630129337310791
I0428 15:46:20.795117 140088188094272 spec.py:298] Evaluating on the training split.
I0428 15:46:23.792229 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:48:47.717051 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 15:48:50.375107 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:51:12.712620 140088188094272 spec.py:326] Evaluating on the test split.
I0428 15:51:15.430369 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 15:53:28.772651 140088188094272 submission_runner.py:415] Time since start: 7782.58s, 	Step: 11726, 	{'train/accuracy': 0.5811959505081177, 'train/loss': 2.7048816680908203, 'train/bleu': 26.97897512649319, 'validation/accuracy': 0.5898996591567993, 'validation/loss': 2.5914483070373535, 'validation/bleu': 23.401268508894745, 'validation/num_examples': 3000, 'test/accuracy': 0.5934228301048279, 'test/loss': 2.5892891883850098, 'test/bleu': 22.00204952033998, 'test/num_examples': 3003, 'score': 4231.427165031433, 'total_duration': 7782.5792915821075, 'accumulated_submission_time': 4231.427165031433, 'accumulated_eval_time': 3550.8902490139008, 'accumulated_logging_time': 0.1026914119720459}
I0428 15:53:28.781738 139900732614400 logging_writer.py:48] [11726] accumulated_eval_time=3550.890249, accumulated_logging_time=0.102691, accumulated_submission_time=4231.427165, global_step=11726, preemption_count=0, score=4231.427165, test/accuracy=0.593423, test/bleu=22.002050, test/loss=2.589289, test/num_examples=3003, total_duration=7782.579292, train/accuracy=0.581196, train/bleu=26.978975, train/loss=2.704882, validation/accuracy=0.589900, validation/bleu=23.401269, validation/loss=2.591448, validation/num_examples=3000
I0428 15:53:55.587181 139900741007104 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.33386898040771484, loss=4.5859856605529785
I0428 15:54:31.418886 139900732614400 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.3451378047466278, loss=4.608649730682373
I0428 15:55:07.236833 139900741007104 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.34993696212768555, loss=4.651847839355469
I0428 15:55:43.046575 139900732614400 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3394288718700409, loss=4.624967098236084
I0428 15:56:18.858118 139900741007104 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3329209089279175, loss=4.610508918762207
I0428 15:56:54.704954 139900732614400 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.33752110600471497, loss=4.63809871673584
I0428 15:57:30.559537 139900741007104 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.33232471346855164, loss=4.617742538452148
I0428 15:58:06.363272 139900732614400 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3550977408885956, loss=4.665062427520752
I0428 15:58:42.148133 139900741007104 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3407326340675354, loss=4.5757927894592285
I0428 15:59:17.946115 139900732614400 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3318568766117096, loss=4.526484966278076
I0428 15:59:53.755249 139900741007104 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.34710127115249634, loss=4.628823280334473
I0428 16:00:29.522428 139900732614400 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.33459389209747314, loss=4.582485675811768
I0428 16:01:05.381637 139900741007104 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.32686710357666016, loss=4.559107780456543
I0428 16:01:41.205099 139900732614400 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3408038020133972, loss=4.580718994140625
I0428 16:02:17.049730 139900741007104 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3472611606121063, loss=4.59896993637085
I0428 16:02:52.856748 139900732614400 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3380507528781891, loss=4.570778846740723
I0428 16:03:28.674449 139900741007104 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3287299871444702, loss=4.631752967834473
I0428 16:04:04.505958 139900732614400 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3165273368358612, loss=4.493930339813232
I0428 16:04:40.336846 139900741007104 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3399280607700348, loss=4.555629730224609
I0428 16:05:16.137466 139900732614400 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3479979336261749, loss=4.536633014678955
I0428 16:05:51.966502 139900741007104 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.34176379442214966, loss=4.578408241271973
I0428 16:06:27.793760 139900732614400 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.34764620661735535, loss=4.5876240730285645
I0428 16:07:03.650258 139900741007104 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3559933602809906, loss=4.492658615112305
I0428 16:07:29.142561 140088188094272 spec.py:298] Evaluating on the training split.
I0428 16:07:32.133233 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:10:03.529959 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 16:10:06.180557 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:12:31.577659 140088188094272 spec.py:326] Evaluating on the test split.
I0428 16:12:34.294301 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:14:53.084616 140088188094272 submission_runner.py:415] Time since start: 9066.89s, 	Step: 14073, 	{'train/accuracy': 0.6017056703567505, 'train/loss': 2.520822286605835, 'train/bleu': 28.749430562957457, 'validation/accuracy': 0.6044934391975403, 'validation/loss': 2.465629816055298, 'validation/bleu': 24.55368451606445, 'validation/num_examples': 3000, 'test/accuracy': 0.6090291142463684, 'test/loss': 2.447542667388916, 'test/bleu': 23.40397510056625, 'test/num_examples': 3003, 'score': 5071.74657034874, 'total_duration': 9066.89125585556, 'accumulated_submission_time': 5071.74657034874, 'accumulated_eval_time': 3994.832263946533, 'accumulated_logging_time': 0.12152934074401855}
I0428 16:14:53.093729 139900732614400 logging_writer.py:48] [14073] accumulated_eval_time=3994.832264, accumulated_logging_time=0.121529, accumulated_submission_time=5071.746570, global_step=14073, preemption_count=0, score=5071.746570, test/accuracy=0.609029, test/bleu=23.403975, test/loss=2.447543, test/num_examples=3003, total_duration=9066.891256, train/accuracy=0.601706, train/bleu=28.749431, train/loss=2.520822, validation/accuracy=0.604493, validation/bleu=24.553685, validation/loss=2.465630, validation/num_examples=3000
I0428 16:15:03.118602 139900741007104 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3549996614456177, loss=4.545654296875
I0428 16:15:38.895474 139900732614400 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.3345097303390503, loss=4.557595252990723
I0428 16:16:14.701684 139900741007104 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.33345896005630493, loss=4.5466437339782715
I0428 16:16:50.497876 139900732614400 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.33536162972450256, loss=4.574606895446777
I0428 16:17:26.355446 139900741007104 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.3210415244102478, loss=4.543519973754883
I0428 16:18:02.185603 139900732614400 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3308483958244324, loss=4.485037326812744
I0428 16:18:38.021262 139900741007104 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.32132285833358765, loss=4.488420009613037
I0428 16:19:13.843870 139900732614400 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.345947265625, loss=4.563827037811279
I0428 16:19:49.670674 139900741007104 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3253137171268463, loss=4.495968818664551
I0428 16:20:25.497981 139900732614400 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.34303441643714905, loss=4.502084255218506
I0428 16:21:01.297547 139900741007104 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3298545777797699, loss=4.502351760864258
I0428 16:21:37.120492 139900732614400 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.33112892508506775, loss=4.537009239196777
I0428 16:22:12.937318 139900741007104 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3243906497955322, loss=4.514338493347168
I0428 16:22:48.719683 139900732614400 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3270412087440491, loss=4.453448295593262
I0428 16:23:24.481503 139900741007104 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3175777196884155, loss=4.5207672119140625
I0428 16:24:00.331353 139900732614400 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3246499001979828, loss=4.4731526374816895
I0428 16:24:36.126948 139900741007104 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.328682541847229, loss=4.515836238861084
I0428 16:25:11.965559 139900732614400 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3257426917552948, loss=4.474079608917236
I0428 16:25:47.767939 139900741007104 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.32548245787620544, loss=4.387086391448975
I0428 16:26:23.608309 139900732614400 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3258621394634247, loss=4.451563358306885
I0428 16:26:59.422801 139900741007104 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.33557531237602234, loss=4.516503810882568
I0428 16:27:35.264006 139900732614400 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3361153304576874, loss=4.456842422485352
I0428 16:28:11.098818 139900741007104 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3265683054924011, loss=4.409549236297607
I0428 16:28:46.926543 139900732614400 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.32092931866645813, loss=4.417940139770508
I0428 16:28:53.424808 140088188094272 spec.py:298] Evaluating on the training split.
I0428 16:28:56.410121 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:31:20.948607 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 16:31:23.596848 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:33:46.203098 140088188094272 spec.py:326] Evaluating on the test split.
I0428 16:33:48.930588 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:36:06.823573 140088188094272 submission_runner.py:415] Time since start: 10340.63s, 	Step: 16420, 	{'train/accuracy': 0.6049696803092957, 'train/loss': 2.4801793098449707, 'train/bleu': 28.953288942148568, 'validation/accuracy': 0.6129992008209229, 'validation/loss': 2.378849506378174, 'validation/bleu': 25.25638839917031, 'validation/num_examples': 3000, 'test/accuracy': 0.6183719635009766, 'test/loss': 2.3523499965667725, 'test/bleu': 23.96508815191501, 'test/num_examples': 3003, 'score': 5912.036168813705, 'total_duration': 10340.630200147629, 'accumulated_submission_time': 5912.036168813705, 'accumulated_eval_time': 4428.230977535248, 'accumulated_logging_time': 0.14023900032043457}
I0428 16:36:06.832895 139900741007104 logging_writer.py:48] [16420] accumulated_eval_time=4428.230978, accumulated_logging_time=0.140239, accumulated_submission_time=5912.036169, global_step=16420, preemption_count=0, score=5912.036169, test/accuracy=0.618372, test/bleu=23.965088, test/loss=2.352350, test/num_examples=3003, total_duration=10340.630200, train/accuracy=0.604970, train/bleu=28.953289, train/loss=2.480179, validation/accuracy=0.612999, validation/bleu=25.256388, validation/loss=2.378850, validation/num_examples=3000
I0428 16:36:35.850853 139900732614400 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.33235064148902893, loss=4.494517803192139
I0428 16:37:11.657809 139900741007104 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.34430792927742004, loss=4.414126396179199
I0428 16:37:47.459924 139900732614400 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.32059046626091003, loss=4.47465181350708
I0428 16:38:23.286049 139900741007104 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3240981101989746, loss=4.456089496612549
I0428 16:38:59.116152 139900732614400 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.32452353835105896, loss=4.462947845458984
I0428 16:39:34.949178 139900741007104 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.33791384100914, loss=4.519752025604248
I0428 16:40:10.786992 139900732614400 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3248599171638489, loss=4.45066499710083
I0428 16:40:46.628293 139900741007104 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.31383946537971497, loss=4.471338748931885
I0428 16:41:22.426494 139900732614400 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3182734251022339, loss=4.400946617126465
I0428 16:41:58.245190 139900741007104 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.32996851205825806, loss=4.46641206741333
I0428 16:42:34.040447 139900732614400 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3211657702922821, loss=4.467347621917725
I0428 16:43:09.863131 139900741007104 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.33032047748565674, loss=4.457522392272949
I0428 16:43:45.706537 139900732614400 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.32124465703964233, loss=4.4456048011779785
I0428 16:44:21.563905 139900741007104 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3219605088233948, loss=4.404407501220703
I0428 16:44:57.372149 139900732614400 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.31844690442085266, loss=4.315998077392578
I0428 16:45:33.185529 139900741007104 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3124604821205139, loss=4.437655448913574
I0428 16:46:08.993678 139900732614400 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.33200886845588684, loss=4.446430683135986
I0428 16:46:44.790452 139900741007104 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3186717927455902, loss=4.391635417938232
I0428 16:47:20.597435 139900732614400 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.31491735577583313, loss=4.479338645935059
I0428 16:47:56.389853 139900741007104 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.31665244698524475, loss=4.477566719055176
I0428 16:48:32.229773 139900732614400 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3205156922340393, loss=4.398895263671875
I0428 16:49:08.045142 139900741007104 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.31636273860931396, loss=4.379524230957031
I0428 16:49:43.864431 139900732614400 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.32348108291625977, loss=4.415182590484619
I0428 16:50:07.191029 140088188094272 spec.py:298] Evaluating on the training split.
I0428 16:50:10.183945 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:52:39.090825 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 16:52:41.762293 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:55:10.724789 140088188094272 spec.py:326] Evaluating on the test split.
I0428 16:55:13.433678 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 16:57:36.644795 140088188094272 submission_runner.py:415] Time since start: 11630.45s, 	Step: 18767, 	{'train/accuracy': 0.6219401955604553, 'train/loss': 2.335089921951294, 'train/bleu': 30.16596649492327, 'validation/accuracy': 0.621046245098114, 'validation/loss': 2.298863410949707, 'validation/bleu': 25.764284715358382, 'validation/num_examples': 3000, 'test/accuracy': 0.6250072717666626, 'test/loss': 2.271073818206787, 'test/bleu': 24.3551826475579, 'test/num_examples': 3003, 'score': 6752.352305173874, 'total_duration': 11630.45143532753, 'accumulated_submission_time': 6752.352305173874, 'accumulated_eval_time': 4877.684729337692, 'accumulated_logging_time': 0.15923714637756348}
I0428 16:57:36.654526 139900741007104 logging_writer.py:48] [18767] accumulated_eval_time=4877.684729, accumulated_logging_time=0.159237, accumulated_submission_time=6752.352305, global_step=18767, preemption_count=0, score=6752.352305, test/accuracy=0.625007, test/bleu=24.355183, test/loss=2.271074, test/num_examples=3003, total_duration=11630.451435, train/accuracy=0.621940, train/bleu=30.165966, train/loss=2.335090, validation/accuracy=0.621046, validation/bleu=25.764285, validation/loss=2.298863, validation/num_examples=3000
I0428 16:57:48.805773 139900732614400 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.32209187746047974, loss=4.454736232757568
I0428 16:58:24.570942 139900741007104 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.31162741780281067, loss=4.354606628417969
I0428 16:59:00.399662 139900732614400 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3068881034851074, loss=4.332121849060059
I0428 16:59:36.179038 139900741007104 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.31320682168006897, loss=4.349680423736572
I0428 17:00:11.986981 139900732614400 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.32209378480911255, loss=4.433265209197998
I0428 17:00:47.855056 139900741007104 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3126145899295807, loss=4.335293292999268
I0428 17:01:23.671514 139900732614400 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.326071172952652, loss=4.356230735778809
I0428 17:01:59.470073 139900741007104 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.31831735372543335, loss=4.385905742645264
I0428 17:02:35.288045 139900732614400 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3200421929359436, loss=4.3930864334106445
I0428 17:03:11.138815 139900741007104 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.320913702249527, loss=4.43395471572876
I0428 17:03:46.983151 139900732614400 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.30712682008743286, loss=4.434695720672607
I0428 17:04:22.833877 139900741007104 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.32258912920951843, loss=4.391153812408447
I0428 17:04:57.976851 140088188094272 spec.py:298] Evaluating on the training split.
I0428 17:05:00.966780 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 17:07:28.585128 140088188094272 spec.py:310] Evaluating on the validation split.
I0428 17:07:31.243536 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 17:09:47.242046 140088188094272 spec.py:326] Evaluating on the test split.
I0428 17:09:49.957612 140088188094272 workload.py:179] Translating evaluation dataset.
I0428 17:12:03.943897 140088188094272 submission_runner.py:415] Time since start: 12497.75s, 	Step: 20000, 	{'train/accuracy': 0.6182262897491455, 'train/loss': 2.3306233882904053, 'train/bleu': 30.421241988282144, 'validation/accuracy': 0.6239476203918457, 'validation/loss': 2.2728118896484375, 'validation/bleu': 25.798625935233087, 'validation/num_examples': 3000, 'test/accuracy': 0.6291441917419434, 'test/loss': 2.240325927734375, 'test/bleu': 24.559430325501893, 'test/num_examples': 3003, 'score': 7193.647973775864, 'total_duration': 12497.750518321991, 'accumulated_submission_time': 7193.647973775864, 'accumulated_eval_time': 5303.651719331741, 'accumulated_logging_time': 0.1788320541381836}
I0428 17:12:03.954052 139900732614400 logging_writer.py:48] [20000] accumulated_eval_time=5303.651719, accumulated_logging_time=0.178832, accumulated_submission_time=7193.647974, global_step=20000, preemption_count=0, score=7193.647974, test/accuracy=0.629144, test/bleu=24.559430, test/loss=2.240326, test/num_examples=3003, total_duration=12497.750518, train/accuracy=0.618226, train/bleu=30.421242, train/loss=2.330623, validation/accuracy=0.623948, validation/bleu=25.798626, validation/loss=2.272812, validation/num_examples=3000
I0428 17:12:03.971065 139900741007104 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7193.647974
I0428 17:12:04.689850 140088188094272 checkpoints.py:356] Saving checkpoint at step: 20000
I0428 17:12:07.359786 140088188094272 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1/checkpoint_20000
I0428 17:12:07.362927 140088188094272 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_nesterov/wmt_jax/trial_1/checkpoint_20000.
I0428 17:12:07.416858 140088188094272 submission_runner.py:578] Tuning trial 1/1
I0428 17:12:07.417036 140088188094272 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0428 17:12:07.418657 140088188094272 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005968915647827089, 'train/loss': 11.008682250976562, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.031411170959473, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.011260986328125, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.652264833450317, 'total_duration': 915.1577713489532, 'accumulated_submission_time': 30.652264833450317, 'accumulated_eval_time': 884.5053508281708, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2346, {'train/accuracy': 0.3049754202365875, 'train/loss': 5.55180025100708, 'train/bleu': 6.278616102473807, 'validation/accuracy': 0.2819555699825287, 'validation/loss': 5.813656806945801, 'validation/bleu': 3.5052125931010347, 'validation/num_examples': 3000, 'test/accuracy': 0.2622857391834259, 'test/loss': 6.10167932510376, 'test/bleu': 2.619755183998621, 'test/num_examples': 3003, 'score': 870.9549169540405, 'total_duration': 2584.4747166633606, 'accumulated_submission_time': 870.9549169540405, 'accumulated_eval_time': 1713.4613864421844, 'accumulated_logging_time': 0.02757120132446289, 'global_step': 2346, 'preemption_count': 0}), (4690, {'train/accuracy': 0.45951518416404724, 'train/loss': 3.8847429752349854, 'train/bleu': 17.64822150160748, 'validation/accuracy': 0.4485127329826355, 'validation/loss': 3.968470573425293, 'validation/bleu': 13.042222086072792, 'validation/num_examples': 3000, 'test/accuracy': 0.4397187829017639, 'test/loss': 4.111972808837891, 'test/bleu': 11.601581408927313, 'test/num_examples': 3003, 'score': 1710.9479629993439, 'total_duration': 3943.7288670539856, 'accumulated_submission_time': 1710.9479629993439, 'accumulated_eval_time': 2232.6729652881622, 'accumulated_logging_time': 0.04538154602050781, 'global_step': 4690, 'preemption_count': 0}), (7035, {'train/accuracy': 0.5399675369262695, 'train/loss': 3.135305643081665, 'train/bleu': 23.07175898486892, 'validation/accuracy': 0.5325290560722351, 'validation/loss': 3.1738529205322266, 'validation/bleu': 19.19782104630395, 'validation/num_examples': 3000, 'test/accuracy': 0.5293591618537903, 'test/loss': 3.2300455570220947, 'test/bleu': 17.779616726279002, 'test/num_examples': 3003, 'score': 2550.969848871231, 'total_duration': 5225.471460580826, 'accumulated_submission_time': 2550.969848871231, 'accumulated_eval_time': 2674.3400292396545, 'accumulated_logging_time': 0.06692123413085938, 'global_step': 7035, 'preemption_count': 0}), (9380, {'train/accuracy': 0.5656463503837585, 'train/loss': 2.864206075668335, 'train/bleu': 26.554353269540474, 'validation/accuracy': 0.5702595114707947, 'validation/loss': 2.7922470569610596, 'validation/bleu': 22.196821487464653, 'validation/num_examples': 3000, 'test/accuracy': 0.5710417628288269, 'test/loss': 2.809493064880371, 'test/bleu': 20.853969490707758, 'test/num_examples': 3003, 'score': 3391.1207592487335, 'total_duration': 6514.245016336441, 'accumulated_submission_time': 3391.1207592487335, 'accumulated_eval_time': 3122.9127571582794, 'accumulated_logging_time': 0.08474254608154297, 'global_step': 9380, 'preemption_count': 0}), (11726, {'train/accuracy': 0.5811959505081177, 'train/loss': 2.7048816680908203, 'train/bleu': 26.97897512649319, 'validation/accuracy': 0.5898996591567993, 'validation/loss': 2.5914483070373535, 'validation/bleu': 23.401268508894745, 'validation/num_examples': 3000, 'test/accuracy': 0.5934228301048279, 'test/loss': 2.5892891883850098, 'test/bleu': 22.00204952033998, 'test/num_examples': 3003, 'score': 4231.427165031433, 'total_duration': 7782.5792915821075, 'accumulated_submission_time': 4231.427165031433, 'accumulated_eval_time': 3550.8902490139008, 'accumulated_logging_time': 0.1026914119720459, 'global_step': 11726, 'preemption_count': 0}), (14073, {'train/accuracy': 0.6017056703567505, 'train/loss': 2.520822286605835, 'train/bleu': 28.749430562957457, 'validation/accuracy': 0.6044934391975403, 'validation/loss': 2.465629816055298, 'validation/bleu': 24.55368451606445, 'validation/num_examples': 3000, 'test/accuracy': 0.6090291142463684, 'test/loss': 2.447542667388916, 'test/bleu': 23.40397510056625, 'test/num_examples': 3003, 'score': 5071.74657034874, 'total_duration': 9066.89125585556, 'accumulated_submission_time': 5071.74657034874, 'accumulated_eval_time': 3994.832263946533, 'accumulated_logging_time': 0.12152934074401855, 'global_step': 14073, 'preemption_count': 0}), (16420, {'train/accuracy': 0.6049696803092957, 'train/loss': 2.4801793098449707, 'train/bleu': 28.953288942148568, 'validation/accuracy': 0.6129992008209229, 'validation/loss': 2.378849506378174, 'validation/bleu': 25.25638839917031, 'validation/num_examples': 3000, 'test/accuracy': 0.6183719635009766, 'test/loss': 2.3523499965667725, 'test/bleu': 23.96508815191501, 'test/num_examples': 3003, 'score': 5912.036168813705, 'total_duration': 10340.630200147629, 'accumulated_submission_time': 5912.036168813705, 'accumulated_eval_time': 4428.230977535248, 'accumulated_logging_time': 0.14023900032043457, 'global_step': 16420, 'preemption_count': 0}), (18767, {'train/accuracy': 0.6219401955604553, 'train/loss': 2.335089921951294, 'train/bleu': 30.16596649492327, 'validation/accuracy': 0.621046245098114, 'validation/loss': 2.298863410949707, 'validation/bleu': 25.764284715358382, 'validation/num_examples': 3000, 'test/accuracy': 0.6250072717666626, 'test/loss': 2.271073818206787, 'test/bleu': 24.3551826475579, 'test/num_examples': 3003, 'score': 6752.352305173874, 'total_duration': 11630.45143532753, 'accumulated_submission_time': 6752.352305173874, 'accumulated_eval_time': 4877.684729337692, 'accumulated_logging_time': 0.15923714637756348, 'global_step': 18767, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6182262897491455, 'train/loss': 2.3306233882904053, 'train/bleu': 30.421241988282144, 'validation/accuracy': 0.6239476203918457, 'validation/loss': 2.2728118896484375, 'validation/bleu': 25.798625935233087, 'validation/num_examples': 3000, 'test/accuracy': 0.6291441917419434, 'test/loss': 2.240325927734375, 'test/bleu': 24.559430325501893, 'test/num_examples': 3003, 'score': 7193.647973775864, 'total_duration': 12497.750518321991, 'accumulated_submission_time': 7193.647973775864, 'accumulated_eval_time': 5303.651719331741, 'accumulated_logging_time': 0.1788320541381836, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0428 17:12:07.418796 140088188094272 submission_runner.py:581] Timing: 7193.647973775864
I0428 17:12:07.418838 140088188094272 submission_runner.py:582] ====================
I0428 17:12:07.418931 140088188094272 submission_runner.py:645] Final wmt score: 7193.647973775864
