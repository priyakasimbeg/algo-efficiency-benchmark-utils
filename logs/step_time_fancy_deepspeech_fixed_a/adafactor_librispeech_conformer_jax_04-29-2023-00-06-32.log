python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_04-29-2023-00-06-32.log
I0429 00:06:52.972893 139898416887616 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax.
I0429 00:06:53.061030 139898416887616 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 00:06:53.990085 139898416887616 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0429 00:06:53.990840 139898416887616 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 00:06:53.994612 139898416887616 submission_runner.py:538] Using RNG seed 1658911135
I0429 00:06:56.626093 139898416887616 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 00:06:56.626332 139898416887616 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1.
I0429 00:06:56.628100 139898416887616 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1/hparams.json.
I0429 00:06:56.759688 139898416887616 submission_runner.py:241] Initializing dataset.
I0429 00:06:56.759915 139898416887616 submission_runner.py:248] Initializing model.
I0429 00:07:02.680650 139898416887616 submission_runner.py:258] Initializing optimizer.
I0429 00:07:04.854530 139898416887616 submission_runner.py:265] Initializing metrics bundle.
I0429 00:07:04.854733 139898416887616 submission_runner.py:282] Initializing checkpoint and logger.
I0429 00:07:04.855797 139898416887616 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0429 00:07:04.856104 139898416887616 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 00:07:04.856185 139898416887616 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 00:07:05.595094 139898416887616 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0429 00:07:05.596029 139898416887616 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1/flags_0.json.
I0429 00:07:05.602746 139898416887616 submission_runner.py:318] Starting training loop.
I0429 00:07:05.798825 139898416887616 input_pipeline.py:20] Loading split = train-clean-100
I0429 00:07:05.829807 139898416887616 input_pipeline.py:20] Loading split = train-clean-360
I0429 00:07:06.170871 139898416887616 input_pipeline.py:20] Loading split = train-other-500
2023-04-29 00:08:50.313082: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-29 00:08:50.741271: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0429 00:08:52.597151 139723028817664 logging_writer.py:48] [0] global_step=0, grad_norm=61.389801025390625, loss=32.131256103515625
I0429 00:08:52.630840 139898416887616 spec.py:298] Evaluating on the training split.
I0429 00:08:52.739974 139898416887616 input_pipeline.py:20] Loading split = train-clean-100
I0429 00:08:52.767542 139898416887616 input_pipeline.py:20] Loading split = train-clean-360
I0429 00:08:52.852566 139898416887616 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0429 00:09:50.907342 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 00:09:50.972061 139898416887616 input_pipeline.py:20] Loading split = dev-clean
I0429 00:09:50.977092 139898416887616 input_pipeline.py:20] Loading split = dev-other
I0429 00:10:34.100443 139898416887616 spec.py:326] Evaluating on the test split.
I0429 00:10:34.165837 139898416887616 input_pipeline.py:20] Loading split = test-clean
I0429 00:11:04.481599 139898416887616 submission_runner.py:415] Time since start: 238.88s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.389648, dtype=float32), 'train/wer': 1.6704703996001915, 'validation/ctc_loss': DeviceArray(30.50612, dtype=float32), 'validation/wer': 1.4180841107970168, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.613726, dtype=float32), 'test/wer': 1.4736660370076982, 'test/num_examples': 2472, 'score': 107.02790260314941, 'total_duration': 238.87737894058228, 'accumulated_submission_time': 107.02790260314941, 'accumulated_eval_time': 131.84931564331055, 'accumulated_logging_time': 0}
I0429 00:11:04.505030 139720537396992 logging_writer.py:48] [1] accumulated_eval_time=131.849316, accumulated_logging_time=0, accumulated_submission_time=107.027903, global_step=1, preemption_count=0, score=107.027903, test/ctc_loss=30.613725662231445, test/num_examples=2472, test/wer=1.473666, total_duration=238.877379, train/ctc_loss=31.3896484375, train/wer=1.670470, validation/ctc_loss=30.506120681762695, validation/num_examples=5348, validation/wer=1.418084
I0429 00:13:35.824392 139725195093760 logging_writer.py:48] [100] global_step=100, grad_norm=8.933778762817383, loss=6.449948310852051
I0429 00:15:04.763559 139725203486464 logging_writer.py:48] [200] global_step=200, grad_norm=0.38234034180641174, loss=5.833558082580566
I0429 00:16:33.104110 139725195093760 logging_writer.py:48] [300] global_step=300, grad_norm=1.0794544219970703, loss=5.795963764190674
I0429 00:18:02.991131 139725203486464 logging_writer.py:48] [400] global_step=400, grad_norm=0.3505314290523529, loss=5.803552627563477
I0429 00:19:33.286946 139725195093760 logging_writer.py:48] [500] global_step=500, grad_norm=0.5282692313194275, loss=5.769009113311768
I0429 00:21:03.153537 139725203486464 logging_writer.py:48] [600] global_step=600, grad_norm=2.7896618843078613, loss=5.810163497924805
I0429 00:22:32.900443 139725195093760 logging_writer.py:48] [700] global_step=700, grad_norm=2.6942243576049805, loss=5.740904331207275
I0429 00:24:03.004996 139725203486464 logging_writer.py:48] [800] global_step=800, grad_norm=0.9662615060806274, loss=5.549489498138428
I0429 00:25:32.867871 139725195093760 logging_writer.py:48] [900] global_step=900, grad_norm=1.6913496255874634, loss=5.496207237243652
I0429 00:27:02.770339 139725203486464 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.4663828611373901, loss=5.459231376647949
I0429 00:28:36.703114 139725159003904 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.9026839137077332, loss=5.379217624664307
I0429 00:30:06.635347 139725150611200 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6160969734191895, loss=4.827200889587402
I0429 00:31:37.131456 139725159003904 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.019862174987793, loss=4.1332197189331055
I0429 00:33:08.301012 139725150611200 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0723191499710083, loss=3.7165162563323975
I0429 00:34:40.236064 139725159003904 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.763008713722229, loss=3.47672438621521
I0429 00:36:11.964403 139725150611200 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1209243535995483, loss=3.314980983734131
I0429 00:37:43.590551 139725159003904 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7816587090492249, loss=3.025749683380127
I0429 00:39:15.422917 139725150611200 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9088851809501648, loss=2.9526751041412354
I0429 00:40:46.101664 139725159003904 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9763451814651489, loss=2.8578760623931885
I0429 00:42:16.956828 139725150611200 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1046428680419922, loss=2.761270761489868
I0429 00:43:51.719531 139725159003904 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7117536067962646, loss=2.7032253742218018
I0429 00:45:22.895356 139725150611200 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0494407415390015, loss=2.581688165664673
I0429 00:46:53.482433 139725159003904 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8296743035316467, loss=2.4702253341674805
I0429 00:48:24.541042 139725150611200 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7135646343231201, loss=2.46024489402771
I0429 00:49:55.499862 139725159003904 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9403858184814453, loss=2.4180946350097656
I0429 00:51:04.927809 139898416887616 spec.py:298] Evaluating on the training split.
I0429 00:51:41.297987 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 00:52:19.882937 139898416887616 spec.py:326] Evaluating on the test split.
I0429 00:52:39.426676 139898416887616 submission_runner.py:415] Time since start: 2733.82s, 	Step: 2577, 	{'train/ctc_loss': DeviceArray(1.9334558, dtype=float32), 'train/wer': 0.46180305246029946, 'validation/ctc_loss': DeviceArray(2.4250479, dtype=float32), 'validation/wer': 0.5093729799612152, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.060485, dtype=float32), 'test/wer': 0.4648711230272378, 'test/num_examples': 2472, 'score': 2507.4045674800873, 'total_duration': 2733.820568084717, 'accumulated_submission_time': 2507.4045674800873, 'accumulated_eval_time': 226.3448896408081, 'accumulated_logging_time': 0.03588533401489258}
I0429 00:52:39.447536 139725159003904 logging_writer.py:48] [2577] accumulated_eval_time=226.344890, accumulated_logging_time=0.035885, accumulated_submission_time=2507.404567, global_step=2577, preemption_count=0, score=2507.404567, test/ctc_loss=2.0604848861694336, test/num_examples=2472, test/wer=0.464871, total_duration=2733.820568, train/ctc_loss=1.9334558248519897, train/wer=0.461803, validation/ctc_loss=2.4250478744506836, validation/num_examples=5348, validation/wer=0.509373
I0429 00:53:01.287244 139725150611200 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9474879503250122, loss=2.3354549407958984
I0429 00:54:32.007506 139725159003904 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9174759387969971, loss=2.2900795936584473
I0429 00:56:03.252463 139725150611200 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6817570924758911, loss=2.1948142051696777
I0429 00:57:34.628116 139725159003904 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5922958254814148, loss=2.1091184616088867
I0429 00:59:05.666426 139725150611200 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6091622710227966, loss=2.1262824535369873
I0429 01:00:40.311289 139725159003904 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5836139917373657, loss=2.1252756118774414
I0429 01:02:12.480184 139725150611200 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5919615030288696, loss=2.1059489250183105
I0429 01:03:43.657379 139725159003904 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5461978316307068, loss=2.0156092643737793
I0429 01:05:15.265054 139725150611200 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5263726115226746, loss=2.057554006576538
I0429 01:06:46.898535 139725159003904 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7147012948989868, loss=2.039262533187866
I0429 01:08:17.952129 139725150611200 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8155122995376587, loss=1.9578726291656494
I0429 01:09:49.085936 139725159003904 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5941961407661438, loss=1.9660190343856812
I0429 01:11:21.393008 139725150611200 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5254948735237122, loss=1.9534302949905396
I0429 01:12:52.575324 139725159003904 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5499512553215027, loss=1.8820286989212036
I0429 01:14:23.950383 139725150611200 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6410998106002808, loss=1.9423214197158813
I0429 01:15:55.332926 139725159003904 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6855618357658386, loss=1.948997974395752
I0429 01:17:30.542163 139725159003904 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5129870176315308, loss=1.860846757888794
I0429 01:19:01.537795 139725150611200 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6602045297622681, loss=1.8794876337051392
I0429 01:20:33.246740 139725159003904 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.43949076533317566, loss=1.8715415000915527
I0429 01:22:05.812241 139725150611200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8954733610153198, loss=1.9098888635635376
I0429 01:23:38.297958 139725159003904 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4906994104385376, loss=1.902138113975525
I0429 01:25:10.946316 139725150611200 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8494072556495667, loss=1.8049991130828857
I0429 01:26:43.372629 139725159003904 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5676451921463013, loss=1.846340537071228
I0429 01:28:15.947287 139725150611200 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8232935667037964, loss=1.8324307203292847
I0429 01:29:47.686023 139725159003904 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6802571415901184, loss=1.8911070823669434
I0429 01:31:19.865352 139725150611200 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6738729476928711, loss=1.8813754320144653
I0429 01:32:40.215678 139898416887616 spec.py:298] Evaluating on the training split.
I0429 01:33:17.120036 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 01:33:56.137397 139898416887616 spec.py:326] Evaluating on the test split.
I0429 01:34:16.101502 139898416887616 submission_runner.py:415] Time since start: 5230.50s, 	Step: 5185, 	{'train/ctc_loss': DeviceArray(0.5892229, dtype=float32), 'train/wer': 0.19940867804917028, 'validation/ctc_loss': DeviceArray(0.92123014, dtype=float32), 'validation/wer': 0.26985306177580104, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6343679, dtype=float32), 'test/wer': 0.2074624743566307, 'test/num_examples': 2472, 'score': 4908.125363588333, 'total_duration': 5230.4951729774475, 'accumulated_submission_time': 4908.125363588333, 'accumulated_eval_time': 322.22739601135254, 'accumulated_logging_time': 0.0702519416809082}
I0429 01:34:16.123248 139725056603904 logging_writer.py:48] [5185] accumulated_eval_time=322.227396, accumulated_logging_time=0.070252, accumulated_submission_time=4908.125364, global_step=5185, preemption_count=0, score=4908.125364, test/ctc_loss=0.6343678832054138, test/num_examples=2472, test/wer=0.207462, total_duration=5230.495173, train/ctc_loss=0.5892229080200195, train/wer=0.199409, validation/ctc_loss=0.921230137348175, validation/num_examples=5348, validation/wer=0.269853
I0429 01:34:30.586793 139725048211200 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6711465120315552, loss=1.755641222000122
I0429 01:36:01.511163 139725056603904 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4837052822113037, loss=1.8786879777908325
I0429 01:37:32.768495 139725048211200 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4981217086315155, loss=1.8007534742355347
I0429 01:39:04.432515 139725056603904 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.4824824631214142, loss=1.7931088209152222
I0429 01:40:35.910840 139725048211200 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5185548663139343, loss=1.7712254524230957
I0429 01:42:07.503742 139725056603904 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5625021457672119, loss=1.756995677947998
I0429 01:43:38.954852 139725048211200 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6478492617607117, loss=1.7943416833877563
I0429 01:45:10.809017 139725056603904 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.552312433719635, loss=1.7486164569854736
I0429 01:46:42.115257 139725048211200 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.45481961965560913, loss=1.7389284372329712
I0429 01:48:14.218409 139725056603904 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4204285740852356, loss=1.798271656036377
I0429 01:49:49.269700 139725056603904 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.40469789505004883, loss=1.731348991394043
I0429 01:51:21.506087 139725048211200 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5233414173126221, loss=1.775166630744934
I0429 01:52:52.780639 139725056603904 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4552767276763916, loss=1.7342052459716797
I0429 01:54:24.608804 139725048211200 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.3849598467350006, loss=1.6966428756713867
I0429 01:55:56.351083 139725056603904 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.35260090231895447, loss=1.7227247953414917
I0429 01:57:28.972593 139725048211200 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.45700711011886597, loss=1.6974525451660156
I0429 01:59:00.812136 139725056603904 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5220741033554077, loss=1.7476873397827148
I0429 02:00:32.839365 139725048211200 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.30788305401802063, loss=1.6468348503112793
I0429 02:02:04.651333 139725056603904 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.35299617052078247, loss=1.657616138458252
I0429 02:03:36.682438 139725048211200 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.42549440264701843, loss=1.7077337503433228
I0429 02:05:07.408152 139725056603904 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3819097578525543, loss=1.6889872550964355
I0429 02:06:44.073232 139725056603904 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.44354188442230225, loss=1.5796469449996948
I0429 02:08:15.743026 139725048211200 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.44588467478752136, loss=1.6139649152755737
I0429 02:09:47.714380 139725056603904 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3698030412197113, loss=1.5795578956604004
I0429 02:11:19.667213 139725048211200 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.45721063017845154, loss=1.6500476598739624
I0429 02:12:51.337690 139725056603904 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.47025662660598755, loss=1.5898874998092651
I0429 02:14:16.499907 139898416887616 spec.py:298] Evaluating on the training split.
I0429 02:14:53.529060 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 02:15:33.529536 139898416887616 spec.py:326] Evaluating on the test split.
I0429 02:15:53.914088 139898416887616 submission_runner.py:415] Time since start: 7728.31s, 	Step: 7794, 	{'train/ctc_loss': DeviceArray(0.3930027, dtype=float32), 'train/wer': 0.14513127862907263, 'validation/ctc_loss': DeviceArray(0.7285207, dtype=float32), 'validation/wer': 0.2185742264758946, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47339234, dtype=float32), 'test/wer': 0.1601771169743871, 'test/num_examples': 2472, 'score': 7308.456769704819, 'total_duration': 7728.307473897934, 'accumulated_submission_time': 7308.456769704819, 'accumulated_eval_time': 419.6377902030945, 'accumulated_logging_time': 0.1063528060913086}
I0429 02:15:53.937460 139725486683904 logging_writer.py:48] [7794] accumulated_eval_time=419.637790, accumulated_logging_time=0.106353, accumulated_submission_time=7308.456770, global_step=7794, preemption_count=0, score=7308.456770, test/ctc_loss=0.4733923375606537, test/num_examples=2472, test/wer=0.160177, total_duration=7728.307474, train/ctc_loss=0.3930026888847351, train/wer=0.145131, validation/ctc_loss=0.7285206913948059, validation/num_examples=5348, validation/wer=0.218574
I0429 02:16:00.330560 139725478291200 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4361146092414856, loss=1.6046977043151855
I0429 02:17:32.335971 139725486683904 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5435953736305237, loss=1.6235722303390503
I0429 02:19:04.960837 139725478291200 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.28660130500793457, loss=1.6262038946151733
I0429 02:20:36.758828 139725486683904 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3410627245903015, loss=1.588899850845337
I0429 02:22:08.404746 139725478291200 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.36446741223335266, loss=1.5790928602218628
I0429 02:23:43.884752 139725159003904 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.35795989632606506, loss=1.5935426950454712
I0429 02:25:15.455394 139725150611200 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.35329514741897583, loss=1.577918291091919
I0429 02:26:47.267923 139725159003904 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3100142478942871, loss=1.5541571378707886
I0429 02:28:19.580702 139725150611200 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3588711619377136, loss=1.5709019899368286
I0429 02:29:51.256315 139725159003904 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.36480438709259033, loss=1.5077272653579712
I0429 02:31:22.843838 139725150611200 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4070150554180145, loss=1.637255072593689
I0429 02:32:54.659227 139725159003904 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3965815603733063, loss=1.575060486793518
I0429 02:34:26.491421 139725150611200 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.30744972825050354, loss=1.5358283519744873
I0429 02:35:58.538379 139725159003904 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.36980772018432617, loss=1.5776801109313965
I0429 02:37:30.818715 139725150611200 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3909788131713867, loss=1.5831053256988525
I0429 02:39:06.130673 139725159003904 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.404266893863678, loss=1.4956835508346558
I0429 02:40:37.907839 139725150611200 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4690534174442291, loss=1.5396074056625366
I0429 02:42:09.767919 139725159003904 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4594641923904419, loss=1.4900054931640625
I0429 02:43:42.038752 139725150611200 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3769025504589081, loss=1.5501298904418945
I0429 02:45:14.468339 139725159003904 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3990415036678314, loss=1.5255661010742188
I0429 02:46:46.690917 139725150611200 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.34470921754837036, loss=1.4967765808105469
I0429 02:48:18.658526 139725159003904 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4365256726741791, loss=1.531186580657959
I0429 02:49:50.835193 139725150611200 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3579978048801422, loss=1.4994043111801147
I0429 02:51:23.011500 139725159003904 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.4184015393257141, loss=1.4617341756820679
I0429 02:52:55.519684 139725150611200 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3278917074203491, loss=1.5430840253829956
I0429 02:54:30.580888 139725159003904 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.32029208540916443, loss=1.447943925857544
I0429 02:55:54.056940 139898416887616 spec.py:298] Evaluating on the training split.
I0429 02:56:31.611561 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 02:57:10.497896 139898416887616 spec.py:326] Evaluating on the test split.
I0429 02:57:30.675984 139898416887616 submission_runner.py:415] Time since start: 10225.07s, 	Step: 10391, 	{'train/ctc_loss': DeviceArray(0.31769958, dtype=float32), 'train/wer': 0.11792981390526658, 'validation/ctc_loss': DeviceArray(0.62080353, dtype=float32), 'validation/wer': 0.18717980877770166, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.39088482, dtype=float32), 'test/wer': 0.13436109926268966, 'test/num_examples': 2472, 'score': 9708.526545524597, 'total_duration': 10225.07002902031, 'accumulated_submission_time': 9708.526545524597, 'accumulated_eval_time': 516.2536911964417, 'accumulated_logging_time': 0.14785361289978027}
I0429 02:57:30.697612 139725159003904 logging_writer.py:48] [10391] accumulated_eval_time=516.253691, accumulated_logging_time=0.147854, accumulated_submission_time=9708.526546, global_step=10391, preemption_count=0, score=9708.526546, test/ctc_loss=0.3908848166465759, test/num_examples=2472, test/wer=0.134361, total_duration=10225.070029, train/ctc_loss=0.3176995813846588, train/wer=0.117930, validation/ctc_loss=0.6208035349845886, validation/num_examples=5348, validation/wer=0.187180
I0429 02:57:39.842165 139725150611200 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4417553246021271, loss=1.501929521560669
I0429 02:59:11.708742 139725159003904 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3564291000366211, loss=1.5335133075714111
I0429 03:00:44.607400 139725150611200 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3453747034072876, loss=1.4471776485443115
I0429 03:02:16.819553 139725159003904 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.364012748003006, loss=1.4692065715789795
I0429 03:03:48.671676 139725150611200 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.41516053676605225, loss=1.6042836904525757
I0429 03:05:20.676702 139725159003904 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.3934403955936432, loss=1.5294288396835327
I0429 03:06:53.705188 139725150611200 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.42906883358955383, loss=1.4712163209915161
I0429 03:08:25.878026 139725159003904 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.28970643877983093, loss=1.4444211721420288
I0429 03:09:58.177157 139725150611200 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.37486886978149414, loss=1.4914475679397583
I0429 03:11:30.244214 139725159003904 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.40287891030311584, loss=1.4200842380523682
I0429 03:13:06.044795 139725159003904 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.3160863220691681, loss=1.4599400758743286
I0429 03:14:37.915259 139725150611200 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3473718762397766, loss=1.4533882141113281
I0429 03:16:10.531396 139725159003904 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4683690369129181, loss=1.463442325592041
I0429 03:17:42.971674 139725150611200 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3276793658733368, loss=1.3911439180374146
I0429 03:19:15.383890 139725159003904 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4044981002807617, loss=1.4476629495620728
I0429 03:20:47.345007 139725150611200 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.34603938460350037, loss=1.401166558265686
I0429 03:22:19.837127 139725159003904 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3565767705440521, loss=1.3792822360992432
I0429 03:23:51.960482 139725150611200 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.42372381687164307, loss=1.4975786209106445
I0429 03:25:23.850494 139725159003904 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.345596045255661, loss=1.4455846548080444
I0429 03:26:56.172989 139725150611200 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3283781409263611, loss=1.4039077758789062
I0429 03:28:31.544404 139725159003904 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4185575544834137, loss=1.3800469636917114
I0429 03:30:03.853319 139725150611200 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3431603014469147, loss=1.4284944534301758
I0429 03:31:36.289770 139725159003904 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3908475637435913, loss=1.3397281169891357
I0429 03:33:08.469260 139725150611200 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3644396662712097, loss=1.440032958984375
I0429 03:34:40.710855 139725159003904 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.32579267024993896, loss=1.40301513671875
I0429 03:36:12.659798 139725150611200 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.39212217926979065, loss=1.4274076223373413
I0429 03:37:30.724208 139898416887616 spec.py:298] Evaluating on the training split.
I0429 03:38:07.134190 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 03:38:46.393510 139898416887616 spec.py:326] Evaluating on the test split.
I0429 03:39:06.703740 139898416887616 submission_runner.py:415] Time since start: 12721.10s, 	Step: 12986, 	{'train/ctc_loss': DeviceArray(0.27296534, dtype=float32), 'train/wer': 0.10288873426254104, 'validation/ctc_loss': DeviceArray(0.5687029, dtype=float32), 'validation/wer': 0.17310345492961823, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3492545, dtype=float32), 'test/wer': 0.11991956614465907, 'test/num_examples': 2472, 'score': 12108.504441976547, 'total_duration': 12721.097202539444, 'accumulated_submission_time': 12108.504441976547, 'accumulated_eval_time': 612.2295000553131, 'accumulated_logging_time': 0.18547606468200684}
I0429 03:39:06.726012 139725194843904 logging_writer.py:48] [12986] accumulated_eval_time=612.229500, accumulated_logging_time=0.185476, accumulated_submission_time=12108.504442, global_step=12986, preemption_count=0, score=12108.504442, test/ctc_loss=0.3492544889450073, test/num_examples=2472, test/wer=0.119920, total_duration=12721.097203, train/ctc_loss=0.27296534180641174, train/wer=0.102889, validation/ctc_loss=0.5687028765678406, validation/num_examples=5348, validation/wer=0.173103
I0429 03:39:20.445966 139725186451200 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.4057108163833618, loss=1.481982946395874
I0429 03:40:52.664761 139725194843904 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.33777227997779846, loss=1.3679789304733276
I0429 03:42:24.697740 139725186451200 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.34119313955307007, loss=1.4041547775268555
I0429 03:43:56.802293 139725194843904 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4283031225204468, loss=1.3636394739151
I0429 03:45:32.332539 139725194843904 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.4104873538017273, loss=1.3771781921386719
I0429 03:47:04.923627 139725186451200 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.36814332008361816, loss=1.3840086460113525
I0429 03:48:36.618346 139725194843904 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3831876516342163, loss=1.3665781021118164
I0429 03:50:08.922563 139725186451200 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3683370351791382, loss=1.3937071561813354
I0429 03:51:41.106993 139725194843904 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.39477312564849854, loss=1.3601771593093872
I0429 03:53:13.220948 139725186451200 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.31646785140037537, loss=1.366577386856079
I0429 03:54:44.934478 139725194843904 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3440238833427429, loss=1.3569397926330566
I0429 03:56:17.014500 139725186451200 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.31477874517440796, loss=1.3519107103347778
I0429 03:57:48.857214 139725194843904 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.35952410101890564, loss=1.3867632150650024
I0429 03:59:20.860095 139725186451200 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.4293369948863983, loss=1.3442530632019043
I0429 04:00:52.231896 139725194843904 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.4713541865348816, loss=1.428316354751587
I0429 04:02:28.453398 139725194843904 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.35989224910736084, loss=1.3513380289077759
I0429 04:04:00.503595 139725186451200 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.38060081005096436, loss=1.3661819696426392
I0429 04:05:32.544148 139725194843904 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2917150855064392, loss=1.3467825651168823
I0429 04:07:04.579530 139725186451200 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3450142741203308, loss=1.3738353252410889
I0429 04:08:37.188880 139725194843904 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.33598995208740234, loss=1.414936900138855
I0429 04:10:08.814633 139725186451200 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.28942203521728516, loss=1.346046805381775
I0429 04:11:40.527072 139725194843904 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3246680498123169, loss=1.2890980243682861
I0429 04:13:12.164929 139725186451200 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.42416688799858093, loss=1.3696657419204712
I0429 04:14:44.354527 139725194843904 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.314282089471817, loss=1.4361017942428589
I0429 04:16:16.330729 139725186451200 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.38167616724967957, loss=1.4274331331253052
I0429 04:17:52.608492 139725194843904 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.33705905079841614, loss=1.3542284965515137
I0429 04:19:07.274928 139898416887616 spec.py:298] Evaluating on the training split.
I0429 04:19:44.563069 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 04:20:23.962512 139898416887616 spec.py:326] Evaluating on the test split.
I0429 04:20:44.039711 139898416887616 submission_runner.py:415] Time since start: 15218.43s, 	Step: 15582, 	{'train/ctc_loss': DeviceArray(0.26935056, dtype=float32), 'train/wer': 0.09776955044251565, 'validation/ctc_loss': DeviceArray(0.5434005, dtype=float32), 'validation/wer': 0.16291522349467916, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32438818, dtype=float32), 'test/wer': 0.10964190685109582, 'test/num_examples': 2472, 'score': 14509.005048513412, 'total_duration': 15218.433732032776, 'accumulated_submission_time': 14509.005048513412, 'accumulated_eval_time': 708.9911298751831, 'accumulated_logging_time': 0.2244892120361328}
I0429 04:20:44.062325 139725486683904 logging_writer.py:48] [15582] accumulated_eval_time=708.991130, accumulated_logging_time=0.224489, accumulated_submission_time=14509.005049, global_step=15582, preemption_count=0, score=14509.005049, test/ctc_loss=0.32438817620277405, test/num_examples=2472, test/wer=0.109642, total_duration=15218.433732, train/ctc_loss=0.2693505585193634, train/wer=0.097770, validation/ctc_loss=0.5434005260467529, validation/num_examples=5348, validation/wer=0.162915
I0429 04:21:01.365059 139725478291200 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4500221908092499, loss=1.283895492553711
I0429 04:22:33.710416 139725486683904 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3439990282058716, loss=1.3006324768066406
I0429 04:24:06.126817 139725478291200 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.29667627811431885, loss=1.341970443725586
I0429 04:25:38.278472 139725486683904 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.35167616605758667, loss=1.2728749513626099
I0429 04:27:10.561179 139725478291200 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.38006991147994995, loss=1.3633955717086792
I0429 04:28:43.190788 139725486683904 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3885751962661743, loss=1.2748050689697266
I0429 04:30:15.210855 139725478291200 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.339973121881485, loss=1.3335297107696533
I0429 04:31:47.838432 139725486683904 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.32941827178001404, loss=1.3349026441574097
I0429 04:33:20.828456 139725478291200 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3198089599609375, loss=1.3000820875167847
I0429 04:34:56.744201 139725486683904 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3661600947380066, loss=1.3115447759628296
I0429 04:36:28.768040 139725478291200 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.29164570569992065, loss=1.3118617534637451
I0429 04:38:00.832681 139725486683904 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2937251627445221, loss=1.305356502532959
I0429 04:39:33.198019 139725478291200 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3330172598361969, loss=1.3250575065612793
I0429 04:41:05.533224 139725486683904 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.346866637468338, loss=1.296525478363037
I0429 04:42:37.804653 139725478291200 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2875675857067108, loss=1.3107115030288696
I0429 04:44:09.855108 139725486683904 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.37520191073417664, loss=1.3111400604248047
I0429 04:45:41.985042 139725478291200 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3049730658531189, loss=1.3441474437713623
I0429 04:47:14.409789 139725486683904 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.34405869245529175, loss=1.2907408475875854
I0429 04:48:46.255004 139725478291200 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.35526153445243835, loss=1.269706130027771
I0429 04:50:18.042407 139725486683904 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.36226603388786316, loss=1.3397382497787476
I0429 04:51:54.954696 139725159003904 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2675860822200775, loss=1.2694859504699707
I0429 04:53:27.067431 139725150611200 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.31554466485977173, loss=1.3187497854232788
I0429 04:54:59.320325 139725159003904 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.34519073367118835, loss=1.3057658672332764
I0429 04:56:31.532356 139725150611200 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3408176004886627, loss=1.3474153280258179
I0429 04:58:03.904364 139725159003904 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4009963870048523, loss=1.2854254245758057
I0429 04:59:36.452761 139725150611200 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.39480990171432495, loss=1.3000999689102173
I0429 05:00:44.305188 139898416887616 spec.py:298] Evaluating on the training split.
I0429 05:01:22.107478 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 05:02:02.419710 139898416887616 spec.py:326] Evaluating on the test split.
I0429 05:02:22.913187 139898416887616 submission_runner.py:415] Time since start: 17717.31s, 	Step: 18174, 	{'train/ctc_loss': DeviceArray(0.22502439, dtype=float32), 'train/wer': 0.08539659243188054, 'validation/ctc_loss': DeviceArray(0.500098, dtype=float32), 'validation/wer': 0.15259192080965567, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30141842, dtype=float32), 'test/wer': 0.10295939715231654, 'test/num_examples': 2472, 'score': 16909.20236825943, 'total_duration': 17717.30684518814, 'accumulated_submission_time': 16909.20236825943, 'accumulated_eval_time': 807.5956008434296, 'accumulated_logging_time': 0.26116347312927246}
I0429 05:02:22.936687 139725486683904 logging_writer.py:48] [18174] accumulated_eval_time=807.595601, accumulated_logging_time=0.261163, accumulated_submission_time=16909.202368, global_step=18174, preemption_count=0, score=16909.202368, test/ctc_loss=0.3014184236526489, test/num_examples=2472, test/wer=0.102959, total_duration=17717.306845, train/ctc_loss=0.22502438724040985, train/wer=0.085397, validation/ctc_loss=0.5000979900360107, validation/num_examples=5348, validation/wer=0.152592
I0429 05:02:47.742333 139725478291200 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.2624867260456085, loss=1.2454798221588135
I0429 05:04:19.885540 139725486683904 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3516318202018738, loss=1.3336563110351562
I0429 05:05:52.712626 139725478291200 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3337479829788208, loss=1.2912687063217163
I0429 05:07:25.365798 139725486683904 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3970828056335449, loss=1.3369544744491577
I0429 05:09:01.178021 139725159003904 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.4007134437561035, loss=1.3311185836791992
I0429 05:10:33.393828 139725150611200 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3090794086456299, loss=1.3136069774627686
I0429 05:12:05.808739 139725159003904 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.31475234031677246, loss=1.2987614870071411
I0429 05:13:38.430738 139725150611200 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3815905749797821, loss=1.3554904460906982
I0429 05:15:10.465437 139725159003904 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2792128026485443, loss=1.1941325664520264
I0429 05:16:42.790244 139725150611200 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.46815699338912964, loss=1.2394107580184937
I0429 05:18:15.046577 139725159003904 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.31921687722206116, loss=1.289089322090149
I0429 05:19:46.901361 139725150611200 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3647381067276001, loss=1.29757559299469
I0429 05:21:19.596647 139725159003904 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.33809342980384827, loss=1.2818721532821655
I0429 05:22:51.953713 139725150611200 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3941350281238556, loss=1.2704002857208252
I0429 05:24:27.996890 139725486683904 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.300563246011734, loss=1.252374529838562
I0429 05:26:00.500440 139725478291200 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3517027497291565, loss=1.2861073017120361
I0429 05:27:32.661587 139725486683904 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.32549580931663513, loss=1.2637889385223389
I0429 05:29:04.786292 139725478291200 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.34656640887260437, loss=1.314611792564392
I0429 05:30:36.638895 139898416887616 spec.py:298] Evaluating on the training split.
I0429 05:31:14.162142 139898416887616 spec.py:310] Evaluating on the validation split.
I0429 05:31:54.324879 139898416887616 spec.py:326] Evaluating on the test split.
I0429 05:32:14.708812 139898416887616 submission_runner.py:415] Time since start: 19509.10s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.18972267, dtype=float32), 'train/wer': 0.07676746665279909, 'validation/ctc_loss': DeviceArray(0.48166442, dtype=float32), 'validation/wer': 0.14904147652172234, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.28169098, dtype=float32), 'test/wer': 0.09838929173521825, 'test/num_examples': 2472, 'score': 18602.86381816864, 'total_duration': 19509.102707624435, 'accumulated_submission_time': 18602.86381816864, 'accumulated_eval_time': 905.6622250080109, 'accumulated_logging_time': 0.3017394542694092}
I0429 05:32:14.732495 139725486683904 logging_writer.py:48] [20000] accumulated_eval_time=905.662225, accumulated_logging_time=0.301739, accumulated_submission_time=18602.863818, global_step=20000, preemption_count=0, score=18602.863818, test/ctc_loss=0.2816909849643707, test/num_examples=2472, test/wer=0.098389, total_duration=19509.102708, train/ctc_loss=0.1897226721048355, train/wer=0.076767, validation/ctc_loss=0.48166441917419434, validation/num_examples=5348, validation/wer=0.149041
I0429 05:32:14.754503 139725478291200 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=18602.863818
I0429 05:32:14.944732 139898416887616 checkpoints.py:356] Saving checkpoint at step: 20000
I0429 05:32:15.628234 139898416887616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1/checkpoint_20000
I0429 05:32:15.642641 139898416887616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_adafactor/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0429 05:32:17.070250 139898416887616 submission_runner.py:578] Tuning trial 1/1
I0429 05:32:17.070499 139898416887616 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 05:32:17.081815 139898416887616 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.389648, dtype=float32), 'train/wer': 1.6704703996001915, 'validation/ctc_loss': DeviceArray(30.50612, dtype=float32), 'validation/wer': 1.4180841107970168, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.613726, dtype=float32), 'test/wer': 1.4736660370076982, 'test/num_examples': 2472, 'score': 107.02790260314941, 'total_duration': 238.87737894058228, 'accumulated_submission_time': 107.02790260314941, 'accumulated_eval_time': 131.84931564331055, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2577, {'train/ctc_loss': DeviceArray(1.9334558, dtype=float32), 'train/wer': 0.46180305246029946, 'validation/ctc_loss': DeviceArray(2.4250479, dtype=float32), 'validation/wer': 0.5093729799612152, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.060485, dtype=float32), 'test/wer': 0.4648711230272378, 'test/num_examples': 2472, 'score': 2507.4045674800873, 'total_duration': 2733.820568084717, 'accumulated_submission_time': 2507.4045674800873, 'accumulated_eval_time': 226.3448896408081, 'accumulated_logging_time': 0.03588533401489258, 'global_step': 2577, 'preemption_count': 0}), (5185, {'train/ctc_loss': DeviceArray(0.5892229, dtype=float32), 'train/wer': 0.19940867804917028, 'validation/ctc_loss': DeviceArray(0.92123014, dtype=float32), 'validation/wer': 0.26985306177580104, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6343679, dtype=float32), 'test/wer': 0.2074624743566307, 'test/num_examples': 2472, 'score': 4908.125363588333, 'total_duration': 5230.4951729774475, 'accumulated_submission_time': 4908.125363588333, 'accumulated_eval_time': 322.22739601135254, 'accumulated_logging_time': 0.0702519416809082, 'global_step': 5185, 'preemption_count': 0}), (7794, {'train/ctc_loss': DeviceArray(0.3930027, dtype=float32), 'train/wer': 0.14513127862907263, 'validation/ctc_loss': DeviceArray(0.7285207, dtype=float32), 'validation/wer': 0.2185742264758946, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47339234, dtype=float32), 'test/wer': 0.1601771169743871, 'test/num_examples': 2472, 'score': 7308.456769704819, 'total_duration': 7728.307473897934, 'accumulated_submission_time': 7308.456769704819, 'accumulated_eval_time': 419.6377902030945, 'accumulated_logging_time': 0.1063528060913086, 'global_step': 7794, 'preemption_count': 0}), (10391, {'train/ctc_loss': DeviceArray(0.31769958, dtype=float32), 'train/wer': 0.11792981390526658, 'validation/ctc_loss': DeviceArray(0.62080353, dtype=float32), 'validation/wer': 0.18717980877770166, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.39088482, dtype=float32), 'test/wer': 0.13436109926268966, 'test/num_examples': 2472, 'score': 9708.526545524597, 'total_duration': 10225.07002902031, 'accumulated_submission_time': 9708.526545524597, 'accumulated_eval_time': 516.2536911964417, 'accumulated_logging_time': 0.14785361289978027, 'global_step': 10391, 'preemption_count': 0}), (12986, {'train/ctc_loss': DeviceArray(0.27296534, dtype=float32), 'train/wer': 0.10288873426254104, 'validation/ctc_loss': DeviceArray(0.5687029, dtype=float32), 'validation/wer': 0.17310345492961823, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3492545, dtype=float32), 'test/wer': 0.11991956614465907, 'test/num_examples': 2472, 'score': 12108.504441976547, 'total_duration': 12721.097202539444, 'accumulated_submission_time': 12108.504441976547, 'accumulated_eval_time': 612.2295000553131, 'accumulated_logging_time': 0.18547606468200684, 'global_step': 12986, 'preemption_count': 0}), (15582, {'train/ctc_loss': DeviceArray(0.26935056, dtype=float32), 'train/wer': 0.09776955044251565, 'validation/ctc_loss': DeviceArray(0.5434005, dtype=float32), 'validation/wer': 0.16291522349467916, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32438818, dtype=float32), 'test/wer': 0.10964190685109582, 'test/num_examples': 2472, 'score': 14509.005048513412, 'total_duration': 15218.433732032776, 'accumulated_submission_time': 14509.005048513412, 'accumulated_eval_time': 708.9911298751831, 'accumulated_logging_time': 0.2244892120361328, 'global_step': 15582, 'preemption_count': 0}), (18174, {'train/ctc_loss': DeviceArray(0.22502439, dtype=float32), 'train/wer': 0.08539659243188054, 'validation/ctc_loss': DeviceArray(0.500098, dtype=float32), 'validation/wer': 0.15259192080965567, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30141842, dtype=float32), 'test/wer': 0.10295939715231654, 'test/num_examples': 2472, 'score': 16909.20236825943, 'total_duration': 17717.30684518814, 'accumulated_submission_time': 16909.20236825943, 'accumulated_eval_time': 807.5956008434296, 'accumulated_logging_time': 0.26116347312927246, 'global_step': 18174, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.18972267, dtype=float32), 'train/wer': 0.07676746665279909, 'validation/ctc_loss': DeviceArray(0.48166442, dtype=float32), 'validation/wer': 0.14904147652172234, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.28169098, dtype=float32), 'test/wer': 0.09838929173521825, 'test/num_examples': 2472, 'score': 18602.86381816864, 'total_duration': 19509.102707624435, 'accumulated_submission_time': 18602.86381816864, 'accumulated_eval_time': 905.6622250080109, 'accumulated_logging_time': 0.3017394542694092, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0429 05:32:17.082081 139898416887616 submission_runner.py:581] Timing: 18602.86381816864
I0429 05:32:17.082181 139898416887616 submission_runner.py:582] ====================
I0429 05:32:17.083131 139898416887616 submission_runner.py:645] Final librispeech_conformer score: 18602.86381816864
