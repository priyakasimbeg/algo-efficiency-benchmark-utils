python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_04-29-2023-13-31-36.log
I0429 13:31:57.227091 140703374882624 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax.
I0429 13:31:57.299554 140703374882624 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 13:31:58.125309 140703374882624 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0429 13:31:58.125936 140703374882624 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 13:31:58.131844 140703374882624 submission_runner.py:538] Using RNG seed 4151970012
I0429 13:32:00.772007 140703374882624 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 13:32:00.772226 140703374882624 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1.
I0429 13:32:00.772433 140703374882624 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1/hparams.json.
I0429 13:32:00.909382 140703374882624 submission_runner.py:241] Initializing dataset.
I0429 13:32:00.909600 140703374882624 submission_runner.py:248] Initializing model.
I0429 13:32:07.392676 140703374882624 submission_runner.py:258] Initializing optimizer.
I0429 13:32:08.325153 140703374882624 submission_runner.py:265] Initializing metrics bundle.
I0429 13:32:08.325348 140703374882624 submission_runner.py:282] Initializing checkpoint and logger.
I0429 13:32:08.326487 140703374882624 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0429 13:32:08.326756 140703374882624 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 13:32:08.326819 140703374882624 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 13:32:09.110233 140703374882624 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0429 13:32:09.111172 140703374882624 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1/flags_0.json.
I0429 13:32:09.117551 140703374882624 submission_runner.py:318] Starting training loop.
I0429 13:32:09.330653 140703374882624 input_pipeline.py:20] Loading split = train-clean-100
I0429 13:32:09.367043 140703374882624 input_pipeline.py:20] Loading split = train-clean-360
I0429 13:32:09.702022 140703374882624 input_pipeline.py:20] Loading split = train-other-500
2023-04-29 13:33:27.089789: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-29 13:33:27.861077: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0429 13:33:30.305884 140527865423616 logging_writer.py:48] [0] global_step=0, grad_norm=45.185218811035156, loss=31.88578224182129
I0429 13:33:30.334164 140703374882624 spec.py:298] Evaluating on the training split.
I0429 13:33:30.445222 140703374882624 input_pipeline.py:20] Loading split = train-clean-100
I0429 13:33:30.478332 140703374882624 input_pipeline.py:20] Loading split = train-clean-360
I0429 13:33:30.781247 140703374882624 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0429 13:34:46.707307 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 13:34:46.783740 140703374882624 input_pipeline.py:20] Loading split = dev-clean
I0429 13:34:46.790584 140703374882624 input_pipeline.py:20] Loading split = dev-other
I0429 13:35:33.625846 140703374882624 spec.py:326] Evaluating on the test split.
I0429 13:35:33.699224 140703374882624 input_pipeline.py:20] Loading split = test-clean
I0429 13:36:06.205188 140703374882624 submission_runner.py:415] Time since start: 237.09s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.185383, dtype=float32), 'train/wer': 1.5947775024467443, 'validation/ctc_loss': DeviceArray(30.23235, dtype=float32), 'validation/wer': 1.1394321218728594, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.346806, dtype=float32), 'test/wer': 1.2072187354010522, 'test/num_examples': 2472, 'score': 81.21641492843628, 'total_duration': 237.08610701560974, 'accumulated_submission_time': 81.21641492843628, 'accumulated_eval_time': 155.8695342540741, 'accumulated_logging_time': 0}
I0429 13:36:06.230730 140523847284480 logging_writer.py:48] [1] accumulated_eval_time=155.869534, accumulated_logging_time=0, accumulated_submission_time=81.216415, global_step=1, preemption_count=0, score=81.216415, test/ctc_loss=30.346805572509766, test/num_examples=2472, test/wer=1.207219, total_duration=237.086107, train/ctc_loss=31.185382843017578, train/wer=1.594778, validation/ctc_loss=30.232349395751953, validation/num_examples=5348, validation/wer=1.139432
I0429 13:39:07.295143 140529081833216 logging_writer.py:48] [100] global_step=100, grad_norm=2.067453145980835, loss=6.533634185791016
I0429 13:41:36.094678 140529090225920 logging_writer.py:48] [200] global_step=200, grad_norm=4.288929462432861, loss=5.9305033683776855
I0429 13:44:05.152356 140529081833216 logging_writer.py:48] [300] global_step=300, grad_norm=4.251312732696533, loss=5.841409683227539
I0429 13:46:33.760210 140529090225920 logging_writer.py:48] [400] global_step=400, grad_norm=0.30033743381500244, loss=5.820716381072998
I0429 13:49:02.500367 140529081833216 logging_writer.py:48] [500] global_step=500, grad_norm=4.308667182922363, loss=5.818746089935303
I0429 13:51:31.343190 140529090225920 logging_writer.py:48] [600] global_step=600, grad_norm=0.39699944853782654, loss=5.784421443939209
I0429 13:54:00.272578 140529081833216 logging_writer.py:48] [700] global_step=700, grad_norm=2.9728684425354004, loss=5.803907871246338
I0429 13:56:29.253065 140529090225920 logging_writer.py:48] [800] global_step=800, grad_norm=0.5753332376480103, loss=5.7833662033081055
I0429 13:58:58.291260 140529081833216 logging_writer.py:48] [900] global_step=900, grad_norm=2.7080984115600586, loss=5.790125370025635
I0429 14:01:27.348371 140529090225920 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3411640226840973, loss=5.7808732986450195
I0429 14:03:59.494061 140530593978112 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.774562656879425, loss=5.791890621185303
I0429 14:06:28.394782 140530585585408 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.31793132424354553, loss=5.764875888824463
I0429 14:08:57.484961 140530593978112 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.270624876022339, loss=5.779911518096924
I0429 14:11:26.891471 140530585585408 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8724613785743713, loss=5.66715145111084
I0429 14:13:55.919921 140530593978112 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.2667180299758911, loss=5.559643745422363
I0429 14:16:06.501169 140703374882624 spec.py:298] Evaluating on the training split.
I0429 14:16:36.354035 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 14:17:12.577306 140703374882624 spec.py:326] Evaluating on the test split.
I0429 14:17:30.934830 140703374882624 submission_runner.py:415] Time since start: 2721.82s, 	Step: 1589, 	{'train/ctc_loss': DeviceArray(6.0062547, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.0638266, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.037105, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2481.4508645534515, 'total_duration': 2721.815409421921, 'accumulated_submission_time': 2481.4508645534515, 'accumulated_eval_time': 240.30140352249146, 'accumulated_logging_time': 0.04048013687133789}
I0429 14:17:30.956903 140530302138112 logging_writer.py:48] [1589] accumulated_eval_time=240.301404, accumulated_logging_time=0.040480, accumulated_submission_time=2481.450865, global_step=1589, preemption_count=0, score=2481.450865, test/ctc_loss=6.037105083465576, test/num_examples=2472, test/wer=0.899580, total_duration=2721.815409, train/ctc_loss=6.00625467300415, train/wer=0.944636, validation/ctc_loss=6.063826560974121, validation/num_examples=5348, validation/wer=0.895995
I0429 14:17:48.814039 140530293745408 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.5349948406219482, loss=5.406895637512207
I0429 14:20:17.052930 140530302138112 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8853985667228699, loss=4.983002662658691
I0429 14:22:45.010285 140530293745408 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1902484893798828, loss=4.331910133361816
I0429 14:25:13.150669 140530302138112 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.5499907732009888, loss=4.000138759613037
I0429 14:27:41.306689 140530293745408 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0403234958648682, loss=3.7813897132873535
I0429 14:30:12.668078 140530302138112 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.236477017402649, loss=3.5251119136810303
I0429 14:32:40.478147 140530293745408 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.134016752243042, loss=3.4304356575012207
I0429 14:35:08.167153 140530302138112 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2736421823501587, loss=3.256253957748413
I0429 14:37:35.876181 140530293745408 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.9350415468215942, loss=3.23179030418396
I0429 14:40:03.453243 140530302138112 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8833878636360168, loss=3.142791271209717
I0429 14:42:30.989959 140530293745408 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.5134992599487305, loss=3.046983242034912
I0429 14:44:58.832842 140530302138112 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9191828370094299, loss=2.9578425884246826
I0429 14:47:26.485630 140530293745408 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2239052057266235, loss=2.9240777492523193
I0429 14:49:54.111968 140530302138112 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0032919645309448, loss=2.8632242679595947
I0429 14:52:22.077599 140530293745408 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.7297199964523315, loss=2.7903130054473877
I0429 14:54:52.943554 140530302138112 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8694897890090942, loss=2.7055561542510986
I0429 14:57:20.415983 140530293745408 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0805925130844116, loss=2.6564810276031494
I0429 14:57:31.901606 140703374882624 spec.py:298] Evaluating on the training split.
I0429 14:58:17.472730 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 14:58:59.620930 140703374882624 spec.py:326] Evaluating on the test split.
I0429 14:59:20.872130 140703374882624 submission_runner.py:415] Time since start: 5231.75s, 	Step: 3209, 	{'train/ctc_loss': DeviceArray(2.7745118, dtype=float32), 'train/wer': 0.6253096449404683, 'validation/ctc_loss': DeviceArray(3.1379733, dtype=float32), 'validation/wer': 0.6315545736090074, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.8346608, dtype=float32), 'test/wer': 0.6008774602400828, 'test/num_examples': 2472, 'score': 4882.357090950012, 'total_duration': 5231.7526314258575, 'accumulated_submission_time': 4882.357090950012, 'accumulated_eval_time': 349.2700505256653, 'accumulated_logging_time': 0.07782959938049316}
I0429 14:59:20.894246 140530302138112 logging_writer.py:48] [3209] accumulated_eval_time=349.270051, accumulated_logging_time=0.077830, accumulated_submission_time=4882.357091, global_step=3209, preemption_count=0, score=4882.357091, test/ctc_loss=2.834660768508911, test/num_examples=2472, test/wer=0.600877, total_duration=5231.752631, train/ctc_loss=2.7745118141174316, train/wer=0.625310, validation/ctc_loss=3.1379733085632324, validation/num_examples=5348, validation/wer=0.631555
I0429 15:01:36.504959 140530293745408 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9453997611999512, loss=2.7657713890075684
I0429 15:04:04.228472 140530302138112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9046576023101807, loss=2.6189305782318115
I0429 15:06:31.957085 140530293745408 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7671363949775696, loss=2.517727851867676
I0429 15:08:59.508693 140530302138112 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9727367758750916, loss=2.5393283367156982
I0429 15:11:27.006925 140530293745408 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.2391873598098755, loss=2.4401304721832275
I0429 15:13:54.515954 140530302138112 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.5721009969711304, loss=2.4206771850585938
I0429 15:16:21.993733 140530293745408 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8853079080581665, loss=2.4241583347320557
I0429 15:18:49.506124 140530302138112 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0290582180023193, loss=2.4046051502227783
I0429 15:21:16.929387 140530293745408 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8445870280265808, loss=2.2568359375
I0429 15:23:48.192662 140530302138112 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8102495074272156, loss=2.3272883892059326
I0429 15:26:15.549326 140530293745408 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7078108191490173, loss=2.2644596099853516
I0429 15:28:42.957854 140530302138112 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8240894079208374, loss=2.215747356414795
I0429 15:31:10.303229 140530293745408 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7054138779640198, loss=2.191272258758545
I0429 15:33:37.717282 140530302138112 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6914944052696228, loss=2.2011988162994385
I0429 15:36:05.142568 140530293745408 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8265522122383118, loss=2.1272170543670654
I0429 15:38:32.954437 140530302138112 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8321106433868408, loss=2.1354613304138184
I0429 15:39:21.445920 140703374882624 spec.py:298] Evaluating on the training split.
I0429 15:40:09.774849 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 15:40:54.466392 140703374882624 spec.py:326] Evaluating on the test split.
I0429 15:41:16.703353 140703374882624 submission_runner.py:415] Time since start: 7747.58s, 	Step: 4834, 	{'train/ctc_loss': DeviceArray(0.89442486, dtype=float32), 'train/wer': 0.29831691362124496, 'validation/ctc_loss': DeviceArray(1.2458347, dtype=float32), 'validation/wer': 0.3603411513859275, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.95447236, dtype=float32), 'test/wer': 0.3035768691731156, 'test/num_examples': 2472, 'score': 7282.867134094238, 'total_duration': 7747.582359075546, 'accumulated_submission_time': 7282.867134094238, 'accumulated_eval_time': 464.52415132522583, 'accumulated_logging_time': 0.11585044860839844}
I0429 15:41:16.726190 140530010298112 logging_writer.py:48] [4834] accumulated_eval_time=464.524151, accumulated_logging_time=0.115850, accumulated_submission_time=7282.867134, global_step=4834, preemption_count=0, score=7282.867134, test/ctc_loss=0.9544723629951477, test/num_examples=2472, test/wer=0.303577, total_duration=7747.582359, train/ctc_loss=0.8944248557090759, train/wer=0.298317, validation/ctc_loss=1.2458347082138062, validation/num_examples=5348, validation/wer=0.360341
I0429 15:42:55.781353 140530001905408 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.745402455329895, loss=2.0986199378967285
I0429 15:45:23.649012 140530010298112 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.718395471572876, loss=2.0294172763824463
I0429 15:47:51.367109 140530001905408 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.722995400428772, loss=2.0864953994750977
I0429 15:50:22.425551 140530010298112 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9481997489929199, loss=1.9809051752090454
I0429 15:52:50.053500 140530001905408 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.0571355819702148, loss=2.020693302154541
I0429 15:55:17.629780 140530010298112 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7936254143714905, loss=2.0161633491516113
I0429 15:57:45.228683 140530001905408 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6292237639427185, loss=1.9860535860061646
I0429 16:00:12.947591 140530010298112 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9745579957962036, loss=1.9368499517440796
I0429 16:02:40.552272 140530001905408 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7124962210655212, loss=1.9416755437850952
I0429 16:05:08.256611 140530010298112 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9484369158744812, loss=1.884155511856079
I0429 16:07:35.850430 140530001905408 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7105042338371277, loss=1.9113514423370361
I0429 16:10:03.442795 140530010298112 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6587256193161011, loss=1.8971922397613525
I0429 16:12:31.010767 140530001905408 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6400268077850342, loss=1.8745017051696777
I0429 16:15:01.969356 140530010298112 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6688672304153442, loss=1.8536216020584106
I0429 16:17:29.209634 140530001905408 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6682539582252502, loss=1.8036433458328247
I0429 16:19:56.829803 140530010298112 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6295525431632996, loss=1.865382432937622
I0429 16:21:17.665627 140703374882624 spec.py:298] Evaluating on the training split.
I0429 16:22:04.831681 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 16:22:48.082532 140703374882624 spec.py:326] Evaluating on the test split.
I0429 16:23:09.632185 140703374882624 submission_runner.py:415] Time since start: 10260.51s, 	Step: 6456, 	{'train/ctc_loss': DeviceArray(0.58315635, dtype=float32), 'train/wer': 0.2025730712438082, 'validation/ctc_loss': DeviceArray(0.90454537, dtype=float32), 'validation/wer': 0.2717826510627213, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6294147, dtype=float32), 'test/wer': 0.2078280827899986, 'test/num_examples': 2472, 'score': 9683.767071723938, 'total_duration': 10260.512612819672, 'accumulated_submission_time': 9683.767071723938, 'accumulated_eval_time': 576.4887778759003, 'accumulated_logging_time': 0.15392541885375977}
I0429 16:23:09.652187 140530010298112 logging_writer.py:48] [6456] accumulated_eval_time=576.488778, accumulated_logging_time=0.153925, accumulated_submission_time=9683.767072, global_step=6456, preemption_count=0, score=9683.767072, test/ctc_loss=0.6294146776199341, test/num_examples=2472, test/wer=0.207828, total_duration=10260.512613, train/ctc_loss=0.5831563472747803, train/wer=0.202573, validation/ctc_loss=0.9045453667640686, validation/num_examples=5348, validation/wer=0.271783
I0429 16:24:15.844037 140530001905408 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6189990043640137, loss=1.8093316555023193
I0429 16:26:43.100202 140530010298112 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5650957226753235, loss=1.8494118452072144
I0429 16:29:10.285975 140530001905408 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7367115616798401, loss=1.8151347637176514
I0429 16:31:37.421682 140530010298112 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.013963222503662, loss=1.8785172700881958
I0429 16:34:04.587034 140530001905408 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8813132643699646, loss=1.7867225408554077
I0429 16:36:31.867766 140530010298112 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.586776614189148, loss=1.8453785181045532
I0429 16:38:59.403890 140530001905408 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5995463132858276, loss=1.7492789030075073
I0429 16:41:26.480147 140530010298112 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6163163781166077, loss=1.7837419509887695
I0429 16:43:57.331321 140530010298112 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6367881298065186, loss=1.759451150894165
I0429 16:46:24.741796 140530001905408 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8528733253479004, loss=1.7921580076217651
I0429 16:48:52.061015 140530010298112 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5512359738349915, loss=1.6627492904663086
I0429 16:51:19.464045 140530001905408 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5457074046134949, loss=1.756488561630249
I0429 16:53:46.950086 140530010298112 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7414173483848572, loss=1.7267086505889893
I0429 16:56:14.297262 140530001905408 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5273010730743408, loss=1.70966637134552
I0429 16:58:41.627094 140530010298112 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5846316814422607, loss=1.6916706562042236
I0429 17:01:08.935874 140530001905408 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4708801507949829, loss=1.6822781562805176
I0429 17:03:10.843929 140703374882624 spec.py:298] Evaluating on the training split.
I0429 17:03:57.529003 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 17:04:41.355060 140703374882624 spec.py:326] Evaluating on the test split.
I0429 17:05:03.720732 140703374882624 submission_runner.py:415] Time since start: 12774.60s, 	Step: 8084, 	{'train/ctc_loss': DeviceArray(0.47032303, dtype=float32), 'train/wer': 0.1679205897062341, 'validation/ctc_loss': DeviceArray(0.78244716, dtype=float32), 'validation/wer': 0.23347065577091916, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.52680254, dtype=float32), 'test/wer': 0.17740133650193976, 'test/num_examples': 2472, 'score': 12084.921975374222, 'total_duration': 12774.599415779114, 'accumulated_submission_time': 12084.921975374222, 'accumulated_eval_time': 689.3619017601013, 'accumulated_logging_time': 0.18725061416625977}
I0429 17:05:03.745055 140530010298112 logging_writer.py:48] [8084] accumulated_eval_time=689.361902, accumulated_logging_time=0.187251, accumulated_submission_time=12084.921975, global_step=8084, preemption_count=0, score=12084.921975, test/ctc_loss=0.5268025398254395, test/num_examples=2472, test/wer=0.177401, total_duration=12774.599416, train/ctc_loss=0.47032302618026733, train/wer=0.167921, validation/ctc_loss=0.7824471592903137, validation/num_examples=5348, validation/wer=0.233471
I0429 17:05:28.774523 140530001905408 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5623798370361328, loss=1.6993671655654907
I0429 17:07:55.983224 140530010298112 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.604025661945343, loss=1.718838095664978
I0429 17:10:26.594539 140530010298112 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.604941725730896, loss=1.7320828437805176
I0429 17:12:53.725456 140530001905408 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5386171340942383, loss=1.732862949371338
I0429 17:15:20.712408 140530010298112 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5296249985694885, loss=1.6004307270050049
I0429 17:17:47.918061 140530001905408 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5477567315101624, loss=1.7129217386245728
I0429 17:20:15.154760 140530010298112 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5266193151473999, loss=1.653183937072754
I0429 17:22:42.411415 140530001905408 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5498105883598328, loss=1.6675355434417725
I0429 17:25:09.688552 140530010298112 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6734593510627747, loss=1.6926896572113037
I0429 17:27:36.990272 140530001905408 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5076125264167786, loss=1.6683447360992432
I0429 17:30:04.326158 140530010298112 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5048782229423523, loss=1.5929006338119507
I0429 17:32:31.379762 140530001905408 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6162422299385071, loss=1.6526942253112793
I0429 17:35:02.035560 140530010298112 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.492788702249527, loss=1.6174170970916748
I0429 17:37:29.047918 140530001905408 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.47043880820274353, loss=1.5913023948669434
I0429 17:39:56.030354 140530010298112 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5339441299438477, loss=1.5532945394515991
I0429 17:42:23.086588 140530001905408 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6630303859710693, loss=1.6283148527145386
I0429 17:44:50.216585 140530010298112 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.49371010065078735, loss=1.5966328382492065
I0429 17:45:04.630044 140703374882624 spec.py:298] Evaluating on the training split.
I0429 17:45:51.354320 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 17:46:34.607997 140703374882624 spec.py:326] Evaluating on the test split.
I0429 17:46:57.034022 140703374882624 submission_runner.py:415] Time since start: 15287.91s, 	Step: 9711, 	{'train/ctc_loss': DeviceArray(0.4275425, dtype=float32), 'train/wer': 0.15053533990216109, 'validation/ctc_loss': DeviceArray(0.7146133, dtype=float32), 'validation/wer': 0.2135958861156403, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45189074, dtype=float32), 'test/wer': 0.1549164178498162, 'test/num_examples': 2472, 'score': 14485.76400232315, 'total_duration': 15287.914178609848, 'accumulated_submission_time': 14485.76400232315, 'accumulated_eval_time': 801.7636504173279, 'accumulated_logging_time': 0.22987723350524902}
I0429 17:46:57.059466 140530593978112 logging_writer.py:48] [9711] accumulated_eval_time=801.763650, accumulated_logging_time=0.229877, accumulated_submission_time=14485.764002, global_step=9711, preemption_count=0, score=14485.764002, test/ctc_loss=0.4518907368183136, test/num_examples=2472, test/wer=0.154916, total_duration=15287.914179, train/ctc_loss=0.427542507648468, train/wer=0.150535, validation/ctc_loss=0.7146133184432983, validation/num_examples=5348, validation/wer=0.213596
I0429 17:49:09.323011 140530585585408 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5172731280326843, loss=1.5521295070648193
I0429 17:51:36.553787 140530593978112 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5652467012405396, loss=1.6544911861419678
I0429 17:54:03.679703 140530585585408 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5442876815795898, loss=1.6873705387115479
I0429 17:56:30.854285 140530593978112 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5546535849571228, loss=1.5577763319015503
I0429 17:58:58.304041 140530585585408 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5333729982376099, loss=1.5624916553497314
I0429 18:01:28.943165 140530593978112 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5232245326042175, loss=1.596601963043213
I0429 18:03:56.236829 140530585585408 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.500299334526062, loss=1.5408236980438232
I0429 18:06:23.432471 140530593978112 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5523242950439453, loss=1.5213979482650757
I0429 18:08:50.955159 140530585585408 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.48177483677864075, loss=1.5473122596740723
I0429 18:11:18.253138 140530593978112 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6133279800415039, loss=1.5834258794784546
I0429 18:13:45.126261 140530585585408 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.49195870757102966, loss=1.5986579656600952
I0429 18:16:12.172450 140530593978112 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5290323495864868, loss=1.5525970458984375
I0429 18:18:39.383418 140530585585408 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5631664395332336, loss=1.5177134275436401
I0429 18:21:06.734740 140530593978112 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6614742875099182, loss=1.5560091733932495
I0429 18:23:33.834642 140530585585408 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4814551770687103, loss=1.5671814680099487
I0429 18:26:00.936388 140530593978112 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5760924220085144, loss=1.5217669010162354
I0429 18:26:58.331059 140703374882624 spec.py:298] Evaluating on the training split.
I0429 18:27:46.584252 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 18:28:29.633474 140703374882624 spec.py:326] Evaluating on the test split.
I0429 18:28:51.799330 140703374882624 submission_runner.py:415] Time since start: 17802.68s, 	Step: 11338, 	{'train/ctc_loss': DeviceArray(0.36348006, dtype=float32), 'train/wer': 0.13035224299661796, 'validation/ctc_loss': DeviceArray(0.6548425, dtype=float32), 'validation/wer': 0.19901783905295758, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41988423, dtype=float32), 'test/wer': 0.14240448479678264, 'test/num_examples': 2472, 'score': 16886.99340081215, 'total_duration': 17802.678042650223, 'accumulated_submission_time': 16886.99340081215, 'accumulated_eval_time': 915.2284815311432, 'accumulated_logging_time': 0.2715754508972168}
I0429 18:28:51.825219 140530163898112 logging_writer.py:48] [11338] accumulated_eval_time=915.228482, accumulated_logging_time=0.271575, accumulated_submission_time=16886.993401, global_step=11338, preemption_count=0, score=16886.993401, test/ctc_loss=0.41988423466682434, test/num_examples=2472, test/wer=0.142404, total_duration=17802.678043, train/ctc_loss=0.3634800612926483, train/wer=0.130352, validation/ctc_loss=0.6548424959182739, validation/num_examples=5348, validation/wer=0.199018
I0429 18:30:24.367489 140530155505408 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5127872824668884, loss=1.4941084384918213
I0429 18:32:51.552129 140530163898112 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5953758358955383, loss=1.587834119796753
I0429 18:35:18.861923 140530155505408 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.434538334608078, loss=1.4664808511734009
I0429 18:37:46.182458 140530163898112 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5658910870552063, loss=1.5175588130950928
I0429 18:40:13.312173 140530155505408 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.49419376254081726, loss=1.5210050344467163
I0429 18:42:40.586632 140530163898112 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6367455124855042, loss=1.5038999319076538
I0429 18:45:07.965536 140530155505408 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6210760474205017, loss=1.5258666276931763
I0429 18:47:35.248123 140530163898112 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.482992559671402, loss=1.5873429775238037
I0429 18:50:02.220155 140530155505408 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5743073225021362, loss=1.5255606174468994
I0429 18:52:29.393742 140530163898112 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5206120014190674, loss=1.486287236213684
I0429 18:55:00.089139 140530163898112 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5113248825073242, loss=1.4678430557250977
I0429 18:57:27.235238 140530155505408 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.45039600133895874, loss=1.4785183668136597
I0429 18:59:54.338845 140530163898112 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4689784646034241, loss=1.5137561559677124
I0429 19:02:21.397423 140530155505408 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5286135673522949, loss=1.5171806812286377
I0429 19:04:48.398941 140530163898112 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6398413181304932, loss=1.4553298950195312
I0429 19:07:15.545943 140530155505408 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5147230625152588, loss=1.4650919437408447
I0429 19:08:52.449825 140703374882624 spec.py:298] Evaluating on the training split.
I0429 19:09:40.528530 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 19:10:24.094266 140703374882624 spec.py:326] Evaluating on the test split.
I0429 19:10:46.045276 140703374882624 submission_runner.py:415] Time since start: 20316.92s, 	Step: 12967, 	{'train/ctc_loss': DeviceArray(0.30580756, dtype=float32), 'train/wer': 0.11398112275410177, 'validation/ctc_loss': DeviceArray(0.6168455, dtype=float32), 'validation/wer': 0.1894663720827022, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3875305, dtype=float32), 'test/wer': 0.13314240448479678, 'test/num_examples': 2472, 'score': 19287.579617261887, 'total_duration': 20316.92440366745, 'accumulated_submission_time': 19287.579617261887, 'accumulated_eval_time': 1028.8206899166107, 'accumulated_logging_time': 0.312833309173584}
I0429 19:10:46.068095 140530593978112 logging_writer.py:48] [12967] accumulated_eval_time=1028.820690, accumulated_logging_time=0.312833, accumulated_submission_time=19287.579617, global_step=12967, preemption_count=0, score=19287.579617, test/ctc_loss=0.38753050565719604, test/num_examples=2472, test/wer=0.133142, total_duration=20316.924404, train/ctc_loss=0.30580756068229675, train/wer=0.113981, validation/ctc_loss=0.6168454885482788, validation/num_examples=5348, validation/wer=0.189466
I0429 19:11:36.067599 140530585585408 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.42438238859176636, loss=1.5145092010498047
I0429 19:14:02.948834 140530593978112 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.42673832178115845, loss=1.4674232006072998
I0429 19:16:29.840647 140530585585408 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5241034030914307, loss=1.5128164291381836
I0429 19:18:56.850800 140530593978112 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5166873931884766, loss=1.4531800746917725
I0429 19:21:27.714797 140529836218112 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.46021583676338196, loss=1.4110777378082275
I0429 19:23:55.006160 140529827825408 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.4330035150051117, loss=1.4650322198867798
I0429 19:26:21.999273 140529836218112 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5575393438339233, loss=1.429059386253357
I0429 19:28:48.956548 140529827825408 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.4834744334220886, loss=1.4462356567382812
I0429 19:31:15.857635 140529836218112 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5457227826118469, loss=1.501280665397644
I0429 19:33:42.855976 140529827825408 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.47472354769706726, loss=1.4220622777938843
I0429 19:36:09.703889 140529836218112 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.44441941380500793, loss=1.4477049112319946
I0429 19:38:36.615694 140529827825408 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5350253582000732, loss=1.4345067739486694
I0429 19:41:03.326919 140529836218112 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5122786164283752, loss=1.3719885349273682
I0429 19:43:30.534138 140529827825408 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6017491817474365, loss=1.5072522163391113
I0429 19:45:57.369640 140529836218112 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.47925758361816406, loss=1.4492746591567993
I0429 19:48:27.699954 140530593978112 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.43963533639907837, loss=1.4060328006744385
I0429 19:50:46.790930 140703374882624 spec.py:298] Evaluating on the training split.
I0429 19:51:34.309798 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 19:52:18.397050 140703374882624 spec.py:326] Evaluating on the test split.
I0429 19:52:40.320944 140703374882624 submission_runner.py:415] Time since start: 22831.20s, 	Step: 14596, 	{'train/ctc_loss': DeviceArray(0.28282642, dtype=float32), 'train/wer': 0.10648564390936803, 'validation/ctc_loss': DeviceArray(0.59350836, dtype=float32), 'validation/wer': 0.18147787243485225, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3729367, dtype=float32), 'test/wer': 0.1270895537545955, 'test/num_examples': 2472, 'score': 21688.262000083923, 'total_duration': 22831.19981455803, 'accumulated_submission_time': 21688.262000083923, 'accumulated_eval_time': 1142.3472480773926, 'accumulated_logging_time': 0.35146260261535645}
I0429 19:52:40.345365 140530593978112 logging_writer.py:48] [14596] accumulated_eval_time=1142.347248, accumulated_logging_time=0.351463, accumulated_submission_time=21688.262000, global_step=14596, preemption_count=0, score=21688.262000, test/ctc_loss=0.3729366958141327, test/num_examples=2472, test/wer=0.127090, total_duration=22831.199815, train/ctc_loss=0.28282642364501953, train/wer=0.106486, validation/ctc_loss=0.5935083627700806, validation/num_examples=5348, validation/wer=0.181478
I0429 19:52:47.745720 140530585585408 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.461176335811615, loss=1.433451533317566
I0429 19:55:14.868494 140530593978112 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4874155819416046, loss=1.3633337020874023
I0429 19:57:41.919180 140530585585408 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.4441106617450714, loss=1.429161548614502
I0429 20:00:09.045918 140530593978112 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.5157520771026611, loss=1.3959887027740479
I0429 20:02:36.028397 140530585585408 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.45050403475761414, loss=1.4266340732574463
I0429 20:05:02.979858 140530593978112 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.46914374828338623, loss=1.4431990385055542
I0429 20:07:30.206996 140530585585408 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5979617834091187, loss=1.4767154455184937
I0429 20:09:57.417372 140530593978112 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.45890510082244873, loss=1.4037251472473145
I0429 20:12:24.603643 140530585585408 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5798614621162415, loss=1.4834133386611938
I0429 20:14:55.046327 140530593978112 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.538271427154541, loss=1.4035283327102661
I0429 20:17:21.881692 140530585585408 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.44806572794914246, loss=1.413857102394104
I0429 20:19:48.590424 140530593978112 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6090259552001953, loss=1.4104669094085693
I0429 20:22:15.239420 140530585585408 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6474611163139343, loss=1.4410855770111084
I0429 20:24:41.922504 140530593978112 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5312671661376953, loss=1.4492496252059937
I0429 20:27:08.740493 140530585585408 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.48451733589172363, loss=1.3763906955718994
I0429 20:29:35.625107 140530593978112 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.4860084056854248, loss=1.482978105545044
I0429 20:32:02.382412 140530585585408 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.511091411113739, loss=1.4184956550598145
I0429 20:32:41.695499 140703374882624 spec.py:298] Evaluating on the training split.
I0429 20:33:28.897143 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 20:34:12.579506 140703374882624 spec.py:326] Evaluating on the test split.
I0429 20:34:34.899679 140703374882624 submission_runner.py:415] Time since start: 25345.78s, 	Step: 16228, 	{'train/ctc_loss': DeviceArray(0.26580587, dtype=float32), 'train/wer': 0.1017659961280012, 'validation/ctc_loss': DeviceArray(0.56762064, dtype=float32), 'validation/wer': 0.17144400814286678, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3488056, dtype=float32), 'test/wer': 0.11961489245018585, 'test/num_examples': 2472, 'score': 24089.57100701332, 'total_duration': 25345.77844595909, 'accumulated_submission_time': 24089.57100701332, 'accumulated_eval_time': 1255.5478179454803, 'accumulated_logging_time': 0.3922257423400879}
I0429 20:34:34.926533 140530163898112 logging_writer.py:48] [16228] accumulated_eval_time=1255.547818, accumulated_logging_time=0.392226, accumulated_submission_time=24089.571007, global_step=16228, preemption_count=0, score=24089.571007, test/ctc_loss=0.34880560636520386, test/num_examples=2472, test/wer=0.119615, total_duration=25345.778446, train/ctc_loss=0.2658058702945709, train/wer=0.101766, validation/ctc_loss=0.5676206350326538, validation/num_examples=5348, validation/wer=0.171444
I0429 20:36:22.260741 140530155505408 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3978627622127533, loss=1.3855869770050049
I0429 20:38:49.441565 140530163898112 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.4185687303543091, loss=1.3724483251571655
I0429 20:41:20.428809 140529508538112 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.49914872646331787, loss=1.3565696477890015
I0429 20:43:47.532945 140529500145408 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5430186986923218, loss=1.4074007272720337
I0429 20:46:14.300258 140529508538112 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.46657437086105347, loss=1.4583778381347656
I0429 20:48:41.082204 140529500145408 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5389692187309265, loss=1.4165805578231812
I0429 20:51:08.006758 140529508538112 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.54832524061203, loss=1.33528733253479
I0429 20:53:34.952200 140529500145408 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.42531704902648926, loss=1.4089462757110596
I0429 20:56:01.984161 140529508538112 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5866554379463196, loss=1.3998759984970093
I0429 20:58:28.985566 140529500145408 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.46877050399780273, loss=1.4088165760040283
I0429 21:00:55.956099 140529508538112 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.40706944465637207, loss=1.364758014678955
I0429 21:03:23.142635 140529500145408 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6266942024230957, loss=1.4613125324249268
I0429 21:05:50.217473 140529508538112 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.47714853286743164, loss=1.3611688613891602
I0429 21:08:20.562129 140529508538112 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.4455120861530304, loss=1.4058982133865356
I0429 21:10:47.388168 140529500145408 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.39935871958732605, loss=1.3867021799087524
I0429 21:13:14.364576 140529508538112 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.4516371786594391, loss=1.3907798528671265
I0429 21:14:36.314368 140703374882624 spec.py:298] Evaluating on the training split.
I0429 21:15:23.476102 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 21:16:06.818202 140703374882624 spec.py:326] Evaluating on the test split.
I0429 21:16:28.734438 140703374882624 submission_runner.py:415] Time since start: 27859.61s, 	Step: 17857, 	{'train/ctc_loss': DeviceArray(0.26894605, dtype=float32), 'train/wer': 0.09884978449985457, 'validation/ctc_loss': DeviceArray(0.552694, dtype=float32), 'validation/wer': 0.16641742805043946, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32845163, dtype=float32), 'test/wer': 0.11207929640688157, 'test/num_examples': 2472, 'score': 26490.917573451996, 'total_duration': 27859.614844322205, 'accumulated_submission_time': 26490.917573451996, 'accumulated_eval_time': 1367.965933561325, 'accumulated_logging_time': 0.43675971031188965}
I0429 21:16:28.756408 140530593978112 logging_writer.py:48] [17857] accumulated_eval_time=1367.965934, accumulated_logging_time=0.436760, accumulated_submission_time=26490.917573, global_step=17857, preemption_count=0, score=26490.917573, test/ctc_loss=0.32845163345336914, test/num_examples=2472, test/wer=0.112079, total_duration=27859.614844, train/ctc_loss=0.2689460515975952, train/wer=0.098850, validation/ctc_loss=0.5526940226554871, validation/num_examples=5348, validation/wer=0.166417
I0429 21:17:33.308639 140530585585408 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.46150386333465576, loss=1.4125280380249023
I0429 21:20:00.131746 140530593978112 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4057062268257141, loss=1.3975812196731567
I0429 21:22:26.983270 140530585585408 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4546590745449066, loss=1.4387669563293457
I0429 21:24:53.936354 140530593978112 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5839962959289551, loss=1.4076032638549805
I0429 21:27:20.717668 140530585585408 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5311975479125977, loss=1.3294693231582642
I0429 21:29:47.678386 140530593978112 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5279037952423096, loss=1.3463501930236816
I0429 21:32:14.753064 140530585585408 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4642939865589142, loss=1.3669551610946655
I0429 21:34:44.950097 140530593978112 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.42646896839141846, loss=1.3771346807479858
I0429 21:37:11.948668 140530585585408 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.47243160009384155, loss=1.3560389280319214
I0429 21:39:38.974931 140530593978112 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5039106011390686, loss=1.4105920791625977
I0429 21:42:05.758260 140530585585408 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.4502561092376709, loss=1.383189082145691
I0429 21:44:32.852379 140530593978112 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.43604427576065063, loss=1.3300855159759521
I0429 21:47:00.196359 140530585585408 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5268473029136658, loss=1.3417041301727295
I0429 21:49:27.573615 140530593978112 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5331407189369202, loss=1.3706046342849731
I0429 21:51:55.034843 140530585585408 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.49471354484558105, loss=1.3806159496307373
I0429 21:54:22.414787 140530593978112 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5219195485115051, loss=1.3964027166366577
I0429 21:56:30.208921 140703374882624 spec.py:298] Evaluating on the training split.
I0429 21:57:17.784214 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 21:58:02.529060 140703374882624 spec.py:326] Evaluating on the test split.
I0429 21:58:24.525094 140703374882624 submission_runner.py:415] Time since start: 30375.40s, 	Step: 19488, 	{'train/ctc_loss': DeviceArray(0.2612764, dtype=float32), 'train/wer': 0.09755454137758197, 'validation/ctc_loss': DeviceArray(0.53098905, dtype=float32), 'validation/wer': 0.16102422599349728, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31985983, dtype=float32), 'test/wer': 0.10846383523246603, 'test/num_examples': 2472, 'score': 28892.333297014236, 'total_duration': 30375.404258966446, 'accumulated_submission_time': 28892.333297014236, 'accumulated_eval_time': 1482.278916835785, 'accumulated_logging_time': 0.4714221954345703}
I0429 21:58:24.549951 140530593978112 logging_writer.py:48] [19488] accumulated_eval_time=1482.278917, accumulated_logging_time=0.471422, accumulated_submission_time=28892.333297, global_step=19488, preemption_count=0, score=28892.333297, test/ctc_loss=0.3198598325252533, test/num_examples=2472, test/wer=0.108464, total_duration=30375.404259, train/ctc_loss=0.26127639412879944, train/wer=0.097555, validation/ctc_loss=0.5309890508651733, validation/num_examples=5348, validation/wer=0.161024
I0429 21:58:43.639750 140530585585408 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4874172806739807, loss=1.3518190383911133
I0429 22:01:14.235420 140529938618112 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.4468887448310852, loss=1.3564162254333496
I0429 22:03:41.066147 140529930225408 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4933808743953705, loss=1.3210750818252563
I0429 22:06:07.800393 140529938618112 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.45958611369132996, loss=1.3197494745254517
I0429 22:08:34.612099 140529930225408 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5773129463195801, loss=1.3462245464324951
I0429 22:10:59.740733 140703374882624 spec.py:298] Evaluating on the training split.
I0429 22:11:47.808999 140703374882624 spec.py:310] Evaluating on the validation split.
I0429 22:12:31.655883 140703374882624 spec.py:326] Evaluating on the test split.
I0429 22:12:53.566687 140703374882624 submission_runner.py:415] Time since start: 31244.45s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.24720298, dtype=float32), 'train/wer': 0.09287478135610659, 'validation/ctc_loss': DeviceArray(0.5258633, dtype=float32), 'validation/wer': 0.158998157242231, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31753623, dtype=float32), 'test/wer': 0.10795604574167733, 'test/num_examples': 2472, 'score': 29647.498578071594, 'total_duration': 31244.447150230408, 'accumulated_submission_time': 29647.498578071594, 'accumulated_eval_time': 1596.1029767990112, 'accumulated_logging_time': 0.5134584903717041}
I0429 22:12:53.588837 140529216698112 logging_writer.py:48] [20000] accumulated_eval_time=1596.102977, accumulated_logging_time=0.513458, accumulated_submission_time=29647.498578, global_step=20000, preemption_count=0, score=29647.498578, test/ctc_loss=0.31753623485565186, test/num_examples=2472, test/wer=0.107956, total_duration=31244.447150, train/ctc_loss=0.24720297753810883, train/wer=0.092875, validation/ctc_loss=0.5258632898330688, validation/num_examples=5348, validation/wer=0.158998
I0429 22:12:53.609902 140529208305408 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=29647.498578
I0429 22:12:53.946864 140703374882624 checkpoints.py:356] Saving checkpoint at step: 20000
I0429 22:12:55.356273 140703374882624 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1/checkpoint_20000
I0429 22:12:55.386902 140703374882624 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_sam/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0429 22:12:56.655133 140703374882624 submission_runner.py:578] Tuning trial 1/1
I0429 22:12:56.655397 140703374882624 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0429 22:12:56.664772 140703374882624 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.185383, dtype=float32), 'train/wer': 1.5947775024467443, 'validation/ctc_loss': DeviceArray(30.23235, dtype=float32), 'validation/wer': 1.1394321218728594, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.346806, dtype=float32), 'test/wer': 1.2072187354010522, 'test/num_examples': 2472, 'score': 81.21641492843628, 'total_duration': 237.08610701560974, 'accumulated_submission_time': 81.21641492843628, 'accumulated_eval_time': 155.8695342540741, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1589, {'train/ctc_loss': DeviceArray(6.0062547, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.0638266, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.037105, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2481.4508645534515, 'total_duration': 2721.815409421921, 'accumulated_submission_time': 2481.4508645534515, 'accumulated_eval_time': 240.30140352249146, 'accumulated_logging_time': 0.04048013687133789, 'global_step': 1589, 'preemption_count': 0}), (3209, {'train/ctc_loss': DeviceArray(2.7745118, dtype=float32), 'train/wer': 0.6253096449404683, 'validation/ctc_loss': DeviceArray(3.1379733, dtype=float32), 'validation/wer': 0.6315545736090074, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.8346608, dtype=float32), 'test/wer': 0.6008774602400828, 'test/num_examples': 2472, 'score': 4882.357090950012, 'total_duration': 5231.7526314258575, 'accumulated_submission_time': 4882.357090950012, 'accumulated_eval_time': 349.2700505256653, 'accumulated_logging_time': 0.07782959938049316, 'global_step': 3209, 'preemption_count': 0}), (4834, {'train/ctc_loss': DeviceArray(0.89442486, dtype=float32), 'train/wer': 0.29831691362124496, 'validation/ctc_loss': DeviceArray(1.2458347, dtype=float32), 'validation/wer': 0.3603411513859275, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.95447236, dtype=float32), 'test/wer': 0.3035768691731156, 'test/num_examples': 2472, 'score': 7282.867134094238, 'total_duration': 7747.582359075546, 'accumulated_submission_time': 7282.867134094238, 'accumulated_eval_time': 464.52415132522583, 'accumulated_logging_time': 0.11585044860839844, 'global_step': 4834, 'preemption_count': 0}), (6456, {'train/ctc_loss': DeviceArray(0.58315635, dtype=float32), 'train/wer': 0.2025730712438082, 'validation/ctc_loss': DeviceArray(0.90454537, dtype=float32), 'validation/wer': 0.2717826510627213, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6294147, dtype=float32), 'test/wer': 0.2078280827899986, 'test/num_examples': 2472, 'score': 9683.767071723938, 'total_duration': 10260.512612819672, 'accumulated_submission_time': 9683.767071723938, 'accumulated_eval_time': 576.4887778759003, 'accumulated_logging_time': 0.15392541885375977, 'global_step': 6456, 'preemption_count': 0}), (8084, {'train/ctc_loss': DeviceArray(0.47032303, dtype=float32), 'train/wer': 0.1679205897062341, 'validation/ctc_loss': DeviceArray(0.78244716, dtype=float32), 'validation/wer': 0.23347065577091916, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.52680254, dtype=float32), 'test/wer': 0.17740133650193976, 'test/num_examples': 2472, 'score': 12084.921975374222, 'total_duration': 12774.599415779114, 'accumulated_submission_time': 12084.921975374222, 'accumulated_eval_time': 689.3619017601013, 'accumulated_logging_time': 0.18725061416625977, 'global_step': 8084, 'preemption_count': 0}), (9711, {'train/ctc_loss': DeviceArray(0.4275425, dtype=float32), 'train/wer': 0.15053533990216109, 'validation/ctc_loss': DeviceArray(0.7146133, dtype=float32), 'validation/wer': 0.2135958861156403, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45189074, dtype=float32), 'test/wer': 0.1549164178498162, 'test/num_examples': 2472, 'score': 14485.76400232315, 'total_duration': 15287.914178609848, 'accumulated_submission_time': 14485.76400232315, 'accumulated_eval_time': 801.7636504173279, 'accumulated_logging_time': 0.22987723350524902, 'global_step': 9711, 'preemption_count': 0}), (11338, {'train/ctc_loss': DeviceArray(0.36348006, dtype=float32), 'train/wer': 0.13035224299661796, 'validation/ctc_loss': DeviceArray(0.6548425, dtype=float32), 'validation/wer': 0.19901783905295758, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41988423, dtype=float32), 'test/wer': 0.14240448479678264, 'test/num_examples': 2472, 'score': 16886.99340081215, 'total_duration': 17802.678042650223, 'accumulated_submission_time': 16886.99340081215, 'accumulated_eval_time': 915.2284815311432, 'accumulated_logging_time': 0.2715754508972168, 'global_step': 11338, 'preemption_count': 0}), (12967, {'train/ctc_loss': DeviceArray(0.30580756, dtype=float32), 'train/wer': 0.11398112275410177, 'validation/ctc_loss': DeviceArray(0.6168455, dtype=float32), 'validation/wer': 0.1894663720827022, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3875305, dtype=float32), 'test/wer': 0.13314240448479678, 'test/num_examples': 2472, 'score': 19287.579617261887, 'total_duration': 20316.92440366745, 'accumulated_submission_time': 19287.579617261887, 'accumulated_eval_time': 1028.8206899166107, 'accumulated_logging_time': 0.312833309173584, 'global_step': 12967, 'preemption_count': 0}), (14596, {'train/ctc_loss': DeviceArray(0.28282642, dtype=float32), 'train/wer': 0.10648564390936803, 'validation/ctc_loss': DeviceArray(0.59350836, dtype=float32), 'validation/wer': 0.18147787243485225, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3729367, dtype=float32), 'test/wer': 0.1270895537545955, 'test/num_examples': 2472, 'score': 21688.262000083923, 'total_duration': 22831.19981455803, 'accumulated_submission_time': 21688.262000083923, 'accumulated_eval_time': 1142.3472480773926, 'accumulated_logging_time': 0.35146260261535645, 'global_step': 14596, 'preemption_count': 0}), (16228, {'train/ctc_loss': DeviceArray(0.26580587, dtype=float32), 'train/wer': 0.1017659961280012, 'validation/ctc_loss': DeviceArray(0.56762064, dtype=float32), 'validation/wer': 0.17144400814286678, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3488056, dtype=float32), 'test/wer': 0.11961489245018585, 'test/num_examples': 2472, 'score': 24089.57100701332, 'total_duration': 25345.77844595909, 'accumulated_submission_time': 24089.57100701332, 'accumulated_eval_time': 1255.5478179454803, 'accumulated_logging_time': 0.3922257423400879, 'global_step': 16228, 'preemption_count': 0}), (17857, {'train/ctc_loss': DeviceArray(0.26894605, dtype=float32), 'train/wer': 0.09884978449985457, 'validation/ctc_loss': DeviceArray(0.552694, dtype=float32), 'validation/wer': 0.16641742805043946, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32845163, dtype=float32), 'test/wer': 0.11207929640688157, 'test/num_examples': 2472, 'score': 26490.917573451996, 'total_duration': 27859.614844322205, 'accumulated_submission_time': 26490.917573451996, 'accumulated_eval_time': 1367.965933561325, 'accumulated_logging_time': 0.43675971031188965, 'global_step': 17857, 'preemption_count': 0}), (19488, {'train/ctc_loss': DeviceArray(0.2612764, dtype=float32), 'train/wer': 0.09755454137758197, 'validation/ctc_loss': DeviceArray(0.53098905, dtype=float32), 'validation/wer': 0.16102422599349728, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31985983, dtype=float32), 'test/wer': 0.10846383523246603, 'test/num_examples': 2472, 'score': 28892.333297014236, 'total_duration': 30375.404258966446, 'accumulated_submission_time': 28892.333297014236, 'accumulated_eval_time': 1482.278916835785, 'accumulated_logging_time': 0.4714221954345703, 'global_step': 19488, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.24720298, dtype=float32), 'train/wer': 0.09287478135610659, 'validation/ctc_loss': DeviceArray(0.5258633, dtype=float32), 'validation/wer': 0.158998157242231, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31753623, dtype=float32), 'test/wer': 0.10795604574167733, 'test/num_examples': 2472, 'score': 29647.498578071594, 'total_duration': 31244.447150230408, 'accumulated_submission_time': 29647.498578071594, 'accumulated_eval_time': 1596.1029767990112, 'accumulated_logging_time': 0.5134584903717041, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0429 22:12:56.665004 140703374882624 submission_runner.py:581] Timing: 29647.498578071594
I0429 22:12:56.665086 140703374882624 submission_runner.py:582] ====================
I0429 22:12:56.666146 140703374882624 submission_runner.py:645] Final librispeech_conformer score: 29647.498578071594
