python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/nesterov/jax/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_nesterov --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_04-28-2023-12-27-48.log
I0428 12:28:09.513191 140327390295872 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax.
I0428 12:28:09.580150 140327390295872 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 12:28:10.448239 140327390295872 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0428 12:28:10.449294 140327390295872 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 12:28:10.453466 140327390295872 submission_runner.py:538] Using RNG seed 3013358237
I0428 12:28:13.032511 140327390295872 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 12:28:13.032723 140327390295872 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1.
I0428 12:28:13.032919 140327390295872 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1/hparams.json.
I0428 12:28:13.159250 140327390295872 submission_runner.py:241] Initializing dataset.
I0428 12:28:13.394759 140327390295872 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:28:13.399636 140327390295872 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 12:28:13.622237 140327390295872 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:28:13.674443 140327390295872 submission_runner.py:248] Initializing model.
I0428 12:28:20.955750 140327390295872 submission_runner.py:258] Initializing optimizer.
I0428 12:28:21.295193 140327390295872 submission_runner.py:265] Initializing metrics bundle.
I0428 12:28:21.295371 140327390295872 submission_runner.py:282] Initializing checkpoint and logger.
I0428 12:28:21.296397 140327390295872 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1 with prefix checkpoint_
I0428 12:28:21.296624 140327390295872 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 12:28:21.296685 140327390295872 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 12:28:22.126507 140327390295872 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1/meta_data_0.json.
I0428 12:28:22.127441 140327390295872 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1/flags_0.json.
I0428 12:28:22.133213 140327390295872 submission_runner.py:318] Starting training loop.
I0428 12:28:41.588390 140151191762688 logging_writer.py:48] [0] global_step=0, grad_norm=3.3154680728912354, loss=0.7286390066146851
I0428 12:28:41.600039 140327390295872 spec.py:298] Evaluating on the training split.
I0428 12:28:41.608090 140327390295872 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:28:41.611892 140327390295872 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 12:28:41.669135 140327390295872 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:30:12.393991 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 12:30:12.396740 140327390295872 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:30:12.400312 140327390295872 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 12:30:12.453911 140327390295872 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:31:15.558340 140327390295872 spec.py:326] Evaluating on the test split.
I0428 12:31:15.561000 140327390295872 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:31:15.564637 140327390295872 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 12:31:15.618604 140327390295872 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 12:32:20.113971 140327390295872 submission_runner.py:415] Time since start: 237.98s, 	Step: 1, 	{'train/accuracy': 0.5024756789207458, 'train/loss': 0.7288255095481873, 'train/mean_average_precision': 0.02415142946925777, 'validation/accuracy': 0.5087609887123108, 'validation/loss': 0.7304736375808716, 'validation/mean_average_precision': 0.027352464209319464, 'validation/num_examples': 43793, 'test/accuracy': 0.5105212330818176, 'test/loss': 0.730847954750061, 'test/mean_average_precision': 0.029412253490863103, 'test/num_examples': 43793, 'score': 19.466670751571655, 'total_duration': 237.9807116985321, 'accumulated_submission_time': 19.466670751571655, 'accumulated_eval_time': 218.51390266418457, 'accumulated_logging_time': 0}
I0428 12:32:20.130079 140141636617984 logging_writer.py:48] [1] accumulated_eval_time=218.513903, accumulated_logging_time=0, accumulated_submission_time=19.466671, global_step=1, preemption_count=0, score=19.466671, test/accuracy=0.510521, test/loss=0.730848, test/mean_average_precision=0.029412, test/num_examples=43793, total_duration=237.980712, train/accuracy=0.502476, train/loss=0.728826, train/mean_average_precision=0.024151, validation/accuracy=0.508761, validation/loss=0.730474, validation/mean_average_precision=0.027352, validation/num_examples=43793
I0428 12:32:44.439687 140141645010688 logging_writer.py:48] [100] global_step=100, grad_norm=0.06492434442043304, loss=0.08427160233259201
I0428 12:33:08.761617 140141636617984 logging_writer.py:48] [200] global_step=200, grad_norm=0.016004187986254692, loss=0.05569758266210556
I0428 12:33:33.104261 140141645010688 logging_writer.py:48] [300] global_step=300, grad_norm=0.01028470415621996, loss=0.05317273363471031
I0428 12:33:57.648492 140141636617984 logging_writer.py:48] [400] global_step=400, grad_norm=0.011030170135200024, loss=0.0584077388048172
I0428 12:34:22.148477 140141645010688 logging_writer.py:48] [500] global_step=500, grad_norm=0.008852689526975155, loss=0.0529695563018322
I0428 12:34:46.501855 140141636617984 logging_writer.py:48] [600] global_step=600, grad_norm=0.012656038627028465, loss=0.054310426115989685
I0428 12:35:10.863262 140141645010688 logging_writer.py:48] [700] global_step=700, grad_norm=0.011008722707629204, loss=0.05578180029988289
I0428 12:35:35.074504 140141636617984 logging_writer.py:48] [800] global_step=800, grad_norm=0.012021549046039581, loss=0.054663244634866714
I0428 12:35:59.035609 140141645010688 logging_writer.py:48] [900] global_step=900, grad_norm=0.03798407316207886, loss=0.052737828344106674
I0428 12:36:20.253177 140327390295872 spec.py:298] Evaluating on the training split.
I0428 12:37:34.083069 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 12:37:36.680609 140327390295872 spec.py:326] Evaluating on the test split.
I0428 12:37:39.195183 140327390295872 submission_runner.py:415] Time since start: 557.06s, 	Step: 989, 	{'train/accuracy': 0.9868044257164001, 'train/loss': 0.05426831543445587, 'train/mean_average_precision': 0.034148035969027636, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06463412195444107, 'validation/mean_average_precision': 0.03810721638076439, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0679437667131424, 'test/mean_average_precision': 0.03924969839645322, 'test/num_examples': 43793, 'score': 259.5731270313263, 'total_duration': 557.0619051456451, 'accumulated_submission_time': 259.5731270313263, 'accumulated_eval_time': 297.45587825775146, 'accumulated_logging_time': 0.025253772735595703}
I0428 12:37:39.203339 140141636617984 logging_writer.py:48] [989] accumulated_eval_time=297.455878, accumulated_logging_time=0.025254, accumulated_submission_time=259.573127, global_step=989, preemption_count=0, score=259.573127, test/accuracy=0.983142, test/loss=0.067944, test/mean_average_precision=0.039250, test/num_examples=43793, total_duration=557.061905, train/accuracy=0.986804, train/loss=0.054268, train/mean_average_precision=0.034148, validation/accuracy=0.984118, validation/loss=0.064634, validation/mean_average_precision=0.038107, validation/num_examples=43793
I0428 12:37:42.093434 140141645010688 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.014982450753450394, loss=0.050676945596933365
I0428 12:38:06.111275 140141636617984 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0364115908741951, loss=0.05230824276804924
I0428 12:38:30.179121 140141645010688 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.05910872295498848, loss=0.05476722866296768
I0428 12:38:54.403113 140141636617984 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0651041641831398, loss=0.052358079701662064
I0428 12:39:18.562835 140141645010688 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.051360342651605606, loss=0.04862526059150696
I0428 12:39:42.631686 140141636617984 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.11749015003442764, loss=0.0564698651432991
I0428 12:40:06.598200 140141645010688 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.11599478870630264, loss=0.05688795819878578
I0428 12:40:30.568371 140141636617984 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0704524889588356, loss=0.05518848076462746
I0428 12:40:54.687663 140141645010688 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.052212826907634735, loss=0.05889435485005379
I0428 12:41:18.911076 140141636617984 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.06280619651079178, loss=0.04760418459773064
I0428 12:41:39.327432 140327390295872 spec.py:298] Evaluating on the training split.
I0428 12:42:54.257023 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 12:42:56.853408 140327390295872 spec.py:326] Evaluating on the test split.
I0428 12:42:59.357134 140327390295872 submission_runner.py:415] Time since start: 877.22s, 	Step: 1986, 	{'train/accuracy': 0.9867419600486755, 'train/loss': 0.05115160346031189, 'train/mean_average_precision': 0.06257172577375161, 'validation/accuracy': 0.9841305017471313, 'validation/loss': 0.060121774673461914, 'validation/mean_average_precision': 0.06021660418452359, 'validation/num_examples': 43793, 'test/accuracy': 0.983154296875, 'test/loss': 0.06331733614206314, 'test/mean_average_precision': 0.06063212816208897, 'test/num_examples': 43793, 'score': 499.68025493621826, 'total_duration': 877.22385597229, 'accumulated_submission_time': 499.68025493621826, 'accumulated_eval_time': 377.4855463504791, 'accumulated_logging_time': 0.04318380355834961}
I0428 12:42:59.365231 140141645010688 logging_writer.py:48] [1986] accumulated_eval_time=377.485546, accumulated_logging_time=0.043184, accumulated_submission_time=499.680255, global_step=1986, preemption_count=0, score=499.680255, test/accuracy=0.983154, test/loss=0.063317, test/mean_average_precision=0.060632, test/num_examples=43793, total_duration=877.223856, train/accuracy=0.986742, train/loss=0.051152, train/mean_average_precision=0.062572, validation/accuracy=0.984131, validation/loss=0.060122, validation/mean_average_precision=0.060217, validation/num_examples=43793
I0428 12:43:03.014000 140141636617984 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.09422606974840164, loss=0.054686810821294785
I0428 12:43:27.248445 140141645010688 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.07920578122138977, loss=0.05030292272567749
I0428 12:43:51.629474 140141636617984 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.03384897857904434, loss=0.04795217514038086
I0428 12:44:15.608673 140141645010688 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.048790913075208664, loss=0.04814176633954048
I0428 12:44:39.919435 140141636617984 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.12824614346027374, loss=0.04965469613671303
I0428 12:45:04.321491 140141645010688 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.09629824757575989, loss=0.05110911652445793
I0428 12:45:28.236939 140141636617984 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.07233243435621262, loss=0.04778042435646057
I0428 12:45:52.095565 140141645010688 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.050624776631593704, loss=0.05456235259771347
I0428 12:46:16.843179 140141636617984 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.07777103036642075, loss=0.049541447311639786
I0428 12:46:41.598215 140141645010688 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.055199284106492996, loss=0.05106918513774872
I0428 12:46:59.601946 140327390295872 spec.py:298] Evaluating on the training split.
I0428 12:48:15.045881 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 12:48:17.659431 140327390295872 spec.py:326] Evaluating on the test split.
I0428 12:48:20.220531 140327390295872 submission_runner.py:415] Time since start: 1198.09s, 	Step: 2974, 	{'train/accuracy': 0.9870172739028931, 'train/loss': 0.049812622368335724, 'train/mean_average_precision': 0.09219071678834041, 'validation/accuracy': 0.9843111634254456, 'validation/loss': 0.06044057011604309, 'validation/mean_average_precision': 0.08412881589403369, 'validation/num_examples': 43793, 'test/accuracy': 0.9833135008811951, 'test/loss': 0.06361602246761322, 'test/mean_average_precision': 0.08788961438024721, 'test/num_examples': 43793, 'score': 739.8981008529663, 'total_duration': 1198.0872535705566, 'accumulated_submission_time': 739.8981008529663, 'accumulated_eval_time': 458.10410809516907, 'accumulated_logging_time': 0.06235384941101074}
I0428 12:48:20.229194 140141636617984 logging_writer.py:48] [2974] accumulated_eval_time=458.104108, accumulated_logging_time=0.062354, accumulated_submission_time=739.898101, global_step=2974, preemption_count=0, score=739.898101, test/accuracy=0.983314, test/loss=0.063616, test/mean_average_precision=0.087890, test/num_examples=43793, total_duration=1198.087254, train/accuracy=0.987017, train/loss=0.049813, train/mean_average_precision=0.092191, validation/accuracy=0.984311, validation/loss=0.060441, validation/mean_average_precision=0.084129, validation/num_examples=43793
I0428 12:48:26.880953 140141645010688 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.03474917262792587, loss=0.05109859257936478
I0428 12:48:51.274417 140141636617984 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.049575820565223694, loss=0.049251481890678406
I0428 12:49:15.504297 140141645010688 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05966173857450485, loss=0.05104043334722519
I0428 12:49:39.831347 140141636617984 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07945837825536728, loss=0.04996560513973236
I0428 12:50:03.832499 140141645010688 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.062092363834381104, loss=0.04731054604053497
I0428 12:50:27.881150 140141636617984 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.07404401153326035, loss=0.04712264612317085
I0428 12:50:51.922525 140141645010688 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.10716807097196579, loss=0.05464133620262146
I0428 12:51:16.092779 140141636617984 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.04367537796497345, loss=0.051426924765110016
I0428 12:51:40.521629 140141645010688 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.05259927362203598, loss=0.04569843038916588
I0428 12:52:04.736482 140141636617984 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.07275990396738052, loss=0.04591592401266098
I0428 12:52:20.418295 140327390295872 spec.py:298] Evaluating on the training split.
I0428 12:53:36.144412 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 12:53:38.730739 140327390295872 spec.py:326] Evaluating on the test split.
I0428 12:53:41.254778 140327390295872 submission_runner.py:415] Time since start: 1519.12s, 	Step: 3966, 	{'train/accuracy': 0.9872814416885376, 'train/loss': 0.045778002589941025, 'train/mean_average_precision': 0.11824389363434343, 'validation/accuracy': 0.9845506548881531, 'validation/loss': 0.05473804846405983, 'validation/mean_average_precision': 0.11792782109944548, 'validation/num_examples': 43793, 'test/accuracy': 0.9835383892059326, 'test/loss': 0.05772077292203903, 'test/mean_average_precision': 0.11673622535710373, 'test/num_examples': 43793, 'score': 980.0703732967377, 'total_duration': 1519.1214997768402, 'accumulated_submission_time': 980.0703732967377, 'accumulated_eval_time': 538.9405505657196, 'accumulated_logging_time': 0.08030343055725098}
I0428 12:53:41.262640 140141645010688 logging_writer.py:48] [3966] accumulated_eval_time=538.940551, accumulated_logging_time=0.080303, accumulated_submission_time=980.070373, global_step=3966, preemption_count=0, score=980.070373, test/accuracy=0.983538, test/loss=0.057721, test/mean_average_precision=0.116736, test/num_examples=43793, total_duration=1519.121500, train/accuracy=0.987281, train/loss=0.045778, train/mean_average_precision=0.118244, validation/accuracy=0.984551, validation/loss=0.054738, validation/mean_average_precision=0.117928, validation/num_examples=43793
I0428 12:53:49.813411 140141636617984 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.054478034377098083, loss=0.0519559308886528
I0428 12:54:13.852982 140141645010688 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.10050380975008011, loss=0.049114253371953964
I0428 12:54:38.056864 140141636617984 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0784166231751442, loss=0.04535233974456787
I0428 12:55:02.184653 140141645010688 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.046289410442113876, loss=0.04753318428993225
I0428 12:55:26.214167 140141636617984 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.050735801458358765, loss=0.047289684414863586
I0428 12:55:50.561952 140141645010688 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03729166463017464, loss=0.04621604457497597
I0428 12:56:14.909226 140141636617984 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.07078103721141815, loss=0.0446205697953701
I0428 12:56:38.953424 140141645010688 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.11149758845567703, loss=0.05260976403951645
I0428 12:57:03.370481 140141636617984 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04786446690559387, loss=0.04723581299185753
I0428 12:57:27.705047 140141645010688 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.057089176028966904, loss=0.03955548256635666
I0428 12:57:41.454187 140327390295872 spec.py:298] Evaluating on the training split.
I0428 12:58:56.000205 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 12:58:58.599500 140327390295872 spec.py:326] Evaluating on the test split.
I0428 12:59:01.110825 140327390295872 submission_runner.py:415] Time since start: 1838.98s, 	Step: 4958, 	{'train/accuracy': 0.9875807166099548, 'train/loss': 0.04342483729124069, 'train/mean_average_precision': 0.1556984295444017, 'validation/accuracy': 0.9848685264587402, 'validation/loss': 0.053310465067625046, 'validation/mean_average_precision': 0.14012368821889998, 'validation/num_examples': 43793, 'test/accuracy': 0.9838804006576538, 'test/loss': 0.05634255334734917, 'test/mean_average_precision': 0.14158214739750102, 'test/num_examples': 43793, 'score': 1220.2452318668365, 'total_duration': 1838.9775319099426, 'accumulated_submission_time': 1220.2452318668365, 'accumulated_eval_time': 618.5971353054047, 'accumulated_logging_time': 0.09748983383178711}
I0428 12:59:01.122644 140141636617984 logging_writer.py:48] [4958] accumulated_eval_time=618.597135, accumulated_logging_time=0.097490, accumulated_submission_time=1220.245232, global_step=4958, preemption_count=0, score=1220.245232, test/accuracy=0.983880, test/loss=0.056343, test/mean_average_precision=0.141582, test/num_examples=43793, total_duration=1838.977532, train/accuracy=0.987581, train/loss=0.043425, train/mean_average_precision=0.155698, validation/accuracy=0.984869, validation/loss=0.053310, validation/mean_average_precision=0.140124, validation/num_examples=43793
I0428 12:59:11.535434 140141645010688 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.06368330866098404, loss=0.0447736456990242
I0428 12:59:35.547033 140141636617984 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.06130431219935417, loss=0.047899216413497925
I0428 12:59:59.462013 140141645010688 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.07060042768716812, loss=0.04424970597028732
I0428 13:00:23.405890 140141636617984 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.047706250101327896, loss=0.04917117953300476
I0428 13:00:47.531755 140141645010688 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0935661718249321, loss=0.051391344517469406
I0428 13:01:12.366861 140141636617984 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.053276218473911285, loss=0.043340254575014114
I0428 13:01:36.068861 140141645010688 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.08891794085502625, loss=0.04614542797207832
I0428 13:01:59.999030 140141636617984 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.04759013652801514, loss=0.04414292424917221
I0428 13:02:23.943137 140141645010688 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.08621038496494293, loss=0.04420837387442589
I0428 13:02:47.900209 140141636617984 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.05486253648996353, loss=0.04561195895075798
I0428 13:03:01.305175 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:04:16.006016 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:04:18.596332 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:04:21.119867 140327390295872 submission_runner.py:415] Time since start: 2158.99s, 	Step: 5957, 	{'train/accuracy': 0.9881903529167175, 'train/loss': 0.04151694476604462, 'train/mean_average_precision': 0.16806894241419962, 'validation/accuracy': 0.9853357672691345, 'validation/loss': 0.051293112337589264, 'validation/mean_average_precision': 0.1533962819973666, 'validation/num_examples': 43793, 'test/accuracy': 0.9843125939369202, 'test/loss': 0.05413689464330673, 'test/mean_average_precision': 0.15477913905171836, 'test/num_examples': 43793, 'score': 1460.411060333252, 'total_duration': 2158.98659157753, 'accumulated_submission_time': 1460.411060333252, 'accumulated_eval_time': 698.4117965698242, 'accumulated_logging_time': 0.11851191520690918}
I0428 13:04:21.127742 140141645010688 logging_writer.py:48] [5957] accumulated_eval_time=698.411797, accumulated_logging_time=0.118512, accumulated_submission_time=1460.411060, global_step=5957, preemption_count=0, score=1460.411060, test/accuracy=0.984313, test/loss=0.054137, test/mean_average_precision=0.154779, test/num_examples=43793, total_duration=2158.986592, train/accuracy=0.988190, train/loss=0.041517, train/mean_average_precision=0.168069, validation/accuracy=0.985336, validation/loss=0.051293, validation/mean_average_precision=0.153396, validation/num_examples=43793
I0428 13:04:31.741645 140141636617984 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.07342532277107239, loss=0.04674946144223213
I0428 13:04:55.856115 140141645010688 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.08526023477315903, loss=0.04344414919614792
I0428 13:05:20.949173 140141636617984 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.05254339054226875, loss=0.04170539602637291
I0428 13:05:47.173523 140141645010688 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0751299187541008, loss=0.052138205617666245
I0428 13:06:12.105568 140141636617984 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.05389442667365074, loss=0.04258575290441513
I0428 13:06:36.196664 140141645010688 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.11445048451423645, loss=0.04801962524652481
I0428 13:07:00.311909 140141636617984 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.07076647877693176, loss=0.04243217781186104
I0428 13:07:24.458518 140141645010688 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06011752784252167, loss=0.043979212641716
I0428 13:07:48.534242 140141636617984 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.056678786873817444, loss=0.048728566616773605
I0428 13:08:12.716752 140141645010688 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0792093575000763, loss=0.04552916809916496
I0428 13:08:21.349937 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:09:35.736247 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:09:38.341737 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:09:40.873331 140327390295872 submission_runner.py:415] Time since start: 2478.74s, 	Step: 6936, 	{'train/accuracy': 0.9882644414901733, 'train/loss': 0.04068177938461304, 'train/mean_average_precision': 0.18795921059135834, 'validation/accuracy': 0.9854193925857544, 'validation/loss': 0.05004572495818138, 'validation/mean_average_precision': 0.167004872785467, 'validation/num_examples': 43793, 'test/accuracy': 0.984484851360321, 'test/loss': 0.05285721272230148, 'test/mean_average_precision': 0.16559216279574898, 'test/num_examples': 43793, 'score': 1700.6163828372955, 'total_duration': 2478.740056037903, 'accumulated_submission_time': 1700.6163828372955, 'accumulated_eval_time': 777.9351794719696, 'accumulated_logging_time': 0.13597798347473145}
I0428 13:09:40.881453 140141636617984 logging_writer.py:48] [6936] accumulated_eval_time=777.935179, accumulated_logging_time=0.135978, accumulated_submission_time=1700.616383, global_step=6936, preemption_count=0, score=1700.616383, test/accuracy=0.984485, test/loss=0.052857, test/mean_average_precision=0.165592, test/num_examples=43793, total_duration=2478.740056, train/accuracy=0.988264, train/loss=0.040682, train/mean_average_precision=0.187959, validation/accuracy=0.985419, validation/loss=0.050046, validation/mean_average_precision=0.167005, validation/num_examples=43793
I0428 13:09:56.758491 140141645010688 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.046124011278152466, loss=0.04022042453289032
I0428 13:10:21.116346 140141636617984 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.04058775678277016, loss=0.04376349225640297
I0428 13:10:45.493998 140141645010688 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.03460666164755821, loss=0.04101288691163063
I0428 13:11:09.686069 140141636617984 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.034818753600120544, loss=0.043370865285396576
I0428 13:11:33.632037 140141645010688 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.06557372957468033, loss=0.04036400467157364
I0428 13:11:57.509766 140141636617984 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.06733151525259018, loss=0.04350075125694275
I0428 13:12:21.547188 140141645010688 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.05983583256602287, loss=0.041831403970718384
I0428 13:12:45.696720 140141636617984 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.06807014346122742, loss=0.04851435124874115
I0428 13:13:09.933068 140141645010688 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.055821869522333145, loss=0.046327777206897736
I0428 13:13:33.840440 140141636617984 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.11983166635036469, loss=0.045624323189258575
I0428 13:13:41.104378 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:14:55.297318 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:14:57.870420 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:15:00.389438 140327390295872 submission_runner.py:415] Time since start: 2798.26s, 	Step: 7931, 	{'train/accuracy': 0.9883278012275696, 'train/loss': 0.039605431258678436, 'train/mean_average_precision': 0.21643672287470417, 'validation/accuracy': 0.9853032827377319, 'validation/loss': 0.050120942294597626, 'validation/mean_average_precision': 0.18246143929027542, 'validation/num_examples': 43793, 'test/accuracy': 0.9843635559082031, 'test/loss': 0.052920978516340256, 'test/mean_average_precision': 0.18203604731273032, 'test/num_examples': 43793, 'score': 1940.82279586792, 'total_duration': 2798.2561621665955, 'accumulated_submission_time': 1940.82279586792, 'accumulated_eval_time': 857.220211982727, 'accumulated_logging_time': 0.15325379371643066}
I0428 13:15:00.397794 140141645010688 logging_writer.py:48] [7931] accumulated_eval_time=857.220212, accumulated_logging_time=0.153254, accumulated_submission_time=1940.822796, global_step=7931, preemption_count=0, score=1940.822796, test/accuracy=0.984364, test/loss=0.052921, test/mean_average_precision=0.182036, test/num_examples=43793, total_duration=2798.256162, train/accuracy=0.988328, train/loss=0.039605, train/mean_average_precision=0.216437, validation/accuracy=0.985303, validation/loss=0.050121, validation/mean_average_precision=0.182461, validation/num_examples=43793
I0428 13:15:17.513927 140141636617984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.05946988984942436, loss=0.0446142740547657
I0428 13:15:41.706985 140141645010688 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.03987889364361763, loss=0.04113041236996651
I0428 13:16:06.088032 140141636617984 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.06377660483121872, loss=0.0443200059235096
I0428 13:16:30.069417 140141645010688 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.059570781886577606, loss=0.041633907705545425
I0428 13:16:54.104451 140141636617984 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.041083917021751404, loss=0.04156176745891571
I0428 13:17:18.302389 140141645010688 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.053651705384254456, loss=0.04135916754603386
I0428 13:17:42.473283 140141636617984 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.06260140985250473, loss=0.044035010039806366
I0428 13:18:06.516452 140141645010688 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.05816949903964996, loss=0.04231588542461395
I0428 13:18:30.422490 140141636617984 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.06798458844423294, loss=0.04096923768520355
I0428 13:18:54.503409 140141645010688 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0445784293115139, loss=0.04752299562096596
I0428 13:19:00.499733 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:20:15.137672 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:20:17.741101 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:20:20.254847 140327390295872 submission_runner.py:415] Time since start: 3118.12s, 	Step: 8926, 	{'train/accuracy': 0.9885217547416687, 'train/loss': 0.03891540318727493, 'train/mean_average_precision': 0.22655577006081629, 'validation/accuracy': 0.9855472445487976, 'validation/loss': 0.049825944006443024, 'validation/mean_average_precision': 0.19023721787568143, 'validation/num_examples': 43793, 'test/accuracy': 0.984586775302887, 'test/loss': 0.05284470319747925, 'test/mean_average_precision': 0.19067145664569246, 'test/num_examples': 43793, 'score': 2180.908219099045, 'total_duration': 3118.1215720176697, 'accumulated_submission_time': 2180.908219099045, 'accumulated_eval_time': 936.9752886295319, 'accumulated_logging_time': 0.17089009284973145}
I0428 13:20:20.263730 140141636617984 logging_writer.py:48] [8926] accumulated_eval_time=936.975289, accumulated_logging_time=0.170890, accumulated_submission_time=2180.908219, global_step=8926, preemption_count=0, score=2180.908219, test/accuracy=0.984587, test/loss=0.052845, test/mean_average_precision=0.190671, test/num_examples=43793, total_duration=3118.121572, train/accuracy=0.988522, train/loss=0.038915, train/mean_average_precision=0.226556, validation/accuracy=0.985547, validation/loss=0.049826, validation/mean_average_precision=0.190237, validation/num_examples=43793
I0428 13:20:38.705863 140141645010688 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.07561202347278595, loss=0.04024132341146469
I0428 13:21:02.810769 140141636617984 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0753156915307045, loss=0.0380663201212883
I0428 13:21:26.855796 140141645010688 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.08122705668210983, loss=0.044763289391994476
I0428 13:21:50.638496 140141636617984 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.04412858560681343, loss=0.043275244534015656
I0428 13:22:14.510807 140141645010688 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.07494369894266129, loss=0.04323165863752365
I0428 13:22:38.635152 140141636617984 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.07632583379745483, loss=0.04024768993258476
I0428 13:23:02.582087 140141645010688 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.04290105402469635, loss=0.044401898980140686
I0428 13:23:26.456882 140141636617984 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03658472001552582, loss=0.04071267321705818
I0428 13:23:50.385006 140141645010688 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.047430772334337234, loss=0.04527467116713524
I0428 13:24:14.581847 140141636617984 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.048792995512485504, loss=0.04616698622703552
I0428 13:24:20.359041 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:25:33.623424 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:25:36.177570 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:25:38.665857 140327390295872 submission_runner.py:415] Time since start: 3436.53s, 	Step: 9925, 	{'train/accuracy': 0.9887782335281372, 'train/loss': 0.03832365199923515, 'train/mean_average_precision': 0.24533272158363167, 'validation/accuracy': 0.9857770204544067, 'validation/loss': 0.04883429408073425, 'validation/mean_average_precision': 0.19927598279688824, 'validation/num_examples': 43793, 'test/accuracy': 0.9848613739013672, 'test/loss': 0.051510296761989594, 'test/mean_average_precision': 0.1977062786100664, 'test/num_examples': 43793, 'score': 2420.986745595932, 'total_duration': 3436.5325815677643, 'accumulated_submission_time': 2420.986745595932, 'accumulated_eval_time': 1015.2820663452148, 'accumulated_logging_time': 0.18903112411499023}
I0428 13:25:38.675265 140141645010688 logging_writer.py:48] [9925] accumulated_eval_time=1015.282066, accumulated_logging_time=0.189031, accumulated_submission_time=2420.986746, global_step=9925, preemption_count=0, score=2420.986746, test/accuracy=0.984861, test/loss=0.051510, test/mean_average_precision=0.197706, test/num_examples=43793, total_duration=3436.532582, train/accuracy=0.988778, train/loss=0.038324, train/mean_average_precision=0.245333, validation/accuracy=0.985777, validation/loss=0.048834, validation/mean_average_precision=0.199276, validation/num_examples=43793
I0428 13:25:56.893424 140141636617984 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.04444592818617821, loss=0.04078042507171631
I0428 13:26:21.159334 140141645010688 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.035854704678058624, loss=0.03989467769861221
I0428 13:26:45.423931 140141636617984 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.06837531924247742, loss=0.046138640493154526
I0428 13:27:09.771180 140141645010688 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04876406118273735, loss=0.04247784987092018
I0428 13:27:34.041790 140141636617984 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.05798027291893959, loss=0.04314620792865753
I0428 13:27:58.356636 140141645010688 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.06101466715335846, loss=0.04203157499432564
I0428 13:28:22.691349 140141636617984 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.046858735382556915, loss=0.03923143446445465
I0428 13:28:46.900485 140141645010688 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.057578276842832565, loss=0.03853670880198479
I0428 13:29:10.979912 140141636617984 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.08183278143405914, loss=0.040885649621486664
I0428 13:29:35.209722 140141645010688 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.08719921112060547, loss=0.04071498289704323
I0428 13:29:38.831000 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:30:54.508724 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:30:57.074651 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:30:59.593015 140327390295872 submission_runner.py:415] Time since start: 3757.46s, 	Step: 10916, 	{'train/accuracy': 0.9891546368598938, 'train/loss': 0.03718456253409386, 'train/mean_average_precision': 0.26368965738588857, 'validation/accuracy': 0.9859207272529602, 'validation/loss': 0.04780237749218941, 'validation/mean_average_precision': 0.20493798045604966, 'validation/num_examples': 43793, 'test/accuracy': 0.985004186630249, 'test/loss': 0.0505712628364563, 'test/mean_average_precision': 0.20289454143450325, 'test/num_examples': 43793, 'score': 2661.125837087631, 'total_duration': 3757.4597306251526, 'accumulated_submission_time': 2661.125837087631, 'accumulated_eval_time': 1096.0440530776978, 'accumulated_logging_time': 0.20751237869262695}
I0428 13:30:59.602384 140141636617984 logging_writer.py:48] [10916] accumulated_eval_time=1096.044053, accumulated_logging_time=0.207512, accumulated_submission_time=2661.125837, global_step=10916, preemption_count=0, score=2661.125837, test/accuracy=0.985004, test/loss=0.050571, test/mean_average_precision=0.202895, test/num_examples=43793, total_duration=3757.459731, train/accuracy=0.989155, train/loss=0.037185, train/mean_average_precision=0.263690, validation/accuracy=0.985921, validation/loss=0.047802, validation/mean_average_precision=0.204938, validation/num_examples=43793
I0428 13:31:20.059033 140141645010688 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.07711786776781082, loss=0.04241376742720604
I0428 13:31:44.250411 140141636617984 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.07475762069225311, loss=0.044541362673044205
I0428 13:32:08.491826 140141645010688 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.11558911949396133, loss=0.04064331576228142
I0428 13:32:32.613647 140141636617984 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.047600213438272476, loss=0.038581732660532
I0428 13:32:56.949687 140141645010688 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.047781310975551605, loss=0.04324972257018089
I0428 13:33:21.342696 140141636617984 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.065548837184906, loss=0.046845369040966034
I0428 13:33:45.485880 140141645010688 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.05718741565942764, loss=0.04165123775601387
I0428 13:34:09.887864 140141636617984 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.08852308243513107, loss=0.04230017215013504
I0428 13:34:34.576520 140141645010688 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.046828124672174454, loss=0.0389457568526268
I0428 13:34:59.028625 140141636617984 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.05733374133706093, loss=0.03914785012602806
I0428 13:34:59.770278 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:36:15.259833 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:36:17.852946 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:36:20.400484 140327390295872 submission_runner.py:415] Time since start: 4078.27s, 	Step: 11904, 	{'train/accuracy': 0.9891635775566101, 'train/loss': 0.03650268539786339, 'train/mean_average_precision': 0.2708061013708831, 'validation/accuracy': 0.9860506057739258, 'validation/loss': 0.04715700075030327, 'validation/mean_average_precision': 0.21873999191618126, 'validation/num_examples': 43793, 'test/accuracy': 0.9850955605506897, 'test/loss': 0.05002865195274353, 'test/mean_average_precision': 0.21066734440041476, 'test/num_examples': 43793, 'score': 2901.277104139328, 'total_duration': 4078.2671880722046, 'accumulated_submission_time': 2901.277104139328, 'accumulated_eval_time': 1176.674198627472, 'accumulated_logging_time': 0.22605323791503906}
I0428 13:36:20.410252 140141645010688 logging_writer.py:48] [11904] accumulated_eval_time=1176.674199, accumulated_logging_time=0.226053, accumulated_submission_time=2901.277104, global_step=11904, preemption_count=0, score=2901.277104, test/accuracy=0.985096, test/loss=0.050029, test/mean_average_precision=0.210667, test/num_examples=43793, total_duration=4078.267188, train/accuracy=0.989164, train/loss=0.036503, train/mean_average_precision=0.270806, validation/accuracy=0.986051, validation/loss=0.047157, validation/mean_average_precision=0.218740, validation/num_examples=43793
I0428 13:36:43.757036 140327390295872 spec.py:298] Evaluating on the training split.
I0428 13:37:57.921816 140327390295872 spec.py:310] Evaluating on the validation split.
I0428 13:38:00.530786 140327390295872 spec.py:326] Evaluating on the test split.
I0428 13:38:03.159820 140327390295872 submission_runner.py:415] Time since start: 4181.03s, 	Step: 12000, 	{'train/accuracy': 0.9895005226135254, 'train/loss': 0.03599487245082855, 'train/mean_average_precision': 0.27765564296877665, 'validation/accuracy': 0.9862191081047058, 'validation/loss': 0.0467895083129406, 'validation/mean_average_precision': 0.22187835039012624, 'validation/num_examples': 43793, 'test/accuracy': 0.9852543473243713, 'test/loss': 0.04954680800437927, 'test/mean_average_precision': 0.21454158480244595, 'test/num_examples': 43793, 'score': 2924.6124749183655, 'total_duration': 4181.026522159576, 'accumulated_submission_time': 2924.6124749183655, 'accumulated_eval_time': 1256.0769217014313, 'accumulated_logging_time': 0.24627423286437988}
I0428 13:38:03.168535 140141636617984 logging_writer.py:48] [12000] accumulated_eval_time=1256.076922, accumulated_logging_time=0.246274, accumulated_submission_time=2924.612475, global_step=12000, preemption_count=0, score=2924.612475, test/accuracy=0.985254, test/loss=0.049547, test/mean_average_precision=0.214542, test/num_examples=43793, total_duration=4181.026522, train/accuracy=0.989501, train/loss=0.035995, train/mean_average_precision=0.277656, validation/accuracy=0.986219, validation/loss=0.046790, validation/mean_average_precision=0.221878, validation/num_examples=43793
I0428 13:38:03.184236 140141645010688 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2924.612475
I0428 13:38:03.202954 140327390295872 checkpoints.py:356] Saving checkpoint at step: 12000
I0428 13:38:03.263621 140327390295872 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000
I0428 13:38:03.263816 140327390295872 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_nesterov/ogbg_jax/trial_1/checkpoint_12000.
I0428 13:38:03.431869 140327390295872 submission_runner.py:578] Tuning trial 1/1
I0428 13:38:03.432133 140327390295872 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0428 13:38:03.433454 140327390295872 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5024756789207458, 'train/loss': 0.7288255095481873, 'train/mean_average_precision': 0.02415142946925777, 'validation/accuracy': 0.5087609887123108, 'validation/loss': 0.7304736375808716, 'validation/mean_average_precision': 0.027352464209319464, 'validation/num_examples': 43793, 'test/accuracy': 0.5105212330818176, 'test/loss': 0.730847954750061, 'test/mean_average_precision': 0.029412253490863103, 'test/num_examples': 43793, 'score': 19.466670751571655, 'total_duration': 237.9807116985321, 'accumulated_submission_time': 19.466670751571655, 'accumulated_eval_time': 218.51390266418457, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (989, {'train/accuracy': 0.9868044257164001, 'train/loss': 0.05426831543445587, 'train/mean_average_precision': 0.034148035969027636, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06463412195444107, 'validation/mean_average_precision': 0.03810721638076439, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0679437667131424, 'test/mean_average_precision': 0.03924969839645322, 'test/num_examples': 43793, 'score': 259.5731270313263, 'total_duration': 557.0619051456451, 'accumulated_submission_time': 259.5731270313263, 'accumulated_eval_time': 297.45587825775146, 'accumulated_logging_time': 0.025253772735595703, 'global_step': 989, 'preemption_count': 0}), (1986, {'train/accuracy': 0.9867419600486755, 'train/loss': 0.05115160346031189, 'train/mean_average_precision': 0.06257172577375161, 'validation/accuracy': 0.9841305017471313, 'validation/loss': 0.060121774673461914, 'validation/mean_average_precision': 0.06021660418452359, 'validation/num_examples': 43793, 'test/accuracy': 0.983154296875, 'test/loss': 0.06331733614206314, 'test/mean_average_precision': 0.06063212816208897, 'test/num_examples': 43793, 'score': 499.68025493621826, 'total_duration': 877.22385597229, 'accumulated_submission_time': 499.68025493621826, 'accumulated_eval_time': 377.4855463504791, 'accumulated_logging_time': 0.04318380355834961, 'global_step': 1986, 'preemption_count': 0}), (2974, {'train/accuracy': 0.9870172739028931, 'train/loss': 0.049812622368335724, 'train/mean_average_precision': 0.09219071678834041, 'validation/accuracy': 0.9843111634254456, 'validation/loss': 0.06044057011604309, 'validation/mean_average_precision': 0.08412881589403369, 'validation/num_examples': 43793, 'test/accuracy': 0.9833135008811951, 'test/loss': 0.06361602246761322, 'test/mean_average_precision': 0.08788961438024721, 'test/num_examples': 43793, 'score': 739.8981008529663, 'total_duration': 1198.0872535705566, 'accumulated_submission_time': 739.8981008529663, 'accumulated_eval_time': 458.10410809516907, 'accumulated_logging_time': 0.06235384941101074, 'global_step': 2974, 'preemption_count': 0}), (3966, {'train/accuracy': 0.9872814416885376, 'train/loss': 0.045778002589941025, 'train/mean_average_precision': 0.11824389363434343, 'validation/accuracy': 0.9845506548881531, 'validation/loss': 0.05473804846405983, 'validation/mean_average_precision': 0.11792782109944548, 'validation/num_examples': 43793, 'test/accuracy': 0.9835383892059326, 'test/loss': 0.05772077292203903, 'test/mean_average_precision': 0.11673622535710373, 'test/num_examples': 43793, 'score': 980.0703732967377, 'total_duration': 1519.1214997768402, 'accumulated_submission_time': 980.0703732967377, 'accumulated_eval_time': 538.9405505657196, 'accumulated_logging_time': 0.08030343055725098, 'global_step': 3966, 'preemption_count': 0}), (4958, {'train/accuracy': 0.9875807166099548, 'train/loss': 0.04342483729124069, 'train/mean_average_precision': 0.1556984295444017, 'validation/accuracy': 0.9848685264587402, 'validation/loss': 0.053310465067625046, 'validation/mean_average_precision': 0.14012368821889998, 'validation/num_examples': 43793, 'test/accuracy': 0.9838804006576538, 'test/loss': 0.05634255334734917, 'test/mean_average_precision': 0.14158214739750102, 'test/num_examples': 43793, 'score': 1220.2452318668365, 'total_duration': 1838.9775319099426, 'accumulated_submission_time': 1220.2452318668365, 'accumulated_eval_time': 618.5971353054047, 'accumulated_logging_time': 0.09748983383178711, 'global_step': 4958, 'preemption_count': 0}), (5957, {'train/accuracy': 0.9881903529167175, 'train/loss': 0.04151694476604462, 'train/mean_average_precision': 0.16806894241419962, 'validation/accuracy': 0.9853357672691345, 'validation/loss': 0.051293112337589264, 'validation/mean_average_precision': 0.1533962819973666, 'validation/num_examples': 43793, 'test/accuracy': 0.9843125939369202, 'test/loss': 0.05413689464330673, 'test/mean_average_precision': 0.15477913905171836, 'test/num_examples': 43793, 'score': 1460.411060333252, 'total_duration': 2158.98659157753, 'accumulated_submission_time': 1460.411060333252, 'accumulated_eval_time': 698.4117965698242, 'accumulated_logging_time': 0.11851191520690918, 'global_step': 5957, 'preemption_count': 0}), (6936, {'train/accuracy': 0.9882644414901733, 'train/loss': 0.04068177938461304, 'train/mean_average_precision': 0.18795921059135834, 'validation/accuracy': 0.9854193925857544, 'validation/loss': 0.05004572495818138, 'validation/mean_average_precision': 0.167004872785467, 'validation/num_examples': 43793, 'test/accuracy': 0.984484851360321, 'test/loss': 0.05285721272230148, 'test/mean_average_precision': 0.16559216279574898, 'test/num_examples': 43793, 'score': 1700.6163828372955, 'total_duration': 2478.740056037903, 'accumulated_submission_time': 1700.6163828372955, 'accumulated_eval_time': 777.9351794719696, 'accumulated_logging_time': 0.13597798347473145, 'global_step': 6936, 'preemption_count': 0}), (7931, {'train/accuracy': 0.9883278012275696, 'train/loss': 0.039605431258678436, 'train/mean_average_precision': 0.21643672287470417, 'validation/accuracy': 0.9853032827377319, 'validation/loss': 0.050120942294597626, 'validation/mean_average_precision': 0.18246143929027542, 'validation/num_examples': 43793, 'test/accuracy': 0.9843635559082031, 'test/loss': 0.052920978516340256, 'test/mean_average_precision': 0.18203604731273032, 'test/num_examples': 43793, 'score': 1940.82279586792, 'total_duration': 2798.2561621665955, 'accumulated_submission_time': 1940.82279586792, 'accumulated_eval_time': 857.220211982727, 'accumulated_logging_time': 0.15325379371643066, 'global_step': 7931, 'preemption_count': 0}), (8926, {'train/accuracy': 0.9885217547416687, 'train/loss': 0.03891540318727493, 'train/mean_average_precision': 0.22655577006081629, 'validation/accuracy': 0.9855472445487976, 'validation/loss': 0.049825944006443024, 'validation/mean_average_precision': 0.19023721787568143, 'validation/num_examples': 43793, 'test/accuracy': 0.984586775302887, 'test/loss': 0.05284470319747925, 'test/mean_average_precision': 0.19067145664569246, 'test/num_examples': 43793, 'score': 2180.908219099045, 'total_duration': 3118.1215720176697, 'accumulated_submission_time': 2180.908219099045, 'accumulated_eval_time': 936.9752886295319, 'accumulated_logging_time': 0.17089009284973145, 'global_step': 8926, 'preemption_count': 0}), (9925, {'train/accuracy': 0.9887782335281372, 'train/loss': 0.03832365199923515, 'train/mean_average_precision': 0.24533272158363167, 'validation/accuracy': 0.9857770204544067, 'validation/loss': 0.04883429408073425, 'validation/mean_average_precision': 0.19927598279688824, 'validation/num_examples': 43793, 'test/accuracy': 0.9848613739013672, 'test/loss': 0.051510296761989594, 'test/mean_average_precision': 0.1977062786100664, 'test/num_examples': 43793, 'score': 2420.986745595932, 'total_duration': 3436.5325815677643, 'accumulated_submission_time': 2420.986745595932, 'accumulated_eval_time': 1015.2820663452148, 'accumulated_logging_time': 0.18903112411499023, 'global_step': 9925, 'preemption_count': 0}), (10916, {'train/accuracy': 0.9891546368598938, 'train/loss': 0.03718456253409386, 'train/mean_average_precision': 0.26368965738588857, 'validation/accuracy': 0.9859207272529602, 'validation/loss': 0.04780237749218941, 'validation/mean_average_precision': 0.20493798045604966, 'validation/num_examples': 43793, 'test/accuracy': 0.985004186630249, 'test/loss': 0.0505712628364563, 'test/mean_average_precision': 0.20289454143450325, 'test/num_examples': 43793, 'score': 2661.125837087631, 'total_duration': 3757.4597306251526, 'accumulated_submission_time': 2661.125837087631, 'accumulated_eval_time': 1096.0440530776978, 'accumulated_logging_time': 0.20751237869262695, 'global_step': 10916, 'preemption_count': 0}), (11904, {'train/accuracy': 0.9891635775566101, 'train/loss': 0.03650268539786339, 'train/mean_average_precision': 0.2708061013708831, 'validation/accuracy': 0.9860506057739258, 'validation/loss': 0.04715700075030327, 'validation/mean_average_precision': 0.21873999191618126, 'validation/num_examples': 43793, 'test/accuracy': 0.9850955605506897, 'test/loss': 0.05002865195274353, 'test/mean_average_precision': 0.21066734440041476, 'test/num_examples': 43793, 'score': 2901.277104139328, 'total_duration': 4078.2671880722046, 'accumulated_submission_time': 2901.277104139328, 'accumulated_eval_time': 1176.674198627472, 'accumulated_logging_time': 0.22605323791503906, 'global_step': 11904, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9895005226135254, 'train/loss': 0.03599487245082855, 'train/mean_average_precision': 0.27765564296877665, 'validation/accuracy': 0.9862191081047058, 'validation/loss': 0.0467895083129406, 'validation/mean_average_precision': 0.22187835039012624, 'validation/num_examples': 43793, 'test/accuracy': 0.9852543473243713, 'test/loss': 0.04954680800437927, 'test/mean_average_precision': 0.21454158480244595, 'test/num_examples': 43793, 'score': 2924.6124749183655, 'total_duration': 4181.026522159576, 'accumulated_submission_time': 2924.6124749183655, 'accumulated_eval_time': 1256.0769217014313, 'accumulated_logging_time': 0.24627423286437988, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0428 13:38:03.433584 140327390295872 submission_runner.py:581] Timing: 2924.6124749183655
I0428 13:38:03.433627 140327390295872 submission_runner.py:582] ====================
I0428 13:38:03.433742 140327390295872 submission_runner.py:645] Final ogbg score: 2924.6124749183655
