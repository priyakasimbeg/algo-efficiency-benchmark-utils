python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-09-2023-19-38-46.log
2023-10-09 19:38:51.746699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1009 19:39:10.401705 139760355301184 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax because --overwrite was set.
I1009 19:39:10.420852 139760355301184 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax.
I1009 19:39:11.490316 139760355301184 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I1009 19:39:11.491049 139760355301184 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1009 19:39:11.491194 139760355301184 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1009 19:39:11.496222 139760355301184 submission_runner.py:510] Using RNG seed 2846039931
I1009 19:39:17.555234 139760355301184 submission_runner.py:519] --- Tuning run 1/1 ---
I1009 19:39:17.555457 139760355301184 submission_runner.py:524] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1.
I1009 19:39:17.555648 139760355301184 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/hparams.json.
I1009 19:39:17.735958 139760355301184 submission_runner.py:194] Initializing dataset.
I1009 19:39:17.736191 139760355301184 submission_runner.py:201] Initializing model.
I1009 19:39:22.722526 139760355301184 submission_runner.py:235] Initializing optimizer.
I1009 19:39:23.940382 139760355301184 submission_runner.py:242] Initializing metrics bundle.
I1009 19:39:23.940623 139760355301184 submission_runner.py:260] Initializing checkpoint and logger.
I1009 19:39:23.941933 139760355301184 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1009 19:39:23.942095 139760355301184 submission_runner.py:280] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1009 19:39:23.942312 139760355301184 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1009 19:39:23.942398 139760355301184 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1009 19:39:24.252035 139760355301184 logger_utils.py:220] Unable to record git information. Continuing without it.
I1009 19:39:24.546960 139760355301184 submission_runner.py:283] Saving flags to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/flags_0.json.
I1009 19:39:24.563106 139760355301184 submission_runner.py:293] Starting training loop.
I1009 19:39:24.866855 139760355301184 input_pipeline.py:20] Loading split = train-clean-100
I1009 19:39:24.927079 139760355301184 input_pipeline.py:20] Loading split = train-clean-360
I1009 19:39:25.438222 139760355301184 input_pipeline.py:20] Loading split = train-other-500
2023-10-09 19:40:34.859608: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-09 19:40:37.049054: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1009 19:40:39.028908 139585757636352 logging_writer.py:48] [0] global_step=0, grad_norm=27.835145950317383, loss=32.9799919128418
I1009 19:40:39.065835 139760355301184 spec.py:321] Evaluating on the training split.
I1009 19:40:39.239372 139760355301184 input_pipeline.py:20] Loading split = train-clean-100
I1009 19:40:39.274660 139760355301184 input_pipeline.py:20] Loading split = train-clean-360
I1009 19:40:39.709194 139760355301184 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1009 19:41:50.638797 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 19:41:50.751393 139760355301184 input_pipeline.py:20] Loading split = dev-clean
I1009 19:41:50.756699 139760355301184 input_pipeline.py:20] Loading split = dev-other
I1009 19:42:41.134622 139760355301184 spec.py:349] Evaluating on the test split.
I1009 19:42:41.251951 139760355301184 input_pipeline.py:20] Loading split = test-clean
I1009 19:43:04.573505 139760355301184 submission_runner.py:384] Time since start: 220.01s, 	Step: 1, 	{'train/ctc_loss': Array(31.720894, dtype=float32), 'train/wer': 1.1345761405992962, 'validation/ctc_loss': Array(31.137352, dtype=float32), 'validation/wer': 1.0191994134048568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.172659, dtype=float32), 'test/wer': 1.028688607390347, 'test/num_examples': 2472, 'score': 74.50265550613403, 'total_duration': 220.00778794288635, 'accumulated_submission_time': 74.50265550613403, 'accumulated_eval_time': 145.50507068634033, 'accumulated_logging_time': 0}
I1009 19:43:04.600708 139580422481664 logging_writer.py:48] [1] accumulated_eval_time=145.505071, accumulated_logging_time=0, accumulated_submission_time=74.502656, global_step=1, preemption_count=0, score=74.502656, test/ctc_loss=31.172658920288086, test/num_examples=2472, test/wer=1.028689, total_duration=220.007788, train/ctc_loss=31.72089385986328, train/wer=1.134576, validation/ctc_loss=31.137351989746094, validation/num_examples=5348, validation/wer=1.019199
I1009 19:43:26.207699 139587698222848 logging_writer.py:48] [1] global_step=1, grad_norm=28.009977340698242, loss=32.488800048828125
I1009 19:43:27.097490 139587706615552 logging_writer.py:48] [2] global_step=2, grad_norm=26.42432975769043, loss=32.643592834472656
I1009 19:43:27.973701 139587698222848 logging_writer.py:48] [3] global_step=3, grad_norm=28.736717224121094, loss=32.74985885620117
I1009 19:43:28.856056 139587706615552 logging_writer.py:48] [4] global_step=4, grad_norm=29.933549880981445, loss=33.204872131347656
I1009 19:43:29.739631 139587698222848 logging_writer.py:48] [5] global_step=5, grad_norm=28.986467361450195, loss=32.58707809448242
I1009 19:43:30.622929 139587706615552 logging_writer.py:48] [6] global_step=6, grad_norm=27.991361618041992, loss=32.25825119018555
I1009 19:43:31.494607 139587698222848 logging_writer.py:48] [7] global_step=7, grad_norm=30.397436141967773, loss=32.55638885498047
I1009 19:43:32.373378 139587706615552 logging_writer.py:48] [8] global_step=8, grad_norm=31.67100715637207, loss=32.74666976928711
I1009 19:43:33.263936 139587698222848 logging_writer.py:48] [9] global_step=9, grad_norm=33.286312103271484, loss=32.347564697265625
I1009 19:43:34.154773 139587706615552 logging_writer.py:48] [10] global_step=10, grad_norm=35.55896759033203, loss=32.413944244384766
I1009 19:43:35.034565 139587698222848 logging_writer.py:48] [11] global_step=11, grad_norm=37.00658416748047, loss=31.99158477783203
I1009 19:43:35.926121 139587706615552 logging_writer.py:48] [12] global_step=12, grad_norm=41.62263488769531, loss=32.14340591430664
I1009 19:43:36.820201 139587698222848 logging_writer.py:48] [13] global_step=13, grad_norm=46.55521011352539, loss=31.958391189575195
I1009 19:43:37.694669 139587706615552 logging_writer.py:48] [14] global_step=14, grad_norm=49.0362434387207, loss=32.39801025390625
I1009 19:43:38.559463 139587698222848 logging_writer.py:48] [15] global_step=15, grad_norm=58.536651611328125, loss=31.140962600708008
I1009 19:43:39.468801 139587706615552 logging_writer.py:48] [16] global_step=16, grad_norm=74.64601135253906, loss=31.676441192626953
I1009 19:43:40.364741 139587698222848 logging_writer.py:48] [17] global_step=17, grad_norm=85.15461730957031, loss=31.284048080444336
I1009 19:43:41.263978 139587706615552 logging_writer.py:48] [18] global_step=18, grad_norm=101.58943176269531, loss=30.72071075439453
I1009 19:43:42.147951 139587698222848 logging_writer.py:48] [19] global_step=19, grad_norm=118.18518829345703, loss=30.503061294555664
I1009 19:43:43.051389 139587706615552 logging_writer.py:48] [20] global_step=20, grad_norm=133.38197326660156, loss=29.789541244506836
I1009 19:43:43.948938 139587698222848 logging_writer.py:48] [21] global_step=21, grad_norm=147.3145751953125, loss=29.71454429626465
I1009 19:43:44.833410 139587706615552 logging_writer.py:48] [22] global_step=22, grad_norm=146.30511474609375, loss=28.451051712036133
I1009 19:43:45.714866 139587698222848 logging_writer.py:48] [23] global_step=23, grad_norm=166.1171417236328, loss=28.288270950317383
I1009 19:43:46.613540 139587706615552 logging_writer.py:48] [24] global_step=24, grad_norm=165.92835998535156, loss=26.857566833496094
I1009 19:43:47.508484 139587698222848 logging_writer.py:48] [25] global_step=25, grad_norm=174.26812744140625, loss=26.136568069458008
I1009 19:43:48.391132 139587706615552 logging_writer.py:48] [26] global_step=26, grad_norm=174.79403686523438, loss=25.210275650024414
I1009 19:43:49.271285 139587698222848 logging_writer.py:48] [27] global_step=27, grad_norm=175.70054626464844, loss=24.062646865844727
I1009 19:43:50.168015 139587706615552 logging_writer.py:48] [28] global_step=28, grad_norm=178.46792602539062, loss=23.384557723999023
I1009 19:43:51.062756 139587698222848 logging_writer.py:48] [29] global_step=29, grad_norm=170.943359375, loss=21.598342895507812
I1009 19:43:51.952502 139587706615552 logging_writer.py:48] [30] global_step=30, grad_norm=165.54515075683594, loss=20.338241577148438
I1009 19:43:52.835410 139587698222848 logging_writer.py:48] [31] global_step=31, grad_norm=165.3134765625, loss=19.498077392578125
I1009 19:43:53.731761 139587706615552 logging_writer.py:48] [32] global_step=32, grad_norm=162.37789916992188, loss=18.384029388427734
I1009 19:43:54.616703 139587698222848 logging_writer.py:48] [33] global_step=33, grad_norm=152.57266235351562, loss=17.06553840637207
I1009 19:43:55.502436 139587706615552 logging_writer.py:48] [34] global_step=34, grad_norm=151.03883361816406, loss=16.199249267578125
I1009 19:43:56.398496 139587698222848 logging_writer.py:48] [35] global_step=35, grad_norm=143.65065002441406, loss=15.091744422912598
I1009 19:43:57.296527 139587706615552 logging_writer.py:48] [36] global_step=36, grad_norm=135.2976837158203, loss=13.98735237121582
I1009 19:43:58.191072 139587698222848 logging_writer.py:48] [37] global_step=37, grad_norm=124.75050354003906, loss=12.815347671508789
I1009 19:43:59.074644 139587706615552 logging_writer.py:48] [38] global_step=38, grad_norm=112.82113647460938, loss=11.854264259338379
I1009 19:43:59.966521 139587698222848 logging_writer.py:48] [39] global_step=39, grad_norm=104.7998046875, loss=11.05001449584961
I1009 19:44:00.861075 139587706615552 logging_writer.py:48] [40] global_step=40, grad_norm=92.86536407470703, loss=10.264581680297852
I1009 19:44:01.747734 139587698222848 logging_writer.py:48] [41] global_step=41, grad_norm=75.98245239257812, loss=9.46047306060791
I1009 19:44:02.663410 139587706615552 logging_writer.py:48] [42] global_step=42, grad_norm=63.8502082824707, loss=8.83056640625
I1009 19:44:03.555231 139587698222848 logging_writer.py:48] [43] global_step=43, grad_norm=52.49529266357422, loss=8.390849113464355
I1009 19:44:04.448148 139587706615552 logging_writer.py:48] [44] global_step=44, grad_norm=43.051082611083984, loss=8.060647010803223
I1009 19:44:05.352773 139587698222848 logging_writer.py:48] [45] global_step=45, grad_norm=29.860286712646484, loss=7.694211959838867
I1009 19:44:06.242026 139587706615552 logging_writer.py:48] [46] global_step=46, grad_norm=21.470481872558594, loss=7.502135276794434
I1009 19:44:07.134686 139587698222848 logging_writer.py:48] [47] global_step=47, grad_norm=13.519725799560547, loss=7.352540969848633
I1009 19:44:08.031916 139587706615552 logging_writer.py:48] [48] global_step=48, grad_norm=6.767157554626465, loss=7.270596504211426
I1009 19:44:08.930804 139587698222848 logging_writer.py:48] [49] global_step=49, grad_norm=3.476146936416626, loss=7.233681678771973
I1009 19:44:09.819446 139587706615552 logging_writer.py:48] [50] global_step=50, grad_norm=5.2226104736328125, loss=7.234013557434082
I1009 19:44:10.704771 139587698222848 logging_writer.py:48] [51] global_step=51, grad_norm=7.877896308898926, loss=7.281310081481934
I1009 19:44:11.598181 139587706615552 logging_writer.py:48] [52] global_step=52, grad_norm=11.156597137451172, loss=7.320186138153076
I1009 19:44:12.489296 139587698222848 logging_writer.py:48] [53] global_step=53, grad_norm=12.521448135375977, loss=7.386884689331055
I1009 19:44:13.379482 139587706615552 logging_writer.py:48] [54] global_step=54, grad_norm=15.28795051574707, loss=7.458743572235107
I1009 19:44:14.274480 139587698222848 logging_writer.py:48] [55] global_step=55, grad_norm=16.515636444091797, loss=7.531008720397949
I1009 19:44:15.167311 139587706615552 logging_writer.py:48] [56] global_step=56, grad_norm=18.055133819580078, loss=7.627955436706543
I1009 19:44:16.056878 139587698222848 logging_writer.py:48] [57] global_step=57, grad_norm=18.868711471557617, loss=7.664453983306885
I1009 19:44:16.947224 139587706615552 logging_writer.py:48] [58] global_step=58, grad_norm=19.707820892333984, loss=7.7386250495910645
I1009 19:44:17.845229 139587698222848 logging_writer.py:48] [59] global_step=59, grad_norm=20.500057220458984, loss=7.805132865905762
I1009 19:44:18.754073 139587706615552 logging_writer.py:48] [60] global_step=60, grad_norm=21.08308982849121, loss=7.890673637390137
I1009 19:44:19.648044 139587698222848 logging_writer.py:48] [61] global_step=61, grad_norm=21.529006958007812, loss=7.934958457946777
I1009 19:44:20.540225 139587706615552 logging_writer.py:48] [62] global_step=62, grad_norm=21.753694534301758, loss=7.964203357696533
I1009 19:44:21.443091 139587698222848 logging_writer.py:48] [63] global_step=63, grad_norm=22.189456939697266, loss=8.01683521270752
I1009 19:44:22.341827 139587706615552 logging_writer.py:48] [64] global_step=64, grad_norm=22.446063995361328, loss=8.061875343322754
I1009 19:44:23.235691 139587698222848 logging_writer.py:48] [65] global_step=65, grad_norm=22.553421020507812, loss=8.083650588989258
I1009 19:44:24.128281 139587706615552 logging_writer.py:48] [66] global_step=66, grad_norm=22.852523803710938, loss=8.124324798583984
I1009 19:44:25.022259 139587698222848 logging_writer.py:48] [67] global_step=67, grad_norm=23.042219161987305, loss=8.150873184204102
I1009 19:44:25.906084 139587706615552 logging_writer.py:48] [68] global_step=68, grad_norm=23.0660457611084, loss=8.151154518127441
I1009 19:44:26.795178 139587698222848 logging_writer.py:48] [69] global_step=69, grad_norm=23.211374282836914, loss=8.167304039001465
I1009 19:44:27.685566 139587706615552 logging_writer.py:48] [70] global_step=70, grad_norm=23.136714935302734, loss=8.149994850158691
I1009 19:44:28.569709 139587698222848 logging_writer.py:48] [71] global_step=71, grad_norm=23.25376319885254, loss=8.167997360229492
I1009 19:44:29.461897 139587706615552 logging_writer.py:48] [72] global_step=72, grad_norm=23.104249954223633, loss=8.12834644317627
I1009 19:44:30.350741 139587698222848 logging_writer.py:48] [73] global_step=73, grad_norm=22.974149703979492, loss=8.084511756896973
I1009 19:44:31.234359 139587706615552 logging_writer.py:48] [74] global_step=74, grad_norm=22.928876876831055, loss=8.066805839538574
I1009 19:44:32.123616 139587698222848 logging_writer.py:48] [75] global_step=75, grad_norm=22.985668182373047, loss=8.07533073425293
I1009 19:44:33.011216 139587706615552 logging_writer.py:48] [76] global_step=76, grad_norm=22.749622344970703, loss=8.012064933776855
I1009 19:44:33.899366 139587698222848 logging_writer.py:48] [77] global_step=77, grad_norm=22.60377311706543, loss=7.974489688873291
I1009 19:44:34.784607 139587706615552 logging_writer.py:48] [78] global_step=78, grad_norm=22.23288917541504, loss=7.922788143157959
I1009 19:44:35.676491 139587698222848 logging_writer.py:48] [79] global_step=79, grad_norm=22.15137481689453, loss=7.8888421058654785
I1009 19:44:36.572464 139587706615552 logging_writer.py:48] [80] global_step=80, grad_norm=21.633037567138672, loss=7.814693450927734
I1009 19:44:37.460569 139587698222848 logging_writer.py:48] [81] global_step=81, grad_norm=21.21094512939453, loss=7.740176677703857
I1009 19:44:38.348695 139587706615552 logging_writer.py:48] [82] global_step=82, grad_norm=20.793228149414062, loss=7.698308944702148
I1009 19:44:39.242792 139587698222848 logging_writer.py:48] [83] global_step=83, grad_norm=20.252717971801758, loss=7.620633602142334
I1009 19:44:40.133378 139587706615552 logging_writer.py:48] [84] global_step=84, grad_norm=19.611370086669922, loss=7.567265510559082
I1009 19:44:41.022062 139587698222848 logging_writer.py:48] [85] global_step=85, grad_norm=18.441503524780273, loss=7.477921962738037
I1009 19:44:41.904105 139587706615552 logging_writer.py:48] [86] global_step=86, grad_norm=17.822954177856445, loss=7.420832633972168
I1009 19:44:42.787960 139587698222848 logging_writer.py:48] [87] global_step=87, grad_norm=16.399980545043945, loss=7.342552661895752
I1009 19:44:43.670009 139587706615552 logging_writer.py:48] [88] global_step=88, grad_norm=15.329462051391602, loss=7.288557052612305
I1009 19:44:44.540082 139587698222848 logging_writer.py:48] [89] global_step=89, grad_norm=13.653482437133789, loss=7.216224670410156
I1009 19:44:45.435501 139587706615552 logging_writer.py:48] [90] global_step=90, grad_norm=12.49926471710205, loss=7.162189960479736
I1009 19:44:46.319467 139587698222848 logging_writer.py:48] [91] global_step=91, grad_norm=10.482043266296387, loss=7.1110992431640625
I1009 19:44:47.198494 139587706615552 logging_writer.py:48] [92] global_step=92, grad_norm=8.287718772888184, loss=7.057497501373291
I1009 19:44:48.088300 139587698222848 logging_writer.py:48] [93] global_step=93, grad_norm=5.794396877288818, loss=7.03636360168457
I1009 19:44:48.978475 139587706615552 logging_writer.py:48] [94] global_step=94, grad_norm=3.1553783416748047, loss=7.002211570739746
I1009 19:44:49.875077 139587698222848 logging_writer.py:48] [95] global_step=95, grad_norm=2.5774848461151123, loss=6.989558219909668
I1009 19:44:50.765248 139587706615552 logging_writer.py:48] [96] global_step=96, grad_norm=4.444161415100098, loss=6.999692440032959
I1009 19:44:51.647495 139587698222848 logging_writer.py:48] [97] global_step=97, grad_norm=6.824219226837158, loss=7.010051250457764
I1009 19:44:52.527395 139587706615552 logging_writer.py:48] [98] global_step=98, grad_norm=9.987106323242188, loss=7.051008701324463
I1009 19:44:53.403702 139587698222848 logging_writer.py:48] [99] global_step=99, grad_norm=13.375539779663086, loss=7.079440593719482
I1009 19:44:54.294241 139587706615552 logging_writer.py:48] [100] global_step=100, grad_norm=17.156301498413086, loss=7.130987644195557
I1009 19:49:58.214411 139587698222848 logging_writer.py:48] [500] global_step=500, grad_norm=0.29215559363365173, loss=5.801028251647949
I1009 19:56:47.067932 139587706615552 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6013674139976501, loss=5.778500556945801
I1009 20:03:11.649454 139587798935296 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.4873526096343994, loss=5.78906774520874
I1009 20:07:05.641990 139760355301184 spec.py:321] Evaluating on the training split.
I1009 20:07:38.866034 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 20:08:21.068058 139760355301184 spec.py:349] Evaluating on the test split.
I1009 20:08:43.726336 139760355301184 submission_runner.py:384] Time since start: 1759.16s, 	Step: 1787, 	{'train/ctc_loss': Array(6.8751073, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.96964, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.9392056, dtype=float32), 'test/wer': 0.8947815019092066, 'test/num_examples': 2472, 'score': 1515.4991357326508, 'total_duration': 1759.1583213806152, 'accumulated_submission_time': 1515.4991357326508, 'accumulated_eval_time': 243.58453154563904, 'accumulated_logging_time': 0.04083824157714844}
I1009 20:08:43.765739 139587798935296 logging_writer.py:48] [1787] accumulated_eval_time=243.584532, accumulated_logging_time=0.040838, accumulated_submission_time=1515.499136, global_step=1787, preemption_count=0, score=1515.499136, test/ctc_loss=6.939205646514893, test/num_examples=2472, test/wer=0.894782, total_duration=1759.158321, train/ctc_loss=6.875107288360596, train/wer=0.944636, validation/ctc_loss=6.969639778137207, validation/num_examples=5348, validation/wer=0.895995
I1009 20:11:26.703495 139587790542592 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8666008710861206, loss=5.2748494148254395
I1009 20:17:49.402415 139587471255296 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9130141735076904, loss=3.5331342220306396
I1009 20:24:37.714792 139587462862592 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0365029573440552, loss=2.980870485305786
I1009 20:31:05.704871 139587471255296 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7700471878051758, loss=2.689481258392334
I1009 20:32:44.844536 139760355301184 spec.py:321] Evaluating on the training split.
I1009 20:33:35.094480 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 20:34:24.219721 139760355301184 spec.py:349] Evaluating on the test split.
I1009 20:34:49.860848 139760355301184 submission_runner.py:384] Time since start: 3325.29s, 	Step: 3627, 	{'train/ctc_loss': Array(2.2803457, dtype=float32), 'train/wer': 0.5863942679061342, 'validation/ctc_loss': Array(2.642085, dtype=float32), 'validation/wer': 0.6013372053758358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.3293712, dtype=float32), 'test/wer': 0.5614683718205143, 'test/num_examples': 2472, 'score': 2956.530928373337, 'total_duration': 3325.2920405864716, 'accumulated_submission_time': 2956.530928373337, 'accumulated_eval_time': 368.5951874256134, 'accumulated_logging_time': 0.0946662425994873}
I1009 20:34:49.891874 139587179415296 logging_writer.py:48] [3627] accumulated_eval_time=368.595187, accumulated_logging_time=0.094666, accumulated_submission_time=2956.530928, global_step=3627, preemption_count=0, score=2956.530928, test/ctc_loss=2.329371213912964, test/num_examples=2472, test/wer=0.561468, total_duration=3325.292041, train/ctc_loss=2.2803456783294678, train/wer=0.586394, validation/ctc_loss=2.642085075378418, validation/num_examples=5348, validation/wer=0.601337
I1009 20:39:36.910534 139587171022592 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8175473213195801, loss=2.456461191177368
I1009 20:46:08.657799 139587179415296 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8848996162414551, loss=2.31358003616333
I1009 20:53:06.112697 139587171022592 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8876120448112488, loss=2.136181354522705
I1009 20:58:50.435536 139760355301184 spec.py:321] Evaluating on the training split.
I1009 20:59:41.177133 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 21:00:29.085825 139760355301184 spec.py:349] Evaluating on the test split.
I1009 21:00:54.249220 139760355301184 submission_runner.py:384] Time since start: 4889.68s, 	Step: 5437, 	{'train/ctc_loss': Array(0.7408186, dtype=float32), 'train/wer': 0.25559403513765677, 'validation/ctc_loss': Array(1.0931926, dtype=float32), 'validation/wer': 0.3201671024322473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8059633, dtype=float32), 'test/wer': 0.25926823847909974, 'test/num_examples': 2472, 'score': 4397.0271916389465, 'total_duration': 4889.680223941803, 'accumulated_submission_time': 4397.0271916389465, 'accumulated_eval_time': 492.40317273139954, 'accumulated_logging_time': 0.14118218421936035}
I1009 21:00:54.282085 139587179415296 logging_writer.py:48] [5437] accumulated_eval_time=492.403173, accumulated_logging_time=0.141182, accumulated_submission_time=4397.027192, global_step=5437, preemption_count=0, score=4397.027192, test/ctc_loss=0.8059632778167725, test/num_examples=2472, test/wer=0.259268, total_duration=4889.680224, train/ctc_loss=0.7408186197280884, train/wer=0.255594, validation/ctc_loss=1.0931925773620605, validation/num_examples=5348, validation/wer=0.320167
I1009 21:01:42.929759 139587171022592 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7335621118545532, loss=1.9673646688461304
I1009 21:08:23.878518 139587179415296 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6865450739860535, loss=1.9676958322525024
I1009 21:15:02.211207 139587179415296 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7931550145149231, loss=1.901939868927002
I1009 21:21:55.400696 139587171022592 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7956615090370178, loss=1.8446780443191528
I1009 21:24:56.821583 139760355301184 spec.py:321] Evaluating on the training split.
I1009 21:25:47.982849 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 21:26:35.696151 139760355301184 spec.py:349] Evaluating on the test split.
I1009 21:27:00.455635 139760355301184 submission_runner.py:384] Time since start: 6455.89s, 	Step: 7211, 	{'train/ctc_loss': Array(0.53395754, dtype=float32), 'train/wer': 0.18275927070133732, 'validation/ctc_loss': Array(0.8519093, dtype=float32), 'validation/wer': 0.25202365676465766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5807321, dtype=float32), 'test/wer': 0.1891225730852375, 'test/num_examples': 2472, 'score': 5839.5187838077545, 'total_duration': 6455.885906934738, 'accumulated_submission_time': 5839.5187838077545, 'accumulated_eval_time': 616.0307807922363, 'accumulated_logging_time': 0.1905350685119629}
I1009 21:27:00.491845 139587368855296 logging_writer.py:48] [7211] accumulated_eval_time=616.030781, accumulated_logging_time=0.190535, accumulated_submission_time=5839.518784, global_step=7211, preemption_count=0, score=5839.518784, test/ctc_loss=0.5807321071624756, test/num_examples=2472, test/wer=0.189123, total_duration=6455.885907, train/ctc_loss=0.5339575409889221, train/wer=0.182759, validation/ctc_loss=0.8519092798233032, validation/num_examples=5348, validation/wer=0.252024
I1009 21:30:39.000880 139587360462592 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7101803421974182, loss=1.8035928010940552
I1009 21:37:24.318034 139587368855296 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5965031385421753, loss=1.786064624786377
I1009 21:44:12.236431 139587368855296 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7453461289405823, loss=1.7093006372451782
I1009 21:50:58.965078 139587360462592 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8246639370918274, loss=1.7045239210128784
I1009 21:51:01.077415 139760355301184 spec.py:321] Evaluating on the training split.
I1009 21:51:51.220595 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 21:52:38.914986 139760355301184 spec.py:349] Evaluating on the test split.
I1009 21:53:04.148817 139760355301184 submission_runner.py:384] Time since start: 8019.58s, 	Step: 9004, 	{'train/ctc_loss': Array(0.48503104, dtype=float32), 'train/wer': 0.1693828959098915, 'validation/ctc_loss': Array(0.7920252, dtype=float32), 'validation/wer': 0.23427143532499106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5327625, dtype=float32), 'test/wer': 0.17708143927914824, 'test/num_examples': 2472, 'score': 7280.057913064957, 'total_duration': 8019.579313993454, 'accumulated_submission_time': 7280.057913064957, 'accumulated_eval_time': 739.0957989692688, 'accumulated_logging_time': 0.24090147018432617}
I1009 21:53:04.180961 139587798935296 logging_writer.py:48] [9004] accumulated_eval_time=739.095799, accumulated_logging_time=0.240901, accumulated_submission_time=7280.057913, global_step=9004, preemption_count=0, score=7280.057913, test/ctc_loss=0.5327625274658203, test/num_examples=2472, test/wer=0.177081, total_duration=8019.579314, train/ctc_loss=0.48503103852272034, train/wer=0.169383, validation/ctc_loss=0.7920252084732056, validation/num_examples=5348, validation/wer=0.234271
I1009 21:59:20.743096 139587798935296 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7625136375427246, loss=1.77421236038208
I1009 22:06:11.860436 139587790542592 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.659677267074585, loss=1.7409684658050537
I1009 22:13:02.310453 139587798935296 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6687895655632019, loss=1.6821177005767822
I1009 22:17:04.714655 139760355301184 spec.py:321] Evaluating on the training split.
I1009 22:17:55.520455 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 22:18:43.322866 139760355301184 spec.py:349] Evaluating on the test split.
I1009 22:19:09.232776 139760355301184 submission_runner.py:384] Time since start: 9584.66s, 	Step: 10813, 	{'train/ctc_loss': Array(0.456519, dtype=float32), 'train/wer': 0.1578705901809006, 'validation/ctc_loss': Array(0.73195475, dtype=float32), 'validation/wer': 0.2200021225482156, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4817248, dtype=float32), 'test/wer': 0.16225225771258864, 'test/num_examples': 2472, 'score': 8720.545222043991, 'total_duration': 9584.663981199265, 'accumulated_submission_time': 8720.545222043991, 'accumulated_eval_time': 863.6082570552826, 'accumulated_logging_time': 0.2878406047821045}
I1009 22:19:09.267401 139587798935296 logging_writer.py:48] [10813] accumulated_eval_time=863.608257, accumulated_logging_time=0.287841, accumulated_submission_time=8720.545222, global_step=10813, preemption_count=0, score=8720.545222, test/ctc_loss=0.4817247986793518, test/num_examples=2472, test/wer=0.162252, total_duration=9584.663981, train/ctc_loss=0.4565190076828003, train/wer=0.157871, validation/ctc_loss=0.7319547533988953, validation/num_examples=5348, validation/wer=0.220002
I1009 22:21:31.102594 139587790542592 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5919838547706604, loss=1.673750400543213
I1009 22:28:12.955135 139587798935296 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7483630180358887, loss=1.6309469938278198
I1009 22:34:57.468007 139587790542592 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6925957202911377, loss=1.6668323278427124
I1009 22:41:54.923601 139587798935296 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5776665806770325, loss=1.5910522937774658
I1009 22:43:09.901455 139760355301184 spec.py:321] Evaluating on the training split.
I1009 22:44:02.437047 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 22:44:51.244082 139760355301184 spec.py:349] Evaluating on the test split.
I1009 22:45:16.484438 139760355301184 submission_runner.py:384] Time since start: 11151.92s, 	Step: 12601, 	{'train/ctc_loss': Array(0.38347533, dtype=float32), 'train/wer': 0.13859464403462873, 'validation/ctc_loss': Array(0.6809297, dtype=float32), 'validation/wer': 0.20569421798570175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43111068, dtype=float32), 'test/wer': 0.14615027173364042, 'test/num_examples': 2472, 'score': 10161.133113861084, 'total_duration': 11151.915655851364, 'accumulated_submission_time': 10161.133113861084, 'accumulated_eval_time': 990.1856024265289, 'accumulated_logging_time': 0.3368568420410156}
I1009 22:45:16.518759 139587368855296 logging_writer.py:48] [12601] accumulated_eval_time=990.185602, accumulated_logging_time=0.336857, accumulated_submission_time=10161.133114, global_step=12601, preemption_count=0, score=10161.133114, test/ctc_loss=0.431110680103302, test/num_examples=2472, test/wer=0.146150, total_duration=11151.915656, train/ctc_loss=0.38347533345222473, train/wer=0.138595, validation/ctc_loss=0.6809297204017639, validation/num_examples=5348, validation/wer=0.205694
I1009 22:50:34.315409 139587360462592 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6395201683044434, loss=1.5507240295410156
I1009 22:57:42.544420 139587368855296 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7326058149337769, loss=1.5368852615356445
I1009 23:04:20.747019 139587360462592 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7000025510787964, loss=1.5538604259490967
I1009 23:09:16.511289 139760355301184 spec.py:321] Evaluating on the training split.
I1009 23:10:07.763648 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 23:10:56.272130 139760355301184 spec.py:349] Evaluating on the test split.
I1009 23:11:21.552832 139760355301184 submission_runner.py:384] Time since start: 12716.98s, 	Step: 14340, 	{'train/ctc_loss': Array(0.3263766, dtype=float32), 'train/wer': 0.11955589068878546, 'validation/ctc_loss': Array(0.63979626, dtype=float32), 'validation/wer': 0.1923704039595172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40513235, dtype=float32), 'test/wer': 0.13564458452027395, 'test/num_examples': 2472, 'score': 11601.08046913147, 'total_duration': 12716.984174966812, 'accumulated_submission_time': 11601.08046913147, 'accumulated_eval_time': 1115.2216124534607, 'accumulated_logging_time': 0.3843390941619873}
I1009 23:11:21.587848 139587368855296 logging_writer.py:48] [14340] accumulated_eval_time=1115.221612, accumulated_logging_time=0.384339, accumulated_submission_time=11601.080469, global_step=14340, preemption_count=0, score=11601.080469, test/ctc_loss=0.40513235330581665, test/num_examples=2472, test/wer=0.135645, total_duration=12716.984175, train/ctc_loss=0.3263765871524811, train/wer=0.119556, validation/ctc_loss=0.639796257019043, validation/num_examples=5348, validation/wer=0.192370
I1009 23:13:26.568362 139587368855296 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4915608763694763, loss=1.5328211784362793
I1009 23:20:07.062928 139587360462592 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.695825457572937, loss=1.5550265312194824
I1009 23:27:20.330222 139587368855296 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6310606598854065, loss=1.6336047649383545
I1009 23:33:48.699322 139587360462592 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5698983669281006, loss=1.477311134338379
I1009 23:35:21.756070 139760355301184 spec.py:321] Evaluating on the training split.
I1009 23:36:12.793809 139760355301184 spec.py:333] Evaluating on the validation split.
I1009 23:37:02.119414 139760355301184 spec.py:349] Evaluating on the test split.
I1009 23:37:27.749558 139760355301184 submission_runner.py:384] Time since start: 14283.18s, 	Step: 16108, 	{'train/ctc_loss': Array(0.2981369, dtype=float32), 'train/wer': 0.10746984875165586, 'validation/ctc_loss': Array(0.6117659, dtype=float32), 'validation/wer': 0.18166118341710966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.382525, dtype=float32), 'test/wer': 0.125482352465806, 'test/num_examples': 2472, 'score': 13041.200325965881, 'total_duration': 14283.1800096035, 'accumulated_submission_time': 13041.200325965881, 'accumulated_eval_time': 1241.2086775302887, 'accumulated_logging_time': 0.43555116653442383}
I1009 23:37:27.782374 139587798935296 logging_writer.py:48] [16108] accumulated_eval_time=1241.208678, accumulated_logging_time=0.435551, accumulated_submission_time=13041.200326, global_step=16108, preemption_count=0, score=13041.200326, test/ctc_loss=0.3825249969959259, test/num_examples=2472, test/wer=0.125482, total_duration=14283.180010, train/ctc_loss=0.2981368899345398, train/wer=0.107470, validation/ctc_loss=0.6117659211158752, validation/num_examples=5348, validation/wer=0.181661
I1009 23:42:42.016839 139587798935296 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6283095479011536, loss=1.5110900402069092
I1009 23:49:12.471528 139587790542592 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6327764987945557, loss=1.5178884267807007
I1009 23:56:23.273276 139587798935296 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7121268510818481, loss=1.554210901260376
I1010 00:01:28.181423 139760355301184 spec.py:321] Evaluating on the training split.
I1010 00:02:19.119239 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 00:03:08.309067 139760355301184 spec.py:349] Evaluating on the test split.
I1010 00:03:33.767822 139760355301184 submission_runner.py:384] Time since start: 15849.20s, 	Step: 17899, 	{'train/ctc_loss': Array(0.2859081, dtype=float32), 'train/wer': 0.1062708146749827, 'validation/ctc_loss': Array(0.5785946, dtype=float32), 'validation/wer': 0.17298767957240302, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35436678, dtype=float32), 'test/wer': 0.11901731418065742, 'test/num_examples': 2472, 'score': 14481.553364992142, 'total_duration': 15849.19946527481, 'accumulated_submission_time': 14481.553364992142, 'accumulated_eval_time': 1366.7899730205536, 'accumulated_logging_time': 0.481656551361084}
I1010 00:03:33.806566 139587798935296 logging_writer.py:48] [17899] accumulated_eval_time=1366.789973, accumulated_logging_time=0.481657, accumulated_submission_time=14481.553365, global_step=17899, preemption_count=0, score=14481.553365, test/ctc_loss=0.3543667793273926, test/num_examples=2472, test/wer=0.119017, total_duration=15849.199465, train/ctc_loss=0.2859081029891968, train/wer=0.106271, validation/ctc_loss=0.5785946249961853, validation/num_examples=5348, validation/wer=0.172988
I1010 00:04:50.684135 139587790542592 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6808585524559021, loss=1.4900102615356445
I1010 00:11:46.693098 139587798935296 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.745883584022522, loss=1.483574628829956
I1010 00:18:21.754541 139587798935296 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.651428759098053, loss=1.4448580741882324
I1010 00:25:40.055749 139587790542592 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5924581289291382, loss=1.404197335243225
I1010 00:27:34.031230 139760355301184 spec.py:321] Evaluating on the training split.
I1010 00:28:25.934937 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 00:29:14.659060 139760355301184 spec.py:349] Evaluating on the test split.
I1010 00:29:40.495959 139760355301184 submission_runner.py:384] Time since start: 17415.93s, 	Step: 19637, 	{'train/ctc_loss': Array(0.29576388, dtype=float32), 'train/wer': 0.10601020650995531, 'validation/ctc_loss': Array(0.569922, dtype=float32), 'validation/wer': 0.17127034510704398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34808585, dtype=float32), 'test/wer': 0.11606764046305837, 'test/num_examples': 2472, 'score': 15921.729849100113, 'total_duration': 17415.92658495903, 'accumulated_submission_time': 15921.729849100113, 'accumulated_eval_time': 1493.2486491203308, 'accumulated_logging_time': 0.5371494293212891}
I1010 00:29:40.530041 139587368855296 logging_writer.py:48] [19637] accumulated_eval_time=1493.248649, accumulated_logging_time=0.537149, accumulated_submission_time=15921.729849, global_step=19637, preemption_count=0, score=15921.729849, test/ctc_loss=0.34808585047721863, test/num_examples=2472, test/wer=0.116068, total_duration=17415.926585, train/ctc_loss=0.29576388001441956, train/wer=0.106010, validation/ctc_loss=0.5699219703674316, validation/num_examples=5348, validation/wer=0.171270
I1010 00:34:15.114388 139587360462592 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6733497977256775, loss=1.4036564826965332
I1010 00:41:28.410775 139587368855296 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7906008362770081, loss=1.4167286157608032
I1010 00:48:05.356923 139587368855296 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8708543181419373, loss=1.411243200302124
I1010 00:53:40.545786 139760355301184 spec.py:321] Evaluating on the training split.
I1010 00:54:32.112871 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 00:55:21.372919 139760355301184 spec.py:349] Evaluating on the test split.
I1010 00:55:46.492938 139760355301184 submission_runner.py:384] Time since start: 18981.92s, 	Step: 21386, 	{'train/ctc_loss': Array(0.2875218, dtype=float32), 'train/wer': 0.10301527422375814, 'validation/ctc_loss': Array(0.54769206, dtype=float32), 'validation/wer': 0.16500882787098767, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33853814, dtype=float32), 'test/wer': 0.11144109744024891, 'test/num_examples': 2472, 'score': 17361.699501276016, 'total_duration': 18981.923527240753, 'accumulated_submission_time': 17361.699501276016, 'accumulated_eval_time': 1619.1895277500153, 'accumulated_logging_time': 0.5851902961730957}
I1010 00:55:46.527813 139587368855296 logging_writer.py:48] [21386] accumulated_eval_time=1619.189528, accumulated_logging_time=0.585190, accumulated_submission_time=17361.699501, global_step=21386, preemption_count=0, score=17361.699501, test/ctc_loss=0.33853814005851746, test/num_examples=2472, test/wer=0.111441, total_duration=18981.923527, train/ctc_loss=0.2875218093395233, train/wer=0.103015, validation/ctc_loss=0.547692060470581, validation/num_examples=5348, validation/wer=0.165009
I1010 00:57:13.435689 139587360462592 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5874432325363159, loss=1.392793893814087
I1010 01:03:38.124405 139587368855296 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6444135308265686, loss=1.5188531875610352
I1010 01:10:47.108299 139587360462592 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.755240797996521, loss=1.4267793893814087
I1010 01:17:30.866776 139587368855296 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5915274024009705, loss=1.3907673358917236
I1010 01:19:47.197360 139760355301184 spec.py:321] Evaluating on the training split.
I1010 01:20:39.980775 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 01:21:29.402539 139760355301184 spec.py:349] Evaluating on the test split.
I1010 01:21:54.997571 139760355301184 submission_runner.py:384] Time since start: 20550.43s, 	Step: 23163, 	{'train/ctc_loss': Array(0.26769987, dtype=float32), 'train/wer': 0.09815310217100524, 'validation/ctc_loss': Array(0.5341378, dtype=float32), 'validation/wer': 0.16004013545716794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32812852, dtype=float32), 'test/wer': 0.10790552962805827, 'test/num_examples': 2472, 'score': 18802.321515083313, 'total_duration': 20550.428798913956, 'accumulated_submission_time': 18802.321515083313, 'accumulated_eval_time': 1746.9840960502625, 'accumulated_logging_time': 0.6353154182434082}
I1010 01:21:55.029632 139587798935296 logging_writer.py:48] [23163] accumulated_eval_time=1746.984096, accumulated_logging_time=0.635315, accumulated_submission_time=18802.321515, global_step=23163, preemption_count=0, score=18802.321515, test/ctc_loss=0.32812851667404175, test/num_examples=2472, test/wer=0.107906, total_duration=20550.428799, train/ctc_loss=0.26769986748695374, train/wer=0.098153, validation/ctc_loss=0.5341377854347229, validation/num_examples=5348, validation/wer=0.160040
I1010 01:26:23.446816 139587790542592 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6580992937088013, loss=1.3843042850494385
I1010 01:33:09.442632 139587798935296 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6060362458229065, loss=1.391289472579956
I1010 01:40:18.386574 139587790542592 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7048090100288391, loss=1.3771870136260986
I1010 01:45:55.349873 139760355301184 spec.py:321] Evaluating on the training split.
I1010 01:46:46.656589 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 01:47:35.719572 139760355301184 spec.py:349] Evaluating on the test split.
I1010 01:48:01.325693 139760355301184 submission_runner.py:384] Time since start: 22116.76s, 	Step: 24910, 	{'train/ctc_loss': Array(0.24160087, dtype=float32), 'train/wer': 0.08819306617119209, 'validation/ctc_loss': Array(0.52083457, dtype=float32), 'validation/wer': 0.15511003482908664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31390664, dtype=float32), 'test/wer': 0.10376386447663495, 'test/num_examples': 2472, 'score': 20242.595469474792, 'total_duration': 22116.756570339203, 'accumulated_submission_time': 20242.595469474792, 'accumulated_eval_time': 1872.9540824890137, 'accumulated_logging_time': 0.6821458339691162}
I1010 01:48:01.366742 139587798935296 logging_writer.py:48] [24910] accumulated_eval_time=1872.954082, accumulated_logging_time=0.682146, accumulated_submission_time=20242.595469, global_step=24910, preemption_count=0, score=20242.595469, test/ctc_loss=0.31390663981437683, test/num_examples=2472, test/wer=0.103764, total_duration=22116.756570, train/ctc_loss=0.2416008710861206, train/wer=0.088193, validation/ctc_loss=0.5208345651626587, validation/num_examples=5348, validation/wer=0.155110
I1010 01:49:10.542537 139587790542592 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5668407678604126, loss=1.3657209873199463
I1010 01:56:00.776980 139587798935296 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5728620290756226, loss=1.3687251806259155
I1010 02:02:53.979123 139587798935296 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6279740929603577, loss=1.3484116792678833
I1010 02:10:03.148999 139587790542592 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6483972072601318, loss=1.3797833919525146
I1010 02:12:01.582600 139760355301184 spec.py:321] Evaluating on the training split.
I1010 02:12:53.187582 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 02:13:43.062967 139760355301184 spec.py:349] Evaluating on the test split.
I1010 02:14:08.869507 139760355301184 submission_runner.py:384] Time since start: 23684.30s, 	Step: 26634, 	{'train/ctc_loss': Array(0.22323658, dtype=float32), 'train/wer': 0.0842588486919325, 'validation/ctc_loss': Array(0.4935157, dtype=float32), 'validation/wer': 0.14816351339617362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2964971, dtype=float32), 'test/wer': 0.10146069458755076, 'test/num_examples': 2472, 'score': 21682.764047145844, 'total_duration': 23684.300047159195, 'accumulated_submission_time': 21682.764047145844, 'accumulated_eval_time': 2000.2347030639648, 'accumulated_logging_time': 0.7379710674285889}
I1010 02:14:08.906663 139587798935296 logging_writer.py:48] [26634] accumulated_eval_time=2000.234703, accumulated_logging_time=0.737971, accumulated_submission_time=21682.764047, global_step=26634, preemption_count=0, score=21682.764047, test/ctc_loss=0.296497106552124, test/num_examples=2472, test/wer=0.101461, total_duration=23684.300047, train/ctc_loss=0.2232365757226944, train/wer=0.084259, validation/ctc_loss=0.4935157001018524, validation/num_examples=5348, validation/wer=0.148164
I1010 02:18:49.047737 139587798935296 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6915736794471741, loss=1.326866626739502
I1010 02:25:57.262464 139587790542592 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7102923393249512, loss=1.3839130401611328
I1010 02:32:56.840232 139587798935296 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5881720185279846, loss=1.3350635766983032
I1010 02:38:09.559504 139760355301184 spec.py:321] Evaluating on the training split.
I1010 02:39:02.788064 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 02:39:52.548903 139760355301184 spec.py:349] Evaluating on the test split.
I1010 02:40:18.506884 139760355301184 submission_runner.py:384] Time since start: 25253.94s, 	Step: 28386, 	{'train/ctc_loss': Array(0.21349154, dtype=float32), 'train/wer': 0.07821849014533962, 'validation/ctc_loss': Array(0.48505414, dtype=float32), 'validation/wer': 0.14503757875136278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28211427, dtype=float32), 'test/wer': 0.09503606279168435, 'test/num_examples': 2472, 'score': 23123.37104678154, 'total_duration': 25253.93640947342, 'accumulated_submission_time': 23123.37104678154, 'accumulated_eval_time': 2129.174768924713, 'accumulated_logging_time': 0.7888081073760986}
I1010 02:40:18.541490 139587798935296 logging_writer.py:48] [28386] accumulated_eval_time=2129.174769, accumulated_logging_time=0.788808, accumulated_submission_time=23123.371047, global_step=28386, preemption_count=0, score=23123.371047, test/ctc_loss=0.28211426734924316, test/num_examples=2472, test/wer=0.095036, total_duration=25253.936409, train/ctc_loss=0.2134915441274643, train/wer=0.078218, validation/ctc_loss=0.4850541353225708, validation/num_examples=5348, validation/wer=0.145038
I1010 02:41:45.205703 139587790542592 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7803963422775269, loss=1.3551841974258423
I1010 02:48:32.112307 139587798935296 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6672800779342651, loss=1.2740976810455322
I1010 02:55:24.608844 139587790542592 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.654599130153656, loss=1.3035961389541626
I1010 03:02:29.048054 139587798935296 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6248410940170288, loss=1.3268373012542725
I1010 03:04:18.552760 139760355301184 spec.py:321] Evaluating on the training split.
I1010 03:05:09.238755 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 03:05:58.288757 139760355301184 spec.py:349] Evaluating on the test split.
I1010 03:06:23.611839 139760355301184 submission_runner.py:384] Time since start: 26819.04s, 	Step: 30147, 	{'train/ctc_loss': Array(0.22487819, dtype=float32), 'train/wer': 0.08162159819726122, 'validation/ctc_loss': Array(0.4816879, dtype=float32), 'validation/wer': 0.14194058794585573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27987337, dtype=float32), 'test/wer': 0.09242984423298382, 'test/num_examples': 2472, 'score': 24563.335582256317, 'total_duration': 26819.04343867302, 'accumulated_submission_time': 24563.335582256317, 'accumulated_eval_time': 2254.228602409363, 'accumulated_logging_time': 0.8377974033355713}
I1010 03:06:23.650294 139587798935296 logging_writer.py:48] [30147] accumulated_eval_time=2254.228602, accumulated_logging_time=0.837797, accumulated_submission_time=24563.335582, global_step=30147, preemption_count=0, score=24563.335582, test/ctc_loss=0.2798733711242676, test/num_examples=2472, test/wer=0.092430, total_duration=26819.043439, train/ctc_loss=0.224878191947937, train/wer=0.081622, validation/ctc_loss=0.48168790340423584, validation/num_examples=5348, validation/wer=0.141941
I1010 03:11:01.371568 139587790542592 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6445277333259583, loss=1.3238105773925781
I1010 03:18:11.777683 139587798935296 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6724015474319458, loss=1.3102326393127441
I1010 03:24:59.656358 139587790542592 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.5961841940879822, loss=1.3137527704238892
I1010 03:30:24.306211 139760355301184 spec.py:321] Evaluating on the training split.
I1010 03:31:14.853148 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 03:32:03.798690 139760355301184 spec.py:349] Evaluating on the test split.
I1010 03:32:29.702786 139760355301184 submission_runner.py:384] Time since start: 28385.13s, 	Step: 31869, 	{'train/ctc_loss': Array(0.2107978, dtype=float32), 'train/wer': 0.0764681305921356, 'validation/ctc_loss': Array(0.46005985, dtype=float32), 'validation/wer': 0.13796563401479994, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2707439, dtype=float32), 'test/wer': 0.09059134897064469, 'test/num_examples': 2472, 'score': 26003.94373512268, 'total_duration': 28385.133793592453, 'accumulated_submission_time': 26003.94373512268, 'accumulated_eval_time': 2379.619324207306, 'accumulated_logging_time': 0.8926985263824463}
I1010 03:32:29.740806 139587368855296 logging_writer.py:48] [31869] accumulated_eval_time=2379.619324, accumulated_logging_time=0.892699, accumulated_submission_time=26003.943735, global_step=31869, preemption_count=0, score=26003.943735, test/ctc_loss=0.2707439064979553, test/num_examples=2472, test/wer=0.090591, total_duration=28385.133794, train/ctc_loss=0.21079780161380768, train/wer=0.076468, validation/ctc_loss=0.46005985140800476, validation/num_examples=5348, validation/wer=0.137966
I1010 03:34:14.223603 139587368855296 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.0046268701553345, loss=1.277488112449646
I1010 03:41:02.367930 139587360462592 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.2782310247421265, loss=1.2663166522979736
I1010 03:48:27.721778 139587368855296 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7996393442153931, loss=1.2841993570327759
I1010 03:55:09.688091 139587360462592 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6439414024353027, loss=1.2953083515167236
I1010 03:56:30.195998 139760355301184 spec.py:321] Evaluating on the training split.
I1010 03:57:22.740583 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 03:58:12.290616 139760355301184 spec.py:349] Evaluating on the test split.
I1010 03:58:38.308770 139760355301184 submission_runner.py:384] Time since start: 29953.74s, 	Step: 33593, 	{'train/ctc_loss': Array(0.21789105, dtype=float32), 'train/wer': 0.07734153871443322, 'validation/ctc_loss': Array(0.45696384, dtype=float32), 'validation/wer': 0.134116103387394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26811406, dtype=float32), 'test/wer': 0.08521728589611492, 'test/num_examples': 2472, 'score': 27444.353387355804, 'total_duration': 29953.740344285965, 'accumulated_submission_time': 27444.353387355804, 'accumulated_eval_time': 2507.7268373966217, 'accumulated_logging_time': 0.9440710544586182}
I1010 03:58:38.344792 139587368855296 logging_writer.py:48] [33593] accumulated_eval_time=2507.726837, accumulated_logging_time=0.944071, accumulated_submission_time=27444.353387, global_step=33593, preemption_count=0, score=27444.353387, test/ctc_loss=0.2681140601634979, test/num_examples=2472, test/wer=0.085217, total_duration=29953.740344, train/ctc_loss=0.21789105236530304, train/wer=0.077342, validation/ctc_loss=0.45696383714675903, validation/num_examples=5348, validation/wer=0.134116
I1010 04:04:07.609868 139587368855296 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5631744265556335, loss=1.261694073677063
I1010 04:10:42.602791 139587360462592 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7315478324890137, loss=1.244849443435669
I1010 04:18:03.187873 139587368855296 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7151952385902405, loss=1.2511323690414429
I1010 04:22:38.913707 139760355301184 spec.py:321] Evaluating on the training split.
I1010 04:23:31.236287 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 04:24:21.027592 139760355301184 spec.py:349] Evaluating on the test split.
I1010 04:24:47.357893 139760355301184 submission_runner.py:384] Time since start: 31522.79s, 	Step: 35355, 	{'train/ctc_loss': Array(0.19773735, dtype=float32), 'train/wer': 0.06897599028101921, 'validation/ctc_loss': Array(0.434604, dtype=float32), 'validation/wer': 0.12980347133112716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2498267, dtype=float32), 'test/wer': 0.08121704345717923, 'test/num_examples': 2472, 'score': 28884.875313043594, 'total_duration': 31522.789919614792, 'accumulated_submission_time': 28884.875313043594, 'accumulated_eval_time': 2636.166344642639, 'accumulated_logging_time': 0.9943187236785889}
I1010 04:24:47.392419 139587798935296 logging_writer.py:48] [35355] accumulated_eval_time=2636.166345, accumulated_logging_time=0.994319, accumulated_submission_time=28884.875313, global_step=35355, preemption_count=0, score=28884.875313, test/ctc_loss=0.24982669949531555, test/num_examples=2472, test/wer=0.081217, total_duration=31522.789920, train/ctc_loss=0.19773735105991364, train/wer=0.068976, validation/ctc_loss=0.4346039891242981, validation/num_examples=5348, validation/wer=0.129803
I1010 04:26:38.387618 139587790542592 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8034375905990601, loss=1.2045916318893433
I1010 04:33:47.473624 139587798935296 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6979992389678955, loss=1.2827494144439697
I1010 04:40:23.968633 139587798935296 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6916831135749817, loss=1.245280146598816
I1010 04:47:52.450843 139587790542592 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8325228095054626, loss=1.24461030960083
I1010 04:48:47.680397 139760355301184 spec.py:321] Evaluating on the training split.
I1010 04:49:39.569242 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 04:50:29.342144 139760355301184 spec.py:349] Evaluating on the test split.
I1010 04:50:55.090188 139760355301184 submission_runner.py:384] Time since start: 33090.52s, 	Step: 37065, 	{'train/ctc_loss': Array(0.15390655, dtype=float32), 'train/wer': 0.057491057060565995, 'validation/ctc_loss': Array(0.4286001, dtype=float32), 'validation/wer': 0.12687049561500835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24446504, dtype=float32), 'test/wer': 0.080711962341152, 'test/num_examples': 2472, 'score': 30325.11698436737, 'total_duration': 33090.52071261406, 'accumulated_submission_time': 30325.11698436737, 'accumulated_eval_time': 2763.5697836875916, 'accumulated_logging_time': 1.0425512790679932}
I1010 04:50:55.122763 139587368855296 logging_writer.py:48] [37065] accumulated_eval_time=2763.569784, accumulated_logging_time=1.042551, accumulated_submission_time=30325.116984, global_step=37065, preemption_count=0, score=30325.116984, test/ctc_loss=0.24446503818035126, test/num_examples=2472, test/wer=0.080712, total_duration=33090.520713, train/ctc_loss=0.15390655398368835, train/wer=0.057491, validation/ctc_loss=0.42860010266304016, validation/num_examples=5348, validation/wer=0.126870
I1010 04:56:41.191342 139587368855296 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.730816125869751, loss=1.2037357091903687
I1010 05:04:10.882394 139587360462592 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6567625403404236, loss=1.2192758321762085
I1010 05:10:52.986068 139587368855296 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.838162362575531, loss=1.221508264541626
I1010 05:14:55.396470 139760355301184 spec.py:321] Evaluating on the training split.
I1010 05:15:47.914101 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 05:16:37.927764 139760355301184 spec.py:349] Evaluating on the test split.
I1010 05:17:04.876579 139760355301184 submission_runner.py:384] Time since start: 34660.31s, 	Step: 38780, 	{'train/ctc_loss': Array(0.1652866, dtype=float32), 'train/wer': 0.05985090072015375, 'validation/ctc_loss': Array(0.4101026, dtype=float32), 'validation/wer': 0.12054144275390984, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23171791, dtype=float32), 'test/wer': 0.07648948421116432, 'test/num_examples': 2472, 'score': 31765.343761205673, 'total_duration': 34660.3071846962, 'accumulated_submission_time': 31765.343761205673, 'accumulated_eval_time': 2893.043671607971, 'accumulated_logging_time': 1.089216709136963}
I1010 05:17:04.912842 139587368855296 logging_writer.py:48] [38780] accumulated_eval_time=2893.043672, accumulated_logging_time=1.089217, accumulated_submission_time=31765.343761, global_step=38780, preemption_count=0, score=31765.343761, test/ctc_loss=0.23171791434288025, test/num_examples=2472, test/wer=0.076489, total_duration=34660.307185, train/ctc_loss=0.1652866005897522, train/wer=0.059851, validation/ctc_loss=0.41010260581970215, validation/num_examples=5348, validation/wer=0.120541
I1010 05:19:51.212249 139587360462592 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.9728670716285706, loss=1.2528667449951172
I1010 05:26:29.368809 139587368855296 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7755697965621948, loss=1.1626427173614502
I1010 05:33:43.705814 139587360462592 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8843538761138916, loss=1.2000871896743774
I1010 05:40:22.554405 139587368855296 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7559489607810974, loss=1.139120101928711
I1010 05:41:04.954694 139760355301184 spec.py:321] Evaluating on the training split.
I1010 05:41:56.424208 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 05:42:45.890657 139760355301184 spec.py:349] Evaluating on the test split.
I1010 05:43:11.807311 139760355301184 submission_runner.py:384] Time since start: 36227.24s, 	Step: 40556, 	{'train/ctc_loss': Array(0.20123275, dtype=float32), 'train/wer': 0.07268269663652685, 'validation/ctc_loss': Array(0.39494124, dtype=float32), 'validation/wer': 0.11778213007361384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22860794, dtype=float32), 'test/wer': 0.07491363112915934, 'test/num_examples': 2472, 'score': 33205.33744955063, 'total_duration': 36227.239282369614, 'accumulated_submission_time': 33205.33744955063, 'accumulated_eval_time': 3019.8914070129395, 'accumulated_logging_time': 1.1400611400604248}
I1010 05:43:11.844412 139587798935296 logging_writer.py:48] [40556] accumulated_eval_time=3019.891407, accumulated_logging_time=1.140061, accumulated_submission_time=33205.337450, global_step=40556, preemption_count=0, score=33205.337450, test/ctc_loss=0.2286079376935959, test/num_examples=2472, test/wer=0.074914, total_duration=36227.239282, train/ctc_loss=0.20123274624347687, train/wer=0.072683, validation/ctc_loss=0.3949412405490875, validation/num_examples=5348, validation/wer=0.117782
I1010 05:49:12.150401 139587790542592 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7137651443481445, loss=1.164015293121338
I1010 05:55:57.853925 139587798935296 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.0242081880569458, loss=1.1541208028793335
I1010 06:03:16.227074 139587790542592 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7616963982582092, loss=1.1764631271362305
I1010 06:07:11.998166 139760355301184 spec.py:321] Evaluating on the training split.
I1010 06:08:02.234640 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 06:08:52.595561 139760355301184 spec.py:349] Evaluating on the test split.
I1010 06:09:18.876555 139760355301184 submission_runner.py:384] Time since start: 37794.31s, 	Step: 42266, 	{'train/ctc_loss': Array(0.19825664, dtype=float32), 'train/wer': 0.07227832149974742, 'validation/ctc_loss': Array(0.39015985, dtype=float32), 'validation/wer': 0.11504211328618703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21761815, dtype=float32), 'test/wer': 0.07186294118835485, 'test/num_examples': 2472, 'score': 34645.44293189049, 'total_duration': 37794.30714058876, 'accumulated_submission_time': 34645.44293189049, 'accumulated_eval_time': 3146.7637164592743, 'accumulated_logging_time': 1.192563533782959}
I1010 06:09:18.911494 139587798935296 logging_writer.py:48] [42266] accumulated_eval_time=3146.763716, accumulated_logging_time=1.192564, accumulated_submission_time=34645.442932, global_step=42266, preemption_count=0, score=34645.442932, test/ctc_loss=0.2176181524991989, test/num_examples=2472, test/wer=0.071863, total_duration=37794.307141, train/ctc_loss=0.19825664162635803, train/wer=0.072278, validation/ctc_loss=0.39015984535217285, validation/num_examples=5348, validation/wer=0.115042
I1010 06:12:16.228630 139587790542592 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8494443297386169, loss=1.1179982423782349
I1010 06:19:27.230365 139587798935296 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7891887426376343, loss=1.1572178602218628
I1010 06:26:28.055637 139587798935296 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8524406552314758, loss=1.09342360496521
I1010 06:33:19.408217 139760355301184 spec.py:321] Evaluating on the training split.
I1010 06:34:08.373531 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 06:34:58.304190 139760355301184 spec.py:349] Evaluating on the test split.
I1010 06:35:24.367746 139760355301184 submission_runner.py:384] Time since start: 39359.80s, 	Step: 43980, 	{'train/ctc_loss': Array(0.21884592, dtype=float32), 'train/wer': 0.080467502383724, 'validation/ctc_loss': Array(0.3740899, dtype=float32), 'validation/wer': 0.11006377292593271, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20820065, dtype=float32), 'test/wer': 0.06897387720467907, 'test/num_examples': 2472, 'score': 36085.892963171005, 'total_duration': 39359.79950046539, 'accumulated_submission_time': 36085.892963171005, 'accumulated_eval_time': 3271.7181429862976, 'accumulated_logging_time': 1.241924524307251}
I1010 06:35:24.403573 139587368855296 logging_writer.py:48] [43980] accumulated_eval_time=3271.718143, accumulated_logging_time=1.241925, accumulated_submission_time=36085.892963, global_step=43980, preemption_count=0, score=36085.892963, test/ctc_loss=0.20820064842700958, test/num_examples=2472, test/wer=0.068974, total_duration=39359.799500, train/ctc_loss=0.2188459187746048, train/wer=0.080468, validation/ctc_loss=0.37408989667892456, validation/num_examples=5348, validation/wer=0.110064
I1010 06:35:40.275325 139587360462592 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0252007246017456, loss=1.1134538650512695
I1010 06:42:14.186315 139587368855296 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1819859743118286, loss=1.1470963954925537
I1010 06:49:23.336524 139587360462592 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6598491668701172, loss=1.0678993463516235
I1010 06:56:27.456880 139587368855296 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8011051416397095, loss=1.0848184823989868
I1010 06:59:24.715129 139760355301184 spec.py:321] Evaluating on the training split.
I1010 07:00:13.245211 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 07:01:02.589822 139760355301184 spec.py:349] Evaluating on the test split.
I1010 07:01:28.434478 139760355301184 submission_runner.py:384] Time since start: 40923.87s, 	Step: 45721, 	{'train/ctc_loss': Array(0.18242821, dtype=float32), 'train/wer': 0.06503423629223605, 'validation/ctc_loss': Array(0.36401728, dtype=float32), 'validation/wer': 0.10469951470829433, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19969043, dtype=float32), 'test/wer': 0.065115057478231, 'test/num_examples': 2472, 'score': 37526.15644979477, 'total_duration': 40923.86538338661, 'accumulated_submission_time': 37526.15644979477, 'accumulated_eval_time': 3395.431552886963, 'accumulated_logging_time': 1.293529748916626}
I1010 07:01:28.475907 139587798935296 logging_writer.py:48] [45721] accumulated_eval_time=3395.431553, accumulated_logging_time=1.293530, accumulated_submission_time=37526.156450, global_step=45721, preemption_count=0, score=37526.156450, test/ctc_loss=0.19969043135643005, test/num_examples=2472, test/wer=0.065115, total_duration=40923.865383, train/ctc_loss=0.18242821097373962, train/wer=0.065034, validation/ctc_loss=0.3640172779560089, validation/num_examples=5348, validation/wer=0.104700
I1010 07:05:11.299640 139587790542592 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.737582802772522, loss=1.120582103729248
I1010 07:12:22.757694 139587798935296 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8807176947593689, loss=1.110045313835144
I1010 07:19:19.798148 139587790542592 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6883190274238586, loss=1.1054749488830566
I1010 07:25:28.862133 139760355301184 spec.py:321] Evaluating on the training split.
I1010 07:26:19.644934 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 07:27:09.310353 139760355301184 spec.py:349] Evaluating on the test split.
I1010 07:27:35.709089 139760355301184 submission_runner.py:384] Time since start: 42491.14s, 	Step: 47418, 	{'train/ctc_loss': Array(0.1607592, dtype=float32), 'train/wer': 0.06036431251600548, 'validation/ctc_loss': Array(0.35039896, dtype=float32), 'validation/wer': 0.10160252390278729, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19369689, dtype=float32), 'test/wer': 0.06442814716043396, 'test/num_examples': 2472, 'score': 38966.495310783386, 'total_duration': 42491.13949394226, 'accumulated_submission_time': 38966.495310783386, 'accumulated_eval_time': 3522.272211790085, 'accumulated_logging_time': 1.3507068157196045}
I1010 07:27:35.749276 139587798935296 logging_writer.py:48] [47418] accumulated_eval_time=3522.272212, accumulated_logging_time=1.350707, accumulated_submission_time=38966.495311, global_step=47418, preemption_count=0, score=38966.495311, test/ctc_loss=0.19369688630104065, test/num_examples=2472, test/wer=0.064428, total_duration=42491.139494, train/ctc_loss=0.16075919568538666, train/wer=0.060364, validation/ctc_loss=0.3503989577293396, validation/num_examples=5348, validation/wer=0.101603
I1010 07:28:38.168637 139587790542592 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7365192174911499, loss=1.0135743618011475
I1010 07:35:19.012389 139587798935296 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7099730372428894, loss=1.0842431783676147
I1010 07:42:38.661084 139587798935296 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0405412912368774, loss=0.9962687492370605
I1010 07:49:30.732909 139587790542592 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.7209764719009399, loss=1.0027095079421997
I1010 07:51:35.852959 139760355301184 spec.py:321] Evaluating on the training split.
I1010 07:52:27.608080 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 07:53:16.986444 139760355301184 spec.py:349] Evaluating on the test split.
I1010 07:53:42.778528 139760355301184 submission_runner.py:384] Time since start: 44058.21s, 	Step: 49139, 	{'train/ctc_loss': Array(0.12839854, dtype=float32), 'train/wer': 0.04873897486650208, 'validation/ctc_loss': Array(0.33969137, dtype=float32), 'validation/wer': 0.09855377282945325, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18558753, dtype=float32), 'test/wer': 0.06194314806957998, 'test/num_examples': 2472, 'score': 40406.55139708519, 'total_duration': 44058.21048665047, 'accumulated_submission_time': 40406.55139708519, 'accumulated_eval_time': 3649.1928713321686, 'accumulated_logging_time': 1.4066059589385986}
I1010 07:53:42.815433 139587798935296 logging_writer.py:48] [49139] accumulated_eval_time=3649.192871, accumulated_logging_time=1.406606, accumulated_submission_time=40406.551397, global_step=49139, preemption_count=0, score=40406.551397, test/ctc_loss=0.18558752536773682, test/num_examples=2472, test/wer=0.061943, total_duration=44058.210487, train/ctc_loss=0.12839853763580322, train/wer=0.048739, validation/ctc_loss=0.3396913707256317, validation/num_examples=5348, validation/wer=0.098554
I1010 07:58:28.466016 139587798935296 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.072158932685852, loss=0.9809833765029907
I1010 08:05:15.356154 139587790542592 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.6156100034713745, loss=1.0412226915359497
I1010 08:12:52.809498 139587798935296 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7309700846672058, loss=0.9851619601249695
I1010 08:17:43.773396 139760355301184 spec.py:321] Evaluating on the training split.
I1010 08:18:34.202733 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 08:19:24.290860 139760355301184 spec.py:349] Evaluating on the test split.
I1010 08:19:50.603912 139760355301184 submission_runner.py:384] Time since start: 45626.03s, 	Step: 50871, 	{'train/ctc_loss': Array(0.13819452, dtype=float32), 'train/wer': 0.05127439213151792, 'validation/ctc_loss': Array(0.33163464, dtype=float32), 'validation/wer': 0.09527347104168878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17842153, dtype=float32), 'test/wer': 0.057902499141362106, 'test/num_examples': 2472, 'score': 41847.462961912155, 'total_duration': 45626.0348110199, 'accumulated_submission_time': 41847.462961912155, 'accumulated_eval_time': 3776.0174107551575, 'accumulated_logging_time': 1.457076072692871}
I1010 08:19:50.642726 139587798935296 logging_writer.py:48] [50871] accumulated_eval_time=3776.017411, accumulated_logging_time=1.457076, accumulated_submission_time=41847.462962, global_step=50871, preemption_count=0, score=41847.462962, test/ctc_loss=0.1784215271472931, test/num_examples=2472, test/wer=0.057902, total_duration=45626.034811, train/ctc_loss=0.1381945163011551, train/wer=0.051274, validation/ctc_loss=0.33163464069366455, validation/num_examples=5348, validation/wer=0.095273
I1010 08:21:29.658685 139587790542592 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.9763254523277283, loss=1.0239794254302979
I1010 08:28:45.253288 139587798935296 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.261803388595581, loss=0.9989897608757019
I1010 08:35:22.716405 139587790542592 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7888038754463196, loss=0.9961706399917603
I1010 08:42:49.306941 139587798935296 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.9171155691146851, loss=0.9729411005973816
I1010 08:43:51.090581 139760355301184 spec.py:321] Evaluating on the training split.
I1010 08:44:42.185239 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 08:45:32.914222 139760355301184 spec.py:349] Evaluating on the test split.
I1010 08:45:58.712774 139760355301184 submission_runner.py:384] Time since start: 47194.14s, 	Step: 52574, 	{'train/ctc_loss': Array(0.12008364, dtype=float32), 'train/wer': 0.04481081429307679, 'validation/ctc_loss': Array(0.32633117, dtype=float32), 'validation/wer': 0.09296761184381905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1735457, dtype=float32), 'test/wer': 0.055720548720124455, 'test/num_examples': 2472, 'score': 43287.863677978516, 'total_duration': 47194.14335393906, 'accumulated_submission_time': 43287.863677978516, 'accumulated_eval_time': 3903.6335020065308, 'accumulated_logging_time': 1.5103745460510254}
I1010 08:45:58.754382 139587798935296 logging_writer.py:48] [52574] accumulated_eval_time=3903.633502, accumulated_logging_time=1.510375, accumulated_submission_time=43287.863678, global_step=52574, preemption_count=0, score=43287.863678, test/ctc_loss=0.173545703291893, test/num_examples=2472, test/wer=0.055721, total_duration=47194.143354, train/ctc_loss=0.12008364498615265, train/wer=0.044811, validation/ctc_loss=0.32633116841316223, validation/num_examples=5348, validation/wer=0.092968
I1010 08:51:22.808767 139587790542592 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9948403239250183, loss=0.9529433846473694
I1010 08:58:52.193705 139587798935296 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.179997205734253, loss=0.9830307364463806
I1010 09:05:31.452238 139587798935296 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8590233325958252, loss=0.979460597038269
I1010 09:09:58.779312 139760355301184 spec.py:321] Evaluating on the training split.
I1010 09:10:49.854023 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 09:11:39.680150 139760355301184 spec.py:349] Evaluating on the test split.
I1010 09:12:05.306789 139760355301184 submission_runner.py:384] Time since start: 48760.74s, 	Step: 54305, 	{'train/ctc_loss': Array(0.1184141, dtype=float32), 'train/wer': 0.0449939791058701, 'validation/ctc_loss': Array(0.3162087, dtype=float32), 'validation/wer': 0.0899092128240504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17026861, dtype=float32), 'test/wer': 0.05517506111481504, 'test/num_examples': 2472, 'score': 44727.84099626541, 'total_duration': 48760.73849654198, 'accumulated_submission_time': 44727.84099626541, 'accumulated_eval_time': 4030.155822277069, 'accumulated_logging_time': 1.5671231746673584}
I1010 09:12:05.349466 139587798935296 logging_writer.py:48] [54305] accumulated_eval_time=4030.155822, accumulated_logging_time=1.567123, accumulated_submission_time=44727.840996, global_step=54305, preemption_count=0, score=44727.840996, test/ctc_loss=0.17026861011981964, test/num_examples=2472, test/wer=0.055175, total_duration=48760.738497, train/ctc_loss=0.11841410398483276, train/wer=0.044994, validation/ctc_loss=0.31620869040489197, validation/num_examples=5348, validation/wer=0.089909
I1010 09:14:32.921208 139587790542592 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.1751972436904907, loss=0.9839392900466919
I1010 09:21:07.146310 139587798935296 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.060638666152954, loss=0.9591861367225647
I1010 09:28:30.482877 139587790542592 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9018807411193848, loss=0.9642557501792908
I1010 09:35:13.245069 139587798935296 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.7527662515640259, loss=0.9892176985740662
I1010 09:36:06.031152 139760355301184 spec.py:321] Evaluating on the training split.
I1010 09:36:58.572796 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 09:37:49.381026 139760355301184 spec.py:349] Evaluating on the test split.
I1010 09:38:15.198112 139760355301184 submission_runner.py:384] Time since start: 50330.63s, 	Step: 56064, 	{'train/ctc_loss': Array(0.11180616, dtype=float32), 'train/wer': 0.0420151828847481, 'validation/ctc_loss': Array(0.31352618, dtype=float32), 'validation/wer': 0.08894441818059026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16738182, dtype=float32), 'test/wer': 0.05359920803281007, 'test/num_examples': 2472, 'score': 46168.47443771362, 'total_duration': 50330.62917804718, 'accumulated_submission_time': 46168.47443771362, 'accumulated_eval_time': 4159.316988945007, 'accumulated_logging_time': 1.6241438388824463}
I1010 09:38:15.238347 139587798935296 logging_writer.py:48] [56064] accumulated_eval_time=4159.316989, accumulated_logging_time=1.624144, accumulated_submission_time=46168.474438, global_step=56064, preemption_count=0, score=46168.474438, test/ctc_loss=0.16738182306289673, test/num_examples=2472, test/wer=0.053599, total_duration=50330.629178, train/ctc_loss=0.11180616170167923, train/wer=0.042015, validation/ctc_loss=0.3135261833667755, validation/num_examples=5348, validation/wer=0.088944
I1010 09:44:17.112504 139587790542592 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9926364421844482, loss=0.9971340894699097
I1010 09:51:04.568449 139587798935296 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7853022217750549, loss=0.9699975252151489
I1010 09:58:28.842372 139587790542592 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0250340700149536, loss=0.970389723777771
I1010 10:02:15.389623 139760355301184 spec.py:321] Evaluating on the training split.
I1010 10:03:05.705620 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 10:03:55.261958 139760355301184 spec.py:349] Evaluating on the test split.
I1010 10:04:21.484555 139760355301184 submission_runner.py:384] Time since start: 51896.92s, 	Step: 57765, 	{'train/ctc_loss': Array(0.11782079, dtype=float32), 'train/wer': 0.04438377802960063, 'validation/ctc_loss': Array(0.3091093, dtype=float32), 'validation/wer': 0.08812434273364915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1657533, dtype=float32), 'test/wer': 0.05281128149180758, 'test/num_examples': 2472, 'score': 47608.57710289955, 'total_duration': 51896.91540527344, 'accumulated_submission_time': 47608.57710289955, 'accumulated_eval_time': 4285.40629196167, 'accumulated_logging_time': 1.6799185276031494}
I1010 10:04:21.523650 139587368855296 logging_writer.py:48] [57765] accumulated_eval_time=4285.406292, accumulated_logging_time=1.679919, accumulated_submission_time=47608.577103, global_step=57765, preemption_count=0, score=47608.577103, test/ctc_loss=0.1657533049583435, test/num_examples=2472, test/wer=0.052811, total_duration=51896.915405, train/ctc_loss=0.11782079190015793, train/wer=0.044384, validation/ctc_loss=0.30910930037498474, validation/num_examples=5348, validation/wer=0.088124
I1010 10:07:19.568766 139587360462592 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.8367311954498291, loss=0.9959229826927185
I1010 10:14:30.329987 139587368855296 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8629869222640991, loss=0.9800504446029663
I1010 10:21:24.708822 139587368855296 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.80827397108078, loss=0.901249885559082
I1010 10:28:22.161411 139760355301184 spec.py:321] Evaluating on the training split.
I1010 10:29:13.218048 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 10:30:02.845474 139760355301184 spec.py:349] Evaluating on the test split.
I1010 10:30:28.924104 139760355301184 submission_runner.py:384] Time since start: 53464.36s, 	Step: 59480, 	{'train/ctc_loss': Array(0.12776782, dtype=float32), 'train/wer': 0.04668968296322112, 'validation/ctc_loss': Array(0.30818343, dtype=float32), 'validation/wer': 0.08774807282269968, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16560148, dtype=float32), 'test/wer': 0.05317493989534719, 'test/num_examples': 2472, 'score': 49049.16763854027, 'total_duration': 53464.35554718971, 'accumulated_submission_time': 49049.16763854027, 'accumulated_eval_time': 4412.163593053818, 'accumulated_logging_time': 1.7332429885864258}
I1010 10:30:28.967817 139587798935296 logging_writer.py:48] [59480] accumulated_eval_time=4412.163593, accumulated_logging_time=1.733243, accumulated_submission_time=49049.167639, global_step=59480, preemption_count=0, score=49049.167639, test/ctc_loss=0.1656014770269394, test/num_examples=2472, test/wer=0.053175, total_duration=53464.355547, train/ctc_loss=0.12776781618595123, train/wer=0.046690, validation/ctc_loss=0.3081834316253662, validation/num_examples=5348, validation/wer=0.087748
I1010 10:30:44.995964 139587790542592 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.8000929355621338, loss=0.9539910554885864
I1010 10:37:09.598803 139760355301184 spec.py:321] Evaluating on the training split.
I1010 10:37:59.569948 139760355301184 spec.py:333] Evaluating on the validation split.
I1010 10:38:49.779217 139760355301184 spec.py:349] Evaluating on the test split.
I1010 10:39:15.835130 139760355301184 submission_runner.py:384] Time since start: 53991.27s, 	Step: 60000, 	{'train/ctc_loss': Array(0.10947262, dtype=float32), 'train/wer': 0.04107539175465734, 'validation/ctc_loss': Array(0.3081884, dtype=float32), 'validation/wer': 0.0877673687155689, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16555075, dtype=float32), 'test/wer': 0.05323554962927046, 'test/num_examples': 2472, 'score': 49449.77249479294, 'total_duration': 53991.26954269409, 'accumulated_submission_time': 49449.77249479294, 'accumulated_eval_time': 4538.397559165955, 'accumulated_logging_time': 1.7924187183380127}
I1010 10:39:15.867590 139587798935296 logging_writer.py:48] [60000] accumulated_eval_time=4538.397559, accumulated_logging_time=1.792419, accumulated_submission_time=49449.772495, global_step=60000, preemption_count=0, score=49449.772495, test/ctc_loss=0.16555075347423553, test/num_examples=2472, test/wer=0.053236, total_duration=53991.269543, train/ctc_loss=0.10947262495756149, train/wer=0.041075, validation/ctc_loss=0.30818840861320496, validation/num_examples=5348, validation/wer=0.087767
I1010 10:39:15.886910 139587790542592 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=49449.772495
I1010 10:39:16.339034 139760355301184 checkpoints.py:490] Saving checkpoint at step: 60000
I1010 10:39:17.881356 139760355301184 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000
I1010 10:39:17.915594 139760355301184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1010 10:39:19.302993 139760355301184 submission_runner.py:552] Tuning trial 1/1
I1010 10:39:19.303263 139760355301184 submission_runner.py:553] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I1010 10:39:19.324189 139760355301184 submission_runner.py:554] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.720894, dtype=float32), 'train/wer': 1.1345761405992962, 'validation/ctc_loss': Array(31.137352, dtype=float32), 'validation/wer': 1.0191994134048568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.172659, dtype=float32), 'test/wer': 1.028688607390347, 'test/num_examples': 2472, 'score': 74.50265550613403, 'total_duration': 220.00778794288635, 'accumulated_submission_time': 74.50265550613403, 'accumulated_eval_time': 145.50507068634033, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1787, {'train/ctc_loss': Array(6.8751073, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.96964, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.9392056, dtype=float32), 'test/wer': 0.8947815019092066, 'test/num_examples': 2472, 'score': 1515.4991357326508, 'total_duration': 1759.1583213806152, 'accumulated_submission_time': 1515.4991357326508, 'accumulated_eval_time': 243.58453154563904, 'accumulated_logging_time': 0.04083824157714844, 'global_step': 1787, 'preemption_count': 0}), (3627, {'train/ctc_loss': Array(2.2803457, dtype=float32), 'train/wer': 0.5863942679061342, 'validation/ctc_loss': Array(2.642085, dtype=float32), 'validation/wer': 0.6013372053758358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.3293712, dtype=float32), 'test/wer': 0.5614683718205143, 'test/num_examples': 2472, 'score': 2956.530928373337, 'total_duration': 3325.2920405864716, 'accumulated_submission_time': 2956.530928373337, 'accumulated_eval_time': 368.5951874256134, 'accumulated_logging_time': 0.0946662425994873, 'global_step': 3627, 'preemption_count': 0}), (5437, {'train/ctc_loss': Array(0.7408186, dtype=float32), 'train/wer': 0.25559403513765677, 'validation/ctc_loss': Array(1.0931926, dtype=float32), 'validation/wer': 0.3201671024322473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8059633, dtype=float32), 'test/wer': 0.25926823847909974, 'test/num_examples': 2472, 'score': 4397.0271916389465, 'total_duration': 4889.680223941803, 'accumulated_submission_time': 4397.0271916389465, 'accumulated_eval_time': 492.40317273139954, 'accumulated_logging_time': 0.14118218421936035, 'global_step': 5437, 'preemption_count': 0}), (7211, {'train/ctc_loss': Array(0.53395754, dtype=float32), 'train/wer': 0.18275927070133732, 'validation/ctc_loss': Array(0.8519093, dtype=float32), 'validation/wer': 0.25202365676465766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5807321, dtype=float32), 'test/wer': 0.1891225730852375, 'test/num_examples': 2472, 'score': 5839.5187838077545, 'total_duration': 6455.885906934738, 'accumulated_submission_time': 5839.5187838077545, 'accumulated_eval_time': 616.0307807922363, 'accumulated_logging_time': 0.1905350685119629, 'global_step': 7211, 'preemption_count': 0}), (9004, {'train/ctc_loss': Array(0.48503104, dtype=float32), 'train/wer': 0.1693828959098915, 'validation/ctc_loss': Array(0.7920252, dtype=float32), 'validation/wer': 0.23427143532499106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5327625, dtype=float32), 'test/wer': 0.17708143927914824, 'test/num_examples': 2472, 'score': 7280.057913064957, 'total_duration': 8019.579313993454, 'accumulated_submission_time': 7280.057913064957, 'accumulated_eval_time': 739.0957989692688, 'accumulated_logging_time': 0.24090147018432617, 'global_step': 9004, 'preemption_count': 0}), (10813, {'train/ctc_loss': Array(0.456519, dtype=float32), 'train/wer': 0.1578705901809006, 'validation/ctc_loss': Array(0.73195475, dtype=float32), 'validation/wer': 0.2200021225482156, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4817248, dtype=float32), 'test/wer': 0.16225225771258864, 'test/num_examples': 2472, 'score': 8720.545222043991, 'total_duration': 9584.663981199265, 'accumulated_submission_time': 8720.545222043991, 'accumulated_eval_time': 863.6082570552826, 'accumulated_logging_time': 0.2878406047821045, 'global_step': 10813, 'preemption_count': 0}), (12601, {'train/ctc_loss': Array(0.38347533, dtype=float32), 'train/wer': 0.13859464403462873, 'validation/ctc_loss': Array(0.6809297, dtype=float32), 'validation/wer': 0.20569421798570175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43111068, dtype=float32), 'test/wer': 0.14615027173364042, 'test/num_examples': 2472, 'score': 10161.133113861084, 'total_duration': 11151.915655851364, 'accumulated_submission_time': 10161.133113861084, 'accumulated_eval_time': 990.1856024265289, 'accumulated_logging_time': 0.3368568420410156, 'global_step': 12601, 'preemption_count': 0}), (14340, {'train/ctc_loss': Array(0.3263766, dtype=float32), 'train/wer': 0.11955589068878546, 'validation/ctc_loss': Array(0.63979626, dtype=float32), 'validation/wer': 0.1923704039595172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40513235, dtype=float32), 'test/wer': 0.13564458452027395, 'test/num_examples': 2472, 'score': 11601.08046913147, 'total_duration': 12716.984174966812, 'accumulated_submission_time': 11601.08046913147, 'accumulated_eval_time': 1115.2216124534607, 'accumulated_logging_time': 0.3843390941619873, 'global_step': 14340, 'preemption_count': 0}), (16108, {'train/ctc_loss': Array(0.2981369, dtype=float32), 'train/wer': 0.10746984875165586, 'validation/ctc_loss': Array(0.6117659, dtype=float32), 'validation/wer': 0.18166118341710966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.382525, dtype=float32), 'test/wer': 0.125482352465806, 'test/num_examples': 2472, 'score': 13041.200325965881, 'total_duration': 14283.1800096035, 'accumulated_submission_time': 13041.200325965881, 'accumulated_eval_time': 1241.2086775302887, 'accumulated_logging_time': 0.43555116653442383, 'global_step': 16108, 'preemption_count': 0}), (17899, {'train/ctc_loss': Array(0.2859081, dtype=float32), 'train/wer': 0.1062708146749827, 'validation/ctc_loss': Array(0.5785946, dtype=float32), 'validation/wer': 0.17298767957240302, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35436678, dtype=float32), 'test/wer': 0.11901731418065742, 'test/num_examples': 2472, 'score': 14481.553364992142, 'total_duration': 15849.19946527481, 'accumulated_submission_time': 14481.553364992142, 'accumulated_eval_time': 1366.7899730205536, 'accumulated_logging_time': 0.481656551361084, 'global_step': 17899, 'preemption_count': 0}), (19637, {'train/ctc_loss': Array(0.29576388, dtype=float32), 'train/wer': 0.10601020650995531, 'validation/ctc_loss': Array(0.569922, dtype=float32), 'validation/wer': 0.17127034510704398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34808585, dtype=float32), 'test/wer': 0.11606764046305837, 'test/num_examples': 2472, 'score': 15921.729849100113, 'total_duration': 17415.92658495903, 'accumulated_submission_time': 15921.729849100113, 'accumulated_eval_time': 1493.2486491203308, 'accumulated_logging_time': 0.5371494293212891, 'global_step': 19637, 'preemption_count': 0}), (21386, {'train/ctc_loss': Array(0.2875218, dtype=float32), 'train/wer': 0.10301527422375814, 'validation/ctc_loss': Array(0.54769206, dtype=float32), 'validation/wer': 0.16500882787098767, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33853814, dtype=float32), 'test/wer': 0.11144109744024891, 'test/num_examples': 2472, 'score': 17361.699501276016, 'total_duration': 18981.923527240753, 'accumulated_submission_time': 17361.699501276016, 'accumulated_eval_time': 1619.1895277500153, 'accumulated_logging_time': 0.5851902961730957, 'global_step': 21386, 'preemption_count': 0}), (23163, {'train/ctc_loss': Array(0.26769987, dtype=float32), 'train/wer': 0.09815310217100524, 'validation/ctc_loss': Array(0.5341378, dtype=float32), 'validation/wer': 0.16004013545716794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32812852, dtype=float32), 'test/wer': 0.10790552962805827, 'test/num_examples': 2472, 'score': 18802.321515083313, 'total_duration': 20550.428798913956, 'accumulated_submission_time': 18802.321515083313, 'accumulated_eval_time': 1746.9840960502625, 'accumulated_logging_time': 0.6353154182434082, 'global_step': 23163, 'preemption_count': 0}), (24910, {'train/ctc_loss': Array(0.24160087, dtype=float32), 'train/wer': 0.08819306617119209, 'validation/ctc_loss': Array(0.52083457, dtype=float32), 'validation/wer': 0.15511003482908664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31390664, dtype=float32), 'test/wer': 0.10376386447663495, 'test/num_examples': 2472, 'score': 20242.595469474792, 'total_duration': 22116.756570339203, 'accumulated_submission_time': 20242.595469474792, 'accumulated_eval_time': 1872.9540824890137, 'accumulated_logging_time': 0.6821458339691162, 'global_step': 24910, 'preemption_count': 0}), (26634, {'train/ctc_loss': Array(0.22323658, dtype=float32), 'train/wer': 0.0842588486919325, 'validation/ctc_loss': Array(0.4935157, dtype=float32), 'validation/wer': 0.14816351339617362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2964971, dtype=float32), 'test/wer': 0.10146069458755076, 'test/num_examples': 2472, 'score': 21682.764047145844, 'total_duration': 23684.300047159195, 'accumulated_submission_time': 21682.764047145844, 'accumulated_eval_time': 2000.2347030639648, 'accumulated_logging_time': 0.7379710674285889, 'global_step': 26634, 'preemption_count': 0}), (28386, {'train/ctc_loss': Array(0.21349154, dtype=float32), 'train/wer': 0.07821849014533962, 'validation/ctc_loss': Array(0.48505414, dtype=float32), 'validation/wer': 0.14503757875136278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28211427, dtype=float32), 'test/wer': 0.09503606279168435, 'test/num_examples': 2472, 'score': 23123.37104678154, 'total_duration': 25253.93640947342, 'accumulated_submission_time': 23123.37104678154, 'accumulated_eval_time': 2129.174768924713, 'accumulated_logging_time': 0.7888081073760986, 'global_step': 28386, 'preemption_count': 0}), (30147, {'train/ctc_loss': Array(0.22487819, dtype=float32), 'train/wer': 0.08162159819726122, 'validation/ctc_loss': Array(0.4816879, dtype=float32), 'validation/wer': 0.14194058794585573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27987337, dtype=float32), 'test/wer': 0.09242984423298382, 'test/num_examples': 2472, 'score': 24563.335582256317, 'total_duration': 26819.04343867302, 'accumulated_submission_time': 24563.335582256317, 'accumulated_eval_time': 2254.228602409363, 'accumulated_logging_time': 0.8377974033355713, 'global_step': 30147, 'preemption_count': 0}), (31869, {'train/ctc_loss': Array(0.2107978, dtype=float32), 'train/wer': 0.0764681305921356, 'validation/ctc_loss': Array(0.46005985, dtype=float32), 'validation/wer': 0.13796563401479994, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2707439, dtype=float32), 'test/wer': 0.09059134897064469, 'test/num_examples': 2472, 'score': 26003.94373512268, 'total_duration': 28385.133793592453, 'accumulated_submission_time': 26003.94373512268, 'accumulated_eval_time': 2379.619324207306, 'accumulated_logging_time': 0.8926985263824463, 'global_step': 31869, 'preemption_count': 0}), (33593, {'train/ctc_loss': Array(0.21789105, dtype=float32), 'train/wer': 0.07734153871443322, 'validation/ctc_loss': Array(0.45696384, dtype=float32), 'validation/wer': 0.134116103387394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26811406, dtype=float32), 'test/wer': 0.08521728589611492, 'test/num_examples': 2472, 'score': 27444.353387355804, 'total_duration': 29953.740344285965, 'accumulated_submission_time': 27444.353387355804, 'accumulated_eval_time': 2507.7268373966217, 'accumulated_logging_time': 0.9440710544586182, 'global_step': 33593, 'preemption_count': 0}), (35355, {'train/ctc_loss': Array(0.19773735, dtype=float32), 'train/wer': 0.06897599028101921, 'validation/ctc_loss': Array(0.434604, dtype=float32), 'validation/wer': 0.12980347133112716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2498267, dtype=float32), 'test/wer': 0.08121704345717923, 'test/num_examples': 2472, 'score': 28884.875313043594, 'total_duration': 31522.789919614792, 'accumulated_submission_time': 28884.875313043594, 'accumulated_eval_time': 2636.166344642639, 'accumulated_logging_time': 0.9943187236785889, 'global_step': 35355, 'preemption_count': 0}), (37065, {'train/ctc_loss': Array(0.15390655, dtype=float32), 'train/wer': 0.057491057060565995, 'validation/ctc_loss': Array(0.4286001, dtype=float32), 'validation/wer': 0.12687049561500835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24446504, dtype=float32), 'test/wer': 0.080711962341152, 'test/num_examples': 2472, 'score': 30325.11698436737, 'total_duration': 33090.52071261406, 'accumulated_submission_time': 30325.11698436737, 'accumulated_eval_time': 2763.5697836875916, 'accumulated_logging_time': 1.0425512790679932, 'global_step': 37065, 'preemption_count': 0}), (38780, {'train/ctc_loss': Array(0.1652866, dtype=float32), 'train/wer': 0.05985090072015375, 'validation/ctc_loss': Array(0.4101026, dtype=float32), 'validation/wer': 0.12054144275390984, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23171791, dtype=float32), 'test/wer': 0.07648948421116432, 'test/num_examples': 2472, 'score': 31765.343761205673, 'total_duration': 34660.3071846962, 'accumulated_submission_time': 31765.343761205673, 'accumulated_eval_time': 2893.043671607971, 'accumulated_logging_time': 1.089216709136963, 'global_step': 38780, 'preemption_count': 0}), (40556, {'train/ctc_loss': Array(0.20123275, dtype=float32), 'train/wer': 0.07268269663652685, 'validation/ctc_loss': Array(0.39494124, dtype=float32), 'validation/wer': 0.11778213007361384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22860794, dtype=float32), 'test/wer': 0.07491363112915934, 'test/num_examples': 2472, 'score': 33205.33744955063, 'total_duration': 36227.239282369614, 'accumulated_submission_time': 33205.33744955063, 'accumulated_eval_time': 3019.8914070129395, 'accumulated_logging_time': 1.1400611400604248, 'global_step': 40556, 'preemption_count': 0}), (42266, {'train/ctc_loss': Array(0.19825664, dtype=float32), 'train/wer': 0.07227832149974742, 'validation/ctc_loss': Array(0.39015985, dtype=float32), 'validation/wer': 0.11504211328618703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21761815, dtype=float32), 'test/wer': 0.07186294118835485, 'test/num_examples': 2472, 'score': 34645.44293189049, 'total_duration': 37794.30714058876, 'accumulated_submission_time': 34645.44293189049, 'accumulated_eval_time': 3146.7637164592743, 'accumulated_logging_time': 1.192563533782959, 'global_step': 42266, 'preemption_count': 0}), (43980, {'train/ctc_loss': Array(0.21884592, dtype=float32), 'train/wer': 0.080467502383724, 'validation/ctc_loss': Array(0.3740899, dtype=float32), 'validation/wer': 0.11006377292593271, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20820065, dtype=float32), 'test/wer': 0.06897387720467907, 'test/num_examples': 2472, 'score': 36085.892963171005, 'total_duration': 39359.79950046539, 'accumulated_submission_time': 36085.892963171005, 'accumulated_eval_time': 3271.7181429862976, 'accumulated_logging_time': 1.241924524307251, 'global_step': 43980, 'preemption_count': 0}), (45721, {'train/ctc_loss': Array(0.18242821, dtype=float32), 'train/wer': 0.06503423629223605, 'validation/ctc_loss': Array(0.36401728, dtype=float32), 'validation/wer': 0.10469951470829433, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19969043, dtype=float32), 'test/wer': 0.065115057478231, 'test/num_examples': 2472, 'score': 37526.15644979477, 'total_duration': 40923.86538338661, 'accumulated_submission_time': 37526.15644979477, 'accumulated_eval_time': 3395.431552886963, 'accumulated_logging_time': 1.293529748916626, 'global_step': 45721, 'preemption_count': 0}), (47418, {'train/ctc_loss': Array(0.1607592, dtype=float32), 'train/wer': 0.06036431251600548, 'validation/ctc_loss': Array(0.35039896, dtype=float32), 'validation/wer': 0.10160252390278729, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19369689, dtype=float32), 'test/wer': 0.06442814716043396, 'test/num_examples': 2472, 'score': 38966.495310783386, 'total_duration': 42491.13949394226, 'accumulated_submission_time': 38966.495310783386, 'accumulated_eval_time': 3522.272211790085, 'accumulated_logging_time': 1.3507068157196045, 'global_step': 47418, 'preemption_count': 0}), (49139, {'train/ctc_loss': Array(0.12839854, dtype=float32), 'train/wer': 0.04873897486650208, 'validation/ctc_loss': Array(0.33969137, dtype=float32), 'validation/wer': 0.09855377282945325, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18558753, dtype=float32), 'test/wer': 0.06194314806957998, 'test/num_examples': 2472, 'score': 40406.55139708519, 'total_duration': 44058.21048665047, 'accumulated_submission_time': 40406.55139708519, 'accumulated_eval_time': 3649.1928713321686, 'accumulated_logging_time': 1.4066059589385986, 'global_step': 49139, 'preemption_count': 0}), (50871, {'train/ctc_loss': Array(0.13819452, dtype=float32), 'train/wer': 0.05127439213151792, 'validation/ctc_loss': Array(0.33163464, dtype=float32), 'validation/wer': 0.09527347104168878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17842153, dtype=float32), 'test/wer': 0.057902499141362106, 'test/num_examples': 2472, 'score': 41847.462961912155, 'total_duration': 45626.0348110199, 'accumulated_submission_time': 41847.462961912155, 'accumulated_eval_time': 3776.0174107551575, 'accumulated_logging_time': 1.457076072692871, 'global_step': 50871, 'preemption_count': 0}), (52574, {'train/ctc_loss': Array(0.12008364, dtype=float32), 'train/wer': 0.04481081429307679, 'validation/ctc_loss': Array(0.32633117, dtype=float32), 'validation/wer': 0.09296761184381905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1735457, dtype=float32), 'test/wer': 0.055720548720124455, 'test/num_examples': 2472, 'score': 43287.863677978516, 'total_duration': 47194.14335393906, 'accumulated_submission_time': 43287.863677978516, 'accumulated_eval_time': 3903.6335020065308, 'accumulated_logging_time': 1.5103745460510254, 'global_step': 52574, 'preemption_count': 0}), (54305, {'train/ctc_loss': Array(0.1184141, dtype=float32), 'train/wer': 0.0449939791058701, 'validation/ctc_loss': Array(0.3162087, dtype=float32), 'validation/wer': 0.0899092128240504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17026861, dtype=float32), 'test/wer': 0.05517506111481504, 'test/num_examples': 2472, 'score': 44727.84099626541, 'total_duration': 48760.73849654198, 'accumulated_submission_time': 44727.84099626541, 'accumulated_eval_time': 4030.155822277069, 'accumulated_logging_time': 1.5671231746673584, 'global_step': 54305, 'preemption_count': 0}), (56064, {'train/ctc_loss': Array(0.11180616, dtype=float32), 'train/wer': 0.0420151828847481, 'validation/ctc_loss': Array(0.31352618, dtype=float32), 'validation/wer': 0.08894441818059026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16738182, dtype=float32), 'test/wer': 0.05359920803281007, 'test/num_examples': 2472, 'score': 46168.47443771362, 'total_duration': 50330.62917804718, 'accumulated_submission_time': 46168.47443771362, 'accumulated_eval_time': 4159.316988945007, 'accumulated_logging_time': 1.6241438388824463, 'global_step': 56064, 'preemption_count': 0}), (57765, {'train/ctc_loss': Array(0.11782079, dtype=float32), 'train/wer': 0.04438377802960063, 'validation/ctc_loss': Array(0.3091093, dtype=float32), 'validation/wer': 0.08812434273364915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1657533, dtype=float32), 'test/wer': 0.05281128149180758, 'test/num_examples': 2472, 'score': 47608.57710289955, 'total_duration': 51896.91540527344, 'accumulated_submission_time': 47608.57710289955, 'accumulated_eval_time': 4285.40629196167, 'accumulated_logging_time': 1.6799185276031494, 'global_step': 57765, 'preemption_count': 0}), (59480, {'train/ctc_loss': Array(0.12776782, dtype=float32), 'train/wer': 0.04668968296322112, 'validation/ctc_loss': Array(0.30818343, dtype=float32), 'validation/wer': 0.08774807282269968, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16560148, dtype=float32), 'test/wer': 0.05317493989534719, 'test/num_examples': 2472, 'score': 49049.16763854027, 'total_duration': 53464.35554718971, 'accumulated_submission_time': 49049.16763854027, 'accumulated_eval_time': 4412.163593053818, 'accumulated_logging_time': 1.7332429885864258, 'global_step': 59480, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.10947262, dtype=float32), 'train/wer': 0.04107539175465734, 'validation/ctc_loss': Array(0.3081884, dtype=float32), 'validation/wer': 0.0877673687155689, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16555075, dtype=float32), 'test/wer': 0.05323554962927046, 'test/num_examples': 2472, 'score': 49449.77249479294, 'total_duration': 53991.26954269409, 'accumulated_submission_time': 49449.77249479294, 'accumulated_eval_time': 4538.397559165955, 'accumulated_logging_time': 1.7924187183380127, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1010 10:39:19.324450 139760355301184 submission_runner.py:555] Timing: 49449.77249479294
I1010 10:39:19.324522 139760355301184 submission_runner.py:557] Total number of evals: 36
I1010 10:39:19.324581 139760355301184 submission_runner.py:558] ====================
I1010 10:39:19.329213 139760355301184 submission_runner.py:628] Final librispeech_conformer score: 49449.77249479294
