python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_jax/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_09-14-2023-04-33-24.log
2023-09-14 04:33:29.702818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0914 04:33:46.604569 139758520063808 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax.
I0914 04:33:48.278331 139758520063808 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0914 04:33:48.279143 139758520063808 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0914 04:33:48.279330 139758520063808 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0914 04:33:48.284710 139758520063808 submission_runner.py:500] Using RNG seed 877302125
I0914 04:33:54.317976 139758520063808 submission_runner.py:509] --- Tuning run 1/1 ---
I0914 04:33:54.318214 139758520063808 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1.
I0914 04:33:54.318408 139758520063808 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1/hparams.json.
I0914 04:33:54.497093 139758520063808 submission_runner.py:185] Initializing dataset.
I0914 04:33:54.497330 139758520063808 submission_runner.py:192] Initializing model.
I0914 04:34:00.234251 139758520063808 submission_runner.py:226] Initializing optimizer.
I0914 04:34:03.226596 139758520063808 submission_runner.py:233] Initializing metrics bundle.
I0914 04:34:03.226849 139758520063808 submission_runner.py:251] Initializing checkpoint and logger.
I0914 04:34:03.228203 139758520063808 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1 with prefix checkpoint_
I0914 04:34:03.228563 139758520063808 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0914 04:34:03.228640 139758520063808 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0914 04:34:03.943162 139758520063808 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1/meta_data_0.json.
I0914 04:34:03.944250 139758520063808 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1/flags_0.json.
I0914 04:34:04.034359 139758520063808 submission_runner.py:285] Starting training loop.
I0914 04:34:30.943376 139594306672384 logging_writer.py:48] [0] global_step=0, grad_norm=13.899029731750488, loss=1.8770571947097778
I0914 04:34:30.953968 139758520063808 spec.py:320] Evaluating on the training split.
I0914 04:38:41.983728 139758520063808 spec.py:332] Evaluating on the validation split.
I0914 04:42:55.801451 139758520063808 spec.py:348] Evaluating on the test split.
I0914 04:47:12.590895 139758520063808 submission_runner.py:376] Time since start: 788.56s, 	Step: 1, 	{'train/loss': 1.8823116526884192, 'validation/loss': 1.867852404494382, 'validation/num_examples': 89000000, 'test/loss': 1.8833504077983538, 'test/num_examples': 89274637, 'score': 26.919605493545532, 'total_duration': 788.556474685669, 'accumulated_submission_time': 26.919605493545532, 'accumulated_eval_time': 761.6368346214294, 'accumulated_logging_time': 0}
I0914 04:47:12.610067 139574273627904 logging_writer.py:48] [1] accumulated_eval_time=761.636835, accumulated_logging_time=0, accumulated_submission_time=26.919605, global_step=1, preemption_count=0, score=26.919605, test/loss=1.883350, test/num_examples=89274637, total_duration=788.556475, train/loss=1.882312, validation/loss=1.867852, validation/num_examples=89000000
I0914 04:47:12.723112 139574265235200 logging_writer.py:48] [1] global_step=1, grad_norm=13.8823823928833, loss=1.8779610395431519
I0914 04:47:12.831329 139574273627904 logging_writer.py:48] [2] global_step=2, grad_norm=13.078275680541992, loss=1.7118420600891113
I0914 04:47:12.936199 139574265235200 logging_writer.py:48] [3] global_step=3, grad_norm=11.143219947814941, loss=1.4601424932479858
I0914 04:47:13.040381 139574273627904 logging_writer.py:48] [4] global_step=4, grad_norm=9.126725196838379, loss=1.1856557130813599
I0914 04:47:13.143570 139574265235200 logging_writer.py:48] [5] global_step=5, grad_norm=7.401397705078125, loss=0.929911196231842
I0914 04:47:13.246936 139574273627904 logging_writer.py:48] [6] global_step=6, grad_norm=6.229712009429932, loss=0.6962074041366577
I0914 04:47:13.353132 139574265235200 logging_writer.py:48] [7] global_step=7, grad_norm=4.696631908416748, loss=0.4910118579864502
I0914 04:47:13.456249 139574273627904 logging_writer.py:48] [8] global_step=8, grad_norm=3.0066308975219727, loss=0.34201523661613464
I0914 04:47:13.560561 139574265235200 logging_writer.py:48] [9] global_step=9, grad_norm=1.4114314317703247, loss=0.2509717047214508
I0914 04:47:13.666372 139574273627904 logging_writer.py:48] [10] global_step=10, grad_norm=0.3430750072002411, loss=0.2191115915775299
I0914 04:47:13.771461 139574265235200 logging_writer.py:48] [11] global_step=11, grad_norm=0.8075507283210754, loss=0.22399379312992096
I0914 04:47:13.876564 139574273627904 logging_writer.py:48] [12] global_step=12, grad_norm=1.3344449996948242, loss=0.2495419830083847
I0914 04:47:13.981514 139574265235200 logging_writer.py:48] [13] global_step=13, grad_norm=1.711918830871582, loss=0.2833591401576996
I0914 04:47:14.086432 139574273627904 logging_writer.py:48] [14] global_step=14, grad_norm=2.0335724353790283, loss=0.3226014971733093
I0914 04:47:14.190380 139574265235200 logging_writer.py:48] [15] global_step=15, grad_norm=2.1803622245788574, loss=0.3436342179775238
I0914 04:47:14.294188 139574273627904 logging_writer.py:48] [16] global_step=16, grad_norm=2.213799238204956, loss=0.3496094048023224
I0914 04:47:14.397860 139574265235200 logging_writer.py:48] [17] global_step=17, grad_norm=2.3458662033081055, loss=0.3678141236305237
I0914 04:47:14.501550 139574273627904 logging_writer.py:48] [18] global_step=18, grad_norm=2.170607089996338, loss=0.34101900458335876
I0914 04:47:14.604958 139574265235200 logging_writer.py:48] [19] global_step=19, grad_norm=2.041865110397339, loss=0.3129154145717621
I0914 04:47:14.708889 139574273627904 logging_writer.py:48] [20] global_step=20, grad_norm=1.9348187446594238, loss=0.29418495297431946
I0914 04:47:14.813177 139574265235200 logging_writer.py:48] [21] global_step=21, grad_norm=1.6757538318634033, loss=0.2578454315662384
I0914 04:47:14.916994 139574273627904 logging_writer.py:48] [22] global_step=22, grad_norm=1.3764313459396362, loss=0.22231821715831757
I0914 04:47:15.021088 139574265235200 logging_writer.py:48] [23] global_step=23, grad_norm=1.0195996761322021, loss=0.19002585113048553
I0914 04:47:15.125438 139574273627904 logging_writer.py:48] [24] global_step=24, grad_norm=0.6191898584365845, loss=0.17023657262325287
I0914 04:47:15.229209 139574265235200 logging_writer.py:48] [25] global_step=25, grad_norm=0.17961376905441284, loss=0.1542508602142334
I0914 04:47:15.333927 139574273627904 logging_writer.py:48] [26] global_step=26, grad_norm=0.44516509771347046, loss=0.15645870566368103
I0914 04:47:15.439042 139574265235200 logging_writer.py:48] [27] global_step=27, grad_norm=0.5437039136886597, loss=0.15689054131507874
I0914 04:47:15.738370 139574273627904 logging_writer.py:48] [28] global_step=28, grad_norm=0.3560785949230194, loss=0.15172997117042542
I0914 04:47:16.529443 139574265235200 logging_writer.py:48] [29] global_step=29, grad_norm=0.08388356864452362, loss=0.146329864859581
I0914 04:47:17.283370 139574273627904 logging_writer.py:48] [30] global_step=30, grad_norm=0.17219975590705872, loss=0.14347504079341888
I0914 04:47:18.039892 139574265235200 logging_writer.py:48] [31] global_step=31, grad_norm=0.26572027802467346, loss=0.14713890850543976
I0914 04:47:18.716899 139574273627904 logging_writer.py:48] [32] global_step=32, grad_norm=0.1210951879620552, loss=0.14311842620372772
I0914 04:47:19.432710 139574265235200 logging_writer.py:48] [33] global_step=33, grad_norm=0.05126804858446121, loss=0.13962604105472565
I0914 04:47:20.216194 139574273627904 logging_writer.py:48] [34] global_step=34, grad_norm=0.058685239404439926, loss=0.14047998189926147
I0914 04:47:20.885709 139574265235200 logging_writer.py:48] [35] global_step=35, grad_norm=0.0622030571103096, loss=0.14189667999744415
I0914 04:47:21.531827 139574273627904 logging_writer.py:48] [36] global_step=36, grad_norm=0.051831308752298355, loss=0.14046813547611237
I0914 04:47:22.283863 139574265235200 logging_writer.py:48] [37] global_step=37, grad_norm=0.04770956188440323, loss=0.1387021392583847
I0914 04:47:22.889190 139574273627904 logging_writer.py:48] [38] global_step=38, grad_norm=0.04401659592986107, loss=0.13826029002666473
I0914 04:47:23.544856 139574265235200 logging_writer.py:48] [39] global_step=39, grad_norm=0.04087147116661072, loss=0.1366947591304779
I0914 04:47:24.099351 139574273627904 logging_writer.py:48] [40] global_step=40, grad_norm=0.034981198608875275, loss=0.13893696665763855
I0914 04:47:24.646697 139574265235200 logging_writer.py:48] [41] global_step=41, grad_norm=0.04850899800658226, loss=0.13552765548229218
I0914 04:47:25.299693 139574273627904 logging_writer.py:48] [42] global_step=42, grad_norm=0.05954886972904205, loss=0.13345377147197723
I0914 04:47:25.729648 139574265235200 logging_writer.py:48] [43] global_step=43, grad_norm=0.12321209162473679, loss=0.13320350646972656
I0914 04:47:26.377517 139574273627904 logging_writer.py:48] [44] global_step=44, grad_norm=0.14663079380989075, loss=0.13058778643608093
I0914 04:47:26.884076 139574265235200 logging_writer.py:48] [45] global_step=45, grad_norm=0.19614852964878082, loss=0.13239558041095734
I0914 04:47:27.530275 139574273627904 logging_writer.py:48] [46] global_step=46, grad_norm=0.1858202964067459, loss=0.13322201371192932
I0914 04:47:27.987202 139574265235200 logging_writer.py:48] [47] global_step=47, grad_norm=0.1890455186367035, loss=0.1348491758108139
I0914 04:47:28.562662 139574273627904 logging_writer.py:48] [48] global_step=48, grad_norm=0.12779071927070618, loss=0.1328447163105011
I0914 04:47:29.012202 139574265235200 logging_writer.py:48] [49] global_step=49, grad_norm=0.1366194486618042, loss=0.1302204132080078
I0914 04:47:29.566363 139574273627904 logging_writer.py:48] [50] global_step=50, grad_norm=0.12067367136478424, loss=0.1290196180343628
I0914 04:47:30.197843 139574265235200 logging_writer.py:48] [51] global_step=51, grad_norm=0.10834737867116928, loss=0.1293923407793045
I0914 04:47:30.657699 139574273627904 logging_writer.py:48] [52] global_step=52, grad_norm=0.09564251452684402, loss=0.12777402997016907
I0914 04:47:31.193535 139574265235200 logging_writer.py:48] [53] global_step=53, grad_norm=0.0925288125872612, loss=0.12852725386619568
I0914 04:47:31.761158 139574273627904 logging_writer.py:48] [54] global_step=54, grad_norm=0.10858425498008728, loss=0.12886571884155273
I0914 04:47:32.372995 139574265235200 logging_writer.py:48] [55] global_step=55, grad_norm=0.1684049516916275, loss=0.13020147383213043
I0914 04:47:32.918534 139574273627904 logging_writer.py:48] [56] global_step=56, grad_norm=0.18134814500808716, loss=0.12615297734737396
I0914 04:47:33.747006 139574265235200 logging_writer.py:48] [57] global_step=57, grad_norm=0.1584915965795517, loss=0.1362311989068985
I0914 04:47:34.372076 139574273627904 logging_writer.py:48] [58] global_step=58, grad_norm=0.17856398224830627, loss=0.13786286115646362
I0914 04:47:35.031000 139574265235200 logging_writer.py:48] [59] global_step=59, grad_norm=0.31544867157936096, loss=0.13982713222503662
I0914 04:47:35.767137 139574273627904 logging_writer.py:48] [60] global_step=60, grad_norm=0.358242928981781, loss=0.13756683468818665
I0914 04:47:36.470560 139574265235200 logging_writer.py:48] [61] global_step=61, grad_norm=0.3530072569847107, loss=0.13909906148910522
I0914 04:47:37.264034 139574273627904 logging_writer.py:48] [62] global_step=62, grad_norm=0.33516693115234375, loss=0.1404213011264801
I0914 04:47:37.993531 139574265235200 logging_writer.py:48] [63] global_step=63, grad_norm=0.3388425409793854, loss=0.13552527129650116
I0914 04:47:38.735652 139574273627904 logging_writer.py:48] [64] global_step=64, grad_norm=0.3132571280002594, loss=0.13981308043003082
I0914 04:47:39.509276 139574265235200 logging_writer.py:48] [65] global_step=65, grad_norm=0.24518080055713654, loss=0.13656550645828247
I0914 04:47:40.309777 139574273627904 logging_writer.py:48] [66] global_step=66, grad_norm=0.16177591681480408, loss=0.13428834080696106
I0914 04:47:41.204582 139574265235200 logging_writer.py:48] [67] global_step=67, grad_norm=0.09799695760011673, loss=0.13403081893920898
I0914 04:47:41.960905 139574273627904 logging_writer.py:48] [68] global_step=68, grad_norm=0.06316914409399033, loss=0.13409484922885895
I0914 04:47:42.585748 139574265235200 logging_writer.py:48] [69] global_step=69, grad_norm=0.03237662836909294, loss=0.13541698455810547
I0914 04:47:43.351856 139574273627904 logging_writer.py:48] [70] global_step=70, grad_norm=0.011128156445920467, loss=0.13220015168190002
I0914 04:47:44.069181 139574265235200 logging_writer.py:48] [71] global_step=71, grad_norm=0.021512918174266815, loss=0.13323064148426056
I0914 04:47:44.876554 139574273627904 logging_writer.py:48] [72] global_step=72, grad_norm=0.010470879264175892, loss=0.134220153093338
I0914 04:47:45.580037 139574265235200 logging_writer.py:48] [73] global_step=73, grad_norm=0.014933635480701923, loss=0.13540637493133545
I0914 04:47:46.213850 139574273627904 logging_writer.py:48] [74] global_step=74, grad_norm=0.052801646292209625, loss=0.13108818233013153
I0914 04:47:47.047322 139574265235200 logging_writer.py:48] [75] global_step=75, grad_norm=0.09689520299434662, loss=0.13323058187961578
I0914 04:47:47.642702 139574273627904 logging_writer.py:48] [76] global_step=76, grad_norm=0.15256965160369873, loss=0.13908693194389343
I0914 04:47:48.342127 139574265235200 logging_writer.py:48] [77] global_step=77, grad_norm=0.21672683954238892, loss=0.14363320171833038
I0914 04:47:49.224469 139574273627904 logging_writer.py:48] [78] global_step=78, grad_norm=0.33444586396217346, loss=0.14278656244277954
I0914 04:47:49.959832 139574265235200 logging_writer.py:48] [79] global_step=79, grad_norm=0.43053948879241943, loss=0.14695537090301514
I0914 04:47:50.683617 139574273627904 logging_writer.py:48] [80] global_step=80, grad_norm=0.5096644759178162, loss=0.144477978348732
I0914 04:47:51.397770 139574265235200 logging_writer.py:48] [81] global_step=81, grad_norm=0.49230310320854187, loss=0.14745177328586578
I0914 04:47:52.197722 139574273627904 logging_writer.py:48] [82] global_step=82, grad_norm=0.32194921374320984, loss=0.1415034681558609
I0914 04:47:52.826020 139574265235200 logging_writer.py:48] [83] global_step=83, grad_norm=0.22697828710079193, loss=0.14293289184570312
I0914 04:47:53.576148 139574273627904 logging_writer.py:48] [84] global_step=84, grad_norm=0.18886195123195648, loss=0.1402367800474167
I0914 04:47:54.297088 139574265235200 logging_writer.py:48] [85] global_step=85, grad_norm=0.1519830971956253, loss=0.14218439161777496
I0914 04:47:55.237658 139574273627904 logging_writer.py:48] [86] global_step=86, grad_norm=0.12773080170154572, loss=0.14046639204025269
I0914 04:47:55.855335 139574265235200 logging_writer.py:48] [87] global_step=87, grad_norm=0.08632893115282059, loss=0.13954895734786987
I0914 04:47:56.559716 139574273627904 logging_writer.py:48] [88] global_step=88, grad_norm=0.06580052524805069, loss=0.13847577571868896
I0914 04:47:57.438827 139574265235200 logging_writer.py:48] [89] global_step=89, grad_norm=0.06635887175798416, loss=0.13862639665603638
I0914 04:47:57.987416 139574273627904 logging_writer.py:48] [90] global_step=90, grad_norm=0.054383620619773865, loss=0.1404997706413269
I0914 04:47:58.669579 139574265235200 logging_writer.py:48] [91] global_step=91, grad_norm=0.03634766861796379, loss=0.13959726691246033
I0914 04:47:59.383507 139574273627904 logging_writer.py:48] [92] global_step=92, grad_norm=0.059962887316942215, loss=0.13753020763397217
I0914 04:48:00.104598 139574265235200 logging_writer.py:48] [93] global_step=93, grad_norm=0.058533720672130585, loss=0.13699495792388916
I0914 04:48:00.749077 139574273627904 logging_writer.py:48] [94] global_step=94, grad_norm=0.033411070704460144, loss=0.13879907131195068
I0914 04:48:01.523231 139574265235200 logging_writer.py:48] [95] global_step=95, grad_norm=0.01856047473847866, loss=0.13253335654735565
I0914 04:48:02.226198 139574273627904 logging_writer.py:48] [96] global_step=96, grad_norm=0.00872740801423788, loss=0.12897290289402008
I0914 04:48:02.958463 139574265235200 logging_writer.py:48] [97] global_step=97, grad_norm=0.012100457213819027, loss=0.1285085380077362
I0914 04:48:03.644919 139574273627904 logging_writer.py:48] [98] global_step=98, grad_norm=0.011447320692241192, loss=0.12811003625392914
I0914 04:48:04.333218 139574265235200 logging_writer.py:48] [99] global_step=99, grad_norm=0.0071762618608772755, loss=0.12875144183635712
I0914 04:48:05.044015 139574273627904 logging_writer.py:48] [100] global_step=100, grad_norm=0.0087570296600461, loss=0.12737947702407837
I0914 04:52:55.808250 139574265235200 logging_writer.py:48] [500] global_step=500, grad_norm=0.08770859986543655, loss=0.12699081003665924
I0914 04:58:52.353788 139574273627904 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.01039548683911562, loss=0.1223212406039238
I0914 05:04:49.646031 139574265235200 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04550425335764885, loss=0.12387709319591522
I0914 05:07:13.191095 139758520063808 spec.py:320] Evaluating on the training split.
I0914 05:10:35.628876 139758520063808 spec.py:332] Evaluating on the validation split.
I0914 05:14:01.790480 139758520063808 spec.py:348] Evaluating on the test split.
I0914 05:17:34.595175 139758520063808 submission_runner.py:376] Time since start: 2610.56s, 	Step: 1699, 	{'train/loss': 0.12231887368594899, 'validation/loss': 0.1253474606741573, 'validation/num_examples': 89000000, 'test/loss': 0.12844242648670753, 'test/num_examples': 89274637, 'score': 1227.4699466228485, 'total_duration': 2610.5607631206512, 'accumulated_submission_time': 1227.4699466228485, 'accumulated_eval_time': 1383.0408952236176, 'accumulated_logging_time': 0.02660083770751953}
I0914 05:17:34.614303 139574273627904 logging_writer.py:48] [1699] accumulated_eval_time=1383.040895, accumulated_logging_time=0.026601, accumulated_submission_time=1227.469947, global_step=1699, preemption_count=0, score=1227.469947, test/loss=0.128442, test/num_examples=89274637, total_duration=2610.560763, train/loss=0.122319, validation/loss=0.125347, validation/num_examples=89000000
I0914 05:20:51.717016 139574265235200 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.017007017508149147, loss=0.12302717566490173
I0914 05:26:51.315697 139574273627904 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.005988923832774162, loss=0.1238579973578453
I0914 05:32:51.799754 139574265235200 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.008937660604715347, loss=0.12237381935119629
I0914 05:37:35.157201 139758520063808 spec.py:320] Evaluating on the training split.
I0914 05:40:55.228979 139758520063808 spec.py:332] Evaluating on the validation split.
I0914 05:44:02.341714 139758520063808 spec.py:348] Evaluating on the test split.
I0914 05:47:09.665210 139758520063808 submission_runner.py:376] Time since start: 4385.63s, 	Step: 3396, 	{'train/loss': 0.12332801818847657, 'validation/loss': 0.12463929213483146, 'validation/num_examples': 89000000, 'test/loss': 0.1279797306820749, 'test/num_examples': 89274637, 'score': 2427.9822726249695, 'total_duration': 4385.630787849426, 'accumulated_submission_time': 2427.9822726249695, 'accumulated_eval_time': 1957.5488657951355, 'accumulated_logging_time': 0.05364203453063965}
I0914 05:47:09.679230 139574273627904 logging_writer.py:48] [3396] accumulated_eval_time=1957.548866, accumulated_logging_time=0.053642, accumulated_submission_time=2427.982273, global_step=3396, preemption_count=0, score=2427.982273, test/loss=0.127980, test/num_examples=89274637, total_duration=4385.630788, train/loss=0.123328, validation/loss=0.124639, validation/num_examples=89000000
I0914 05:48:10.355907 139574265235200 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.005120398942381144, loss=0.1291469931602478
I0914 05:54:04.377296 139574273627904 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.00520826131105423, loss=0.12154964357614517
I0914 06:00:00.897792 139574265235200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03511021286249161, loss=0.12740162014961243
I0914 06:05:55.688668 139574273627904 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.006249576807022095, loss=0.13410839438438416
I0914 06:07:09.836003 139758520063808 spec.py:320] Evaluating on the training split.
I0914 06:10:23.561270 139758520063808 spec.py:332] Evaluating on the validation split.
I0914 06:13:44.506039 139758520063808 spec.py:348] Evaluating on the test split.
I0914 06:16:43.605504 139758520063808 submission_runner.py:376] Time since start: 6159.57s, 	Step: 5103, 	{'train/loss': 0.12174596225514131, 'validation/loss': 0.12403152808988764, 'validation/num_examples': 89000000, 'test/loss': 0.1272757345403712, 'test/num_examples': 89274637, 'score': 3628.1082146167755, 'total_duration': 6159.571078538895, 'accumulated_submission_time': 3628.1082146167755, 'accumulated_eval_time': 2531.318306684494, 'accumulated_logging_time': 0.07555937767028809}
I0914 06:16:43.623295 139574265235200 logging_writer.py:48] [5103] accumulated_eval_time=2531.318307, accumulated_logging_time=0.075559, accumulated_submission_time=3628.108215, global_step=5103, preemption_count=0, score=3628.108215, test/loss=0.127276, test/num_examples=89274637, total_duration=6159.571079, train/loss=0.121746, validation/loss=0.124032, validation/num_examples=89000000
I0914 06:21:07.178861 139574273627904 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.011122309602797031, loss=0.11312489956617355
I0914 06:27:02.334191 139574265235200 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005166520830243826, loss=0.11836463958024979
I0914 06:33:01.719633 139574273627904 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.01643657311797142, loss=0.12314831465482712
I0914 06:36:43.963366 139758520063808 spec.py:320] Evaluating on the training split.
I0914 06:39:52.736147 139758520063808 spec.py:332] Evaluating on the validation split.
I0914 06:42:59.786263 139758520063808 spec.py:348] Evaluating on the test split.
I0914 06:45:40.081627 139758520063808 submission_runner.py:376] Time since start: 7896.05s, 	Step: 6807, 	{'train/loss': 0.12207242180319393, 'validation/loss': 0.12366040449438202, 'validation/num_examples': 89000000, 'test/loss': 0.12677098871877798, 'test/num_examples': 89274637, 'score': 4828.41726064682, 'total_duration': 7896.047209262848, 'accumulated_submission_time': 4828.41726064682, 'accumulated_eval_time': 3067.436534643173, 'accumulated_logging_time': 0.10125041007995605}
I0914 06:45:40.099912 139574265235200 logging_writer.py:48] [6807] accumulated_eval_time=3067.436535, accumulated_logging_time=0.101250, accumulated_submission_time=4828.417261, global_step=6807, preemption_count=0, score=4828.417261, test/loss=0.126771, test/num_examples=89274637, total_duration=7896.047209, train/loss=0.122072, validation/loss=0.123660, validation/num_examples=89000000
I0914 06:47:43.837101 139574273627904 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.006831914186477661, loss=0.1298542022705078
I0914 06:53:40.867326 139574265235200 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.006117559038102627, loss=0.12473693490028381
I0914 06:59:33.721864 139758520063808 spec.py:320] Evaluating on the training split.
I0914 07:02:09.370532 139758520063808 spec.py:332] Evaluating on the validation split.
I0914 07:04:29.769485 139758520063808 spec.py:348] Evaluating on the test split.
I0914 07:06:52.581684 139758520063808 submission_runner.py:376] Time since start: 9168.55s, 	Step: 8000, 	{'train/loss': 0.12188865437227137, 'validation/loss': 0.12360365168539326, 'validation/num_examples': 89000000, 'test/loss': 0.12666802554459, 'test/num_examples': 89274637, 'score': 5662.0161418914795, 'total_duration': 9168.547255516052, 'accumulated_submission_time': 5662.0161418914795, 'accumulated_eval_time': 3506.296296596527, 'accumulated_logging_time': 0.12683701515197754}
I0914 07:06:52.603952 139574273627904 logging_writer.py:48] [8000] accumulated_eval_time=3506.296297, accumulated_logging_time=0.126837, accumulated_submission_time=5662.016142, global_step=8000, preemption_count=0, score=5662.016142, test/loss=0.126668, test/num_examples=89274637, total_duration=9168.547256, train/loss=0.121889, validation/loss=0.123604, validation/num_examples=89000000
I0914 07:06:52.621972 139574265235200 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5662.016142
I0914 07:06:57.777957 139758520063808 checkpoints.py:490] Saving checkpoint at step: 8000
I0914 07:07:27.626955 139758520063808 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1/checkpoint_8000
I0914 07:07:27.830920 139758520063808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_jax/nadamw_run_0/criteo1tb_jax/trial_1/checkpoint_8000.
I0914 07:07:28.144519 139758520063808 submission_runner.py:540] Tuning trial 1/1
I0914 07:07:28.144763 139758520063808 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I0914 07:07:28.145817 139758520063808 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/loss': 1.8823116526884192, 'validation/loss': 1.867852404494382, 'validation/num_examples': 89000000, 'test/loss': 1.8833504077983538, 'test/num_examples': 89274637, 'score': 26.919605493545532, 'total_duration': 788.556474685669, 'accumulated_submission_time': 26.919605493545532, 'accumulated_eval_time': 761.6368346214294, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1699, {'train/loss': 0.12231887368594899, 'validation/loss': 0.1253474606741573, 'validation/num_examples': 89000000, 'test/loss': 0.12844242648670753, 'test/num_examples': 89274637, 'score': 1227.4699466228485, 'total_duration': 2610.5607631206512, 'accumulated_submission_time': 1227.4699466228485, 'accumulated_eval_time': 1383.0408952236176, 'accumulated_logging_time': 0.02660083770751953, 'global_step': 1699, 'preemption_count': 0}), (3396, {'train/loss': 0.12332801818847657, 'validation/loss': 0.12463929213483146, 'validation/num_examples': 89000000, 'test/loss': 0.1279797306820749, 'test/num_examples': 89274637, 'score': 2427.9822726249695, 'total_duration': 4385.630787849426, 'accumulated_submission_time': 2427.9822726249695, 'accumulated_eval_time': 1957.5488657951355, 'accumulated_logging_time': 0.05364203453063965, 'global_step': 3396, 'preemption_count': 0}), (5103, {'train/loss': 0.12174596225514131, 'validation/loss': 0.12403152808988764, 'validation/num_examples': 89000000, 'test/loss': 0.1272757345403712, 'test/num_examples': 89274637, 'score': 3628.1082146167755, 'total_duration': 6159.571078538895, 'accumulated_submission_time': 3628.1082146167755, 'accumulated_eval_time': 2531.318306684494, 'accumulated_logging_time': 0.07555937767028809, 'global_step': 5103, 'preemption_count': 0}), (6807, {'train/loss': 0.12207242180319393, 'validation/loss': 0.12366040449438202, 'validation/num_examples': 89000000, 'test/loss': 0.12677098871877798, 'test/num_examples': 89274637, 'score': 4828.41726064682, 'total_duration': 7896.047209262848, 'accumulated_submission_time': 4828.41726064682, 'accumulated_eval_time': 3067.436534643173, 'accumulated_logging_time': 0.10125041007995605, 'global_step': 6807, 'preemption_count': 0}), (8000, {'train/loss': 0.12188865437227137, 'validation/loss': 0.12360365168539326, 'validation/num_examples': 89000000, 'test/loss': 0.12666802554459, 'test/num_examples': 89274637, 'score': 5662.0161418914795, 'total_duration': 9168.547255516052, 'accumulated_submission_time': 5662.0161418914795, 'accumulated_eval_time': 3506.296296596527, 'accumulated_logging_time': 0.12683701515197754, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0914 07:07:28.145924 139758520063808 submission_runner.py:543] Timing: 5662.0161418914795
I0914 07:07:28.145976 139758520063808 submission_runner.py:545] Total number of evals: 6
I0914 07:07:28.146024 139758520063808 submission_runner.py:546] ====================
I0914 07:07:28.146119 139758520063808 submission_runner.py:614] Final criteo1tb score: 5662.0161418914795
