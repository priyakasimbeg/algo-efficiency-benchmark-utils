python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_jax/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_09-16-2023-15-51-59.log
2023-09-16 15:52:03.872554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0916 15:52:21.254816 140396256630592 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax.
I0916 15:52:22.253191 140396256630592 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0916 15:52:22.253880 140396256630592 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0916 15:52:22.254053 140396256630592 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0916 15:52:22.259562 140396256630592 submission_runner.py:500] Using RNG seed 1141393470
I0916 15:52:27.921041 140396256630592 submission_runner.py:509] --- Tuning run 1/1 ---
I0916 15:52:27.921238 140396256630592 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1.
I0916 15:52:27.921410 140396256630592 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1/hparams.json.
I0916 15:52:28.104996 140396256630592 submission_runner.py:185] Initializing dataset.
I0916 15:52:28.105204 140396256630592 submission_runner.py:192] Initializing model.
I0916 15:52:33.034319 140396256630592 submission_runner.py:226] Initializing optimizer.
I0916 15:52:34.306185 140396256630592 submission_runner.py:233] Initializing metrics bundle.
I0916 15:52:34.306422 140396256630592 submission_runner.py:251] Initializing checkpoint and logger.
I0916 15:52:34.307638 140396256630592 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0916 15:52:34.307901 140396256630592 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0916 15:52:34.307970 140396256630592 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0916 15:52:35.143615 140396256630592 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0916 15:52:35.144708 140396256630592 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1/flags_0.json.
I0916 15:52:35.159266 140396256630592 submission_runner.py:285] Starting training loop.
I0916 15:52:35.455635 140396256630592 input_pipeline.py:20] Loading split = train-clean-100
I0916 15:52:35.494511 140396256630592 input_pipeline.py:20] Loading split = train-clean-360
I0916 15:52:35.906109 140396256630592 input_pipeline.py:20] Loading split = train-other-500
2023-09-16 15:53:48.010250: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-09-16 15:53:51.016897: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0916 15:53:52.986123 140221186307840 logging_writer.py:48] [0] global_step=0, grad_norm=47.88982009887695, loss=32.71891784667969
I0916 15:53:53.026969 140396256630592 spec.py:320] Evaluating on the training split.
I0916 15:53:53.212724 140396256630592 input_pipeline.py:20] Loading split = train-clean-100
I0916 15:53:53.254304 140396256630592 input_pipeline.py:20] Loading split = train-clean-360
I0916 15:53:53.753861 140396256630592 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0916 15:55:12.045502 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 15:55:12.169425 140396256630592 input_pipeline.py:20] Loading split = dev-clean
I0916 15:55:12.175012 140396256630592 input_pipeline.py:20] Loading split = dev-other
I0916 15:56:06.613651 140396256630592 spec.py:348] Evaluating on the test split.
I0916 15:56:06.740666 140396256630592 input_pipeline.py:20] Loading split = test-clean
I0916 15:56:44.457428 140396256630592 submission_runner.py:376] Time since start: 249.30s, 	Step: 1, 	{'train/ctc_loss': Array(31.993778, dtype=float32), 'train/wer': 1.2477198425754326, 'validation/ctc_loss': Array(30.964561, dtype=float32), 'validation/wer': 1.1193354494495846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.096544, dtype=float32), 'test/wer': 1.1393577478520505, 'test/num_examples': 2472, 'score': 77.86762166023254, 'total_duration': 249.29528999328613, 'accumulated_submission_time': 77.86762166023254, 'accumulated_eval_time': 171.4276065826416, 'accumulated_logging_time': 0}
I0916 15:56:44.486609 140214878074624 logging_writer.py:48] [1] accumulated_eval_time=171.427607, accumulated_logging_time=0, accumulated_submission_time=77.867622, global_step=1, preemption_count=0, score=77.867622, test/ctc_loss=31.09654426574707, test/num_examples=2472, test/wer=1.139358, total_duration=249.295290, train/ctc_loss=31.993778228759766, train/wer=1.247720, validation/ctc_loss=30.964561462402344, validation/num_examples=5348, validation/wer=1.119335
I0916 15:57:09.051295 140222210451200 logging_writer.py:48] [1] global_step=1, grad_norm=41.80438232421875, loss=32.59353256225586
I0916 15:57:09.945613 140222218843904 logging_writer.py:48] [2] global_step=2, grad_norm=45.86474609375, loss=32.39568328857422
I0916 15:57:10.780051 140222210451200 logging_writer.py:48] [3] global_step=3, grad_norm=43.8601188659668, loss=33.037994384765625
I0916 15:57:11.614129 140222218843904 logging_writer.py:48] [4] global_step=4, grad_norm=45.795780181884766, loss=31.9683895111084
I0916 15:57:12.509172 140222210451200 logging_writer.py:48] [5] global_step=5, grad_norm=48.374794006347656, loss=32.35086441040039
I0916 15:57:13.404699 140222218843904 logging_writer.py:48] [6] global_step=6, grad_norm=57.58542251586914, loss=32.202083587646484
I0916 15:57:14.294979 140222210451200 logging_writer.py:48] [7] global_step=7, grad_norm=59.41910934448242, loss=32.23814392089844
I0916 15:57:15.187730 140222218843904 logging_writer.py:48] [8] global_step=8, grad_norm=71.95597076416016, loss=32.36720657348633
I0916 15:57:16.085108 140222210451200 logging_writer.py:48] [9] global_step=9, grad_norm=65.5030517578125, loss=31.612510681152344
I0916 15:57:16.976327 140222218843904 logging_writer.py:48] [10] global_step=10, grad_norm=78.92755126953125, loss=31.875137329101562
I0916 15:57:17.860087 140222210451200 logging_writer.py:48] [11] global_step=11, grad_norm=77.5947036743164, loss=30.778507232666016
I0916 15:57:18.755528 140222218843904 logging_writer.py:48] [12] global_step=12, grad_norm=80.99022674560547, loss=31.23314666748047
I0916 15:57:19.650838 140222210451200 logging_writer.py:48] [13] global_step=13, grad_norm=87.70283508300781, loss=30.486473083496094
I0916 15:57:20.540500 140222218843904 logging_writer.py:48] [14] global_step=14, grad_norm=89.28199768066406, loss=30.2841854095459
I0916 15:57:21.426084 140222210451200 logging_writer.py:48] [15] global_step=15, grad_norm=96.43309783935547, loss=31.38323402404785
I0916 15:57:22.317480 140222218843904 logging_writer.py:48] [16] global_step=16, grad_norm=87.57160949707031, loss=30.498798370361328
I0916 15:57:23.213853 140222210451200 logging_writer.py:48] [17] global_step=17, grad_norm=95.85110473632812, loss=29.76289939880371
I0916 15:57:24.100487 140222218843904 logging_writer.py:48] [18] global_step=18, grad_norm=87.64103698730469, loss=29.119298934936523
I0916 15:57:24.983857 140222210451200 logging_writer.py:48] [19] global_step=19, grad_norm=94.5326156616211, loss=28.56586456298828
I0916 15:57:25.876795 140222218843904 logging_writer.py:48] [20] global_step=20, grad_norm=92.23448944091797, loss=28.376235961914062
I0916 15:57:26.772855 140222210451200 logging_writer.py:48] [21] global_step=21, grad_norm=98.10153198242188, loss=27.007205963134766
I0916 15:57:27.670199 140222218843904 logging_writer.py:48] [22] global_step=22, grad_norm=91.74012756347656, loss=27.256868362426758
I0916 15:57:28.554505 140222210451200 logging_writer.py:48] [23] global_step=23, grad_norm=82.90752410888672, loss=26.674301147460938
I0916 15:57:29.442272 140222218843904 logging_writer.py:48] [24] global_step=24, grad_norm=83.71852111816406, loss=26.01483726501465
I0916 15:57:30.335303 140222210451200 logging_writer.py:48] [25] global_step=25, grad_norm=75.05093383789062, loss=26.648256301879883
I0916 15:57:31.233327 140222218843904 logging_writer.py:48] [26] global_step=26, grad_norm=74.34650421142578, loss=25.382619857788086
I0916 15:57:32.118894 140222210451200 logging_writer.py:48] [27] global_step=27, grad_norm=77.47543334960938, loss=24.75296974182129
I0916 15:57:33.008147 140222218843904 logging_writer.py:48] [28] global_step=28, grad_norm=69.11598205566406, loss=24.058223724365234
I0916 15:57:33.900027 140222210451200 logging_writer.py:48] [29] global_step=29, grad_norm=70.85040283203125, loss=23.686363220214844
I0916 15:57:34.791270 140222218843904 logging_writer.py:48] [30] global_step=30, grad_norm=61.11090087890625, loss=23.380189895629883
I0916 15:57:35.677142 140222210451200 logging_writer.py:48] [31] global_step=31, grad_norm=57.1682243347168, loss=23.17801284790039
I0916 15:57:36.564735 140222218843904 logging_writer.py:48] [32] global_step=32, grad_norm=49.79987716674805, loss=23.350622177124023
I0916 15:57:37.457143 140222210451200 logging_writer.py:48] [33] global_step=33, grad_norm=42.18513488769531, loss=23.179967880249023
I0916 15:57:38.344189 140222218843904 logging_writer.py:48] [34] global_step=34, grad_norm=41.18377685546875, loss=22.519485473632812
I0916 15:57:39.226534 140222210451200 logging_writer.py:48] [35] global_step=35, grad_norm=42.27163314819336, loss=21.50560760498047
I0916 15:57:40.116233 140222218843904 logging_writer.py:48] [36] global_step=36, grad_norm=41.23080062866211, loss=22.658313751220703
I0916 15:57:41.014824 140222210451200 logging_writer.py:48] [37] global_step=37, grad_norm=39.1027717590332, loss=22.43707275390625
I0916 15:57:41.899173 140222218843904 logging_writer.py:48] [38] global_step=38, grad_norm=43.63450241088867, loss=22.064414978027344
I0916 15:57:42.785165 140222210451200 logging_writer.py:48] [39] global_step=39, grad_norm=35.67398452758789, loss=20.48158073425293
I0916 15:57:43.679501 140222218843904 logging_writer.py:48] [40] global_step=40, grad_norm=37.23030471801758, loss=19.793310165405273
I0916 15:57:44.563942 140222210451200 logging_writer.py:48] [41] global_step=41, grad_norm=42.82830810546875, loss=20.95623207092285
I0916 15:57:45.459715 140222218843904 logging_writer.py:48] [42] global_step=42, grad_norm=50.024810791015625, loss=20.70025062561035
I0916 15:57:46.356621 140222210451200 logging_writer.py:48] [43] global_step=43, grad_norm=60.140663146972656, loss=21.75779914855957
I0916 15:57:47.247420 140222218843904 logging_writer.py:48] [44] global_step=44, grad_norm=53.526798248291016, loss=19.9539794921875
I0916 15:57:48.145260 140222210451200 logging_writer.py:48] [45] global_step=45, grad_norm=68.54387664794922, loss=21.6304931640625
I0916 15:57:49.046334 140222218843904 logging_writer.py:48] [46] global_step=46, grad_norm=50.61729049682617, loss=19.609418869018555
I0916 15:57:49.942014 140222210451200 logging_writer.py:48] [47] global_step=47, grad_norm=73.34642791748047, loss=22.095035552978516
I0916 15:57:50.830265 140222218843904 logging_writer.py:48] [48] global_step=48, grad_norm=67.6826400756836, loss=20.46070098876953
I0916 15:57:51.728074 140222210451200 logging_writer.py:48] [49] global_step=49, grad_norm=68.63054656982422, loss=21.21995735168457
I0916 15:57:52.630387 140222218843904 logging_writer.py:48] [50] global_step=50, grad_norm=54.27874755859375, loss=18.35454750061035
I0916 15:57:53.524487 140222210451200 logging_writer.py:48] [51] global_step=51, grad_norm=55.620235443115234, loss=19.680408477783203
I0916 15:57:54.420776 140222218843904 logging_writer.py:48] [52] global_step=52, grad_norm=66.93803405761719, loss=20.89303207397461
I0916 15:57:55.315253 140222210451200 logging_writer.py:48] [53] global_step=53, grad_norm=42.36565399169922, loss=18.313552856445312
I0916 15:57:56.216694 140222218843904 logging_writer.py:48] [54] global_step=54, grad_norm=37.874549865722656, loss=18.354280471801758
I0916 15:57:57.109491 140222210451200 logging_writer.py:48] [55] global_step=55, grad_norm=51.441993713378906, loss=18.415483474731445
I0916 15:57:58.008275 140222218843904 logging_writer.py:48] [56] global_step=56, grad_norm=31.015310287475586, loss=17.055150985717773
I0916 15:57:58.902686 140222210451200 logging_writer.py:48] [57] global_step=57, grad_norm=35.30740737915039, loss=17.86581039428711
I0916 15:57:59.806374 140222218843904 logging_writer.py:48] [58] global_step=58, grad_norm=29.74829864501953, loss=16.656980514526367
I0916 15:58:00.696218 140222210451200 logging_writer.py:48] [59] global_step=59, grad_norm=25.25908660888672, loss=16.274639129638672
I0916 15:58:01.589704 140222218843904 logging_writer.py:48] [60] global_step=60, grad_norm=15.941937446594238, loss=16.467788696289062
I0916 15:58:02.487801 140222210451200 logging_writer.py:48] [61] global_step=61, grad_norm=17.395214080810547, loss=16.28422737121582
I0916 15:58:03.389236 140222218843904 logging_writer.py:48] [62] global_step=62, grad_norm=19.294069290161133, loss=16.260353088378906
I0916 15:58:04.278727 140222210451200 logging_writer.py:48] [63] global_step=63, grad_norm=17.172283172607422, loss=17.01291847229004
I0916 15:58:05.177171 140222218843904 logging_writer.py:48] [64] global_step=64, grad_norm=19.481277465820312, loss=17.456796646118164
I0916 15:58:06.070995 140222210451200 logging_writer.py:48] [65] global_step=65, grad_norm=36.00263214111328, loss=16.898094177246094
I0916 15:58:06.967996 140222218843904 logging_writer.py:48] [66] global_step=66, grad_norm=28.698246002197266, loss=16.57333755493164
I0916 15:58:07.863238 140222210451200 logging_writer.py:48] [67] global_step=67, grad_norm=23.66407585144043, loss=15.951407432556152
I0916 15:58:08.760295 140222218843904 logging_writer.py:48] [68] global_step=68, grad_norm=18.095441818237305, loss=16.92139434814453
I0916 15:58:09.657744 140222210451200 logging_writer.py:48] [69] global_step=69, grad_norm=15.308204650878906, loss=16.17679214477539
I0916 15:58:10.560968 140222218843904 logging_writer.py:48] [70] global_step=70, grad_norm=17.128408432006836, loss=16.125526428222656
I0916 15:58:11.451560 140222210451200 logging_writer.py:48] [71] global_step=71, grad_norm=21.824825286865234, loss=15.843648910522461
I0916 15:58:12.353867 140222218843904 logging_writer.py:48] [72] global_step=72, grad_norm=31.42038917541504, loss=17.00558090209961
I0916 15:58:13.253556 140222210451200 logging_writer.py:48] [73] global_step=73, grad_norm=25.396093368530273, loss=15.528741836547852
I0916 15:58:14.145396 140222218843904 logging_writer.py:48] [74] global_step=74, grad_norm=31.44540023803711, loss=16.001483917236328
I0916 15:58:15.039042 140222210451200 logging_writer.py:48] [75] global_step=75, grad_norm=39.43450927734375, loss=16.53580665588379
I0916 15:58:15.943072 140222218843904 logging_writer.py:48] [76] global_step=76, grad_norm=41.78227615356445, loss=16.284469604492188
I0916 15:58:16.838377 140222210451200 logging_writer.py:48] [77] global_step=77, grad_norm=44.08831024169922, loss=16.853179931640625
I0916 15:58:17.738824 140222218843904 logging_writer.py:48] [78] global_step=78, grad_norm=30.69153594970703, loss=15.155893325805664
I0916 15:58:18.630427 140222210451200 logging_writer.py:48] [79] global_step=79, grad_norm=41.20859146118164, loss=16.201826095581055
I0916 15:58:19.534162 140222218843904 logging_writer.py:48] [80] global_step=80, grad_norm=46.22398376464844, loss=16.977733612060547
I0916 15:58:20.437531 140222210451200 logging_writer.py:48] [81] global_step=81, grad_norm=46.68379592895508, loss=16.545440673828125
I0916 15:58:21.340539 140222218843904 logging_writer.py:48] [82] global_step=82, grad_norm=48.2606315612793, loss=17.388914108276367
I0916 15:58:22.244839 140222210451200 logging_writer.py:48] [83] global_step=83, grad_norm=42.84638977050781, loss=16.419557571411133
I0916 15:58:23.152744 140222218843904 logging_writer.py:48] [84] global_step=84, grad_norm=30.818422317504883, loss=15.349801063537598
I0916 15:58:24.054474 140222210451200 logging_writer.py:48] [85] global_step=85, grad_norm=27.074424743652344, loss=14.935382843017578
I0916 15:58:24.955552 140222218843904 logging_writer.py:48] [86] global_step=86, grad_norm=32.84474182128906, loss=16.11701202392578
I0916 15:58:25.858694 140222210451200 logging_writer.py:48] [87] global_step=87, grad_norm=26.220211029052734, loss=15.138758659362793
I0916 15:58:26.762579 140222218843904 logging_writer.py:48] [88] global_step=88, grad_norm=22.96843910217285, loss=14.85989761352539
I0916 15:58:27.672506 140222210451200 logging_writer.py:48] [89] global_step=89, grad_norm=28.741552352905273, loss=15.591696739196777
I0916 15:58:28.569898 140222218843904 logging_writer.py:48] [90] global_step=90, grad_norm=20.869848251342773, loss=14.705872535705566
I0916 15:58:29.465423 140222210451200 logging_writer.py:48] [91] global_step=91, grad_norm=16.875526428222656, loss=14.070954322814941
I0916 15:58:30.367506 140222218843904 logging_writer.py:48] [92] global_step=92, grad_norm=20.56707191467285, loss=15.044540405273438
I0916 15:58:31.272168 140222210451200 logging_writer.py:48] [93] global_step=93, grad_norm=18.432466506958008, loss=14.839476585388184
I0916 15:58:32.173123 140222218843904 logging_writer.py:48] [94] global_step=94, grad_norm=21.291229248046875, loss=14.653079986572266
I0916 15:58:33.067566 140222210451200 logging_writer.py:48] [95] global_step=95, grad_norm=21.81970977783203, loss=14.312810897827148
I0916 15:58:33.962497 140222218843904 logging_writer.py:48] [96] global_step=96, grad_norm=26.867143630981445, loss=13.949854850769043
I0916 15:58:34.872734 140222210451200 logging_writer.py:48] [97] global_step=97, grad_norm=29.995975494384766, loss=14.452001571655273
I0916 15:58:35.771487 140222218843904 logging_writer.py:48] [98] global_step=98, grad_norm=32.04046630859375, loss=14.062198638916016
I0916 15:58:36.660283 140222210451200 logging_writer.py:48] [99] global_step=99, grad_norm=31.358306884765625, loss=14.295759201049805
I0916 15:58:37.561752 140222218843904 logging_writer.py:48] [100] global_step=100, grad_norm=32.04213333129883, loss=14.163113594055176
I0916 16:03:43.964824 140222210451200 logging_writer.py:48] [500] global_step=500, grad_norm=0.2726088762283325, loss=5.83836030960083
I0916 16:10:38.062969 140222218843904 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4361157715320587, loss=5.807092189788818
I0916 16:17:07.073145 140216305272576 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7561950087547302, loss=5.798033714294434
I0916 16:20:44.637371 140396256630592 spec.py:320] Evaluating on the training split.
I0916 16:21:22.113875 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 16:22:07.733636 140396256630592 spec.py:348] Evaluating on the test split.
I0916 16:22:30.619970 140396256630592 submission_runner.py:376] Time since start: 1795.45s, 	Step: 1754, 	{'train/ctc_loss': Array(6.0282903, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.1358495, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.1042094, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1517.971349954605, 'total_duration': 1795.4528942108154, 'accumulated_submission_time': 1517.971349954605, 'accumulated_eval_time': 277.40246629714966, 'accumulated_logging_time': 0.04566502571105957}
I0916 16:22:30.658415 140216305272576 logging_writer.py:48] [1754] accumulated_eval_time=277.402466, accumulated_logging_time=0.045665, accumulated_submission_time=1517.971350, global_step=1754, preemption_count=0, score=1517.971350, test/ctc_loss=6.1042094230651855, test/num_examples=2472, test/wer=0.899580, total_duration=1795.452894, train/ctc_loss=6.028290271759033, train/wer=0.944636, validation/ctc_loss=6.135849475860596, validation/num_examples=5348, validation/wer=0.895995
I0916 16:25:40.462307 140216296879872 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1417680978775024, loss=5.55832052230835
I0916 16:32:09.660109 140216305272576 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6661387085914612, loss=3.7581613063812256
I0916 16:39:21.423165 140216296879872 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8809261322021484, loss=3.027317523956299
I0916 16:45:55.354083 140223588312832 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9650667905807495, loss=2.67779541015625
I0916 16:46:30.797585 140396256630592 spec.py:320] Evaluating on the training split.
I0916 16:47:24.631358 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 16:48:16.810858 140396256630592 spec.py:348] Evaluating on the test split.
I0916 16:48:43.030375 140396256630592 submission_runner.py:376] Time since start: 3367.87s, 	Step: 3544, 	{'train/ctc_loss': Array(2.6819842, dtype=float32), 'train/wer': 0.6688810164344885, 'validation/ctc_loss': Array(2.9259045, dtype=float32), 'validation/wer': 0.6682360659533618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6804616, dtype=float32), 'test/wer': 0.6440801901163854, 'test/num_examples': 2472, 'score': 2958.0613248348236, 'total_duration': 3367.86585688591, 'accumulated_submission_time': 2958.0613248348236, 'accumulated_eval_time': 409.6300756931305, 'accumulated_logging_time': 0.10097885131835938}
I0916 16:48:43.069954 140223588312832 logging_writer.py:48] [3544] accumulated_eval_time=409.630076, accumulated_logging_time=0.100979, accumulated_submission_time=2958.061325, global_step=3544, preemption_count=0, score=2958.061325, test/ctc_loss=2.6804616451263428, test/num_examples=2472, test/wer=0.644080, total_duration=3367.865857, train/ctc_loss=2.6819841861724854, train/wer=0.668881, validation/ctc_loss=2.9259045124053955, validation/num_examples=5348, validation/wer=0.668236
I0916 16:54:48.087262 140223579920128 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1735206842422485, loss=2.486729860305786
I0916 17:01:24.108925 140222932952832 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9325137138366699, loss=2.3605101108551025
I0916 17:08:36.262048 140222924560128 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.0777688026428223, loss=2.2566137313842773
I0916 17:12:43.235283 140396256630592 spec.py:320] Evaluating on the training split.
I0916 17:13:37.133871 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 17:14:28.831663 140396256630592 spec.py:348] Evaluating on the test split.
I0916 17:14:54.484230 140396256630592 submission_runner.py:376] Time since start: 4939.32s, 	Step: 5300, 	{'train/ctc_loss': Array(0.8298036, dtype=float32), 'train/wer': 0.27917200373199236, 'validation/ctc_loss': Array(1.1898717, dtype=float32), 'validation/wer': 0.3423091394996575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.88903236, dtype=float32), 'test/wer': 0.2833059127008307, 'test/num_examples': 2472, 'score': 4398.178380012512, 'total_duration': 4939.317852497101, 'accumulated_submission_time': 4398.178380012512, 'accumulated_eval_time': 540.8720512390137, 'accumulated_logging_time': 0.15682339668273926}
I0916 17:14:54.517165 140223588312832 logging_writer.py:48] [5300] accumulated_eval_time=540.872051, accumulated_logging_time=0.156823, accumulated_submission_time=4398.178380, global_step=5300, preemption_count=0, score=4398.178380, test/ctc_loss=0.8890323638916016, test/num_examples=2472, test/wer=0.283306, total_duration=4939.317852, train/ctc_loss=0.8298035860061646, train/wer=0.279172, validation/ctc_loss=1.1898716688156128, validation/num_examples=5348, validation/wer=0.342309
I0916 17:17:27.461912 140223579920128 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0479815006256104, loss=2.105618476867676
I0916 17:24:30.045825 140223588312832 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6898428797721863, loss=2.0004515647888184
I0916 17:31:19.211390 140223588312832 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.0566792488098145, loss=1.9754726886749268
I0916 17:38:25.695361 140223579920128 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0669946670532227, loss=1.9162075519561768
I0916 17:38:54.510508 140396256630592 spec.py:320] Evaluating on the training split.
I0916 17:39:48.261348 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 17:40:40.397262 140396256630592 spec.py:348] Evaluating on the test split.
I0916 17:41:06.656788 140396256630592 submission_runner.py:376] Time since start: 6511.49s, 	Step: 7035, 	{'train/ctc_loss': Array(0.6160929, dtype=float32), 'train/wer': 0.20914974499680591, 'validation/ctc_loss': Array(0.9512876, dtype=float32), 'validation/wer': 0.2771083174946213, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6582261, dtype=float32), 'test/wer': 0.21260130400341234, 'test/num_examples': 2472, 'score': 5838.125876188278, 'total_duration': 6511.488792181015, 'accumulated_submission_time': 5838.125876188278, 'accumulated_eval_time': 673.0096666812897, 'accumulated_logging_time': 0.20478272438049316}
I0916 17:41:06.693245 140223296472832 logging_writer.py:48] [7035] accumulated_eval_time=673.009667, accumulated_logging_time=0.204783, accumulated_submission_time=5838.125876, global_step=7035, preemption_count=0, score=5838.125876, test/ctc_loss=0.6582260727882385, test/num_examples=2472, test/wer=0.212601, total_duration=6511.488792, train/ctc_loss=0.6160929203033447, train/wer=0.209150, validation/ctc_loss=0.9512876272201538, validation/num_examples=5348, validation/wer=0.277108
I0916 17:47:07.741762 140222968792832 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8690235018730164, loss=1.90325927734375
I0916 17:54:19.597539 140222960400128 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.2782291173934937, loss=1.9411883354187012
I0916 18:01:12.982456 140223296472832 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7076976895332336, loss=1.7803236246109009
I0916 18:05:06.733363 140396256630592 spec.py:320] Evaluating on the training split.
I0916 18:05:59.437311 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 18:06:51.167933 140396256630592 spec.py:348] Evaluating on the test split.
I0916 18:07:17.288042 140396256630592 submission_runner.py:376] Time since start: 8082.12s, 	Step: 8779, 	{'train/ctc_loss': Array(0.50635326, dtype=float32), 'train/wer': 0.1764443671312705, 'validation/ctc_loss': Array(0.8235931, dtype=float32), 'validation/wer': 0.24602263408233557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5515629, dtype=float32), 'test/wer': 0.1853025409786119, 'test/num_examples': 2472, 'score': 7278.118975162506, 'total_duration': 8082.120104789734, 'accumulated_submission_time': 7278.118975162506, 'accumulated_eval_time': 803.5557680130005, 'accumulated_logging_time': 0.25684642791748047}
I0916 18:07:17.324349 140223296472832 logging_writer.py:48] [8779] accumulated_eval_time=803.555768, accumulated_logging_time=0.256846, accumulated_submission_time=7278.118975, global_step=8779, preemption_count=0, score=7278.118975, test/ctc_loss=0.5515629053115845, test/num_examples=2472, test/wer=0.185303, total_duration=8082.120105, train/ctc_loss=0.5063532590866089, train/wer=0.176444, validation/ctc_loss=0.8235930800437927, validation/num_examples=5348, validation/wer=0.246023
I0916 18:10:05.651583 140223288080128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9949161410331726, loss=1.8311208486557007
I0916 18:16:58.873899 140223296472832 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.4803814888000488, loss=1.8235350847244263
I0916 18:24:02.318713 140223288080128 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7086116075515747, loss=1.79795241355896
I0916 18:31:02.972164 140223296472832 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9551400542259216, loss=1.7077192068099976
I0916 18:31:17.932123 140396256630592 spec.py:320] Evaluating on the training split.
I0916 18:32:11.680160 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 18:33:03.814573 140396256630592 spec.py:348] Evaluating on the test split.
I0916 18:33:29.776053 140396256630592 submission_runner.py:376] Time since start: 9654.61s, 	Step: 10521, 	{'train/ctc_loss': Array(0.49570626, dtype=float32), 'train/wer': 0.17184757616657154, 'validation/ctc_loss': Array(0.7927408, dtype=float32), 'validation/wer': 0.23569933139731208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5140534, dtype=float32), 'test/wer': 0.17195783316068491, 'test/num_examples': 2472, 'score': 8718.680030345917, 'total_duration': 9654.611117124557, 'accumulated_submission_time': 8718.680030345917, 'accumulated_eval_time': 935.3940994739532, 'accumulated_logging_time': 0.30868983268737793}
I0916 18:33:29.809606 140223296472832 logging_writer.py:48] [10521] accumulated_eval_time=935.394099, accumulated_logging_time=0.308690, accumulated_submission_time=8718.680030, global_step=10521, preemption_count=0, score=8718.680030, test/ctc_loss=0.5140534043312073, test/num_examples=2472, test/wer=0.171958, total_duration=9654.611117, train/ctc_loss=0.4957062602043152, train/wer=0.171848, validation/ctc_loss=0.7927408218383789, validation/num_examples=5348, validation/wer=0.235699
I0916 18:39:56.870887 140223288080128 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7436590194702148, loss=1.7728599309921265
I0916 18:46:59.263557 140223296472832 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6633532643318176, loss=1.7532553672790527
I0916 18:54:03.612806 140223288080128 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9059620499610901, loss=1.8145859241485596
I0916 18:57:29.774232 140396256630592 spec.py:320] Evaluating on the training split.
I0916 18:58:23.503367 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 18:59:14.628388 140396256630592 spec.py:348] Evaluating on the test split.
I0916 18:59:40.486180 140396256630592 submission_runner.py:376] Time since start: 11225.32s, 	Step: 12238, 	{'train/ctc_loss': Array(0.4280237, dtype=float32), 'train/wer': 0.15185161551060344, 'validation/ctc_loss': Array(0.7326277, dtype=float32), 'validation/wer': 0.22067747879863772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47017014, dtype=float32), 'test/wer': 0.1587349948205472, 'test/num_examples': 2472, 'score': 10158.598539113998, 'total_duration': 11225.319172859192, 'accumulated_submission_time': 10158.598539113998, 'accumulated_eval_time': 1066.098382472992, 'accumulated_logging_time': 0.3575417995452881}
I0916 18:59:40.520596 140223373272832 logging_writer.py:48] [12238] accumulated_eval_time=1066.098382, accumulated_logging_time=0.357542, accumulated_submission_time=10158.598539, global_step=12238, preemption_count=0, score=10158.598539, test/ctc_loss=0.47017014026641846, test/num_examples=2472, test/wer=0.158735, total_duration=11225.319173, train/ctc_loss=0.42802369594573975, train/wer=0.151852, validation/ctc_loss=0.7326276898384094, validation/num_examples=5348, validation/wer=0.220677
I0916 19:03:04.552856 140223373272832 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5976861119270325, loss=1.77267324924469
I0916 19:10:04.028058 140223364880128 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6858277916908264, loss=1.6370575428009033
I0916 19:17:18.754609 140223373272832 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6698925495147705, loss=1.6900625228881836
I0916 19:23:40.652090 140396256630592 spec.py:320] Evaluating on the training split.
I0916 19:24:35.345639 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 19:25:26.626423 140396256630592 spec.py:348] Evaluating on the test split.
I0916 19:25:52.765187 140396256630592 submission_runner.py:376] Time since start: 12797.60s, 	Step: 13965, 	{'train/ctc_loss': Array(0.36184743, dtype=float32), 'train/wer': 0.13320679164824878, 'validation/ctc_loss': Array(0.6875857, dtype=float32), 'validation/wer': 0.20737296066532238, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4393874, dtype=float32), 'test/wer': 0.1492494871326143, 'test/num_examples': 2472, 'score': 11598.683979988098, 'total_duration': 12797.59767794609, 'accumulated_submission_time': 11598.683979988098, 'accumulated_eval_time': 1198.203332901001, 'accumulated_logging_time': 0.4070308208465576}
I0916 19:25:52.885774 140223081432832 logging_writer.py:48] [13965] accumulated_eval_time=1198.203333, accumulated_logging_time=0.407031, accumulated_submission_time=11598.683980, global_step=13965, preemption_count=0, score=11598.683980, test/ctc_loss=0.43938741087913513, test/num_examples=2472, test/wer=0.149249, total_duration=12797.597678, train/ctc_loss=0.3618474304676056, train/wer=0.133207, validation/ctc_loss=0.687585711479187, validation/num_examples=5348, validation/wer=0.207373
I0916 19:26:20.349766 140223073040128 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6028633713722229, loss=1.65656578540802
I0916 19:33:14.507981 140222426072832 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5577285885810852, loss=1.6279665231704712
I0916 19:40:08.664680 140222417680128 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8172418475151062, loss=1.5781433582305908
I0916 19:47:32.451685 140222426072832 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.74427729845047, loss=1.6367559432983398
I0916 19:49:52.816513 140396256630592 spec.py:320] Evaluating on the training split.
I0916 19:50:46.372002 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 19:51:37.433345 140396256630592 spec.py:348] Evaluating on the test split.
I0916 19:52:03.258222 140396256630592 submission_runner.py:376] Time since start: 14368.09s, 	Step: 15685, 	{'train/ctc_loss': Array(0.34628588, dtype=float32), 'train/wer': 0.12451521261484606, 'validation/ctc_loss': Array(0.66167533, dtype=float32), 'validation/wer': 0.19895995137434996, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42385334, dtype=float32), 'test/wer': 0.14311539008388682, 'test/num_examples': 2472, 'score': 13038.56060218811, 'total_duration': 14368.093508005142, 'accumulated_submission_time': 13038.56060218811, 'accumulated_eval_time': 1328.6396594047546, 'accumulated_logging_time': 0.5501930713653564}
I0916 19:52:03.293421 140222134224640 logging_writer.py:48] [15685] accumulated_eval_time=1328.639659, accumulated_logging_time=0.550193, accumulated_submission_time=13038.560602, global_step=15685, preemption_count=0, score=13038.560602, test/ctc_loss=0.42385333776474, test/num_examples=2472, test/wer=0.143115, total_duration=14368.093508, train/ctc_loss=0.346285879611969, train/wer=0.124515, validation/ctc_loss=0.6616753339767456, validation/num_examples=5348, validation/wer=0.198960
I0916 19:56:08.532536 140222125831936 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.2198835611343384, loss=1.5868103504180908
I0916 20:03:37.050806 140221806544640 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.8625365495681763, loss=1.5805256366729736
I0916 20:10:17.721958 140221798151936 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9604732990264893, loss=1.6319823265075684
I0916 20:16:03.948937 140396256630592 spec.py:320] Evaluating on the training split.
I0916 20:16:57.348870 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 20:17:49.149761 140396256630592 spec.py:348] Evaluating on the test split.
I0916 20:18:14.930962 140396256630592 submission_runner.py:376] Time since start: 15939.76s, 	Step: 17384, 	{'train/ctc_loss': Array(0.33084786, dtype=float32), 'train/wer': 0.12375380094064901, 'validation/ctc_loss': Array(0.639592, dtype=float32), 'validation/wer': 0.1921967409236944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40249684, dtype=float32), 'test/wer': 0.13716409725184328, 'test/num_examples': 2472, 'score': 14479.171476602554, 'total_duration': 15939.764713048935, 'accumulated_submission_time': 14479.171476602554, 'accumulated_eval_time': 1459.6147727966309, 'accumulated_logging_time': 0.5997335910797119}
I0916 20:18:14.967782 140221919184640 logging_writer.py:48] [17384] accumulated_eval_time=1459.614773, accumulated_logging_time=0.599734, accumulated_submission_time=14479.171477, global_step=17384, preemption_count=0, score=14479.171477, test/ctc_loss=0.4024968445301056, test/num_examples=2472, test/wer=0.137164, total_duration=15939.764713, train/ctc_loss=0.3308478593826294, train/wer=0.123754, validation/ctc_loss=0.6395919919013977, validation/num_examples=5348, validation/wer=0.192197
I0916 20:19:43.758013 140221910791936 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7434404492378235, loss=1.5455095767974854
I0916 20:26:30.418170 140221919184640 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.9145403504371643, loss=1.5876965522766113
I0916 20:33:52.308597 140221910791936 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7484233379364014, loss=1.5993390083312988
I0916 20:40:32.789702 140221919184640 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8492476940155029, loss=1.643080234527588
I0916 20:42:15.149714 140396256630592 spec.py:320] Evaluating on the training split.
I0916 20:43:09.029245 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 20:44:00.269562 140396256630592 spec.py:348] Evaluating on the test split.
I0916 20:44:26.353172 140396256630592 submission_runner.py:376] Time since start: 17511.19s, 	Step: 19119, 	{'train/ctc_loss': Array(0.3453708, dtype=float32), 'train/wer': 0.12373674608001269, 'validation/ctc_loss': Array(0.63859004, dtype=float32), 'validation/wer': 0.19033468726181632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39520597, dtype=float32), 'test/wer': 0.13360957081632238, 'test/num_examples': 2472, 'score': 15919.306365728378, 'total_duration': 17511.188287734985, 'accumulated_submission_time': 15919.306365728378, 'accumulated_eval_time': 1590.8126966953278, 'accumulated_logging_time': 0.6519181728363037}
I0916 20:44:26.392626 140222574552832 logging_writer.py:48] [19119] accumulated_eval_time=1590.812697, accumulated_logging_time=0.651918, accumulated_submission_time=15919.306366, global_step=19119, preemption_count=0, score=15919.306366, test/ctc_loss=0.3952059745788574, test/num_examples=2472, test/wer=0.133610, total_duration=17511.188288, train/ctc_loss=0.3453707993030548, train/wer=0.123737, validation/ctc_loss=0.6385900378227234, validation/num_examples=5348, validation/wer=0.190335
I0916 20:49:41.754554 140222566160128 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8589596152305603, loss=1.5794895887374878
I0916 20:56:30.297624 140222574552832 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6260959506034851, loss=1.5047723054885864
I0916 21:03:58.418396 140222566160128 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6015955209732056, loss=1.5126409530639648
I0916 21:08:26.788265 140396256630592 spec.py:320] Evaluating on the training split.
I0916 21:09:20.996651 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 21:10:12.935880 140396256630592 spec.py:348] Evaluating on the test split.
I0916 21:10:38.925297 140396256630592 submission_runner.py:376] Time since start: 19083.76s, 	Step: 20833, 	{'train/ctc_loss': Array(0.3405871, dtype=float32), 'train/wer': 0.12208299258712112, 'validation/ctc_loss': Array(0.6191101, dtype=float32), 'validation/wer': 0.1854721222587772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38428658, dtype=float32), 'test/wer': 0.1279426400991205, 'test/num_examples': 2472, 'score': 17359.654372692108, 'total_duration': 19083.759892225266, 'accumulated_submission_time': 17359.654372692108, 'accumulated_eval_time': 1722.9438631534576, 'accumulated_logging_time': 0.7070457935333252}
I0916 21:10:38.965548 140223588312832 logging_writer.py:48] [20833] accumulated_eval_time=1722.943863, accumulated_logging_time=0.707046, accumulated_submission_time=17359.654373, global_step=20833, preemption_count=0, score=17359.654373, test/ctc_loss=0.3842865824699402, test/num_examples=2472, test/wer=0.127943, total_duration=19083.759892, train/ctc_loss=0.3405871093273163, train/wer=0.122083, validation/ctc_loss=0.619110107421875, validation/num_examples=5348, validation/wer=0.185472
I0916 21:12:46.712364 140223579920128 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6354542374610901, loss=1.5053372383117676
I0916 21:19:54.959200 140223588312832 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6896682977676392, loss=1.5293983221054077
I0916 21:26:40.397202 140222717912832 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7706097364425659, loss=1.433837890625
I0916 21:33:57.945070 140222709520128 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8418660163879395, loss=1.568379521369934
I0916 21:34:39.527969 140396256630592 spec.py:320] Evaluating on the training split.
I0916 21:35:34.631809 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 21:36:25.920334 140396256630592 spec.py:348] Evaluating on the test split.
I0916 21:36:51.550350 140396256630592 submission_runner.py:376] Time since start: 20656.38s, 	Step: 22548, 	{'train/ctc_loss': Array(0.32231936, dtype=float32), 'train/wer': 0.1167506945158967, 'validation/ctc_loss': Array(0.60697156, dtype=float32), 'validation/wer': 0.18249090681048538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.373131, dtype=float32), 'test/wer': 0.1253630694859139, 'test/num_examples': 2472, 'score': 18800.1708650589, 'total_duration': 20656.38356256485, 'accumulated_submission_time': 18800.1708650589, 'accumulated_eval_time': 1854.9587969779968, 'accumulated_logging_time': 0.761925220489502}
I0916 21:36:51.588029 140223158232832 logging_writer.py:48] [22548] accumulated_eval_time=1854.958797, accumulated_logging_time=0.761925, accumulated_submission_time=18800.170865, global_step=22548, preemption_count=0, score=18800.170865, test/ctc_loss=0.37313100695610046, test/num_examples=2472, test/wer=0.125363, total_duration=20656.383563, train/ctc_loss=0.322319358587265, train/wer=0.116751, validation/ctc_loss=0.6069715619087219, validation/num_examples=5348, validation/wer=0.182491
I0916 21:42:45.422456 140222830552832 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.631832480430603, loss=1.5207160711288452
I0916 21:50:05.868554 140222822160128 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5881755948066711, loss=1.4713947772979736
I0916 21:56:58.924511 140222830552832 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8525115251541138, loss=1.463140845298767
I0916 22:00:51.942415 140396256630592 spec.py:320] Evaluating on the training split.
I0916 22:01:46.134271 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 22:02:38.381945 140396256630592 spec.py:348] Evaluating on the test split.
I0916 22:03:04.139922 140396256630592 submission_runner.py:376] Time since start: 22228.97s, 	Step: 24270, 	{'train/ctc_loss': Array(0.28810805, dtype=float32), 'train/wer': 0.10571238545669524, 'validation/ctc_loss': Array(0.5808014, dtype=float32), 'validation/wer': 0.17467607019845827, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3530474, dtype=float32), 'test/wer': 0.120102370361343, 'test/num_examples': 2472, 'score': 20240.479501485825, 'total_duration': 22228.974871873856, 'accumulated_submission_time': 20240.479501485825, 'accumulated_eval_time': 1987.1506052017212, 'accumulated_logging_time': 0.8146564960479736}
I0916 22:03:04.180102 140221919188736 logging_writer.py:48] [24270] accumulated_eval_time=1987.150605, accumulated_logging_time=0.814656, accumulated_submission_time=20240.479501, global_step=24270, preemption_count=0, score=20240.479501, test/ctc_loss=0.3530474007129669, test/num_examples=2472, test/wer=0.120102, total_duration=22228.974872, train/ctc_loss=0.28810805082321167, train/wer=0.105712, validation/ctc_loss=0.5808014273643494, validation/num_examples=5348, validation/wer=0.174676
I0916 22:05:59.450756 140221910796032 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5837600231170654, loss=1.5026249885559082
I0916 22:12:54.123819 140221919188736 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7092580199241638, loss=1.4520704746246338
I0916 22:20:09.779221 140221910796032 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.4728045463562012, loss=1.4539686441421509
I0916 22:27:04.642683 140396256630592 spec.py:320] Evaluating on the training split.
I0916 22:27:59.429703 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 22:28:50.846506 140396256630592 spec.py:348] Evaluating on the test split.
I0916 22:29:17.400820 140396256630592 submission_runner.py:376] Time since start: 23802.24s, 	Step: 25996, 	{'train/ctc_loss': Array(0.26905876, dtype=float32), 'train/wer': 0.09922011528730536, 'validation/ctc_loss': Array(0.5550443, dtype=float32), 'validation/wer': 0.16720855965807677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33755738, dtype=float32), 'test/wer': 0.1131964332866167, 'test/num_examples': 2472, 'score': 21680.89455127716, 'total_duration': 23802.236016511917, 'accumulated_submission_time': 21680.89455127716, 'accumulated_eval_time': 2119.9034657478333, 'accumulated_logging_time': 0.8712265491485596}
I0916 22:29:17.436877 140222246872832 logging_writer.py:48] [25996] accumulated_eval_time=2119.903466, accumulated_logging_time=0.871227, accumulated_submission_time=21680.894551, global_step=25996, preemption_count=0, score=21680.894551, test/ctc_loss=0.3375573754310608, test/num_examples=2472, test/wer=0.113196, total_duration=23802.236017, train/ctc_loss=0.2690587639808655, train/wer=0.099220, validation/ctc_loss=0.5550442934036255, validation/num_examples=5348, validation/wer=0.167209
I0916 22:29:21.343151 140222238480128 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7394706010818481, loss=1.4632503986358643
I0916 22:36:08.621839 140222246872832 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6831060647964478, loss=1.4281190633773804
I0916 22:43:07.305629 140221919192832 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7562003135681152, loss=1.3806216716766357
I0916 22:50:15.618553 140221910800128 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1407387256622314, loss=1.3754059076309204
I0916 22:53:18.444575 140396256630592 spec.py:320] Evaluating on the training split.
I0916 22:54:14.008270 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 22:55:05.476044 140396256630592 spec.py:348] Evaluating on the test split.
I0916 22:55:31.716324 140396256630592 submission_runner.py:376] Time since start: 25376.55s, 	Step: 27706, 	{'train/ctc_loss': Array(0.25408566, dtype=float32), 'train/wer': 0.09259391844207382, 'validation/ctc_loss': Array(0.53598136, dtype=float32), 'validation/wer': 0.161709230190354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3195851, dtype=float32), 'test/wer': 0.10797635732130888, 'test/num_examples': 2472, 'score': 23121.85632967949, 'total_duration': 25376.55044579506, 'accumulated_submission_time': 23121.85632967949, 'accumulated_eval_time': 2253.1686873435974, 'accumulated_logging_time': 0.9225003719329834}
I0916 22:55:31.753193 140223588312832 logging_writer.py:48] [27706] accumulated_eval_time=2253.168687, accumulated_logging_time=0.922500, accumulated_submission_time=23121.856330, global_step=27706, preemption_count=0, score=23121.856330, test/ctc_loss=0.3195851147174835, test/num_examples=2472, test/wer=0.107976, total_duration=25376.550446, train/ctc_loss=0.2540856599807739, train/wer=0.092594, validation/ctc_loss=0.5359813570976257, validation/num_examples=5348, validation/wer=0.161709
I0916 22:59:20.774083 140223588312832 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.171422004699707, loss=1.4137743711471558
I0916 23:06:27.182399 140223579920128 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7764943242073059, loss=1.4648220539093018
I0916 23:13:35.377844 140223260632832 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7000417709350586, loss=1.3801460266113281
I0916 23:19:32.020317 140396256630592 spec.py:320] Evaluating on the training split.
I0916 23:20:25.770237 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 23:21:16.622372 140396256630592 spec.py:348] Evaluating on the test split.
I0916 23:21:43.445897 140396256630592 submission_runner.py:376] Time since start: 26948.28s, 	Step: 29425, 	{'train/ctc_loss': Array(0.25806317, dtype=float32), 'train/wer': 0.09442169353440805, 'validation/ctc_loss': Array(0.511958, dtype=float32), 'validation/wer': 0.15412594429275728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30581048, dtype=float32), 'test/wer': 0.10206568764852843, 'test/num_examples': 2472, 'score': 24562.075604200363, 'total_duration': 26948.279821395874, 'accumulated_submission_time': 24562.075604200363, 'accumulated_eval_time': 2384.587548971176, 'accumulated_logging_time': 0.9760639667510986}
I0916 23:21:43.490644 140222753752832 logging_writer.py:48] [29425] accumulated_eval_time=2384.587549, accumulated_logging_time=0.976064, accumulated_submission_time=24562.075604, global_step=29425, preemption_count=0, score=24562.075604, test/ctc_loss=0.30581048130989075, test/num_examples=2472, test/wer=0.102066, total_duration=26948.279821, train/ctc_loss=0.2580631673336029, train/wer=0.094422, validation/ctc_loss=0.5119580030441284, validation/num_examples=5348, validation/wer=0.154126
I0916 23:22:41.204863 140222745360128 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6858652830123901, loss=1.39287531375885
I0916 23:29:40.989567 140223588312832 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7139570713043213, loss=1.3696271181106567
I0916 23:36:39.943286 140223579920128 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6678535342216492, loss=1.3322696685791016
I0916 23:44:02.301765 140222753752832 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7426906824111938, loss=1.378036379814148
I0916 23:45:43.679255 140396256630592 spec.py:320] Evaluating on the training split.
I0916 23:46:36.894331 140396256630592 spec.py:332] Evaluating on the validation split.
I0916 23:47:27.456527 140396256630592 spec.py:348] Evaluating on the test split.
I0916 23:47:53.561800 140396256630592 submission_runner.py:376] Time since start: 28518.40s, 	Step: 31135, 	{'train/ctc_loss': Array(0.2387584, dtype=float32), 'train/wer': 0.08508456367099117, 'validation/ctc_loss': Array(0.49811587, dtype=float32), 'validation/wer': 0.14883886964659573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29544124, dtype=float32), 'test/wer': 0.09855178437227063, 'test/num_examples': 2472, 'score': 26002.209939718246, 'total_duration': 28518.396995067596, 'accumulated_submission_time': 26002.209939718246, 'accumulated_eval_time': 2514.4646530151367, 'accumulated_logging_time': 1.0440433025360107}
I0916 23:47:53.597626 140222574552832 logging_writer.py:48] [31135] accumulated_eval_time=2514.464653, accumulated_logging_time=1.044043, accumulated_submission_time=26002.209940, global_step=31135, preemption_count=0, score=26002.209940, test/ctc_loss=0.29544124007225037, test/num_examples=2472, test/wer=0.098552, total_duration=28518.396995, train/ctc_loss=0.2387584000825882, train/wer=0.085085, validation/ctc_loss=0.4981158673763275, validation/num_examples=5348, validation/wer=0.148839
I0916 23:52:46.085682 140222566160128 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1700797080993652, loss=1.3499724864959717
I0917 00:00:06.259770 140222574552832 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.7147204279899597, loss=1.3123308420181274
I0917 00:06:58.893276 140222566160128 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7410888075828552, loss=1.3100605010986328
I0917 00:11:53.614876 140396256630592 spec.py:320] Evaluating on the training split.
I0917 00:12:48.507211 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 00:13:40.415234 140396256630592 spec.py:348] Evaluating on the test split.
I0917 00:14:06.446168 140396256630592 submission_runner.py:376] Time since start: 30091.28s, 	Step: 32823, 	{'train/ctc_loss': Array(0.24006222, dtype=float32), 'train/wer': 0.08529349819437096, 'validation/ctc_loss': Array(0.48322594, dtype=float32), 'validation/wer': 0.1445262375903289, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2839885, dtype=float32), 'test/wer': 0.09631751061280036, 'test/num_examples': 2472, 'score': 27442.179909706116, 'total_duration': 30091.28090572357, 'accumulated_submission_time': 27442.179909706116, 'accumulated_eval_time': 2647.29003405571, 'accumulated_logging_time': 1.0966832637786865}
I0917 00:14:06.483524 140223373272832 logging_writer.py:48] [32823] accumulated_eval_time=2647.290034, accumulated_logging_time=1.096683, accumulated_submission_time=27442.179910, global_step=32823, preemption_count=0, score=27442.179910, test/ctc_loss=0.28398850560188293, test/num_examples=2472, test/wer=0.096318, total_duration=30091.280906, train/ctc_loss=0.24006222188472748, train/wer=0.085293, validation/ctc_loss=0.48322594165802, validation/num_examples=5348, validation/wer=0.144526
I0917 00:16:25.462472 140222717912832 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.803459107875824, loss=1.3197475671768188
I0917 00:23:15.901477 140222709520128 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.8476354479789734, loss=1.3248772621154785
I0917 00:30:48.020959 140223373272832 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.705224871635437, loss=1.330080270767212
I0917 00:37:30.224671 140223364880128 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7428269982337952, loss=1.2993119955062866
I0917 00:38:07.143793 140396256630592 spec.py:320] Evaluating on the training split.
I0917 00:39:01.316089 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 00:39:53.368813 140396256630592 spec.py:348] Evaluating on the test split.
I0917 00:40:19.966166 140396256630592 submission_runner.py:376] Time since start: 31664.80s, 	Step: 34544, 	{'train/ctc_loss': Array(0.22858001, dtype=float32), 'train/wer': 0.07955817001822309, 'validation/ctc_loss': Array(0.46635282, dtype=float32), 'validation/wer': 0.14082142615944196, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27313003, dtype=float32), 'test/wer': 0.091930209412386, 'test/num_examples': 2472, 'score': 28882.79427242279, 'total_duration': 31664.800646543503, 'accumulated_submission_time': 28882.79427242279, 'accumulated_eval_time': 2780.1062252521515, 'accumulated_logging_time': 1.1494791507720947}
I0917 00:40:20.007941 140222789592832 logging_writer.py:48] [34544] accumulated_eval_time=2780.106225, accumulated_logging_time=1.149479, accumulated_submission_time=28882.794272, global_step=34544, preemption_count=0, score=28882.794272, test/ctc_loss=0.27313002943992615, test/num_examples=2472, test/wer=0.091930, total_duration=31664.800647, train/ctc_loss=0.22858001291751862, train/wer=0.079558, validation/ctc_loss=0.46635282039642334, validation/num_examples=5348, validation/wer=0.140821
I0917 00:46:34.731286 140222781200128 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.9614691138267517, loss=1.3427894115447998
I0917 00:53:18.276710 140222789592832 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.826221227645874, loss=1.2218971252441406
I0917 01:00:46.518490 140222781200128 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7385772466659546, loss=1.3188618421554565
I0917 01:04:19.991841 140396256630592 spec.py:320] Evaluating on the training split.
I0917 01:05:15.673091 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 01:06:06.898859 140396256630592 spec.py:348] Evaluating on the test split.
I0917 01:06:33.077381 140396256630592 submission_runner.py:376] Time since start: 33237.91s, 	Step: 36268, 	{'train/ctc_loss': Array(0.17746131, dtype=float32), 'train/wer': 0.06534239414532149, 'validation/ctc_loss': Array(0.4614134, dtype=float32), 'validation/wer': 0.13673069687117098, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26614812, dtype=float32), 'test/wer': 0.08987873986959966, 'test/num_examples': 2472, 'score': 30322.732354164124, 'total_duration': 33237.91140341759, 'accumulated_submission_time': 30322.732354164124, 'accumulated_eval_time': 2913.185308456421, 'accumulated_logging_time': 1.2062292098999023}
I0917 01:06:33.119699 140222789592832 logging_writer.py:48] [36268] accumulated_eval_time=2913.185308, accumulated_logging_time=1.206229, accumulated_submission_time=30322.732354, global_step=36268, preemption_count=0, score=30322.732354, test/ctc_loss=0.2661481201648712, test/num_examples=2472, test/wer=0.089879, total_duration=33237.911403, train/ctc_loss=0.17746131122112274, train/wer=0.065342, validation/ctc_loss=0.4614134132862091, validation/num_examples=5348, validation/wer=0.136731
I0917 01:09:29.817409 140222781200128 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7140623331069946, loss=1.2485806941986084
I0917 01:16:52.244591 140222789592832 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8259363770484924, loss=1.2054603099822998
I0917 01:23:39.930643 140222789592832 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8240963816642761, loss=1.2665112018585205
I0917 01:30:33.453152 140396256630592 spec.py:320] Evaluating on the training split.
I0917 01:31:28.086844 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 01:32:19.348388 140396256630592 spec.py:348] Evaluating on the test split.
I0917 01:32:45.510611 140396256630592 submission_runner.py:376] Time since start: 34810.34s, 	Step: 37969, 	{'train/ctc_loss': Array(0.19375935, dtype=float32), 'train/wer': 0.06916960571054466, 'validation/ctc_loss': Array(0.43913734, dtype=float32), 'validation/wer': 0.12953332883095833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25307044, dtype=float32), 'test/wer': 0.08309467227266265, 'test/num_examples': 2472, 'score': 31763.01865506172, 'total_duration': 34810.34302806854, 'accumulated_submission_time': 31763.01865506172, 'accumulated_eval_time': 3045.2345747947693, 'accumulated_logging_time': 1.2649643421173096}
I0917 01:32:45.549369 140222789592832 logging_writer.py:48] [37969] accumulated_eval_time=3045.234575, accumulated_logging_time=1.264964, accumulated_submission_time=31763.018655, global_step=37969, preemption_count=0, score=31763.018655, test/ctc_loss=0.2530704438686371, test/num_examples=2472, test/wer=0.083095, total_duration=34810.343028, train/ctc_loss=0.19375935196876526, train/wer=0.069170, validation/ctc_loss=0.43913733959198, validation/num_examples=5348, validation/wer=0.129533
I0917 01:33:10.291069 140222781200128 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6739710569381714, loss=1.243922472000122
I0917 01:39:47.860203 140222789592832 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.6690405011177063, loss=1.2550621032714844
I0917 01:47:11.848332 140222781200128 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.2512494325637817, loss=1.2700958251953125
I0917 01:54:01.397309 140222789592832 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7409298419952393, loss=1.2478110790252686
I0917 01:56:46.332188 140396256630592 spec.py:320] Evaluating on the training split.
I0917 01:57:40.654375 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 01:58:33.300614 140396256630592 spec.py:348] Evaluating on the test split.
I0917 01:58:59.976929 140396256630592 submission_runner.py:376] Time since start: 36384.81s, 	Step: 39690, 	{'train/ctc_loss': Array(0.23304108, dtype=float32), 'train/wer': 0.08401902767908963, 'validation/ctc_loss': Array(0.42711672, dtype=float32), 'validation/wer': 0.1267161284720547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24648286, dtype=float32), 'test/wer': 0.08211971645034834, 'test/num_examples': 2472, 'score': 33203.75463581085, 'total_duration': 36384.812042713165, 'accumulated_submission_time': 33203.75463581085, 'accumulated_eval_time': 3178.8737857341766, 'accumulated_logging_time': 1.3197791576385498}
I0917 01:59:00.016195 140222789592832 logging_writer.py:48] [39690] accumulated_eval_time=3178.873786, accumulated_logging_time=1.319779, accumulated_submission_time=33203.754636, global_step=39690, preemption_count=0, score=33203.754636, test/ctc_loss=0.24648286402225494, test/num_examples=2472, test/wer=0.082120, total_duration=36384.812043, train/ctc_loss=0.23304107785224915, train/wer=0.084019, validation/ctc_loss=0.427116721868515, validation/num_examples=5348, validation/wer=0.126716
I0917 02:03:06.913842 140222781200128 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.0520875453948975, loss=1.2879709005355835
I0917 02:10:00.730538 140222789592832 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6797773838043213, loss=1.2021942138671875
I0917 02:17:29.162339 140222781200128 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.651561975479126, loss=1.221158504486084
I0917 02:23:00.260581 140396256630592 spec.py:320] Evaluating on the training split.
I0917 02:23:53.213670 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 02:24:45.324785 140396256630592 spec.py:348] Evaluating on the test split.
I0917 02:25:11.842476 140396256630592 submission_runner.py:376] Time since start: 37956.68s, 	Step: 41391, 	{'train/ctc_loss': Array(0.22626993, dtype=float32), 'train/wer': 0.08148242726369374, 'validation/ctc_loss': Array(0.41549647, dtype=float32), 'validation/wer': 0.12058968248608283, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23625007, dtype=float32), 'test/wer': 0.07860581317409054, 'test/num_examples': 2472, 'score': 34643.95214295387, 'total_duration': 37956.677730321884, 'accumulated_submission_time': 34643.95214295387, 'accumulated_eval_time': 3310.4505717754364, 'accumulated_logging_time': 1.3748981952667236}
I0917 02:25:11.885956 140222789592832 logging_writer.py:48] [41391] accumulated_eval_time=3310.450572, accumulated_logging_time=1.374898, accumulated_submission_time=34643.952143, global_step=41391, preemption_count=0, score=34643.952143, test/ctc_loss=0.23625007271766663, test/num_examples=2472, test/wer=0.078606, total_duration=37956.677730, train/ctc_loss=0.22626993060112, train/wer=0.081482, validation/ctc_loss=0.41549646854400635, validation/num_examples=5348, validation/wer=0.120590
I0917 02:26:35.623877 140222781200128 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7803877592086792, loss=1.211304783821106
I0917 02:33:42.700006 140222789592832 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7601445913314819, loss=1.2647385597229004
I0917 02:40:41.074662 140222789592832 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6876536011695862, loss=1.1424050331115723
I0917 02:47:53.397686 140222781200128 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0211549997329712, loss=1.196628212928772
I0917 02:49:12.555492 140396256630592 spec.py:320] Evaluating on the training split.
I0917 02:50:04.183048 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 02:50:55.762541 140396256630592 spec.py:348] Evaluating on the test split.
I0917 02:51:22.109402 140396256630592 submission_runner.py:376] Time since start: 39526.94s, 	Step: 43089, 	{'train/ctc_loss': Array(0.26177764, dtype=float32), 'train/wer': 0.0950798766898117, 'validation/ctc_loss': Array(0.4056806, dtype=float32), 'validation/wer': 0.12113961543285512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23231052, dtype=float32), 'test/wer': 0.07691995206467207, 'test/num_examples': 2472, 'score': 36084.57434678078, 'total_duration': 39526.94174718857, 'accumulated_submission_time': 36084.57434678078, 'accumulated_eval_time': 3439.9961721897125, 'accumulated_logging_time': 1.4347047805786133}
I0917 02:51:22.154563 140223588312832 logging_writer.py:48] [43089] accumulated_eval_time=3439.996172, accumulated_logging_time=1.434705, accumulated_submission_time=36084.574347, global_step=43089, preemption_count=0, score=36084.574347, test/ctc_loss=0.23231051862239838, test/num_examples=2472, test/wer=0.076920, total_duration=39526.941747, train/ctc_loss=0.2617776393890381, train/wer=0.095080, validation/ctc_loss=0.4056805968284607, validation/num_examples=5348, validation/wer=0.121140
I0917 02:56:42.578859 140222932952832 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8586297035217285, loss=1.1625665426254272
I0917 03:03:59.873338 140222924560128 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9415302872657776, loss=1.148425579071045
I0917 03:11:06.271832 140222932952832 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.8502927422523499, loss=1.1196640729904175
I0917 03:15:23.334162 140396256630592 spec.py:320] Evaluating on the training split.
I0917 03:16:14.914349 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 03:17:06.423112 140396256630592 spec.py:348] Evaluating on the test split.
I0917 03:17:32.297556 140396256630592 submission_runner.py:376] Time since start: 41097.13s, 	Step: 44802, 	{'train/ctc_loss': Array(0.21420988, dtype=float32), 'train/wer': 0.07610289139176563, 'validation/ctc_loss': Array(0.3891207, dtype=float32), 'validation/wer': 0.11438605292863414, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21852037, dtype=float32), 'test/wer': 0.07200454979383747, 'test/num_examples': 2472, 'score': 37525.706513643265, 'total_duration': 41097.13309407234, 'accumulated_submission_time': 37525.706513643265, 'accumulated_eval_time': 3568.9544579982758, 'accumulated_logging_time': 1.4963467121124268}
I0917 03:17:32.335317 140222717912832 logging_writer.py:48] [44802] accumulated_eval_time=3568.954458, accumulated_logging_time=1.496347, accumulated_submission_time=37525.706514, global_step=44802, preemption_count=0, score=37525.706514, test/ctc_loss=0.2185203731060028, test/num_examples=2472, test/wer=0.072005, total_duration=41097.133094, train/ctc_loss=0.2142098844051361, train/wer=0.076103, validation/ctc_loss=0.3891206979751587, validation/num_examples=5348, validation/wer=0.114386
I0917 03:20:03.620698 140222709520128 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.2168385982513428, loss=1.1407129764556885
I0917 03:27:00.870031 140222062552832 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0017989873886108, loss=1.1580842733383179
I0917 03:34:06.816981 140222054160128 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7953182458877563, loss=1.1237314939498901
I0917 03:41:18.830348 140222717912832 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7283002734184265, loss=1.1074683666229248
I0917 03:41:33.018989 140396256630592 spec.py:320] Evaluating on the training split.
I0917 03:42:26.294541 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 03:43:17.793768 140396256630592 spec.py:348] Evaluating on the test split.
I0917 03:43:43.959321 140396256630592 submission_runner.py:376] Time since start: 42668.79s, 	Step: 46520, 	{'train/ctc_loss': Array(0.1861183, dtype=float32), 'train/wer': 0.06867602685580039, 'validation/ctc_loss': Array(0.3697318, dtype=float32), 'validation/wer': 0.10876130015726153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20677927, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 38966.34355831146, 'total_duration': 42668.79448843002, 'accumulated_submission_time': 38966.34355831146, 'accumulated_eval_time': 3699.8893325328827, 'accumulated_logging_time': 1.549628496170044}
I0917 03:43:43.997777 140222717912832 logging_writer.py:48] [46520] accumulated_eval_time=3699.889333, accumulated_logging_time=1.549628, accumulated_submission_time=38966.343558, global_step=46520, preemption_count=0, score=38966.343558, test/ctc_loss=0.20677927136421204, test/num_examples=2472, test/wer=0.068348, total_duration=42668.794488, train/ctc_loss=0.18611830472946167, train/wer=0.068676, validation/ctc_loss=0.3697318136692047, validation/num_examples=5348, validation/wer=0.108761
I0917 03:50:12.175988 140222709520128 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8375288248062134, loss=1.1722842454910278
I0917 03:57:23.213702 140223588312832 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0234618186950684, loss=1.0774251222610474
I0917 04:04:25.808671 140223579920128 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.2846908569335938, loss=1.099229097366333
I0917 04:07:45.041125 140396256630592 spec.py:320] Evaluating on the training split.
I0917 04:08:39.038029 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 04:09:30.416015 140396256630592 spec.py:348] Evaluating on the test split.
I0917 04:09:56.512203 140396256630592 submission_runner.py:376] Time since start: 44241.35s, 	Step: 48224, 	{'train/ctc_loss': Array(0.1559851, dtype=float32), 'train/wer': 0.058592263102371964, 'validation/ctc_loss': Array(0.3639677, dtype=float32), 'validation/wer': 0.10703431774546787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20397796, dtype=float32), 'test/wer': 0.06660166961184572, 'test/num_examples': 2472, 'score': 40407.33912777901, 'total_duration': 44241.34569525719, 'accumulated_submission_time': 40407.33912777901, 'accumulated_eval_time': 3831.353259563446, 'accumulated_logging_time': 1.6055097579956055}
I0917 04:09:56.554426 140223588312832 logging_writer.py:48] [48224] accumulated_eval_time=3831.353260, accumulated_logging_time=1.605510, accumulated_submission_time=40407.339128, global_step=48224, preemption_count=0, score=40407.339128, test/ctc_loss=0.20397795736789703, test/num_examples=2472, test/wer=0.066602, total_duration=44241.345695, train/ctc_loss=0.15598510205745697, train/wer=0.058592, validation/ctc_loss=0.3639676868915558, validation/num_examples=5348, validation/wer=0.107034
I0917 04:13:30.985769 140223588312832 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8352349400520325, loss=1.0886858701705933
I0917 04:20:31.031581 140223579920128 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.7899910807609558, loss=1.1131956577301025
I0917 04:27:56.765856 140223588312832 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.8726711273193359, loss=1.0340114831924438
I0917 04:33:57.151636 140396256630592 spec.py:320] Evaluating on the training split.
I0917 04:34:50.836901 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 04:35:41.865948 140396256630592 spec.py:348] Evaluating on the test split.
I0917 04:36:07.939753 140396256630592 submission_runner.py:376] Time since start: 45812.77s, 	Step: 49946, 	{'train/ctc_loss': Array(0.1676263, dtype=float32), 'train/wer': 0.06252206357568417, 'validation/ctc_loss': Array(0.35159767, dtype=float32), 'validation/wer': 0.10255767059981283, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1909934, dtype=float32), 'test/wer': 0.06294558527816708, 'test/num_examples': 2472, 'score': 41847.889263153076, 'total_duration': 45812.77472615242, 'accumulated_submission_time': 41847.889263153076, 'accumulated_eval_time': 3962.135717391968, 'accumulated_logging_time': 1.6631088256835938}
I0917 04:36:07.986574 140222789592832 logging_writer.py:48] [49946] accumulated_eval_time=3962.135717, accumulated_logging_time=1.663109, accumulated_submission_time=41847.889263, global_step=49946, preemption_count=0, score=41847.889263, test/ctc_loss=0.19099339842796326, test/num_examples=2472, test/wer=0.062946, total_duration=45812.774726, train/ctc_loss=0.1676263064146042, train/wer=0.062522, validation/ctc_loss=0.3515976667404175, validation/num_examples=5348, validation/wer=0.102558
I0917 04:36:49.699175 140222781200128 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.2976185083389282, loss=1.0681487321853638
I0917 04:43:51.891628 140222789592832 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7891823649406433, loss=1.0678315162658691
I0917 04:50:45.343588 140222781200128 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8194251656532288, loss=1.0972710847854614
I0917 04:58:15.477180 140222789592832 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.9032978415489197, loss=1.0390806198120117
I0917 05:00:08.233612 140396256630592 spec.py:320] Evaluating on the training split.
I0917 05:01:01.765996 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 05:01:52.893592 140396256630592 spec.py:348] Evaluating on the test split.
I0917 05:02:18.787748 140396256630592 submission_runner.py:376] Time since start: 47383.62s, 	Step: 51650, 	{'train/ctc_loss': Array(0.14399317, dtype=float32), 'train/wer': 0.05273471053083663, 'validation/ctc_loss': Array(0.3433903, dtype=float32), 'validation/wer': 0.09934490443709057, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1865926, dtype=float32), 'test/wer': 0.06205187577437897, 'test/num_examples': 2472, 'score': 43288.07773184776, 'total_duration': 47383.62333202362, 'accumulated_submission_time': 43288.07773184776, 'accumulated_eval_time': 4092.684789419174, 'accumulated_logging_time': 1.7371020317077637}
I0917 05:02:18.824652 140223588312832 logging_writer.py:48] [51650] accumulated_eval_time=4092.684789, accumulated_logging_time=1.737102, accumulated_submission_time=43288.077732, global_step=51650, preemption_count=0, score=43288.077732, test/ctc_loss=0.18659259378910065, test/num_examples=2472, test/wer=0.062052, total_duration=47383.623332, train/ctc_loss=0.14399316906929016, train/wer=0.052735, validation/ctc_loss=0.3433902859687805, validation/num_examples=5348, validation/wer=0.099345
I0917 05:06:54.698849 140223579920128 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.6479626893997192, loss=1.0684850215911865
I0917 05:14:25.379304 140223588312832 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8917165994644165, loss=1.0875235795974731
I0917 05:21:16.169317 140221847512832 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9094151258468628, loss=1.060491681098938
I0917 05:26:18.854845 140396256630592 spec.py:320] Evaluating on the training split.
I0917 05:27:13.202163 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 05:28:05.512542 140396256630592 spec.py:348] Evaluating on the test split.
I0917 05:28:31.934700 140396256630592 submission_runner.py:376] Time since start: 48956.77s, 	Step: 53334, 	{'train/ctc_loss': Array(0.14230087, dtype=float32), 'train/wer': 0.0533635643693249, 'validation/ctc_loss': Array(0.33492908, dtype=float32), 'validation/wer': 0.09783017684685814, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1820707, dtype=float32), 'test/wer': 0.05853797249812118, 'test/num_examples': 2472, 'score': 44728.063279390335, 'total_duration': 48956.769812107086, 'accumulated_submission_time': 44728.063279390335, 'accumulated_eval_time': 4225.759132385254, 'accumulated_logging_time': 1.7885398864746094}
I0917 05:28:31.973546 140222932952832 logging_writer.py:48] [53334] accumulated_eval_time=4225.759132, accumulated_logging_time=1.788540, accumulated_submission_time=44728.063279, global_step=53334, preemption_count=0, score=44728.063279, test/ctc_loss=0.18207070231437683, test/num_examples=2472, test/wer=0.058538, total_duration=48956.769812, train/ctc_loss=0.14230087399482727, train/wer=0.053364, validation/ctc_loss=0.33492907881736755, validation/num_examples=5348, validation/wer=0.097830
I0917 05:30:39.108707 140222924560128 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.12873113155365, loss=0.9750233888626099
I0917 05:37:20.988318 140222932952832 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0692628622055054, loss=1.020202398300171
I0917 05:44:53.322099 140222924560128 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.0753799676895142, loss=0.9971055388450623
I0917 05:51:40.983301 140222932952832 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1257954835891724, loss=1.0031700134277344
I0917 05:52:32.204579 140396256630592 spec.py:320] Evaluating on the training split.
I0917 05:53:25.302106 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 05:54:16.532104 140396256630592 spec.py:348] Evaluating on the test split.
I0917 05:54:42.654522 140396256630592 submission_runner.py:376] Time since start: 50527.49s, 	Step: 55059, 	{'train/ctc_loss': Array(0.13326927, dtype=float32), 'train/wer': 0.049071083505866114, 'validation/ctc_loss': Array(0.327043, dtype=float32), 'validation/wer': 0.0948103696128279, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17946132, dtype=float32), 'test/wer': 0.05851766091848963, 'test/num_examples': 2472, 'score': 46168.247462272644, 'total_duration': 50527.48977017403, 'accumulated_submission_time': 46168.247462272644, 'accumulated_eval_time': 4356.203681707382, 'accumulated_logging_time': 1.842792272567749}
I0917 05:54:42.693768 140223588312832 logging_writer.py:48] [55059] accumulated_eval_time=4356.203682, accumulated_logging_time=1.842792, accumulated_submission_time=46168.247462, global_step=55059, preemption_count=0, score=46168.247462, test/ctc_loss=0.1794613152742386, test/num_examples=2472, test/wer=0.058518, total_duration=50527.489770, train/ctc_loss=0.133269265294075, train/wer=0.049071, validation/ctc_loss=0.32704299688339233, validation/num_examples=5348, validation/wer=0.094810
I0917 06:00:47.984861 140223579920128 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1904488801956177, loss=1.0207439661026
I0917 06:07:39.690394 140223588312832 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.8288074731826782, loss=0.9971449375152588
I0917 06:15:12.393215 140223579920128 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.6364246606826782, loss=1.0561460256576538
I0917 06:18:43.308562 140396256630592 spec.py:320] Evaluating on the training split.
I0917 06:19:35.093698 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 06:20:27.183139 140396256630592 spec.py:348] Evaluating on the test split.
I0917 06:20:53.647256 140396256630592 submission_runner.py:376] Time since start: 52098.48s, 	Step: 56745, 	{'train/ctc_loss': Array(0.13918956, dtype=float32), 'train/wer': 0.05079381431498461, 'validation/ctc_loss': Array(0.32410333, dtype=float32), 'validation/wer': 0.0940481818444944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17642444, dtype=float32), 'test/wer': 0.05658806085349258, 'test/num_examples': 2472, 'score': 47608.81355500221, 'total_duration': 52098.480689525604, 'accumulated_submission_time': 47608.81355500221, 'accumulated_eval_time': 4486.535350084305, 'accumulated_logging_time': 1.899409294128418}
I0917 06:20:53.683334 140223588312832 logging_writer.py:48] [56745] accumulated_eval_time=4486.535350, accumulated_logging_time=1.899409, accumulated_submission_time=47608.813555, global_step=56745, preemption_count=0, score=47608.813555, test/ctc_loss=0.17642444372177124, test/num_examples=2472, test/wer=0.056588, total_duration=52098.480690, train/ctc_loss=0.13918955624103546, train/wer=0.050794, validation/ctc_loss=0.32410332560539246, validation/num_examples=5348, validation/wer=0.094048
I0917 06:24:07.727590 140223579920128 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.141251564025879, loss=1.0632818937301636
I0917 06:31:24.499847 140223588312832 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0695894956588745, loss=0.9843483567237854
I0917 06:38:17.894154 140223588312832 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2513427734375, loss=0.9869900345802307
I0917 06:44:53.779527 140396256630592 spec.py:320] Evaluating on the training split.
I0917 06:45:47.428330 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 06:46:40.048789 140396256630592 spec.py:348] Evaluating on the test split.
I0917 06:47:06.279078 140396256630592 submission_runner.py:376] Time since start: 53671.11s, 	Step: 58449, 	{'train/ctc_loss': Array(0.14613272, dtype=float32), 'train/wer': 0.053071804310654655, 'validation/ctc_loss': Array(0.32211512, dtype=float32), 'validation/wer': 0.09317986666538028, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17494087, dtype=float32), 'test/wer': 0.05624276399975626, 'test/num_examples': 2472, 'score': 49048.862654685974, 'total_duration': 53671.11095905304, 'accumulated_submission_time': 49048.862654685974, 'accumulated_eval_time': 4619.026136398315, 'accumulated_logging_time': 1.950777530670166}
I0917 06:47:06.321383 140222676948736 logging_writer.py:48] [58449] accumulated_eval_time=4619.026136, accumulated_logging_time=1.950778, accumulated_submission_time=49048.862655, global_step=58449, preemption_count=0, score=49048.862655, test/ctc_loss=0.17494086921215057, test/num_examples=2472, test/wer=0.056243, total_duration=53671.110959, train/ctc_loss=0.1461327224969864, train/wer=0.053072, validation/ctc_loss=0.32211512327194214, validation/num_examples=5348, validation/wer=0.093180
I0917 06:47:46.290177 140222668556032 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.0685384273529053, loss=0.9842998385429382
I0917 06:54:20.212260 140222676948736 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0176998376846313, loss=0.9444858431816101
I0917 07:01:40.626223 140222668556032 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0125505924224854, loss=0.9878302216529846
I0917 07:08:39.383121 140396256630592 spec.py:320] Evaluating on the training split.
I0917 07:09:31.866139 140396256630592 spec.py:332] Evaluating on the validation split.
I0917 07:10:23.129595 140396256630592 spec.py:348] Evaluating on the test split.
I0917 07:10:49.467331 140396256630592 submission_runner.py:376] Time since start: 55094.31s, 	Step: 60000, 	{'train/ctc_loss': Array(0.12644494, dtype=float32), 'train/wer': 0.04747012800681508, 'validation/ctc_loss': Array(0.32128876, dtype=float32), 'validation/wer': 0.09311233104033806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17482372, dtype=float32), 'test/wer': 0.05599902504417768, 'test/num_examples': 2472, 'score': 50341.87585115433, 'total_duration': 55094.30537009239, 'accumulated_submission_time': 50341.87585115433, 'accumulated_eval_time': 4749.107924461365, 'accumulated_logging_time': 2.0124266147613525}
I0917 07:10:49.510907 140222676948736 logging_writer.py:48] [60000] accumulated_eval_time=4749.107924, accumulated_logging_time=2.012427, accumulated_submission_time=50341.875851, global_step=60000, preemption_count=0, score=50341.875851, test/ctc_loss=0.17482371628284454, test/num_examples=2472, test/wer=0.055999, total_duration=55094.305370, train/ctc_loss=0.12644493579864502, train/wer=0.047470, validation/ctc_loss=0.3212887644767761, validation/num_examples=5348, validation/wer=0.093112
I0917 07:10:49.534608 140222668556032 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=50341.875851
I0917 07:10:50.010600 140396256630592 checkpoints.py:490] Saving checkpoint at step: 60000
I0917 07:10:51.521241 140396256630592 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000
I0917 07:10:51.556338 140396256630592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000.
I0917 07:10:52.867103 140396256630592 submission_runner.py:540] Tuning trial 1/1
I0917 07:10:52.867360 140396256630592 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I0917 07:10:52.888185 140396256630592 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.993778, dtype=float32), 'train/wer': 1.2477198425754326, 'validation/ctc_loss': Array(30.964561, dtype=float32), 'validation/wer': 1.1193354494495846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.096544, dtype=float32), 'test/wer': 1.1393577478520505, 'test/num_examples': 2472, 'score': 77.86762166023254, 'total_duration': 249.29528999328613, 'accumulated_submission_time': 77.86762166023254, 'accumulated_eval_time': 171.4276065826416, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1754, {'train/ctc_loss': Array(6.0282903, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.1358495, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.1042094, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1517.971349954605, 'total_duration': 1795.4528942108154, 'accumulated_submission_time': 1517.971349954605, 'accumulated_eval_time': 277.40246629714966, 'accumulated_logging_time': 0.04566502571105957, 'global_step': 1754, 'preemption_count': 0}), (3544, {'train/ctc_loss': Array(2.6819842, dtype=float32), 'train/wer': 0.6688810164344885, 'validation/ctc_loss': Array(2.9259045, dtype=float32), 'validation/wer': 0.6682360659533618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6804616, dtype=float32), 'test/wer': 0.6440801901163854, 'test/num_examples': 2472, 'score': 2958.0613248348236, 'total_duration': 3367.86585688591, 'accumulated_submission_time': 2958.0613248348236, 'accumulated_eval_time': 409.6300756931305, 'accumulated_logging_time': 0.10097885131835938, 'global_step': 3544, 'preemption_count': 0}), (5300, {'train/ctc_loss': Array(0.8298036, dtype=float32), 'train/wer': 0.27917200373199236, 'validation/ctc_loss': Array(1.1898717, dtype=float32), 'validation/wer': 0.3423091394996575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.88903236, dtype=float32), 'test/wer': 0.2833059127008307, 'test/num_examples': 2472, 'score': 4398.178380012512, 'total_duration': 4939.317852497101, 'accumulated_submission_time': 4398.178380012512, 'accumulated_eval_time': 540.8720512390137, 'accumulated_logging_time': 0.15682339668273926, 'global_step': 5300, 'preemption_count': 0}), (7035, {'train/ctc_loss': Array(0.6160929, dtype=float32), 'train/wer': 0.20914974499680591, 'validation/ctc_loss': Array(0.9512876, dtype=float32), 'validation/wer': 0.2771083174946213, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6582261, dtype=float32), 'test/wer': 0.21260130400341234, 'test/num_examples': 2472, 'score': 5838.125876188278, 'total_duration': 6511.488792181015, 'accumulated_submission_time': 5838.125876188278, 'accumulated_eval_time': 673.0096666812897, 'accumulated_logging_time': 0.20478272438049316, 'global_step': 7035, 'preemption_count': 0}), (8779, {'train/ctc_loss': Array(0.50635326, dtype=float32), 'train/wer': 0.1764443671312705, 'validation/ctc_loss': Array(0.8235931, dtype=float32), 'validation/wer': 0.24602263408233557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5515629, dtype=float32), 'test/wer': 0.1853025409786119, 'test/num_examples': 2472, 'score': 7278.118975162506, 'total_duration': 8082.120104789734, 'accumulated_submission_time': 7278.118975162506, 'accumulated_eval_time': 803.5557680130005, 'accumulated_logging_time': 0.25684642791748047, 'global_step': 8779, 'preemption_count': 0}), (10521, {'train/ctc_loss': Array(0.49570626, dtype=float32), 'train/wer': 0.17184757616657154, 'validation/ctc_loss': Array(0.7927408, dtype=float32), 'validation/wer': 0.23569933139731208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5140534, dtype=float32), 'test/wer': 0.17195783316068491, 'test/num_examples': 2472, 'score': 8718.680030345917, 'total_duration': 9654.611117124557, 'accumulated_submission_time': 8718.680030345917, 'accumulated_eval_time': 935.3940994739532, 'accumulated_logging_time': 0.30868983268737793, 'global_step': 10521, 'preemption_count': 0}), (12238, {'train/ctc_loss': Array(0.4280237, dtype=float32), 'train/wer': 0.15185161551060344, 'validation/ctc_loss': Array(0.7326277, dtype=float32), 'validation/wer': 0.22067747879863772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47017014, dtype=float32), 'test/wer': 0.1587349948205472, 'test/num_examples': 2472, 'score': 10158.598539113998, 'total_duration': 11225.319172859192, 'accumulated_submission_time': 10158.598539113998, 'accumulated_eval_time': 1066.098382472992, 'accumulated_logging_time': 0.3575417995452881, 'global_step': 12238, 'preemption_count': 0}), (13965, {'train/ctc_loss': Array(0.36184743, dtype=float32), 'train/wer': 0.13320679164824878, 'validation/ctc_loss': Array(0.6875857, dtype=float32), 'validation/wer': 0.20737296066532238, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4393874, dtype=float32), 'test/wer': 0.1492494871326143, 'test/num_examples': 2472, 'score': 11598.683979988098, 'total_duration': 12797.59767794609, 'accumulated_submission_time': 11598.683979988098, 'accumulated_eval_time': 1198.203332901001, 'accumulated_logging_time': 0.4070308208465576, 'global_step': 13965, 'preemption_count': 0}), (15685, {'train/ctc_loss': Array(0.34628588, dtype=float32), 'train/wer': 0.12451521261484606, 'validation/ctc_loss': Array(0.66167533, dtype=float32), 'validation/wer': 0.19895995137434996, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42385334, dtype=float32), 'test/wer': 0.14311539008388682, 'test/num_examples': 2472, 'score': 13038.56060218811, 'total_duration': 14368.093508005142, 'accumulated_submission_time': 13038.56060218811, 'accumulated_eval_time': 1328.6396594047546, 'accumulated_logging_time': 0.5501930713653564, 'global_step': 15685, 'preemption_count': 0}), (17384, {'train/ctc_loss': Array(0.33084786, dtype=float32), 'train/wer': 0.12375380094064901, 'validation/ctc_loss': Array(0.639592, dtype=float32), 'validation/wer': 0.1921967409236944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40249684, dtype=float32), 'test/wer': 0.13716409725184328, 'test/num_examples': 2472, 'score': 14479.171476602554, 'total_duration': 15939.764713048935, 'accumulated_submission_time': 14479.171476602554, 'accumulated_eval_time': 1459.6147727966309, 'accumulated_logging_time': 0.5997335910797119, 'global_step': 17384, 'preemption_count': 0}), (19119, {'train/ctc_loss': Array(0.3453708, dtype=float32), 'train/wer': 0.12373674608001269, 'validation/ctc_loss': Array(0.63859004, dtype=float32), 'validation/wer': 0.19033468726181632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39520597, dtype=float32), 'test/wer': 0.13360957081632238, 'test/num_examples': 2472, 'score': 15919.306365728378, 'total_duration': 17511.188287734985, 'accumulated_submission_time': 15919.306365728378, 'accumulated_eval_time': 1590.8126966953278, 'accumulated_logging_time': 0.6519181728363037, 'global_step': 19119, 'preemption_count': 0}), (20833, {'train/ctc_loss': Array(0.3405871, dtype=float32), 'train/wer': 0.12208299258712112, 'validation/ctc_loss': Array(0.6191101, dtype=float32), 'validation/wer': 0.1854721222587772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38428658, dtype=float32), 'test/wer': 0.1279426400991205, 'test/num_examples': 2472, 'score': 17359.654372692108, 'total_duration': 19083.759892225266, 'accumulated_submission_time': 17359.654372692108, 'accumulated_eval_time': 1722.9438631534576, 'accumulated_logging_time': 0.7070457935333252, 'global_step': 20833, 'preemption_count': 0}), (22548, {'train/ctc_loss': Array(0.32231936, dtype=float32), 'train/wer': 0.1167506945158967, 'validation/ctc_loss': Array(0.60697156, dtype=float32), 'validation/wer': 0.18249090681048538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.373131, dtype=float32), 'test/wer': 0.1253630694859139, 'test/num_examples': 2472, 'score': 18800.1708650589, 'total_duration': 20656.38356256485, 'accumulated_submission_time': 18800.1708650589, 'accumulated_eval_time': 1854.9587969779968, 'accumulated_logging_time': 0.761925220489502, 'global_step': 22548, 'preemption_count': 0}), (24270, {'train/ctc_loss': Array(0.28810805, dtype=float32), 'train/wer': 0.10571238545669524, 'validation/ctc_loss': Array(0.5808014, dtype=float32), 'validation/wer': 0.17467607019845827, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3530474, dtype=float32), 'test/wer': 0.120102370361343, 'test/num_examples': 2472, 'score': 20240.479501485825, 'total_duration': 22228.974871873856, 'accumulated_submission_time': 20240.479501485825, 'accumulated_eval_time': 1987.1506052017212, 'accumulated_logging_time': 0.8146564960479736, 'global_step': 24270, 'preemption_count': 0}), (25996, {'train/ctc_loss': Array(0.26905876, dtype=float32), 'train/wer': 0.09922011528730536, 'validation/ctc_loss': Array(0.5550443, dtype=float32), 'validation/wer': 0.16720855965807677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33755738, dtype=float32), 'test/wer': 0.1131964332866167, 'test/num_examples': 2472, 'score': 21680.89455127716, 'total_duration': 23802.236016511917, 'accumulated_submission_time': 21680.89455127716, 'accumulated_eval_time': 2119.9034657478333, 'accumulated_logging_time': 0.8712265491485596, 'global_step': 25996, 'preemption_count': 0}), (27706, {'train/ctc_loss': Array(0.25408566, dtype=float32), 'train/wer': 0.09259391844207382, 'validation/ctc_loss': Array(0.53598136, dtype=float32), 'validation/wer': 0.161709230190354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3195851, dtype=float32), 'test/wer': 0.10797635732130888, 'test/num_examples': 2472, 'score': 23121.85632967949, 'total_duration': 25376.55044579506, 'accumulated_submission_time': 23121.85632967949, 'accumulated_eval_time': 2253.1686873435974, 'accumulated_logging_time': 0.9225003719329834, 'global_step': 27706, 'preemption_count': 0}), (29425, {'train/ctc_loss': Array(0.25806317, dtype=float32), 'train/wer': 0.09442169353440805, 'validation/ctc_loss': Array(0.511958, dtype=float32), 'validation/wer': 0.15412594429275728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30581048, dtype=float32), 'test/wer': 0.10206568764852843, 'test/num_examples': 2472, 'score': 24562.075604200363, 'total_duration': 26948.279821395874, 'accumulated_submission_time': 24562.075604200363, 'accumulated_eval_time': 2384.587548971176, 'accumulated_logging_time': 0.9760639667510986, 'global_step': 29425, 'preemption_count': 0}), (31135, {'train/ctc_loss': Array(0.2387584, dtype=float32), 'train/wer': 0.08508456367099117, 'validation/ctc_loss': Array(0.49811587, dtype=float32), 'validation/wer': 0.14883886964659573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29544124, dtype=float32), 'test/wer': 0.09855178437227063, 'test/num_examples': 2472, 'score': 26002.209939718246, 'total_duration': 28518.396995067596, 'accumulated_submission_time': 26002.209939718246, 'accumulated_eval_time': 2514.4646530151367, 'accumulated_logging_time': 1.0440433025360107, 'global_step': 31135, 'preemption_count': 0}), (32823, {'train/ctc_loss': Array(0.24006222, dtype=float32), 'train/wer': 0.08529349819437096, 'validation/ctc_loss': Array(0.48322594, dtype=float32), 'validation/wer': 0.1445262375903289, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2839885, dtype=float32), 'test/wer': 0.09631751061280036, 'test/num_examples': 2472, 'score': 27442.179909706116, 'total_duration': 30091.28090572357, 'accumulated_submission_time': 27442.179909706116, 'accumulated_eval_time': 2647.29003405571, 'accumulated_logging_time': 1.0966832637786865, 'global_step': 32823, 'preemption_count': 0}), (34544, {'train/ctc_loss': Array(0.22858001, dtype=float32), 'train/wer': 0.07955817001822309, 'validation/ctc_loss': Array(0.46635282, dtype=float32), 'validation/wer': 0.14082142615944196, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27313003, dtype=float32), 'test/wer': 0.091930209412386, 'test/num_examples': 2472, 'score': 28882.79427242279, 'total_duration': 31664.800646543503, 'accumulated_submission_time': 28882.79427242279, 'accumulated_eval_time': 2780.1062252521515, 'accumulated_logging_time': 1.1494791507720947, 'global_step': 34544, 'preemption_count': 0}), (36268, {'train/ctc_loss': Array(0.17746131, dtype=float32), 'train/wer': 0.06534239414532149, 'validation/ctc_loss': Array(0.4614134, dtype=float32), 'validation/wer': 0.13673069687117098, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26614812, dtype=float32), 'test/wer': 0.08987873986959966, 'test/num_examples': 2472, 'score': 30322.732354164124, 'total_duration': 33237.91140341759, 'accumulated_submission_time': 30322.732354164124, 'accumulated_eval_time': 2913.185308456421, 'accumulated_logging_time': 1.2062292098999023, 'global_step': 36268, 'preemption_count': 0}), (37969, {'train/ctc_loss': Array(0.19375935, dtype=float32), 'train/wer': 0.06916960571054466, 'validation/ctc_loss': Array(0.43913734, dtype=float32), 'validation/wer': 0.12953332883095833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25307044, dtype=float32), 'test/wer': 0.08309467227266265, 'test/num_examples': 2472, 'score': 31763.01865506172, 'total_duration': 34810.34302806854, 'accumulated_submission_time': 31763.01865506172, 'accumulated_eval_time': 3045.2345747947693, 'accumulated_logging_time': 1.2649643421173096, 'global_step': 37969, 'preemption_count': 0}), (39690, {'train/ctc_loss': Array(0.23304108, dtype=float32), 'train/wer': 0.08401902767908963, 'validation/ctc_loss': Array(0.42711672, dtype=float32), 'validation/wer': 0.1267161284720547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24648286, dtype=float32), 'test/wer': 0.08211971645034834, 'test/num_examples': 2472, 'score': 33203.75463581085, 'total_duration': 36384.812042713165, 'accumulated_submission_time': 33203.75463581085, 'accumulated_eval_time': 3178.8737857341766, 'accumulated_logging_time': 1.3197791576385498, 'global_step': 39690, 'preemption_count': 0}), (41391, {'train/ctc_loss': Array(0.22626993, dtype=float32), 'train/wer': 0.08148242726369374, 'validation/ctc_loss': Array(0.41549647, dtype=float32), 'validation/wer': 0.12058968248608283, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23625007, dtype=float32), 'test/wer': 0.07860581317409054, 'test/num_examples': 2472, 'score': 34643.95214295387, 'total_duration': 37956.677730321884, 'accumulated_submission_time': 34643.95214295387, 'accumulated_eval_time': 3310.4505717754364, 'accumulated_logging_time': 1.3748981952667236, 'global_step': 41391, 'preemption_count': 0}), (43089, {'train/ctc_loss': Array(0.26177764, dtype=float32), 'train/wer': 0.0950798766898117, 'validation/ctc_loss': Array(0.4056806, dtype=float32), 'validation/wer': 0.12113961543285512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23231052, dtype=float32), 'test/wer': 0.07691995206467207, 'test/num_examples': 2472, 'score': 36084.57434678078, 'total_duration': 39526.94174718857, 'accumulated_submission_time': 36084.57434678078, 'accumulated_eval_time': 3439.9961721897125, 'accumulated_logging_time': 1.4347047805786133, 'global_step': 43089, 'preemption_count': 0}), (44802, {'train/ctc_loss': Array(0.21420988, dtype=float32), 'train/wer': 0.07610289139176563, 'validation/ctc_loss': Array(0.3891207, dtype=float32), 'validation/wer': 0.11438605292863414, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21852037, dtype=float32), 'test/wer': 0.07200454979383747, 'test/num_examples': 2472, 'score': 37525.706513643265, 'total_duration': 41097.13309407234, 'accumulated_submission_time': 37525.706513643265, 'accumulated_eval_time': 3568.9544579982758, 'accumulated_logging_time': 1.4963467121124268, 'global_step': 44802, 'preemption_count': 0}), (46520, {'train/ctc_loss': Array(0.1861183, dtype=float32), 'train/wer': 0.06867602685580039, 'validation/ctc_loss': Array(0.3697318, dtype=float32), 'validation/wer': 0.10876130015726153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20677927, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 38966.34355831146, 'total_duration': 42668.79448843002, 'accumulated_submission_time': 38966.34355831146, 'accumulated_eval_time': 3699.8893325328827, 'accumulated_logging_time': 1.549628496170044, 'global_step': 46520, 'preemption_count': 0}), (48224, {'train/ctc_loss': Array(0.1559851, dtype=float32), 'train/wer': 0.058592263102371964, 'validation/ctc_loss': Array(0.3639677, dtype=float32), 'validation/wer': 0.10703431774546787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20397796, dtype=float32), 'test/wer': 0.06660166961184572, 'test/num_examples': 2472, 'score': 40407.33912777901, 'total_duration': 44241.34569525719, 'accumulated_submission_time': 40407.33912777901, 'accumulated_eval_time': 3831.353259563446, 'accumulated_logging_time': 1.6055097579956055, 'global_step': 48224, 'preemption_count': 0}), (49946, {'train/ctc_loss': Array(0.1676263, dtype=float32), 'train/wer': 0.06252206357568417, 'validation/ctc_loss': Array(0.35159767, dtype=float32), 'validation/wer': 0.10255767059981283, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1909934, dtype=float32), 'test/wer': 0.06294558527816708, 'test/num_examples': 2472, 'score': 41847.889263153076, 'total_duration': 45812.77472615242, 'accumulated_submission_time': 41847.889263153076, 'accumulated_eval_time': 3962.135717391968, 'accumulated_logging_time': 1.6631088256835938, 'global_step': 49946, 'preemption_count': 0}), (51650, {'train/ctc_loss': Array(0.14399317, dtype=float32), 'train/wer': 0.05273471053083663, 'validation/ctc_loss': Array(0.3433903, dtype=float32), 'validation/wer': 0.09934490443709057, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1865926, dtype=float32), 'test/wer': 0.06205187577437897, 'test/num_examples': 2472, 'score': 43288.07773184776, 'total_duration': 47383.62333202362, 'accumulated_submission_time': 43288.07773184776, 'accumulated_eval_time': 4092.684789419174, 'accumulated_logging_time': 1.7371020317077637, 'global_step': 51650, 'preemption_count': 0}), (53334, {'train/ctc_loss': Array(0.14230087, dtype=float32), 'train/wer': 0.0533635643693249, 'validation/ctc_loss': Array(0.33492908, dtype=float32), 'validation/wer': 0.09783017684685814, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1820707, dtype=float32), 'test/wer': 0.05853797249812118, 'test/num_examples': 2472, 'score': 44728.063279390335, 'total_duration': 48956.769812107086, 'accumulated_submission_time': 44728.063279390335, 'accumulated_eval_time': 4225.759132385254, 'accumulated_logging_time': 1.7885398864746094, 'global_step': 53334, 'preemption_count': 0}), (55059, {'train/ctc_loss': Array(0.13326927, dtype=float32), 'train/wer': 0.049071083505866114, 'validation/ctc_loss': Array(0.327043, dtype=float32), 'validation/wer': 0.0948103696128279, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17946132, dtype=float32), 'test/wer': 0.05851766091848963, 'test/num_examples': 2472, 'score': 46168.247462272644, 'total_duration': 50527.48977017403, 'accumulated_submission_time': 46168.247462272644, 'accumulated_eval_time': 4356.203681707382, 'accumulated_logging_time': 1.842792272567749, 'global_step': 55059, 'preemption_count': 0}), (56745, {'train/ctc_loss': Array(0.13918956, dtype=float32), 'train/wer': 0.05079381431498461, 'validation/ctc_loss': Array(0.32410333, dtype=float32), 'validation/wer': 0.0940481818444944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17642444, dtype=float32), 'test/wer': 0.05658806085349258, 'test/num_examples': 2472, 'score': 47608.81355500221, 'total_duration': 52098.480689525604, 'accumulated_submission_time': 47608.81355500221, 'accumulated_eval_time': 4486.535350084305, 'accumulated_logging_time': 1.899409294128418, 'global_step': 56745, 'preemption_count': 0}), (58449, {'train/ctc_loss': Array(0.14613272, dtype=float32), 'train/wer': 0.053071804310654655, 'validation/ctc_loss': Array(0.32211512, dtype=float32), 'validation/wer': 0.09317986666538028, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17494087, dtype=float32), 'test/wer': 0.05624276399975626, 'test/num_examples': 2472, 'score': 49048.862654685974, 'total_duration': 53671.11095905304, 'accumulated_submission_time': 49048.862654685974, 'accumulated_eval_time': 4619.026136398315, 'accumulated_logging_time': 1.950777530670166, 'global_step': 58449, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.12644494, dtype=float32), 'train/wer': 0.04747012800681508, 'validation/ctc_loss': Array(0.32128876, dtype=float32), 'validation/wer': 0.09311233104033806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17482372, dtype=float32), 'test/wer': 0.05599902504417768, 'test/num_examples': 2472, 'score': 50341.87585115433, 'total_duration': 55094.30537009239, 'accumulated_submission_time': 50341.87585115433, 'accumulated_eval_time': 4749.107924461365, 'accumulated_logging_time': 2.0124266147613525, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I0917 07:10:52.888420 140396256630592 submission_runner.py:543] Timing: 50341.87585115433
I0917 07:10:52.888486 140396256630592 submission_runner.py:545] Total number of evals: 36
I0917 07:10:52.888538 140396256630592 submission_runner.py:546] ====================
I0917 07:10:52.893360 140396256630592 submission_runner.py:614] Final librispeech_conformer score: 50341.87585115433
