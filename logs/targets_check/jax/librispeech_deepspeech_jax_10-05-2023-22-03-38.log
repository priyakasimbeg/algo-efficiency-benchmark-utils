python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_deepspeech/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_deepspeech/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=36000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_10-05-2023-22-03-38.log
2023-10-05 22:03:44.348097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 22:04:02.296896 139821904316224 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax because --overwrite was set.
I1005 22:04:02.299568 139821904316224 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax.
I1005 22:04:03.352347 139821904316224 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I1005 22:04:03.353470 139821904316224 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 22:04:03.353598 139821904316224 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 22:04:03.359306 139821904316224 submission_runner.py:507] Using RNG seed 1373876952
I1005 22:04:09.182141 139821904316224 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 22:04:09.182358 139821904316224 submission_runner.py:521] Creating tuning directory at /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1.
I1005 22:04:09.182544 139821904316224 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1/hparams.json.
I1005 22:04:09.365736 139821904316224 submission_runner.py:191] Initializing dataset.
I1005 22:04:09.365961 139821904316224 submission_runner.py:198] Initializing model.
I1005 22:04:11.892543 139821904316224 submission_runner.py:232] Initializing optimizer.
I1005 22:04:12.595401 139821904316224 submission_runner.py:239] Initializing metrics bundle.
I1005 22:04:12.595600 139821904316224 submission_runner.py:257] Initializing checkpoint and logger.
I1005 22:04:12.596526 139821904316224 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I1005 22:04:12.596653 139821904316224 submission_runner.py:277] Saving meta data to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I1005 22:04:12.596863 139821904316224 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 22:04:12.596934 139821904316224 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1005 22:04:12.918302 139821904316224 logger_utils.py:220] Unable to record git information. Continuing without it.
I1005 22:04:13.218164 139821904316224 submission_runner.py:280] Saving flags to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1/flags_0.json.
I1005 22:04:13.231888 139821904316224 submission_runner.py:290] Starting training loop.
I1005 22:04:13.523407 139821904316224 input_pipeline.py:20] Loading split = train-clean-100
I1005 22:04:13.559772 139821904316224 input_pipeline.py:20] Loading split = train-clean-360
I1005 22:04:13.686484 139821904316224 input_pipeline.py:20] Loading split = train-other-500
2023-10-05 22:05:05.997075: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-05 22:05:07.966285: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1005 22:05:13.134903 139653982189312 logging_writer.py:48] [0] global_step=0, grad_norm=25.840713500976562, loss=33.480228424072266
I1005 22:05:13.171070 139821904316224 spec.py:321] Evaluating on the training split.
I1005 22:05:13.443135 139821904316224 input_pipeline.py:20] Loading split = train-clean-100
I1005 22:05:13.480723 139821904316224 input_pipeline.py:20] Loading split = train-clean-360
I1005 22:05:13.925968 139821904316224 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1005 22:06:42.519866 139821904316224 spec.py:333] Evaluating on the validation split.
I1005 22:06:42.717575 139821904316224 input_pipeline.py:20] Loading split = dev-clean
I1005 22:06:42.723898 139821904316224 input_pipeline.py:20] Loading split = dev-other
I1005 22:07:40.110237 139821904316224 spec.py:349] Evaluating on the test split.
I1005 22:07:40.319021 139821904316224 input_pipeline.py:20] Loading split = test-clean
I1005 22:08:19.285022 139821904316224 submission_runner.py:381] Time since start: 246.05s, 	Step: 1, 	{'train/ctc_loss': Array(31.686937, dtype=float32), 'train/wer': 2.0426930845636466, 'validation/ctc_loss': Array(30.769913, dtype=float32), 'validation/wer': 1.7273683296510338, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.775833, dtype=float32), 'test/wer': 2.0614831515446954, 'test/num_examples': 2472, 'score': 59.93907356262207, 'total_duration': 246.05032634735107, 'accumulated_submission_time': 59.93907356262207, 'accumulated_eval_time': 186.11118054389954, 'accumulated_logging_time': 0}
I1005 22:08:19.311157 139652631619328 logging_writer.py:48] [1] accumulated_eval_time=186.111181, accumulated_logging_time=0, accumulated_submission_time=59.939074, global_step=1, preemption_count=0, score=59.939074, test/ctc_loss=30.775833129882812, test/num_examples=2472, test/wer=2.061483, total_duration=246.050326, train/ctc_loss=31.68693733215332, train/wer=2.042693, validation/ctc_loss=30.769912719726562, validation/num_examples=5348, validation/wer=1.727368
I1005 22:08:28.907003 139661760706304 logging_writer.py:48] [1] global_step=1, grad_norm=25.33824920654297, loss=33.0052490234375
I1005 22:08:29.732019 139661769099008 logging_writer.py:48] [2] global_step=2, grad_norm=30.428646087646484, loss=32.51097106933594
I1005 22:08:30.568447 139661760706304 logging_writer.py:48] [3] global_step=3, grad_norm=38.88590621948242, loss=32.13796615600586
I1005 22:08:31.405142 139661769099008 logging_writer.py:48] [4] global_step=4, grad_norm=44.57761764526367, loss=31.313161849975586
I1005 22:08:32.352608 139661760706304 logging_writer.py:48] [5] global_step=5, grad_norm=45.39494705200195, loss=29.28394889831543
I1005 22:08:33.247148 139661769099008 logging_writer.py:48] [6] global_step=6, grad_norm=41.107513427734375, loss=27.172607421875
I1005 22:08:34.133646 139661760706304 logging_writer.py:48] [7] global_step=7, grad_norm=35.987178802490234, loss=25.205747604370117
I1005 22:08:35.041008 139661769099008 logging_writer.py:48] [8] global_step=8, grad_norm=31.19203758239746, loss=23.14064598083496
I1005 22:08:35.947366 139661760706304 logging_writer.py:48] [9] global_step=9, grad_norm=27.797805786132812, loss=21.71782684326172
I1005 22:08:36.845301 139661769099008 logging_writer.py:48] [10] global_step=10, grad_norm=24.609371185302734, loss=19.844661712646484
I1005 22:08:37.743220 139661760706304 logging_writer.py:48] [11] global_step=11, grad_norm=20.39624786376953, loss=18.49015998840332
I1005 22:08:38.653303 139661769099008 logging_writer.py:48] [12] global_step=12, grad_norm=18.94548988342285, loss=17.495576858520508
I1005 22:08:39.582428 139661760706304 logging_writer.py:48] [13] global_step=13, grad_norm=16.76132583618164, loss=15.684714317321777
I1005 22:08:40.487846 139661769099008 logging_writer.py:48] [14] global_step=14, grad_norm=15.337362289428711, loss=14.84363842010498
I1005 22:08:41.390635 139661760706304 logging_writer.py:48] [15] global_step=15, grad_norm=12.78054428100586, loss=13.894136428833008
I1005 22:08:42.295919 139661769099008 logging_writer.py:48] [16] global_step=16, grad_norm=10.198254585266113, loss=12.91741943359375
I1005 22:08:43.233814 139661760706304 logging_writer.py:48] [17] global_step=17, grad_norm=7.992696762084961, loss=11.904021263122559
I1005 22:08:44.151743 139661769099008 logging_writer.py:48] [18] global_step=18, grad_norm=6.306186199188232, loss=11.23493766784668
I1005 22:08:45.047597 139661760706304 logging_writer.py:48] [19] global_step=19, grad_norm=6.37123966217041, loss=10.644949913024902
I1005 22:08:45.934974 139661769099008 logging_writer.py:48] [20] global_step=20, grad_norm=6.9056925773620605, loss=10.351012229919434
I1005 22:08:46.827548 139661760706304 logging_writer.py:48] [21] global_step=21, grad_norm=5.082513809204102, loss=9.59183406829834
I1005 22:08:47.726502 139661769099008 logging_writer.py:48] [22] global_step=22, grad_norm=4.643838405609131, loss=9.204005241394043
I1005 22:08:48.603139 139661760706304 logging_writer.py:48] [23] global_step=23, grad_norm=4.370878219604492, loss=9.138192176818848
I1005 22:08:49.495889 139661769099008 logging_writer.py:48] [24] global_step=24, grad_norm=3.9851467609405518, loss=8.71625804901123
I1005 22:08:50.389879 139661760706304 logging_writer.py:48] [25] global_step=25, grad_norm=4.4333696365356445, loss=8.466364860534668
I1005 22:08:51.273176 139661769099008 logging_writer.py:48] [26] global_step=26, grad_norm=4.5975422859191895, loss=8.206851959228516
I1005 22:08:52.146974 139661760706304 logging_writer.py:48] [27] global_step=27, grad_norm=4.201681613922119, loss=7.889749050140381
I1005 22:08:53.043253 139661769099008 logging_writer.py:48] [28] global_step=28, grad_norm=6.621489524841309, loss=7.600052833557129
I1005 22:08:53.932223 139661760706304 logging_writer.py:48] [29] global_step=29, grad_norm=6.891790866851807, loss=7.531096458435059
I1005 22:08:54.805426 139661769099008 logging_writer.py:48] [30] global_step=30, grad_norm=7.720381736755371, loss=7.112002372741699
I1005 22:08:55.683454 139661760706304 logging_writer.py:48] [31] global_step=31, grad_norm=7.828800201416016, loss=7.119235515594482
I1005 22:08:56.573436 139661769099008 logging_writer.py:48] [32] global_step=32, grad_norm=4.687602996826172, loss=6.782065391540527
I1005 22:08:57.459987 139661760706304 logging_writer.py:48] [33] global_step=33, grad_norm=6.99336576461792, loss=6.768094539642334
I1005 22:08:58.331381 139661769099008 logging_writer.py:48] [34] global_step=34, grad_norm=8.9751615524292, loss=6.754878044128418
I1005 22:08:59.212759 139661760706304 logging_writer.py:48] [35] global_step=35, grad_norm=5.068031311035156, loss=6.489565372467041
I1005 22:09:00.103955 139661769099008 logging_writer.py:48] [36] global_step=36, grad_norm=3.44146466255188, loss=6.4047627449035645
I1005 22:09:00.989296 139661760706304 logging_writer.py:48] [37] global_step=37, grad_norm=5.399290561676025, loss=6.376364231109619
I1005 22:09:01.911097 139661769099008 logging_writer.py:48] [38] global_step=38, grad_norm=4.230582237243652, loss=6.351921558380127
I1005 22:09:02.790337 139661760706304 logging_writer.py:48] [39] global_step=39, grad_norm=1.6339006423950195, loss=6.18755578994751
I1005 22:09:03.684685 139661769099008 logging_writer.py:48] [40] global_step=40, grad_norm=1.2129815816879272, loss=6.159945964813232
I1005 22:09:04.556760 139661760706304 logging_writer.py:48] [41] global_step=41, grad_norm=1.8107037544250488, loss=6.116996765136719
I1005 22:09:05.444905 139661769099008 logging_writer.py:48] [42] global_step=42, grad_norm=5.807278633117676, loss=6.111494064331055
I1005 22:09:06.322480 139661760706304 logging_writer.py:48] [43] global_step=43, grad_norm=9.425817489624023, loss=6.2500152587890625
I1005 22:09:07.212325 139661769099008 logging_writer.py:48] [44] global_step=44, grad_norm=4.547593116760254, loss=6.06655740737915
I1005 22:09:08.091395 139661760706304 logging_writer.py:48] [45] global_step=45, grad_norm=0.47224950790405273, loss=5.980093002319336
I1005 22:09:08.974355 139661769099008 logging_writer.py:48] [46] global_step=46, grad_norm=1.1084482669830322, loss=6.001896381378174
I1005 22:09:09.851397 139661760706304 logging_writer.py:48] [47] global_step=47, grad_norm=2.11706805229187, loss=5.951315402984619
I1005 22:09:10.726336 139661769099008 logging_writer.py:48] [48] global_step=48, grad_norm=5.802981853485107, loss=6.034182071685791
I1005 22:09:11.606310 139661760706304 logging_writer.py:48] [49] global_step=49, grad_norm=7.060715198516846, loss=6.078448295593262
I1005 22:09:12.491305 139661769099008 logging_writer.py:48] [50] global_step=50, grad_norm=3.0415971279144287, loss=5.95943021774292
I1005 22:09:13.379592 139661760706304 logging_writer.py:48] [51] global_step=51, grad_norm=1.3131200075149536, loss=5.9623332023620605
I1005 22:09:14.262901 139661769099008 logging_writer.py:48] [52] global_step=52, grad_norm=1.783921480178833, loss=5.93740701675415
I1005 22:09:15.139492 139661760706304 logging_writer.py:48] [53] global_step=53, grad_norm=2.79076886177063, loss=5.9377899169921875
I1005 22:09:16.016387 139661769099008 logging_writer.py:48] [54] global_step=54, grad_norm=6.946045398712158, loss=6.0189127922058105
I1005 22:09:16.895728 139661760706304 logging_writer.py:48] [55] global_step=55, grad_norm=6.8073883056640625, loss=6.045246601104736
I1005 22:09:17.775629 139661769099008 logging_writer.py:48] [56] global_step=56, grad_norm=0.6616978049278259, loss=5.925036907196045
I1005 22:09:18.662231 139661760706304 logging_writer.py:48] [57] global_step=57, grad_norm=1.2815053462982178, loss=5.905610084533691
I1005 22:09:19.554181 139661769099008 logging_writer.py:48] [58] global_step=58, grad_norm=0.591110110282898, loss=5.911450386047363
I1005 22:09:20.441545 139661760706304 logging_writer.py:48] [59] global_step=59, grad_norm=1.7208914756774902, loss=5.914830207824707
I1005 22:09:21.351146 139661769099008 logging_writer.py:48] [60] global_step=60, grad_norm=3.8559372425079346, loss=5.942142486572266
I1005 22:09:22.261532 139661760706304 logging_writer.py:48] [61] global_step=61, grad_norm=7.648617267608643, loss=6.017259120941162
I1005 22:09:23.158523 139661769099008 logging_writer.py:48] [62] global_step=62, grad_norm=5.7217912673950195, loss=5.993207931518555
I1005 22:09:24.071778 139661760706304 logging_writer.py:48] [63] global_step=63, grad_norm=0.49181321263313293, loss=5.888967037200928
I1005 22:09:24.965632 139661769099008 logging_writer.py:48] [64] global_step=64, grad_norm=2.1528971195220947, loss=5.907588958740234
I1005 22:09:25.866117 139661760706304 logging_writer.py:48] [65] global_step=65, grad_norm=2.127262592315674, loss=5.915376663208008
I1005 22:09:26.767361 139661769099008 logging_writer.py:48] [66] global_step=66, grad_norm=2.6463310718536377, loss=5.897786617279053
I1005 22:09:27.652825 139661760706304 logging_writer.py:48] [67] global_step=67, grad_norm=4.450563430786133, loss=5.9282331466674805
I1005 22:09:28.529427 139661769099008 logging_writer.py:48] [68] global_step=68, grad_norm=6.315557479858398, loss=5.9601054191589355
I1005 22:09:29.407219 139661760706304 logging_writer.py:48] [69] global_step=69, grad_norm=4.838062286376953, loss=5.92719841003418
I1005 22:09:30.288736 139661769099008 logging_writer.py:48] [70] global_step=70, grad_norm=2.0063722133636475, loss=5.891650676727295
I1005 22:09:31.168160 139661760706304 logging_writer.py:48] [71] global_step=71, grad_norm=1.3562960624694824, loss=5.8741559982299805
I1005 22:09:32.040921 139661769099008 logging_writer.py:48] [72] global_step=72, grad_norm=1.8468592166900635, loss=5.873581886291504
I1005 22:09:32.929524 139661760706304 logging_writer.py:48] [73] global_step=73, grad_norm=3.423816204071045, loss=5.916632175445557
I1005 22:09:33.810265 139661769099008 logging_writer.py:48] [74] global_step=74, grad_norm=7.009513854980469, loss=5.946592330932617
I1005 22:09:34.683334 139661760706304 logging_writer.py:48] [75] global_step=75, grad_norm=5.422596454620361, loss=5.923370838165283
I1005 22:09:35.552186 139661769099008 logging_writer.py:48] [76] global_step=76, grad_norm=0.7211668491363525, loss=5.847383975982666
I1005 22:09:36.425626 139661760706304 logging_writer.py:48] [77] global_step=77, grad_norm=0.3660629093647003, loss=5.835318565368652
I1005 22:09:37.299069 139661769099008 logging_writer.py:48] [78] global_step=78, grad_norm=0.6966252326965332, loss=5.800188064575195
I1005 22:09:38.176939 139661760706304 logging_writer.py:48] [79] global_step=79, grad_norm=0.6365554928779602, loss=5.833272933959961
I1005 22:09:39.059755 139661769099008 logging_writer.py:48] [80] global_step=80, grad_norm=1.3318781852722168, loss=5.839478492736816
I1005 22:09:39.932463 139661760706304 logging_writer.py:48] [81] global_step=81, grad_norm=4.713551998138428, loss=5.88366174697876
I1005 22:09:40.809144 139661769099008 logging_writer.py:48] [82] global_step=82, grad_norm=9.54319953918457, loss=5.981325149536133
I1005 22:09:41.681334 139661760706304 logging_writer.py:48] [83] global_step=83, grad_norm=3.701751708984375, loss=5.848038673400879
I1005 22:09:42.557499 139661769099008 logging_writer.py:48] [84] global_step=84, grad_norm=1.3269836902618408, loss=5.814542293548584
I1005 22:09:43.430089 139661760706304 logging_writer.py:48] [85] global_step=85, grad_norm=0.8406311273574829, loss=5.7928996086120605
I1005 22:09:44.307674 139661769099008 logging_writer.py:48] [86] global_step=86, grad_norm=1.1131398677825928, loss=5.79548978805542
I1005 22:09:45.208593 139661760706304 logging_writer.py:48] [87] global_step=87, grad_norm=1.4856655597686768, loss=5.822157859802246
I1005 22:09:46.098446 139661769099008 logging_writer.py:48] [88] global_step=88, grad_norm=3.8437654972076416, loss=5.8180460929870605
I1005 22:09:46.997332 139661760706304 logging_writer.py:48] [89] global_step=89, grad_norm=15.58041763305664, loss=5.883306980133057
I1005 22:09:47.888896 139661769099008 logging_writer.py:48] [90] global_step=90, grad_norm=4.540832996368408, loss=5.836064338684082
I1005 22:09:48.782985 139661760706304 logging_writer.py:48] [91] global_step=91, grad_norm=1.738173484802246, loss=5.793048858642578
I1005 22:09:49.682477 139661769099008 logging_writer.py:48] [92] global_step=92, grad_norm=1.7642658948898315, loss=5.811594009399414
I1005 22:09:50.568763 139661760706304 logging_writer.py:48] [93] global_step=93, grad_norm=2.486992835998535, loss=5.816703796386719
I1005 22:09:51.460354 139661769099008 logging_writer.py:48] [94] global_step=94, grad_norm=4.382259845733643, loss=5.839110374450684
I1005 22:09:52.359235 139661760706304 logging_writer.py:48] [95] global_step=95, grad_norm=4.640510082244873, loss=5.832456111907959
I1005 22:09:53.248126 139661769099008 logging_writer.py:48] [96] global_step=96, grad_norm=3.3296968936920166, loss=5.792722702026367
I1005 22:09:54.141618 139661760706304 logging_writer.py:48] [97] global_step=97, grad_norm=2.6343798637390137, loss=5.809106826782227
I1005 22:09:55.030706 139661769099008 logging_writer.py:48] [98] global_step=98, grad_norm=3.231659173965454, loss=5.791330337524414
I1005 22:09:55.917471 139661760706304 logging_writer.py:48] [99] global_step=99, grad_norm=3.5942208766937256, loss=5.787324905395508
I1005 22:09:56.810583 139661769099008 logging_writer.py:48] [100] global_step=100, grad_norm=3.662876844406128, loss=5.803425312042236
I1005 22:14:56.701177 139661760706304 logging_writer.py:48] [500] global_step=500, grad_norm=1.8361104726791382, loss=2.982415199279785
I1005 22:22:00.833387 139661769099008 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.818669080734253, loss=2.4874114990234375
I1005 22:28:33.405788 139663768749824 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.8984798192977905, loss=2.3009328842163086
I1005 22:32:19.659733 139821904316224 spec.py:321] Evaluating on the training split.
I1005 22:33:14.387017 139821904316224 spec.py:333] Evaluating on the validation split.
I1005 22:34:04.066268 139821904316224 spec.py:349] Evaluating on the test split.
I1005 22:34:28.980387 139821904316224 submission_runner.py:381] Time since start: 1815.74s, 	Step: 1756, 	{'train/ctc_loss': Array(0.7864269, dtype=float32), 'train/wer': 0.2410763938950794, 'validation/ctc_loss': Array(1.2276683, dtype=float32), 'validation/wer': 0.3297861050275449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.84212476, dtype=float32), 'test/wer': 0.2577945686836065, 'test/num_examples': 2472, 'score': 1500.2411789894104, 'total_duration': 1815.7425785064697, 'accumulated_submission_time': 1500.2411789894104, 'accumulated_eval_time': 315.426025390625, 'accumulated_logging_time': 0.04021787643432617}
I1005 22:34:29.015358 139663768749824 logging_writer.py:48] [1756] accumulated_eval_time=315.426025, accumulated_logging_time=0.040218, accumulated_submission_time=1500.241179, global_step=1756, preemption_count=0, score=1500.241179, test/ctc_loss=0.8421247601509094, test/num_examples=2472, test/wer=0.257795, total_duration=1815.742579, train/ctc_loss=0.7864269018173218, train/wer=0.241076, validation/ctc_loss=1.227668285369873, validation/num_examples=5348, validation/wer=0.329786
I1005 22:37:33.166514 139663760357120 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.503040075302124, loss=2.16286039352417
I1005 22:44:04.344327 139663768749824 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.944309234619141, loss=2.052469491958618
I1005 22:51:21.858378 139663760357120 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.4022023677825928, loss=2.0909156799316406
I1005 22:58:00.315571 139663113389824 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.34975528717041, loss=2.0256967544555664
I1005 22:58:29.147046 139821904316224 spec.py:321] Evaluating on the training split.
I1005 22:59:22.270596 139821904316224 spec.py:333] Evaluating on the validation split.
I1005 23:00:12.035770 139821904316224 spec.py:349] Evaluating on the test split.
I1005 23:00:37.430416 139821904316224 submission_runner.py:381] Time since start: 3384.19s, 	Step: 3536, 	{'train/ctc_loss': Array(0.64847004, dtype=float32), 'train/wer': 0.20871001251897825, 'validation/ctc_loss': Array(1.0219175, dtype=float32), 'validation/wer': 0.2798386863356135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68159974, dtype=float32), 'test/wer': 0.21489651250177727, 'test/num_examples': 2472, 'score': 2940.321792125702, 'total_duration': 3384.191831588745, 'accumulated_submission_time': 2940.321792125702, 'accumulated_eval_time': 443.7027630805969, 'accumulated_logging_time': 0.09131360054016113}
I1005 23:00:37.468856 139663768749824 logging_writer.py:48] [3536] accumulated_eval_time=443.702763, accumulated_logging_time=0.091314, accumulated_submission_time=2940.321792, global_step=3536, preemption_count=0, score=2940.321792, test/ctc_loss=0.6815997362136841, test/num_examples=2472, test/wer=0.214897, total_duration=3384.191832, train/ctc_loss=0.6484700441360474, train/wer=0.208710, validation/ctc_loss=1.021917462348938, validation/num_examples=5348, validation/wer=0.279839
I1005 23:06:57.406805 139663760357120 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.630089044570923, loss=2.0448765754699707
I1005 23:13:36.420987 139663768749824 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.598635673522949, loss=1.9584957361221313
I1005 23:20:58.074867 139663760357120 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.0920262336730957, loss=1.9946738481521606
I1005 23:24:38.028686 139821904316224 spec.py:321] Evaluating on the training split.
I1005 23:25:33.337619 139821904316224 spec.py:333] Evaluating on the validation split.
I1005 23:26:23.223720 139821904316224 spec.py:349] Evaluating on the test split.
I1005 23:26:48.923334 139821904316224 submission_runner.py:381] Time since start: 4955.69s, 	Step: 5261, 	{'train/ctc_loss': Array(0.55869925, dtype=float32), 'train/wer': 0.18488648523280146, 'validation/ctc_loss': Array(0.94505787, dtype=float32), 'validation/wer': 0.26108307846674833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.616913, dtype=float32), 'test/wer': 0.1947474255072817, 'test/num_examples': 2472, 'score': 4380.831243276596, 'total_duration': 4955.6853704452515, 'accumulated_submission_time': 4380.831243276596, 'accumulated_eval_time': 574.591635465622, 'accumulated_logging_time': 0.14569759368896484}
I1005 23:26:48.956377 139663261869824 logging_writer.py:48] [5261] accumulated_eval_time=574.591635, accumulated_logging_time=0.145698, accumulated_submission_time=4380.831243, global_step=5261, preemption_count=0, score=4380.831243, test/ctc_loss=0.6169130206108093, test/num_examples=2472, test/wer=0.194747, total_duration=4955.685370, train/ctc_loss=0.5586992502212524, train/wer=0.184886, validation/ctc_loss=0.9450578689575195, validation/num_examples=5348, validation/wer=0.261083
I1005 23:29:48.334454 139663253477120 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.4850878715515137, loss=2.006035327911377
I1005 23:37:08.926662 139663261869824 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.0994770526885986, loss=1.8769254684448242
I1005 23:44:00.507441 139663261869824 logging_writer.py:48] [6500] global_step=6500, grad_norm=8.084965705871582, loss=1.9463258981704712
I1005 23:50:50.026844 139821904316224 spec.py:321] Evaluating on the training split.
I1005 23:51:45.384635 139821904316224 spec.py:333] Evaluating on the validation split.
I1005 23:52:36.163365 139821904316224 spec.py:349] Evaluating on the test split.
I1005 23:53:02.032882 139821904316224 submission_runner.py:381] Time since start: 6528.79s, 	Step: 6968, 	{'train/ctc_loss': Array(0.5617382, dtype=float32), 'train/wer': 0.17899443915005916, 'validation/ctc_loss': Array(0.9198307, dtype=float32), 'validation/wer': 0.25389535837297034, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.591965, dtype=float32), 'test/wer': 0.18392135356366665, 'test/num_examples': 2472, 'score': 5821.852741718292, 'total_duration': 6528.794979095459, 'accumulated_submission_time': 5821.852741718292, 'accumulated_eval_time': 706.5917866230011, 'accumulated_logging_time': 0.19396734237670898}
I1005 23:53:02.070893 139662754989824 logging_writer.py:48] [6968] accumulated_eval_time=706.591787, accumulated_logging_time=0.193967, accumulated_submission_time=5821.852742, global_step=6968, preemption_count=0, score=5821.852742, test/ctc_loss=0.5919650197029114, test/num_examples=2472, test/wer=0.183921, total_duration=6528.794979, train/ctc_loss=0.5617381930351257, train/wer=0.178994, validation/ctc_loss=0.9198306798934937, validation/num_examples=5348, validation/wer=0.253895
I1005 23:53:27.216316 139662746597120 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.963829517364502, loss=1.9849555492401123
I1005 23:59:57.561372 139662427309824 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.5074799060821533, loss=1.924751877784729
I1006 00:07:26.277680 139662418917120 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.212616443634033, loss=1.9520896673202515
I1006 00:14:33.528172 139662427309824 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.5471267700195312, loss=1.9560706615447998
I1006 00:17:02.306188 139821904316224 spec.py:321] Evaluating on the training split.
I1006 00:17:56.749500 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 00:18:47.820709 139821904316224 spec.py:349] Evaluating on the test split.
I1006 00:19:13.409968 139821904316224 submission_runner.py:381] Time since start: 8100.17s, 	Step: 8678, 	{'train/ctc_loss': Array(0.5424624, dtype=float32), 'train/wer': 0.1760638413533671, 'validation/ctc_loss': Array(0.891351, dtype=float32), 'validation/wer': 0.24924504819149243, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5773453, dtype=float32), 'test/wer': 0.18154489874677554, 'test/num_examples': 2472, 'score': 7262.037584543228, 'total_duration': 8100.171071052551, 'accumulated_submission_time': 7262.037584543228, 'accumulated_eval_time': 837.6886546611786, 'accumulated_logging_time': 0.2475128173828125}
I1006 00:19:13.443218 139662427309824 logging_writer.py:48] [8678] accumulated_eval_time=837.688655, accumulated_logging_time=0.247513, accumulated_submission_time=7262.037585, global_step=8678, preemption_count=0, score=7262.037585, test/ctc_loss=0.5773453116416931, test/num_examples=2472, test/wer=0.181545, total_duration=8100.171071, train/ctc_loss=0.5424624085426331, train/wer=0.176064, validation/ctc_loss=0.8913509845733643, validation/num_examples=5348, validation/wer=0.249245
I1006 00:23:35.173645 139662418917120 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.403467893600464, loss=1.9009379148483276
I1006 00:30:38.982446 139662754989824 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.6871583461761475, loss=1.8035732507705688
I1006 00:37:55.053582 139662746597120 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.7034125328063965, loss=1.8773068189620972
I1006 00:43:14.049265 139821904316224 spec.py:321] Evaluating on the training split.
I1006 00:44:08.104288 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 00:45:00.193826 139821904316224 spec.py:349] Evaluating on the test split.
I1006 00:45:25.703958 139821904316224 submission_runner.py:381] Time since start: 9672.47s, 	Step: 10360, 	{'train/ctc_loss': Array(0.53431463, dtype=float32), 'train/wer': 0.17031117465364282, 'validation/ctc_loss': Array(0.8380471, dtype=float32), 'validation/wer': 0.23540024505783944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5362395, dtype=float32), 'test/wer': 0.16651432981943007, 'test/num_examples': 2472, 'score': 8702.593554019928, 'total_duration': 9672.465755462646, 'accumulated_submission_time': 8702.593554019928, 'accumulated_eval_time': 969.3373708724976, 'accumulated_logging_time': 0.29662537574768066}
I1006 00:45:25.743637 139662427309824 logging_writer.py:48] [10360] accumulated_eval_time=969.337371, accumulated_logging_time=0.296625, accumulated_submission_time=8702.593554, global_step=10360, preemption_count=0, score=8702.593554, test/ctc_loss=0.536239504814148, test/num_examples=2472, test/wer=0.166514, total_duration=9672.465755, train/ctc_loss=0.5343146324157715, train/wer=0.170311, validation/ctc_loss=0.8380470871925354, validation/num_examples=5348, validation/wer=0.235400
I1006 00:47:10.897178 139662418917120 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.1905221939086914, loss=1.8645706176757812
I1006 00:54:04.210241 139662427309824 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.575953960418701, loss=1.8139188289642334
I1006 01:01:16.300209 139662427309824 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.145261526107788, loss=1.7945784330368042
I1006 01:08:23.459126 139662418917120 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.290276050567627, loss=1.8823158740997314
I1006 01:09:26.093855 139821904316224 spec.py:321] Evaluating on the training split.
I1006 01:10:21.277111 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 01:11:12.410068 139821904316224 spec.py:349] Evaluating on the test split.
I1006 01:11:38.583330 139821904316224 submission_runner.py:381] Time since start: 11245.35s, 	Step: 12071, 	{'train/ctc_loss': Array(0.48240015, dtype=float32), 'train/wer': 0.1580998872652245, 'validation/ctc_loss': Array(0.8067362, dtype=float32), 'validation/wer': 0.22743104130285868, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50352377, dtype=float32), 'test/wer': 0.15938496536875674, 'test/num_examples': 2472, 'score': 10142.895012617111, 'total_duration': 11245.345475912094, 'accumulated_submission_time': 10142.895012617111, 'accumulated_eval_time': 1101.8209691047668, 'accumulated_logging_time': 0.3508286476135254}
I1006 01:11:38.615029 139662427309824 logging_writer.py:48] [12071] accumulated_eval_time=1101.820969, accumulated_logging_time=0.350829, accumulated_submission_time=10142.895013, global_step=12071, preemption_count=0, score=10142.895013, test/ctc_loss=0.5035237669944763, test/num_examples=2472, test/wer=0.159385, total_duration=11245.345476, train/ctc_loss=0.48240014910697937, train/wer=0.158100, validation/ctc_loss=0.806736171245575, validation/num_examples=5348, validation/wer=0.227431
I1006 01:17:21.694678 139662754989824 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.4271554946899414, loss=1.7533382177352905
I1006 01:24:31.946465 139662746597120 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.3776583671569824, loss=1.790344476699829
I1006 01:31:54.899604 139662099629824 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.496403455734253, loss=1.6842008829116821
I1006 01:35:39.782479 139821904316224 spec.py:321] Evaluating on the training split.
I1006 01:36:35.107979 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 01:37:27.117282 139821904316224 spec.py:349] Evaluating on the test split.
I1006 01:37:52.488309 139821904316224 submission_runner.py:381] Time since start: 12819.25s, 	Step: 13776, 	{'train/ctc_loss': Array(0.4215641, dtype=float32), 'train/wer': 0.14147013702904393, 'validation/ctc_loss': Array(0.7837072, dtype=float32), 'validation/wer': 0.22020472942334224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48125437, dtype=float32), 'test/wer': 0.15156500721061078, 'test/num_examples': 2472, 'score': 11584.013818740845, 'total_duration': 12819.248528718948, 'accumulated_submission_time': 11584.013818740845, 'accumulated_eval_time': 1234.5190012454987, 'accumulated_logging_time': 0.3971695899963379}
I1006 01:37:52.528286 139662099629824 logging_writer.py:48] [13776] accumulated_eval_time=1234.519001, accumulated_logging_time=0.397170, accumulated_submission_time=11584.013819, global_step=13776, preemption_count=0, score=11584.013819, test/ctc_loss=0.48125436902046204, test/num_examples=2472, test/wer=0.151565, total_duration=12819.248529, train/ctc_loss=0.42156410217285156, train/wer=0.141470, validation/ctc_loss=0.7837072014808655, validation/num_examples=5348, validation/wer=0.220205
I1006 01:40:43.653931 139662091237120 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.485283136367798, loss=1.7334016561508179
I1006 01:48:12.939667 139662754989824 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.3026249408721924, loss=1.7330046892166138
I1006 01:55:09.539370 139662746597120 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.6642038822174072, loss=1.752781867980957
I1006 02:01:52.817835 139821904316224 spec.py:321] Evaluating on the training split.
I1006 02:02:47.466823 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 02:03:38.626335 139821904316224 spec.py:349] Evaluating on the test split.
I1006 02:04:04.414988 139821904316224 submission_runner.py:381] Time since start: 14391.18s, 	Step: 15450, 	{'train/ctc_loss': Array(0.3998983, dtype=float32), 'train/wer': 0.1309577643122004, 'validation/ctc_loss': Array(0.7513619, dtype=float32), 'validation/wer': 0.2149562465629191, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4669773, dtype=float32), 'test/wer': 0.14476062803404222, 'test/num_examples': 2472, 'score': 13024.255194664001, 'total_duration': 14391.177129507065, 'accumulated_submission_time': 13024.255194664001, 'accumulated_eval_time': 1366.1103031635284, 'accumulated_logging_time': 0.4521174430847168}
I1006 02:04:04.457351 139662754989824 logging_writer.py:48] [15450] accumulated_eval_time=1366.110303, accumulated_logging_time=0.452117, accumulated_submission_time=13024.255195, global_step=15450, preemption_count=0, score=13024.255195, test/ctc_loss=0.4669772982597351, test/num_examples=2472, test/wer=0.144761, total_duration=14391.177130, train/ctc_loss=0.3998982906341553, train/wer=0.130958, validation/ctc_loss=0.7513619065284729, validation/num_examples=5348, validation/wer=0.214956
I1006 02:04:47.380815 139662754989824 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.8253395557403564, loss=1.718660831451416
I1006 02:11:35.944256 139662746597120 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.1612839698791504, loss=1.6911635398864746
I1006 02:19:05.416485 139662754989824 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.5673322677612305, loss=1.654190182685852
I1006 02:25:51.489259 139662746597120 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.09920072555542, loss=1.666766881942749
I1006 02:28:04.505344 139821904316224 spec.py:321] Evaluating on the training split.
I1006 02:28:58.526454 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 02:29:49.599599 139821904316224 spec.py:349] Evaluating on the test split.
I1006 02:30:15.480625 139821904316224 submission_runner.py:381] Time since start: 15962.24s, 	Step: 17146, 	{'train/ctc_loss': Array(0.3869279, dtype=float32), 'train/wer': 0.1299211120466356, 'validation/ctc_loss': Array(0.7105021, dtype=float32), 'validation/wer': 0.1997703788748565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4342872, dtype=float32), 'test/wer': 0.13610789511100277, 'test/num_examples': 2472, 'score': 14464.25069475174, 'total_duration': 15962.242888212204, 'accumulated_submission_time': 14464.25069475174, 'accumulated_eval_time': 1497.079836845398, 'accumulated_logging_time': 0.513319730758667}
I1006 02:30:15.516317 139662754989824 logging_writer.py:48] [17146] accumulated_eval_time=1497.079837, accumulated_logging_time=0.513320, accumulated_submission_time=14464.250695, global_step=17146, preemption_count=0, score=14464.250695, test/ctc_loss=0.4342871904373169, test/num_examples=2472, test/wer=0.136108, total_duration=15962.242888, train/ctc_loss=0.38692790269851685, train/wer=0.129921, validation/ctc_loss=0.7105020880699158, validation/num_examples=5348, validation/wer=0.199770
I1006 02:35:04.438182 139662746597120 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.8994777202606201, loss=1.6770317554473877
I1006 02:42:02.979134 139662754989824 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.886305809020996, loss=1.6688584089279175
I1006 02:49:42.120752 139662746597120 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.3875415325164795, loss=1.6405119895935059
I1006 02:54:16.171179 139821904316224 spec.py:321] Evaluating on the training split.
I1006 02:55:11.640293 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 02:56:02.942083 139821904316224 spec.py:349] Evaluating on the test split.
I1006 02:56:28.905808 139821904316224 submission_runner.py:381] Time since start: 17535.67s, 	Step: 18843, 	{'train/ctc_loss': Array(0.37419647, dtype=float32), 'train/wer': 0.12054258441524102, 'validation/ctc_loss': Array(0.6736266, dtype=float32), 'validation/wer': 0.18975581047574025, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40205064, dtype=float32), 'test/wer': 0.12515995368959845, 'test/num_examples': 2472, 'score': 15904.856006622314, 'total_duration': 17535.671298265457, 'accumulated_submission_time': 15904.856006622314, 'accumulated_eval_time': 1629.8120965957642, 'accumulated_logging_time': 0.5649142265319824}
I1006 02:56:28.934025 139662754989824 logging_writer.py:48] [18843] accumulated_eval_time=1629.812097, accumulated_logging_time=0.564914, accumulated_submission_time=15904.856007, global_step=18843, preemption_count=0, score=15904.856007, test/ctc_loss=0.402050644159317, test/num_examples=2472, test/wer=0.125160, total_duration=17535.671298, train/ctc_loss=0.37419646978378296, train/wer=0.120543, validation/ctc_loss=0.6736266016960144, validation/num_examples=5348, validation/wer=0.189756
I1006 02:58:30.373155 139662746597120 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.0400688648223877, loss=1.6018880605697632
I1006 03:05:50.169992 139662754989824 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7311460971832275, loss=1.56330144405365
I1006 03:12:39.803794 139662754989824 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.950772762298584, loss=1.5561941862106323
I1006 03:20:15.641526 139662746597120 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.635279417037964, loss=1.6346136331558228
I1006 03:20:29.604862 139821904316224 spec.py:321] Evaluating on the training split.
I1006 03:21:24.700632 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 03:22:15.886263 139821904316224 spec.py:349] Evaluating on the test split.
I1006 03:22:41.422920 139821904316224 submission_runner.py:381] Time since start: 19108.18s, 	Step: 20517, 	{'train/ctc_loss': Array(0.36038643, dtype=float32), 'train/wer': 0.11729232067955786, 'validation/ctc_loss': Array(0.64048874, dtype=float32), 'validation/wer': 0.18223041225675116, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37996715, dtype=float32), 'test/wer': 0.12105701460402575, 'test/num_examples': 2472, 'score': 17345.481957912445, 'total_duration': 19108.183992385864, 'accumulated_submission_time': 17345.481957912445, 'accumulated_eval_time': 1761.6231853961945, 'accumulated_logging_time': 0.6045510768890381}
I1006 03:22:41.457195 139662754989824 logging_writer.py:48] [20517] accumulated_eval_time=1761.623185, accumulated_logging_time=0.604551, accumulated_submission_time=17345.481958, global_step=20517, preemption_count=0, score=17345.481958, test/ctc_loss=0.3799671530723572, test/num_examples=2472, test/wer=0.121057, total_duration=19108.183992, train/ctc_loss=0.3603864312171936, train/wer=0.117292, validation/ctc_loss=0.6404887437820435, validation/num_examples=5348, validation/wer=0.182230
I1006 03:29:08.063866 139662754989824 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.932936429977417, loss=1.5751383304595947
I1006 03:36:43.712390 139662746597120 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.5367887020111084, loss=1.5569145679473877
I1006 03:43:37.018768 139662754989824 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.1462242603302, loss=1.489201307296753
I1006 03:46:42.016222 139821904316224 spec.py:321] Evaluating on the training split.
I1006 03:47:39.246418 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 03:48:31.276421 139821904316224 spec.py:349] Evaluating on the test split.
I1006 03:48:57.136355 139821904316224 submission_runner.py:381] Time since start: 20683.90s, 	Step: 22209, 	{'train/ctc_loss': Array(0.32083884, dtype=float32), 'train/wer': 0.1072692663854306, 'validation/ctc_loss': Array(0.60738987, dtype=float32), 'validation/wer': 0.17400071394803615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3566334, dtype=float32), 'test/wer': 0.11398858489224707, 'test/num_examples': 2472, 'score': 18785.99226617813, 'total_duration': 20683.899030685425, 'accumulated_submission_time': 18785.99226617813, 'accumulated_eval_time': 1896.7379913330078, 'accumulated_logging_time': 0.653832197189331}
I1006 03:48:57.171174 139662754989824 logging_writer.py:48] [22209] accumulated_eval_time=1896.737991, accumulated_logging_time=0.653832, accumulated_submission_time=18785.992266, global_step=22209, preemption_count=0, score=18785.992266, test/ctc_loss=0.35663339495658875, test/num_examples=2472, test/wer=0.113989, total_duration=20683.899031, train/ctc_loss=0.3208388388156891, train/wer=0.107269, validation/ctc_loss=0.6073898673057556, validation/num_examples=5348, validation/wer=0.174001
I1006 03:52:52.169574 139662746597120 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.3697450160980225, loss=1.5575155019760132
I1006 03:59:53.747653 139662754989824 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.0494067668914795, loss=1.49210786819458
I1006 04:07:28.437747 139662746597120 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.6509355306625366, loss=1.487432599067688
I1006 04:12:57.398760 139821904316224 spec.py:321] Evaluating on the training split.
I1006 04:13:51.560957 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 04:14:42.272083 139821904316224 spec.py:349] Evaluating on the test split.
I1006 04:15:07.796392 139821904316224 submission_runner.py:381] Time since start: 22254.56s, 	Step: 23882, 	{'train/ctc_loss': Array(0.2835242, dtype=float32), 'train/wer': 0.09528736117562603, 'validation/ctc_loss': Array(0.5716396, dtype=float32), 'validation/wer': 0.1640150893882237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33034423, dtype=float32), 'test/wer': 0.10631080779152195, 'test/num_examples': 2472, 'score': 20226.171179771423, 'total_duration': 22254.558356761932, 'accumulated_submission_time': 20226.171179771423, 'accumulated_eval_time': 2027.1297717094421, 'accumulated_logging_time': 0.7045257091522217}
I1006 04:15:07.832243 139662754989824 logging_writer.py:48] [23882] accumulated_eval_time=2027.129772, accumulated_logging_time=0.704526, accumulated_submission_time=20226.171180, global_step=23882, preemption_count=0, score=20226.171180, test/ctc_loss=0.33034422993659973, test/num_examples=2472, test/wer=0.106311, total_duration=22254.558357, train/ctc_loss=0.28352418541908264, train/wer=0.095287, validation/ctc_loss=0.5716395974159241, validation/num_examples=5348, validation/wer=0.164015
I1006 04:16:37.102468 139662746597120 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.1375679969787598, loss=1.4653297662734985
I1006 04:23:58.291428 139662754989824 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.2976605892181396, loss=1.4604675769805908
I1006 04:30:57.976498 139662754989824 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.7360284328460693, loss=1.423575520515442
I1006 04:38:20.364023 139662746597120 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.2415337562561035, loss=1.3920761346817017
I1006 04:39:08.492272 139821904316224 spec.py:321] Evaluating on the training split.
I1006 04:40:04.496572 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 04:40:55.140327 139821904316224 spec.py:349] Evaluating on the test split.
I1006 04:41:20.909862 139821904316224 submission_runner.py:381] Time since start: 23827.67s, 	Step: 25554, 	{'train/ctc_loss': Array(0.25289425, dtype=float32), 'train/wer': 0.08755575262787241, 'validation/ctc_loss': Array(0.54563844, dtype=float32), 'validation/wer': 0.15645109938349622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31139702, dtype=float32), 'test/wer': 0.09975016757053196, 'test/num_examples': 2472, 'score': 21666.783137321472, 'total_duration': 23827.672288894653, 'accumulated_submission_time': 21666.783137321472, 'accumulated_eval_time': 2159.541759490967, 'accumulated_logging_time': 0.7551836967468262}
I1006 04:41:20.946547 139662754989824 logging_writer.py:48] [25554] accumulated_eval_time=2159.541759, accumulated_logging_time=0.755184, accumulated_submission_time=21666.783137, global_step=25554, preemption_count=0, score=21666.783137, test/ctc_loss=0.3113970160484314, test/num_examples=2472, test/wer=0.099750, total_duration=23827.672289, train/ctc_loss=0.25289425253868103, train/wer=0.087556, validation/ctc_loss=0.5456384420394897, validation/num_examples=5348, validation/wer=0.156451
I1006 04:47:05.847488 139662754989824 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.765415906906128, loss=1.4013534784317017
I1006 04:54:26.174905 139662746597120 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.8805723190307617, loss=1.4399456977844238
I1006 05:01:31.984495 139662754989824 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.424626111984253, loss=1.4056165218353271
I1006 05:05:21.387457 139821904316224 spec.py:321] Evaluating on the training split.
I1006 05:06:18.611006 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 05:07:09.387649 139821904316224 spec.py:349] Evaluating on the test split.
I1006 05:07:34.999828 139821904316224 submission_runner.py:381] Time since start: 25401.76s, 	Step: 27273, 	{'train/ctc_loss': Array(0.23077488, dtype=float32), 'train/wer': 0.07650530320851787, 'validation/ctc_loss': Array(0.5115235, dtype=float32), 'validation/wer': 0.14748815714575153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29063657, dtype=float32), 'test/wer': 0.09172709361607052, 'test/num_examples': 2472, 'score': 23107.173696279526, 'total_duration': 25401.76122689247, 'accumulated_submission_time': 23107.173696279526, 'accumulated_eval_time': 2293.147508621216, 'accumulated_logging_time': 0.8076622486114502}
I1006 05:07:35.042928 139662754989824 logging_writer.py:48] [27273] accumulated_eval_time=2293.147509, accumulated_logging_time=0.807662, accumulated_submission_time=23107.173696, global_step=27273, preemption_count=0, score=23107.173696, test/ctc_loss=0.2906365692615509, test/num_examples=2472, test/wer=0.091727, total_duration=25401.761227, train/ctc_loss=0.2307748794555664, train/wer=0.076505, validation/ctc_loss=0.5115234851837158, validation/num_examples=5348, validation/wer=0.147488
I1006 05:10:33.371119 139662746597120 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5685349702835083, loss=1.3167636394500732
I1006 05:17:46.382323 139662754989824 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.7152652740478516, loss=1.3854187726974487
I1006 05:25:05.130924 139662746597120 logging_writer.py:48] [28500] global_step=28500, grad_norm=4.248451232910156, loss=1.3252389430999756
I1006 05:31:35.045793 139821904316224 spec.py:321] Evaluating on the training split.
I1006 05:32:31.775813 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 05:33:24.143424 139821904316224 spec.py:349] Evaluating on the test split.
I1006 05:33:49.622758 139821904316224 submission_runner.py:381] Time since start: 26976.38s, 	Step: 28931, 	{'train/ctc_loss': Array(0.22995028, dtype=float32), 'train/wer': 0.07742893049055295, 'validation/ctc_loss': Array(0.48274842, dtype=float32), 'validation/wer': 0.13813929705062278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27128604, dtype=float32), 'test/wer': 0.08585704710255317, 'test/num_examples': 2472, 'score': 24547.12677192688, 'total_duration': 26976.383147239685, 'accumulated_submission_time': 24547.12677192688, 'accumulated_eval_time': 2427.7170763015747, 'accumulated_logging_time': 0.8673276901245117}
I1006 05:33:49.664263 139662754989824 logging_writer.py:48] [28931] accumulated_eval_time=2427.717076, accumulated_logging_time=0.867328, accumulated_submission_time=24547.126772, global_step=28931, preemption_count=0, score=24547.126772, test/ctc_loss=0.2712860405445099, test/num_examples=2472, test/wer=0.085857, total_duration=26976.383147, train/ctc_loss=0.22995027899742126, train/wer=0.077429, validation/ctc_loss=0.482748419046402, validation/num_examples=5348, validation/wer=0.138139
I1006 05:34:43.687787 139662746597120 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.4836971759796143, loss=1.3350861072540283
I1006 05:41:41.341529 139662754989824 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.662102222442627, loss=1.3477742671966553
I1006 05:49:03.613062 139662754989824 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.8705174922943115, loss=1.2793217897415161
I1006 05:56:15.647495 139662746597120 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.8465529680252075, loss=1.2547682523727417
I1006 05:57:49.772281 139821904316224 spec.py:321] Evaluating on the training split.
I1006 05:58:45.006026 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 05:59:35.920886 139821904316224 spec.py:349] Evaluating on the test split.
I1006 06:00:01.627380 139821904316224 submission_runner.py:381] Time since start: 28548.39s, 	Step: 30603, 	{'train/ctc_loss': Array(0.20214693, dtype=float32), 'train/wer': 0.06679906022279014, 'validation/ctc_loss': Array(0.46044794, dtype=float32), 'validation/wer': 0.13249524838638096, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2545671, dtype=float32), 'test/wer': 0.08065728271687689, 'test/num_examples': 2472, 'score': 25987.186353206635, 'total_duration': 28548.389353752136, 'accumulated_submission_time': 25987.186353206635, 'accumulated_eval_time': 2559.566133737564, 'accumulated_logging_time': 0.9241898059844971}
I1006 06:00:01.667731 139662970029824 logging_writer.py:48] [30603] accumulated_eval_time=2559.566134, accumulated_logging_time=0.924190, accumulated_submission_time=25987.186353, global_step=30603, preemption_count=0, score=25987.186353, test/ctc_loss=0.25456708669662476, test/num_examples=2472, test/wer=0.080657, total_duration=28548.389354, train/ctc_loss=0.20214693248271942, train/wer=0.066799, validation/ctc_loss=0.46044793725013733, validation/num_examples=5348, validation/wer=0.132495
I1006 06:05:22.865434 139662970029824 logging_writer.py:48] [31000] global_step=31000, grad_norm=4.231259822845459, loss=1.3344566822052002
I1006 06:12:32.689713 139662961637120 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7622004747390747, loss=1.2785613536834717
I1006 06:19:59.762099 139662099629824 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.697324752807617, loss=1.2448301315307617
I1006 06:24:02.033379 139821904316224 spec.py:321] Evaluating on the training split.
I1006 06:24:57.255986 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 06:25:48.732685 139821904316224 spec.py:349] Evaluating on the test split.
I1006 06:26:15.880955 139821904316224 submission_runner.py:381] Time since start: 30122.64s, 	Step: 32305, 	{'train/ctc_loss': Array(0.20133561, dtype=float32), 'train/wer': 0.06663618757994338, 'validation/ctc_loss': Array(0.43979, dtype=float32), 'validation/wer': 0.12565485436424856, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24128245, dtype=float32), 'test/wer': 0.07588406150346312, 'test/num_examples': 2472, 'score': 27427.502437353134, 'total_duration': 30122.642430067062, 'accumulated_submission_time': 27427.502437353134, 'accumulated_eval_time': 2693.4071941375732, 'accumulated_logging_time': 0.9798026084899902}
I1006 06:26:15.917731 139662970029824 logging_writer.py:48] [32305] accumulated_eval_time=2693.407194, accumulated_logging_time=0.979803, accumulated_submission_time=27427.502437, global_step=32305, preemption_count=0, score=27427.502437, test/ctc_loss=0.24128244817256927, test/num_examples=2472, test/wer=0.075884, total_duration=30122.642430, train/ctc_loss=0.201335608959198, train/wer=0.066636, validation/ctc_loss=0.4397900104522705, validation/num_examples=5348, validation/wer=0.125655
I1006 06:28:43.810616 139662961637120 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.148700714111328, loss=1.2702842950820923
I1006 06:36:22.311679 139662970029824 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.57613205909729, loss=1.2369370460510254
I1006 06:43:22.841904 139662961637120 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.4331231117248535, loss=1.1458853483200073
I1006 06:50:16.166738 139821904316224 spec.py:321] Evaluating on the training split.
I1006 06:51:12.002873 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 06:52:04.284159 139821904316224 spec.py:349] Evaluating on the test split.
I1006 06:52:30.461584 139821904316224 submission_runner.py:381] Time since start: 31697.22s, 	Step: 33950, 	{'train/ctc_loss': Array(0.19308913, dtype=float32), 'train/wer': 0.06138836493067767, 'validation/ctc_loss': Array(0.42690834, dtype=float32), 'validation/wer': 0.12286659784464876, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23378204, dtype=float32), 'test/wer': 0.07334511404951963, 'test/num_examples': 2472, 'score': 28867.70143842697, 'total_duration': 31697.223484039307, 'accumulated_submission_time': 28867.70143842697, 'accumulated_eval_time': 2827.6959364414215, 'accumulated_logging_time': 1.0333607196807861}
I1006 06:52:30.501086 139663046829824 logging_writer.py:48] [33950] accumulated_eval_time=2827.695936, accumulated_logging_time=1.033361, accumulated_submission_time=28867.701438, global_step=33950, preemption_count=0, score=28867.701438, test/ctc_loss=0.23378203809261322, test/num_examples=2472, test/wer=0.073345, total_duration=31697.223484, train/ctc_loss=0.19308912754058838, train/wer=0.061388, validation/ctc_loss=0.42690834403038025, validation/num_examples=5348, validation/wer=0.122867
I1006 06:53:13.406156 139662391469824 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.0367822647094727, loss=1.1688323020935059
I1006 07:00:06.073252 139662383077120 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.7754812240600586, loss=1.1749837398529053
I1006 07:07:42.479300 139662391469824 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.101184606552124, loss=1.222816824913025
I1006 07:14:37.512124 139662391469824 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.040208578109741, loss=1.201899528503418
I1006 07:16:31.170027 139821904316224 spec.py:321] Evaluating on the training split.
I1006 07:17:27.896987 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 07:18:20.203781 139821904316224 spec.py:349] Evaluating on the test split.
I1006 07:18:46.372825 139821904316224 submission_runner.py:381] Time since start: 33273.13s, 	Step: 35627, 	{'train/ctc_loss': Array(0.14589979, dtype=float32), 'train/wer': 0.05061857466457571, 'validation/ctc_loss': Array(0.42508355, dtype=float32), 'validation/wer': 0.12191145114762322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23114148, dtype=float32), 'test/wer': 0.07285763613836248, 'test/num_examples': 2472, 'score': 30308.3208425045, 'total_duration': 33273.13421392441, 'accumulated_submission_time': 30308.3208425045, 'accumulated_eval_time': 2962.8920962810516, 'accumulated_logging_time': 1.0885753631591797}
I1006 07:18:46.411236 139662391469824 logging_writer.py:48] [35627] accumulated_eval_time=2962.892096, accumulated_logging_time=1.088575, accumulated_submission_time=30308.320843, global_step=35627, preemption_count=0, score=30308.320843, test/ctc_loss=0.23114147782325745, test/num_examples=2472, test/wer=0.072858, total_duration=33273.134214, train/ctc_loss=0.14589978754520416, train/wer=0.050619, validation/ctc_loss=0.42508354783058167, validation/num_examples=5348, validation/wer=0.121911
I1006 07:23:51.593182 139821904316224 spec.py:321] Evaluating on the training split.
I1006 07:24:46.788525 139821904316224 spec.py:333] Evaluating on the validation split.
I1006 07:25:38.887706 139821904316224 spec.py:349] Evaluating on the test split.
I1006 07:26:04.747992 139821904316224 submission_runner.py:381] Time since start: 33711.51s, 	Step: 36000, 	{'train/ctc_loss': Array(0.17479801, dtype=float32), 'train/wer': 0.05793964224620388, 'validation/ctc_loss': Array(0.42536253, dtype=float32), 'validation/wer': 0.12182461962971182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23126462, dtype=float32), 'test/wer': 0.07318262141246724, 'test/num_examples': 2472, 'score': 30613.479635238647, 'total_duration': 33711.51296496391, 'accumulated_submission_time': 30613.479635238647, 'accumulated_eval_time': 3096.0438344478607, 'accumulated_logging_time': 1.141819953918457}
I1006 07:26:04.785160 139663476909824 logging_writer.py:48] [36000] accumulated_eval_time=3096.043834, accumulated_logging_time=1.141820, accumulated_submission_time=30613.479635, global_step=36000, preemption_count=0, score=30613.479635, test/ctc_loss=0.2312646210193634, test/num_examples=2472, test/wer=0.073183, total_duration=33711.512965, train/ctc_loss=0.17479801177978516, train/wer=0.057940, validation/ctc_loss=0.4253625273704529, validation/num_examples=5348, validation/wer=0.121825
I1006 07:26:04.810225 139663468517120 logging_writer.py:48] [36000] global_step=36000, preemption_count=0, score=30613.479635
I1006 07:26:05.089687 139821904316224 checkpoints.py:490] Saving checkpoint at step: 36000
I1006 07:26:06.212783 139821904316224 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1/checkpoint_36000
I1006 07:26:06.233882 139821904316224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_jax/trial_1/checkpoint_36000.
I1006 07:26:07.697900 139821904316224 submission_runner.py:549] Tuning trial 1/1
I1006 07:26:07.698178 139821904316224 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.004958460849689891, beta1=0.863744242567442, beta2=0.6291854735396584, warmup_steps=720, weight_decay=0.1147386261512052)
I1006 07:26:07.710005 139821904316224 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.686937, dtype=float32), 'train/wer': 2.0426930845636466, 'validation/ctc_loss': Array(30.769913, dtype=float32), 'validation/wer': 1.7273683296510338, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.775833, dtype=float32), 'test/wer': 2.0614831515446954, 'test/num_examples': 2472, 'score': 59.93907356262207, 'total_duration': 246.05032634735107, 'accumulated_submission_time': 59.93907356262207, 'accumulated_eval_time': 186.11118054389954, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1756, {'train/ctc_loss': Array(0.7864269, dtype=float32), 'train/wer': 0.2410763938950794, 'validation/ctc_loss': Array(1.2276683, dtype=float32), 'validation/wer': 0.3297861050275449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.84212476, dtype=float32), 'test/wer': 0.2577945686836065, 'test/num_examples': 2472, 'score': 1500.2411789894104, 'total_duration': 1815.7425785064697, 'accumulated_submission_time': 1500.2411789894104, 'accumulated_eval_time': 315.426025390625, 'accumulated_logging_time': 0.04021787643432617, 'global_step': 1756, 'preemption_count': 0}), (3536, {'train/ctc_loss': Array(0.64847004, dtype=float32), 'train/wer': 0.20871001251897825, 'validation/ctc_loss': Array(1.0219175, dtype=float32), 'validation/wer': 0.2798386863356135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68159974, dtype=float32), 'test/wer': 0.21489651250177727, 'test/num_examples': 2472, 'score': 2940.321792125702, 'total_duration': 3384.191831588745, 'accumulated_submission_time': 2940.321792125702, 'accumulated_eval_time': 443.7027630805969, 'accumulated_logging_time': 0.09131360054016113, 'global_step': 3536, 'preemption_count': 0}), (5261, {'train/ctc_loss': Array(0.55869925, dtype=float32), 'train/wer': 0.18488648523280146, 'validation/ctc_loss': Array(0.94505787, dtype=float32), 'validation/wer': 0.26108307846674833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.616913, dtype=float32), 'test/wer': 0.1947474255072817, 'test/num_examples': 2472, 'score': 4380.831243276596, 'total_duration': 4955.6853704452515, 'accumulated_submission_time': 4380.831243276596, 'accumulated_eval_time': 574.591635465622, 'accumulated_logging_time': 0.14569759368896484, 'global_step': 5261, 'preemption_count': 0}), (6968, {'train/ctc_loss': Array(0.5617382, dtype=float32), 'train/wer': 0.17899443915005916, 'validation/ctc_loss': Array(0.9198307, dtype=float32), 'validation/wer': 0.25389535837297034, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.591965, dtype=float32), 'test/wer': 0.18392135356366665, 'test/num_examples': 2472, 'score': 5821.852741718292, 'total_duration': 6528.794979095459, 'accumulated_submission_time': 5821.852741718292, 'accumulated_eval_time': 706.5917866230011, 'accumulated_logging_time': 0.19396734237670898, 'global_step': 6968, 'preemption_count': 0}), (8678, {'train/ctc_loss': Array(0.5424624, dtype=float32), 'train/wer': 0.1760638413533671, 'validation/ctc_loss': Array(0.891351, dtype=float32), 'validation/wer': 0.24924504819149243, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5773453, dtype=float32), 'test/wer': 0.18154489874677554, 'test/num_examples': 2472, 'score': 7262.037584543228, 'total_duration': 8100.171071052551, 'accumulated_submission_time': 7262.037584543228, 'accumulated_eval_time': 837.6886546611786, 'accumulated_logging_time': 0.2475128173828125, 'global_step': 8678, 'preemption_count': 0}), (10360, {'train/ctc_loss': Array(0.53431463, dtype=float32), 'train/wer': 0.17031117465364282, 'validation/ctc_loss': Array(0.8380471, dtype=float32), 'validation/wer': 0.23540024505783944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5362395, dtype=float32), 'test/wer': 0.16651432981943007, 'test/num_examples': 2472, 'score': 8702.593554019928, 'total_duration': 9672.465755462646, 'accumulated_submission_time': 8702.593554019928, 'accumulated_eval_time': 969.3373708724976, 'accumulated_logging_time': 0.29662537574768066, 'global_step': 10360, 'preemption_count': 0}), (12071, {'train/ctc_loss': Array(0.48240015, dtype=float32), 'train/wer': 0.1580998872652245, 'validation/ctc_loss': Array(0.8067362, dtype=float32), 'validation/wer': 0.22743104130285868, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50352377, dtype=float32), 'test/wer': 0.15938496536875674, 'test/num_examples': 2472, 'score': 10142.895012617111, 'total_duration': 11245.345475912094, 'accumulated_submission_time': 10142.895012617111, 'accumulated_eval_time': 1101.8209691047668, 'accumulated_logging_time': 0.3508286476135254, 'global_step': 12071, 'preemption_count': 0}), (13776, {'train/ctc_loss': Array(0.4215641, dtype=float32), 'train/wer': 0.14147013702904393, 'validation/ctc_loss': Array(0.7837072, dtype=float32), 'validation/wer': 0.22020472942334224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48125437, dtype=float32), 'test/wer': 0.15156500721061078, 'test/num_examples': 2472, 'score': 11584.013818740845, 'total_duration': 12819.248528718948, 'accumulated_submission_time': 11584.013818740845, 'accumulated_eval_time': 1234.5190012454987, 'accumulated_logging_time': 0.3971695899963379, 'global_step': 13776, 'preemption_count': 0}), (15450, {'train/ctc_loss': Array(0.3998983, dtype=float32), 'train/wer': 0.1309577643122004, 'validation/ctc_loss': Array(0.7513619, dtype=float32), 'validation/wer': 0.2149562465629191, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4669773, dtype=float32), 'test/wer': 0.14476062803404222, 'test/num_examples': 2472, 'score': 13024.255194664001, 'total_duration': 14391.177129507065, 'accumulated_submission_time': 13024.255194664001, 'accumulated_eval_time': 1366.1103031635284, 'accumulated_logging_time': 0.4521174430847168, 'global_step': 15450, 'preemption_count': 0}), (17146, {'train/ctc_loss': Array(0.3869279, dtype=float32), 'train/wer': 0.1299211120466356, 'validation/ctc_loss': Array(0.7105021, dtype=float32), 'validation/wer': 0.1997703788748565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4342872, dtype=float32), 'test/wer': 0.13610789511100277, 'test/num_examples': 2472, 'score': 14464.25069475174, 'total_duration': 15962.242888212204, 'accumulated_submission_time': 14464.25069475174, 'accumulated_eval_time': 1497.079836845398, 'accumulated_logging_time': 0.513319730758667, 'global_step': 17146, 'preemption_count': 0}), (18843, {'train/ctc_loss': Array(0.37419647, dtype=float32), 'train/wer': 0.12054258441524102, 'validation/ctc_loss': Array(0.6736266, dtype=float32), 'validation/wer': 0.18975581047574025, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40205064, dtype=float32), 'test/wer': 0.12515995368959845, 'test/num_examples': 2472, 'score': 15904.856006622314, 'total_duration': 17535.671298265457, 'accumulated_submission_time': 15904.856006622314, 'accumulated_eval_time': 1629.8120965957642, 'accumulated_logging_time': 0.5649142265319824, 'global_step': 18843, 'preemption_count': 0}), (20517, {'train/ctc_loss': Array(0.36038643, dtype=float32), 'train/wer': 0.11729232067955786, 'validation/ctc_loss': Array(0.64048874, dtype=float32), 'validation/wer': 0.18223041225675116, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37996715, dtype=float32), 'test/wer': 0.12105701460402575, 'test/num_examples': 2472, 'score': 17345.481957912445, 'total_duration': 19108.183992385864, 'accumulated_submission_time': 17345.481957912445, 'accumulated_eval_time': 1761.6231853961945, 'accumulated_logging_time': 0.6045510768890381, 'global_step': 20517, 'preemption_count': 0}), (22209, {'train/ctc_loss': Array(0.32083884, dtype=float32), 'train/wer': 0.1072692663854306, 'validation/ctc_loss': Array(0.60738987, dtype=float32), 'validation/wer': 0.17400071394803615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3566334, dtype=float32), 'test/wer': 0.11398858489224707, 'test/num_examples': 2472, 'score': 18785.99226617813, 'total_duration': 20683.899030685425, 'accumulated_submission_time': 18785.99226617813, 'accumulated_eval_time': 1896.7379913330078, 'accumulated_logging_time': 0.653832197189331, 'global_step': 22209, 'preemption_count': 0}), (23882, {'train/ctc_loss': Array(0.2835242, dtype=float32), 'train/wer': 0.09528736117562603, 'validation/ctc_loss': Array(0.5716396, dtype=float32), 'validation/wer': 0.1640150893882237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33034423, dtype=float32), 'test/wer': 0.10631080779152195, 'test/num_examples': 2472, 'score': 20226.171179771423, 'total_duration': 22254.558356761932, 'accumulated_submission_time': 20226.171179771423, 'accumulated_eval_time': 2027.1297717094421, 'accumulated_logging_time': 0.7045257091522217, 'global_step': 23882, 'preemption_count': 0}), (25554, {'train/ctc_loss': Array(0.25289425, dtype=float32), 'train/wer': 0.08755575262787241, 'validation/ctc_loss': Array(0.54563844, dtype=float32), 'validation/wer': 0.15645109938349622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31139702, dtype=float32), 'test/wer': 0.09975016757053196, 'test/num_examples': 2472, 'score': 21666.783137321472, 'total_duration': 23827.672288894653, 'accumulated_submission_time': 21666.783137321472, 'accumulated_eval_time': 2159.541759490967, 'accumulated_logging_time': 0.7551836967468262, 'global_step': 25554, 'preemption_count': 0}), (27273, {'train/ctc_loss': Array(0.23077488, dtype=float32), 'train/wer': 0.07650530320851787, 'validation/ctc_loss': Array(0.5115235, dtype=float32), 'validation/wer': 0.14748815714575153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29063657, dtype=float32), 'test/wer': 0.09172709361607052, 'test/num_examples': 2472, 'score': 23107.173696279526, 'total_duration': 25401.76122689247, 'accumulated_submission_time': 23107.173696279526, 'accumulated_eval_time': 2293.147508621216, 'accumulated_logging_time': 0.8076622486114502, 'global_step': 27273, 'preemption_count': 0}), (28931, {'train/ctc_loss': Array(0.22995028, dtype=float32), 'train/wer': 0.07742893049055295, 'validation/ctc_loss': Array(0.48274842, dtype=float32), 'validation/wer': 0.13813929705062278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27128604, dtype=float32), 'test/wer': 0.08585704710255317, 'test/num_examples': 2472, 'score': 24547.12677192688, 'total_duration': 26976.383147239685, 'accumulated_submission_time': 24547.12677192688, 'accumulated_eval_time': 2427.7170763015747, 'accumulated_logging_time': 0.8673276901245117, 'global_step': 28931, 'preemption_count': 0}), (30603, {'train/ctc_loss': Array(0.20214693, dtype=float32), 'train/wer': 0.06679906022279014, 'validation/ctc_loss': Array(0.46044794, dtype=float32), 'validation/wer': 0.13249524838638096, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2545671, dtype=float32), 'test/wer': 0.08065728271687689, 'test/num_examples': 2472, 'score': 25987.186353206635, 'total_duration': 28548.389353752136, 'accumulated_submission_time': 25987.186353206635, 'accumulated_eval_time': 2559.566133737564, 'accumulated_logging_time': 0.9241898059844971, 'global_step': 30603, 'preemption_count': 0}), (32305, {'train/ctc_loss': Array(0.20133561, dtype=float32), 'train/wer': 0.06663618757994338, 'validation/ctc_loss': Array(0.43979, dtype=float32), 'validation/wer': 0.12565485436424856, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24128245, dtype=float32), 'test/wer': 0.07588406150346312, 'test/num_examples': 2472, 'score': 27427.502437353134, 'total_duration': 30122.642430067062, 'accumulated_submission_time': 27427.502437353134, 'accumulated_eval_time': 2693.4071941375732, 'accumulated_logging_time': 0.9798026084899902, 'global_step': 32305, 'preemption_count': 0}), (33950, {'train/ctc_loss': Array(0.19308913, dtype=float32), 'train/wer': 0.06138836493067767, 'validation/ctc_loss': Array(0.42690834, dtype=float32), 'validation/wer': 0.12286659784464876, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23378204, dtype=float32), 'test/wer': 0.07334511404951963, 'test/num_examples': 2472, 'score': 28867.70143842697, 'total_duration': 31697.223484039307, 'accumulated_submission_time': 28867.70143842697, 'accumulated_eval_time': 2827.6959364414215, 'accumulated_logging_time': 1.0333607196807861, 'global_step': 33950, 'preemption_count': 0}), (35627, {'train/ctc_loss': Array(0.14589979, dtype=float32), 'train/wer': 0.05061857466457571, 'validation/ctc_loss': Array(0.42508355, dtype=float32), 'validation/wer': 0.12191145114762322, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23114148, dtype=float32), 'test/wer': 0.07285763613836248, 'test/num_examples': 2472, 'score': 30308.3208425045, 'total_duration': 33273.13421392441, 'accumulated_submission_time': 30308.3208425045, 'accumulated_eval_time': 2962.8920962810516, 'accumulated_logging_time': 1.0885753631591797, 'global_step': 35627, 'preemption_count': 0}), (36000, {'train/ctc_loss': Array(0.17479801, dtype=float32), 'train/wer': 0.05793964224620388, 'validation/ctc_loss': Array(0.42536253, dtype=float32), 'validation/wer': 0.12182461962971182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23126462, dtype=float32), 'test/wer': 0.07318262141246724, 'test/num_examples': 2472, 'score': 30613.479635238647, 'total_duration': 33711.51296496391, 'accumulated_submission_time': 30613.479635238647, 'accumulated_eval_time': 3096.0438344478607, 'accumulated_logging_time': 1.141819953918457, 'global_step': 36000, 'preemption_count': 0})], 'global_step': 36000}
I1006 07:26:07.710259 139821904316224 submission_runner.py:552] Timing: 30613.479635238647
I1006 07:26:07.710331 139821904316224 submission_runner.py:554] Total number of evals: 23
I1006 07:26:07.710388 139821904316224 submission_runner.py:555] ====================
I1006 07:26:07.713623 139821904316224 submission_runner.py:625] Final librispeech_deepspeech score: 30613.479635238647
