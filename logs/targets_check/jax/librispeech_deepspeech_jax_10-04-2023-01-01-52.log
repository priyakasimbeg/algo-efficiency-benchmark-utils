python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_deepspeech/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_deepspeech_jax/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=36000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_10-04-2023-01-01-52.log
2023-10-04 01:01:57.761122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 01:02:16.318964 140602800301888 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax.
I1004 01:02:17.286046 140602800301888 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I1004 01:02:17.287368 140602800301888 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 01:02:17.287514 140602800301888 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 01:02:17.292667 140602800301888 submission_runner.py:507] Using RNG seed 1552788321
I1004 01:02:23.109714 140602800301888 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 01:02:23.109962 140602800301888 submission_runner.py:521] Creating tuning directory at /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1.
I1004 01:02:23.110158 140602800301888 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/hparams.json.
I1004 01:02:23.301713 140602800301888 submission_runner.py:191] Initializing dataset.
I1004 01:02:23.301977 140602800301888 submission_runner.py:198] Initializing model.
I1004 01:02:26.126441 140602800301888 submission_runner.py:232] Initializing optimizer.
I1004 01:02:26.823646 140602800301888 submission_runner.py:239] Initializing metrics bundle.
I1004 01:02:26.823892 140602800301888 submission_runner.py:257] Initializing checkpoint and logger.
I1004 01:02:26.824965 140602800301888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I1004 01:02:26.825115 140602800301888 submission_runner.py:277] Saving meta data to /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I1004 01:02:26.825373 140602800301888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 01:02:26.825439 140602800301888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1004 01:02:27.156894 140602800301888 logger_utils.py:220] Unable to record git information. Continuing without it.
I1004 01:02:27.467674 140602800301888 submission_runner.py:280] Saving flags to /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/flags_0.json.
I1004 01:02:27.482511 140602800301888 submission_runner.py:290] Starting training loop.
I1004 01:02:27.786294 140602800301888 input_pipeline.py:20] Loading split = train-clean-100
I1004 01:02:27.826041 140602800301888 input_pipeline.py:20] Loading split = train-clean-360
I1004 01:02:27.960329 140602800301888 input_pipeline.py:20] Loading split = train-other-500
2023-10-04 01:03:20.089636: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-04 01:03:21.866343: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1004 01:03:27.031259 140435146135296 logging_writer.py:48] [0] global_step=0, grad_norm=24.788843154907227, loss=34.20719909667969
I1004 01:03:27.063245 140602800301888 spec.py:321] Evaluating on the training split.
I1004 01:03:27.321066 140602800301888 input_pipeline.py:20] Loading split = train-clean-100
I1004 01:03:27.356885 140602800301888 input_pipeline.py:20] Loading split = train-clean-360
I1004 01:03:27.720593 140602800301888 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1004 01:05:06.241951 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 01:05:06.437574 140602800301888 input_pipeline.py:20] Loading split = dev-clean
I1004 01:05:06.443577 140602800301888 input_pipeline.py:20] Loading split = dev-other
I1004 01:06:06.122954 140602800301888 spec.py:349] Evaluating on the test split.
I1004 01:06:06.326763 140602800301888 input_pipeline.py:20] Loading split = test-clean
I1004 01:06:45.777806 140602800301888 submission_runner.py:381] Time since start: 258.29s, 	Step: 1, 	{'train/ctc_loss': Array(31.469002, dtype=float32), 'train/wer': 2.5833506861295628, 'validation/ctc_loss': Array(30.636894, dtype=float32), 'validation/wer': 2.455112929213017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.664526, dtype=float32), 'test/wer': 2.46109316921577, 'test/num_examples': 2472, 'score': 59.58067512512207, 'total_duration': 258.2927260398865, 'accumulated_submission_time': 59.58067512512207, 'accumulated_eval_time': 198.71199917793274, 'accumulated_logging_time': 0}
I1004 01:06:45.802815 140434038839040 logging_writer.py:48] [1] accumulated_eval_time=198.711999, accumulated_logging_time=0, accumulated_submission_time=59.580675, global_step=1, preemption_count=0, score=59.580675, test/ctc_loss=30.664525985717773, test/num_examples=2472, test/wer=2.461093, total_duration=258.292726, train/ctc_loss=31.46900177001953, train/wer=2.583351, validation/ctc_loss=30.63689422607422, validation/num_examples=5348, validation/wer=2.455113
I1004 01:06:54.733396 140442639447808 logging_writer.py:48] [1] global_step=1, grad_norm=24.228748321533203, loss=32.89699935913086
I1004 01:06:55.554267 140442647840512 logging_writer.py:48] [2] global_step=2, grad_norm=45.28584671020508, loss=33.246883392333984
I1004 01:06:56.373365 140442639447808 logging_writer.py:48] [3] global_step=3, grad_norm=45.76836395263672, loss=31.34255599975586
I1004 01:06:57.204456 140442647840512 logging_writer.py:48] [4] global_step=4, grad_norm=39.6843376159668, loss=30.07351303100586
I1004 01:06:58.091803 140442639447808 logging_writer.py:48] [5] global_step=5, grad_norm=35.61991500854492, loss=29.003223419189453
I1004 01:06:58.977113 140442647840512 logging_writer.py:48] [6] global_step=6, grad_norm=28.398862838745117, loss=27.225948333740234
I1004 01:06:59.854374 140442639447808 logging_writer.py:48] [7] global_step=7, grad_norm=25.70883560180664, loss=26.394357681274414
I1004 01:07:00.761611 140442647840512 logging_writer.py:48] [8] global_step=8, grad_norm=23.828763961791992, loss=25.223121643066406
I1004 01:07:01.703016 140442639447808 logging_writer.py:48] [9] global_step=9, grad_norm=23.885831832885742, loss=23.71463966369629
I1004 01:07:02.593788 140442647840512 logging_writer.py:48] [10] global_step=10, grad_norm=24.491336822509766, loss=23.19518280029297
I1004 01:07:03.473345 140442639447808 logging_writer.py:48] [11] global_step=11, grad_norm=21.878944396972656, loss=20.733783721923828
I1004 01:07:04.354751 140442647840512 logging_writer.py:48] [12] global_step=12, grad_norm=21.081825256347656, loss=19.3850040435791
I1004 01:07:05.237430 140442639447808 logging_writer.py:48] [13] global_step=13, grad_norm=19.289932250976562, loss=17.432369232177734
I1004 01:07:06.114588 140442647840512 logging_writer.py:48] [14] global_step=14, grad_norm=16.494731903076172, loss=16.072038650512695
I1004 01:07:06.982281 140442639447808 logging_writer.py:48] [15] global_step=15, grad_norm=15.019898414611816, loss=14.794849395751953
I1004 01:07:07.866245 140442647840512 logging_writer.py:48] [16] global_step=16, grad_norm=12.881935119628906, loss=13.21190071105957
I1004 01:07:08.748943 140442639447808 logging_writer.py:48] [17] global_step=17, grad_norm=10.842263221740723, loss=12.566941261291504
I1004 01:07:09.617920 140442647840512 logging_writer.py:48] [18] global_step=18, grad_norm=10.108689308166504, loss=11.455375671386719
I1004 01:07:10.484725 140442639447808 logging_writer.py:48] [19] global_step=19, grad_norm=7.447022438049316, loss=10.37877082824707
I1004 01:07:11.367158 140442647840512 logging_writer.py:48] [20] global_step=20, grad_norm=6.062536239624023, loss=9.84601879119873
I1004 01:07:12.246164 140442639447808 logging_writer.py:48] [21] global_step=21, grad_norm=5.352664470672607, loss=9.31674575805664
I1004 01:07:13.111865 140442647840512 logging_writer.py:48] [22] global_step=22, grad_norm=4.656759738922119, loss=8.929917335510254
I1004 01:07:13.975448 140442639447808 logging_writer.py:48] [23] global_step=23, grad_norm=4.254575252532959, loss=8.558816909790039
I1004 01:07:14.853777 140442647840512 logging_writer.py:48] [24] global_step=24, grad_norm=3.795959711074829, loss=8.17862319946289
I1004 01:07:15.745611 140442639447808 logging_writer.py:48] [25] global_step=25, grad_norm=2.3449814319610596, loss=7.826314926147461
I1004 01:07:16.621182 140442647840512 logging_writer.py:48] [26] global_step=26, grad_norm=2.6228151321411133, loss=7.554356098175049
I1004 01:07:17.499350 140442639447808 logging_writer.py:48] [27] global_step=27, grad_norm=2.9507243633270264, loss=7.486429214477539
I1004 01:07:18.391268 140442647840512 logging_writer.py:48] [28] global_step=28, grad_norm=6.28941011428833, loss=7.356072902679443
I1004 01:07:19.281832 140442639447808 logging_writer.py:48] [29] global_step=29, grad_norm=2.4120142459869385, loss=7.326550483703613
I1004 01:07:20.152252 140442647840512 logging_writer.py:48] [30] global_step=30, grad_norm=2.315061330795288, loss=7.17141580581665
I1004 01:07:21.019961 140442639447808 logging_writer.py:48] [31] global_step=31, grad_norm=2.872422933578491, loss=7.028939247131348
I1004 01:07:21.899894 140442647840512 logging_writer.py:48] [32] global_step=32, grad_norm=6.042193412780762, loss=7.161396026611328
I1004 01:07:22.775910 140442639447808 logging_writer.py:48] [33] global_step=33, grad_norm=4.332403182983398, loss=6.933644771575928
I1004 01:07:23.634521 140442647840512 logging_writer.py:48] [34] global_step=34, grad_norm=6.812836647033691, loss=6.7509870529174805
I1004 01:07:24.496658 140442639447808 logging_writer.py:48] [35] global_step=35, grad_norm=4.510297775268555, loss=6.772940158843994
I1004 01:07:25.376006 140442647840512 logging_writer.py:48] [36] global_step=36, grad_norm=3.0716793537139893, loss=6.645339012145996
I1004 01:07:26.258066 140442639447808 logging_writer.py:48] [37] global_step=37, grad_norm=5.680678844451904, loss=6.54766321182251
I1004 01:07:27.124921 140442647840512 logging_writer.py:48] [38] global_step=38, grad_norm=3.530298948287964, loss=6.447266578674316
I1004 01:07:27.998277 140442639447808 logging_writer.py:48] [39] global_step=39, grad_norm=1.5822187662124634, loss=6.3522844314575195
I1004 01:07:28.872956 140442647840512 logging_writer.py:48] [40] global_step=40, grad_norm=0.9229941368103027, loss=6.255838871002197
I1004 01:07:29.772570 140442639447808 logging_writer.py:48] [41] global_step=41, grad_norm=2.8054254055023193, loss=6.193353176116943
I1004 01:07:30.661536 140442647840512 logging_writer.py:48] [42] global_step=42, grad_norm=6.24998140335083, loss=6.2726240158081055
I1004 01:07:31.549976 140442639447808 logging_writer.py:48] [43] global_step=43, grad_norm=7.7051591873168945, loss=6.232021331787109
I1004 01:07:32.440386 140442647840512 logging_writer.py:48] [44] global_step=44, grad_norm=4.018538951873779, loss=6.1029438972473145
I1004 01:07:33.314779 140442639447808 logging_writer.py:48] [45] global_step=45, grad_norm=1.1267423629760742, loss=6.0176215171813965
I1004 01:07:34.198642 140442647840512 logging_writer.py:48] [46] global_step=46, grad_norm=0.6536244750022888, loss=6.000454902648926
I1004 01:07:35.079236 140442639447808 logging_writer.py:48] [47] global_step=47, grad_norm=1.2927442789077759, loss=5.98164701461792
I1004 01:07:35.966900 140442647840512 logging_writer.py:48] [48] global_step=48, grad_norm=2.351170778274536, loss=5.95404577255249
I1004 01:07:36.844551 140442639447808 logging_writer.py:48] [49] global_step=49, grad_norm=6.2218546867370605, loss=6.058041095733643
I1004 01:07:37.719188 140442647840512 logging_writer.py:48] [50] global_step=50, grad_norm=6.630887985229492, loss=6.081254959106445
I1004 01:07:38.607415 140442639447808 logging_writer.py:48] [51] global_step=51, grad_norm=1.6005133390426636, loss=5.94973087310791
I1004 01:07:39.500209 140442647840512 logging_writer.py:48] [52] global_step=52, grad_norm=0.7047032713890076, loss=5.923220634460449
I1004 01:07:40.380140 140442639447808 logging_writer.py:48] [53] global_step=53, grad_norm=1.2246577739715576, loss=5.920858860015869
I1004 01:07:41.268440 140442647840512 logging_writer.py:48] [54] global_step=54, grad_norm=1.4131603240966797, loss=5.895440101623535
I1004 01:07:42.158821 140442639447808 logging_writer.py:48] [55] global_step=55, grad_norm=3.228301525115967, loss=5.941878795623779
I1004 01:07:43.053666 140442647840512 logging_writer.py:48] [56] global_step=56, grad_norm=5.750984191894531, loss=5.968204021453857
I1004 01:07:43.945896 140442639447808 logging_writer.py:48] [57] global_step=57, grad_norm=4.119165420532227, loss=5.937689304351807
I1004 01:07:44.850107 140442647840512 logging_writer.py:48] [58] global_step=58, grad_norm=1.9764384031295776, loss=5.886711597442627
I1004 01:07:45.747513 140442639447808 logging_writer.py:48] [59] global_step=59, grad_norm=1.8946590423583984, loss=5.899221897125244
I1004 01:07:46.650951 140442647840512 logging_writer.py:48] [60] global_step=60, grad_norm=2.5557210445404053, loss=5.905740737915039
I1004 01:07:47.545358 140442639447808 logging_writer.py:48] [61] global_step=61, grad_norm=4.33197546005249, loss=5.9358673095703125
I1004 01:07:48.443073 140442647840512 logging_writer.py:48] [62] global_step=62, grad_norm=5.123772144317627, loss=5.942076206207275
I1004 01:07:49.339550 140442639447808 logging_writer.py:48] [63] global_step=63, grad_norm=2.7974746227264404, loss=5.877671241760254
I1004 01:07:50.234045 140442647840512 logging_writer.py:48] [64] global_step=64, grad_norm=1.3682228326797485, loss=5.8619585037231445
I1004 01:07:51.115477 140442639447808 logging_writer.py:48] [65] global_step=65, grad_norm=2.1242737770080566, loss=5.878804683685303
I1004 01:07:52.023187 140442647840512 logging_writer.py:48] [66] global_step=66, grad_norm=3.075615406036377, loss=5.884340286254883
I1004 01:07:52.912930 140442639447808 logging_writer.py:48] [67] global_step=67, grad_norm=3.8762998580932617, loss=5.881112575531006
I1004 01:07:53.821448 140442647840512 logging_writer.py:48] [68] global_step=68, grad_norm=4.430474758148193, loss=5.885952472686768
I1004 01:07:54.723489 140442639447808 logging_writer.py:48] [69] global_step=69, grad_norm=3.8304443359375, loss=5.878085136413574
I1004 01:07:55.622656 140442647840512 logging_writer.py:48] [70] global_step=70, grad_norm=3.200404405593872, loss=5.858389377593994
I1004 01:07:56.524416 140442639447808 logging_writer.py:48] [71] global_step=71, grad_norm=2.176837682723999, loss=5.833806037902832
I1004 01:07:57.418854 140442647840512 logging_writer.py:48] [72] global_step=72, grad_norm=1.8606622219085693, loss=5.848317623138428
I1004 01:07:58.317669 140442639447808 logging_writer.py:48] [73] global_step=73, grad_norm=2.576601982116699, loss=5.840281009674072
I1004 01:07:59.220183 140442647840512 logging_writer.py:48] [74] global_step=74, grad_norm=3.719088077545166, loss=5.838663578033447
I1004 01:08:00.122842 140442639447808 logging_writer.py:48] [75] global_step=75, grad_norm=5.490014553070068, loss=5.8965301513671875
I1004 01:08:01.028865 140442647840512 logging_writer.py:48] [76] global_step=76, grad_norm=4.0165791511535645, loss=5.870501518249512
I1004 01:08:01.919977 140442639447808 logging_writer.py:48] [77] global_step=77, grad_norm=3.263704299926758, loss=5.8654375076293945
I1004 01:08:02.824869 140442647840512 logging_writer.py:48] [78] global_step=78, grad_norm=4.2238545417785645, loss=5.867334842681885
I1004 01:08:03.728614 140442639447808 logging_writer.py:48] [79] global_step=79, grad_norm=2.382103443145752, loss=5.8157196044921875
I1004 01:08:04.631330 140442647840512 logging_writer.py:48] [80] global_step=80, grad_norm=0.9623355865478516, loss=5.810044765472412
I1004 01:08:05.535531 140442639447808 logging_writer.py:48] [81] global_step=81, grad_norm=1.0040615797042847, loss=5.802674293518066
I1004 01:08:06.443507 140442647840512 logging_writer.py:48] [82] global_step=82, grad_norm=1.4407145977020264, loss=5.819000244140625
I1004 01:08:07.337989 140442639447808 logging_writer.py:48] [83] global_step=83, grad_norm=4.12071418762207, loss=5.831524848937988
I1004 01:08:08.235153 140442647840512 logging_writer.py:48] [84] global_step=84, grad_norm=6.597076892852783, loss=5.928623676300049
I1004 01:08:09.122334 140442639447808 logging_writer.py:48] [85] global_step=85, grad_norm=2.5986368656158447, loss=5.833605766296387
I1004 01:08:10.012113 140442647840512 logging_writer.py:48] [86] global_step=86, grad_norm=0.28144732117652893, loss=5.802667617797852
I1004 01:08:10.905074 140442639447808 logging_writer.py:48] [87] global_step=87, grad_norm=0.31361526250839233, loss=5.772883892059326
I1004 01:08:11.794907 140442647840512 logging_writer.py:48] [88] global_step=88, grad_norm=0.7668628096580505, loss=5.778266906738281
I1004 01:08:12.686618 140442639447808 logging_writer.py:48] [89] global_step=89, grad_norm=1.392773985862732, loss=5.776703834533691
I1004 01:08:13.581763 140442647840512 logging_writer.py:48] [90] global_step=90, grad_norm=4.031787395477295, loss=5.801570892333984
I1004 01:08:14.483457 140442639447808 logging_writer.py:48] [91] global_step=91, grad_norm=6.6071014404296875, loss=5.920907020568848
I1004 01:08:15.373250 140442647840512 logging_writer.py:48] [92] global_step=92, grad_norm=2.6284732818603516, loss=5.806329727172852
I1004 01:08:16.264023 140442639447808 logging_writer.py:48] [93] global_step=93, grad_norm=0.38290661573410034, loss=5.793968677520752
I1004 01:08:17.160205 140442647840512 logging_writer.py:48] [94] global_step=94, grad_norm=0.41680341958999634, loss=5.772828578948975
I1004 01:08:18.049926 140442639447808 logging_writer.py:48] [95] global_step=95, grad_norm=0.9181826114654541, loss=5.765498638153076
I1004 01:08:18.931122 140442647840512 logging_writer.py:48] [96] global_step=96, grad_norm=6.049344062805176, loss=5.84092903137207
I1004 01:08:19.812892 140442639447808 logging_writer.py:48] [97] global_step=97, grad_norm=6.545975208282471, loss=5.897363662719727
I1004 01:08:20.710923 140442647840512 logging_writer.py:48] [98] global_step=98, grad_norm=1.3409656286239624, loss=5.798529624938965
I1004 01:08:21.590752 140442639447808 logging_writer.py:48] [99] global_step=99, grad_norm=1.2248884439468384, loss=5.775246620178223
I1004 01:08:22.477178 140442647840512 logging_writer.py:48] [100] global_step=100, grad_norm=0.9672405123710632, loss=5.766010761260986
I1004 01:13:24.830609 140442639447808 logging_writer.py:48] [500] global_step=500, grad_norm=2.4989585876464844, loss=2.922973871231079
I1004 01:20:27.431347 140442647840512 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.411801815032959, loss=2.459284782409668
I1004 01:26:46.866688 140443336771328 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.8726043701171875, loss=2.3518121242523193
I1004 01:30:46.367029 140602800301888 spec.py:321] Evaluating on the training split.
I1004 01:31:40.134738 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 01:32:28.424456 140602800301888 spec.py:349] Evaluating on the test split.
I1004 01:32:52.664136 140602800301888 submission_runner.py:381] Time since start: 1825.18s, 	Step: 1784, 	{'train/ctc_loss': Array(0.8318721, dtype=float32), 'train/wer': 0.25860950340878663, 'validation/ctc_loss': Array(1.2773172, dtype=float32), 'validation/wer': 0.34347654101824426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8950843, dtype=float32), 'test/wer': 0.27445006398147587, 'test/num_examples': 2472, 'score': 1500.1013827323914, 'total_duration': 1825.1752665042877, 'accumulated_submission_time': 1500.1013827323914, 'accumulated_eval_time': 325.0028088092804, 'accumulated_logging_time': 0.038732290267944336}
I1004 01:32:52.700408 140443562051328 logging_writer.py:48] [1784] accumulated_eval_time=325.002809, accumulated_logging_time=0.038732, accumulated_submission_time=1500.101383, global_step=1784, preemption_count=0, score=1500.101383, test/ctc_loss=0.8950843214988708, test/num_examples=2472, test/wer=0.274450, total_duration=1825.175267, train/ctc_loss=0.8318721055984497, train/wer=0.258610, validation/ctc_loss=1.2773171663284302, validation/num_examples=5348, validation/wer=0.343477
I1004 01:35:34.803033 140443553658624 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.8924163579940796, loss=2.2199974060058594
I1004 01:41:53.519427 140444647491328 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.0100245475769043, loss=2.136256694793701
I1004 01:48:59.244252 140444639098624 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.8794281482696533, loss=2.043961763381958
I1004 01:55:31.497767 140444647491328 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.3762574195861816, loss=2.0352938175201416
I1004 01:56:52.929929 140602800301888 spec.py:321] Evaluating on the training split.
I1004 01:57:45.161691 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 01:58:35.303487 140602800301888 spec.py:349] Evaluating on the test split.
I1004 01:59:00.344499 140602800301888 submission_runner.py:381] Time since start: 3392.86s, 	Step: 3600, 	{'train/ctc_loss': Array(0.64848626, dtype=float32), 'train/wer': 0.20901366433156646, 'validation/ctc_loss': Array(1.0321589, dtype=float32), 'validation/wer': 0.28597478026802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6792263, dtype=float32), 'test/wer': 0.20878272703268133, 'test/num_examples': 2472, 'score': 2940.28547000885, 'total_duration': 3392.8565583229065, 'accumulated_submission_time': 2940.28547000885, 'accumulated_eval_time': 452.4120047092438, 'accumulated_logging_time': 0.08900928497314453}
I1004 01:59:00.378016 140444647491328 logging_writer.py:48] [3600] accumulated_eval_time=452.412005, accumulated_logging_time=0.089009, accumulated_submission_time=2940.285470, global_step=3600, preemption_count=0, score=2940.285470, test/ctc_loss=0.679226279258728, test/num_examples=2472, test/wer=0.208783, total_duration=3392.856558, train/ctc_loss=0.6484862565994263, train/wer=0.209014, validation/ctc_loss=1.0321588516235352, validation/num_examples=5348, validation/wer=0.285975
I1004 02:04:19.397760 140444639098624 logging_writer.py:48] [4000] global_step=4000, grad_norm=6.06894063949585, loss=2.0379741191864014
I1004 02:10:48.884085 140444647491328 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.4214718341827393, loss=1.9448151588439941
I1004 02:17:59.486240 140444639098624 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.134921073913574, loss=1.9186463356018066
I1004 02:23:00.377931 140602800301888 spec.py:321] Evaluating on the training split.
I1004 02:23:54.022998 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 02:24:43.157136 140602800301888 spec.py:349] Evaluating on the test split.
I1004 02:25:07.847961 140602800301888 submission_runner.py:381] Time since start: 4960.36s, 	Step: 5377, 	{'train/ctc_loss': Array(0.74621236, dtype=float32), 'train/wer': 0.2367337687324404, 'validation/ctc_loss': Array(1.3818073, dtype=float32), 'validation/wer': 0.3547356945074241, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8095734, dtype=float32), 'test/wer': 0.24369833241931224, 'test/num_examples': 2472, 'score': 4380.235234498978, 'total_duration': 4960.359409809113, 'accumulated_submission_time': 4380.235234498978, 'accumulated_eval_time': 579.8762528896332, 'accumulated_logging_time': 0.14043927192687988}
I1004 02:25:07.878737 140444647491328 logging_writer.py:48] [5377] accumulated_eval_time=579.876253, accumulated_logging_time=0.140439, accumulated_submission_time=4380.235234, global_step=5377, preemption_count=0, score=4380.235234, test/ctc_loss=0.8095734119415283, test/num_examples=2472, test/wer=0.243698, total_duration=4960.359410, train/ctc_loss=0.746212363243103, train/wer=0.236734, validation/ctc_loss=1.3818073272705078, validation/num_examples=5348, validation/wer=0.354736
I1004 02:26:40.880112 140444639098624 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.1520955562591553, loss=2.014925479888916
I1004 02:33:32.724112 140444647491328 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.317819356918335, loss=1.914149284362793
I1004 02:40:09.008276 140444647491328 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.462688446044922, loss=1.901093602180481
I1004 02:47:13.523447 140444639098624 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.143394708633423, loss=1.9283561706542969
I1004 02:49:08.546992 140602800301888 spec.py:321] Evaluating on the training split.
I1004 02:50:02.838377 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 02:50:52.641767 140602800301888 spec.py:349] Evaluating on the test split.
I1004 02:51:17.798947 140602800301888 submission_runner.py:381] Time since start: 6530.31s, 	Step: 7134, 	{'train/ctc_loss': Array(0.5563652, dtype=float32), 'train/wer': 0.1797798699326624, 'validation/ctc_loss': Array(0.922901, dtype=float32), 'validation/wer': 0.2565871354282241, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58950496, dtype=float32), 'test/wer': 0.18727276420287206, 'test/num_examples': 2472, 'score': 5820.85857462883, 'total_duration': 6530.310212612152, 'accumulated_submission_time': 5820.85857462883, 'accumulated_eval_time': 709.1220633983612, 'accumulated_logging_time': 0.18543100357055664}
I1004 02:51:17.835188 140444647491328 logging_writer.py:48] [7134] accumulated_eval_time=709.122063, accumulated_logging_time=0.185431, accumulated_submission_time=5820.858575, global_step=7134, preemption_count=0, score=5820.858575, test/ctc_loss=0.5895049571990967, test/num_examples=2472, test/wer=0.187273, total_duration=6530.310213, train/ctc_loss=0.5563651919364929, train/wer=0.179780, validation/ctc_loss=0.9229009747505188, validation/num_examples=5348, validation/wer=0.256587
I1004 02:55:57.605903 140444647491328 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.9901509284973145, loss=1.9111372232437134
I1004 03:02:57.785575 140444639098624 logging_writer.py:48] [8000] global_step=8000, grad_norm=7.325298309326172, loss=1.95375394821167
I1004 03:09:45.357397 140444647491328 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.9520604610443115, loss=1.8520551919937134
I1004 03:15:18.633872 140602800301888 spec.py:321] Evaluating on the training split.
I1004 03:16:11.859432 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 03:17:01.050085 140602800301888 spec.py:349] Evaluating on the test split.
I1004 03:17:26.422111 140602800301888 submission_runner.py:381] Time since start: 8098.93s, 	Step: 8901, 	{'train/ctc_loss': Array(0.49882352, dtype=float32), 'train/wer': 0.16288134119028463, 'validation/ctc_loss': Array(0.8343963, dtype=float32), 'validation/wer': 0.2350432710397592, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5336107, dtype=float32), 'test/wer': 0.16568155505453658, 'test/num_examples': 2472, 'score': 7261.61186003685, 'total_duration': 8098.933697223663, 'accumulated_submission_time': 7261.61186003685, 'accumulated_eval_time': 836.9044826030731, 'accumulated_logging_time': 0.23556733131408691}
I1004 03:17:26.457846 140444647491328 logging_writer.py:48] [8901] accumulated_eval_time=836.904483, accumulated_logging_time=0.235567, accumulated_submission_time=7261.611860, global_step=8901, preemption_count=0, score=7261.611860, test/ctc_loss=0.5336107015609741, test/num_examples=2472, test/wer=0.165682, total_duration=8098.933697, train/ctc_loss=0.49882352352142334, train/wer=0.162881, validation/ctc_loss=0.8343963027000427, validation/num_examples=5348, validation/wer=0.235043
I1004 03:18:41.626712 140444639098624 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.1971240043640137, loss=1.824792742729187
I1004 03:25:13.427623 140444647491328 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.048365592956543, loss=1.7797728776931763
I1004 03:32:09.464728 140444639098624 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.4648303985595703, loss=1.8007776737213135
I1004 03:39:08.476047 140444647491328 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.5082879066467285, loss=1.786136507987976
I1004 03:41:26.882748 140602800301888 spec.py:321] Evaluating on the training split.
I1004 03:42:21.404221 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 03:43:11.854480 140602800301888 spec.py:349] Evaluating on the test split.
I1004 03:43:37.660799 140602800301888 submission_runner.py:381] Time since start: 9670.17s, 	Step: 10680, 	{'train/ctc_loss': Array(0.5041183, dtype=float32), 'train/wer': 0.16250113362958854, 'validation/ctc_loss': Array(0.8131147, dtype=float32), 'validation/wer': 0.2282993564819728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5144942, dtype=float32), 'test/wer': 0.1636503970913818, 'test/num_examples': 2472, 'score': 8701.990688085556, 'total_duration': 9670.17188167572, 'accumulated_submission_time': 8701.990688085556, 'accumulated_eval_time': 967.6762025356293, 'accumulated_logging_time': 0.28562211990356445}
I1004 03:43:37.697480 140444063811328 logging_writer.py:48] [10680] accumulated_eval_time=967.676203, accumulated_logging_time=0.285622, accumulated_submission_time=8701.990688, global_step=10680, preemption_count=0, score=8701.990688, test/ctc_loss=0.5144941806793213, test/num_examples=2472, test/wer=0.163650, total_duration=9670.171882, train/ctc_loss=0.5041183233261108, train/wer=0.162501, validation/ctc_loss=0.8131147027015686, validation/num_examples=5348, validation/wer=0.228299
I1004 03:47:49.269250 140444055418624 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.998135805130005, loss=1.8049718141555786
I1004 03:54:59.777902 140444063811328 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.8420301675796509, loss=1.8123468160629272
I1004 04:02:01.329643 140444055418624 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.614957571029663, loss=1.7380880117416382
I1004 04:07:37.821970 140602800301888 spec.py:321] Evaluating on the training split.
I1004 04:08:33.045694 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 04:09:22.218707 140602800301888 spec.py:349] Evaluating on the test split.
I1004 04:09:47.669186 140602800301888 submission_runner.py:381] Time since start: 11240.18s, 	Step: 12382, 	{'train/ctc_loss': Array(0.46414116, dtype=float32), 'train/wer': 0.15183034479824728, 'validation/ctc_loss': Array(0.79548526, dtype=float32), 'validation/wer': 0.22663026174878678, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4964626, dtype=float32), 'test/wer': 0.15737411898523349, 'test/num_examples': 2472, 'score': 10142.067734956741, 'total_duration': 11240.179876327515, 'accumulated_submission_time': 10142.067734956741, 'accumulated_eval_time': 1097.5169973373413, 'accumulated_logging_time': 0.33687829971313477}
I1004 04:09:47.711318 140444217411328 logging_writer.py:48] [12382] accumulated_eval_time=1097.516997, accumulated_logging_time=0.336878, accumulated_submission_time=10142.067735, global_step=12382, preemption_count=0, score=10142.067735, test/ctc_loss=0.4964626133441925, test/num_examples=2472, test/wer=0.157374, total_duration=11240.179876, train/ctc_loss=0.4641411602497101, train/wer=0.151830, validation/ctc_loss=0.795485258102417, validation/num_examples=5348, validation/wer=0.226630
I1004 04:11:17.117536 140444209018624 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.2852752208709717, loss=1.7896095514297485
I1004 04:18:02.608026 140444217411328 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.9901710748672485, loss=1.695173740386963
I1004 04:25:22.660237 140444217411328 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.168320417404175, loss=1.8051197528839111
I1004 04:32:09.802961 140444209018624 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.805194616317749, loss=1.7355437278747559
I1004 04:33:48.714062 140602800301888 spec.py:321] Evaluating on the training split.
I1004 04:34:43.466108 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 04:35:34.506260 140602800301888 spec.py:349] Evaluating on the test split.
I1004 04:35:59.465338 140602800301888 submission_runner.py:381] Time since start: 12811.98s, 	Step: 14113, 	{'train/ctc_loss': Array(0.39048758, dtype=float32), 'train/wer': 0.1337996307756312, 'validation/ctc_loss': Array(0.7435745, dtype=float32), 'validation/wer': 0.21127073102490135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45246524, dtype=float32), 'test/wer': 0.14640586598419758, 'test/num_examples': 2472, 'score': 11583.02232336998, 'total_duration': 12811.975700616837, 'accumulated_submission_time': 11583.02232336998, 'accumulated_eval_time': 1228.2612142562866, 'accumulated_logging_time': 0.3952305316925049}
I1004 04:35:59.505835 140444647491328 logging_writer.py:48] [14113] accumulated_eval_time=1228.261214, accumulated_logging_time=0.395231, accumulated_submission_time=11583.022323, global_step=14113, preemption_count=0, score=11583.022323, test/ctc_loss=0.4524652361869812, test/num_examples=2472, test/wer=0.146406, total_duration=12811.975701, train/ctc_loss=0.39048758149147034, train/wer=0.133800, validation/ctc_loss=0.7435745000839233, validation/num_examples=5348, validation/wer=0.211271
I1004 04:41:04.212925 140444647491328 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.9689667224884033, loss=1.7052847146987915
I1004 04:47:50.932879 140444639098624 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.2984535694122314, loss=1.7280094623565674
I1004 04:55:13.940116 140443889731328 logging_writer.py:48] [15500] global_step=15500, grad_norm=6.556365489959717, loss=1.780971884727478
I1004 04:59:59.578520 140602800301888 spec.py:321] Evaluating on the training split.
I1004 05:00:53.165532 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 05:01:42.619240 140602800301888 spec.py:349] Evaluating on the test split.
I1004 05:02:08.389481 140602800301888 submission_runner.py:381] Time since start: 14380.90s, 	Step: 15871, 	{'train/ctc_loss': Array(0.3636681, dtype=float32), 'train/wer': 0.1209348349993882, 'validation/ctc_loss': Array(0.71133167, dtype=float32), 'validation/wer': 0.20246215593011027, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43275332, dtype=float32), 'test/wer': 0.138524973087157, 'test/num_examples': 2472, 'score': 13023.047841787338, 'total_duration': 14380.900707006454, 'accumulated_submission_time': 13023.047841787338, 'accumulated_eval_time': 1357.0659942626953, 'accumulated_logging_time': 0.4498157501220703}
I1004 05:02:08.427043 140444647491328 logging_writer.py:48] [15871] accumulated_eval_time=1357.065994, accumulated_logging_time=0.449816, accumulated_submission_time=13023.047842, global_step=15871, preemption_count=0, score=13023.047842, test/ctc_loss=0.432753324508667, test/num_examples=2472, test/wer=0.138525, total_duration=14380.900707, train/ctc_loss=0.3636681139469147, train/wer=0.120935, validation/ctc_loss=0.7113316655158997, validation/num_examples=5348, validation/wer=0.202462
I1004 05:03:45.656452 140444639098624 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.043670654296875, loss=1.586633324623108
I1004 05:10:56.598439 140443777091328 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.684997320175171, loss=1.6528607606887817
I1004 05:17:34.676600 140443768698624 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.8690245151519775, loss=1.6218124628067017
I1004 05:24:58.898707 140443777091328 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.7319453954696655, loss=1.61171555519104
I1004 05:26:08.545232 140602800301888 spec.py:321] Evaluating on the training split.
I1004 05:27:02.792175 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 05:27:52.926624 140602800301888 spec.py:349] Evaluating on the test split.
I1004 05:28:18.510622 140602800301888 submission_runner.py:381] Time since start: 15951.02s, 	Step: 17588, 	{'train/ctc_loss': Array(0.36850655, dtype=float32), 'train/wer': 0.12704124590410099, 'validation/ctc_loss': Array(0.69042826, dtype=float32), 'validation/wer': 0.19756099914133277, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41838896, dtype=float32), 'test/wer': 0.13371112871448013, 'test/num_examples': 2472, 'score': 14463.117428064346, 'total_duration': 15951.021684408188, 'accumulated_submission_time': 14463.117428064346, 'accumulated_eval_time': 1487.0252075195312, 'accumulated_logging_time': 0.5037736892700195}
I1004 05:28:18.546307 140443562051328 logging_writer.py:48] [17588] accumulated_eval_time=1487.025208, accumulated_logging_time=0.503774, accumulated_submission_time=14463.117428, global_step=17588, preemption_count=0, score=14463.117428, test/ctc_loss=0.4183889627456665, test/num_examples=2472, test/wer=0.133711, total_duration=15951.021684, train/ctc_loss=0.3685065507888794, train/wer=0.127041, validation/ctc_loss=0.6904282569885254, validation/num_examples=5348, validation/wer=0.197561
I1004 05:33:35.020525 140443553658624 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.439479351043701, loss=1.5677242279052734
I1004 05:41:02.426445 140443562051328 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.5976102352142334, loss=1.6683080196380615
I1004 05:47:41.293092 140443562051328 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.5725574493408203, loss=1.6321865320205688
I1004 05:52:19.884734 140602800301888 spec.py:321] Evaluating on the training split.
I1004 05:53:14.734967 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 05:54:05.167839 140602800301888 spec.py:349] Evaluating on the test split.
I1004 05:54:30.851001 140602800301888 submission_runner.py:381] Time since start: 17523.36s, 	Step: 19307, 	{'train/ctc_loss': Array(0.34868133, dtype=float32), 'train/wer': 0.11443983183056136, 'validation/ctc_loss': Array(0.6443241, dtype=float32), 'validation/wer': 0.1846616947582707, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3873429, dtype=float32), 'test/wer': 0.12253975991712876, 'test/num_examples': 2472, 'score': 15904.408606290817, 'total_duration': 17523.362096071243, 'accumulated_submission_time': 15904.408606290817, 'accumulated_eval_time': 1617.9851703643799, 'accumulated_logging_time': 0.5547370910644531}
I1004 05:54:30.886344 140443562051328 logging_writer.py:48] [19307] accumulated_eval_time=1617.985170, accumulated_logging_time=0.554737, accumulated_submission_time=15904.408606, global_step=19307, preemption_count=0, score=15904.408606, test/ctc_loss=0.3873429000377655, test/num_examples=2472, test/wer=0.122540, total_duration=17523.362096, train/ctc_loss=0.34868133068084717, train/wer=0.114440, validation/ctc_loss=0.6443241238594055, validation/num_examples=5348, validation/wer=0.184662
I1004 05:56:56.979117 140443553658624 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.9627068042755127, loss=1.6296424865722656
I1004 06:03:35.071266 140443234371328 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.2428460121154785, loss=1.516136646270752
I1004 06:11:04.456001 140443225978624 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.900398254394531, loss=1.6225954294204712
I1004 06:17:51.779631 140443234371328 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.290604829788208, loss=1.5611464977264404
I1004 06:18:31.502441 140602800301888 spec.py:321] Evaluating on the training split.
I1004 06:19:25.715666 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 06:20:17.092067 140602800301888 spec.py:349] Evaluating on the test split.
I1004 06:20:42.405555 140602800301888 submission_runner.py:381] Time since start: 19094.92s, 	Step: 21046, 	{'train/ctc_loss': Array(0.3413416, dtype=float32), 'train/wer': 0.11239612736433904, 'validation/ctc_loss': Array(0.6217223, dtype=float32), 'validation/wer': 0.17811073912917635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3672197, dtype=float32), 'test/wer': 0.1186602482075031, 'test/num_examples': 2472, 'score': 17344.97687458992, 'total_duration': 19094.91708922386, 'accumulated_submission_time': 17344.97687458992, 'accumulated_eval_time': 1748.8823819160461, 'accumulated_logging_time': 0.6053192615509033}
I1004 06:20:42.442874 140442978371328 logging_writer.py:48] [21046] accumulated_eval_time=1748.882382, accumulated_logging_time=0.605319, accumulated_submission_time=17344.976875, global_step=21046, preemption_count=0, score=17344.976875, test/ctc_loss=0.3672196865081787, test/num_examples=2472, test/wer=0.118660, total_duration=19094.917089, train/ctc_loss=0.34134161472320557, train/wer=0.112396, validation/ctc_loss=0.6217222809791565, validation/num_examples=5348, validation/wer=0.178111
I1004 06:26:56.962733 140442969978624 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.9486720561981201, loss=1.4975190162658691
I1004 06:33:45.284306 140442978371328 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.3912205696105957, loss=1.528025507926941
I1004 06:41:10.829460 140442969978624 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.0159754753112793, loss=1.4816739559173584
I1004 06:44:43.046414 140602800301888 spec.py:321] Evaluating on the training split.
I1004 06:45:38.437914 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 06:46:28.768052 140602800301888 spec.py:349] Evaluating on the test split.
I1004 06:46:54.507873 140602800301888 submission_runner.py:381] Time since start: 20667.02s, 	Step: 22751, 	{'train/ctc_loss': Array(0.30649957, dtype=float32), 'train/wer': 0.10354974791645231, 'validation/ctc_loss': Array(0.59275293, dtype=float32), 'validation/wer': 0.1707397080531409, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34654742, dtype=float32), 'test/wer': 0.11041374687709463, 'test/num_examples': 2472, 'score': 18785.53312087059, 'total_duration': 20667.019449949265, 'accumulated_submission_time': 18785.53312087059, 'accumulated_eval_time': 1880.3381979465485, 'accumulated_logging_time': 0.6574959754943848}
I1004 06:46:54.544363 140444217411328 logging_writer.py:48] [22751] accumulated_eval_time=1880.338198, accumulated_logging_time=0.657496, accumulated_submission_time=18785.533121, global_step=22751, preemption_count=0, score=18785.533121, test/ctc_loss=0.3465474247932434, test/num_examples=2472, test/wer=0.110414, total_duration=20667.019450, train/ctc_loss=0.30649957060813904, train/wer=0.103550, validation/ctc_loss=0.5927529335021973, validation/num_examples=5348, validation/wer=0.170740
I1004 06:50:01.268311 140444209018624 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.54426646232605, loss=1.476867914199829
I1004 06:57:11.662637 140444217411328 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.8747482299804688, loss=1.491759181022644
I1004 07:04:01.098831 140444217411328 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.4911739826202393, loss=1.4333525896072388
I1004 07:10:54.588743 140602800301888 spec.py:321] Evaluating on the training split.
I1004 07:11:49.446361 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 07:12:40.344911 140602800301888 spec.py:349] Evaluating on the test split.
I1004 07:13:06.172569 140602800301888 submission_runner.py:381] Time since start: 22238.68s, 	Step: 24475, 	{'train/ctc_loss': Array(0.26605812, dtype=float32), 'train/wer': 0.08990329800261813, 'validation/ctc_loss': Array(0.5582985, dtype=float32), 'validation/wer': 0.16077337938619765, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32141584, dtype=float32), 'test/wer': 0.10241098450226474, 'test/num_examples': 2472, 'score': 20225.5305454731, 'total_duration': 22238.683361291885, 'accumulated_submission_time': 20225.5305454731, 'accumulated_eval_time': 2011.915414571762, 'accumulated_logging_time': 0.7086536884307861}
I1004 07:13:06.210125 140444647491328 logging_writer.py:48] [24475] accumulated_eval_time=2011.915415, accumulated_logging_time=0.708654, accumulated_submission_time=20225.530545, global_step=24475, preemption_count=0, score=20225.530545, test/ctc_loss=0.32141584157943726, test/num_examples=2472, test/wer=0.102411, total_duration=22238.683361, train/ctc_loss=0.2660581171512604, train/wer=0.089903, validation/ctc_loss=0.5582985281944275, validation/num_examples=5348, validation/wer=0.160773
I1004 07:13:25.971755 140444639098624 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.930261492729187, loss=1.4250593185424805
I1004 07:19:50.094424 140444319811328 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.0617051124572754, loss=1.431380033493042
I1004 07:27:04.686303 140444311418624 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.2632014751434326, loss=1.4153823852539062
I1004 07:34:02.127010 140444647491328 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.819216728210449, loss=1.3833383321762085
I1004 07:37:06.239921 140602800301888 spec.py:321] Evaluating on the training split.
I1004 07:38:00.946208 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 07:38:51.877063 140602800301888 spec.py:349] Evaluating on the test split.
I1004 07:39:16.926327 140602800301888 submission_runner.py:381] Time since start: 23809.44s, 	Step: 26220, 	{'train/ctc_loss': Array(0.23424065, dtype=float32), 'train/wer': 0.08034638357807976, 'validation/ctc_loss': Array(0.5223064, dtype=float32), 'validation/wer': 0.150720219201343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29964456, dtype=float32), 'test/wer': 0.09448946844596104, 'test/num_examples': 2472, 'score': 21665.507152318954, 'total_duration': 23809.43762254715, 'accumulated_submission_time': 21665.507152318954, 'accumulated_eval_time': 2142.5957176685333, 'accumulated_logging_time': 0.7658460140228271}
I1004 07:39:16.966897 140444647491328 logging_writer.py:48] [26220] accumulated_eval_time=2142.595718, accumulated_logging_time=0.765846, accumulated_submission_time=21665.507152, global_step=26220, preemption_count=0, score=21665.507152, test/ctc_loss=0.2996445596218109, test/num_examples=2472, test/wer=0.094489, total_duration=23809.437623, train/ctc_loss=0.23424065113067627, train/wer=0.080346, validation/ctc_loss=0.5223063826560974, validation/num_examples=5348, validation/wer=0.150720
I1004 07:42:55.075748 140444639098624 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.5161802768707275, loss=1.3802978992462158
I1004 07:49:51.962599 140444319811328 logging_writer.py:48] [27000] global_step=27000, grad_norm=4.358771800994873, loss=1.3622530698776245
I1004 07:56:58.123553 140444311418624 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.6044211387634277, loss=1.3122669458389282
I1004 08:03:17.284180 140602800301888 spec.py:321] Evaluating on the training split.
I1004 08:04:13.731891 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 08:05:05.257014 140602800301888 spec.py:349] Evaluating on the test split.
I1004 08:05:31.048932 140602800301888 submission_runner.py:381] Time since start: 25383.56s, 	Step: 27941, 	{'train/ctc_loss': Array(0.21349773, dtype=float32), 'train/wer': 0.07131460248949074, 'validation/ctc_loss': Array(0.49260858, dtype=float32), 'validation/wer': 0.14226861812463218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2820458, dtype=float32), 'test/wer': 0.0905693335770723, 'test/num_examples': 2472, 'score': 23105.775389432907, 'total_duration': 25383.56040763855, 'accumulated_submission_time': 23105.775389432907, 'accumulated_eval_time': 2276.354704141617, 'accumulated_logging_time': 0.8213834762573242}
I1004 08:05:31.087460 140444647491328 logging_writer.py:48] [27941] accumulated_eval_time=2276.354704, accumulated_logging_time=0.821383, accumulated_submission_time=23105.775389, global_step=27941, preemption_count=0, score=23105.775389, test/ctc_loss=0.28204581141471863, test/num_examples=2472, test/wer=0.090569, total_duration=25383.560408, train/ctc_loss=0.21349772810935974, train/wer=0.071315, validation/ctc_loss=0.49260857701301575, validation/num_examples=5348, validation/wer=0.142269
I1004 08:06:16.644314 140444639098624 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.426725387573242, loss=1.2911995649337769
I1004 08:13:06.263828 140444647491328 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.268832206726074, loss=1.2844247817993164
I1004 08:20:15.128284 140444647491328 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.4106156826019287, loss=1.3525381088256836
I1004 08:27:17.350460 140444639098624 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.7813327312469482, loss=1.2523988485336304
I1004 08:29:31.331002 140602800301888 spec.py:321] Evaluating on the training split.
I1004 08:30:25.452413 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 08:31:16.040322 140602800301888 spec.py:349] Evaluating on the test split.
I1004 08:31:41.694183 140602800301888 submission_runner.py:381] Time since start: 26954.21s, 	Step: 29651, 	{'train/ctc_loss': Array(0.21638809, dtype=float32), 'train/wer': 0.07270540821632865, 'validation/ctc_loss': Array(0.46658435, dtype=float32), 'validation/wer': 0.13451166919121266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26056364, dtype=float32), 'test/wer': 0.08283062173745252, 'test/num_examples': 2472, 'score': 24545.970848560333, 'total_duration': 26954.20566511154, 'accumulated_submission_time': 24545.970848560333, 'accumulated_eval_time': 2406.7119488716125, 'accumulated_logging_time': 0.8748612403869629}
I1004 08:31:41.728663 140443633731328 logging_writer.py:48] [29651] accumulated_eval_time=2406.711949, accumulated_logging_time=0.874861, accumulated_submission_time=24545.970849, global_step=29651, preemption_count=0, score=24545.970849, test/ctc_loss=0.2605636417865753, test/num_examples=2472, test/wer=0.082831, total_duration=26954.205665, train/ctc_loss=0.21638809144496918, train/wer=0.072705, validation/ctc_loss=0.46658435463905334, validation/num_examples=5348, validation/wer=0.134512
I1004 08:36:11.005532 140443633731328 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.634645700454712, loss=1.2640081644058228
I1004 08:43:16.802486 140443625338624 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.8669402599334717, loss=1.2442892789840698
I1004 08:50:41.091298 140442978371328 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.9729926586151123, loss=1.2531930208206177
I1004 08:55:41.987704 140602800301888 spec.py:321] Evaluating on the training split.
I1004 08:56:36.158719 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 08:57:26.299945 140602800301888 spec.py:349] Evaluating on the test split.
I1004 08:57:52.274103 140602800301888 submission_runner.py:381] Time since start: 28524.78s, 	Step: 31374, 	{'train/ctc_loss': Array(0.18494585, dtype=float32), 'train/wer': 0.06096614740177646, 'validation/ctc_loss': Array(0.4439618, dtype=float32), 'validation/wer': 0.12841416704454456, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24449727, dtype=float32), 'test/wer': 0.07752929945361851, 'test/num_examples': 2472, 'score': 25986.181250095367, 'total_duration': 28524.784794569016, 'accumulated_submission_time': 25986.181250095367, 'accumulated_eval_time': 2536.991640329361, 'accumulated_logging_time': 0.9249217510223389}
I1004 08:57:52.320616 140444063811328 logging_writer.py:48] [31374] accumulated_eval_time=2536.991640, accumulated_logging_time=0.924922, accumulated_submission_time=25986.181250, global_step=31374, preemption_count=0, score=25986.181250, test/ctc_loss=0.24449726939201355, test/num_examples=2472, test/wer=0.077529, total_duration=28524.784795, train/ctc_loss=0.18494585156440735, train/wer=0.060966, validation/ctc_loss=0.4439617991447449, validation/num_examples=5348, validation/wer=0.128414
I1004 08:59:27.663988 140444055418624 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.8248648643493652, loss=1.2570470571517944
I1004 09:06:33.612526 140443736131328 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.7699660062789917, loss=1.194518804550171
I1004 09:13:24.848296 140443727738624 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.172386407852173, loss=1.184716820716858
I1004 09:20:50.506798 140443736131328 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.0755038261413574, loss=1.2355339527130127
I1004 09:21:52.492776 140602800301888 spec.py:321] Evaluating on the training split.
I1004 09:22:48.087764 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 09:23:38.148371 140602800301888 spec.py:349] Evaluating on the test split.
I1004 09:24:03.715873 140602800301888 submission_runner.py:381] Time since start: 30096.23s, 	Step: 33084, 	{'train/ctc_loss': Array(0.18844369, dtype=float32), 'train/wer': 0.06236740313044193, 'validation/ctc_loss': Array(0.42690825, dtype=float32), 'validation/wer': 0.12221053748709587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23516156, dtype=float32), 'test/wer': 0.07425913513293929, 'test/num_examples': 2472, 'score': 27426.294216394424, 'total_duration': 30096.22763776779, 'accumulated_submission_time': 27426.294216394424, 'accumulated_eval_time': 2668.2090978622437, 'accumulated_logging_time': 0.9980142116546631}
I1004 09:24:03.750730 140444063811328 logging_writer.py:48] [33084] accumulated_eval_time=2668.209098, accumulated_logging_time=0.998014, accumulated_submission_time=27426.294216, global_step=33084, preemption_count=0, score=27426.294216, test/ctc_loss=0.23516155779361725, test/num_examples=2472, test/wer=0.074259, total_duration=30096.227638, train/ctc_loss=0.18844369053840637, train/wer=0.062367, validation/ctc_loss=0.4269082546234131, validation/num_examples=5348, validation/wer=0.122211
I1004 09:29:28.935721 140444055418624 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6614792346954346, loss=1.1867249011993408
I1004 09:36:58.467693 140444063811328 logging_writer.py:48] [34000] global_step=34000, grad_norm=7.485904216766357, loss=1.2281931638717651
I1004 09:43:30.521529 140444055418624 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.3317744731903076, loss=1.1114836931228638
I1004 09:48:03.985655 140602800301888 spec.py:321] Evaluating on the training split.
I1004 09:48:58.858966 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 09:49:48.986548 140602800301888 spec.py:349] Evaluating on the test split.
I1004 09:50:14.315344 140602800301888 submission_runner.py:381] Time since start: 31666.83s, 	Step: 34809, 	{'train/ctc_loss': Array(0.18278067, dtype=float32), 'train/wer': 0.05917708366635762, 'validation/ctc_loss': Array(0.4175868, dtype=float32), 'validation/wer': 0.11993362212852994, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22993948, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 28866.481368780136, 'total_duration': 31666.82618880272, 'accumulated_submission_time': 28866.481368780136, 'accumulated_eval_time': 2798.5322132110596, 'accumulated_logging_time': 1.047367811203003}
I1004 09:50:14.356064 140443633731328 logging_writer.py:48] [34809] accumulated_eval_time=2798.532213, accumulated_logging_time=1.047368, accumulated_submission_time=28866.481369, global_step=34809, preemption_count=0, score=28866.481369, test/ctc_loss=0.22993947565555573, test/num_examples=2472, test/wer=0.072715, total_duration=31666.826189, train/ctc_loss=0.1827806681394577, train/wer=0.059177, validation/ctc_loss=0.4175868034362793, validation/num_examples=5348, validation/wer=0.119934
I1004 09:52:39.623940 140443625338624 logging_writer.py:48] [35000] global_step=35000, grad_norm=8.497151374816895, loss=1.1444199085235596
I1004 09:59:23.320264 140442978371328 logging_writer.py:48] [35500] global_step=35500, grad_norm=5.4031877517700195, loss=1.212217092514038
I1004 10:06:54.392228 140602800301888 spec.py:321] Evaluating on the training split.
I1004 10:07:49.541030 140602800301888 spec.py:333] Evaluating on the validation split.
I1004 10:08:39.610145 140602800301888 spec.py:349] Evaluating on the test split.
I1004 10:09:05.479454 140602800301888 submission_runner.py:381] Time since start: 32797.99s, 	Step: 36000, 	{'train/ctc_loss': Array(0.1392576, dtype=float32), 'train/wer': 0.04790750591925218, 'validation/ctc_loss': Array(0.41741708, dtype=float32), 'validation/wer': 0.12022306052156799, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23008241, dtype=float32), 'test/wer': 0.0726951435013101, 'test/num_examples': 2472, 'score': 29866.478229761124, 'total_duration': 32797.99414372444, 'accumulated_submission_time': 29866.478229761124, 'accumulated_eval_time': 2929.616721868515, 'accumulated_logging_time': 1.1033222675323486}
I1004 10:09:05.510601 140444063811328 logging_writer.py:48] [36000] accumulated_eval_time=2929.616722, accumulated_logging_time=1.103322, accumulated_submission_time=29866.478230, global_step=36000, preemption_count=0, score=29866.478230, test/ctc_loss=0.2300824075937271, test/num_examples=2472, test/wer=0.072695, total_duration=32797.994144, train/ctc_loss=0.13925759494304657, train/wer=0.047908, validation/ctc_loss=0.41741707921028137, validation/num_examples=5348, validation/wer=0.120223
I1004 10:09:05.533711 140444055418624 logging_writer.py:48] [36000] global_step=36000, preemption_count=0, score=29866.478230
I1004 10:09:05.758084 140602800301888 checkpoints.py:490] Saving checkpoint at step: 36000
I1004 10:09:06.810359 140602800301888 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/checkpoint_36000
I1004 10:09:06.830103 140602800301888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_deepspeech_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/checkpoint_36000.
I1004 10:09:08.102411 140602800301888 submission_runner.py:549] Tuning trial 1/1
I1004 10:09:08.102705 140602800301888 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.004958460849689891, beta1=0.863744242567442, beta2=0.6291854735396584, warmup_steps=720, weight_decay=0.1147386261512052)
I1004 10:09:08.113836 140602800301888 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.469002, dtype=float32), 'train/wer': 2.5833506861295628, 'validation/ctc_loss': Array(30.636894, dtype=float32), 'validation/wer': 2.455112929213017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.664526, dtype=float32), 'test/wer': 2.46109316921577, 'test/num_examples': 2472, 'score': 59.58067512512207, 'total_duration': 258.2927260398865, 'accumulated_submission_time': 59.58067512512207, 'accumulated_eval_time': 198.71199917793274, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1784, {'train/ctc_loss': Array(0.8318721, dtype=float32), 'train/wer': 0.25860950340878663, 'validation/ctc_loss': Array(1.2773172, dtype=float32), 'validation/wer': 0.34347654101824426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8950843, dtype=float32), 'test/wer': 0.27445006398147587, 'test/num_examples': 2472, 'score': 1500.1013827323914, 'total_duration': 1825.1752665042877, 'accumulated_submission_time': 1500.1013827323914, 'accumulated_eval_time': 325.0028088092804, 'accumulated_logging_time': 0.038732290267944336, 'global_step': 1784, 'preemption_count': 0}), (3600, {'train/ctc_loss': Array(0.64848626, dtype=float32), 'train/wer': 0.20901366433156646, 'validation/ctc_loss': Array(1.0321589, dtype=float32), 'validation/wer': 0.28597478026802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6792263, dtype=float32), 'test/wer': 0.20878272703268133, 'test/num_examples': 2472, 'score': 2940.28547000885, 'total_duration': 3392.8565583229065, 'accumulated_submission_time': 2940.28547000885, 'accumulated_eval_time': 452.4120047092438, 'accumulated_logging_time': 0.08900928497314453, 'global_step': 3600, 'preemption_count': 0}), (5377, {'train/ctc_loss': Array(0.74621236, dtype=float32), 'train/wer': 0.2367337687324404, 'validation/ctc_loss': Array(1.3818073, dtype=float32), 'validation/wer': 0.3547356945074241, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8095734, dtype=float32), 'test/wer': 0.24369833241931224, 'test/num_examples': 2472, 'score': 4380.235234498978, 'total_duration': 4960.359409809113, 'accumulated_submission_time': 4380.235234498978, 'accumulated_eval_time': 579.8762528896332, 'accumulated_logging_time': 0.14043927192687988, 'global_step': 5377, 'preemption_count': 0}), (7134, {'train/ctc_loss': Array(0.5563652, dtype=float32), 'train/wer': 0.1797798699326624, 'validation/ctc_loss': Array(0.922901, dtype=float32), 'validation/wer': 0.2565871354282241, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58950496, dtype=float32), 'test/wer': 0.18727276420287206, 'test/num_examples': 2472, 'score': 5820.85857462883, 'total_duration': 6530.310212612152, 'accumulated_submission_time': 5820.85857462883, 'accumulated_eval_time': 709.1220633983612, 'accumulated_logging_time': 0.18543100357055664, 'global_step': 7134, 'preemption_count': 0}), (8901, {'train/ctc_loss': Array(0.49882352, dtype=float32), 'train/wer': 0.16288134119028463, 'validation/ctc_loss': Array(0.8343963, dtype=float32), 'validation/wer': 0.2350432710397592, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5336107, dtype=float32), 'test/wer': 0.16568155505453658, 'test/num_examples': 2472, 'score': 7261.61186003685, 'total_duration': 8098.933697223663, 'accumulated_submission_time': 7261.61186003685, 'accumulated_eval_time': 836.9044826030731, 'accumulated_logging_time': 0.23556733131408691, 'global_step': 8901, 'preemption_count': 0}), (10680, {'train/ctc_loss': Array(0.5041183, dtype=float32), 'train/wer': 0.16250113362958854, 'validation/ctc_loss': Array(0.8131147, dtype=float32), 'validation/wer': 0.2282993564819728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5144942, dtype=float32), 'test/wer': 0.1636503970913818, 'test/num_examples': 2472, 'score': 8701.990688085556, 'total_duration': 9670.17188167572, 'accumulated_submission_time': 8701.990688085556, 'accumulated_eval_time': 967.6762025356293, 'accumulated_logging_time': 0.28562211990356445, 'global_step': 10680, 'preemption_count': 0}), (12382, {'train/ctc_loss': Array(0.46414116, dtype=float32), 'train/wer': 0.15183034479824728, 'validation/ctc_loss': Array(0.79548526, dtype=float32), 'validation/wer': 0.22663026174878678, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4964626, dtype=float32), 'test/wer': 0.15737411898523349, 'test/num_examples': 2472, 'score': 10142.067734956741, 'total_duration': 11240.179876327515, 'accumulated_submission_time': 10142.067734956741, 'accumulated_eval_time': 1097.5169973373413, 'accumulated_logging_time': 0.33687829971313477, 'global_step': 12382, 'preemption_count': 0}), (14113, {'train/ctc_loss': Array(0.39048758, dtype=float32), 'train/wer': 0.1337996307756312, 'validation/ctc_loss': Array(0.7435745, dtype=float32), 'validation/wer': 0.21127073102490135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45246524, dtype=float32), 'test/wer': 0.14640586598419758, 'test/num_examples': 2472, 'score': 11583.02232336998, 'total_duration': 12811.975700616837, 'accumulated_submission_time': 11583.02232336998, 'accumulated_eval_time': 1228.2612142562866, 'accumulated_logging_time': 0.3952305316925049, 'global_step': 14113, 'preemption_count': 0}), (15871, {'train/ctc_loss': Array(0.3636681, dtype=float32), 'train/wer': 0.1209348349993882, 'validation/ctc_loss': Array(0.71133167, dtype=float32), 'validation/wer': 0.20246215593011027, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43275332, dtype=float32), 'test/wer': 0.138524973087157, 'test/num_examples': 2472, 'score': 13023.047841787338, 'total_duration': 14380.900707006454, 'accumulated_submission_time': 13023.047841787338, 'accumulated_eval_time': 1357.0659942626953, 'accumulated_logging_time': 0.4498157501220703, 'global_step': 15871, 'preemption_count': 0}), (17588, {'train/ctc_loss': Array(0.36850655, dtype=float32), 'train/wer': 0.12704124590410099, 'validation/ctc_loss': Array(0.69042826, dtype=float32), 'validation/wer': 0.19756099914133277, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41838896, dtype=float32), 'test/wer': 0.13371112871448013, 'test/num_examples': 2472, 'score': 14463.117428064346, 'total_duration': 15951.021684408188, 'accumulated_submission_time': 14463.117428064346, 'accumulated_eval_time': 1487.0252075195312, 'accumulated_logging_time': 0.5037736892700195, 'global_step': 17588, 'preemption_count': 0}), (19307, {'train/ctc_loss': Array(0.34868133, dtype=float32), 'train/wer': 0.11443983183056136, 'validation/ctc_loss': Array(0.6443241, dtype=float32), 'validation/wer': 0.1846616947582707, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3873429, dtype=float32), 'test/wer': 0.12253975991712876, 'test/num_examples': 2472, 'score': 15904.408606290817, 'total_duration': 17523.362096071243, 'accumulated_submission_time': 15904.408606290817, 'accumulated_eval_time': 1617.9851703643799, 'accumulated_logging_time': 0.5547370910644531, 'global_step': 19307, 'preemption_count': 0}), (21046, {'train/ctc_loss': Array(0.3413416, dtype=float32), 'train/wer': 0.11239612736433904, 'validation/ctc_loss': Array(0.6217223, dtype=float32), 'validation/wer': 0.17811073912917635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3672197, dtype=float32), 'test/wer': 0.1186602482075031, 'test/num_examples': 2472, 'score': 17344.97687458992, 'total_duration': 19094.91708922386, 'accumulated_submission_time': 17344.97687458992, 'accumulated_eval_time': 1748.8823819160461, 'accumulated_logging_time': 0.6053192615509033, 'global_step': 21046, 'preemption_count': 0}), (22751, {'train/ctc_loss': Array(0.30649957, dtype=float32), 'train/wer': 0.10354974791645231, 'validation/ctc_loss': Array(0.59275293, dtype=float32), 'validation/wer': 0.1707397080531409, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34654742, dtype=float32), 'test/wer': 0.11041374687709463, 'test/num_examples': 2472, 'score': 18785.53312087059, 'total_duration': 20667.019449949265, 'accumulated_submission_time': 18785.53312087059, 'accumulated_eval_time': 1880.3381979465485, 'accumulated_logging_time': 0.6574959754943848, 'global_step': 22751, 'preemption_count': 0}), (24475, {'train/ctc_loss': Array(0.26605812, dtype=float32), 'train/wer': 0.08990329800261813, 'validation/ctc_loss': Array(0.5582985, dtype=float32), 'validation/wer': 0.16077337938619765, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32141584, dtype=float32), 'test/wer': 0.10241098450226474, 'test/num_examples': 2472, 'score': 20225.5305454731, 'total_duration': 22238.683361291885, 'accumulated_submission_time': 20225.5305454731, 'accumulated_eval_time': 2011.915414571762, 'accumulated_logging_time': 0.7086536884307861, 'global_step': 24475, 'preemption_count': 0}), (26220, {'train/ctc_loss': Array(0.23424065, dtype=float32), 'train/wer': 0.08034638357807976, 'validation/ctc_loss': Array(0.5223064, dtype=float32), 'validation/wer': 0.150720219201343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29964456, dtype=float32), 'test/wer': 0.09448946844596104, 'test/num_examples': 2472, 'score': 21665.507152318954, 'total_duration': 23809.43762254715, 'accumulated_submission_time': 21665.507152318954, 'accumulated_eval_time': 2142.5957176685333, 'accumulated_logging_time': 0.7658460140228271, 'global_step': 26220, 'preemption_count': 0}), (27941, {'train/ctc_loss': Array(0.21349773, dtype=float32), 'train/wer': 0.07131460248949074, 'validation/ctc_loss': Array(0.49260858, dtype=float32), 'validation/wer': 0.14226861812463218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2820458, dtype=float32), 'test/wer': 0.0905693335770723, 'test/num_examples': 2472, 'score': 23105.775389432907, 'total_duration': 25383.56040763855, 'accumulated_submission_time': 23105.775389432907, 'accumulated_eval_time': 2276.354704141617, 'accumulated_logging_time': 0.8213834762573242, 'global_step': 27941, 'preemption_count': 0}), (29651, {'train/ctc_loss': Array(0.21638809, dtype=float32), 'train/wer': 0.07270540821632865, 'validation/ctc_loss': Array(0.46658435, dtype=float32), 'validation/wer': 0.13451166919121266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26056364, dtype=float32), 'test/wer': 0.08283062173745252, 'test/num_examples': 2472, 'score': 24545.970848560333, 'total_duration': 26954.20566511154, 'accumulated_submission_time': 24545.970848560333, 'accumulated_eval_time': 2406.7119488716125, 'accumulated_logging_time': 0.8748612403869629, 'global_step': 29651, 'preemption_count': 0}), (31374, {'train/ctc_loss': Array(0.18494585, dtype=float32), 'train/wer': 0.06096614740177646, 'validation/ctc_loss': Array(0.4439618, dtype=float32), 'validation/wer': 0.12841416704454456, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24449727, dtype=float32), 'test/wer': 0.07752929945361851, 'test/num_examples': 2472, 'score': 25986.181250095367, 'total_duration': 28524.784794569016, 'accumulated_submission_time': 25986.181250095367, 'accumulated_eval_time': 2536.991640329361, 'accumulated_logging_time': 0.9249217510223389, 'global_step': 31374, 'preemption_count': 0}), (33084, {'train/ctc_loss': Array(0.18844369, dtype=float32), 'train/wer': 0.06236740313044193, 'validation/ctc_loss': Array(0.42690825, dtype=float32), 'validation/wer': 0.12221053748709587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23516156, dtype=float32), 'test/wer': 0.07425913513293929, 'test/num_examples': 2472, 'score': 27426.294216394424, 'total_duration': 30096.22763776779, 'accumulated_submission_time': 27426.294216394424, 'accumulated_eval_time': 2668.2090978622437, 'accumulated_logging_time': 0.9980142116546631, 'global_step': 33084, 'preemption_count': 0}), (34809, {'train/ctc_loss': Array(0.18278067, dtype=float32), 'train/wer': 0.05917708366635762, 'validation/ctc_loss': Array(0.4175868, dtype=float32), 'validation/wer': 0.11993362212852994, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22993948, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 28866.481368780136, 'total_duration': 31666.82618880272, 'accumulated_submission_time': 28866.481368780136, 'accumulated_eval_time': 2798.5322132110596, 'accumulated_logging_time': 1.047367811203003, 'global_step': 34809, 'preemption_count': 0}), (36000, {'train/ctc_loss': Array(0.1392576, dtype=float32), 'train/wer': 0.04790750591925218, 'validation/ctc_loss': Array(0.41741708, dtype=float32), 'validation/wer': 0.12022306052156799, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23008241, dtype=float32), 'test/wer': 0.0726951435013101, 'test/num_examples': 2472, 'score': 29866.478229761124, 'total_duration': 32797.99414372444, 'accumulated_submission_time': 29866.478229761124, 'accumulated_eval_time': 2929.616721868515, 'accumulated_logging_time': 1.1033222675323486, 'global_step': 36000, 'preemption_count': 0})], 'global_step': 36000}
I1004 10:09:08.114052 140602800301888 submission_runner.py:552] Timing: 29866.478229761124
I1004 10:09:08.114124 140602800301888 submission_runner.py:554] Total number of evals: 22
I1004 10:09:08.114182 140602800301888 submission_runner.py:555] ====================
I1004 10:09:08.117284 140602800301888 submission_runner.py:625] Final librispeech_deepspeech score: 29866.478229761124
