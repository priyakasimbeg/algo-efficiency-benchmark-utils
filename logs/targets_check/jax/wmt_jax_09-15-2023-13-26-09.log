python3 submission_runner.py --framework=jax --workload=wmt --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/wmt/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_jax/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=100000 2>&1 | tee -a /logs/wmt_jax_09-15-2023-13-26-09.log
2023-09-15 13:26:13.937792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0915 13:26:33.601240 140200172709696 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax.
I0915 13:26:34.662902 140200172709696 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0915 13:26:34.663658 140200172709696 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0915 13:26:34.663805 140200172709696 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0915 13:26:34.670284 140200172709696 submission_runner.py:500] Using RNG seed 269726624
I0915 13:26:40.689781 140200172709696 submission_runner.py:509] --- Tuning run 1/1 ---
I0915 13:26:40.690043 140200172709696 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1.
I0915 13:26:40.690258 140200172709696 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1/hparams.json.
I0915 13:26:40.877544 140200172709696 submission_runner.py:185] Initializing dataset.
I0915 13:26:40.889157 140200172709696 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0915 13:26:40.893328 140200172709696 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 13:26:41.059636 140200172709696 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0915 13:26:43.026408 140200172709696 submission_runner.py:192] Initializing model.
I0915 13:26:51.995712 140200172709696 submission_runner.py:226] Initializing optimizer.
I0915 13:26:53.029101 140200172709696 submission_runner.py:233] Initializing metrics bundle.
I0915 13:26:53.029313 140200172709696 submission_runner.py:251] Initializing checkpoint and logger.
I0915 13:26:53.030563 140200172709696 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1 with prefix checkpoint_
I0915 13:26:53.030857 140200172709696 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0915 13:26:53.030924 140200172709696 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0915 13:26:53.964699 140200172709696 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1/meta_data_0.json.
I0915 13:26:53.965780 140200172709696 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1/flags_0.json.
I0915 13:26:53.975959 140200172709696 submission_runner.py:285] Starting training loop.
I0915 13:27:39.705450 140035831625472 logging_writer.py:48] [0] global_step=0, grad_norm=5.025815963745117, loss=11.1251859664917
I0915 13:27:39.722815 140200172709696 spec.py:320] Evaluating on the training split.
I0915 13:27:39.726549 140200172709696 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0915 13:27:39.729682 140200172709696 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 13:27:39.769732 140200172709696 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0915 13:27:47.452807 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 13:32:43.304437 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 13:32:43.308144 140200172709696 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 13:32:43.311885 140200172709696 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 13:32:43.348550 140200172709696 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 13:32:50.740160 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 13:37:39.232525 140200172709696 spec.py:348] Evaluating on the test split.
I0915 13:37:39.235396 140200172709696 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 13:37:39.238633 140200172709696 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 13:37:39.275077 140200172709696 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 13:37:46.198925 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 13:42:32.049534 140200172709696 submission_runner.py:376] Time since start: 938.07s, 	Step: 1, 	{'train/accuracy': 0.000594904413446784, 'train/loss': 11.167823791503906, 'train/bleu': 1.6684120676165568e-10, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.146151542663574, 'validation/bleu': 1.1152830692390254e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.15811538696289, 'test/bleu': 2.5300816166015135e-10, 'test/num_examples': 3003, 'score': 45.74680542945862, 'total_duration': 938.0734601020813, 'accumulated_submission_time': 45.74680542945862, 'accumulated_eval_time': 892.3266117572784, 'accumulated_logging_time': 0}
I0915 13:42:32.075509 140023658383104 logging_writer.py:48] [1] accumulated_eval_time=892.326612, accumulated_logging_time=0, accumulated_submission_time=45.746805, global_step=1, preemption_count=0, score=45.746805, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.158115, test/num_examples=3003, total_duration=938.073460, train/accuracy=0.000595, train/bleu=0.000000, train/loss=11.167824, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.146152, validation/num_examples=3000
I0915 13:42:32.449237 140023666775808 logging_writer.py:48] [1] global_step=1, grad_norm=4.932901382446289, loss=11.127994537353516
I0915 13:42:32.822027 140023658383104 logging_writer.py:48] [2] global_step=2, grad_norm=4.944317817687988, loss=11.081155776977539
I0915 13:42:33.194401 140023666775808 logging_writer.py:48] [3] global_step=3, grad_norm=4.764772891998291, loss=11.025629997253418
I0915 13:42:33.567180 140023658383104 logging_writer.py:48] [4] global_step=4, grad_norm=4.700409889221191, loss=10.94967269897461
I0915 13:42:33.936978 140023666775808 logging_writer.py:48] [5] global_step=5, grad_norm=4.635671138763428, loss=10.867131233215332
I0915 13:42:34.306459 140023658383104 logging_writer.py:48] [6] global_step=6, grad_norm=4.4527387619018555, loss=10.754837989807129
I0915 13:42:34.676076 140023666775808 logging_writer.py:48] [7] global_step=7, grad_norm=4.218101978302002, loss=10.638904571533203
I0915 13:42:35.047163 140023658383104 logging_writer.py:48] [8] global_step=8, grad_norm=3.9495530128479004, loss=10.503188133239746
I0915 13:42:35.421356 140023666775808 logging_writer.py:48] [9] global_step=9, grad_norm=3.6284475326538086, loss=10.377284049987793
I0915 13:42:35.797978 140023658383104 logging_writer.py:48] [10] global_step=10, grad_norm=3.204528331756592, loss=10.24887752532959
I0915 13:42:36.174708 140023666775808 logging_writer.py:48] [11] global_step=11, grad_norm=2.977766513824463, loss=10.107841491699219
I0915 13:42:36.548352 140023658383104 logging_writer.py:48] [12] global_step=12, grad_norm=2.6716833114624023, loss=9.972177505493164
I0915 13:42:36.922472 140023666775808 logging_writer.py:48] [13] global_step=13, grad_norm=2.3786911964416504, loss=9.866292953491211
I0915 13:42:37.296669 140023658383104 logging_writer.py:48] [14] global_step=14, grad_norm=2.116149663925171, loss=9.776108741760254
I0915 13:42:37.670092 140023666775808 logging_writer.py:48] [15] global_step=15, grad_norm=1.922607421875, loss=9.633842468261719
I0915 13:42:38.043672 140023658383104 logging_writer.py:48] [16] global_step=16, grad_norm=1.6968834400177002, loss=9.58411979675293
I0915 13:42:38.417694 140023666775808 logging_writer.py:48] [17] global_step=17, grad_norm=1.5452169179916382, loss=9.478080749511719
I0915 13:42:38.793473 140023658383104 logging_writer.py:48] [18] global_step=18, grad_norm=1.410372018814087, loss=9.401885986328125
I0915 13:42:39.168682 140023666775808 logging_writer.py:48] [19] global_step=19, grad_norm=1.2482497692108154, loss=9.377601623535156
I0915 13:42:39.542531 140023658383104 logging_writer.py:48] [20] global_step=20, grad_norm=1.2224129438400269, loss=9.251015663146973
I0915 13:42:39.916286 140023666775808 logging_writer.py:48] [21] global_step=21, grad_norm=1.0999579429626465, loss=9.204401016235352
I0915 13:42:40.288817 140023658383104 logging_writer.py:48] [22] global_step=22, grad_norm=1.0091665983200073, loss=9.163582801818848
I0915 13:42:40.662382 140023666775808 logging_writer.py:48] [23] global_step=23, grad_norm=0.9322729706764221, loss=9.098591804504395
I0915 13:42:41.036578 140023658383104 logging_writer.py:48] [24] global_step=24, grad_norm=0.8501284122467041, loss=9.059540748596191
I0915 13:42:41.412533 140023666775808 logging_writer.py:48] [25] global_step=25, grad_norm=0.775258481502533, loss=8.99535846710205
I0915 13:42:41.783262 140023658383104 logging_writer.py:48] [26] global_step=26, grad_norm=0.7105799913406372, loss=8.969937324523926
I0915 13:42:42.157161 140023666775808 logging_writer.py:48] [27] global_step=27, grad_norm=0.6703822612762451, loss=8.937797546386719
I0915 13:42:42.530937 140023658383104 logging_writer.py:48] [28] global_step=28, grad_norm=0.6305086016654968, loss=8.916544914245605
I0915 13:42:42.905265 140023666775808 logging_writer.py:48] [29] global_step=29, grad_norm=0.6008144617080688, loss=8.834985733032227
I0915 13:42:43.281515 140023658383104 logging_writer.py:48] [30] global_step=30, grad_norm=0.55203777551651, loss=8.819327354431152
I0915 13:42:43.654916 140023666775808 logging_writer.py:48] [31] global_step=31, grad_norm=0.5190713405609131, loss=8.799396514892578
I0915 13:42:44.029135 140023658383104 logging_writer.py:48] [32] global_step=32, grad_norm=0.4922637343406677, loss=8.743566513061523
I0915 13:42:44.402411 140023666775808 logging_writer.py:48] [33] global_step=33, grad_norm=0.46112361550331116, loss=8.74184513092041
I0915 13:42:44.778328 140023658383104 logging_writer.py:48] [34] global_step=34, grad_norm=0.4273923635482788, loss=8.697470664978027
I0915 13:42:45.152505 140023666775808 logging_writer.py:48] [35] global_step=35, grad_norm=0.40348654985427856, loss=8.703102111816406
I0915 13:42:45.526805 140023658383104 logging_writer.py:48] [36] global_step=36, grad_norm=0.3814046084880829, loss=8.722206115722656
I0915 13:42:45.901241 140023666775808 logging_writer.py:48] [37] global_step=37, grad_norm=0.3520272970199585, loss=8.676711082458496
I0915 13:42:46.276388 140023658383104 logging_writer.py:48] [38] global_step=38, grad_norm=0.3372657597064972, loss=8.656397819519043
I0915 13:42:46.649091 140023666775808 logging_writer.py:48] [39] global_step=39, grad_norm=0.31513288617134094, loss=8.657600402832031
I0915 13:42:47.023509 140023658383104 logging_writer.py:48] [40] global_step=40, grad_norm=0.28928834199905396, loss=8.639022827148438
I0915 13:42:47.397619 140023666775808 logging_writer.py:48] [41] global_step=41, grad_norm=0.27232468128204346, loss=8.631919860839844
I0915 13:42:47.771292 140023658383104 logging_writer.py:48] [42] global_step=42, grad_norm=0.2624778747558594, loss=8.607707977294922
I0915 13:42:48.143692 140023666775808 logging_writer.py:48] [43] global_step=43, grad_norm=0.2602161169052124, loss=8.569725036621094
I0915 13:42:48.520007 140023658383104 logging_writer.py:48] [44] global_step=44, grad_norm=0.26370128989219666, loss=8.544177055358887
I0915 13:42:48.894119 140023666775808 logging_writer.py:48] [45] global_step=45, grad_norm=0.22748151421546936, loss=8.58067798614502
I0915 13:42:49.270373 140023658383104 logging_writer.py:48] [46] global_step=46, grad_norm=0.23032374680042267, loss=8.515013694763184
I0915 13:42:49.645314 140023666775808 logging_writer.py:48] [47] global_step=47, grad_norm=0.22666531801223755, loss=8.509754180908203
I0915 13:42:50.019777 140023658383104 logging_writer.py:48] [48] global_step=48, grad_norm=0.2074895203113556, loss=8.533958435058594
I0915 13:42:50.397092 140023666775808 logging_writer.py:48] [49] global_step=49, grad_norm=0.20231276750564575, loss=8.502178192138672
I0915 13:42:50.770770 140023658383104 logging_writer.py:48] [50] global_step=50, grad_norm=0.19509752094745636, loss=8.499176979064941
I0915 13:42:51.147561 140023666775808 logging_writer.py:48] [51] global_step=51, grad_norm=0.19254092872142792, loss=8.485275268554688
I0915 13:42:51.524374 140023658383104 logging_writer.py:48] [52] global_step=52, grad_norm=0.1924912929534912, loss=8.481030464172363
I0915 13:42:51.897716 140023666775808 logging_writer.py:48] [53] global_step=53, grad_norm=0.17895570397377014, loss=8.543872833251953
I0915 13:42:52.273999 140023658383104 logging_writer.py:48] [54] global_step=54, grad_norm=0.1759992390871048, loss=8.54776382446289
I0915 13:42:52.648614 140023666775808 logging_writer.py:48] [55] global_step=55, grad_norm=0.17646656930446625, loss=8.48193073272705
I0915 13:42:53.022267 140023658383104 logging_writer.py:48] [56] global_step=56, grad_norm=0.1776454746723175, loss=8.490402221679688
I0915 13:42:53.397907 140023666775808 logging_writer.py:48] [57] global_step=57, grad_norm=0.17017383873462677, loss=8.461669921875
I0915 13:42:53.772866 140023658383104 logging_writer.py:48] [58] global_step=58, grad_norm=0.1688668429851532, loss=8.457051277160645
I0915 13:42:54.146424 140023666775808 logging_writer.py:48] [59] global_step=59, grad_norm=0.17745666205883026, loss=8.477984428405762
I0915 13:42:54.522756 140023658383104 logging_writer.py:48] [60] global_step=60, grad_norm=0.16855046153068542, loss=8.480098724365234
I0915 13:42:54.897424 140023666775808 logging_writer.py:48] [61] global_step=61, grad_norm=0.16233967244625092, loss=8.437326431274414
I0915 13:42:55.270737 140023658383104 logging_writer.py:48] [62] global_step=62, grad_norm=0.16725806891918182, loss=8.493821144104004
I0915 13:42:55.647827 140023666775808 logging_writer.py:48] [63] global_step=63, grad_norm=0.1585465520620346, loss=8.442180633544922
I0915 13:42:56.024834 140023658383104 logging_writer.py:48] [64] global_step=64, grad_norm=0.1556384563446045, loss=8.455607414245605
I0915 13:42:56.400633 140023666775808 logging_writer.py:48] [65] global_step=65, grad_norm=0.16519875824451447, loss=8.442238807678223
I0915 13:42:56.776816 140023658383104 logging_writer.py:48] [66] global_step=66, grad_norm=0.16193164885044098, loss=8.429814338684082
I0915 13:42:57.149274 140023666775808 logging_writer.py:48] [67] global_step=67, grad_norm=0.16376446187496185, loss=8.41765308380127
I0915 13:42:57.526227 140023658383104 logging_writer.py:48] [68] global_step=68, grad_norm=0.15773701667785645, loss=8.361577033996582
I0915 13:42:57.903224 140023666775808 logging_writer.py:48] [69] global_step=69, grad_norm=0.16399765014648438, loss=8.385022163391113
I0915 13:42:58.283708 140023658383104 logging_writer.py:48] [70] global_step=70, grad_norm=0.1707138568162918, loss=8.3729829788208
I0915 13:42:58.659797 140023666775808 logging_writer.py:48] [71] global_step=71, grad_norm=0.1585136204957962, loss=8.390798568725586
I0915 13:42:59.035419 140023658383104 logging_writer.py:48] [72] global_step=72, grad_norm=0.1777404248714447, loss=8.376537322998047
I0915 13:42:59.412813 140023666775808 logging_writer.py:48] [73] global_step=73, grad_norm=0.16432078182697296, loss=8.370304107666016
I0915 13:42:59.792400 140023658383104 logging_writer.py:48] [74] global_step=74, grad_norm=0.16393208503723145, loss=8.335127830505371
I0915 13:43:00.166587 140023666775808 logging_writer.py:48] [75] global_step=75, grad_norm=0.17107023298740387, loss=8.354022979736328
I0915 13:43:00.542126 140023658383104 logging_writer.py:48] [76] global_step=76, grad_norm=0.16647647321224213, loss=8.323217391967773
I0915 13:43:00.920196 140023666775808 logging_writer.py:48] [77] global_step=77, grad_norm=0.16782423853874207, loss=8.341989517211914
I0915 13:43:01.292482 140023658383104 logging_writer.py:48] [78] global_step=78, grad_norm=0.17599883675575256, loss=8.305039405822754
I0915 13:43:01.666764 140023666775808 logging_writer.py:48] [79] global_step=79, grad_norm=0.15214727818965912, loss=8.319971084594727
I0915 13:43:02.041630 140023658383104 logging_writer.py:48] [80] global_step=80, grad_norm=0.16226406395435333, loss=8.279858589172363
I0915 13:43:02.412923 140023666775808 logging_writer.py:48] [81] global_step=81, grad_norm=0.1456865519285202, loss=8.325493812561035
I0915 13:43:02.783291 140023658383104 logging_writer.py:48] [82] global_step=82, grad_norm=0.15302704274654388, loss=8.286650657653809
I0915 13:43:03.157349 140023666775808 logging_writer.py:48] [83] global_step=83, grad_norm=0.15304939448833466, loss=8.290336608886719
I0915 13:43:03.531201 140023658383104 logging_writer.py:48] [84] global_step=84, grad_norm=0.15551182627677917, loss=8.279248237609863
I0915 13:43:03.904613 140023666775808 logging_writer.py:48] [85] global_step=85, grad_norm=0.1519366204738617, loss=8.338722229003906
I0915 13:43:04.277417 140023658383104 logging_writer.py:48] [86] global_step=86, grad_norm=0.1577950417995453, loss=8.273443222045898
I0915 13:43:04.649955 140023666775808 logging_writer.py:48] [87] global_step=87, grad_norm=0.1566486805677414, loss=8.2269287109375
I0915 13:43:05.022673 140023658383104 logging_writer.py:48] [88] global_step=88, grad_norm=0.173188716173172, loss=8.201592445373535
I0915 13:43:05.394624 140023666775808 logging_writer.py:48] [89] global_step=89, grad_norm=0.15740443766117096, loss=8.242822647094727
I0915 13:43:05.766381 140023658383104 logging_writer.py:48] [90] global_step=90, grad_norm=0.16547536849975586, loss=8.244048118591309
I0915 13:43:06.139989 140023666775808 logging_writer.py:48] [91] global_step=91, grad_norm=0.146604523062706, loss=8.23366928100586
I0915 13:43:06.511132 140023658383104 logging_writer.py:48] [92] global_step=92, grad_norm=0.163418710231781, loss=8.21965503692627
I0915 13:43:06.883750 140023666775808 logging_writer.py:48] [93] global_step=93, grad_norm=0.1704028993844986, loss=8.249321937561035
I0915 13:43:07.255268 140023658383104 logging_writer.py:48] [94] global_step=94, grad_norm=0.18766404688358307, loss=8.237503051757812
I0915 13:43:07.624680 140023666775808 logging_writer.py:48] [95] global_step=95, grad_norm=0.1730021983385086, loss=8.205751419067383
I0915 13:43:07.995386 140023658383104 logging_writer.py:48] [96] global_step=96, grad_norm=0.15679296851158142, loss=8.169769287109375
I0915 13:43:08.367600 140023666775808 logging_writer.py:48] [97] global_step=97, grad_norm=0.16768836975097656, loss=8.205114364624023
I0915 13:43:08.739674 140023658383104 logging_writer.py:48] [98] global_step=98, grad_norm=0.1678730547428131, loss=8.179235458374023
I0915 13:43:09.112101 140023666775808 logging_writer.py:48] [99] global_step=99, grad_norm=0.20737065374851227, loss=8.200484275817871
I0915 13:43:09.482695 140023658383104 logging_writer.py:48] [100] global_step=100, grad_norm=0.1834048628807068, loss=8.154834747314453
I0915 13:45:32.648304 140023666775808 logging_writer.py:48] [500] global_step=500, grad_norm=0.467306524515152, loss=5.630561351776123
I0915 13:48:31.485851 140023658383104 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.46552175283432007, loss=4.377842426300049
I0915 13:51:30.277743 140023666775808 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.4767555594444275, loss=3.43839955329895
I0915 13:54:29.079082 140023658383104 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3434961140155792, loss=3.071336269378662
I0915 13:56:32.089233 140200172709696 spec.py:320] Evaluating on the training split.
I0915 13:56:35.097045 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 13:59:14.869395 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 13:59:17.503879 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:01:54.872619 140200172709696 spec.py:348] Evaluating on the test split.
I0915 14:01:57.581989 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:04:23.260840 140200172709696 submission_runner.py:376] Time since start: 2249.28s, 	Step: 2346, 	{'train/accuracy': 0.5235041379928589, 'train/loss': 2.75996732711792, 'train/bleu': 23.36283316860885, 'validation/accuracy': 0.5274826288223267, 'validation/loss': 2.7242908477783203, 'validation/bleu': 19.380327941442793, 'validation/num_examples': 3000, 'test/accuracy': 0.5237464308738708, 'test/loss': 2.7663111686706543, 'test/bleu': 17.513598452100037, 'test/num_examples': 3003, 'score': 885.715635061264, 'total_duration': 2249.284782886505, 'accumulated_submission_time': 885.715635061264, 'accumulated_eval_time': 1363.4981791973114, 'accumulated_logging_time': 0.03613853454589844}
I0915 14:04:23.276947 140023666775808 logging_writer.py:48] [2346] accumulated_eval_time=1363.498179, accumulated_logging_time=0.036139, accumulated_submission_time=885.715635, global_step=2346, preemption_count=0, score=885.715635, test/accuracy=0.523746, test/bleu=17.513598, test/loss=2.766311, test/num_examples=3003, total_duration=2249.284783, train/accuracy=0.523504, train/bleu=23.362833, train/loss=2.759967, validation/accuracy=0.527483, validation/bleu=19.380328, validation/loss=2.724291, validation/num_examples=3000
I0915 14:05:18.651555 140023658383104 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.2619019150733948, loss=2.7683627605438232
I0915 14:08:17.278454 140023666775808 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.22975626587867737, loss=2.606904983520508
I0915 14:11:15.931607 140023658383104 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.20284415781497955, loss=2.520970344543457
I0915 14:14:14.585734 140023666775808 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.1977120190858841, loss=2.455557346343994
I0915 14:17:13.300690 140023658383104 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.14959827065467834, loss=2.305162191390991
I0915 14:18:23.347408 140200172709696 spec.py:320] Evaluating on the training split.
I0915 14:18:26.338591 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:21:12.999843 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 14:21:15.645667 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:23:37.359395 140200172709696 spec.py:348] Evaluating on the test split.
I0915 14:23:40.066895 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:26:02.292948 140200172709696 submission_runner.py:376] Time since start: 3548.32s, 	Step: 4698, 	{'train/accuracy': 0.579552173614502, 'train/loss': 2.2296831607818604, 'train/bleu': 27.27231776667703, 'validation/accuracy': 0.5946237444877625, 'validation/loss': 2.116241455078125, 'validation/bleu': 23.6002089836129, 'validation/num_examples': 3000, 'test/accuracy': 0.5976642966270447, 'test/loss': 2.0898654460906982, 'test/bleu': 22.19579714477223, 'test/num_examples': 3003, 'score': 1725.7396528720856, 'total_duration': 3548.31689620018, 'accumulated_submission_time': 1725.7396528720856, 'accumulated_eval_time': 1822.4436695575714, 'accumulated_logging_time': 0.06357741355895996}
I0915 14:26:02.309134 140023666775808 logging_writer.py:48] [4698] accumulated_eval_time=1822.443670, accumulated_logging_time=0.063577, accumulated_submission_time=1725.739653, global_step=4698, preemption_count=0, score=1725.739653, test/accuracy=0.597664, test/bleu=22.195797, test/loss=2.089865, test/num_examples=3003, total_duration=3548.316896, train/accuracy=0.579552, train/bleu=27.272318, train/loss=2.229683, validation/accuracy=0.594624, validation/bleu=23.600209, validation/loss=2.116241, validation/num_examples=3000
I0915 14:27:50.627071 140023658383104 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.15279050171375275, loss=2.184817314147949
I0915 14:30:49.343242 140023666775808 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.182752788066864, loss=2.184095859527588
I0915 14:33:48.068974 140023658383104 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.1673070341348648, loss=2.070046901702881
I0915 14:36:46.799180 140023666775808 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1526016891002655, loss=2.1464083194732666
I0915 14:39:45.457359 140023658383104 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.19693760573863983, loss=2.1315906047821045
I0915 14:40:02.335287 140200172709696 spec.py:320] Evaluating on the training split.
I0915 14:40:05.323299 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:42:56.858307 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 14:42:59.522586 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:45:22.101589 140200172709696 spec.py:348] Evaluating on the test split.
I0915 14:45:24.813526 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 14:47:38.517320 140200172709696 submission_runner.py:376] Time since start: 4844.54s, 	Step: 7049, 	{'train/accuracy': 0.6090830564498901, 'train/loss': 1.9835175275802612, 'train/bleu': 28.9226257207205, 'validation/accuracy': 0.6177976727485657, 'validation/loss': 1.9118613004684448, 'validation/bleu': 25.21162722790785, 'validation/num_examples': 3000, 'test/accuracy': 0.6218581199645996, 'test/loss': 1.8709417581558228, 'test/bleu': 23.83966507801399, 'test/num_examples': 3003, 'score': 2565.7210092544556, 'total_duration': 4844.541260957718, 'accumulated_submission_time': 2565.7210092544556, 'accumulated_eval_time': 2278.6256353855133, 'accumulated_logging_time': 0.0899801254272461}
I0915 14:47:38.534046 140023666775808 logging_writer.py:48] [7049] accumulated_eval_time=2278.625635, accumulated_logging_time=0.089980, accumulated_submission_time=2565.721009, global_step=7049, preemption_count=0, score=2565.721009, test/accuracy=0.621858, test/bleu=23.839665, test/loss=1.870942, test/num_examples=3003, total_duration=4844.541261, train/accuracy=0.609083, train/bleu=28.922626, train/loss=1.983518, validation/accuracy=0.617798, validation/bleu=25.211627, validation/loss=1.911861, validation/num_examples=3000
I0915 14:50:20.106528 140023658383104 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.14749357104301453, loss=2.067861795425415
I0915 14:53:18.721035 140023666775808 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.16350127756595612, loss=2.1695804595947266
I0915 14:56:17.489061 140023658383104 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1569971740245819, loss=2.052476167678833
I0915 14:59:16.230014 140023666775808 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1816283017396927, loss=2.090883731842041
I0915 15:01:38.869477 140200172709696 spec.py:320] Evaluating on the training split.
I0915 15:01:41.856972 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:04:06.782030 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 15:04:09.421575 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:06:24.252869 140200172709696 spec.py:348] Evaluating on the test split.
I0915 15:06:26.968727 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:08:39.684641 140200172709696 submission_runner.py:376] Time since start: 6105.71s, 	Step: 9401, 	{'train/accuracy': 0.6170135140419006, 'train/loss': 1.9261927604675293, 'train/bleu': 29.61187248686974, 'validation/accuracy': 0.6302463412284851, 'validation/loss': 1.8087116479873657, 'validation/bleu': 26.1521308761062, 'validation/num_examples': 3000, 'test/accuracy': 0.6373017430305481, 'test/loss': 1.7505528926849365, 'test/bleu': 25.116535533384756, 'test/num_examples': 3003, 'score': 3406.0114896297455, 'total_duration': 6105.708568096161, 'accumulated_submission_time': 3406.0114896297455, 'accumulated_eval_time': 2699.4407522678375, 'accumulated_logging_time': 0.11698603630065918}
I0915 15:08:39.702913 140023658383104 logging_writer.py:48] [9401] accumulated_eval_time=2699.440752, accumulated_logging_time=0.116986, accumulated_submission_time=3406.011490, global_step=9401, preemption_count=0, score=3406.011490, test/accuracy=0.637302, test/bleu=25.116536, test/loss=1.750553, test/num_examples=3003, total_duration=6105.708568, train/accuracy=0.617014, train/bleu=29.611872, train/loss=1.926193, validation/accuracy=0.630246, validation/bleu=26.152131, validation/loss=1.808712, validation/num_examples=3000
I0915 15:09:15.492251 140023666775808 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.159218430519104, loss=2.0206222534179688
I0915 15:12:14.358221 140023658383104 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.2528401017189026, loss=1.9643428325653076
I0915 15:15:13.091102 140023666775808 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.21938110888004303, loss=2.012117624282837
I0915 15:18:11.776694 140023658383104 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.19262102246284485, loss=2.091284990310669
I0915 15:21:10.488473 140023666775808 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.16972939670085907, loss=1.9654127359390259
I0915 15:22:39.881450 140200172709696 spec.py:320] Evaluating on the training split.
I0915 15:22:42.880120 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:25:33.458142 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 15:25:36.113538 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:27:50.729610 140200172709696 spec.py:348] Evaluating on the test split.
I0915 15:27:53.435204 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:30:09.544648 140200172709696 submission_runner.py:376] Time since start: 7395.57s, 	Step: 11752, 	{'train/accuracy': 0.6229552626609802, 'train/loss': 1.8726989030838013, 'train/bleu': 30.029023551583855, 'validation/accuracy': 0.6390249133110046, 'validation/loss': 1.7448806762695312, 'validation/bleu': 26.690216466648614, 'validation/num_examples': 3000, 'test/accuracy': 0.6469815969467163, 'test/loss': 1.6859023571014404, 'test/bleu': 25.546572599410325, 'test/num_examples': 3003, 'score': 4246.143506288528, 'total_duration': 7395.568585634232, 'accumulated_submission_time': 4246.143506288528, 'accumulated_eval_time': 3149.103892326355, 'accumulated_logging_time': 0.14639687538146973}
I0915 15:30:09.561797 140023658383104 logging_writer.py:48] [11752] accumulated_eval_time=3149.103892, accumulated_logging_time=0.146397, accumulated_submission_time=4246.143506, global_step=11752, preemption_count=0, score=4246.143506, test/accuracy=0.646982, test/bleu=25.546573, test/loss=1.685902, test/num_examples=3003, total_duration=7395.568586, train/accuracy=0.622955, train/bleu=30.029024, train/loss=1.872699, validation/accuracy=0.639025, validation/bleu=26.690216, validation/loss=1.744881, validation/num_examples=3000
I0915 15:31:38.610813 140023666775808 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.20694565773010254, loss=2.000589609146118
I0915 15:34:37.433503 140023658383104 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2190602719783783, loss=1.899661898612976
I0915 15:37:36.160032 140023666775808 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.18797560036182404, loss=1.9030734300613403
I0915 15:40:34.911286 140023658383104 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.18729348480701447, loss=1.9631646871566772
I0915 15:43:33.650736 140023666775808 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.19101740419864655, loss=1.908037543296814
I0915 15:44:09.859934 140200172709696 spec.py:320] Evaluating on the training split.
I0915 15:44:12.857386 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:47:06.379045 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 15:47:09.029015 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:49:28.315908 140200172709696 spec.py:348] Evaluating on the test split.
I0915 15:49:31.033605 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 15:51:44.321485 140200172709696 submission_runner.py:376] Time since start: 8690.35s, 	Step: 14103, 	{'train/accuracy': 0.6291584968566895, 'train/loss': 1.821905255317688, 'train/bleu': 30.272824576136717, 'validation/accuracy': 0.6436870098114014, 'validation/loss': 1.7103956937789917, 'validation/bleu': 26.90387007376632, 'validation/num_examples': 3000, 'test/accuracy': 0.6522805094718933, 'test/loss': 1.6426098346710205, 'test/bleu': 26.18302709510793, 'test/num_examples': 3003, 'score': 5086.39458823204, 'total_duration': 8690.345397233963, 'accumulated_submission_time': 5086.39458823204, 'accumulated_eval_time': 3603.565372943878, 'accumulated_logging_time': 0.17497968673706055}
I0915 15:51:44.337989 140023658383104 logging_writer.py:48] [14103] accumulated_eval_time=3603.565373, accumulated_logging_time=0.174980, accumulated_submission_time=5086.394588, global_step=14103, preemption_count=0, score=5086.394588, test/accuracy=0.652281, test/bleu=26.183027, test/loss=1.642610, test/num_examples=3003, total_duration=8690.345397, train/accuracy=0.629158, train/bleu=30.272825, train/loss=1.821905, validation/accuracy=0.643687, validation/bleu=26.903870, validation/loss=1.710396, validation/num_examples=3000
I0915 15:54:06.537917 140023666775808 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.16562004387378693, loss=1.9864964485168457
I0915 15:57:05.167132 140023658383104 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18699334561824799, loss=1.86296546459198
I0915 16:00:03.841842 140023666775808 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18784181773662567, loss=1.9131275415420532
I0915 16:03:02.500595 140023658383104 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.18267957866191864, loss=1.8416353464126587
I0915 16:05:44.460299 140200172709696 spec.py:320] Evaluating on the training split.
I0915 16:05:47.450496 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:08:45.929923 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 16:08:48.587732 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:11:23.609526 140200172709696 spec.py:348] Evaluating on the test split.
I0915 16:11:26.315821 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:13:56.411260 140200172709696 submission_runner.py:376] Time since start: 10022.44s, 	Step: 16455, 	{'train/accuracy': 0.6286917924880981, 'train/loss': 1.8241031169891357, 'train/bleu': 30.32994731026418, 'validation/accuracy': 0.6472951173782349, 'validation/loss': 1.678662896156311, 'validation/bleu': 27.3464595466423, 'validation/num_examples': 3000, 'test/accuracy': 0.657184362411499, 'test/loss': 1.6122846603393555, 'test/bleu': 26.652709537763407, 'test/num_examples': 3003, 'score': 5926.471277952194, 'total_duration': 10022.435180425644, 'accumulated_submission_time': 5926.471277952194, 'accumulated_eval_time': 4095.5162653923035, 'accumulated_logging_time': 0.20159125328063965}
I0915 16:13:56.428314 140023666775808 logging_writer.py:48] [16455] accumulated_eval_time=4095.516265, accumulated_logging_time=0.201591, accumulated_submission_time=5926.471278, global_step=16455, preemption_count=0, score=5926.471278, test/accuracy=0.657184, test/bleu=26.652710, test/loss=1.612285, test/num_examples=3003, total_duration=10022.435180, train/accuracy=0.628692, train/bleu=30.329947, train/loss=1.824103, validation/accuracy=0.647295, validation/bleu=27.346460, validation/loss=1.678663, validation/num_examples=3000
I0915 16:14:12.894151 140023658383104 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.19957761466503143, loss=1.7923582792282104
I0915 16:17:11.536007 140023666775808 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.1655440330505371, loss=1.9421498775482178
I0915 16:20:10.143756 140023658383104 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.21080903708934784, loss=1.9500893354415894
I0915 16:23:08.703679 140023666775808 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.185606449842453, loss=1.8989447355270386
I0915 16:26:07.461042 140023658383104 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.17509503662586212, loss=1.7783764600753784
I0915 16:27:56.466588 140200172709696 spec.py:320] Evaluating on the training split.
I0915 16:27:59.456065 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:30:43.864068 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 16:30:46.505459 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:33:02.502619 140200172709696 spec.py:348] Evaluating on the test split.
I0915 16:33:05.209502 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:35:17.154452 140200172709696 submission_runner.py:376] Time since start: 11303.18s, 	Step: 18807, 	{'train/accuracy': 0.6742820143699646, 'train/loss': 1.5166453123092651, 'train/bleu': 33.3908808933066, 'validation/accuracy': 0.6507917046546936, 'validation/loss': 1.6511542797088623, 'validation/bleu': 27.81718379995774, 'validation/num_examples': 3000, 'test/accuracy': 0.6619836091995239, 'test/loss': 1.5844849348068237, 'test/bleu': 26.802111027370753, 'test/num_examples': 3003, 'score': 6766.463502407074, 'total_duration': 11303.178382635117, 'accumulated_submission_time': 6766.463502407074, 'accumulated_eval_time': 4536.204072237015, 'accumulated_logging_time': 0.2286224365234375}
I0915 16:35:17.171228 140023666775808 logging_writer.py:48] [18807] accumulated_eval_time=4536.204072, accumulated_logging_time=0.228622, accumulated_submission_time=6766.463502, global_step=18807, preemption_count=0, score=6766.463502, test/accuracy=0.661984, test/bleu=26.802111, test/loss=1.584485, test/num_examples=3003, total_duration=11303.178383, train/accuracy=0.674282, train/bleu=33.390881, train/loss=1.516645, validation/accuracy=0.650792, validation/bleu=27.817184, validation/loss=1.651154, validation/num_examples=3000
I0915 16:36:26.474932 140023658383104 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.18688277900218964, loss=1.7899653911590576
I0915 16:39:25.146043 140023666775808 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.19617173075675964, loss=1.837404727935791
I0915 16:42:23.827245 140023658383104 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.19200293719768524, loss=1.8599953651428223
I0915 16:45:22.463582 140023666775808 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.18336233496665955, loss=1.8483954668045044
I0915 16:48:21.064463 140023658383104 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.18311059474945068, loss=1.8370932340621948
I0915 16:49:17.247890 140200172709696 spec.py:320] Evaluating on the training split.
I0915 16:49:20.241459 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:52:42.622683 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 16:52:45.272377 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:55:08.516958 140200172709696 spec.py:348] Evaluating on the test split.
I0915 16:55:11.232139 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 16:57:22.458734 140200172709696 submission_runner.py:376] Time since start: 12628.48s, 	Step: 21159, 	{'train/accuracy': 0.6358057260513306, 'train/loss': 1.7667934894561768, 'train/bleu': 30.812733623456655, 'validation/accuracy': 0.6530483365058899, 'validation/loss': 1.6341582536697388, 'validation/bleu': 27.44028540483933, 'validation/num_examples': 3000, 'test/accuracy': 0.6640985608100891, 'test/loss': 1.5679447650909424, 'test/bleu': 27.115582618045106, 'test/num_examples': 3003, 'score': 7606.495560407639, 'total_duration': 12628.482678890228, 'accumulated_submission_time': 7606.495560407639, 'accumulated_eval_time': 5021.414864778519, 'accumulated_logging_time': 0.2551116943359375}
I0915 16:57:22.475673 140023666775808 logging_writer.py:48] [21159] accumulated_eval_time=5021.414865, accumulated_logging_time=0.255112, accumulated_submission_time=7606.495560, global_step=21159, preemption_count=0, score=7606.495560, test/accuracy=0.664099, test/bleu=27.115583, test/loss=1.567945, test/num_examples=3003, total_duration=12628.482679, train/accuracy=0.635806, train/bleu=30.812734, train/loss=1.766793, validation/accuracy=0.653048, validation/bleu=27.440285, validation/loss=1.634158, validation/num_examples=3000
I0915 16:59:24.635030 140023658383104 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.1900751292705536, loss=1.9121471643447876
I0915 17:02:23.174395 140023666775808 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.19145311415195465, loss=1.840930461883545
I0915 17:05:21.775864 140023658383104 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.21400782465934753, loss=1.8873525857925415
I0915 17:08:20.335090 140023666775808 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.1710054576396942, loss=1.8634400367736816
I0915 17:11:18.984560 140023658383104 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.17952722311019897, loss=1.7868635654449463
I0915 17:11:22.643094 140200172709696 spec.py:320] Evaluating on the training split.
I0915 17:11:25.628716 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 17:14:58.196410 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 17:15:00.851489 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 17:17:19.775344 140200172709696 spec.py:348] Evaluating on the test split.
I0915 17:17:22.466851 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 17:19:31.885652 140200172709696 submission_runner.py:376] Time since start: 13957.91s, 	Step: 23512, 	{'train/accuracy': 0.6352537870407104, 'train/loss': 1.7718756198883057, 'train/bleu': 30.706208737651444, 'validation/accuracy': 0.6563217043876648, 'validation/loss': 1.6186645030975342, 'validation/bleu': 27.953983619837004, 'validation/num_examples': 3000, 'test/accuracy': 0.6672360897064209, 'test/loss': 1.5510153770446777, 'test/bleu': 27.049800648124933, 'test/num_examples': 3003, 'score': 8446.618193149567, 'total_duration': 13957.909576654434, 'accumulated_submission_time': 8446.618193149567, 'accumulated_eval_time': 5510.657336473465, 'accumulated_logging_time': 0.28184986114501953}
I0915 17:19:31.902744 140023666775808 logging_writer.py:48] [23512] accumulated_eval_time=5510.657336, accumulated_logging_time=0.281850, accumulated_submission_time=8446.618193, global_step=23512, preemption_count=0, score=8446.618193, test/accuracy=0.667236, test/bleu=27.049801, test/loss=1.551015, test/num_examples=3003, total_duration=13957.909577, train/accuracy=0.635254, train/bleu=30.706209, train/loss=1.771876, validation/accuracy=0.656322, validation/bleu=27.953984, validation/loss=1.618665, validation/num_examples=3000
I0915 17:22:26.590552 140023658383104 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.19419120252132416, loss=1.9415066242218018
I0915 17:25:25.241028 140023666775808 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.18247240781784058, loss=1.7778009176254272
I0915 17:28:23.933603 140023658383104 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.20935125648975372, loss=1.776050329208374
I0915 17:31:22.813912 140023666775808 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.1838539093732834, loss=1.7298654317855835
I0915 17:33:32.195681 140200172709696 spec.py:320] Evaluating on the training split.
I0915 17:33:35.181768 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 17:37:22.218826 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 17:37:24.870457 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 17:39:45.622589 140200172709696 spec.py:348] Evaluating on the test split.
I0915 17:39:48.319273 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 17:42:06.270839 140200172709696 submission_runner.py:376] Time since start: 15312.29s, 	Step: 25864, 	{'train/accuracy': 0.6477124691009521, 'train/loss': 1.6808216571807861, 'train/bleu': 31.74158602660301, 'validation/accuracy': 0.6586527228355408, 'validation/loss': 1.6096842288970947, 'validation/bleu': 28.152183940088808, 'validation/num_examples': 3000, 'test/accuracy': 0.6683748960494995, 'test/loss': 1.5386584997177124, 'test/bleu': 27.767962044249014, 'test/num_examples': 3003, 'score': 9286.86467552185, 'total_duration': 15312.294780015945, 'accumulated_submission_time': 9286.86467552185, 'accumulated_eval_time': 6024.732455253601, 'accumulated_logging_time': 0.3088691234588623}
I0915 17:42:06.287695 140023658383104 logging_writer.py:48] [25864] accumulated_eval_time=6024.732455, accumulated_logging_time=0.308869, accumulated_submission_time=9286.864676, global_step=25864, preemption_count=0, score=9286.864676, test/accuracy=0.668375, test/bleu=27.767962, test/loss=1.538658, test/num_examples=3003, total_duration=15312.294780, train/accuracy=0.647712, train/bleu=31.741586, train/loss=1.680822, validation/accuracy=0.658653, validation/bleu=28.152184, validation/loss=1.609684, validation/num_examples=3000
I0915 17:42:55.234306 140023666775808 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2176980823278427, loss=1.7967581748962402
I0915 17:45:54.077785 140023658383104 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.18512369692325592, loss=1.7876096963882446
I0915 17:48:52.712444 140023666775808 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.18738171458244324, loss=1.8251891136169434
I0915 17:51:51.373198 140023658383104 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.23523607850074768, loss=1.948931336402893
I0915 17:54:49.991612 140023666775808 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.2301032692193985, loss=1.8433833122253418
I0915 17:56:06.532758 140200172709696 spec.py:320] Evaluating on the training split.
I0915 17:56:09.538842 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:00:49.947913 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 18:00:52.605256 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:05:31.165218 140200172709696 spec.py:348] Evaluating on the test split.
I0915 18:05:33.885818 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:10:16.037202 140200172709696 submission_runner.py:376] Time since start: 17002.06s, 	Step: 28216, 	{'train/accuracy': 0.6431559324264526, 'train/loss': 1.7156288623809814, 'train/bleu': 31.15831597828048, 'validation/accuracy': 0.6597438454627991, 'validation/loss': 1.5974607467651367, 'validation/bleu': 27.93922404873176, 'validation/num_examples': 3000, 'test/accuracy': 0.67082679271698, 'test/loss': 1.5214282274246216, 'test/bleu': 27.47985343173673, 'test/num_examples': 3003, 'score': 10127.062792539597, 'total_duration': 17002.061151742935, 'accumulated_submission_time': 10127.062792539597, 'accumulated_eval_time': 6874.236850976944, 'accumulated_logging_time': 0.33684802055358887}
I0915 18:10:16.054470 140023658383104 logging_writer.py:48] [28216] accumulated_eval_time=6874.236851, accumulated_logging_time=0.336848, accumulated_submission_time=10127.062793, global_step=28216, preemption_count=0, score=10127.062793, test/accuracy=0.670827, test/bleu=27.479853, test/loss=1.521428, test/num_examples=3003, total_duration=17002.061152, train/accuracy=0.643156, train/bleu=31.158316, train/loss=1.715629, validation/accuracy=0.659744, validation/bleu=27.939224, validation/loss=1.597461, validation/num_examples=3000
I0915 18:11:57.871343 140023666775808 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.1960260272026062, loss=1.8560124635696411
I0915 18:14:56.490004 140023658383104 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.18144108355045319, loss=1.783031702041626
I0915 18:17:55.104970 140023666775808 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.449721097946167, loss=1.7276397943496704
I0915 18:20:53.853788 140023658383104 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2382316291332245, loss=1.7576559782028198
I0915 18:23:52.530439 140023666775808 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.19919079542160034, loss=1.8948593139648438
I0915 18:24:16.199611 140200172709696 spec.py:320] Evaluating on the training split.
I0915 18:24:19.192894 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:27:47.287088 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 18:27:49.929952 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:30:17.440536 140200172709696 spec.py:348] Evaluating on the test split.
I0915 18:30:20.144273 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:32:39.536563 140200172709696 submission_runner.py:376] Time since start: 18345.56s, 	Step: 30568, 	{'train/accuracy': 0.6407042741775513, 'train/loss': 1.7355269193649292, 'train/bleu': 31.736939869576855, 'validation/accuracy': 0.6623228192329407, 'validation/loss': 1.5837912559509277, 'validation/bleu': 28.12987130389086, 'validation/num_examples': 3000, 'test/accuracy': 0.6727790236473083, 'test/loss': 1.5093638896942139, 'test/bleu': 27.61588321451924, 'test/num_examples': 3003, 'score': 10967.162513256073, 'total_duration': 18345.560475587845, 'accumulated_submission_time': 10967.162513256073, 'accumulated_eval_time': 7377.573710203171, 'accumulated_logging_time': 0.3637700080871582}
I0915 18:32:39.557022 140023658383104 logging_writer.py:48] [30568] accumulated_eval_time=7377.573710, accumulated_logging_time=0.363770, accumulated_submission_time=10967.162513, global_step=30568, preemption_count=0, score=10967.162513, test/accuracy=0.672779, test/bleu=27.615883, test/loss=1.509364, test/num_examples=3003, total_duration=18345.560476, train/accuracy=0.640704, train/bleu=31.736940, train/loss=1.735527, validation/accuracy=0.662323, validation/bleu=28.129871, validation/loss=1.583791, validation/num_examples=3000
I0915 18:35:14.343508 140023666775808 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.1757565289735794, loss=1.8370983600616455
I0915 18:38:13.079536 140023658383104 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.208451509475708, loss=1.8064539432525635
I0915 18:41:11.728513 140023666775808 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.17625577747821808, loss=1.7351819276809692
I0915 18:44:10.366708 140023658383104 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.19749820232391357, loss=1.8494229316711426
I0915 18:46:39.807999 140200172709696 spec.py:320] Evaluating on the training split.
I0915 18:46:42.802175 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:50:41.431719 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 18:50:44.072206 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:53:24.765112 140200172709696 spec.py:348] Evaluating on the test split.
I0915 18:53:27.478799 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 18:56:24.490120 140200172709696 submission_runner.py:376] Time since start: 19770.51s, 	Step: 32920, 	{'train/accuracy': 0.6482195854187012, 'train/loss': 1.6781005859375, 'train/bleu': 31.291625033907437, 'validation/accuracy': 0.6596074104309082, 'validation/loss': 1.5918610095977783, 'validation/bleu': 28.307900253113104, 'validation/num_examples': 3000, 'test/accuracy': 0.6717215776443481, 'test/loss': 1.52046537399292, 'test/bleu': 27.438516771206867, 'test/num_examples': 3003, 'score': 11807.36433839798, 'total_duration': 19770.514050483704, 'accumulated_submission_time': 11807.36433839798, 'accumulated_eval_time': 7962.2557721138, 'accumulated_logging_time': 0.3961160182952881}
I0915 18:56:24.509716 140023666775808 logging_writer.py:48] [32920] accumulated_eval_time=7962.255772, accumulated_logging_time=0.396116, accumulated_submission_time=11807.364338, global_step=32920, preemption_count=0, score=11807.364338, test/accuracy=0.671722, test/bleu=27.438517, test/loss=1.520465, test/num_examples=3003, total_duration=19770.514050, train/accuracy=0.648220, train/bleu=31.291625, train/loss=1.678101, validation/accuracy=0.659607, validation/bleu=28.307900, validation/loss=1.591861, validation/num_examples=3000
I0915 18:56:53.471549 140023658383104 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.1894422471523285, loss=1.808512568473816
I0915 18:59:52.140548 140023666775808 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.19449260830879211, loss=1.833269715309143
I0915 19:02:50.771882 140023658383104 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1847822666168213, loss=1.7747026681900024
I0915 19:05:49.384513 140023666775808 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.18222332000732422, loss=1.7708638906478882
I0915 19:08:48.013630 140023658383104 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.177202045917511, loss=1.7823710441589355
I0915 19:10:24.581197 140200172709696 spec.py:320] Evaluating on the training split.
I0915 19:10:27.562474 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 19:13:57.729649 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 19:14:00.368241 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 19:16:41.383110 140200172709696 spec.py:348] Evaluating on the test split.
I0915 19:16:44.093798 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 19:19:29.293431 140200172709696 submission_runner.py:376] Time since start: 21155.32s, 	Step: 35272, 	{'train/accuracy': 0.6445931792259216, 'train/loss': 1.6923820972442627, 'train/bleu': 31.449931629767125, 'validation/accuracy': 0.6640711426734924, 'validation/loss': 1.566933035850525, 'validation/bleu': 28.530366820724332, 'validation/num_examples': 3000, 'test/accuracy': 0.6769391894340515, 'test/loss': 1.4909327030181885, 'test/bleu': 28.05892782595901, 'test/num_examples': 3003, 'score': 12647.389313459396, 'total_duration': 21155.317359924316, 'accumulated_submission_time': 12647.389313459396, 'accumulated_eval_time': 8506.967953205109, 'accumulated_logging_time': 0.42643117904663086}
I0915 19:19:29.311008 140023666775808 logging_writer.py:48] [35272] accumulated_eval_time=8506.967953, accumulated_logging_time=0.426431, accumulated_submission_time=12647.389313, global_step=35272, preemption_count=0, score=12647.389313, test/accuracy=0.676939, test/bleu=28.058928, test/loss=1.490933, test/num_examples=3003, total_duration=21155.317360, train/accuracy=0.644593, train/bleu=31.449932, train/loss=1.692382, validation/accuracy=0.664071, validation/bleu=28.530367, validation/loss=1.566933, validation/num_examples=3000
I0915 19:20:51.196184 140023658383104 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.18347905576229095, loss=1.8391783237457275
I0915 19:23:49.884665 140023666775808 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.17366711795330048, loss=1.7814600467681885
I0915 19:26:48.569049 140023658383104 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.1900712102651596, loss=1.80579674243927
I0915 19:29:47.257795 140023666775808 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.23607909679412842, loss=1.7800062894821167
I0915 19:32:45.848498 140023658383104 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.23397059738636017, loss=1.733823537826538
I0915 19:33:29.531824 140200172709696 spec.py:320] Evaluating on the training split.
I0915 19:33:32.533107 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 19:37:19.466985 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 19:37:22.115751 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 19:39:42.789931 140200172709696 spec.py:348] Evaluating on the test split.
I0915 19:39:45.474471 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 19:42:17.805907 140200172709696 submission_runner.py:376] Time since start: 22523.83s, 	Step: 37624, 	{'train/accuracy': 0.6766722202301025, 'train/loss': 1.4892700910568237, 'train/bleu': 33.74145405479451, 'validation/accuracy': 0.665571391582489, 'validation/loss': 1.552937388420105, 'validation/bleu': 28.49283971031092, 'validation/num_examples': 3000, 'test/accuracy': 0.678740382194519, 'test/loss': 1.4733619689941406, 'test/bleu': 28.221332307282264, 'test/num_examples': 3003, 'score': 13487.563034296036, 'total_duration': 22523.829854488373, 'accumulated_submission_time': 13487.563034296036, 'accumulated_eval_time': 9035.241974592209, 'accumulated_logging_time': 0.4555542469024658}
I0915 19:42:17.824237 140023666775808 logging_writer.py:48] [37624] accumulated_eval_time=9035.241975, accumulated_logging_time=0.455554, accumulated_submission_time=13487.563034, global_step=37624, preemption_count=0, score=13487.563034, test/accuracy=0.678740, test/bleu=28.221332, test/loss=1.473362, test/num_examples=3003, total_duration=22523.829854, train/accuracy=0.676672, train/bleu=33.741454, train/loss=1.489270, validation/accuracy=0.665571, validation/bleu=28.492840, validation/loss=1.552937, validation/num_examples=3000
I0915 19:44:32.565290 140023658383104 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.20080925524234772, loss=1.8772084712982178
I0915 19:47:31.346448 140023666775808 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.1997237205505371, loss=1.6981773376464844
I0915 19:50:30.220202 140023658383104 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.19611325860023499, loss=1.7477097511291504
I0915 19:53:28.855918 140023666775808 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.18719659745693207, loss=1.7801350355148315
I0915 19:56:18.021994 140200172709696 spec.py:320] Evaluating on the training split.
I0915 19:56:21.026710 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:00:38.262252 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 20:00:40.918450 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:03:18.344089 140200172709696 spec.py:348] Evaluating on the test split.
I0915 20:03:21.061022 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:05:42.582734 140200172709696 submission_runner.py:376] Time since start: 23928.61s, 	Step: 39975, 	{'train/accuracy': 0.6486798524856567, 'train/loss': 1.6690161228179932, 'train/bleu': 31.76872930189234, 'validation/accuracy': 0.6666129231452942, 'validation/loss': 1.5478057861328125, 'validation/bleu': 28.342506901149452, 'validation/num_examples': 3000, 'test/accuracy': 0.6792051792144775, 'test/loss': 1.4670010805130005, 'test/bleu': 28.275114585104728, 'test/num_examples': 3003, 'score': 14327.71306014061, 'total_duration': 23928.60667324066, 'accumulated_submission_time': 14327.71306014061, 'accumulated_eval_time': 9599.802678823471, 'accumulated_logging_time': 0.4839303493499756}
I0915 20:05:42.600608 140023658383104 logging_writer.py:48] [39975] accumulated_eval_time=9599.802679, accumulated_logging_time=0.483930, accumulated_submission_time=14327.713060, global_step=39975, preemption_count=0, score=14327.713060, test/accuracy=0.679205, test/bleu=28.275115, test/loss=1.467001, test/num_examples=3003, total_duration=23928.606673, train/accuracy=0.648680, train/bleu=31.768729, train/loss=1.669016, validation/accuracy=0.666613, validation/bleu=28.342507, validation/loss=1.547806, validation/num_examples=3000
I0915 20:05:51.902616 140023666775808 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.19965147972106934, loss=1.6704412698745728
I0915 20:08:50.600537 140023658383104 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.22649222612380981, loss=1.7660086154937744
I0915 20:11:49.293483 140023666775808 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.1881028115749359, loss=1.71548593044281
I0915 20:14:48.088454 140023658383104 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1895102709531784, loss=1.685325026512146
I0915 20:17:46.671747 140023666775808 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.24732834100723267, loss=1.7226744890213013
I0915 20:19:42.850063 140200172709696 spec.py:320] Evaluating on the training split.
I0915 20:19:45.844702 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:22:59.208107 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 20:23:01.858138 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:25:29.306312 140200172709696 spec.py:348] Evaluating on the test split.
I0915 20:25:32.021269 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:27:54.725692 140200172709696 submission_runner.py:376] Time since start: 25260.75s, 	Step: 42327, 	{'train/accuracy': 0.6474726796150208, 'train/loss': 1.6788183450698853, 'train/bleu': 31.87535373036659, 'validation/accuracy': 0.6694647073745728, 'validation/loss': 1.5372003316879272, 'validation/bleu': 28.96074352225166, 'validation/num_examples': 3000, 'test/accuracy': 0.6830283403396606, 'test/loss': 1.4495502710342407, 'test/bleu': 28.245123367880478, 'test/num_examples': 3003, 'score': 15167.914673089981, 'total_duration': 25260.749643325806, 'accumulated_submission_time': 15167.914673089981, 'accumulated_eval_time': 10091.67827796936, 'accumulated_logging_time': 0.5133004188537598}
I0915 20:27:54.743300 140023658383104 logging_writer.py:48] [42327] accumulated_eval_time=10091.678278, accumulated_logging_time=0.513300, accumulated_submission_time=15167.914673, global_step=42327, preemption_count=0, score=15167.914673, test/accuracy=0.683028, test/bleu=28.245123, test/loss=1.449550, test/num_examples=3003, total_duration=25260.749643, train/accuracy=0.647473, train/bleu=31.875354, train/loss=1.678818, validation/accuracy=0.669465, validation/bleu=28.960744, validation/loss=1.537200, validation/num_examples=3000
I0915 20:28:56.921119 140023666775808 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.17320209741592407, loss=1.6572957038879395
I0915 20:31:55.568237 140023658383104 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.21355758607387543, loss=1.7195526361465454
I0915 20:34:54.184070 140023666775808 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2486870288848877, loss=1.7202730178833008
I0915 20:37:52.826740 140023658383104 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.5741331577301025, loss=1.653975486755371
I0915 20:40:51.445554 140023666775808 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19972644746303558, loss=1.8055613040924072
I0915 20:41:54.739529 140200172709696 spec.py:320] Evaluating on the training split.
I0915 20:41:57.721902 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:46:12.806415 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 20:46:15.467030 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:49:02.982281 140200172709696 spec.py:348] Evaluating on the test split.
I0915 20:49:05.699494 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 20:51:40.037749 140200172709696 submission_runner.py:376] Time since start: 26686.06s, 	Step: 44679, 	{'train/accuracy': 0.6564565896987915, 'train/loss': 1.597154974937439, 'train/bleu': 32.23296806244103, 'validation/accuracy': 0.6703822612762451, 'validation/loss': 1.5270098447799683, 'validation/bleu': 28.823661584305334, 'validation/num_examples': 3000, 'test/accuracy': 0.6820754408836365, 'test/loss': 1.446740984916687, 'test/bleu': 28.525445100360116, 'test/num_examples': 3003, 'score': 16007.864673376083, 'total_duration': 26686.061677455902, 'accumulated_submission_time': 16007.864673376083, 'accumulated_eval_time': 10676.976427316666, 'accumulated_logging_time': 0.5415115356445312}
I0915 20:51:40.057945 140023658383104 logging_writer.py:48] [44679] accumulated_eval_time=10676.976427, accumulated_logging_time=0.541512, accumulated_submission_time=16007.864673, global_step=44679, preemption_count=0, score=16007.864673, test/accuracy=0.682075, test/bleu=28.525445, test/loss=1.446741, test/num_examples=3003, total_duration=26686.061677, train/accuracy=0.656457, train/bleu=32.232968, train/loss=1.597155, validation/accuracy=0.670382, validation/bleu=28.823662, validation/loss=1.527010, validation/num_examples=3000
I0915 20:53:35.157604 140023666775808 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.23057106137275696, loss=1.7616634368896484
I0915 20:56:33.890878 140023658383104 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.20277021825313568, loss=1.8230711221694946
I0915 20:59:32.610984 140023666775808 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.1910221129655838, loss=1.7084791660308838
I0915 21:02:31.284863 140023658383104 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1984521448612213, loss=1.8118600845336914
I0915 21:05:30.034826 140023666775808 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.1866075098514557, loss=1.6730417013168335
I0915 21:05:40.115531 140200172709696 spec.py:320] Evaluating on the training split.
I0915 21:05:43.110585 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:10:00.468494 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 21:10:03.117047 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:13:05.548471 140200172709696 spec.py:348] Evaluating on the test split.
I0915 21:13:08.241116 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:16:11.164240 140200172709696 submission_runner.py:376] Time since start: 28157.19s, 	Step: 47030, 	{'train/accuracy': 0.6522310972213745, 'train/loss': 1.6504321098327637, 'train/bleu': 32.629683239875604, 'validation/accuracy': 0.671808123588562, 'validation/loss': 1.5212384462356567, 'validation/bleu': 28.34424867325631, 'validation/num_examples': 3000, 'test/accuracy': 0.6866539120674133, 'test/loss': 1.4293789863586426, 'test/bleu': 28.93234883794651, 'test/num_examples': 3003, 'score': 16847.876141548157, 'total_duration': 28157.188164711, 'accumulated_submission_time': 16847.876141548157, 'accumulated_eval_time': 11308.02505350113, 'accumulated_logging_time': 0.5724122524261475}
I0915 21:16:11.184730 140023658383104 logging_writer.py:48] [47030] accumulated_eval_time=11308.025054, accumulated_logging_time=0.572412, accumulated_submission_time=16847.876142, global_step=47030, preemption_count=0, score=16847.876142, test/accuracy=0.686654, test/bleu=28.932349, test/loss=1.429379, test/num_examples=3003, total_duration=28157.188165, train/accuracy=0.652231, train/bleu=32.629683, train/loss=1.650432, validation/accuracy=0.671808, validation/bleu=28.344249, validation/loss=1.521238, validation/num_examples=3000
I0915 21:18:59.452998 140023666775808 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.21005560457706451, loss=1.6798715591430664
I0915 21:21:58.140638 140023658383104 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2244720458984375, loss=1.7599983215332031
I0915 21:24:56.800268 140023666775808 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.17205990850925446, loss=1.6238375902175903
I0915 21:27:55.410173 140023658383104 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.20260904729366302, loss=1.6680463552474976
I0915 21:30:11.309201 140200172709696 spec.py:320] Evaluating on the training split.
I0915 21:30:14.309850 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:34:51.794507 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 21:34:54.422486 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:37:31.996003 140200172709696 spec.py:348] Evaluating on the test split.
I0915 21:37:34.689340 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:40:16.642490 140200172709696 submission_runner.py:376] Time since start: 29602.67s, 	Step: 49382, 	{'train/accuracy': 0.6565055251121521, 'train/loss': 1.629167079925537, 'train/bleu': 31.98144433819645, 'validation/accuracy': 0.6723164916038513, 'validation/loss': 1.5097647905349731, 'validation/bleu': 29.13128336574811, 'validation/num_examples': 3000, 'test/accuracy': 0.6870025396347046, 'test/loss': 1.4261798858642578, 'test/bleu': 28.720813148139765, 'test/num_examples': 3003, 'score': 17687.95327615738, 'total_duration': 29602.666443824768, 'accumulated_submission_time': 17687.95327615738, 'accumulated_eval_time': 11913.358323812485, 'accumulated_logging_time': 0.6036539077758789}
I0915 21:40:16.660387 140023666775808 logging_writer.py:48] [49382] accumulated_eval_time=11913.358324, accumulated_logging_time=0.603654, accumulated_submission_time=17687.953276, global_step=49382, preemption_count=0, score=17687.953276, test/accuracy=0.687003, test/bleu=28.720813, test/loss=1.426180, test/num_examples=3003, total_duration=29602.666444, train/accuracy=0.656506, train/bleu=31.981444, train/loss=1.629167, validation/accuracy=0.672316, validation/bleu=29.131283, validation/loss=1.509765, validation/num_examples=3000
I0915 21:40:59.155013 140023658383104 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.19491472840309143, loss=1.7546712160110474
I0915 21:43:57.787377 140023666775808 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.20358075201511383, loss=1.671996831893921
I0915 21:46:56.473515 140023658383104 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21211810410022736, loss=1.7059745788574219
I0915 21:49:55.108824 140023666775808 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.18309471011161804, loss=1.6365762948989868
I0915 21:52:53.761463 140023658383104 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.22568173706531525, loss=1.724546194076538
I0915 21:54:16.729922 140200172709696 spec.py:320] Evaluating on the training split.
I0915 21:54:19.721276 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 21:58:11.805181 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 21:58:14.458594 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:00:46.951060 140200172709696 spec.py:348] Evaluating on the test split.
I0915 22:00:49.646209 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:03:25.926147 140200172709696 submission_runner.py:376] Time since start: 30991.95s, 	Step: 51734, 	{'train/accuracy': 0.662792444229126, 'train/loss': 1.5787047147750854, 'train/bleu': 32.682576293721304, 'validation/accuracy': 0.6716965436935425, 'validation/loss': 1.5070016384124756, 'validation/bleu': 28.89194143728194, 'validation/num_examples': 3000, 'test/accuracy': 0.6849108338356018, 'test/loss': 1.4270668029785156, 'test/bleu': 28.666265192643948, 'test/num_examples': 3003, 'score': 18527.97706103325, 'total_duration': 30991.950076818466, 'accumulated_submission_time': 18527.97706103325, 'accumulated_eval_time': 12462.5544860363, 'accumulated_logging_time': 0.6314728260040283}
I0915 22:03:25.946929 140023666775808 logging_writer.py:48] [51734] accumulated_eval_time=12462.554486, accumulated_logging_time=0.631473, accumulated_submission_time=18527.977061, global_step=51734, preemption_count=0, score=18527.977061, test/accuracy=0.684911, test/bleu=28.666265, test/loss=1.427067, test/num_examples=3003, total_duration=30991.950077, train/accuracy=0.662792, train/bleu=32.682576, train/loss=1.578705, validation/accuracy=0.671697, validation/bleu=28.891941, validation/loss=1.507002, validation/num_examples=3000
I0915 22:05:01.546559 140023658383104 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.1904287189245224, loss=1.628303050994873
I0915 22:08:00.256870 140023666775808 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.21626298129558563, loss=1.7390481233596802
I0915 22:10:59.077343 140023658383104 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.19573751091957092, loss=1.7941780090332031
I0915 22:13:57.730987 140023666775808 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.1937258243560791, loss=1.6856454610824585
I0915 22:16:56.416557 140023658383104 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.18415485322475433, loss=1.7176432609558105
I0915 22:17:26.149513 140200172709696 spec.py:320] Evaluating on the training split.
I0915 22:17:29.146264 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:21:25.392337 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 22:21:28.042692 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:24:10.463123 140200172709696 spec.py:348] Evaluating on the test split.
I0915 22:24:13.169712 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:27:06.912782 140200172709696 submission_runner.py:376] Time since start: 32412.94s, 	Step: 54085, 	{'train/accuracy': 0.6590363383293152, 'train/loss': 1.608697772026062, 'train/bleu': 32.559509901286184, 'validation/accuracy': 0.6745111346244812, 'validation/loss': 1.4913569688796997, 'validation/bleu': 29.32784994159188, 'validation/num_examples': 3000, 'test/accuracy': 0.6896635890007019, 'test/loss': 1.4021748304367065, 'test/bleu': 29.049958080078103, 'test/num_examples': 3003, 'score': 19368.131568193436, 'total_duration': 32412.93671989441, 'accumulated_submission_time': 19368.131568193436, 'accumulated_eval_time': 13043.317687273026, 'accumulated_logging_time': 0.6635177135467529}
I0915 22:27:06.932698 140023666775808 logging_writer.py:48] [54085] accumulated_eval_time=13043.317687, accumulated_logging_time=0.663518, accumulated_submission_time=19368.131568, global_step=54085, preemption_count=0, score=19368.131568, test/accuracy=0.689664, test/bleu=29.049958, test/loss=1.402175, test/num_examples=3003, total_duration=32412.936720, train/accuracy=0.659036, train/bleu=32.559510, train/loss=1.608698, validation/accuracy=0.674511, validation/bleu=29.327850, validation/loss=1.491357, validation/num_examples=3000
I0915 22:29:35.528596 140023658383104 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.19935061037540436, loss=1.6529333591461182
I0915 22:32:34.157319 140023666775808 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.18847745656967163, loss=1.6496193408966064
I0915 22:35:32.786014 140023658383104 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.19252225756645203, loss=1.7274681329727173
I0915 22:38:31.404954 140023666775808 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.228052020072937, loss=1.677312970161438
I0915 22:41:07.188381 140200172709696 spec.py:320] Evaluating on the training split.
I0915 22:41:10.181381 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:45:08.420797 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 22:45:11.059140 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:47:47.389871 140200172709696 spec.py:348] Evaluating on the test split.
I0915 22:47:50.095489 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 22:50:18.912194 140200172709696 submission_runner.py:376] Time since start: 33804.94s, 	Step: 56438, 	{'train/accuracy': 0.683269202709198, 'train/loss': 1.4353021383285522, 'train/bleu': 34.12081370506152, 'validation/accuracy': 0.6767181754112244, 'validation/loss': 1.4791769981384277, 'validation/bleu': 29.283518109006234, 'validation/num_examples': 3000, 'test/accuracy': 0.6906397342681885, 'test/loss': 1.3920460939407349, 'test/bleu': 28.965194170055046, 'test/num_examples': 3003, 'score': 20208.34093117714, 'total_duration': 33804.93611049652, 'accumulated_submission_time': 20208.34093117714, 'accumulated_eval_time': 13595.041430234909, 'accumulated_logging_time': 0.6939771175384521}
I0915 22:50:18.931062 140023658383104 logging_writer.py:48] [56438] accumulated_eval_time=13595.041430, accumulated_logging_time=0.693977, accumulated_submission_time=20208.340931, global_step=56438, preemption_count=0, score=20208.340931, test/accuracy=0.690640, test/bleu=28.965194, test/loss=1.392046, test/num_examples=3003, total_duration=33804.936110, train/accuracy=0.683269, train/bleu=34.120814, train/loss=1.435302, validation/accuracy=0.676718, validation/bleu=29.283518, validation/loss=1.479177, validation/num_examples=3000
I0915 22:50:41.445615 140023666775808 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.19272948801517487, loss=1.6158063411712646
I0915 22:53:40.109361 140023658383104 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.22702504694461823, loss=1.6819911003112793
I0915 22:56:38.719849 140023666775808 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.19970226287841797, loss=1.703757405281067
I0915 22:59:37.364231 140023658383104 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.18807440996170044, loss=1.5328609943389893
I0915 23:02:35.979785 140023666775808 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.18667474389076233, loss=1.6655287742614746
I0915 23:04:18.952354 140200172709696 spec.py:320] Evaluating on the training split.
I0915 23:04:21.939646 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:08:33.327826 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 23:08:35.972369 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:11:22.280444 140200172709696 spec.py:348] Evaluating on the test split.
I0915 23:11:24.981701 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:14:29.422435 140200172709696 submission_runner.py:376] Time since start: 35255.45s, 	Step: 58790, 	{'train/accuracy': 0.6633440256118774, 'train/loss': 1.5697834491729736, 'train/bleu': 32.70040078168986, 'validation/accuracy': 0.6774621605873108, 'validation/loss': 1.4731673002243042, 'validation/bleu': 29.36555143145377, 'validation/num_examples': 3000, 'test/accuracy': 0.691557765007019, 'test/loss': 1.3868361711502075, 'test/bleu': 29.23068514578597, 'test/num_examples': 3003, 'score': 21048.31526875496, 'total_duration': 35255.446363925934, 'accumulated_submission_time': 21048.31526875496, 'accumulated_eval_time': 14205.511446714401, 'accumulated_logging_time': 0.724341630935669}
I0915 23:14:29.441328 140023658383104 logging_writer.py:48] [58790] accumulated_eval_time=14205.511447, accumulated_logging_time=0.724342, accumulated_submission_time=21048.315269, global_step=58790, preemption_count=0, score=21048.315269, test/accuracy=0.691558, test/bleu=29.230685, test/loss=1.386836, test/num_examples=3003, total_duration=35255.446364, train/accuracy=0.663344, train/bleu=32.700401, train/loss=1.569783, validation/accuracy=0.677462, validation/bleu=29.365551, validation/loss=1.473167, validation/num_examples=3000
I0915 23:15:44.851947 140023666775808 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.1935468465089798, loss=1.6327240467071533
I0915 23:18:43.449458 140023658383104 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.1896219551563263, loss=1.6361110210418701
I0915 23:21:42.030985 140023666775808 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.21260590851306915, loss=1.6051197052001953
I0915 23:24:40.615330 140023658383104 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2122056782245636, loss=1.6601496934890747
I0915 23:27:39.329294 140023666775808 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.19242353737354279, loss=1.6890400648117065
I0915 23:28:29.741352 140200172709696 spec.py:320] Evaluating on the training split.
I0915 23:28:32.731838 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:32:52.056486 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 23:32:54.703945 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:35:35.035486 140200172709696 spec.py:348] Evaluating on the test split.
I0915 23:35:37.735591 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:38:20.279906 140200172709696 submission_runner.py:376] Time since start: 36686.30s, 	Step: 61143, 	{'train/accuracy': 0.6639869213104248, 'train/loss': 1.5631855726242065, 'train/bleu': 32.584606643198825, 'validation/accuracy': 0.6802147626876831, 'validation/loss': 1.4607304334640503, 'validation/bleu': 29.77535455886182, 'validation/num_examples': 3000, 'test/accuracy': 0.694311797618866, 'test/loss': 1.3735520839691162, 'test/bleu': 29.522124516555948, 'test/num_examples': 3003, 'score': 21888.56804585457, 'total_duration': 36686.303837537766, 'accumulated_submission_time': 21888.56804585457, 'accumulated_eval_time': 14796.049931287766, 'accumulated_logging_time': 0.7542929649353027}
I0915 23:38:20.298652 140023658383104 logging_writer.py:48] [61143] accumulated_eval_time=14796.049931, accumulated_logging_time=0.754293, accumulated_submission_time=21888.568046, global_step=61143, preemption_count=0, score=21888.568046, test/accuracy=0.694312, test/bleu=29.522125, test/loss=1.373552, test/num_examples=3003, total_duration=36686.303838, train/accuracy=0.663987, train/bleu=32.584607, train/loss=1.563186, validation/accuracy=0.680215, validation/bleu=29.775355, validation/loss=1.460730, validation/num_examples=3000
I0915 23:40:28.306363 140023666775808 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19546671211719513, loss=1.5050344467163086
I0915 23:43:26.879254 140023658383104 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.21154171228408813, loss=1.676874041557312
I0915 23:46:25.522433 140023666775808 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.18765118718147278, loss=1.5820324420928955
I0915 23:49:24.100950 140023658383104 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.20590005815029144, loss=1.6813585758209229
I0915 23:52:20.569139 140200172709696 spec.py:320] Evaluating on the training split.
I0915 23:52:23.559657 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:56:10.144376 140200172709696 spec.py:332] Evaluating on the validation split.
I0915 23:56:12.796955 140200172709696 workload.py:179] Translating evaluation dataset.
I0915 23:58:34.330833 140200172709696 spec.py:348] Evaluating on the test split.
I0915 23:58:37.036755 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:00:52.483792 140200172709696 submission_runner.py:376] Time since start: 38038.51s, 	Step: 63496, 	{'train/accuracy': 0.6726361513137817, 'train/loss': 1.5125588178634644, 'train/bleu': 33.887505490130884, 'validation/accuracy': 0.682223379611969, 'validation/loss': 1.4498581886291504, 'validation/bleu': 29.861537345406855, 'validation/num_examples': 3000, 'test/accuracy': 0.6964848041534424, 'test/loss': 1.362645149230957, 'test/bleu': 29.801053616014823, 'test/num_examples': 3003, 'score': 22728.792023181915, 'total_duration': 38038.50773406029, 'accumulated_submission_time': 22728.792023181915, 'accumulated_eval_time': 15307.964537382126, 'accumulated_logging_time': 0.7837402820587158}
I0916 00:00:52.502484 140023666775808 logging_writer.py:48] [63496] accumulated_eval_time=15307.964537, accumulated_logging_time=0.783740, accumulated_submission_time=22728.792023, global_step=63496, preemption_count=0, score=22728.792023, test/accuracy=0.696485, test/bleu=29.801054, test/loss=1.362645, test/num_examples=3003, total_duration=38038.507734, train/accuracy=0.672636, train/bleu=33.887505, train/loss=1.512559, validation/accuracy=0.682223, validation/bleu=29.861537, validation/loss=1.449858, validation/num_examples=3000
I0916 00:00:54.307407 140023658383104 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.18502795696258545, loss=1.6069597005844116
I0916 00:03:52.853509 140023666775808 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.20815202593803406, loss=1.635054111480713
I0916 00:06:51.557133 140023658383104 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2055221050977707, loss=1.7422016859054565
I0916 00:09:50.240044 140023666775808 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.18386970460414886, loss=1.574514389038086
I0916 00:12:49.030486 140023658383104 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.19814035296440125, loss=1.679875135421753
I0916 00:14:52.663127 140200172709696 spec.py:320] Evaluating on the training split.
I0916 00:14:55.664283 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:19:01.904839 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 00:19:04.563308 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:21:48.902968 140200172709696 spec.py:348] Evaluating on the test split.
I0916 00:21:51.626832 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:24:33.075003 140200172709696 submission_runner.py:376] Time since start: 39459.10s, 	Step: 65848, 	{'train/accuracy': 0.6686924695968628, 'train/loss': 1.531067132949829, 'train/bleu': 33.59518280344375, 'validation/accuracy': 0.6813926696777344, 'validation/loss': 1.440827488899231, 'validation/bleu': 29.601585956948888, 'validation/num_examples': 3000, 'test/accuracy': 0.6982743740081787, 'test/loss': 1.3472319841384888, 'test/bleu': 29.776164290615625, 'test/num_examples': 3003, 'score': 23568.90534734726, 'total_duration': 39459.09894824028, 'accumulated_submission_time': 23568.90534734726, 'accumulated_eval_time': 15888.376370191574, 'accumulated_logging_time': 0.812269926071167}
I0916 00:24:33.094150 140023666775808 logging_writer.py:48] [65848] accumulated_eval_time=15888.376370, accumulated_logging_time=0.812270, accumulated_submission_time=23568.905347, global_step=65848, preemption_count=0, score=23568.905347, test/accuracy=0.698274, test/bleu=29.776164, test/loss=1.347232, test/num_examples=3003, total_duration=39459.098948, train/accuracy=0.668692, train/bleu=33.595183, train/loss=1.531067, validation/accuracy=0.681393, validation/bleu=29.601586, validation/loss=1.440827, validation/num_examples=3000
I0916 00:25:27.716449 140023658383104 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19486528635025024, loss=1.5388267040252686
I0916 00:28:26.353687 140023666775808 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2283467799425125, loss=1.5742274522781372
I0916 00:31:24.967922 140023658383104 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.21409747004508972, loss=1.6443535089492798
I0916 00:34:23.526511 140023666775808 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.22071950137615204, loss=1.5871939659118652
I0916 00:37:22.327287 140023658383104 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2042372077703476, loss=1.5098479986190796
I0916 00:38:33.153505 140200172709696 spec.py:320] Evaluating on the training split.
I0916 00:38:36.148153 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:43:03.067358 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 00:43:05.712623 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:45:50.627472 140200172709696 spec.py:348] Evaluating on the test split.
I0916 00:45:53.334170 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 00:48:11.942257 140200172709696 submission_runner.py:376] Time since start: 40877.97s, 	Step: 68200, 	{'train/accuracy': 0.6689242720603943, 'train/loss': 1.5433770418167114, 'train/bleu': 33.45396394498059, 'validation/accuracy': 0.6838352680206299, 'validation/loss': 1.4317246675491333, 'validation/bleu': 29.673474661236817, 'validation/num_examples': 3000, 'test/accuracy': 0.7009587287902832, 'test/loss': 1.3379542827606201, 'test/bleu': 29.967740918292552, 'test/num_examples': 3003, 'score': 24408.918486356735, 'total_duration': 40877.966178417206, 'accumulated_submission_time': 24408.918486356735, 'accumulated_eval_time': 16467.165045022964, 'accumulated_logging_time': 0.8411040306091309}
I0916 00:48:11.963317 140023666775808 logging_writer.py:48] [68200] accumulated_eval_time=16467.165045, accumulated_logging_time=0.841104, accumulated_submission_time=24408.918486, global_step=68200, preemption_count=0, score=24408.918486, test/accuracy=0.700959, test/bleu=29.967741, test/loss=1.337954, test/num_examples=3003, total_duration=40877.966178, train/accuracy=0.668924, train/bleu=33.453964, train/loss=1.543377, validation/accuracy=0.683835, validation/bleu=29.673475, validation/loss=1.431725, validation/num_examples=3000
I0916 00:49:59.662651 140023658383104 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.227984219789505, loss=1.613369107246399
I0916 00:52:58.268631 140023666775808 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.19702085852622986, loss=1.6609262228012085
I0916 00:55:56.934937 140023658383104 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.20583005249500275, loss=1.6122057437896729
I0916 00:58:55.606327 140023666775808 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.20650367438793182, loss=1.5685933828353882
I0916 01:01:54.306477 140023658383104 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.20912140607833862, loss=1.5615640878677368
I0916 01:02:12.275900 140200172709696 spec.py:320] Evaluating on the training split.
I0916 01:02:15.269984 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:06:09.349104 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 01:06:11.999550 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:08:38.416845 140200172709696 spec.py:348] Evaluating on the test split.
I0916 01:08:41.132724 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:11:11.110802 140200172709696 submission_runner.py:376] Time since start: 42257.13s, 	Step: 70552, 	{'train/accuracy': 0.6784619688987732, 'train/loss': 1.4757046699523926, 'train/bleu': 33.838587925956006, 'validation/accuracy': 0.6847652196884155, 'validation/loss': 1.4239221811294556, 'validation/bleu': 29.906966293141938, 'validation/num_examples': 3000, 'test/accuracy': 0.7004590034484863, 'test/loss': 1.33260977268219, 'test/bleu': 29.894544787017473, 'test/num_examples': 3003, 'score': 25249.183546066284, 'total_duration': 42257.13475418091, 'accumulated_submission_time': 25249.183546066284, 'accumulated_eval_time': 17005.999893188477, 'accumulated_logging_time': 0.8733737468719482}
I0916 01:11:11.130093 140023666775808 logging_writer.py:48] [70552] accumulated_eval_time=17005.999893, accumulated_logging_time=0.873374, accumulated_submission_time=25249.183546, global_step=70552, preemption_count=0, score=25249.183546, test/accuracy=0.700459, test/bleu=29.894545, test/loss=1.332610, test/num_examples=3003, total_duration=42257.134754, train/accuracy=0.678462, train/bleu=33.838588, train/loss=1.475705, validation/accuracy=0.684765, validation/bleu=29.906966, validation/loss=1.423922, validation/num_examples=3000
I0916 01:13:51.508839 140023658383104 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.2191982865333557, loss=1.506537675857544
I0916 01:16:50.214709 140023666775808 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.20879387855529785, loss=1.5845743417739868
I0916 01:19:49.011430 140023658383104 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.20820128917694092, loss=1.565851092338562
I0916 01:22:47.676733 140023666775808 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.19569262862205505, loss=1.6033378839492798
I0916 01:25:11.396771 140200172709696 spec.py:320] Evaluating on the training split.
I0916 01:25:14.396354 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:29:14.580205 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 01:29:17.220065 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:31:54.077371 140200172709696 spec.py:348] Evaluating on the test split.
I0916 01:31:56.780533 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:34:30.099449 140200172709696 submission_runner.py:376] Time since start: 43656.12s, 	Step: 72904, 	{'train/accuracy': 0.6751803755760193, 'train/loss': 1.5031684637069702, 'train/bleu': 33.85136010510074, 'validation/accuracy': 0.6861786842346191, 'validation/loss': 1.4201096296310425, 'validation/bleu': 30.25119936945212, 'validation/num_examples': 3000, 'test/accuracy': 0.7040846347808838, 'test/loss': 1.3219733238220215, 'test/bleu': 30.031453147472963, 'test/num_examples': 3003, 'score': 26089.403928756714, 'total_duration': 43656.123386621475, 'accumulated_submission_time': 26089.403928756714, 'accumulated_eval_time': 17564.70252108574, 'accumulated_logging_time': 0.9026870727539062}
I0916 01:34:30.118952 140023658383104 logging_writer.py:48] [72904] accumulated_eval_time=17564.702521, accumulated_logging_time=0.902687, accumulated_submission_time=26089.403929, global_step=72904, preemption_count=0, score=26089.403929, test/accuracy=0.704085, test/bleu=30.031453, test/loss=1.321973, test/num_examples=3003, total_duration=43656.123387, train/accuracy=0.675180, train/bleu=33.851360, train/loss=1.503168, validation/accuracy=0.686179, validation/bleu=30.251199, validation/loss=1.420110, validation/num_examples=3000
I0916 01:35:04.772073 140023666775808 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.21577151119709015, loss=1.5695836544036865
I0916 01:38:03.484254 140023658383104 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.21135860681533813, loss=1.55322265625
I0916 01:41:02.151967 140023666775808 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20539061725139618, loss=1.5500376224517822
I0916 01:44:00.808254 140023658383104 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.2034287005662918, loss=1.545677661895752
I0916 01:46:59.459366 140023666775808 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.1985333412885666, loss=1.5309510231018066
I0916 01:48:30.282303 140200172709696 spec.py:320] Evaluating on the training split.
I0916 01:48:33.285963 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:52:47.531934 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 01:52:50.177949 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:55:24.257358 140200172709696 spec.py:348] Evaluating on the test split.
I0916 01:55:26.959173 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 01:57:57.478721 140200172709696 submission_runner.py:376] Time since start: 45063.50s, 	Step: 75256, 	{'train/accuracy': 0.6910349130630493, 'train/loss': 1.4020071029663086, 'train/bleu': 34.927196110023715, 'validation/accuracy': 0.6873194575309753, 'validation/loss': 1.41226065158844, 'validation/bleu': 30.188396800787512, 'validation/num_examples': 3000, 'test/accuracy': 0.7040032744407654, 'test/loss': 1.3139973878860474, 'test/bleu': 29.86310908102282, 'test/num_examples': 3003, 'score': 26929.519035577774, 'total_duration': 45063.5026140213, 'accumulated_submission_time': 26929.519035577774, 'accumulated_eval_time': 18131.898837804794, 'accumulated_logging_time': 0.933844804763794}
I0916 01:57:57.499166 140023658383104 logging_writer.py:48] [75256] accumulated_eval_time=18131.898838, accumulated_logging_time=0.933845, accumulated_submission_time=26929.519036, global_step=75256, preemption_count=0, score=26929.519036, test/accuracy=0.704003, test/bleu=29.863109, test/loss=1.313997, test/num_examples=3003, total_duration=45063.502614, train/accuracy=0.691035, train/bleu=34.927196, train/loss=1.402007, validation/accuracy=0.687319, validation/bleu=30.188397, validation/loss=1.412261, validation/num_examples=3000
I0916 01:59:25.039873 140023666775808 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.20162326097488403, loss=1.5701932907104492
I0916 02:02:23.853922 140023658383104 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20509527623653412, loss=1.595990538597107
I0916 02:05:22.589550 140023666775808 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.21867205202579498, loss=1.574485421180725
I0916 02:08:21.415633 140023658383104 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.20629779994487762, loss=1.5536389350891113
I0916 02:11:20.246067 140023666775808 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.20756521821022034, loss=1.5164048671722412
I0916 02:11:57.500984 140200172709696 spec.py:320] Evaluating on the training split.
I0916 02:12:00.493778 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 02:16:25.109480 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 02:16:27.744665 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 02:19:24.712475 140200172709696 spec.py:348] Evaluating on the test split.
I0916 02:19:27.405415 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 02:22:12.926481 140200172709696 submission_runner.py:376] Time since start: 46518.95s, 	Step: 77606, 	{'train/accuracy': 0.6816911101341248, 'train/loss': 1.4577269554138184, 'train/bleu': 34.16135647052811, 'validation/accuracy': 0.6901092529296875, 'validation/loss': 1.401729702949524, 'validation/bleu': 30.25529899733195, 'validation/num_examples': 3000, 'test/accuracy': 0.7070245742797852, 'test/loss': 1.3021745681762695, 'test/bleu': 30.381383254530427, 'test/num_examples': 3003, 'score': 27769.473219394684, 'total_duration': 46518.950401067734, 'accumulated_submission_time': 27769.473219394684, 'accumulated_eval_time': 18747.324250936508, 'accumulated_logging_time': 0.9653325080871582}
I0916 02:22:12.948873 140023658383104 logging_writer.py:48] [77606] accumulated_eval_time=18747.324251, accumulated_logging_time=0.965333, accumulated_submission_time=27769.473219, global_step=77606, preemption_count=0, score=27769.473219, test/accuracy=0.707025, test/bleu=30.381383, test/loss=1.302175, test/num_examples=3003, total_duration=46518.950401, train/accuracy=0.681691, train/bleu=34.161356, train/loss=1.457727, validation/accuracy=0.690109, validation/bleu=30.255299, validation/loss=1.401730, validation/num_examples=3000
I0916 02:24:34.215321 140023666775808 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.23108239471912384, loss=1.5906262397766113
I0916 02:27:32.979244 140023658383104 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2098962664604187, loss=1.5202713012695312
I0916 02:30:31.882024 140023666775808 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21175844967365265, loss=1.5514425039291382
I0916 02:33:30.665280 140023658383104 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.21015098690986633, loss=1.5972719192504883
I0916 02:36:13.021946 140200172709696 spec.py:320] Evaluating on the training split.
I0916 02:36:16.009311 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 02:40:53.599452 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 02:40:56.267907 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 02:43:58.768557 140200172709696 spec.py:348] Evaluating on the test split.
I0916 02:44:01.472389 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 02:47:08.094569 140200172709696 submission_runner.py:376] Time since start: 48014.12s, 	Step: 79956, 	{'train/accuracy': 0.6826333403587341, 'train/loss': 1.453797698020935, 'train/bleu': 34.182064548347434, 'validation/accuracy': 0.6910887360572815, 'validation/loss': 1.392814040184021, 'validation/bleu': 30.48268493849697, 'validation/num_examples': 3000, 'test/accuracy': 0.7065830230712891, 'test/loss': 1.2955222129821777, 'test/bleu': 30.508525449054453, 'test/num_examples': 3003, 'score': 28609.496497154236, 'total_duration': 48014.11851024628, 'accumulated_submission_time': 28609.496497154236, 'accumulated_eval_time': 19402.396826267242, 'accumulated_logging_time': 0.9997520446777344}
I0916 02:47:08.114337 140023666775808 logging_writer.py:48] [79956] accumulated_eval_time=19402.396826, accumulated_logging_time=0.999752, accumulated_submission_time=28609.496497, global_step=79956, preemption_count=0, score=28609.496497, test/accuracy=0.706583, test/bleu=30.508525, test/loss=1.295522, test/num_examples=3003, total_duration=48014.118510, train/accuracy=0.682633, train/bleu=34.182065, train/loss=1.453798, validation/accuracy=0.691089, validation/bleu=30.482685, validation/loss=1.392814, validation/num_examples=3000
I0916 02:47:24.211796 140023658383104 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2102775275707245, loss=1.5378164052963257
I0916 02:50:22.847100 140023666775808 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.20519553124904633, loss=1.4867970943450928
I0916 02:53:21.650184 140023658383104 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.22212278842926025, loss=1.4893689155578613
I0916 02:56:20.381122 140023666775808 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.22263039648532867, loss=1.5029340982437134
I0916 02:59:19.092466 140023658383104 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.22294454276561737, loss=1.5312658548355103
I0916 03:01:08.267724 140200172709696 spec.py:320] Evaluating on the training split.
I0916 03:01:11.262871 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:05:35.844298 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 03:05:38.502871 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:08:30.873801 140200172709696 spec.py:348] Evaluating on the test split.
I0916 03:08:33.576010 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:10:58.170505 140200172709696 submission_runner.py:376] Time since start: 49444.19s, 	Step: 82307, 	{'train/accuracy': 0.6881935000419617, 'train/loss': 1.4140995740890503, 'train/bleu': 35.20752929388257, 'validation/accuracy': 0.6915723085403442, 'validation/loss': 1.3875857591629028, 'validation/bleu': 30.41800441228426, 'validation/num_examples': 3000, 'test/accuracy': 0.7078612446784973, 'test/loss': 1.2876874208450317, 'test/bleu': 30.58130553757828, 'test/num_examples': 3003, 'score': 29449.603117465973, 'total_duration': 49444.19443368912, 'accumulated_submission_time': 29449.603117465973, 'accumulated_eval_time': 19992.299550294876, 'accumulated_logging_time': 1.0306241512298584}
I0916 03:10:58.190561 140023666775808 logging_writer.py:48] [82307] accumulated_eval_time=19992.299550, accumulated_logging_time=1.030624, accumulated_submission_time=29449.603117, global_step=82307, preemption_count=0, score=29449.603117, test/accuracy=0.707861, test/bleu=30.581306, test/loss=1.287687, test/num_examples=3003, total_duration=49444.194434, train/accuracy=0.688194, train/bleu=35.207529, train/loss=1.414100, validation/accuracy=0.691572, validation/bleu=30.418004, validation/loss=1.387586, validation/num_examples=3000
I0916 03:12:07.498069 140023658383104 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.21391065418720245, loss=1.5747984647750854
I0916 03:15:06.152670 140023666775808 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.22018392384052277, loss=1.5038683414459229
I0916 03:18:04.919769 140023658383104 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.21840642392635345, loss=1.4795866012573242
I0916 03:21:03.613412 140023666775808 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21465818583965302, loss=1.5459446907043457
I0916 03:24:02.279099 140023658383104 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.2172047644853592, loss=1.4887733459472656
I0916 03:24:58.454404 140200172709696 spec.py:320] Evaluating on the training split.
I0916 03:25:01.445200 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:29:33.122766 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 03:29:35.767767 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:32:28.365591 140200172709696 spec.py:348] Evaluating on the test split.
I0916 03:32:31.064598 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:35:37.674597 140200172709696 submission_runner.py:376] Time since start: 50923.70s, 	Step: 84659, 	{'train/accuracy': 0.6858822703361511, 'train/loss': 1.431738257408142, 'train/bleu': 34.84152757725994, 'validation/accuracy': 0.6918451189994812, 'validation/loss': 1.3819169998168945, 'validation/bleu': 30.552772328293962, 'validation/num_examples': 3000, 'test/accuracy': 0.7091162800788879, 'test/loss': 1.2818939685821533, 'test/bleu': 30.6324119297336, 'test/num_examples': 3003, 'score': 30289.819618701935, 'total_duration': 50923.69852876663, 'accumulated_submission_time': 30289.819618701935, 'accumulated_eval_time': 20631.51967906952, 'accumulated_logging_time': 1.0619540214538574}
I0916 03:35:37.695462 140023666775808 logging_writer.py:48] [84659] accumulated_eval_time=20631.519679, accumulated_logging_time=1.061954, accumulated_submission_time=30289.819619, global_step=84659, preemption_count=0, score=30289.819619, test/accuracy=0.709116, test/bleu=30.632412, test/loss=1.281894, test/num_examples=3003, total_duration=50923.698529, train/accuracy=0.685882, train/bleu=34.841528, train/loss=1.431738, validation/accuracy=0.691845, validation/bleu=30.552772, validation/loss=1.381917, validation/num_examples=3000
I0916 03:37:39.964354 140023658383104 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.34855788946151733, loss=1.4796093702316284
I0916 03:40:38.790763 140023666775808 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.21217641234397888, loss=1.475509524345398
I0916 03:43:37.678859 140023658383104 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.23733538389205933, loss=1.459104061126709
I0916 03:46:36.382426 140023666775808 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2306162714958191, loss=1.4873305559158325
I0916 03:49:35.119170 140023658383104 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2060311734676361, loss=1.5130621194839478
I0916 03:49:37.711717 140200172709696 spec.py:320] Evaluating on the training split.
I0916 03:49:40.721064 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:53:58.355995 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 03:54:01.015848 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:56:44.995364 140200172709696 spec.py:348] Evaluating on the test split.
I0916 03:56:47.702192 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 03:59:03.358563 140200172709696 submission_runner.py:376] Time since start: 52329.38s, 	Step: 87009, 	{'train/accuracy': 0.6864525675773621, 'train/loss': 1.4331843852996826, 'train/bleu': 35.53816882244098, 'validation/accuracy': 0.6934570074081421, 'validation/loss': 1.3768035173416138, 'validation/bleu': 30.825226734244094, 'validation/num_examples': 3000, 'test/accuracy': 0.7111731171607971, 'test/loss': 1.2767939567565918, 'test/bleu': 30.776100815282874, 'test/num_examples': 3003, 'score': 31129.788384199142, 'total_duration': 52329.38246154785, 'accumulated_submission_time': 31129.788384199142, 'accumulated_eval_time': 21197.166420698166, 'accumulated_logging_time': 1.0935418605804443}
I0916 03:59:03.383696 140023666775808 logging_writer.py:48] [87009] accumulated_eval_time=21197.166421, accumulated_logging_time=1.093542, accumulated_submission_time=31129.788384, global_step=87009, preemption_count=0, score=31129.788384, test/accuracy=0.711173, test/bleu=30.776101, test/loss=1.276794, test/num_examples=3003, total_duration=52329.382462, train/accuracy=0.686453, train/bleu=35.538169, train/loss=1.433184, validation/accuracy=0.693457, validation/bleu=30.825227, validation/loss=1.376804, validation/num_examples=3000
I0916 04:01:59.202770 140023658383104 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.21967309713363647, loss=1.4671331644058228
I0916 04:04:57.964936 140023666775808 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.22348327934741974, loss=1.512518286705017
I0916 04:07:56.665938 140023658383104 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.22021327912807465, loss=1.481695532798767
I0916 04:10:55.370376 140023666775808 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21746844053268433, loss=1.489112138748169
I0916 04:13:03.526469 140200172709696 spec.py:320] Evaluating on the training split.
I0916 04:13:06.537354 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 04:17:09.945455 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 04:17:12.578257 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 04:19:44.200700 140200172709696 spec.py:348] Evaluating on the test split.
I0916 04:19:46.903854 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 04:22:25.474669 140200172709696 submission_runner.py:376] Time since start: 53731.50s, 	Step: 89360, 	{'train/accuracy': 0.6890226602554321, 'train/loss': 1.4117536544799805, 'train/bleu': 35.242698152588, 'validation/accuracy': 0.694572925567627, 'validation/loss': 1.3744648694992065, 'validation/bleu': 30.707595011519864, 'validation/num_examples': 3000, 'test/accuracy': 0.7108826041221619, 'test/loss': 1.2732455730438232, 'test/bleu': 30.833011989298747, 'test/num_examples': 3003, 'score': 31969.884179115295, 'total_duration': 53731.49858379364, 'accumulated_submission_time': 31969.884179115295, 'accumulated_eval_time': 21759.11456155777, 'accumulated_logging_time': 1.1296253204345703}
I0916 04:22:25.496888 140023658383104 logging_writer.py:48] [89360] accumulated_eval_time=21759.114562, accumulated_logging_time=1.129625, accumulated_submission_time=31969.884179, global_step=89360, preemption_count=0, score=31969.884179, test/accuracy=0.710883, test/bleu=30.833012, test/loss=1.273246, test/num_examples=3003, total_duration=53731.498584, train/accuracy=0.689023, train/bleu=35.242698, train/loss=1.411754, validation/accuracy=0.694573, validation/bleu=30.707595, validation/loss=1.374465, validation/num_examples=3000
I0916 04:23:15.869555 140023666775808 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.22051990032196045, loss=1.4655603170394897
I0916 04:26:14.599646 140023658383104 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.2230328470468521, loss=1.5062955617904663
I0916 04:29:13.313889 140023666775808 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.22285212576389313, loss=1.5073572397232056
I0916 04:32:12.018549 140023658383104 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.2292664349079132, loss=1.4619053602218628
I0916 04:35:10.681881 140023666775808 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.229023277759552, loss=1.5268141031265259
I0916 04:36:25.817853 140200172709696 spec.py:320] Evaluating on the training split.
I0916 04:36:28.807266 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 04:40:45.794354 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 04:40:48.449759 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 04:43:22.223351 140200172709696 spec.py:348] Evaluating on the test split.
I0916 04:43:24.943898 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 04:45:53.788959 140200172709696 submission_runner.py:376] Time since start: 55139.81s, 	Step: 91712, 	{'train/accuracy': 0.6906339526176453, 'train/loss': 1.4052921533584595, 'train/bleu': 35.31118387042799, 'validation/accuracy': 0.6946721076965332, 'validation/loss': 1.3729954957962036, 'validation/bleu': 30.747611253423468, 'validation/num_examples': 3000, 'test/accuracy': 0.7113590240478516, 'test/loss': 1.2718837261199951, 'test/bleu': 30.809581578825462, 'test/num_examples': 3003, 'score': 32810.15914392471, 'total_duration': 55139.81289434433, 'accumulated_submission_time': 32810.15914392471, 'accumulated_eval_time': 22327.08559536934, 'accumulated_logging_time': 1.1615445613861084}
I0916 04:45:53.809465 140023658383104 logging_writer.py:48] [91712] accumulated_eval_time=22327.085595, accumulated_logging_time=1.161545, accumulated_submission_time=32810.159144, global_step=91712, preemption_count=0, score=32810.159144, test/accuracy=0.711359, test/bleu=30.809582, test/loss=1.271884, test/num_examples=3003, total_duration=55139.812894, train/accuracy=0.690634, train/bleu=35.311184, train/loss=1.405292, validation/accuracy=0.694672, validation/bleu=30.747611, validation/loss=1.372995, validation/num_examples=3000
I0916 04:47:37.058947 140023666775808 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22798024117946625, loss=1.5154478549957275
I0916 04:50:35.718293 140023658383104 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.21398980915546417, loss=1.4019311666488647
I0916 04:53:34.406969 140023666775808 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21516184508800507, loss=1.4902056455612183
I0916 04:56:33.102108 140023658383104 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.21948228776454926, loss=1.4006685018539429
I0916 04:59:31.993124 140023666775808 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.22207117080688477, loss=1.508710503578186
I0916 04:59:53.862422 140200172709696 spec.py:320] Evaluating on the training split.
I0916 04:59:56.851137 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:04:14.127141 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 05:04:16.778590 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:06:55.507822 140200172709696 spec.py:348] Evaluating on the test split.
I0916 05:06:58.209452 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:09:26.040355 140200172709696 submission_runner.py:376] Time since start: 56552.06s, 	Step: 94063, 	{'train/accuracy': 0.694902241230011, 'train/loss': 1.3763175010681152, 'train/bleu': 35.190585179893944, 'validation/accuracy': 0.6945357322692871, 'validation/loss': 1.371010422706604, 'validation/bleu': 30.816372084215267, 'validation/num_examples': 3000, 'test/accuracy': 0.7119865417480469, 'test/loss': 1.2680238485336304, 'test/bleu': 30.871082398160198, 'test/num_examples': 3003, 'score': 33650.166264772415, 'total_duration': 56552.064269304276, 'accumulated_submission_time': 33650.166264772415, 'accumulated_eval_time': 22899.263434171677, 'accumulated_logging_time': 1.191739559173584}
I0916 05:09:26.063009 140023658383104 logging_writer.py:48] [94063] accumulated_eval_time=22899.263434, accumulated_logging_time=1.191740, accumulated_submission_time=33650.166265, global_step=94063, preemption_count=0, score=33650.166265, test/accuracy=0.711987, test/bleu=30.871082, test/loss=1.268024, test/num_examples=3003, total_duration=56552.064269, train/accuracy=0.694902, train/bleu=35.190585, train/loss=1.376318, validation/accuracy=0.694536, validation/bleu=30.816372, validation/loss=1.371010, validation/num_examples=3000
I0916 05:12:02.654013 140023666775808 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21903739869594574, loss=1.4790194034576416
I0916 05:15:01.330908 140023658383104 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.22368887066841125, loss=1.4992483854293823
I0916 05:17:59.981600 140023666775808 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.2264997959136963, loss=1.4454162120819092
I0916 05:20:58.720588 140023658383104 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.21417713165283203, loss=1.5437602996826172
I0916 05:23:26.344674 140200172709696 spec.py:320] Evaluating on the training split.
I0916 05:23:29.337047 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:27:54.441591 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 05:27:57.075503 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:30:36.680948 140200172709696 spec.py:348] Evaluating on the test split.
I0916 05:30:39.398460 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:33:10.409315 140200172709696 submission_runner.py:376] Time since start: 57976.43s, 	Step: 96415, 	{'train/accuracy': 0.6920124888420105, 'train/loss': 1.3959263563156128, 'train/bleu': 35.53895800749216, 'validation/accuracy': 0.694386899471283, 'validation/loss': 1.3700008392333984, 'validation/bleu': 30.7908717009102, 'validation/num_examples': 3000, 'test/accuracy': 0.7119865417480469, 'test/loss': 1.2670915126800537, 'test/bleu': 30.84459263110783, 'test/num_examples': 3003, 'score': 34490.401856184006, 'total_duration': 57976.433227300644, 'accumulated_submission_time': 34490.401856184006, 'accumulated_eval_time': 23483.3280107975, 'accumulated_logging_time': 1.2245352268218994}
I0916 05:33:10.432998 140023666775808 logging_writer.py:48] [96415] accumulated_eval_time=23483.328011, accumulated_logging_time=1.224535, accumulated_submission_time=34490.401856, global_step=96415, preemption_count=0, score=34490.401856, test/accuracy=0.711987, test/bleu=30.844593, test/loss=1.267092, test/num_examples=3003, total_duration=57976.433227, train/accuracy=0.692012, train/bleu=35.538958, train/loss=1.395926, validation/accuracy=0.694387, validation/bleu=30.790872, validation/loss=1.370001, validation/num_examples=3000
I0916 05:33:41.140570 140023658383104 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.2131262868642807, loss=1.485733985900879
I0916 05:36:39.800085 140023666775808 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.217209592461586, loss=1.4630229473114014
I0916 05:39:38.434768 140023658383104 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21922039985656738, loss=1.4941935539245605
I0916 05:42:37.087044 140023666775808 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.22190403938293457, loss=1.4828753471374512
I0916 05:45:35.939181 140023658383104 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21311375498771667, loss=1.4795873165130615
I0916 05:47:10.411884 140200172709696 spec.py:320] Evaluating on the training split.
I0916 05:47:13.404949 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:51:37.291024 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 05:51:39.940623 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:54:24.988805 140200172709696 spec.py:348] Evaluating on the test split.
I0916 05:54:27.693580 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 05:57:00.159388 140200172709696 submission_runner.py:376] Time since start: 59406.18s, 	Step: 98766, 	{'train/accuracy': 0.6938732862472534, 'train/loss': 1.3794183731079102, 'train/bleu': 35.519470369241354, 'validation/accuracy': 0.6945233345031738, 'validation/loss': 1.3695416450500488, 'validation/bleu': 30.787394565809937, 'validation/num_examples': 3000, 'test/accuracy': 0.7118819355964661, 'test/loss': 1.266464114189148, 'test/bleu': 30.882857452738662, 'test/num_examples': 3003, 'score': 35330.333010196686, 'total_duration': 59406.18332052231, 'accumulated_submission_time': 35330.333010196686, 'accumulated_eval_time': 24073.0754570961, 'accumulated_logging_time': 1.2589006423950195}
I0916 05:57:00.179937 140023666775808 logging_writer.py:48] [98766] accumulated_eval_time=24073.075457, accumulated_logging_time=1.258901, accumulated_submission_time=35330.333010, global_step=98766, preemption_count=0, score=35330.333010, test/accuracy=0.711882, test/bleu=30.882857, test/loss=1.266464, test/num_examples=3003, total_duration=59406.183321, train/accuracy=0.693873, train/bleu=35.519470, train/loss=1.379418, validation/accuracy=0.694523, validation/bleu=30.787395, validation/loss=1.369542, validation/num_examples=3000
I0916 05:58:24.105712 140023658383104 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.20927347242832184, loss=1.4482271671295166
I0916 06:01:22.899084 140023666775808 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.20738939940929413, loss=1.3976062536239624
I0916 06:04:20.936674 140200172709696 spec.py:320] Evaluating on the training split.
I0916 06:04:23.945590 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 06:08:49.545894 140200172709696 spec.py:332] Evaluating on the validation split.
I0916 06:08:52.183421 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 06:11:36.296119 140200172709696 spec.py:348] Evaluating on the test split.
I0916 06:11:39.006841 140200172709696 workload.py:179] Translating evaluation dataset.
I0916 06:14:12.812399 140200172709696 submission_runner.py:376] Time since start: 60438.84s, 	Step: 100000, 	{'train/accuracy': 0.6932647824287415, 'train/loss': 1.3888061046600342, 'train/bleu': 35.16523319278526, 'validation/accuracy': 0.6946101188659668, 'validation/loss': 1.3695207834243774, 'validation/bleu': 30.733998502978647, 'validation/num_examples': 3000, 'test/accuracy': 0.7117308974266052, 'test/loss': 1.2664434909820557, 'test/bleu': 30.879454066841273, 'test/num_examples': 3003, 'score': 35771.05850434303, 'total_duration': 60438.8363134861, 'accumulated_submission_time': 35771.05850434303, 'accumulated_eval_time': 24664.951100349426, 'accumulated_logging_time': 1.290508508682251}
I0916 06:14:12.833797 140023658383104 logging_writer.py:48] [100000] accumulated_eval_time=24664.951100, accumulated_logging_time=1.290509, accumulated_submission_time=35771.058504, global_step=100000, preemption_count=0, score=35771.058504, test/accuracy=0.711731, test/bleu=30.879454, test/loss=1.266443, test/num_examples=3003, total_duration=60438.836313, train/accuracy=0.693265, train/bleu=35.165233, train/loss=1.388806, validation/accuracy=0.694610, validation/bleu=30.733999, validation/loss=1.369521, validation/num_examples=3000
I0916 06:14:12.853700 140023666775808 logging_writer.py:48] [100000] global_step=100000, preemption_count=0, score=35771.058504
I0916 06:14:14.059060 140200172709696 checkpoints.py:490] Saving checkpoint at step: 100000
I0916 06:14:18.151854 140200172709696 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1/checkpoint_100000
I0916 06:14:18.156765 140200172709696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_jax/nadamw_run_0/wmt_jax/trial_1/checkpoint_100000.
I0916 06:14:18.198795 140200172709696 submission_runner.py:540] Tuning trial 1/1
I0916 06:14:18.198960 140200172709696 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, beta1=0.9326607383586145, beta2=0.9955159689799007, warmup_steps=1999, weight_decay=0.08121616522670176, label_smoothing=0.0)
I0916 06:14:18.204546 140200172709696 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000594904413446784, 'train/loss': 11.167823791503906, 'train/bleu': 1.6684120676165568e-10, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.146151542663574, 'validation/bleu': 1.1152830692390254e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.15811538696289, 'test/bleu': 2.5300816166015135e-10, 'test/num_examples': 3003, 'score': 45.74680542945862, 'total_duration': 938.0734601020813, 'accumulated_submission_time': 45.74680542945862, 'accumulated_eval_time': 892.3266117572784, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2346, {'train/accuracy': 0.5235041379928589, 'train/loss': 2.75996732711792, 'train/bleu': 23.36283316860885, 'validation/accuracy': 0.5274826288223267, 'validation/loss': 2.7242908477783203, 'validation/bleu': 19.380327941442793, 'validation/num_examples': 3000, 'test/accuracy': 0.5237464308738708, 'test/loss': 2.7663111686706543, 'test/bleu': 17.513598452100037, 'test/num_examples': 3003, 'score': 885.715635061264, 'total_duration': 2249.284782886505, 'accumulated_submission_time': 885.715635061264, 'accumulated_eval_time': 1363.4981791973114, 'accumulated_logging_time': 0.03613853454589844, 'global_step': 2346, 'preemption_count': 0}), (4698, {'train/accuracy': 0.579552173614502, 'train/loss': 2.2296831607818604, 'train/bleu': 27.27231776667703, 'validation/accuracy': 0.5946237444877625, 'validation/loss': 2.116241455078125, 'validation/bleu': 23.6002089836129, 'validation/num_examples': 3000, 'test/accuracy': 0.5976642966270447, 'test/loss': 2.0898654460906982, 'test/bleu': 22.19579714477223, 'test/num_examples': 3003, 'score': 1725.7396528720856, 'total_duration': 3548.31689620018, 'accumulated_submission_time': 1725.7396528720856, 'accumulated_eval_time': 1822.4436695575714, 'accumulated_logging_time': 0.06357741355895996, 'global_step': 4698, 'preemption_count': 0}), (7049, {'train/accuracy': 0.6090830564498901, 'train/loss': 1.9835175275802612, 'train/bleu': 28.9226257207205, 'validation/accuracy': 0.6177976727485657, 'validation/loss': 1.9118613004684448, 'validation/bleu': 25.21162722790785, 'validation/num_examples': 3000, 'test/accuracy': 0.6218581199645996, 'test/loss': 1.8709417581558228, 'test/bleu': 23.83966507801399, 'test/num_examples': 3003, 'score': 2565.7210092544556, 'total_duration': 4844.541260957718, 'accumulated_submission_time': 2565.7210092544556, 'accumulated_eval_time': 2278.6256353855133, 'accumulated_logging_time': 0.0899801254272461, 'global_step': 7049, 'preemption_count': 0}), (9401, {'train/accuracy': 0.6170135140419006, 'train/loss': 1.9261927604675293, 'train/bleu': 29.61187248686974, 'validation/accuracy': 0.6302463412284851, 'validation/loss': 1.8087116479873657, 'validation/bleu': 26.1521308761062, 'validation/num_examples': 3000, 'test/accuracy': 0.6373017430305481, 'test/loss': 1.7505528926849365, 'test/bleu': 25.116535533384756, 'test/num_examples': 3003, 'score': 3406.0114896297455, 'total_duration': 6105.708568096161, 'accumulated_submission_time': 3406.0114896297455, 'accumulated_eval_time': 2699.4407522678375, 'accumulated_logging_time': 0.11698603630065918, 'global_step': 9401, 'preemption_count': 0}), (11752, {'train/accuracy': 0.6229552626609802, 'train/loss': 1.8726989030838013, 'train/bleu': 30.029023551583855, 'validation/accuracy': 0.6390249133110046, 'validation/loss': 1.7448806762695312, 'validation/bleu': 26.690216466648614, 'validation/num_examples': 3000, 'test/accuracy': 0.6469815969467163, 'test/loss': 1.6859023571014404, 'test/bleu': 25.546572599410325, 'test/num_examples': 3003, 'score': 4246.143506288528, 'total_duration': 7395.568585634232, 'accumulated_submission_time': 4246.143506288528, 'accumulated_eval_time': 3149.103892326355, 'accumulated_logging_time': 0.14639687538146973, 'global_step': 11752, 'preemption_count': 0}), (14103, {'train/accuracy': 0.6291584968566895, 'train/loss': 1.821905255317688, 'train/bleu': 30.272824576136717, 'validation/accuracy': 0.6436870098114014, 'validation/loss': 1.7103956937789917, 'validation/bleu': 26.90387007376632, 'validation/num_examples': 3000, 'test/accuracy': 0.6522805094718933, 'test/loss': 1.6426098346710205, 'test/bleu': 26.18302709510793, 'test/num_examples': 3003, 'score': 5086.39458823204, 'total_duration': 8690.345397233963, 'accumulated_submission_time': 5086.39458823204, 'accumulated_eval_time': 3603.565372943878, 'accumulated_logging_time': 0.17497968673706055, 'global_step': 14103, 'preemption_count': 0}), (16455, {'train/accuracy': 0.6286917924880981, 'train/loss': 1.8241031169891357, 'train/bleu': 30.32994731026418, 'validation/accuracy': 0.6472951173782349, 'validation/loss': 1.678662896156311, 'validation/bleu': 27.3464595466423, 'validation/num_examples': 3000, 'test/accuracy': 0.657184362411499, 'test/loss': 1.6122846603393555, 'test/bleu': 26.652709537763407, 'test/num_examples': 3003, 'score': 5926.471277952194, 'total_duration': 10022.435180425644, 'accumulated_submission_time': 5926.471277952194, 'accumulated_eval_time': 4095.5162653923035, 'accumulated_logging_time': 0.20159125328063965, 'global_step': 16455, 'preemption_count': 0}), (18807, {'train/accuracy': 0.6742820143699646, 'train/loss': 1.5166453123092651, 'train/bleu': 33.3908808933066, 'validation/accuracy': 0.6507917046546936, 'validation/loss': 1.6511542797088623, 'validation/bleu': 27.81718379995774, 'validation/num_examples': 3000, 'test/accuracy': 0.6619836091995239, 'test/loss': 1.5844849348068237, 'test/bleu': 26.802111027370753, 'test/num_examples': 3003, 'score': 6766.463502407074, 'total_duration': 11303.178382635117, 'accumulated_submission_time': 6766.463502407074, 'accumulated_eval_time': 4536.204072237015, 'accumulated_logging_time': 0.2286224365234375, 'global_step': 18807, 'preemption_count': 0}), (21159, {'train/accuracy': 0.6358057260513306, 'train/loss': 1.7667934894561768, 'train/bleu': 30.812733623456655, 'validation/accuracy': 0.6530483365058899, 'validation/loss': 1.6341582536697388, 'validation/bleu': 27.44028540483933, 'validation/num_examples': 3000, 'test/accuracy': 0.6640985608100891, 'test/loss': 1.5679447650909424, 'test/bleu': 27.115582618045106, 'test/num_examples': 3003, 'score': 7606.495560407639, 'total_duration': 12628.482678890228, 'accumulated_submission_time': 7606.495560407639, 'accumulated_eval_time': 5021.414864778519, 'accumulated_logging_time': 0.2551116943359375, 'global_step': 21159, 'preemption_count': 0}), (23512, {'train/accuracy': 0.6352537870407104, 'train/loss': 1.7718756198883057, 'train/bleu': 30.706208737651444, 'validation/accuracy': 0.6563217043876648, 'validation/loss': 1.6186645030975342, 'validation/bleu': 27.953983619837004, 'validation/num_examples': 3000, 'test/accuracy': 0.6672360897064209, 'test/loss': 1.5510153770446777, 'test/bleu': 27.049800648124933, 'test/num_examples': 3003, 'score': 8446.618193149567, 'total_duration': 13957.909576654434, 'accumulated_submission_time': 8446.618193149567, 'accumulated_eval_time': 5510.657336473465, 'accumulated_logging_time': 0.28184986114501953, 'global_step': 23512, 'preemption_count': 0}), (25864, {'train/accuracy': 0.6477124691009521, 'train/loss': 1.6808216571807861, 'train/bleu': 31.74158602660301, 'validation/accuracy': 0.6586527228355408, 'validation/loss': 1.6096842288970947, 'validation/bleu': 28.152183940088808, 'validation/num_examples': 3000, 'test/accuracy': 0.6683748960494995, 'test/loss': 1.5386584997177124, 'test/bleu': 27.767962044249014, 'test/num_examples': 3003, 'score': 9286.86467552185, 'total_duration': 15312.294780015945, 'accumulated_submission_time': 9286.86467552185, 'accumulated_eval_time': 6024.732455253601, 'accumulated_logging_time': 0.3088691234588623, 'global_step': 25864, 'preemption_count': 0}), (28216, {'train/accuracy': 0.6431559324264526, 'train/loss': 1.7156288623809814, 'train/bleu': 31.15831597828048, 'validation/accuracy': 0.6597438454627991, 'validation/loss': 1.5974607467651367, 'validation/bleu': 27.93922404873176, 'validation/num_examples': 3000, 'test/accuracy': 0.67082679271698, 'test/loss': 1.5214282274246216, 'test/bleu': 27.47985343173673, 'test/num_examples': 3003, 'score': 10127.062792539597, 'total_duration': 17002.061151742935, 'accumulated_submission_time': 10127.062792539597, 'accumulated_eval_time': 6874.236850976944, 'accumulated_logging_time': 0.33684802055358887, 'global_step': 28216, 'preemption_count': 0}), (30568, {'train/accuracy': 0.6407042741775513, 'train/loss': 1.7355269193649292, 'train/bleu': 31.736939869576855, 'validation/accuracy': 0.6623228192329407, 'validation/loss': 1.5837912559509277, 'validation/bleu': 28.12987130389086, 'validation/num_examples': 3000, 'test/accuracy': 0.6727790236473083, 'test/loss': 1.5093638896942139, 'test/bleu': 27.61588321451924, 'test/num_examples': 3003, 'score': 10967.162513256073, 'total_duration': 18345.560475587845, 'accumulated_submission_time': 10967.162513256073, 'accumulated_eval_time': 7377.573710203171, 'accumulated_logging_time': 0.3637700080871582, 'global_step': 30568, 'preemption_count': 0}), (32920, {'train/accuracy': 0.6482195854187012, 'train/loss': 1.6781005859375, 'train/bleu': 31.291625033907437, 'validation/accuracy': 0.6596074104309082, 'validation/loss': 1.5918610095977783, 'validation/bleu': 28.307900253113104, 'validation/num_examples': 3000, 'test/accuracy': 0.6717215776443481, 'test/loss': 1.52046537399292, 'test/bleu': 27.438516771206867, 'test/num_examples': 3003, 'score': 11807.36433839798, 'total_duration': 19770.514050483704, 'accumulated_submission_time': 11807.36433839798, 'accumulated_eval_time': 7962.2557721138, 'accumulated_logging_time': 0.3961160182952881, 'global_step': 32920, 'preemption_count': 0}), (35272, {'train/accuracy': 0.6445931792259216, 'train/loss': 1.6923820972442627, 'train/bleu': 31.449931629767125, 'validation/accuracy': 0.6640711426734924, 'validation/loss': 1.566933035850525, 'validation/bleu': 28.530366820724332, 'validation/num_examples': 3000, 'test/accuracy': 0.6769391894340515, 'test/loss': 1.4909327030181885, 'test/bleu': 28.05892782595901, 'test/num_examples': 3003, 'score': 12647.389313459396, 'total_duration': 21155.317359924316, 'accumulated_submission_time': 12647.389313459396, 'accumulated_eval_time': 8506.967953205109, 'accumulated_logging_time': 0.42643117904663086, 'global_step': 35272, 'preemption_count': 0}), (37624, {'train/accuracy': 0.6766722202301025, 'train/loss': 1.4892700910568237, 'train/bleu': 33.74145405479451, 'validation/accuracy': 0.665571391582489, 'validation/loss': 1.552937388420105, 'validation/bleu': 28.49283971031092, 'validation/num_examples': 3000, 'test/accuracy': 0.678740382194519, 'test/loss': 1.4733619689941406, 'test/bleu': 28.221332307282264, 'test/num_examples': 3003, 'score': 13487.563034296036, 'total_duration': 22523.829854488373, 'accumulated_submission_time': 13487.563034296036, 'accumulated_eval_time': 9035.241974592209, 'accumulated_logging_time': 0.4555542469024658, 'global_step': 37624, 'preemption_count': 0}), (39975, {'train/accuracy': 0.6486798524856567, 'train/loss': 1.6690161228179932, 'train/bleu': 31.76872930189234, 'validation/accuracy': 0.6666129231452942, 'validation/loss': 1.5478057861328125, 'validation/bleu': 28.342506901149452, 'validation/num_examples': 3000, 'test/accuracy': 0.6792051792144775, 'test/loss': 1.4670010805130005, 'test/bleu': 28.275114585104728, 'test/num_examples': 3003, 'score': 14327.71306014061, 'total_duration': 23928.60667324066, 'accumulated_submission_time': 14327.71306014061, 'accumulated_eval_time': 9599.802678823471, 'accumulated_logging_time': 0.4839303493499756, 'global_step': 39975, 'preemption_count': 0}), (42327, {'train/accuracy': 0.6474726796150208, 'train/loss': 1.6788183450698853, 'train/bleu': 31.87535373036659, 'validation/accuracy': 0.6694647073745728, 'validation/loss': 1.5372003316879272, 'validation/bleu': 28.96074352225166, 'validation/num_examples': 3000, 'test/accuracy': 0.6830283403396606, 'test/loss': 1.4495502710342407, 'test/bleu': 28.245123367880478, 'test/num_examples': 3003, 'score': 15167.914673089981, 'total_duration': 25260.749643325806, 'accumulated_submission_time': 15167.914673089981, 'accumulated_eval_time': 10091.67827796936, 'accumulated_logging_time': 0.5133004188537598, 'global_step': 42327, 'preemption_count': 0}), (44679, {'train/accuracy': 0.6564565896987915, 'train/loss': 1.597154974937439, 'train/bleu': 32.23296806244103, 'validation/accuracy': 0.6703822612762451, 'validation/loss': 1.5270098447799683, 'validation/bleu': 28.823661584305334, 'validation/num_examples': 3000, 'test/accuracy': 0.6820754408836365, 'test/loss': 1.446740984916687, 'test/bleu': 28.525445100360116, 'test/num_examples': 3003, 'score': 16007.864673376083, 'total_duration': 26686.061677455902, 'accumulated_submission_time': 16007.864673376083, 'accumulated_eval_time': 10676.976427316666, 'accumulated_logging_time': 0.5415115356445312, 'global_step': 44679, 'preemption_count': 0}), (47030, {'train/accuracy': 0.6522310972213745, 'train/loss': 1.6504321098327637, 'train/bleu': 32.629683239875604, 'validation/accuracy': 0.671808123588562, 'validation/loss': 1.5212384462356567, 'validation/bleu': 28.34424867325631, 'validation/num_examples': 3000, 'test/accuracy': 0.6866539120674133, 'test/loss': 1.4293789863586426, 'test/bleu': 28.93234883794651, 'test/num_examples': 3003, 'score': 16847.876141548157, 'total_duration': 28157.188164711, 'accumulated_submission_time': 16847.876141548157, 'accumulated_eval_time': 11308.02505350113, 'accumulated_logging_time': 0.5724122524261475, 'global_step': 47030, 'preemption_count': 0}), (49382, {'train/accuracy': 0.6565055251121521, 'train/loss': 1.629167079925537, 'train/bleu': 31.98144433819645, 'validation/accuracy': 0.6723164916038513, 'validation/loss': 1.5097647905349731, 'validation/bleu': 29.13128336574811, 'validation/num_examples': 3000, 'test/accuracy': 0.6870025396347046, 'test/loss': 1.4261798858642578, 'test/bleu': 28.720813148139765, 'test/num_examples': 3003, 'score': 17687.95327615738, 'total_duration': 29602.666443824768, 'accumulated_submission_time': 17687.95327615738, 'accumulated_eval_time': 11913.358323812485, 'accumulated_logging_time': 0.6036539077758789, 'global_step': 49382, 'preemption_count': 0}), (51734, {'train/accuracy': 0.662792444229126, 'train/loss': 1.5787047147750854, 'train/bleu': 32.682576293721304, 'validation/accuracy': 0.6716965436935425, 'validation/loss': 1.5070016384124756, 'validation/bleu': 28.89194143728194, 'validation/num_examples': 3000, 'test/accuracy': 0.6849108338356018, 'test/loss': 1.4270668029785156, 'test/bleu': 28.666265192643948, 'test/num_examples': 3003, 'score': 18527.97706103325, 'total_duration': 30991.950076818466, 'accumulated_submission_time': 18527.97706103325, 'accumulated_eval_time': 12462.5544860363, 'accumulated_logging_time': 0.6314728260040283, 'global_step': 51734, 'preemption_count': 0}), (54085, {'train/accuracy': 0.6590363383293152, 'train/loss': 1.608697772026062, 'train/bleu': 32.559509901286184, 'validation/accuracy': 0.6745111346244812, 'validation/loss': 1.4913569688796997, 'validation/bleu': 29.32784994159188, 'validation/num_examples': 3000, 'test/accuracy': 0.6896635890007019, 'test/loss': 1.4021748304367065, 'test/bleu': 29.049958080078103, 'test/num_examples': 3003, 'score': 19368.131568193436, 'total_duration': 32412.93671989441, 'accumulated_submission_time': 19368.131568193436, 'accumulated_eval_time': 13043.317687273026, 'accumulated_logging_time': 0.6635177135467529, 'global_step': 54085, 'preemption_count': 0}), (56438, {'train/accuracy': 0.683269202709198, 'train/loss': 1.4353021383285522, 'train/bleu': 34.12081370506152, 'validation/accuracy': 0.6767181754112244, 'validation/loss': 1.4791769981384277, 'validation/bleu': 29.283518109006234, 'validation/num_examples': 3000, 'test/accuracy': 0.6906397342681885, 'test/loss': 1.3920460939407349, 'test/bleu': 28.965194170055046, 'test/num_examples': 3003, 'score': 20208.34093117714, 'total_duration': 33804.93611049652, 'accumulated_submission_time': 20208.34093117714, 'accumulated_eval_time': 13595.041430234909, 'accumulated_logging_time': 0.6939771175384521, 'global_step': 56438, 'preemption_count': 0}), (58790, {'train/accuracy': 0.6633440256118774, 'train/loss': 1.5697834491729736, 'train/bleu': 32.70040078168986, 'validation/accuracy': 0.6774621605873108, 'validation/loss': 1.4731673002243042, 'validation/bleu': 29.36555143145377, 'validation/num_examples': 3000, 'test/accuracy': 0.691557765007019, 'test/loss': 1.3868361711502075, 'test/bleu': 29.23068514578597, 'test/num_examples': 3003, 'score': 21048.31526875496, 'total_duration': 35255.446363925934, 'accumulated_submission_time': 21048.31526875496, 'accumulated_eval_time': 14205.511446714401, 'accumulated_logging_time': 0.724341630935669, 'global_step': 58790, 'preemption_count': 0}), (61143, {'train/accuracy': 0.6639869213104248, 'train/loss': 1.5631855726242065, 'train/bleu': 32.584606643198825, 'validation/accuracy': 0.6802147626876831, 'validation/loss': 1.4607304334640503, 'validation/bleu': 29.77535455886182, 'validation/num_examples': 3000, 'test/accuracy': 0.694311797618866, 'test/loss': 1.3735520839691162, 'test/bleu': 29.522124516555948, 'test/num_examples': 3003, 'score': 21888.56804585457, 'total_duration': 36686.303837537766, 'accumulated_submission_time': 21888.56804585457, 'accumulated_eval_time': 14796.049931287766, 'accumulated_logging_time': 0.7542929649353027, 'global_step': 61143, 'preemption_count': 0}), (63496, {'train/accuracy': 0.6726361513137817, 'train/loss': 1.5125588178634644, 'train/bleu': 33.887505490130884, 'validation/accuracy': 0.682223379611969, 'validation/loss': 1.4498581886291504, 'validation/bleu': 29.861537345406855, 'validation/num_examples': 3000, 'test/accuracy': 0.6964848041534424, 'test/loss': 1.362645149230957, 'test/bleu': 29.801053616014823, 'test/num_examples': 3003, 'score': 22728.792023181915, 'total_duration': 38038.50773406029, 'accumulated_submission_time': 22728.792023181915, 'accumulated_eval_time': 15307.964537382126, 'accumulated_logging_time': 0.7837402820587158, 'global_step': 63496, 'preemption_count': 0}), (65848, {'train/accuracy': 0.6686924695968628, 'train/loss': 1.531067132949829, 'train/bleu': 33.59518280344375, 'validation/accuracy': 0.6813926696777344, 'validation/loss': 1.440827488899231, 'validation/bleu': 29.601585956948888, 'validation/num_examples': 3000, 'test/accuracy': 0.6982743740081787, 'test/loss': 1.3472319841384888, 'test/bleu': 29.776164290615625, 'test/num_examples': 3003, 'score': 23568.90534734726, 'total_duration': 39459.09894824028, 'accumulated_submission_time': 23568.90534734726, 'accumulated_eval_time': 15888.376370191574, 'accumulated_logging_time': 0.812269926071167, 'global_step': 65848, 'preemption_count': 0}), (68200, {'train/accuracy': 0.6689242720603943, 'train/loss': 1.5433770418167114, 'train/bleu': 33.45396394498059, 'validation/accuracy': 0.6838352680206299, 'validation/loss': 1.4317246675491333, 'validation/bleu': 29.673474661236817, 'validation/num_examples': 3000, 'test/accuracy': 0.7009587287902832, 'test/loss': 1.3379542827606201, 'test/bleu': 29.967740918292552, 'test/num_examples': 3003, 'score': 24408.918486356735, 'total_duration': 40877.966178417206, 'accumulated_submission_time': 24408.918486356735, 'accumulated_eval_time': 16467.165045022964, 'accumulated_logging_time': 0.8411040306091309, 'global_step': 68200, 'preemption_count': 0}), (70552, {'train/accuracy': 0.6784619688987732, 'train/loss': 1.4757046699523926, 'train/bleu': 33.838587925956006, 'validation/accuracy': 0.6847652196884155, 'validation/loss': 1.4239221811294556, 'validation/bleu': 29.906966293141938, 'validation/num_examples': 3000, 'test/accuracy': 0.7004590034484863, 'test/loss': 1.33260977268219, 'test/bleu': 29.894544787017473, 'test/num_examples': 3003, 'score': 25249.183546066284, 'total_duration': 42257.13475418091, 'accumulated_submission_time': 25249.183546066284, 'accumulated_eval_time': 17005.999893188477, 'accumulated_logging_time': 0.8733737468719482, 'global_step': 70552, 'preemption_count': 0}), (72904, {'train/accuracy': 0.6751803755760193, 'train/loss': 1.5031684637069702, 'train/bleu': 33.85136010510074, 'validation/accuracy': 0.6861786842346191, 'validation/loss': 1.4201096296310425, 'validation/bleu': 30.25119936945212, 'validation/num_examples': 3000, 'test/accuracy': 0.7040846347808838, 'test/loss': 1.3219733238220215, 'test/bleu': 30.031453147472963, 'test/num_examples': 3003, 'score': 26089.403928756714, 'total_duration': 43656.123386621475, 'accumulated_submission_time': 26089.403928756714, 'accumulated_eval_time': 17564.70252108574, 'accumulated_logging_time': 0.9026870727539062, 'global_step': 72904, 'preemption_count': 0}), (75256, {'train/accuracy': 0.6910349130630493, 'train/loss': 1.4020071029663086, 'train/bleu': 34.927196110023715, 'validation/accuracy': 0.6873194575309753, 'validation/loss': 1.41226065158844, 'validation/bleu': 30.188396800787512, 'validation/num_examples': 3000, 'test/accuracy': 0.7040032744407654, 'test/loss': 1.3139973878860474, 'test/bleu': 29.86310908102282, 'test/num_examples': 3003, 'score': 26929.519035577774, 'total_duration': 45063.5026140213, 'accumulated_submission_time': 26929.519035577774, 'accumulated_eval_time': 18131.898837804794, 'accumulated_logging_time': 0.933844804763794, 'global_step': 75256, 'preemption_count': 0}), (77606, {'train/accuracy': 0.6816911101341248, 'train/loss': 1.4577269554138184, 'train/bleu': 34.16135647052811, 'validation/accuracy': 0.6901092529296875, 'validation/loss': 1.401729702949524, 'validation/bleu': 30.25529899733195, 'validation/num_examples': 3000, 'test/accuracy': 0.7070245742797852, 'test/loss': 1.3021745681762695, 'test/bleu': 30.381383254530427, 'test/num_examples': 3003, 'score': 27769.473219394684, 'total_duration': 46518.950401067734, 'accumulated_submission_time': 27769.473219394684, 'accumulated_eval_time': 18747.324250936508, 'accumulated_logging_time': 0.9653325080871582, 'global_step': 77606, 'preemption_count': 0}), (79956, {'train/accuracy': 0.6826333403587341, 'train/loss': 1.453797698020935, 'train/bleu': 34.182064548347434, 'validation/accuracy': 0.6910887360572815, 'validation/loss': 1.392814040184021, 'validation/bleu': 30.48268493849697, 'validation/num_examples': 3000, 'test/accuracy': 0.7065830230712891, 'test/loss': 1.2955222129821777, 'test/bleu': 30.508525449054453, 'test/num_examples': 3003, 'score': 28609.496497154236, 'total_duration': 48014.11851024628, 'accumulated_submission_time': 28609.496497154236, 'accumulated_eval_time': 19402.396826267242, 'accumulated_logging_time': 0.9997520446777344, 'global_step': 79956, 'preemption_count': 0}), (82307, {'train/accuracy': 0.6881935000419617, 'train/loss': 1.4140995740890503, 'train/bleu': 35.20752929388257, 'validation/accuracy': 0.6915723085403442, 'validation/loss': 1.3875857591629028, 'validation/bleu': 30.41800441228426, 'validation/num_examples': 3000, 'test/accuracy': 0.7078612446784973, 'test/loss': 1.2876874208450317, 'test/bleu': 30.58130553757828, 'test/num_examples': 3003, 'score': 29449.603117465973, 'total_duration': 49444.19443368912, 'accumulated_submission_time': 29449.603117465973, 'accumulated_eval_time': 19992.299550294876, 'accumulated_logging_time': 1.0306241512298584, 'global_step': 82307, 'preemption_count': 0}), (84659, {'train/accuracy': 0.6858822703361511, 'train/loss': 1.431738257408142, 'train/bleu': 34.84152757725994, 'validation/accuracy': 0.6918451189994812, 'validation/loss': 1.3819169998168945, 'validation/bleu': 30.552772328293962, 'validation/num_examples': 3000, 'test/accuracy': 0.7091162800788879, 'test/loss': 1.2818939685821533, 'test/bleu': 30.6324119297336, 'test/num_examples': 3003, 'score': 30289.819618701935, 'total_duration': 50923.69852876663, 'accumulated_submission_time': 30289.819618701935, 'accumulated_eval_time': 20631.51967906952, 'accumulated_logging_time': 1.0619540214538574, 'global_step': 84659, 'preemption_count': 0}), (87009, {'train/accuracy': 0.6864525675773621, 'train/loss': 1.4331843852996826, 'train/bleu': 35.53816882244098, 'validation/accuracy': 0.6934570074081421, 'validation/loss': 1.3768035173416138, 'validation/bleu': 30.825226734244094, 'validation/num_examples': 3000, 'test/accuracy': 0.7111731171607971, 'test/loss': 1.2767939567565918, 'test/bleu': 30.776100815282874, 'test/num_examples': 3003, 'score': 31129.788384199142, 'total_duration': 52329.38246154785, 'accumulated_submission_time': 31129.788384199142, 'accumulated_eval_time': 21197.166420698166, 'accumulated_logging_time': 1.0935418605804443, 'global_step': 87009, 'preemption_count': 0}), (89360, {'train/accuracy': 0.6890226602554321, 'train/loss': 1.4117536544799805, 'train/bleu': 35.242698152588, 'validation/accuracy': 0.694572925567627, 'validation/loss': 1.3744648694992065, 'validation/bleu': 30.707595011519864, 'validation/num_examples': 3000, 'test/accuracy': 0.7108826041221619, 'test/loss': 1.2732455730438232, 'test/bleu': 30.833011989298747, 'test/num_examples': 3003, 'score': 31969.884179115295, 'total_duration': 53731.49858379364, 'accumulated_submission_time': 31969.884179115295, 'accumulated_eval_time': 21759.11456155777, 'accumulated_logging_time': 1.1296253204345703, 'global_step': 89360, 'preemption_count': 0}), (91712, {'train/accuracy': 0.6906339526176453, 'train/loss': 1.4052921533584595, 'train/bleu': 35.31118387042799, 'validation/accuracy': 0.6946721076965332, 'validation/loss': 1.3729954957962036, 'validation/bleu': 30.747611253423468, 'validation/num_examples': 3000, 'test/accuracy': 0.7113590240478516, 'test/loss': 1.2718837261199951, 'test/bleu': 30.809581578825462, 'test/num_examples': 3003, 'score': 32810.15914392471, 'total_duration': 55139.81289434433, 'accumulated_submission_time': 32810.15914392471, 'accumulated_eval_time': 22327.08559536934, 'accumulated_logging_time': 1.1615445613861084, 'global_step': 91712, 'preemption_count': 0}), (94063, {'train/accuracy': 0.694902241230011, 'train/loss': 1.3763175010681152, 'train/bleu': 35.190585179893944, 'validation/accuracy': 0.6945357322692871, 'validation/loss': 1.371010422706604, 'validation/bleu': 30.816372084215267, 'validation/num_examples': 3000, 'test/accuracy': 0.7119865417480469, 'test/loss': 1.2680238485336304, 'test/bleu': 30.871082398160198, 'test/num_examples': 3003, 'score': 33650.166264772415, 'total_duration': 56552.064269304276, 'accumulated_submission_time': 33650.166264772415, 'accumulated_eval_time': 22899.263434171677, 'accumulated_logging_time': 1.191739559173584, 'global_step': 94063, 'preemption_count': 0}), (96415, {'train/accuracy': 0.6920124888420105, 'train/loss': 1.3959263563156128, 'train/bleu': 35.53895800749216, 'validation/accuracy': 0.694386899471283, 'validation/loss': 1.3700008392333984, 'validation/bleu': 30.7908717009102, 'validation/num_examples': 3000, 'test/accuracy': 0.7119865417480469, 'test/loss': 1.2670915126800537, 'test/bleu': 30.84459263110783, 'test/num_examples': 3003, 'score': 34490.401856184006, 'total_duration': 57976.433227300644, 'accumulated_submission_time': 34490.401856184006, 'accumulated_eval_time': 23483.3280107975, 'accumulated_logging_time': 1.2245352268218994, 'global_step': 96415, 'preemption_count': 0}), (98766, {'train/accuracy': 0.6938732862472534, 'train/loss': 1.3794183731079102, 'train/bleu': 35.519470369241354, 'validation/accuracy': 0.6945233345031738, 'validation/loss': 1.3695416450500488, 'validation/bleu': 30.787394565809937, 'validation/num_examples': 3000, 'test/accuracy': 0.7118819355964661, 'test/loss': 1.266464114189148, 'test/bleu': 30.882857452738662, 'test/num_examples': 3003, 'score': 35330.333010196686, 'total_duration': 59406.18332052231, 'accumulated_submission_time': 35330.333010196686, 'accumulated_eval_time': 24073.0754570961, 'accumulated_logging_time': 1.2589006423950195, 'global_step': 98766, 'preemption_count': 0}), (100000, {'train/accuracy': 0.6932647824287415, 'train/loss': 1.3888061046600342, 'train/bleu': 35.16523319278526, 'validation/accuracy': 0.6946101188659668, 'validation/loss': 1.3695207834243774, 'validation/bleu': 30.733998502978647, 'validation/num_examples': 3000, 'test/accuracy': 0.7117308974266052, 'test/loss': 1.2664434909820557, 'test/bleu': 30.879454066841273, 'test/num_examples': 3003, 'score': 35771.05850434303, 'total_duration': 60438.8363134861, 'accumulated_submission_time': 35771.05850434303, 'accumulated_eval_time': 24664.951100349426, 'accumulated_logging_time': 1.290508508682251, 'global_step': 100000, 'preemption_count': 0})], 'global_step': 100000}
I0916 06:14:18.204707 140200172709696 submission_runner.py:543] Timing: 35771.05850434303
I0916 06:14:18.204768 140200172709696 submission_runner.py:545] Total number of evals: 44
I0916 06:14:18.204820 140200172709696 submission_runner.py:546] ====================
I0916 06:14:18.204985 140200172709696 submission_runner.py:614] Final wmt score: 35771.05850434303
