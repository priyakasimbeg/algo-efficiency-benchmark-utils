python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_deepspeech/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_jax/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=36000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_09-16-2023-06-26-19.log
2023-09-16 06:26:24.651660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0916 06:26:42.552806 140294737168192 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax.
I0916 06:26:43.545571 140294737168192 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0916 06:26:43.546361 140294737168192 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0916 06:26:43.546504 140294737168192 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0916 06:26:43.552179 140294737168192 submission_runner.py:500] Using RNG seed 1650218171
I0916 06:26:49.470588 140294737168192 submission_runner.py:509] --- Tuning run 1/1 ---
I0916 06:26:49.470825 140294737168192 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1.
I0916 06:26:49.471032 140294737168192 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/hparams.json.
I0916 06:26:49.650267 140294737168192 submission_runner.py:185] Initializing dataset.
I0916 06:26:49.650489 140294737168192 submission_runner.py:192] Initializing model.
I0916 06:26:52.534667 140294737168192 submission_runner.py:226] Initializing optimizer.
I0916 06:26:53.264320 140294737168192 submission_runner.py:233] Initializing metrics bundle.
I0916 06:26:53.264553 140294737168192 submission_runner.py:251] Initializing checkpoint and logger.
I0916 06:26:53.265762 140294737168192 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0916 06:26:53.266157 140294737168192 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0916 06:26:53.266278 140294737168192 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0916 06:26:54.052719 140294737168192 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0916 06:26:54.053795 140294737168192 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0916 06:26:54.067729 140294737168192 submission_runner.py:285] Starting training loop.
I0916 06:26:54.369675 140294737168192 input_pipeline.py:20] Loading split = train-clean-100
I0916 06:26:54.411404 140294737168192 input_pipeline.py:20] Loading split = train-clean-360
I0916 06:26:54.541928 140294737168192 input_pipeline.py:20] Loading split = train-other-500
2023-09-16 06:27:46.390094: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-09-16 06:27:48.732481: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0916 06:27:54.054145 140127502333696 logging_writer.py:48] [0] global_step=0, grad_norm=19.12497901916504, loss=32.856929779052734
I0916 06:27:54.089447 140294737168192 spec.py:320] Evaluating on the training split.
I0916 06:27:54.349936 140294737168192 input_pipeline.py:20] Loading split = train-clean-100
I0916 06:27:54.386288 140294737168192 input_pipeline.py:20] Loading split = train-clean-360
I0916 06:27:54.764319 140294737168192 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0916 06:29:58.166203 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 06:29:58.369047 140294737168192 input_pipeline.py:20] Loading split = dev-clean
I0916 06:29:58.375396 140294737168192 input_pipeline.py:20] Loading split = dev-other
I0916 06:31:07.736765 140294737168192 spec.py:348] Evaluating on the test split.
I0916 06:31:07.945955 140294737168192 input_pipeline.py:20] Loading split = test-clean
I0916 06:31:51.482249 140294737168192 submission_runner.py:376] Time since start: 297.41s, 	Step: 1, 	{'train/ctc_loss': Array(31.51439, dtype=float32), 'train/wer': 3.9196478770589094, 'validation/ctc_loss': Array(30.433355, dtype=float32), 'validation/wer': 3.5589344807957626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.44908, dtype=float32), 'test/wer': 3.7575609855178436, 'test/num_examples': 2472, 'score': 60.02165150642395, 'total_duration': 297.41210770606995, 'accumulated_submission_time': 60.02165150642395, 'accumulated_eval_time': 237.3904013633728, 'accumulated_logging_time': 0}
I0916 06:31:51.508446 140126395037440 logging_writer.py:48] [1] accumulated_eval_time=237.390401, accumulated_logging_time=0, accumulated_submission_time=60.021652, global_step=1, preemption_count=0, score=60.021652, test/ctc_loss=30.449079513549805, test/num_examples=2472, test/wer=3.757561, total_duration=297.412108, train/ctc_loss=31.51439094543457, train/wer=3.919648, validation/ctc_loss=30.4333553314209, validation/num_examples=5348, validation/wer=3.558934
I0916 06:32:00.834882 140135239943936 logging_writer.py:48] [1] global_step=1, grad_norm=19.229717254638672, loss=33.2103157043457
I0916 06:32:01.683203 140135248336640 logging_writer.py:48] [2] global_step=2, grad_norm=21.74655532836914, loss=33.15556716918945
I0916 06:32:02.506865 140135239943936 logging_writer.py:48] [3] global_step=3, grad_norm=26.36038589477539, loss=33.259674072265625
I0916 06:32:03.386118 140135248336640 logging_writer.py:48] [4] global_step=4, grad_norm=31.573192596435547, loss=32.15202331542969
I0916 06:32:04.272335 140135239943936 logging_writer.py:48] [5] global_step=5, grad_norm=32.90966796875, loss=30.998445510864258
I0916 06:32:05.168043 140135248336640 logging_writer.py:48] [6] global_step=6, grad_norm=36.0210075378418, loss=30.055261611938477
I0916 06:32:06.061228 140135239943936 logging_writer.py:48] [7] global_step=7, grad_norm=35.24161148071289, loss=28.863656997680664
I0916 06:32:06.975511 140135248336640 logging_writer.py:48] [8] global_step=8, grad_norm=34.31379318237305, loss=27.882841110229492
I0916 06:32:07.879294 140135239943936 logging_writer.py:48] [9] global_step=9, grad_norm=32.369422912597656, loss=26.133100509643555
I0916 06:32:08.779489 140135248336640 logging_writer.py:48] [10] global_step=10, grad_norm=33.694725036621094, loss=25.127355575561523
I0916 06:32:09.677868 140135239943936 logging_writer.py:48] [11] global_step=11, grad_norm=31.586929321289062, loss=22.879606246948242
I0916 06:32:10.585068 140135248336640 logging_writer.py:48] [12] global_step=12, grad_norm=30.807666778564453, loss=21.732934951782227
I0916 06:32:11.482237 140135239943936 logging_writer.py:48] [13] global_step=13, grad_norm=29.149311065673828, loss=20.15764045715332
I0916 06:32:12.412345 140135248336640 logging_writer.py:48] [14] global_step=14, grad_norm=27.801681518554688, loss=18.88558578491211
I0916 06:32:13.304394 140135239943936 logging_writer.py:48] [15] global_step=15, grad_norm=25.065460205078125, loss=17.19485855102539
I0916 06:32:14.216222 140135248336640 logging_writer.py:48] [16] global_step=16, grad_norm=22.366195678710938, loss=15.694480895996094
I0916 06:32:15.123869 140135239943936 logging_writer.py:48] [17] global_step=17, grad_norm=19.576120376586914, loss=14.594171524047852
I0916 06:32:16.027920 140135248336640 logging_writer.py:48] [18] global_step=18, grad_norm=16.75545883178711, loss=13.630980491638184
I0916 06:32:16.925711 140135239943936 logging_writer.py:48] [19] global_step=19, grad_norm=13.728848457336426, loss=12.403785705566406
I0916 06:32:17.828999 140135248336640 logging_writer.py:48] [20] global_step=20, grad_norm=10.867654800415039, loss=11.506407737731934
I0916 06:32:18.725147 140135239943936 logging_writer.py:48] [21] global_step=21, grad_norm=9.361469268798828, loss=11.005528450012207
I0916 06:32:19.632044 140135248336640 logging_writer.py:48] [22] global_step=22, grad_norm=7.42368221282959, loss=10.284213066101074
I0916 06:32:20.538656 140135239943936 logging_writer.py:48] [23] global_step=23, grad_norm=5.712471008300781, loss=9.519344329833984
I0916 06:32:21.455254 140135248336640 logging_writer.py:48] [24] global_step=24, grad_norm=5.6032185554504395, loss=9.138309478759766
I0916 06:32:22.366867 140135239943936 logging_writer.py:48] [25] global_step=25, grad_norm=4.873762607574463, loss=9.029728889465332
I0916 06:32:23.281161 140135248336640 logging_writer.py:48] [26] global_step=26, grad_norm=4.610563278198242, loss=8.490045547485352
I0916 06:32:24.183712 140135239943936 logging_writer.py:48] [27] global_step=27, grad_norm=3.6258811950683594, loss=8.419191360473633
I0916 06:32:25.089351 140135248336640 logging_writer.py:48] [28] global_step=28, grad_norm=2.9548065662384033, loss=7.915503025054932
I0916 06:32:25.994584 140135239943936 logging_writer.py:48] [29] global_step=29, grad_norm=2.545142412185669, loss=7.802140235900879
I0916 06:32:26.892724 140135248336640 logging_writer.py:48] [30] global_step=30, grad_norm=2.121767997741699, loss=7.468784809112549
I0916 06:32:27.796304 140135239943936 logging_writer.py:48] [31] global_step=31, grad_norm=4.188973426818848, loss=7.458128929138184
I0916 06:32:28.705092 140135248336640 logging_writer.py:48] [32] global_step=32, grad_norm=2.0265841484069824, loss=7.185906410217285
I0916 06:32:29.609919 140135239943936 logging_writer.py:48] [33] global_step=33, grad_norm=1.938328742980957, loss=7.080022811889648
I0916 06:32:30.519389 140135248336640 logging_writer.py:48] [34] global_step=34, grad_norm=1.7169721126556396, loss=6.935807704925537
I0916 06:32:31.408377 140135239943936 logging_writer.py:48] [35] global_step=35, grad_norm=1.7760554552078247, loss=6.890460014343262
I0916 06:32:32.308823 140135248336640 logging_writer.py:48] [36] global_step=36, grad_norm=1.6618828773498535, loss=6.7378926277160645
I0916 06:32:33.201232 140135239943936 logging_writer.py:48] [37] global_step=37, grad_norm=2.1811881065368652, loss=6.657671928405762
I0916 06:32:34.088308 140135248336640 logging_writer.py:48] [38] global_step=38, grad_norm=2.0892367362976074, loss=6.548975944519043
I0916 06:32:34.974866 140135239943936 logging_writer.py:48] [39] global_step=39, grad_norm=4.084691047668457, loss=6.495189189910889
I0916 06:32:35.863915 140135248336640 logging_writer.py:48] [40] global_step=40, grad_norm=10.27544116973877, loss=6.5413384437561035
I0916 06:32:36.741406 140135239943936 logging_writer.py:48] [41] global_step=41, grad_norm=4.265711307525635, loss=6.483477592468262
I0916 06:32:37.627885 140135248336640 logging_writer.py:48] [42] global_step=42, grad_norm=5.619570732116699, loss=6.491522312164307
I0916 06:32:38.519065 140135239943936 logging_writer.py:48] [43] global_step=43, grad_norm=1.067955493927002, loss=6.261562347412109
I0916 06:32:39.409081 140135248336640 logging_writer.py:48] [44] global_step=44, grad_norm=2.9593913555145264, loss=6.274159908294678
I0916 06:32:40.326978 140135239943936 logging_writer.py:48] [45] global_step=45, grad_norm=2.0715267658233643, loss=6.202216625213623
I0916 06:32:41.247160 140135248336640 logging_writer.py:48] [46] global_step=46, grad_norm=3.0114564895629883, loss=6.195947170257568
I0916 06:32:42.157703 140135239943936 logging_writer.py:48] [47] global_step=47, grad_norm=5.080845355987549, loss=6.21272087097168
I0916 06:32:43.068155 140135248336640 logging_writer.py:48] [48] global_step=48, grad_norm=5.72257137298584, loss=6.171292781829834
I0916 06:32:43.982744 140135239943936 logging_writer.py:48] [49] global_step=49, grad_norm=3.752105951309204, loss=6.1109466552734375
I0916 06:32:44.901567 140135248336640 logging_writer.py:48] [50] global_step=50, grad_norm=0.862831711769104, loss=6.054776191711426
I0916 06:32:45.811969 140135239943936 logging_writer.py:48] [51] global_step=51, grad_norm=0.4586222767829895, loss=6.054337501525879
I0916 06:32:46.712267 140135248336640 logging_writer.py:48] [52] global_step=52, grad_norm=0.6651936173439026, loss=6.02419376373291
I0916 06:32:47.619511 140135239943936 logging_writer.py:48] [53] global_step=53, grad_norm=1.746164083480835, loss=5.999786376953125
I0916 06:32:48.533538 140135248336640 logging_writer.py:48] [54] global_step=54, grad_norm=4.895439624786377, loss=6.030948162078857
I0916 06:32:49.440911 140135239943936 logging_writer.py:48] [55] global_step=55, grad_norm=6.485012531280518, loss=6.080233097076416
I0916 06:32:50.353445 140135248336640 logging_writer.py:48] [56] global_step=56, grad_norm=2.543574571609497, loss=5.99036979675293
I0916 06:32:51.258216 140135239943936 logging_writer.py:48] [57] global_step=57, grad_norm=0.39915731549263, loss=5.966156005859375
I0916 06:32:52.162540 140135248336640 logging_writer.py:48] [58] global_step=58, grad_norm=0.39297711849212646, loss=5.954315185546875
I0916 06:32:53.060621 140135239943936 logging_writer.py:48] [59] global_step=59, grad_norm=0.47380703687667847, loss=5.93831205368042
I0916 06:32:53.975369 140135248336640 logging_writer.py:48] [60] global_step=60, grad_norm=1.7157100439071655, loss=5.928008556365967
I0916 06:32:54.916488 140135239943936 logging_writer.py:48] [61] global_step=61, grad_norm=5.968181133270264, loss=5.993626594543457
I0916 06:32:55.825127 140135248336640 logging_writer.py:48] [62] global_step=62, grad_norm=8.14743423461914, loss=6.016180038452148
I0916 06:32:56.750212 140135239943936 logging_writer.py:48] [63] global_step=63, grad_norm=3.7693450450897217, loss=5.925586700439453
I0916 06:32:57.661414 140135248336640 logging_writer.py:48] [64] global_step=64, grad_norm=0.3913351893424988, loss=5.90464973449707
I0916 06:32:58.590973 140135239943936 logging_writer.py:48] [65] global_step=65, grad_norm=0.7930117249488831, loss=5.918282508850098
I0916 06:32:59.499647 140135248336640 logging_writer.py:48] [66] global_step=66, grad_norm=2.2974109649658203, loss=5.888065338134766
I0916 06:33:00.407424 140135239943936 logging_writer.py:48] [67] global_step=67, grad_norm=4.426229000091553, loss=5.937570571899414
I0916 06:33:01.307969 140135248336640 logging_writer.py:48] [68] global_step=68, grad_norm=5.906636714935303, loss=5.967480659484863
I0916 06:33:02.218357 140135239943936 logging_writer.py:48] [69] global_step=69, grad_norm=4.191201686859131, loss=5.9131574630737305
I0916 06:33:03.123057 140135248336640 logging_writer.py:48] [70] global_step=70, grad_norm=2.542513132095337, loss=5.91830587387085
I0916 06:33:04.036918 140135239943936 logging_writer.py:48] [71] global_step=71, grad_norm=2.2322516441345215, loss=5.933891296386719
I0916 06:33:04.939772 140135248336640 logging_writer.py:48] [72] global_step=72, grad_norm=3.438110113143921, loss=5.896919250488281
I0916 06:33:05.837426 140135239943936 logging_writer.py:48] [73] global_step=73, grad_norm=5.4100661277771, loss=5.944982051849365
I0916 06:33:06.745902 140135248336640 logging_writer.py:48] [74] global_step=74, grad_norm=4.050600051879883, loss=5.892498970031738
I0916 06:33:07.642870 140135239943936 logging_writer.py:48] [75] global_step=75, grad_norm=1.3967074155807495, loss=5.878961086273193
I0916 06:33:08.543514 140135248336640 logging_writer.py:48] [76] global_step=76, grad_norm=1.2415931224822998, loss=5.8543829917907715
I0916 06:33:09.456745 140135239943936 logging_writer.py:48] [77] global_step=77, grad_norm=2.884507656097412, loss=5.870456695556641
I0916 06:33:10.362045 140135248336640 logging_writer.py:48] [78] global_step=78, grad_norm=5.382267951965332, loss=5.903940200805664
I0916 06:33:11.261928 140135239943936 logging_writer.py:48] [79] global_step=79, grad_norm=4.572612762451172, loss=5.891970157623291
I0916 06:33:12.158287 140135248336640 logging_writer.py:48] [80] global_step=80, grad_norm=2.3453948497772217, loss=5.866875171661377
I0916 06:33:13.078711 140135239943936 logging_writer.py:48] [81] global_step=81, grad_norm=2.232649087905884, loss=5.853705406188965
I0916 06:33:13.986747 140135248336640 logging_writer.py:48] [82] global_step=82, grad_norm=2.9764368534088135, loss=5.867697715759277
I0916 06:33:14.886493 140135239943936 logging_writer.py:48] [83] global_step=83, grad_norm=3.719782590866089, loss=5.865990161895752
I0916 06:33:15.787529 140135248336640 logging_writer.py:48] [84] global_step=84, grad_norm=3.8583984375, loss=5.861705303192139
I0916 06:33:16.695559 140135239943936 logging_writer.py:48] [85] global_step=85, grad_norm=2.9007813930511475, loss=5.8244853019714355
I0916 06:33:17.601065 140135248336640 logging_writer.py:48] [86] global_step=86, grad_norm=2.674896478652954, loss=5.839079856872559
I0916 06:33:18.508391 140135239943936 logging_writer.py:48] [87] global_step=87, grad_norm=3.876844644546509, loss=5.850848197937012
I0916 06:33:19.406107 140135248336640 logging_writer.py:48] [88] global_step=88, grad_norm=4.004429340362549, loss=5.845874309539795
I0916 06:33:20.321815 140135239943936 logging_writer.py:48] [89] global_step=89, grad_norm=2.9592394828796387, loss=5.854176044464111
I0916 06:33:21.221953 140135248336640 logging_writer.py:48] [90] global_step=90, grad_norm=2.7795891761779785, loss=5.860976219177246
I0916 06:33:22.134773 140135239943936 logging_writer.py:48] [91] global_step=91, grad_norm=2.490812063217163, loss=5.8123393058776855
I0916 06:33:23.038392 140135248336640 logging_writer.py:48] [92] global_step=92, grad_norm=2.382561683654785, loss=5.832934379577637
I0916 06:33:23.938987 140135239943936 logging_writer.py:48] [93] global_step=93, grad_norm=4.040912628173828, loss=5.862113952636719
I0916 06:33:24.847009 140135248336640 logging_writer.py:48] [94] global_step=94, grad_norm=5.071630954742432, loss=5.862185955047607
I0916 06:33:25.765716 140135239943936 logging_writer.py:48] [95] global_step=95, grad_norm=3.1186909675598145, loss=5.842339515686035
I0916 06:33:26.667637 140135248336640 logging_writer.py:48] [96] global_step=96, grad_norm=2.0204532146453857, loss=5.810847282409668
I0916 06:33:27.571227 140135239943936 logging_writer.py:48] [97] global_step=97, grad_norm=2.3881442546844482, loss=5.814142227172852
I0916 06:33:28.469552 140135248336640 logging_writer.py:48] [98] global_step=98, grad_norm=2.609503984451294, loss=5.820002555847168
I0916 06:33:29.361116 140135239943936 logging_writer.py:48] [99] global_step=99, grad_norm=4.115164279937744, loss=5.850419521331787
I0916 06:33:30.252086 140135248336640 logging_writer.py:48] [100] global_step=100, grad_norm=4.798054218292236, loss=5.84860897064209
I0916 06:38:34.947533 140135239943936 logging_writer.py:48] [500] global_step=500, grad_norm=2.5860164165496826, loss=3.0665712356567383
I0916 06:45:35.961647 140135248336640 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.70665979385376, loss=2.5569610595703125
I0916 06:52:14.858463 140135937267456 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.52518367767334, loss=2.351041793823242
I0916 06:55:52.107426 140294737168192 spec.py:320] Evaluating on the training split.
I0916 06:56:45.679331 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 06:57:36.425117 140294737168192 spec.py:348] Evaluating on the test split.
I0916 06:58:01.980377 140294737168192 submission_runner.py:376] Time since start: 1867.91s, 	Step: 1756, 	{'train/ctc_loss': Array(0.8596461, dtype=float32), 'train/wer': 0.26545797202941235, 'validation/ctc_loss': Array(1.3066759, dtype=float32), 'validation/wer': 0.34854171289641, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9163939, dtype=float32), 'test/wer': 0.27723275039099793, 'test/num_examples': 2472, 'score': 1500.5740060806274, 'total_duration': 1867.9097166061401, 'accumulated_submission_time': 1500.5740060806274, 'accumulated_eval_time': 367.2605299949646, 'accumulated_logging_time': 0.041959285736083984}
I0916 06:58:02.013087 140135937267456 logging_writer.py:48] [1756] accumulated_eval_time=367.260530, accumulated_logging_time=0.041959, accumulated_submission_time=1500.574006, global_step=1756, preemption_count=0, score=1500.574006, test/ctc_loss=0.9163938760757446, test/num_examples=2472, test/wer=0.277233, total_duration=1867.909717, train/ctc_loss=0.8596460819244385, train/wer=0.265458, validation/ctc_loss=1.306675910949707, validation/num_examples=5348, validation/wer=0.348542
I0916 07:01:09.465689 140135928874752 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.9354817867279053, loss=2.224385976791382
I0916 07:07:42.511173 140136592627456 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.9928730726242065, loss=2.0398170948028564
I0916 07:14:53.936274 140136584234752 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.186908483505249, loss=2.034731864929199
I0916 07:21:35.131377 140136592627456 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.378170967102051, loss=2.057156562805176
I0916 07:22:02.727077 140294737168192 spec.py:320] Evaluating on the training split.
I0916 07:22:55.343941 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 07:23:46.219917 140294737168192 spec.py:348] Evaluating on the test split.
I0916 07:24:11.650670 140294737168192 submission_runner.py:376] Time since start: 3437.58s, 	Step: 3534, 	{'train/ctc_loss': Array(0.60077345, dtype=float32), 'train/wer': 0.1950563354020723, 'validation/ctc_loss': Array(0.97139984, dtype=float32), 'validation/wer': 0.2691777055253789, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.63538104, dtype=float32), 'test/wer': 0.19838319826132877, 'test/num_examples': 2472, 'score': 2941.2438254356384, 'total_duration': 3437.5800545215607, 'accumulated_submission_time': 2941.2438254356384, 'accumulated_eval_time': 496.18132400512695, 'accumulated_logging_time': 0.08536624908447266}
I0916 07:24:11.680351 140136592627456 logging_writer.py:48] [3534] accumulated_eval_time=496.181324, accumulated_logging_time=0.085366, accumulated_submission_time=2941.243825, global_step=3534, preemption_count=0, score=2941.243825, test/ctc_loss=0.6353810429573059, test/num_examples=2472, test/wer=0.198383, total_duration=3437.580055, train/ctc_loss=0.6007734537124634, train/wer=0.195056, validation/ctc_loss=0.9713998436927795, validation/num_examples=5348, validation/wer=0.269178
I0916 07:30:29.456114 140136584234752 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.7151248455047607, loss=2.0227057933807373
I0916 07:37:16.755290 140136264947456 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.327212810516357, loss=1.941108226776123
I0916 07:44:28.424281 140136256554752 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.9077770709991455, loss=1.950264811515808
I0916 07:48:12.397766 140294737168192 spec.py:320] Evaluating on the training split.
I0916 07:49:05.699836 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 07:49:54.595818 140294737168192 spec.py:348] Evaluating on the test split.
I0916 07:50:19.955932 140294737168192 submission_runner.py:376] Time since start: 5005.89s, 	Step: 5271, 	{'train/ctc_loss': Array(0.5372288, dtype=float32), 'train/wer': 0.1793886490503977, 'validation/ctc_loss': Array(0.9332573, dtype=float32), 'validation/wer': 0.2599542687339, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5952497, dtype=float32), 'test/wer': 0.186825909450978, 'test/num_examples': 2472, 'score': 4381.916770935059, 'total_duration': 5005.885439872742, 'accumulated_submission_time': 4381.916770935059, 'accumulated_eval_time': 623.7370131015778, 'accumulated_logging_time': 0.1269986629486084}
I0916 07:50:19.983963 140136264947456 logging_writer.py:48] [5271] accumulated_eval_time=623.737013, accumulated_logging_time=0.126999, accumulated_submission_time=4381.916771, global_step=5271, preemption_count=0, score=4381.916771, test/ctc_loss=0.5952497124671936, test/num_examples=2472, test/wer=0.186826, total_duration=5005.885440, train/ctc_loss=0.5372288227081299, train/wer=0.179389, validation/ctc_loss=0.9332572817802429, validation/num_examples=5348, validation/wer=0.259954
I0916 07:53:14.213861 140136256554752 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.189107656478882, loss=1.896565318107605
I0916 08:00:12.746084 140136264947456 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.3276920318603516, loss=1.89736008644104
I0916 08:07:03.506978 140136264947456 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.1938376426696777, loss=1.8317445516586304
I0916 08:14:20.287193 140136256554752 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.273345470428467, loss=2.0205752849578857
I0916 08:14:20.294898 140294737168192 spec.py:320] Evaluating on the training split.
I0916 08:15:19.154291 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 08:16:09.791265 140294737168192 spec.py:348] Evaluating on the test split.
I0916 08:16:35.134001 140294737168192 submission_runner.py:376] Time since start: 6581.06s, 	Step: 7001, 	{'train/ctc_loss': Array(0.5345886, dtype=float32), 'train/wer': 0.1722135533935846, 'validation/ctc_loss': Array(0.88570625, dtype=float32), 'validation/wer': 0.24632172042180822, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5725201, dtype=float32), 'test/wer': 0.17762476387788678, 'test/num_examples': 2472, 'score': 5822.183131694794, 'total_duration': 6581.063181877136, 'accumulated_submission_time': 5822.183131694794, 'accumulated_eval_time': 758.5730748176575, 'accumulated_logging_time': 0.16733717918395996}
I0916 08:16:35.162517 140136377587456 logging_writer.py:48] [7001] accumulated_eval_time=758.573075, accumulated_logging_time=0.167337, accumulated_submission_time=5822.183132, global_step=7001, preemption_count=0, score=5822.183132, test/ctc_loss=0.5725200772285461, test/num_examples=2472, test/wer=0.177625, total_duration=6581.063182, train/ctc_loss=0.5345885753631592, train/wer=0.172214, validation/ctc_loss=0.8857062458992004, validation/num_examples=5348, validation/wer=0.246322
I0916 08:23:01.984101 140136377587456 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.069828987121582, loss=1.8651010990142822
I0916 08:30:14.592760 140136369194752 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.559252738952637, loss=1.9432909488677979
I0916 08:37:07.123675 140136377587456 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.493928909301758, loss=1.9440380334854126
I0916 08:40:35.721236 140294737168192 spec.py:320] Evaluating on the training split.
I0916 08:41:29.946315 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 08:42:20.560744 140294737168192 spec.py:348] Evaluating on the test split.
I0916 08:42:45.531764 140294737168192 submission_runner.py:376] Time since start: 8151.45s, 	Step: 8749, 	{'train/ctc_loss': Array(0.5283293, dtype=float32), 'train/wer': 0.17110069799299832, 'validation/ctc_loss': Array(0.8690057, dtype=float32), 'validation/wer': 0.2435045200629046, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5620601, dtype=float32), 'test/wer': 0.17315621635894624, 'test/num_examples': 2472, 'score': 7262.696113586426, 'total_duration': 8151.453425168991, 'accumulated_submission_time': 7262.696113586426, 'accumulated_eval_time': 888.3731138706207, 'accumulated_logging_time': 0.20923757553100586}
I0916 08:42:45.567104 140136377587456 logging_writer.py:48] [8749] accumulated_eval_time=888.373114, accumulated_logging_time=0.209238, accumulated_submission_time=7262.696114, global_step=8749, preemption_count=0, score=7262.696114, test/ctc_loss=0.5620601177215576, test/num_examples=2472, test/wer=0.173156, total_duration=8151.453425, train/ctc_loss=0.5283293128013611, train/wer=0.171101, validation/ctc_loss=0.8690056800842285, validation/num_examples=5348, validation/wer=0.243505
I0916 08:45:56.546066 140136369194752 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.335582256317139, loss=1.8551839590072632
I0916 08:52:54.951895 140136377587456 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.835801601409912, loss=1.9468270540237427
I0916 08:59:56.172843 140136369194752 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.805788278579712, loss=1.8909475803375244
I0916 09:06:45.565628 140294737168192 spec.py:320] Evaluating on the training split.
I0916 09:07:40.894752 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 09:08:32.338733 140294737168192 spec.py:348] Evaluating on the test split.
I0916 09:08:58.234676 140294737168192 submission_runner.py:376] Time since start: 9724.16s, 	Step: 10487, 	{'train/ctc_loss': Array(0.5544736, dtype=float32), 'train/wer': 0.17917215699036015, 'validation/ctc_loss': Array(0.8799448, dtype=float32), 'validation/wer': 0.24462368184931838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55379397, dtype=float32), 'test/wer': 0.17530924379989032, 'test/num_examples': 2472, 'score': 8702.642138004303, 'total_duration': 9724.160254478455, 'accumulated_submission_time': 8702.642138004303, 'accumulated_eval_time': 1021.0357594490051, 'accumulated_logging_time': 0.26417994499206543}
I0916 09:08:58.272369 140136377587456 logging_writer.py:48] [10487] accumulated_eval_time=1021.035759, accumulated_logging_time=0.264180, accumulated_submission_time=8702.642138, global_step=10487, preemption_count=0, score=8702.642138, test/ctc_loss=0.5537939667701721, test/num_examples=2472, test/wer=0.175309, total_duration=9724.160254, train/ctc_loss=0.5544735789299011, train/wer=0.179172, validation/ctc_loss=0.8799448013305664, validation/num_examples=5348, validation/wer=0.244624
I0916 09:09:08.880985 140136369194752 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.2116262912750244, loss=1.7913049459457397
I0916 09:15:48.806962 140136377587456 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.6079530715942383, loss=1.8500818014144897
I0916 09:22:53.013590 140136377587456 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.0508930683135986, loss=1.854394555091858
I0916 09:29:46.118415 140136369194752 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.290082931518555, loss=1.8536574840545654
I0916 09:32:58.686129 140294737168192 spec.py:320] Evaluating on the training split.
I0916 09:33:56.790945 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 09:34:48.770858 140294737168192 spec.py:348] Evaluating on the test split.
I0916 09:35:14.086247 140294737168192 submission_runner.py:376] Time since start: 11300.01s, 	Step: 12220, 	{'train/ctc_loss': Array(0.5008965, dtype=float32), 'train/wer': 0.1613808946461617, 'validation/ctc_loss': Array(0.8313743, dtype=float32), 'validation/wer': 0.2316954336269525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52456063, dtype=float32), 'test/wer': 0.1624317023134889, 'test/num_examples': 2472, 'score': 10143.005124807358, 'total_duration': 11300.010608911514, 'accumulated_submission_time': 10143.005124807358, 'accumulated_eval_time': 1156.42809176445, 'accumulated_logging_time': 0.3197810649871826}
I0916 09:35:14.120954 140136377587456 logging_writer.py:48] [12220] accumulated_eval_time=1156.428092, accumulated_logging_time=0.319781, accumulated_submission_time=10143.005125, global_step=12220, preemption_count=0, score=10143.005125, test/ctc_loss=0.5245606303215027, test/num_examples=2472, test/wer=0.162432, total_duration=11300.010609, train/ctc_loss=0.5008965134620667, train/wer=0.161381, validation/ctc_loss=0.8313742876052856, validation/num_examples=5348, validation/wer=0.231695
I0916 09:38:50.474454 140136377587456 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.381886005401611, loss=1.8349558115005493
I0916 09:45:45.770795 140136369194752 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.9050116539001465, loss=1.8171639442443848
I0916 09:53:02.044129 140136377587456 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.8735499382019043, loss=1.8287538290023804
I0916 09:59:14.316118 140294737168192 spec.py:320] Evaluating on the training split.
I0916 10:00:09.319227 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 10:00:59.654454 140294737168192 spec.py:348] Evaluating on the test split.
I0916 10:01:25.886623 140294737168192 submission_runner.py:376] Time since start: 12871.81s, 	Step: 13954, 	{'train/ctc_loss': Array(0.56000787, dtype=float32), 'train/wer': 0.1573363842013573, 'validation/ctc_loss': Array(1.129067, dtype=float32), 'validation/wer': 0.2413626759544231, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71745837, dtype=float32), 'test/wer': 0.17037352994942417, 'test/num_examples': 2472, 'score': 11583.152604341507, 'total_duration': 12871.812179803848, 'accumulated_submission_time': 11583.152604341507, 'accumulated_eval_time': 1287.9920001029968, 'accumulated_logging_time': 0.36974382400512695}
I0916 10:01:25.928352 140135870707456 logging_writer.py:48] [13954] accumulated_eval_time=1287.992000, accumulated_logging_time=0.369744, accumulated_submission_time=11583.152604, global_step=13954, preemption_count=0, score=11583.152604, test/ctc_loss=0.7174583673477173, test/num_examples=2472, test/wer=0.170374, total_duration=12871.812180, train/ctc_loss=0.5600078701972961, train/wer=0.157336, validation/ctc_loss=1.1290669441223145, validation/num_examples=5348, validation/wer=0.241363
I0916 10:02:01.602071 140135862314752 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.0906691551208496, loss=1.7831109762191772
I0916 10:08:56.851461 140135870707456 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.7834177017211914, loss=1.8126804828643799
I0916 10:15:48.208174 140135862314752 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.999389886856079, loss=1.776972770690918
I0916 10:23:13.551467 140135870707456 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.2321135997772217, loss=1.7527128458023071
I0916 10:25:26.506930 140294737168192 spec.py:320] Evaluating on the training split.
I0916 10:26:21.237761 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 10:27:11.875552 140294737168192 spec.py:348] Evaluating on the test split.
I0916 10:27:37.953485 140294737168192 submission_runner.py:376] Time since start: 14443.88s, 	Step: 15677, 	{'train/ctc_loss': Array(0.42258114, dtype=float32), 'train/wer': 0.1376875974229793, 'validation/ctc_loss': Array(0.7847165, dtype=float32), 'validation/wer': 0.21840056344007178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49246633, dtype=float32), 'test/wer': 0.15355554201450247, 'test/num_examples': 2472, 'score': 13023.676251649857, 'total_duration': 14443.880277395248, 'accumulated_submission_time': 13023.676251649857, 'accumulated_eval_time': 1419.433171749115, 'accumulated_logging_time': 0.4338796138763428}
I0916 10:27:37.987053 140135578867456 logging_writer.py:48] [15677] accumulated_eval_time=1419.433172, accumulated_logging_time=0.433880, accumulated_submission_time=13023.676252, global_step=15677, preemption_count=0, score=13023.676252, test/ctc_loss=0.4924663305282593, test/num_examples=2472, test/wer=0.153556, total_duration=14443.880277, train/ctc_loss=0.42258113622665405, train/wer=0.137688, validation/ctc_loss=0.7847164869308472, validation/num_examples=5348, validation/wer=0.218401
I0916 10:31:48.594368 140135570474752 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.3230981826782227, loss=1.7801700830459595
I0916 10:39:13.882383 140135578867456 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.7742016315460205, loss=1.7798508405685425
I0916 10:45:56.576240 140135570474752 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.3923652172088623, loss=1.7657815217971802
I0916 10:51:38.198732 140294737168192 spec.py:320] Evaluating on the training split.
I0916 10:52:34.134519 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 10:53:24.446354 140294737168192 spec.py:348] Evaluating on the test split.
I0916 10:53:50.735806 140294737168192 submission_runner.py:376] Time since start: 16016.66s, 	Step: 17381, 	{'train/ctc_loss': Array(0.43677148, dtype=float32), 'train/wer': 0.14792429759689382, 'validation/ctc_loss': Array(0.76784194, dtype=float32), 'validation/wer': 0.21754189620739225, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47907305, dtype=float32), 'test/wer': 0.14994008084008692, 'test/num_examples': 2472, 'score': 14463.842086791992, 'total_duration': 16016.661360740662, 'accumulated_submission_time': 14463.842086791992, 'accumulated_eval_time': 1551.9636483192444, 'accumulated_logging_time': 0.4814188480377197}
I0916 10:53:50.773620 140136085747456 logging_writer.py:48] [17381] accumulated_eval_time=1551.963648, accumulated_logging_time=0.481419, accumulated_submission_time=14463.842087, global_step=17381, preemption_count=0, score=14463.842087, test/ctc_loss=0.47907304763793945, test/num_examples=2472, test/wer=0.149940, total_duration=16016.661361, train/ctc_loss=0.4367714822292328, train/wer=0.147924, validation/ctc_loss=0.7678419351577759, validation/num_examples=5348, validation/wer=0.217542
I0916 10:55:21.615617 140136077354752 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.504020094871521, loss=1.7400680780410767
I0916 11:02:01.032022 140136085747456 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.7940236330032349, loss=1.730913519859314
I0916 11:09:28.566834 140136077354752 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.7525389194488525, loss=1.7374250888824463
I0916 11:16:08.947959 140134923507456 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.140211820602417, loss=1.7563713788986206
I0916 11:17:51.485147 140294737168192 spec.py:320] Evaluating on the training split.
I0916 11:18:46.925789 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 11:19:38.532147 140294737168192 spec.py:348] Evaluating on the test split.
I0916 11:20:04.479471 140294737168192 submission_runner.py:376] Time since start: 17590.41s, 	Step: 19117, 	{'train/ctc_loss': Array(0.43248543, dtype=float32), 'train/wer': 0.14020465903381898, 'validation/ctc_loss': Array(0.7409395, dtype=float32), 'validation/wer': 0.20784571004061786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46415368, dtype=float32), 'test/wer': 0.14689334389535474, 'test/num_examples': 2472, 'score': 15904.504876613617, 'total_duration': 17590.40542125702, 'accumulated_submission_time': 15904.504876613617, 'accumulated_eval_time': 1684.9517471790314, 'accumulated_logging_time': 0.5358102321624756}
I0916 11:20:04.514370 140135215347456 logging_writer.py:48] [19117] accumulated_eval_time=1684.951747, accumulated_logging_time=0.535810, accumulated_submission_time=15904.504877, global_step=19117, preemption_count=0, score=15904.504877, test/ctc_loss=0.4641536772251129, test/num_examples=2472, test/wer=0.146893, total_duration=17590.405421, train/ctc_loss=0.432485431432724, train/wer=0.140205, validation/ctc_loss=0.7409394979476929, validation/num_examples=5348, validation/wer=0.207846
I0916 11:25:17.013739 140135206954752 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.054690837860107, loss=1.6800456047058105
I0916 11:32:00.511152 140134887667456 logging_writer.py:48] [20000] global_step=20000, grad_norm=4.097467422485352, loss=1.8249796628952026
I0916 11:39:22.823627 140134879274752 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.154728412628174, loss=1.7002307176589966
I0916 11:44:05.069411 140294737168192 spec.py:320] Evaluating on the training split.
I0916 11:45:01.563882 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 11:45:51.731351 140294737168192 spec.py:348] Evaluating on the test split.
I0916 11:46:17.587648 140294737168192 submission_runner.py:376] Time since start: 19163.51s, 	Step: 20851, 	{'train/ctc_loss': Array(0.43682304, dtype=float32), 'train/wer': 0.14068114068641677, 'validation/ctc_loss': Array(0.7387269, dtype=float32), 'validation/wer': 0.20792289361209468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4537489, dtype=float32), 'test/wer': 0.14130765949667906, 'test/num_examples': 2472, 'score': 17345.011806488037, 'total_duration': 19163.514149665833, 'accumulated_submission_time': 17345.011806488037, 'accumulated_eval_time': 1817.4645256996155, 'accumulated_logging_time': 0.5855922698974609}
I0916 11:46:17.620478 140135430387456 logging_writer.py:48] [20851] accumulated_eval_time=1817.464526, accumulated_logging_time=0.585592, accumulated_submission_time=17345.011806, global_step=20851, preemption_count=0, score=17345.011806, test/ctc_loss=0.4537489116191864, test/num_examples=2472, test/wer=0.141308, total_duration=19163.514150, train/ctc_loss=0.4368230402469635, train/wer=0.140681, validation/ctc_loss=0.7387269139289856, validation/num_examples=5348, validation/wer=0.207923
I0916 11:48:11.360614 140135421994752 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.707923412322998, loss=1.6739585399627686
I0916 11:55:15.823044 140135430387456 logging_writer.py:48] [21500] global_step=21500, grad_norm=5.050450801849365, loss=1.768772840499878
I0916 12:02:02.724628 140134775027456 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.0870587825775146, loss=1.6964436769485474
I0916 12:09:28.693154 140134766634752 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.9787178039550781, loss=1.6670560836791992
I0916 12:10:17.938605 140294737168192 spec.py:320] Evaluating on the training split.
I0916 12:11:14.765731 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 12:12:06.804780 140294737168192 spec.py:348] Evaluating on the test split.
I0916 12:12:32.798282 140294737168192 submission_runner.py:376] Time since start: 20738.72s, 	Step: 22557, 	{'train/ctc_loss': Array(0.42392, dtype=float32), 'train/wer': 0.13699454676407039, 'validation/ctc_loss': Array(0.7202905, dtype=float32), 'validation/wer': 0.20535653986049068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4340122, dtype=float32), 'test/wer': 0.14075924684662727, 'test/num_examples': 2472, 'score': 18785.282359600067, 'total_duration': 20738.723254680634, 'accumulated_submission_time': 18785.282359600067, 'accumulated_eval_time': 1952.3170020580292, 'accumulated_logging_time': 0.6342990398406982}
I0916 12:12:32.833386 140134775027456 logging_writer.py:48] [22557] accumulated_eval_time=1952.317002, accumulated_logging_time=0.634299, accumulated_submission_time=18785.282360, global_step=22557, preemption_count=0, score=18785.282360, test/ctc_loss=0.43401220440864563, test/num_examples=2472, test/wer=0.140759, total_duration=20738.723255, train/ctc_loss=0.42392000555992126, train/wer=0.136995, validation/ctc_loss=0.72029048204422, validation/num_examples=5348, validation/wer=0.205357
I0916 12:18:20.875789 140134775027456 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.6101856231689453, loss=1.695725917816162
I0916 12:25:44.994262 140134766634752 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.175509452819824, loss=1.6519168615341187
I0916 12:32:35.798996 140135430387456 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.8117772340774536, loss=1.629374384880066
I0916 12:36:32.792359 140294737168192 spec.py:320] Evaluating on the training split.
I0916 12:37:28.703635 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 12:38:18.790658 140294737168192 spec.py:348] Evaluating on the test split.
I0916 12:38:44.655421 140294737168192 submission_runner.py:376] Time since start: 22310.58s, 	Step: 24275, 	{'train/ctc_loss': Array(0.39008844, dtype=float32), 'train/wer': 0.12842574215615896, 'validation/ctc_loss': Array(0.6992267, dtype=float32), 'validation/wer': 0.19743557583768295, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4261716, dtype=float32), 'test/wer': 0.1348079540145837, 'test/num_examples': 2472, 'score': 20225.19295024872, 'total_duration': 22310.58238697052, 'accumulated_submission_time': 20225.19295024872, 'accumulated_eval_time': 2084.1748735904694, 'accumulated_logging_time': 0.685528039932251}
I0916 12:38:44.691237 140135430387456 logging_writer.py:48] [24275] accumulated_eval_time=2084.174874, accumulated_logging_time=0.685528, accumulated_submission_time=20225.192950, global_step=24275, preemption_count=0, score=20225.192950, test/ctc_loss=0.42617160081863403, test/num_examples=2472, test/wer=0.134808, total_duration=22310.582387, train/ctc_loss=0.39008843898773193, train/wer=0.128426, validation/ctc_loss=0.6992266774177551, validation/num_examples=5348, validation/wer=0.197436
I0916 12:41:36.313587 140135421994752 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.807340621948242, loss=1.7221964597702026
I0916 12:48:25.820422 140134775027456 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.753690242767334, loss=1.62714684009552
I0916 12:55:38.867757 140134766634752 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.7337489128112793, loss=1.6622843742370605
I0916 13:02:35.705061 140136085747456 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.301501512527466, loss=1.6067074537277222
I0916 13:02:44.946773 140294737168192 spec.py:320] Evaluating on the training split.
I0916 13:03:41.217427 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 13:04:32.498850 140294737168192 spec.py:348] Evaluating on the test split.
I0916 13:04:58.540270 140294737168192 submission_runner.py:376] Time since start: 23884.47s, 	Step: 26013, 	{'train/ctc_loss': Array(0.374774, dtype=float32), 'train/wer': 0.12670127024700698, 'validation/ctc_loss': Array(0.70082295, dtype=float32), 'validation/wer': 0.19887311985643855, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42585066, dtype=float32), 'test/wer': 0.13592509089431884, 'test/num_examples': 2472, 'score': 21665.395996570587, 'total_duration': 23884.467129945755, 'accumulated_submission_time': 21665.395996570587, 'accumulated_eval_time': 2217.7630529403687, 'accumulated_logging_time': 0.7409267425537109}
I0916 13:04:58.573415 140136085747456 logging_writer.py:48] [26013] accumulated_eval_time=2217.763053, accumulated_logging_time=0.740927, accumulated_submission_time=21665.395997, global_step=26013, preemption_count=0, score=21665.395997, test/ctc_loss=0.42585065960884094, test/num_examples=2472, test/wer=0.135925, total_duration=23884.467130, train/ctc_loss=0.3747740089893341, train/wer=0.126701, validation/ctc_loss=0.7008229494094849, validation/num_examples=5348, validation/wer=0.198873
I0916 13:11:40.248314 140136077354752 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.530810594558716, loss=1.6907049417495728
I0916 13:18:43.694253 140135430387456 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.5356178283691406, loss=1.5834455490112305
I0916 13:25:52.590921 140135421994752 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.9663782119750977, loss=1.6536462306976318
I0916 13:28:58.540758 140294737168192 spec.py:320] Evaluating on the training split.
I0916 13:29:55.397677 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 13:30:46.382571 140294737168192 spec.py:348] Evaluating on the test split.
I0916 13:31:12.334488 140294737168192 submission_runner.py:376] Time since start: 25458.26s, 	Step: 27709, 	{'train/ctc_loss': Array(0.35208127, dtype=float32), 'train/wer': 0.11634329197819394, 'validation/ctc_loss': Array(0.67099357, dtype=float32), 'validation/wer': 0.1902382077974703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40303847, dtype=float32), 'test/wer': 0.12824731379359372, 'test/num_examples': 2472, 'score': 23105.315306425095, 'total_duration': 25458.258945941925, 'accumulated_submission_time': 23105.315306425095, 'accumulated_eval_time': 2351.5491061210632, 'accumulated_logging_time': 0.7900862693786621}
I0916 13:31:12.369308 140135430387456 logging_writer.py:48] [27709] accumulated_eval_time=2351.549106, accumulated_logging_time=0.790086, accumulated_submission_time=23105.315306, global_step=27709, preemption_count=0, score=23105.315306, test/ctc_loss=0.40303847193717957, test/num_examples=2472, test/wer=0.128247, total_duration=25458.258946, train/ctc_loss=0.3520812690258026, train/wer=0.116343, validation/ctc_loss=0.6709935665130615, validation/num_examples=5348, validation/wer=0.190238
I0916 13:34:57.634527 140135430387456 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.1520349979400635, loss=1.6656301021575928
I0916 13:42:02.914596 140135421994752 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.330890417098999, loss=1.601561427116394
I0916 13:49:19.149860 140136085747456 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.8526300191879272, loss=1.6109530925750732
I0916 13:55:12.528331 140294737168192 spec.py:320] Evaluating on the training split.
I0916 13:56:07.477854 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 13:56:58.133224 140294737168192 spec.py:348] Evaluating on the test split.
I0916 13:57:24.089980 140294737168192 submission_runner.py:376] Time since start: 27030.02s, 	Step: 29426, 	{'train/ctc_loss': Array(0.3624501, dtype=float32), 'train/wer': 0.12007063615877968, 'validation/ctc_loss': Array(0.6431861, dtype=float32), 'validation/wer': 0.1825198506497892, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38469446, dtype=float32), 'test/wer': 0.12215383990412934, 'test/num_examples': 2472, 'score': 24545.426859378815, 'total_duration': 27030.016496658325, 'accumulated_submission_time': 24545.426859378815, 'accumulated_eval_time': 2483.105142593384, 'accumulated_logging_time': 0.8398141860961914}
I0916 13:57:24.135438 140136085747456 logging_writer.py:48] [29426] accumulated_eval_time=2483.105143, accumulated_logging_time=0.839814, accumulated_submission_time=24545.426859, global_step=29426, preemption_count=0, score=24545.426859, test/ctc_loss=0.3846944570541382, test/num_examples=2472, test/wer=0.122154, total_duration=27030.016497, train/ctc_loss=0.36245009303092957, train/wer=0.120071, validation/ctc_loss=0.643186092376709, validation/num_examples=5348, validation/wer=0.182520
I0916 13:58:20.838080 140136077354752 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.9625985622406006, loss=1.5990612506866455
I0916 14:05:14.177407 140136085747456 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.6231472492218018, loss=1.5819898843765259
I0916 14:12:12.140410 140136077354752 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.712785005569458, loss=1.531735897064209
I0916 14:19:34.849105 140135430387456 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.0489485263824463, loss=1.540326714515686
I0916 14:21:24.826945 140294737168192 spec.py:320] Evaluating on the training split.
I0916 14:22:19.971944 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 14:23:10.394217 140294737168192 spec.py:348] Evaluating on the test split.
I0916 14:23:36.062993 140294737168192 submission_runner.py:376] Time since start: 28601.99s, 	Step: 31145, 	{'train/ctc_loss': Array(0.33710065, dtype=float32), 'train/wer': 0.11115632748956858, 'validation/ctc_loss': Array(0.6294584, dtype=float32), 'validation/wer': 0.18012715993400805, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37788603, dtype=float32), 'test/wer': 0.12164605041334065, 'test/num_examples': 2472, 'score': 25986.060956954956, 'total_duration': 28601.989730596542, 'accumulated_submission_time': 25986.060956954956, 'accumulated_eval_time': 2614.3357541561127, 'accumulated_logging_time': 0.9099061489105225}
I0916 14:23:36.100182 140135793907456 logging_writer.py:48] [31145] accumulated_eval_time=2614.335754, accumulated_logging_time=0.909906, accumulated_submission_time=25986.060957, global_step=31145, preemption_count=0, score=25986.060957, test/ctc_loss=0.377886027097702, test/num_examples=2472, test/wer=0.121646, total_duration=28601.989731, train/ctc_loss=0.33710065484046936, train/wer=0.111156, validation/ctc_loss=0.6294584274291992, validation/num_examples=5348, validation/wer=0.180127
I0916 14:28:20.009044 140135785514752 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.427427291870117, loss=1.5548235177993774
I0916 14:35:44.400190 140135793907456 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.7603418827056885, loss=1.5862510204315186
I0916 14:42:29.423115 140135785514752 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.8052403926849365, loss=1.5976206064224243
I0916 14:47:36.752958 140294737168192 spec.py:320] Evaluating on the training split.
I0916 14:48:33.725370 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 14:49:25.438760 140294737168192 spec.py:348] Evaluating on the test split.
I0916 14:49:51.198301 140294737168192 submission_runner.py:376] Time since start: 30177.12s, 	Step: 32846, 	{'train/ctc_loss': Array(0.34894836, dtype=float32), 'train/wer': 0.11293015323446466, 'validation/ctc_loss': Array(0.61463034, dtype=float32), 'validation/wer': 0.17524529903809974, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36460468, dtype=float32), 'test/wer': 0.11581662705908638, 'test/num_examples': 2472, 'score': 27426.663048267365, 'total_duration': 30177.122668981552, 'accumulated_submission_time': 27426.663048267365, 'accumulated_eval_time': 2748.773370742798, 'accumulated_logging_time': 0.9654128551483154}
I0916 14:49:51.237936 140135793907456 logging_writer.py:48] [32846] accumulated_eval_time=2748.773371, accumulated_logging_time=0.965413, accumulated_submission_time=27426.663048, global_step=32846, preemption_count=0, score=27426.663048, test/ctc_loss=0.3646046817302704, test/num_examples=2472, test/wer=0.115817, total_duration=30177.122669, train/ctc_loss=0.3489483594894409, train/wer=0.112930, validation/ctc_loss=0.6146303415298462, validation/num_examples=5348, validation/wer=0.175245
I0916 14:51:52.598886 140135793907456 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.791656732559204, loss=1.534119963645935
I0916 14:58:35.733529 140135785514752 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.8524281978607178, loss=1.5117647647857666
I0916 15:06:07.035047 140135793907456 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.9478189945220947, loss=1.5054808855056763
I0916 15:12:50.441600 140135785514752 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.353736162185669, loss=1.4752851724624634
I0916 15:13:51.260947 140294737168192 spec.py:320] Evaluating on the training split.
I0916 15:14:46.543741 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 15:15:37.927519 140294737168192 spec.py:348] Evaluating on the test split.
I0916 15:16:03.179482 140294737168192 submission_runner.py:376] Time since start: 31749.10s, 	Step: 34572, 	{'train/ctc_loss': Array(0.32773608, dtype=float32), 'train/wer': 0.10482538870594754, 'validation/ctc_loss': Array(0.5973595, dtype=float32), 'validation/wer': 0.16930216403438528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34726357, dtype=float32), 'test/wer': 0.11108402900493572, 'test/num_examples': 2472, 'score': 28866.636028051376, 'total_duration': 31749.101675987244, 'accumulated_submission_time': 28866.636028051376, 'accumulated_eval_time': 2880.681939601898, 'accumulated_logging_time': 1.0223588943481445}
I0916 15:16:03.214136 140136162547456 logging_writer.py:48] [34572] accumulated_eval_time=2880.681940, accumulated_logging_time=1.022359, accumulated_submission_time=28866.636028, global_step=34572, preemption_count=0, score=28866.636028, test/ctc_loss=0.3472635746002197, test/num_examples=2472, test/wer=0.111084, total_duration=31749.101676, train/ctc_loss=0.3277360796928406, train/wer=0.104825, validation/ctc_loss=0.5973594784736633, validation/num_examples=5348, validation/wer=0.169302
I0916 15:21:52.926642 140136154154752 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.7504191398620605, loss=1.4713373184204102
I0916 15:28:34.517377 140135834867456 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.319939613342285, loss=1.4928562641143799
I0916 15:35:56.240669 140294737168192 spec.py:320] Evaluating on the training split.
I0916 15:36:53.118800 140294737168192 spec.py:332] Evaluating on the validation split.
I0916 15:37:43.692438 140294737168192 spec.py:348] Evaluating on the test split.
I0916 15:38:09.680606 140294737168192 submission_runner.py:376] Time since start: 33075.61s, 	Step: 36000, 	{'train/ctc_loss': Array(0.26228815, dtype=float32), 'train/wer': 0.08922952348738764, 'validation/ctc_loss': Array(0.5815904, dtype=float32), 'validation/wer': 0.1679900433192795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34165108, dtype=float32), 'test/wer': 0.1092153636788333, 'test/num_examples': 2472, 'score': 30059.618671178818, 'total_duration': 33075.607370853424, 'accumulated_submission_time': 30059.618671178818, 'accumulated_eval_time': 3014.1164972782135, 'accumulated_logging_time': 1.0732510089874268}
I0916 15:38:09.721076 140136592627456 logging_writer.py:48] [36000] accumulated_eval_time=3014.116497, accumulated_logging_time=1.073251, accumulated_submission_time=30059.618671, global_step=36000, preemption_count=0, score=30059.618671, test/ctc_loss=0.3416510820388794, test/num_examples=2472, test/wer=0.109215, total_duration=33075.607371, train/ctc_loss=0.2622881531715393, train/wer=0.089230, validation/ctc_loss=0.5815904140472412, validation/num_examples=5348, validation/wer=0.167990
I0916 15:38:09.746923 140136584234752 logging_writer.py:48] [36000] global_step=36000, preemption_count=0, score=30059.618671
I0916 15:38:09.950129 140294737168192 checkpoints.py:490] Saving checkpoint at step: 36000
I0916 15:38:10.896829 140294737168192 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/checkpoint_36000
I0916 15:38:10.917981 140294737168192 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_jax/nadamw_run_0/librispeech_deepspeech_jax/trial_1/checkpoint_36000.
I0916 15:38:12.103852 140294737168192 submission_runner.py:540] Tuning trial 1/1
I0916 15:38:12.104113 140294737168192 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.004958460849689891, beta1=0.863744242567442, beta2=0.6291854735396584, warmup_steps=1200, weight_decay=0.1147386261512052)
I0916 15:38:12.114575 140294737168192 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.51439, dtype=float32), 'train/wer': 3.9196478770589094, 'validation/ctc_loss': Array(30.433355, dtype=float32), 'validation/wer': 3.5589344807957626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.44908, dtype=float32), 'test/wer': 3.7575609855178436, 'test/num_examples': 2472, 'score': 60.02165150642395, 'total_duration': 297.41210770606995, 'accumulated_submission_time': 60.02165150642395, 'accumulated_eval_time': 237.3904013633728, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1756, {'train/ctc_loss': Array(0.8596461, dtype=float32), 'train/wer': 0.26545797202941235, 'validation/ctc_loss': Array(1.3066759, dtype=float32), 'validation/wer': 0.34854171289641, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9163939, dtype=float32), 'test/wer': 0.27723275039099793, 'test/num_examples': 2472, 'score': 1500.5740060806274, 'total_duration': 1867.9097166061401, 'accumulated_submission_time': 1500.5740060806274, 'accumulated_eval_time': 367.2605299949646, 'accumulated_logging_time': 0.041959285736083984, 'global_step': 1756, 'preemption_count': 0}), (3534, {'train/ctc_loss': Array(0.60077345, dtype=float32), 'train/wer': 0.1950563354020723, 'validation/ctc_loss': Array(0.97139984, dtype=float32), 'validation/wer': 0.2691777055253789, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.63538104, dtype=float32), 'test/wer': 0.19838319826132877, 'test/num_examples': 2472, 'score': 2941.2438254356384, 'total_duration': 3437.5800545215607, 'accumulated_submission_time': 2941.2438254356384, 'accumulated_eval_time': 496.18132400512695, 'accumulated_logging_time': 0.08536624908447266, 'global_step': 3534, 'preemption_count': 0}), (5271, {'train/ctc_loss': Array(0.5372288, dtype=float32), 'train/wer': 0.1793886490503977, 'validation/ctc_loss': Array(0.9332573, dtype=float32), 'validation/wer': 0.2599542687339, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5952497, dtype=float32), 'test/wer': 0.186825909450978, 'test/num_examples': 2472, 'score': 4381.916770935059, 'total_duration': 5005.885439872742, 'accumulated_submission_time': 4381.916770935059, 'accumulated_eval_time': 623.7370131015778, 'accumulated_logging_time': 0.1269986629486084, 'global_step': 5271, 'preemption_count': 0}), (7001, {'train/ctc_loss': Array(0.5345886, dtype=float32), 'train/wer': 0.1722135533935846, 'validation/ctc_loss': Array(0.88570625, dtype=float32), 'validation/wer': 0.24632172042180822, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5725201, dtype=float32), 'test/wer': 0.17762476387788678, 'test/num_examples': 2472, 'score': 5822.183131694794, 'total_duration': 6581.063181877136, 'accumulated_submission_time': 5822.183131694794, 'accumulated_eval_time': 758.5730748176575, 'accumulated_logging_time': 0.16733717918395996, 'global_step': 7001, 'preemption_count': 0}), (8749, {'train/ctc_loss': Array(0.5283293, dtype=float32), 'train/wer': 0.17110069799299832, 'validation/ctc_loss': Array(0.8690057, dtype=float32), 'validation/wer': 0.2435045200629046, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5620601, dtype=float32), 'test/wer': 0.17315621635894624, 'test/num_examples': 2472, 'score': 7262.696113586426, 'total_duration': 8151.453425168991, 'accumulated_submission_time': 7262.696113586426, 'accumulated_eval_time': 888.3731138706207, 'accumulated_logging_time': 0.20923757553100586, 'global_step': 8749, 'preemption_count': 0}), (10487, {'train/ctc_loss': Array(0.5544736, dtype=float32), 'train/wer': 0.17917215699036015, 'validation/ctc_loss': Array(0.8799448, dtype=float32), 'validation/wer': 0.24462368184931838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55379397, dtype=float32), 'test/wer': 0.17530924379989032, 'test/num_examples': 2472, 'score': 8702.642138004303, 'total_duration': 9724.160254478455, 'accumulated_submission_time': 8702.642138004303, 'accumulated_eval_time': 1021.0357594490051, 'accumulated_logging_time': 0.26417994499206543, 'global_step': 10487, 'preemption_count': 0}), (12220, {'train/ctc_loss': Array(0.5008965, dtype=float32), 'train/wer': 0.1613808946461617, 'validation/ctc_loss': Array(0.8313743, dtype=float32), 'validation/wer': 0.2316954336269525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52456063, dtype=float32), 'test/wer': 0.1624317023134889, 'test/num_examples': 2472, 'score': 10143.005124807358, 'total_duration': 11300.010608911514, 'accumulated_submission_time': 10143.005124807358, 'accumulated_eval_time': 1156.42809176445, 'accumulated_logging_time': 0.3197810649871826, 'global_step': 12220, 'preemption_count': 0}), (13954, {'train/ctc_loss': Array(0.56000787, dtype=float32), 'train/wer': 0.1573363842013573, 'validation/ctc_loss': Array(1.129067, dtype=float32), 'validation/wer': 0.2413626759544231, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71745837, dtype=float32), 'test/wer': 0.17037352994942417, 'test/num_examples': 2472, 'score': 11583.152604341507, 'total_duration': 12871.812179803848, 'accumulated_submission_time': 11583.152604341507, 'accumulated_eval_time': 1287.9920001029968, 'accumulated_logging_time': 0.36974382400512695, 'global_step': 13954, 'preemption_count': 0}), (15677, {'train/ctc_loss': Array(0.42258114, dtype=float32), 'train/wer': 0.1376875974229793, 'validation/ctc_loss': Array(0.7847165, dtype=float32), 'validation/wer': 0.21840056344007178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49246633, dtype=float32), 'test/wer': 0.15355554201450247, 'test/num_examples': 2472, 'score': 13023.676251649857, 'total_duration': 14443.880277395248, 'accumulated_submission_time': 13023.676251649857, 'accumulated_eval_time': 1419.433171749115, 'accumulated_logging_time': 0.4338796138763428, 'global_step': 15677, 'preemption_count': 0}), (17381, {'train/ctc_loss': Array(0.43677148, dtype=float32), 'train/wer': 0.14792429759689382, 'validation/ctc_loss': Array(0.76784194, dtype=float32), 'validation/wer': 0.21754189620739225, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47907305, dtype=float32), 'test/wer': 0.14994008084008692, 'test/num_examples': 2472, 'score': 14463.842086791992, 'total_duration': 16016.661360740662, 'accumulated_submission_time': 14463.842086791992, 'accumulated_eval_time': 1551.9636483192444, 'accumulated_logging_time': 0.4814188480377197, 'global_step': 17381, 'preemption_count': 0}), (19117, {'train/ctc_loss': Array(0.43248543, dtype=float32), 'train/wer': 0.14020465903381898, 'validation/ctc_loss': Array(0.7409395, dtype=float32), 'validation/wer': 0.20784571004061786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46415368, dtype=float32), 'test/wer': 0.14689334389535474, 'test/num_examples': 2472, 'score': 15904.504876613617, 'total_duration': 17590.40542125702, 'accumulated_submission_time': 15904.504876613617, 'accumulated_eval_time': 1684.9517471790314, 'accumulated_logging_time': 0.5358102321624756, 'global_step': 19117, 'preemption_count': 0}), (20851, {'train/ctc_loss': Array(0.43682304, dtype=float32), 'train/wer': 0.14068114068641677, 'validation/ctc_loss': Array(0.7387269, dtype=float32), 'validation/wer': 0.20792289361209468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4537489, dtype=float32), 'test/wer': 0.14130765949667906, 'test/num_examples': 2472, 'score': 17345.011806488037, 'total_duration': 19163.514149665833, 'accumulated_submission_time': 17345.011806488037, 'accumulated_eval_time': 1817.4645256996155, 'accumulated_logging_time': 0.5855922698974609, 'global_step': 20851, 'preemption_count': 0}), (22557, {'train/ctc_loss': Array(0.42392, dtype=float32), 'train/wer': 0.13699454676407039, 'validation/ctc_loss': Array(0.7202905, dtype=float32), 'validation/wer': 0.20535653986049068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4340122, dtype=float32), 'test/wer': 0.14075924684662727, 'test/num_examples': 2472, 'score': 18785.282359600067, 'total_duration': 20738.723254680634, 'accumulated_submission_time': 18785.282359600067, 'accumulated_eval_time': 1952.3170020580292, 'accumulated_logging_time': 0.6342990398406982, 'global_step': 22557, 'preemption_count': 0}), (24275, {'train/ctc_loss': Array(0.39008844, dtype=float32), 'train/wer': 0.12842574215615896, 'validation/ctc_loss': Array(0.6992267, dtype=float32), 'validation/wer': 0.19743557583768295, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4261716, dtype=float32), 'test/wer': 0.1348079540145837, 'test/num_examples': 2472, 'score': 20225.19295024872, 'total_duration': 22310.58238697052, 'accumulated_submission_time': 20225.19295024872, 'accumulated_eval_time': 2084.1748735904694, 'accumulated_logging_time': 0.685528039932251, 'global_step': 24275, 'preemption_count': 0}), (26013, {'train/ctc_loss': Array(0.374774, dtype=float32), 'train/wer': 0.12670127024700698, 'validation/ctc_loss': Array(0.70082295, dtype=float32), 'validation/wer': 0.19887311985643855, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42585066, dtype=float32), 'test/wer': 0.13592509089431884, 'test/num_examples': 2472, 'score': 21665.395996570587, 'total_duration': 23884.467129945755, 'accumulated_submission_time': 21665.395996570587, 'accumulated_eval_time': 2217.7630529403687, 'accumulated_logging_time': 0.7409267425537109, 'global_step': 26013, 'preemption_count': 0}), (27709, {'train/ctc_loss': Array(0.35208127, dtype=float32), 'train/wer': 0.11634329197819394, 'validation/ctc_loss': Array(0.67099357, dtype=float32), 'validation/wer': 0.1902382077974703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40303847, dtype=float32), 'test/wer': 0.12824731379359372, 'test/num_examples': 2472, 'score': 23105.315306425095, 'total_duration': 25458.258945941925, 'accumulated_submission_time': 23105.315306425095, 'accumulated_eval_time': 2351.5491061210632, 'accumulated_logging_time': 0.7900862693786621, 'global_step': 27709, 'preemption_count': 0}), (29426, {'train/ctc_loss': Array(0.3624501, dtype=float32), 'train/wer': 0.12007063615877968, 'validation/ctc_loss': Array(0.6431861, dtype=float32), 'validation/wer': 0.1825198506497892, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38469446, dtype=float32), 'test/wer': 0.12215383990412934, 'test/num_examples': 2472, 'score': 24545.426859378815, 'total_duration': 27030.016496658325, 'accumulated_submission_time': 24545.426859378815, 'accumulated_eval_time': 2483.105142593384, 'accumulated_logging_time': 0.8398141860961914, 'global_step': 29426, 'preemption_count': 0}), (31145, {'train/ctc_loss': Array(0.33710065, dtype=float32), 'train/wer': 0.11115632748956858, 'validation/ctc_loss': Array(0.6294584, dtype=float32), 'validation/wer': 0.18012715993400805, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37788603, dtype=float32), 'test/wer': 0.12164605041334065, 'test/num_examples': 2472, 'score': 25986.060956954956, 'total_duration': 28601.989730596542, 'accumulated_submission_time': 25986.060956954956, 'accumulated_eval_time': 2614.3357541561127, 'accumulated_logging_time': 0.9099061489105225, 'global_step': 31145, 'preemption_count': 0}), (32846, {'train/ctc_loss': Array(0.34894836, dtype=float32), 'train/wer': 0.11293015323446466, 'validation/ctc_loss': Array(0.61463034, dtype=float32), 'validation/wer': 0.17524529903809974, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36460468, dtype=float32), 'test/wer': 0.11581662705908638, 'test/num_examples': 2472, 'score': 27426.663048267365, 'total_duration': 30177.122668981552, 'accumulated_submission_time': 27426.663048267365, 'accumulated_eval_time': 2748.773370742798, 'accumulated_logging_time': 0.9654128551483154, 'global_step': 32846, 'preemption_count': 0}), (34572, {'train/ctc_loss': Array(0.32773608, dtype=float32), 'train/wer': 0.10482538870594754, 'validation/ctc_loss': Array(0.5973595, dtype=float32), 'validation/wer': 0.16930216403438528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34726357, dtype=float32), 'test/wer': 0.11108402900493572, 'test/num_examples': 2472, 'score': 28866.636028051376, 'total_duration': 31749.101675987244, 'accumulated_submission_time': 28866.636028051376, 'accumulated_eval_time': 2880.681939601898, 'accumulated_logging_time': 1.0223588943481445, 'global_step': 34572, 'preemption_count': 0}), (36000, {'train/ctc_loss': Array(0.26228815, dtype=float32), 'train/wer': 0.08922952348738764, 'validation/ctc_loss': Array(0.5815904, dtype=float32), 'validation/wer': 0.1679900433192795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34165108, dtype=float32), 'test/wer': 0.1092153636788333, 'test/num_examples': 2472, 'score': 30059.618671178818, 'total_duration': 33075.607370853424, 'accumulated_submission_time': 30059.618671178818, 'accumulated_eval_time': 3014.1164972782135, 'accumulated_logging_time': 1.0732510089874268, 'global_step': 36000, 'preemption_count': 0})], 'global_step': 36000}
I0916 15:38:12.114748 140294737168192 submission_runner.py:543] Timing: 30059.618671178818
I0916 15:38:12.114808 140294737168192 submission_runner.py:545] Total number of evals: 22
I0916 15:38:12.114865 140294737168192 submission_runner.py:546] ====================
I0916 15:38:12.117770 140294737168192 submission_runner.py:614] Final librispeech_deepspeech score: 30059.618671178818
