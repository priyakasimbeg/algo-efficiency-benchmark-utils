torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nesterov.py --tuning_search_space=reference_algorithms/target_setting_algorithms/fastmri/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_pytorch/nesterov_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=27142 --torch_compile=true 2>&1 | tee -a /logs/fastmri_pytorch_09-13-2023-19-08-00.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-09-13 19:08:11.360158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-13 19:08:11.360162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0913 19:08:26.242748 139936887863104 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0913 19:08:26.242760 140005131040576 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0913 19:08:26.242788 140481359161152 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0913 19:08:26.242833 140065100478272 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0913 19:08:26.244323 139817535289152 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0913 19:08:27.215668 140433590605632 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0913 19:08:27.215701 140385070839616 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0913 19:08:27.223994 140624454674240 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0913 19:08:27.224696 140624454674240 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.226659 140433590605632 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.226692 140385070839616 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.232629 139817535289152 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.232741 139936887863104 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.232757 140005131040576 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.232807 140065100478272 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.232842 140481359161152 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0913 19:08:27.579593 140624454674240 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch because --overwrite was set.
I0913 19:08:27.582579 140624454674240 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch.
W0913 19:08:28.419074 139936887863104 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.419690 140624454674240 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.419951 140385070839616 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.419956 140065100478272 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.419993 140481359161152 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.420809 139817535289152 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.420827 140433590605632 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0913 19:08:28.421126 140005131040576 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0913 19:08:28.426169 140624454674240 submission_runner.py:500] Using RNG seed 290086809
I0913 19:08:28.428477 140624454674240 submission_runner.py:509] --- Tuning run 1/1 ---
I0913 19:08:28.428608 140624454674240 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch/trial_1.
I0913 19:08:28.428830 140624454674240 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch/trial_1/hparams.json.
I0913 19:08:28.429690 140624454674240 submission_runner.py:185] Initializing dataset.
I0913 19:08:28.429809 140624454674240 submission_runner.py:192] Initializing model.
I0913 19:08:33.092624 140624454674240 submission_runner.py:223] Performing `torch.compile`.
I0913 19:08:33.377785 140624454674240 submission_runner.py:226] Initializing optimizer.
I0913 19:08:33.970988 140624454674240 submission_runner.py:233] Initializing metrics bundle.
I0913 19:08:33.971172 140624454674240 submission_runner.py:251] Initializing checkpoint and logger.
I0913 19:08:33.971886 140624454674240 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0913 19:08:33.972012 140624454674240 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0913 19:08:34.422109 140624454674240 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch/trial_1/meta_data_0.json.
I0913 19:08:34.423029 140624454674240 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch/trial_1/flags_0.json.
I0913 19:08:34.515812 140624454674240 submission_runner.py:285] Starting training loop.
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,889] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,889] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,889] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,889] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:34,889] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:34,889] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:34,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,902] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,903] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,903] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:34,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,906] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,907] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,907] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,907] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:34,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:08:34,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:08:34,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,949] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,949] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,949] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:34,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:08:34,967] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:08:34,967] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:08:34,968] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:08:40,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:08:40,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:08:40,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:08:40,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:08:40,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:08:40,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:08:40,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:09:23,816] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:23,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:23,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:23,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:23,855] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,856] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:23,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:23,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,862] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:23,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:23,868] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:24,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-13 19:09:24,252] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:24,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:24,316] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:24,316] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:24,316] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,697] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,713] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,713] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,717] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,717] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,718] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:27,745] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,746] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,746] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,750] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,750] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,751] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,764] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,765] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,765] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,766] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:27,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:27,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:27,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:27,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:28,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:28,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:28,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:28,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:28,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:28,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:28,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:29,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,394] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,405] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,411] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,441] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,447] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:29,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:29,491] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,527] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,528] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,528] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,538] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,538] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,538] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,538] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,539] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,539] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,540] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:29,574] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,577] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,577] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,579] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:29,604] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:29,605] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:29,605] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:29,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:29,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:29,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:29,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:30,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:30,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:30,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:30,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,860] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,887] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:30,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:30,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:30,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:30,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:30,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:30,973] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:30,974] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:30,974] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:30,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:30,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:30,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:30,997] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:30,997] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:30,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:30,998] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:30,999] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:31,000] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:31,000] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:31,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:31,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:31,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:31,027] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:31,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:31,027] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:31,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:31,040] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:31,040] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:31,041] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:31,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:31,066] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:31,067] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:31,067] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:31,068] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:31,070] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:31,071] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:31,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:31,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-13 19:09:31,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:31,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:32,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:32,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,540] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,545] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,561] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,585] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,595] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,596] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,596] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:32,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,614] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,619] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,623] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:32,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,651] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,651] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,651] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,668] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,671] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,671] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,671] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,683] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:32,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,703] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,703] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,703] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,731] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,734] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,734] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:32,737] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:32,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:32,738] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:32,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:32,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:32,937] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:32,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:32,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:32,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:33,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:33,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-13 19:09:33,303] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:33,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:33,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:33,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:34,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:34,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:34,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:34,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:34,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:34,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,407] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,417] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,498] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,498] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,499] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,514] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,516] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,519] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,519] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,529] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,534] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:34,580] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,596] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,611] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,611] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,634] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,634] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,635] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,641] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,641] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,642] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:34,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,684] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,684] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,684] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:34,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:34,691] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:34,691] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:34,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:34,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:34,956] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:34,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:34,965] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:35,065] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:35,083] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:35,089] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:35,133] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:35,144] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,181] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,182] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,230] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,231] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,232] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,232] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,247] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,247] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:35,275] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:35,275] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:35,276] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:35,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:35,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:35,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:35,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:35,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,922] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,925] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:35,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:35,990] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:35,990] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,014] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,014] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,014] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,014] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,014] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,015] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:36,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:36,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:36,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:36,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:36,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,089] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,092] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,099] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,168] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,169] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,187] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,187] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,210] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,211] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,211] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,211] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,211] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,212] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,212] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,212] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,441] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:36,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,506] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,549] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,551] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,551] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,579] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,580] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,580] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,608] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,624] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,634] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,647] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:36,649] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:36,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:36,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,735] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,735] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,735] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,735] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,736] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,744] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,744] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,745] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,778] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,778] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:36,780] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,780] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,780] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:36,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:36,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:36,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,005] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,072] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,072] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:37,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,272] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,293] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,351] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,351] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,352] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,370] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,371] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,371] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,492] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,502] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,539] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,594] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,595] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,595] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,597] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,597] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,597] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:37,606] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,610] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,610] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,610] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,627] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,628] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,628] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,694] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,694] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-13 19:09:37,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:37,803] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:37,815] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:37,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:37,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:37,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,926] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,926] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:37,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:37,936] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:37,936] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:37,937] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:38,028] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:38,030] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:38,036] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:38,056] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:38,114] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,161] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,161] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,179] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,179] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,180] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,193] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,238] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,238] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,238] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:38,650] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:38,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:38,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,734] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:38,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,734] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,735] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,736] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:38,736] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:38,736] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:38,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:38,906] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:38,911] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:38,917] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:38,947] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:38,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:38,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:38,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:38,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:39,009] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,010] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,010] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,010] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,010] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,011] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,011] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,012] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,012] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,012] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,012] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,040] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,040] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,040] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,233] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,246] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:39,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:39,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:39,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:39,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:39,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,425] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,425] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,438] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,438] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,439] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,535] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,562] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,578] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:39,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-13 19:09:39,663] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:39,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,697] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,729] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:39,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,755] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,802] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,804] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,804] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,805] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,805] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,805] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,806] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,818] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,820] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,820] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,821] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:39,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:39,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:39,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:39,831] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:40,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:40,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:40,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-13 19:09:40,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:40,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:40,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:40,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:40,788] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:41,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,029] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,040] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,296] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,351] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,446] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:41,458] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:41,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-13 19:09:41,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:41,963] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:42,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:42,039] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:42,039] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:42,040] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:42,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-13 19:09:43,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-13 19:09:43,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-13 19:09:44,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-13 19:09:44,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:44,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:44,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:44,248] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:44,248] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:44,249] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:45,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-13 19:09:46,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-13 19:09:46,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-13 19:09:47,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-13 19:09:47,230] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:47,295] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:47,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:47,328] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:47,328] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:47,329] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:47,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-13 19:09:47,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-13 19:09:47,822] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:47,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:47,956] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:47,959] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:47,960] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:47,960] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:48,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-13 19:09:48,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-13 19:09:48,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-13 19:09:48,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-13 19:09:48,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:49,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:49,039] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:49,040] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:49,041] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:49,042] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:49,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-13 19:09:49,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-13 19:09:49,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:49,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:49,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:49,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:49,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:49,727] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:50,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-13 19:09:50,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-13 19:09:50,448] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:50,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:50,527] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:50,528] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:50,529] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:50,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:50,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-13 19:09:50,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-13 19:09:50,973] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:51,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:51,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:51,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:51,099] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:51,100] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:51,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-13 19:09:51,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-13 19:09:51,819] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:51,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:51,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:51,895] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:51,896] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:51,896] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:52,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-13 19:09:52,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-13 19:09:52,398] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:52,529] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:09:52,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:09:52,582] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:09:52,583] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:09:52,583] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:09:53,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-13 19:09:54,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-13 19:09:54,166] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:09:54,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-13 19:09:54,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,286] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:54,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:54,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:54,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:54,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:54,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:55,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:55,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:55,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-13 19:09:55,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:55,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:55,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-13 19:09:56,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:56,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:56,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:56,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:56,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:56,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:56,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:56,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:56,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:56,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:57,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:57,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:57,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:57,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:57,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:57,147] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:57,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-13 19:09:57,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-13 19:09:57,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:57,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-13 19:09:58,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:09:58,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:09:58,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:09:58,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-13 19:09:58,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:58,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:58,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:58,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-13 19:09:58,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:58,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:58,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:58,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-13 19:09:59,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:59,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:59,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:59,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:09:59,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:59,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:59,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:59,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:09:59,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:09:59,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:09:59,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:09:59,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:09:59,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-13 19:09:59,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:09:59,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:00,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:00,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-13 19:10:00,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-13 19:10:00,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-13 19:10:00,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:00,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:00,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:00,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:00,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:00,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:00,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:00,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:00,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:00,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:00,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:00,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:00,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:00,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:00,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:00,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-13 19:10:00,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:00,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:00,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:00,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-13 19:10:00,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:00,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:00,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:01,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:01,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:01,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:01,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:01,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-13 19:10:01,157] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-13 19:10:01,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:01,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:01,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:01,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:01,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:01,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:01,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:01,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:01,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:01,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:01,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:01,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-13 19:10:01,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-13 19:10:01,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:01,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:01,498] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:01,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-09-13 19:10:02,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-13 19:10:02,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-13 19:10:02,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-13 19:10:02,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-13 19:10:02,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-13 19:10:02,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-13 19:10:02,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-13 19:10:03,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-09-13 19:10:03,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-09-13 19:10:03,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-09-13 19:10:03,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-09-13 19:10:03,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-09-13 19:10:03,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0913 19:10:03.739160 140582357821184 logging_writer.py:48] [0] global_step=0, grad_norm=5.357780, loss=1.246129
I0913 19:10:03.753142 140624454674240 pytorch_submission_base.py:86] 0) loss = 1.246, grad_norm = 5.358
I0913 19:10:04.205563 140624454674240 spec.py:320] Evaluating on the training split.
[2023-09-13 19:10:58,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,783] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,783] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:58,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,830] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,831] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,831] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,831] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,831] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,831] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,831] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,832] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,832] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,832] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,833] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,833] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,833] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,834] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:58,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:58,851] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:58,851] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:58,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-13 19:10:59,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,626] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,626] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,630] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,634] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,636] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,643] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:10:59,687] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:10:59,825] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:59,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:59,853] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:59,868] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:59,873] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:59,875] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:59,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:59,876] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:59,876] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:10:59,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:59,897] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:59,897] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:59,898] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:59,902] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:59,903] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:59,903] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:59,916] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:59,917] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:59,917] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:59,930] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:59,930] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:59,931] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:10:59,938] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:10:59,938] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:10:59,938] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:10:59,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:00,045] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:00,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:00,046] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:00,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,102] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-13 19:11:00,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,108] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,295] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:00,353] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:00,354] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:00,354] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:00,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-13 19:11:00,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,694] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,700] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,713] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,749] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,770] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,785] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:00,851] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:00,909] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:00,956] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:00,956] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:00,957] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:00,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,976] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:00,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,979] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:00,979] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:00,979] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:00,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:00,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:00,995] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:00,996] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:00,996] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:01,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:01,015] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:01,016] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:01,016] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:01,027] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:01,029] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:01,029] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:01,030] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:01,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:01,036] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:01,036] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:01,037] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:01,104] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:01,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:01,152] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:01,152] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:01,153] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:01,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:01,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-13 19:11:01,708] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,841] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:01,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,886] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,915] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:01,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:01,949] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:01,949] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:01,950] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:01,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:01,991] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:02,039] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,084] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,087] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,087] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,087] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,197] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,197] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:02,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,236] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,236] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,237] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-13 19:11:02,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:02,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:02,274] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:02,274] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:02,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:02,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:02,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:02,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:02,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:02,958] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:02,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:02,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:02,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:02,993] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:03,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:03,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:03,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:03,109] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:03,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:03,139] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,146] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,152] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,175] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,176] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,176] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:03,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-13 19:11:03,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,195] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,209] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:03,227] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:03,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,335] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,336] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,336] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,389] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,389] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:03,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,449] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,449] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,463] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:03,465] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,466] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,466] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,466] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,466] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,467] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,467] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:03,467] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:03,468] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:03,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:03,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-13 19:11:04,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,292] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:04,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,460] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:04,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-13 19:11:04,482] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,484] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,484] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,484] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,485] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,510] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,511] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,511] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,537] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:04,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,602] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-13 19:11:04,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,655] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,655] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,656] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:04,704] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:04,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,748] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,748] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,749] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-13 19:11:04,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,789] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,818] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,819] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,819] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,822] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,824] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,824] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,824] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,825] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:04,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,874] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,875] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,875] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:04,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:04,901] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:04,901] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:04,901] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:04,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:04,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:04,976] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:04,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:05,048] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:05,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,056] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,057] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,057] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:05,060] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,086] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,086] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,087] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,157] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,157] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,158] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,175] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,211] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,212] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,223] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,223] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,223] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-13 19:11:05,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,233] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:05,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,378] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:05,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,533] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,561] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,562] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,563] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,563] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:05,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,587] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,634] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:05,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,688] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:05,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:05,703] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,713] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:05,805] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:05,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:05,854] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,856] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,856] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,857] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:05,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,900] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,900] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,901] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,928] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,928] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,928] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,936] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,937] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,937] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:05,961] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:05,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:05,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:05,985] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:05,985] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:05,986] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:06,010] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,010] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,011] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:06,017] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:06,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:06,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:06,090] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:06,138] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:06,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:06,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,264] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,265] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,312] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,321] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,322] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,322] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,359] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,359] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,360] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,422] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-13 19:11:06,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,553] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:06,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,646] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,646] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,647] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,647] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,648] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,648] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,672] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-13 19:11:06,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,721] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:06,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:06,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,752] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:06,757] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-13 19:11:06,816] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:06,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:06,884] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:06,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,907] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,909] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,909] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,909] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,909] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,910] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,910] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:06,985] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:06,985] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:06,986] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:06,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:06,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:07,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:07,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,019] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,019] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,019] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,019] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,019] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,020] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:07,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:07,124] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:07,124] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,125] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:07,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:07,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:07,222] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:07,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:07,256] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,256] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-13 19:11:07,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,386] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,386] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-13 19:11:07,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,387] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,463] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,465] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,465] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,466] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,505] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,506] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,506] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,508] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,508] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,571] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,572] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,572] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,582] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,591] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:07,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-13 19:11:07,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,780] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,781] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:07,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-13 19:11:07,853] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,853] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:07,854] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,854] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:07,855] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,855] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:07,855] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,855] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:07,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,874] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:07,898] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:07,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:07,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:07,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-13 19:11:08,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,076] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,083] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,086] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,089] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-13 19:11:08,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,099] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,099] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,099] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,099] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,111] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,112] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,116] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,193] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,193] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,193] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:08,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:08,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:08,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:08,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:08,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,339] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,340] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,350] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,368] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,370] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,370] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:08,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,463] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,465] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,466] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,466] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-13 19:11:08,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-13 19:11:08,625] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:08,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:08,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:08,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,722] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,727] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,729] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,729] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,730] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:08,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,800] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,800] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,800] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,812] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:08,814] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:08,814] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:08,815] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:08,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:08,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:08,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:09,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:09,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:09,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-13 19:11:09,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-13 19:11:09,275] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,301] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,329] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,329] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,380] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:09,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:09,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-09-13 19:11:09,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-09-13 19:11:09,523] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:09,523] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:09,523] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:09,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-13 19:11:09,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-13 19:11:09,786] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-09-13 19:11:09,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:10,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:10,054] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:10,055] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:10,056] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:10,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-13 19:11:10,498] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-13 19:11:10,499] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:10,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:10,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:10,725] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:10,725] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:10,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:10,855] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-13 19:11:10,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-13 19:11:10,989] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-13 19:11:11,285] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-13 19:11:11,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-13 19:11:11,356] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-13 19:11:11,356] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-13 19:11:11,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-13 19:11:11,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-13 19:11:12,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-13 19:11:12,354] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:11:43.620688 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:12:48.919901 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:13:51.599514 140624454674240 submission_runner.py:376] Time since start: 317.08s, 	Step: 1, 	{'train/ssim': 0.14957191262926375, 'train/loss': 1.3122122628348214, 'validation/ssim': 0.14309939683389314, 'validation/loss': 1.3252357049275465, 'validation/num_examples': 3554, 'test/ssim': 0.16294639691797858, 'test/loss': 1.3201016977624964, 'test/num_examples': 3581, 'score': 89.2386724948883, 'total_duration': 317.08421993255615, 'accumulated_submission_time': 89.2386724948883, 'accumulated_eval_time': 227.39448761940002, 'accumulated_logging_time': 0}
I0913 19:13:51.620692 140561453410048 logging_writer.py:48] [1] accumulated_eval_time=227.394488, accumulated_logging_time=0, accumulated_submission_time=89.238672, global_step=1, preemption_count=0, score=89.238672, test/loss=1.320102, test/num_examples=3581, test/ssim=0.162946, total_duration=317.084220, train/loss=1.312212, train/ssim=0.149572, validation/loss=1.325236, validation/num_examples=3554, validation/ssim=0.143099
I0913 19:13:52.072851 140005131040576 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.072868 139817535289152 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.072874 139936887863104 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.072866 140065100478272 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.072881 140433590605632 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.072856 140481359161152 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.072944 140385070839616 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.073395 140624454674240 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0913 19:13:52.239305 140561445017344 logging_writer.py:48] [1] global_step=1, grad_norm=5.336170, loss=1.282323
I0913 19:13:52.243181 140624454674240 pytorch_submission_base.py:86] 1) loss = 1.282, grad_norm = 5.336
I0913 19:13:52.305560 140561453410048 logging_writer.py:48] [2] global_step=2, grad_norm=5.420934, loss=1.340961
I0913 19:13:52.309335 140624454674240 pytorch_submission_base.py:86] 2) loss = 1.341, grad_norm = 5.421
I0913 19:13:52.368861 140561445017344 logging_writer.py:48] [3] global_step=3, grad_norm=5.634981, loss=1.280405
I0913 19:13:52.372634 140624454674240 pytorch_submission_base.py:86] 3) loss = 1.280, grad_norm = 5.635
I0913 19:13:52.441049 140561453410048 logging_writer.py:48] [4] global_step=4, grad_norm=5.553928, loss=1.255491
I0913 19:13:52.447742 140624454674240 pytorch_submission_base.py:86] 4) loss = 1.255, grad_norm = 5.554
I0913 19:13:52.521078 140561445017344 logging_writer.py:48] [5] global_step=5, grad_norm=5.426684, loss=1.273210
I0913 19:13:52.526346 140624454674240 pytorch_submission_base.py:86] 5) loss = 1.273, grad_norm = 5.427
I0913 19:13:52.600563 140561453410048 logging_writer.py:48] [6] global_step=6, grad_norm=4.955234, loss=1.234344
I0913 19:13:52.606661 140624454674240 pytorch_submission_base.py:86] 6) loss = 1.234, grad_norm = 4.955
I0913 19:13:52.681954 140561445017344 logging_writer.py:48] [7] global_step=7, grad_norm=4.769692, loss=1.236802
I0913 19:13:52.686311 140624454674240 pytorch_submission_base.py:86] 7) loss = 1.237, grad_norm = 4.770
I0913 19:13:52.765638 140561453410048 logging_writer.py:48] [8] global_step=8, grad_norm=4.553092, loss=1.166513
I0913 19:13:52.769837 140624454674240 pytorch_submission_base.py:86] 8) loss = 1.167, grad_norm = 4.553
I0913 19:13:52.842520 140561445017344 logging_writer.py:48] [9] global_step=9, grad_norm=3.860414, loss=1.228617
I0913 19:13:52.849117 140624454674240 pytorch_submission_base.py:86] 9) loss = 1.229, grad_norm = 3.860
I0913 19:13:52.922694 140561453410048 logging_writer.py:48] [10] global_step=10, grad_norm=3.835373, loss=1.148155
I0913 19:13:52.929123 140624454674240 pytorch_submission_base.py:86] 10) loss = 1.148, grad_norm = 3.835
I0913 19:13:53.009350 140561445017344 logging_writer.py:48] [11] global_step=11, grad_norm=3.480837, loss=1.102986
I0913 19:13:53.015419 140624454674240 pytorch_submission_base.py:86] 11) loss = 1.103, grad_norm = 3.481
I0913 19:13:53.084569 140561453410048 logging_writer.py:48] [12] global_step=12, grad_norm=3.186263, loss=0.999028
I0913 19:13:53.092065 140624454674240 pytorch_submission_base.py:86] 12) loss = 0.999, grad_norm = 3.186
I0913 19:13:53.175612 140561445017344 logging_writer.py:48] [13] global_step=13, grad_norm=2.746597, loss=0.950958
I0913 19:13:53.180578 140624454674240 pytorch_submission_base.py:86] 13) loss = 0.951, grad_norm = 2.747
I0913 19:13:53.256412 140561453410048 logging_writer.py:48] [14] global_step=14, grad_norm=2.387191, loss=0.962419
I0913 19:13:53.260190 140624454674240 pytorch_submission_base.py:86] 14) loss = 0.962, grad_norm = 2.387
I0913 19:13:53.500493 140561445017344 logging_writer.py:48] [15] global_step=15, grad_norm=2.043758, loss=1.012094
I0913 19:13:53.506814 140624454674240 pytorch_submission_base.py:86] 15) loss = 1.012, grad_norm = 2.044
I0913 19:13:53.763815 140561453410048 logging_writer.py:48] [16] global_step=16, grad_norm=2.044828, loss=0.994447
I0913 19:13:53.768673 140624454674240 pytorch_submission_base.py:86] 16) loss = 0.994, grad_norm = 2.045
I0913 19:13:54.066258 140561445017344 logging_writer.py:48] [17] global_step=17, grad_norm=1.885181, loss=0.914149
I0913 19:13:54.073256 140624454674240 pytorch_submission_base.py:86] 17) loss = 0.914, grad_norm = 1.885
I0913 19:13:54.295204 140561453410048 logging_writer.py:48] [18] global_step=18, grad_norm=1.753631, loss=0.854329
I0913 19:13:54.300524 140624454674240 pytorch_submission_base.py:86] 18) loss = 0.854, grad_norm = 1.754
I0913 19:13:54.591913 140561445017344 logging_writer.py:48] [19] global_step=19, grad_norm=1.774510, loss=0.902621
I0913 19:13:54.598987 140624454674240 pytorch_submission_base.py:86] 19) loss = 0.903, grad_norm = 1.775
I0913 19:13:54.880835 140561453410048 logging_writer.py:48] [20] global_step=20, grad_norm=1.768630, loss=0.818957
I0913 19:13:54.885763 140624454674240 pytorch_submission_base.py:86] 20) loss = 0.819, grad_norm = 1.769
I0913 19:13:55.149614 140561445017344 logging_writer.py:48] [21] global_step=21, grad_norm=1.760753, loss=0.804576
I0913 19:13:55.155804 140624454674240 pytorch_submission_base.py:86] 21) loss = 0.805, grad_norm = 1.761
I0913 19:13:55.397958 140561453410048 logging_writer.py:48] [22] global_step=22, grad_norm=1.811633, loss=0.842160
I0913 19:13:55.403524 140624454674240 pytorch_submission_base.py:86] 22) loss = 0.842, grad_norm = 1.812
I0913 19:13:55.704966 140561445017344 logging_writer.py:48] [23] global_step=23, grad_norm=1.925011, loss=0.745011
I0913 19:13:55.710842 140624454674240 pytorch_submission_base.py:86] 23) loss = 0.745, grad_norm = 1.925
I0913 19:13:55.985891 140561453410048 logging_writer.py:48] [24] global_step=24, grad_norm=1.986376, loss=0.762712
I0913 19:13:55.991921 140624454674240 pytorch_submission_base.py:86] 24) loss = 0.763, grad_norm = 1.986
I0913 19:13:56.268871 140561445017344 logging_writer.py:48] [25] global_step=25, grad_norm=1.908019, loss=0.720817
I0913 19:13:56.276141 140624454674240 pytorch_submission_base.py:86] 25) loss = 0.721, grad_norm = 1.908
I0913 19:13:56.542407 140561453410048 logging_writer.py:48] [26] global_step=26, grad_norm=2.197248, loss=0.654338
I0913 19:13:56.547643 140624454674240 pytorch_submission_base.py:86] 26) loss = 0.654, grad_norm = 2.197
I0913 19:13:56.774817 140561445017344 logging_writer.py:48] [27] global_step=27, grad_norm=2.142623, loss=0.704465
I0913 19:13:56.781147 140624454674240 pytorch_submission_base.py:86] 27) loss = 0.704, grad_norm = 2.143
I0913 19:13:57.044767 140561453410048 logging_writer.py:48] [28] global_step=28, grad_norm=2.228934, loss=0.628196
I0913 19:13:57.048212 140624454674240 pytorch_submission_base.py:86] 28) loss = 0.628, grad_norm = 2.229
I0913 19:13:57.352296 140561445017344 logging_writer.py:48] [29] global_step=29, grad_norm=2.132190, loss=0.639356
I0913 19:13:57.355885 140624454674240 pytorch_submission_base.py:86] 29) loss = 0.639, grad_norm = 2.132
I0913 19:13:57.636778 140561453410048 logging_writer.py:48] [30] global_step=30, grad_norm=2.169060, loss=0.647791
I0913 19:13:57.640462 140624454674240 pytorch_submission_base.py:86] 30) loss = 0.648, grad_norm = 2.169
I0913 19:13:57.944673 140561445017344 logging_writer.py:48] [31] global_step=31, grad_norm=2.133805, loss=0.579058
I0913 19:13:57.948335 140624454674240 pytorch_submission_base.py:86] 31) loss = 0.579, grad_norm = 2.134
I0913 19:13:58.200695 140561453410048 logging_writer.py:48] [32] global_step=32, grad_norm=1.923489, loss=0.610391
I0913 19:13:58.204753 140624454674240 pytorch_submission_base.py:86] 32) loss = 0.610, grad_norm = 1.923
I0913 19:13:58.457215 140561445017344 logging_writer.py:48] [33] global_step=33, grad_norm=1.905316, loss=0.576022
I0913 19:13:58.463904 140624454674240 pytorch_submission_base.py:86] 33) loss = 0.576, grad_norm = 1.905
I0913 19:13:58.755066 140561453410048 logging_writer.py:48] [34] global_step=34, grad_norm=1.557505, loss=0.611993
I0913 19:13:58.761674 140624454674240 pytorch_submission_base.py:86] 34) loss = 0.612, grad_norm = 1.558
I0913 19:13:59.005290 140561445017344 logging_writer.py:48] [35] global_step=35, grad_norm=1.358747, loss=0.490739
I0913 19:13:59.012278 140624454674240 pytorch_submission_base.py:86] 35) loss = 0.491, grad_norm = 1.359
I0913 19:13:59.310796 140561453410048 logging_writer.py:48] [36] global_step=36, grad_norm=1.036570, loss=0.514321
I0913 19:13:59.315966 140624454674240 pytorch_submission_base.py:86] 36) loss = 0.514, grad_norm = 1.037
I0913 19:13:59.570647 140561445017344 logging_writer.py:48] [37] global_step=37, grad_norm=0.737906, loss=0.432678
I0913 19:13:59.576445 140624454674240 pytorch_submission_base.py:86] 37) loss = 0.433, grad_norm = 0.738
I0913 19:13:59.838995 140561453410048 logging_writer.py:48] [38] global_step=38, grad_norm=0.750771, loss=0.421647
I0913 19:13:59.844762 140624454674240 pytorch_submission_base.py:86] 38) loss = 0.422, grad_norm = 0.751
I0913 19:14:00.125256 140561445017344 logging_writer.py:48] [39] global_step=39, grad_norm=0.901641, loss=0.446536
I0913 19:14:00.131128 140624454674240 pytorch_submission_base.py:86] 39) loss = 0.447, grad_norm = 0.902
I0913 19:14:00.348139 140561453410048 logging_writer.py:48] [40] global_step=40, grad_norm=0.960856, loss=0.417925
I0913 19:14:00.354513 140624454674240 pytorch_submission_base.py:86] 40) loss = 0.418, grad_norm = 0.961
I0913 19:14:00.597553 140561445017344 logging_writer.py:48] [41] global_step=41, grad_norm=0.975443, loss=0.397027
I0913 19:14:00.601020 140624454674240 pytorch_submission_base.py:86] 41) loss = 0.397, grad_norm = 0.975
I0913 19:14:00.870527 140561453410048 logging_writer.py:48] [42] global_step=42, grad_norm=0.954337, loss=0.506196
I0913 19:14:00.876423 140624454674240 pytorch_submission_base.py:86] 42) loss = 0.506, grad_norm = 0.954
I0913 19:14:01.167848 140561445017344 logging_writer.py:48] [43] global_step=43, grad_norm=1.048891, loss=0.488388
I0913 19:14:01.174363 140624454674240 pytorch_submission_base.py:86] 43) loss = 0.488, grad_norm = 1.049
I0913 19:14:01.475068 140561453410048 logging_writer.py:48] [44] global_step=44, grad_norm=1.179863, loss=0.459996
I0913 19:14:01.478536 140624454674240 pytorch_submission_base.py:86] 44) loss = 0.460, grad_norm = 1.180
I0913 19:14:01.777436 140561445017344 logging_writer.py:48] [45] global_step=45, grad_norm=1.093483, loss=0.527784
I0913 19:14:01.781257 140624454674240 pytorch_submission_base.py:86] 45) loss = 0.528, grad_norm = 1.093
I0913 19:14:02.013799 140561453410048 logging_writer.py:48] [46] global_step=46, grad_norm=1.171137, loss=0.488974
I0913 19:14:02.017351 140624454674240 pytorch_submission_base.py:86] 46) loss = 0.489, grad_norm = 1.171
I0913 19:14:02.298388 140561445017344 logging_writer.py:48] [47] global_step=47, grad_norm=1.068555, loss=0.539719
I0913 19:14:02.301714 140624454674240 pytorch_submission_base.py:86] 47) loss = 0.540, grad_norm = 1.069
I0913 19:14:02.567206 140561453410048 logging_writer.py:48] [48] global_step=48, grad_norm=1.019909, loss=0.576301
I0913 19:14:02.570993 140624454674240 pytorch_submission_base.py:86] 48) loss = 0.576, grad_norm = 1.020
I0913 19:14:02.877970 140561445017344 logging_writer.py:48] [49] global_step=49, grad_norm=1.169343, loss=0.550487
I0913 19:14:02.881520 140624454674240 pytorch_submission_base.py:86] 49) loss = 0.550, grad_norm = 1.169
I0913 19:14:03.167173 140561453410048 logging_writer.py:48] [50] global_step=50, grad_norm=1.069059, loss=0.547068
I0913 19:14:03.170991 140624454674240 pytorch_submission_base.py:86] 50) loss = 0.547, grad_norm = 1.069
I0913 19:14:03.424297 140561445017344 logging_writer.py:48] [51] global_step=51, grad_norm=1.173041, loss=0.590973
I0913 19:14:03.429451 140624454674240 pytorch_submission_base.py:86] 51) loss = 0.591, grad_norm = 1.173
I0913 19:14:03.703084 140561453410048 logging_writer.py:48] [52] global_step=52, grad_norm=1.194241, loss=0.620045
I0913 19:14:03.708709 140624454674240 pytorch_submission_base.py:86] 52) loss = 0.620, grad_norm = 1.194
I0913 19:14:03.999958 140561445017344 logging_writer.py:48] [53] global_step=53, grad_norm=1.216449, loss=0.559236
I0913 19:14:04.006855 140624454674240 pytorch_submission_base.py:86] 53) loss = 0.559, grad_norm = 1.216
I0913 19:14:04.288785 140561453410048 logging_writer.py:48] [54] global_step=54, grad_norm=1.265955, loss=0.607267
I0913 19:14:04.295202 140624454674240 pytorch_submission_base.py:86] 54) loss = 0.607, grad_norm = 1.266
I0913 19:14:04.571247 140561445017344 logging_writer.py:48] [55] global_step=55, grad_norm=1.263923, loss=0.596611
I0913 19:14:04.576641 140624454674240 pytorch_submission_base.py:86] 55) loss = 0.597, grad_norm = 1.264
I0913 19:14:04.897757 140561453410048 logging_writer.py:48] [56] global_step=56, grad_norm=1.214554, loss=0.621318
I0913 19:14:04.901066 140624454674240 pytorch_submission_base.py:86] 56) loss = 0.621, grad_norm = 1.215
I0913 19:14:05.125544 140561445017344 logging_writer.py:48] [57] global_step=57, grad_norm=1.364806, loss=0.617459
I0913 19:14:05.132996 140624454674240 pytorch_submission_base.py:86] 57) loss = 0.617, grad_norm = 1.365
I0913 19:14:05.471479 140561453410048 logging_writer.py:48] [58] global_step=58, grad_norm=1.254867, loss=0.652135
I0913 19:14:05.476982 140624454674240 pytorch_submission_base.py:86] 58) loss = 0.652, grad_norm = 1.255
I0913 19:14:05.728919 140561445017344 logging_writer.py:48] [59] global_step=59, grad_norm=1.290459, loss=0.608639
I0913 19:14:05.735456 140624454674240 pytorch_submission_base.py:86] 59) loss = 0.609, grad_norm = 1.290
I0913 19:14:06.042995 140561453410048 logging_writer.py:48] [60] global_step=60, grad_norm=1.493068, loss=0.628797
I0913 19:14:06.049712 140624454674240 pytorch_submission_base.py:86] 60) loss = 0.629, grad_norm = 1.493
I0913 19:14:06.282530 140561445017344 logging_writer.py:48] [61] global_step=61, grad_norm=1.291968, loss=0.641523
I0913 19:14:06.288790 140624454674240 pytorch_submission_base.py:86] 61) loss = 0.642, grad_norm = 1.292
I0913 19:14:06.578832 140561453410048 logging_writer.py:48] [62] global_step=62, grad_norm=1.384342, loss=0.632940
I0913 19:14:06.586539 140624454674240 pytorch_submission_base.py:86] 62) loss = 0.633, grad_norm = 1.384
I0913 19:14:06.892947 140561445017344 logging_writer.py:48] [63] global_step=63, grad_norm=1.445000, loss=0.608501
I0913 19:14:06.896503 140624454674240 pytorch_submission_base.py:86] 63) loss = 0.609, grad_norm = 1.445
I0913 19:14:07.132895 140561453410048 logging_writer.py:48] [64] global_step=64, grad_norm=1.397922, loss=0.641164
I0913 19:14:07.138374 140624454674240 pytorch_submission_base.py:86] 64) loss = 0.641, grad_norm = 1.398
I0913 19:14:07.411673 140561445017344 logging_writer.py:48] [65] global_step=65, grad_norm=1.377047, loss=0.590452
I0913 19:14:07.417502 140624454674240 pytorch_submission_base.py:86] 65) loss = 0.590, grad_norm = 1.377
I0913 19:14:07.760173 140561453410048 logging_writer.py:48] [66] global_step=66, grad_norm=1.340039, loss=0.566686
I0913 19:14:07.767919 140624454674240 pytorch_submission_base.py:86] 66) loss = 0.567, grad_norm = 1.340
I0913 19:14:08.039501 140561445017344 logging_writer.py:48] [67] global_step=67, grad_norm=1.266273, loss=0.551077
I0913 19:14:08.043012 140624454674240 pytorch_submission_base.py:86] 67) loss = 0.551, grad_norm = 1.266
I0913 19:14:08.332700 140561453410048 logging_writer.py:48] [68] global_step=68, grad_norm=1.162626, loss=0.610867
I0913 19:14:08.338367 140624454674240 pytorch_submission_base.py:86] 68) loss = 0.611, grad_norm = 1.163
I0913 19:14:08.581650 140561445017344 logging_writer.py:48] [69] global_step=69, grad_norm=1.239127, loss=0.534421
I0913 19:14:08.585657 140624454674240 pytorch_submission_base.py:86] 69) loss = 0.534, grad_norm = 1.239
I0913 19:14:08.917203 140561453410048 logging_writer.py:48] [70] global_step=70, grad_norm=1.228816, loss=0.502009
I0913 19:14:08.920693 140624454674240 pytorch_submission_base.py:86] 70) loss = 0.502, grad_norm = 1.229
I0913 19:14:09.190751 140561445017344 logging_writer.py:48] [71] global_step=71, grad_norm=1.099171, loss=0.483447
I0913 19:14:09.194205 140624454674240 pytorch_submission_base.py:86] 71) loss = 0.483, grad_norm = 1.099
I0913 19:14:09.459216 140561453410048 logging_writer.py:48] [72] global_step=72, grad_norm=0.994942, loss=0.531554
I0913 19:14:09.464752 140624454674240 pytorch_submission_base.py:86] 72) loss = 0.532, grad_norm = 0.995
I0913 19:14:09.740749 140561445017344 logging_writer.py:48] [73] global_step=73, grad_norm=1.066016, loss=0.413150
I0913 19:14:09.745743 140624454674240 pytorch_submission_base.py:86] 73) loss = 0.413, grad_norm = 1.066
I0913 19:14:09.957363 140561453410048 logging_writer.py:48] [74] global_step=74, grad_norm=1.098808, loss=0.402439
I0913 19:14:09.962700 140624454674240 pytorch_submission_base.py:86] 74) loss = 0.402, grad_norm = 1.099
I0913 19:14:10.296544 140561445017344 logging_writer.py:48] [75] global_step=75, grad_norm=0.941513, loss=0.384135
I0913 19:14:10.301825 140624454674240 pytorch_submission_base.py:86] 75) loss = 0.384, grad_norm = 0.942
I0913 19:14:10.575095 140561453410048 logging_writer.py:48] [76] global_step=76, grad_norm=0.871612, loss=0.341267
I0913 19:14:10.582754 140624454674240 pytorch_submission_base.py:86] 76) loss = 0.341, grad_norm = 0.872
I0913 19:14:10.829447 140561445017344 logging_writer.py:48] [77] global_step=77, grad_norm=0.552440, loss=0.388250
I0913 19:14:10.835089 140624454674240 pytorch_submission_base.py:86] 77) loss = 0.388, grad_norm = 0.552
I0913 19:14:11.104050 140561453410048 logging_writer.py:48] [78] global_step=78, grad_norm=0.532326, loss=0.369847
I0913 19:14:11.109857 140624454674240 pytorch_submission_base.py:86] 78) loss = 0.370, grad_norm = 0.532
I0913 19:14:11.388628 140561445017344 logging_writer.py:48] [79] global_step=79, grad_norm=0.786680, loss=0.271075
I0913 19:14:11.394211 140624454674240 pytorch_submission_base.py:86] 79) loss = 0.271, grad_norm = 0.787
I0913 19:14:11.589601 140561453410048 logging_writer.py:48] [80] global_step=80, grad_norm=0.606885, loss=0.328602
I0913 19:14:11.595914 140624454674240 pytorch_submission_base.py:86] 80) loss = 0.329, grad_norm = 0.607
I0913 19:14:11.839623 140561445017344 logging_writer.py:48] [81] global_step=81, grad_norm=0.707481, loss=0.270022
I0913 19:14:11.843457 140624454674240 pytorch_submission_base.py:86] 81) loss = 0.270, grad_norm = 0.707
I0913 19:14:12.126633 140561453410048 logging_writer.py:48] [82] global_step=82, grad_norm=0.789005, loss=0.395344
I0913 19:14:12.132465 140624454674240 pytorch_submission_base.py:86] 82) loss = 0.395, grad_norm = 0.789
I0913 19:14:12.436799 140561445017344 logging_writer.py:48] [83] global_step=83, grad_norm=0.837621, loss=0.295655
I0913 19:14:12.443954 140624454674240 pytorch_submission_base.py:86] 83) loss = 0.296, grad_norm = 0.838
I0913 19:14:12.700530 140561453410048 logging_writer.py:48] [84] global_step=84, grad_norm=0.908773, loss=0.304416
I0913 19:14:12.707023 140624454674240 pytorch_submission_base.py:86] 84) loss = 0.304, grad_norm = 0.909
I0913 19:14:12.926310 140561445017344 logging_writer.py:48] [85] global_step=85, grad_norm=0.944540, loss=0.408684
I0913 19:14:12.933408 140624454674240 pytorch_submission_base.py:86] 85) loss = 0.409, grad_norm = 0.945
I0913 19:14:13.213508 140561453410048 logging_writer.py:48] [86] global_step=86, grad_norm=0.974931, loss=0.450788
I0913 19:14:13.220881 140624454674240 pytorch_submission_base.py:86] 86) loss = 0.451, grad_norm = 0.975
I0913 19:14:13.432710 140561445017344 logging_writer.py:48] [87] global_step=87, grad_norm=1.138295, loss=0.382073
I0913 19:14:13.436390 140624454674240 pytorch_submission_base.py:86] 87) loss = 0.382, grad_norm = 1.138
I0913 19:14:13.694464 140561453410048 logging_writer.py:48] [88] global_step=88, grad_norm=1.133023, loss=0.425040
I0913 19:14:13.698040 140624454674240 pytorch_submission_base.py:86] 88) loss = 0.425, grad_norm = 1.133
I0913 19:14:13.952772 140561445017344 logging_writer.py:48] [89] global_step=89, grad_norm=1.120913, loss=0.356057
I0913 19:14:13.956460 140624454674240 pytorch_submission_base.py:86] 89) loss = 0.356, grad_norm = 1.121
I0913 19:14:14.261229 140561453410048 logging_writer.py:48] [90] global_step=90, grad_norm=1.126371, loss=0.348750
I0913 19:14:14.267740 140624454674240 pytorch_submission_base.py:86] 90) loss = 0.349, grad_norm = 1.126
I0913 19:14:14.523238 140561445017344 logging_writer.py:48] [91] global_step=91, grad_norm=1.139134, loss=0.403791
I0913 19:14:14.529577 140624454674240 pytorch_submission_base.py:86] 91) loss = 0.404, grad_norm = 1.139
I0913 19:14:14.782098 140561453410048 logging_writer.py:48] [92] global_step=92, grad_norm=1.202580, loss=0.420058
I0913 19:14:14.787577 140624454674240 pytorch_submission_base.py:86] 92) loss = 0.420, grad_norm = 1.203
I0913 19:14:15.027664 140561445017344 logging_writer.py:48] [93] global_step=93, grad_norm=1.048746, loss=0.411563
I0913 19:14:15.031552 140624454674240 pytorch_submission_base.py:86] 93) loss = 0.412, grad_norm = 1.049
I0913 19:14:15.357371 140561453410048 logging_writer.py:48] [94] global_step=94, grad_norm=1.099513, loss=0.389092
I0913 19:14:15.361056 140624454674240 pytorch_submission_base.py:86] 94) loss = 0.389, grad_norm = 1.100
I0913 19:14:15.655575 140561445017344 logging_writer.py:48] [95] global_step=95, grad_norm=1.079322, loss=0.380516
I0913 19:14:15.659065 140624454674240 pytorch_submission_base.py:86] 95) loss = 0.381, grad_norm = 1.079
I0913 19:14:15.885784 140561453410048 logging_writer.py:48] [96] global_step=96, grad_norm=1.031831, loss=0.369987
I0913 19:14:15.889219 140624454674240 pytorch_submission_base.py:86] 96) loss = 0.370, grad_norm = 1.032
I0913 19:14:16.138368 140561445017344 logging_writer.py:48] [97] global_step=97, grad_norm=1.015615, loss=0.486774
I0913 19:14:16.144342 140624454674240 pytorch_submission_base.py:86] 97) loss = 0.487, grad_norm = 1.016
I0913 19:14:16.437521 140561453410048 logging_writer.py:48] [98] global_step=98, grad_norm=0.989372, loss=0.364928
I0913 19:14:16.440937 140624454674240 pytorch_submission_base.py:86] 98) loss = 0.365, grad_norm = 0.989
I0913 19:14:16.665725 140561445017344 logging_writer.py:48] [99] global_step=99, grad_norm=0.938944, loss=0.450535
I0913 19:14:16.672164 140624454674240 pytorch_submission_base.py:86] 99) loss = 0.451, grad_norm = 0.939
I0913 19:14:16.972415 140561453410048 logging_writer.py:48] [100] global_step=100, grad_norm=0.862716, loss=0.397658
I0913 19:14:16.977476 140624454674240 pytorch_submission_base.py:86] 100) loss = 0.398, grad_norm = 0.863
I0913 19:15:12.192979 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:15:14.165735 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:15:16.179263 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:15:18.165503 140624454674240 submission_runner.py:376] Time since start: 403.65s, 	Step: 304, 	{'train/ssim': 0.6713861737932477, 'train/loss': 0.33277484348842074, 'validation/ssim': 0.6462071922701533, 'validation/loss': 0.3543467610219119, 'validation/num_examples': 3554, 'test/ssim': 0.665678627761973, 'test/loss': 0.35553246109021575, 'test/num_examples': 3581, 'score': 168.76450395584106, 'total_duration': 403.6501896381378, 'accumulated_submission_time': 168.76450395584106, 'accumulated_eval_time': 233.36722874641418, 'accumulated_logging_time': 0.029778242111206055}
I0913 19:15:18.184746 140561445017344 logging_writer.py:48] [304] accumulated_eval_time=233.367229, accumulated_logging_time=0.029778, accumulated_submission_time=168.764504, global_step=304, preemption_count=0, score=168.764504, test/loss=0.355532, test/num_examples=3581, test/ssim=0.665679, total_duration=403.650190, train/loss=0.332775, train/ssim=0.671386, validation/loss=0.354347, validation/num_examples=3554, validation/ssim=0.646207
I0913 19:16:34.869797 140561453410048 logging_writer.py:48] [500] global_step=500, grad_norm=0.149074, loss=0.242351
I0913 19:16:34.875950 140624454674240 pytorch_submission_base.py:86] 500) loss = 0.242, grad_norm = 0.149
I0913 19:16:38.941060 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:16:40.862765 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:16:42.856149 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:16:44.832709 140624454674240 submission_runner.py:376] Time since start: 490.32s, 	Step: 509, 	{'train/ssim': 0.6984647342136928, 'train/loss': 0.3146581990378244, 'validation/ssim': 0.6773565546743107, 'validation/loss': 0.3343338244539603, 'validation/num_examples': 3554, 'test/ssim': 0.6959390435196174, 'test/loss': 0.33571146027864074, 'test/num_examples': 3581, 'score': 248.55980515480042, 'total_duration': 490.3173952102661, 'accumulated_submission_time': 248.55980515480042, 'accumulated_eval_time': 239.25912737846375, 'accumulated_logging_time': 0.06118941307067871}
I0913 19:16:44.851923 140561445017344 logging_writer.py:48] [509] accumulated_eval_time=239.259127, accumulated_logging_time=0.061189, accumulated_submission_time=248.559805, global_step=509, preemption_count=0, score=248.559805, test/loss=0.335711, test/num_examples=3581, test/ssim=0.695939, total_duration=490.317395, train/loss=0.314658, train/ssim=0.698465, validation/loss=0.334334, validation/num_examples=3554, validation/ssim=0.677357
I0913 19:18:05.321640 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:18:07.270585 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:18:09.250251 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:18:11.217532 140624454674240 submission_runner.py:376] Time since start: 576.70s, 	Step: 716, 	{'train/ssim': 0.7024536814008441, 'train/loss': 0.3062374251229422, 'validation/ssim': 0.6825161377365293, 'validation/loss': 0.3249259609550858, 'validation/num_examples': 3554, 'test/ssim': 0.7006145307918529, 'test/loss': 0.3266439302045518, 'test/num_examples': 3581, 'score': 328.0744700431824, 'total_duration': 576.7022390365601, 'accumulated_submission_time': 328.0744700431824, 'accumulated_eval_time': 245.15544176101685, 'accumulated_logging_time': 0.09543895721435547}
I0913 19:18:11.238353 140561453410048 logging_writer.py:48] [716] accumulated_eval_time=245.155442, accumulated_logging_time=0.095439, accumulated_submission_time=328.074470, global_step=716, preemption_count=0, score=328.074470, test/loss=0.326644, test/num_examples=3581, test/ssim=0.700615, total_duration=576.702239, train/loss=0.306237, train/ssim=0.702454, validation/loss=0.324926, validation/num_examples=3554, validation/ssim=0.682516
I0913 19:19:32.119865 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:19:34.062044 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:19:38.478084 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:19:40.448964 140624454674240 submission_runner.py:376] Time since start: 665.93s, 	Step: 918, 	{'train/ssim': 0.713130065373012, 'train/loss': 0.29831065450395855, 'validation/ssim': 0.6924108398987057, 'validation/loss': 0.3172396529438661, 'validation/num_examples': 3554, 'test/ssim': 0.7103617480932352, 'test/loss': 0.3189298091271642, 'test/num_examples': 3581, 'score': 407.9741961956024, 'total_duration': 665.933581829071, 'accumulated_submission_time': 407.9741961956024, 'accumulated_eval_time': 253.48493003845215, 'accumulated_logging_time': 0.13149571418762207}
I0913 19:19:40.477045 140561445017344 logging_writer.py:48] [918] accumulated_eval_time=253.484930, accumulated_logging_time=0.131496, accumulated_submission_time=407.974196, global_step=918, preemption_count=0, score=407.974196, test/loss=0.318930, test/num_examples=3581, test/ssim=0.710362, total_duration=665.933582, train/loss=0.298311, train/ssim=0.713130, validation/loss=0.317240, validation/num_examples=3554, validation/ssim=0.692411
I0913 19:20:03.858056 140561453410048 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.464942, loss=0.231325
I0913 19:20:03.863132 140624454674240 pytorch_submission_base.py:86] 1000) loss = 0.231, grad_norm = 0.465
I0913 19:21:00.957487 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:21:02.851984 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:21:04.936403 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:21:06.852084 140624454674240 submission_runner.py:376] Time since start: 752.34s, 	Step: 1213, 	{'train/ssim': 0.7194157327924456, 'train/loss': 0.28892179897853304, 'validation/ssim': 0.6983305974562113, 'validation/loss': 0.3073576936308561, 'validation/num_examples': 3554, 'test/ssim': 0.7159623244860025, 'test/loss': 0.30920222670430747, 'test/num_examples': 3581, 'score': 487.44238328933716, 'total_duration': 752.3367941379547, 'accumulated_submission_time': 487.44238328933716, 'accumulated_eval_time': 259.37974095344543, 'accumulated_logging_time': 0.17536449432373047}
I0913 19:21:06.868931 140561445017344 logging_writer.py:48] [1213] accumulated_eval_time=259.379741, accumulated_logging_time=0.175364, accumulated_submission_time=487.442383, global_step=1213, preemption_count=0, score=487.442383, test/loss=0.309202, test/num_examples=3581, test/ssim=0.715962, total_duration=752.336794, train/loss=0.288922, train/ssim=0.719416, validation/loss=0.307358, validation/num_examples=3554, validation/ssim=0.698331
I0913 19:22:21.501919 140561453410048 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.142316, loss=0.286086
I0913 19:22:21.505923 140624454674240 pytorch_submission_base.py:86] 1500) loss = 0.286, grad_norm = 0.142
I0913 19:22:27.325330 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:22:29.218848 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:22:31.195020 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:22:33.116302 140624454674240 submission_runner.py:376] Time since start: 838.60s, 	Step: 1521, 	{'train/ssim': 0.7265268053327288, 'train/loss': 0.2833776814596994, 'validation/ssim': 0.705361696328081, 'validation/loss': 0.3018972282001794, 'validation/num_examples': 3554, 'test/ssim': 0.722781626880585, 'test/loss': 0.3035162590429524, 'test/num_examples': 3581, 'score': 566.9691789150238, 'total_duration': 838.6010181903839, 'accumulated_submission_time': 566.9691789150238, 'accumulated_eval_time': 265.1709449291229, 'accumulated_logging_time': 0.20132207870483398}
I0913 19:22:33.132106 140561445017344 logging_writer.py:48] [1521] accumulated_eval_time=265.170945, accumulated_logging_time=0.201322, accumulated_submission_time=566.969179, global_step=1521, preemption_count=0, score=566.969179, test/loss=0.303516, test/num_examples=3581, test/ssim=0.722782, total_duration=838.601018, train/loss=0.283378, train/ssim=0.726527, validation/loss=0.301897, validation/num_examples=3554, validation/ssim=0.705362
I0913 19:23:53.542384 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:23:55.435669 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:23:57.393412 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:23:59.318010 140624454674240 submission_runner.py:376] Time since start: 924.80s, 	Step: 1829, 	{'train/ssim': 0.7199441364833287, 'train/loss': 0.2844561849321638, 'validation/ssim': 0.7010455456220104, 'validation/loss': 0.3022157994293402, 'validation/num_examples': 3554, 'test/ssim': 0.7176431518997836, 'test/loss': 0.30399468876535884, 'test/num_examples': 3581, 'score': 646.4586946964264, 'total_duration': 924.8027279376984, 'accumulated_submission_time': 646.4586946964264, 'accumulated_eval_time': 270.94679856300354, 'accumulated_logging_time': 0.2254931926727295}
I0913 19:23:59.333599 140561453410048 logging_writer.py:48] [1829] accumulated_eval_time=270.946799, accumulated_logging_time=0.225493, accumulated_submission_time=646.458695, global_step=1829, preemption_count=0, score=646.458695, test/loss=0.303995, test/num_examples=3581, test/ssim=0.717643, total_duration=924.802728, train/loss=0.284456, train/ssim=0.719944, validation/loss=0.302216, validation/num_examples=3554, validation/ssim=0.701046
I0913 19:24:43.593644 140561445017344 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.161265, loss=0.245065
I0913 19:24:43.597498 140624454674240 pytorch_submission_base.py:86] 2000) loss = 0.245, grad_norm = 0.161
I0913 19:25:19.984725 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:25:21.861298 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:25:23.818562 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:25:25.745516 140624454674240 submission_runner.py:376] Time since start: 1011.23s, 	Step: 2135, 	{'train/ssim': 0.7326239177158901, 'train/loss': 0.2802218198776245, 'validation/ssim': 0.7113897165825478, 'validation/loss': 0.29858432784802336, 'validation/num_examples': 3554, 'test/ssim': 0.7282336463278414, 'test/loss': 0.3008277805715059, 'test/num_examples': 3581, 'score': 726.1265122890472, 'total_duration': 1011.2302215099335, 'accumulated_submission_time': 726.1265122890472, 'accumulated_eval_time': 276.707816362381, 'accumulated_logging_time': 0.25042247772216797}
I0913 19:25:25.760821 140561453410048 logging_writer.py:48] [2135] accumulated_eval_time=276.707816, accumulated_logging_time=0.250422, accumulated_submission_time=726.126512, global_step=2135, preemption_count=0, score=726.126512, test/loss=0.300828, test/num_examples=3581, test/ssim=0.728234, total_duration=1011.230222, train/loss=0.280222, train/ssim=0.732624, validation/loss=0.298584, validation/num_examples=3554, validation/ssim=0.711390
I0913 19:26:46.309138 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:26:48.212874 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:26:50.175925 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:26:52.118967 140624454674240 submission_runner.py:376] Time since start: 1097.60s, 	Step: 2444, 	{'train/ssim': 0.734095709664481, 'train/loss': 0.2759362118584769, 'validation/ssim': 0.712030705938731, 'validation/loss': 0.2945085395636958, 'validation/num_examples': 3554, 'test/ssim': 0.7293415852546425, 'test/loss': 0.29620390303206157, 'test/num_examples': 3581, 'score': 805.723030090332, 'total_duration': 1097.6036384105682, 'accumulated_submission_time': 805.723030090332, 'accumulated_eval_time': 282.5178198814392, 'accumulated_logging_time': 0.27398061752319336}
I0913 19:26:52.135073 140561445017344 logging_writer.py:48] [2444] accumulated_eval_time=282.517820, accumulated_logging_time=0.273981, accumulated_submission_time=805.723030, global_step=2444, preemption_count=0, score=805.723030, test/loss=0.296204, test/num_examples=3581, test/ssim=0.729342, total_duration=1097.603638, train/loss=0.275936, train/ssim=0.734096, validation/loss=0.294509, validation/num_examples=3554, validation/ssim=0.712031
I0913 19:27:05.131747 140561453410048 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.177003, loss=0.305956
I0913 19:27:05.135974 140624454674240 pytorch_submission_base.py:86] 2500) loss = 0.306, grad_norm = 0.177
I0913 19:28:12.760830 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:28:14.643689 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:28:16.598354 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:28:18.536491 140624454674240 submission_runner.py:376] Time since start: 1184.02s, 	Step: 2753, 	{'train/ssim': 0.7337126731872559, 'train/loss': 0.2757058824811663, 'validation/ssim': 0.7117318157138084, 'validation/loss': 0.294372867719471, 'validation/num_examples': 3554, 'test/ssim': 0.729050266379852, 'test/loss': 0.2960281095102625, 'test/num_examples': 3581, 'score': 885.3843231201172, 'total_duration': 1184.0211849212646, 'accumulated_submission_time': 885.3843231201172, 'accumulated_eval_time': 288.2936661243439, 'accumulated_logging_time': 0.29852747917175293}
I0913 19:28:18.552301 140561445017344 logging_writer.py:48] [2753] accumulated_eval_time=288.293666, accumulated_logging_time=0.298527, accumulated_submission_time=885.384323, global_step=2753, preemption_count=0, score=885.384323, test/loss=0.296028, test/num_examples=3581, test/ssim=0.729050, total_duration=1184.021185, train/loss=0.275706, train/ssim=0.733713, validation/loss=0.294373, validation/num_examples=3554, validation/ssim=0.711732
I0913 19:29:21.966654 140561453410048 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.295902, loss=0.269818
I0913 19:29:21.970137 140624454674240 pytorch_submission_base.py:86] 3000) loss = 0.270, grad_norm = 0.296
I0913 19:29:39.222579 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:29:41.144127 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:29:43.110417 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:29:45.052458 140624454674240 submission_runner.py:376] Time since start: 1270.54s, 	Step: 3064, 	{'train/ssim': 0.7348917552403041, 'train/loss': 0.27558508941105436, 'validation/ssim': 0.7122638554269837, 'validation/loss': 0.2941456946618247, 'validation/num_examples': 3554, 'test/ssim': 0.7296085650612608, 'test/loss': 0.2959964073626431, 'test/num_examples': 3581, 'score': 965.0553374290466, 'total_duration': 1270.5371618270874, 'accumulated_submission_time': 965.0553374290466, 'accumulated_eval_time': 294.1238112449646, 'accumulated_logging_time': 0.3228447437286377}
I0913 19:29:45.068240 140561445017344 logging_writer.py:48] [3064] accumulated_eval_time=294.123811, accumulated_logging_time=0.322845, accumulated_submission_time=965.055337, global_step=3064, preemption_count=0, score=965.055337, test/loss=0.295996, test/num_examples=3581, test/ssim=0.729609, total_duration=1270.537162, train/loss=0.275585, train/ssim=0.734892, validation/loss=0.294146, validation/num_examples=3554, validation/ssim=0.712264
I0913 19:31:05.511323 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:31:07.396435 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:31:09.354357 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:31:11.307104 140624454674240 submission_runner.py:376] Time since start: 1356.79s, 	Step: 3371, 	{'train/ssim': 0.7375236238752093, 'train/loss': 0.2735832248415266, 'validation/ssim': 0.7148873710739659, 'validation/loss': 0.2927653452755698, 'validation/num_examples': 3554, 'test/ssim': 0.732084877761973, 'test/loss': 0.29453950619371333, 'test/num_examples': 3581, 'score': 1044.5425403118134, 'total_duration': 1356.7918014526367, 'accumulated_submission_time': 1044.5425403118134, 'accumulated_eval_time': 299.9197461605072, 'accumulated_logging_time': 0.34690427780151367}
I0913 19:31:11.323892 140561453410048 logging_writer.py:48] [3371] accumulated_eval_time=299.919746, accumulated_logging_time=0.346904, accumulated_submission_time=1044.542540, global_step=3371, preemption_count=0, score=1044.542540, test/loss=0.294540, test/num_examples=3581, test/ssim=0.732085, total_duration=1356.791801, train/loss=0.273583, train/ssim=0.737524, validation/loss=0.292765, validation/num_examples=3554, validation/ssim=0.714887
I0913 19:31:43.331820 140561445017344 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.609442, loss=0.324771
I0913 19:31:43.335293 140624454674240 pytorch_submission_base.py:86] 3500) loss = 0.325, grad_norm = 0.609
I0913 19:32:31.869717 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:32:33.780380 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:32:35.733540 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:32:37.681304 140624454674240 submission_runner.py:376] Time since start: 1443.17s, 	Step: 3681, 	{'train/ssim': 0.7359226090567452, 'train/loss': 0.2734686817441668, 'validation/ssim': 0.7147759484251899, 'validation/loss': 0.2919789637886009, 'validation/num_examples': 3554, 'test/ssim': 0.7320963996177744, 'test/loss': 0.29348539274120355, 'test/num_examples': 3581, 'score': 1124.1040234565735, 'total_duration': 1443.1660206317902, 'accumulated_submission_time': 1124.1040234565735, 'accumulated_eval_time': 305.73155879974365, 'accumulated_logging_time': 0.3719668388366699}
I0913 19:32:37.697362 140561453410048 logging_writer.py:48] [3681] accumulated_eval_time=305.731559, accumulated_logging_time=0.371967, accumulated_submission_time=1124.104023, global_step=3681, preemption_count=0, score=1124.104023, test/loss=0.293485, test/num_examples=3581, test/ssim=0.732096, total_duration=1443.166021, train/loss=0.273469, train/ssim=0.735923, validation/loss=0.291979, validation/num_examples=3554, validation/ssim=0.714776
I0913 19:33:58.309215 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:34:00.210564 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:34:02.178218 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:34:04.143031 140624454674240 submission_runner.py:376] Time since start: 1529.63s, 	Step: 3991, 	{'train/ssim': 0.7405587605067662, 'train/loss': 0.27273580006190706, 'validation/ssim': 0.7188544844937043, 'validation/loss': 0.2917125317643852, 'validation/num_examples': 3554, 'test/ssim': 0.7361104369196803, 'test/loss': 0.29318950603008936, 'test/num_examples': 3581, 'score': 1203.7507371902466, 'total_duration': 1529.6277277469635, 'accumulated_submission_time': 1203.7507371902466, 'accumulated_eval_time': 311.565477848053, 'accumulated_logging_time': 0.3964200019836426}
I0913 19:34:04.159123 140561445017344 logging_writer.py:48] [3991] accumulated_eval_time=311.565478, accumulated_logging_time=0.396420, accumulated_submission_time=1203.750737, global_step=3991, preemption_count=0, score=1203.750737, test/loss=0.293190, test/num_examples=3581, test/ssim=0.736110, total_duration=1529.627728, train/loss=0.272736, train/ssim=0.740559, validation/loss=0.291713, validation/num_examples=3554, validation/ssim=0.718854
I0913 19:34:05.219446 140561453410048 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.021394, loss=0.251810
I0913 19:34:05.223419 140624454674240 pytorch_submission_base.py:86] 4000) loss = 0.252, grad_norm = 1.021
I0913 19:35:24.605022 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:35:26.495344 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:35:28.461083 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:35:30.391995 140624454674240 submission_runner.py:376] Time since start: 1615.88s, 	Step: 4298, 	{'train/ssim': 0.7356870515005929, 'train/loss': 0.2744684900556292, 'validation/ssim': 0.7158613918735931, 'validation/loss': 0.2925796294118423, 'validation/num_examples': 3554, 'test/ssim': 0.7326277685178721, 'test/loss': 0.2941692387448513, 'test/num_examples': 3581, 'score': 1283.2221932411194, 'total_duration': 1615.8767099380493, 'accumulated_submission_time': 1283.2221932411194, 'accumulated_eval_time': 317.35262274742126, 'accumulated_logging_time': 0.4210073947906494}
I0913 19:35:30.407913 140561445017344 logging_writer.py:48] [4298] accumulated_eval_time=317.352623, accumulated_logging_time=0.421007, accumulated_submission_time=1283.222193, global_step=4298, preemption_count=0, score=1283.222193, test/loss=0.294169, test/num_examples=3581, test/ssim=0.732628, total_duration=1615.876710, train/loss=0.274468, train/ssim=0.735687, validation/loss=0.292580, validation/num_examples=3554, validation/ssim=0.715861
I0913 19:36:21.978362 140561453410048 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.434435, loss=0.227230
I0913 19:36:21.982467 140624454674240 pytorch_submission_base.py:86] 4500) loss = 0.227, grad_norm = 0.434
I0913 19:36:51.050516 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:36:52.949099 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:36:54.902393 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:36:56.836658 140624454674240 submission_runner.py:376] Time since start: 1702.32s, 	Step: 4609, 	{'train/ssim': 0.7426456723894391, 'train/loss': 0.2706260681152344, 'validation/ssim': 0.720315824818866, 'validation/loss': 0.2897289063599114, 'validation/num_examples': 3554, 'test/ssim': 0.7376476161075817, 'test/loss': 0.29124193742800547, 'test/num_examples': 3581, 'score': 1362.904296875, 'total_duration': 1702.3213410377502, 'accumulated_submission_time': 1362.904296875, 'accumulated_eval_time': 323.138956785202, 'accumulated_logging_time': 0.4463460445404053}
I0913 19:36:56.852725 140561445017344 logging_writer.py:48] [4609] accumulated_eval_time=323.138957, accumulated_logging_time=0.446346, accumulated_submission_time=1362.904297, global_step=4609, preemption_count=0, score=1362.904297, test/loss=0.291242, test/num_examples=3581, test/ssim=0.737648, total_duration=1702.321341, train/loss=0.270626, train/ssim=0.742646, validation/loss=0.289729, validation/num_examples=3554, validation/ssim=0.720316
I0913 19:38:17.510124 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:38:19.418870 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:38:21.385281 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:38:23.337994 140624454674240 submission_runner.py:376] Time since start: 1788.82s, 	Step: 4919, 	{'train/ssim': 0.7413635935102191, 'train/loss': 0.27078393527439665, 'validation/ssim': 0.7193580159459412, 'validation/loss': 0.28967711062798956, 'validation/num_examples': 3554, 'test/ssim': 0.7366385333400237, 'test/loss': 0.29116915884180394, 'test/num_examples': 3581, 'score': 1442.6219186782837, 'total_duration': 1788.8227026462555, 'accumulated_submission_time': 1442.6219186782837, 'accumulated_eval_time': 328.9669449329376, 'accumulated_logging_time': 0.47068333625793457}
I0913 19:38:23.354122 140561453410048 logging_writer.py:48] [4919] accumulated_eval_time=328.966945, accumulated_logging_time=0.470683, accumulated_submission_time=1442.621919, global_step=4919, preemption_count=0, score=1442.621919, test/loss=0.291169, test/num_examples=3581, test/ssim=0.736639, total_duration=1788.822703, train/loss=0.270784, train/ssim=0.741364, validation/loss=0.289677, validation/num_examples=3554, validation/ssim=0.719358
I0913 19:38:43.086434 140561445017344 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.443654, loss=0.308488
I0913 19:38:43.089979 140624454674240 pytorch_submission_base.py:86] 5000) loss = 0.308, grad_norm = 0.444
I0913 19:39:43.779180 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:39:45.675109 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:39:47.628861 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:39:49.565250 140624454674240 submission_runner.py:376] Time since start: 1875.05s, 	Step: 5227, 	{'train/ssim': 0.7392706189836774, 'train/loss': 0.2727593353816441, 'validation/ssim': 0.7171125954030669, 'validation/loss': 0.29157517690234597, 'validation/num_examples': 3554, 'test/ssim': 0.7342299882408894, 'test/loss': 0.2932301393203714, 'test/num_examples': 3581, 'score': 1522.107471704483, 'total_duration': 1875.049962759018, 'accumulated_submission_time': 1522.107471704483, 'accumulated_eval_time': 334.7532093524933, 'accumulated_logging_time': 0.4950675964355469}
I0913 19:39:49.581935 140561453410048 logging_writer.py:48] [5227] accumulated_eval_time=334.753209, accumulated_logging_time=0.495068, accumulated_submission_time=1522.107472, global_step=5227, preemption_count=0, score=1522.107472, test/loss=0.293230, test/num_examples=3581, test/ssim=0.734230, total_duration=1875.049963, train/loss=0.272759, train/ssim=0.739271, validation/loss=0.291575, validation/num_examples=3554, validation/ssim=0.717113
I0913 19:41:01.126250 140561445017344 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.312484, loss=0.222165
I0913 19:41:01.129966 140624454674240 pytorch_submission_base.py:86] 5500) loss = 0.222, grad_norm = 0.312
I0913 19:41:10.019314 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:41:11.942211 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:41:13.889281 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:41:15.823380 140624454674240 submission_runner.py:376] Time since start: 1961.31s, 	Step: 5533, 	{'train/ssim': 0.742063045501709, 'train/loss': 0.2722414561680385, 'validation/ssim': 0.7204126155168472, 'validation/loss': 0.29106817637257315, 'validation/num_examples': 3554, 'test/ssim': 0.7373622286023457, 'test/loss': 0.29272505252330006, 'test/num_examples': 3581, 'score': 1601.614014863968, 'total_duration': 1961.308075428009, 'accumulated_submission_time': 1601.614014863968, 'accumulated_eval_time': 340.55744791030884, 'accumulated_logging_time': 0.520179033279419}
I0913 19:41:15.839549 140561453410048 logging_writer.py:48] [5533] accumulated_eval_time=340.557448, accumulated_logging_time=0.520179, accumulated_submission_time=1601.614015, global_step=5533, preemption_count=0, score=1601.614015, test/loss=0.292725, test/num_examples=3581, test/ssim=0.737362, total_duration=1961.308075, train/loss=0.272241, train/ssim=0.742063, validation/loss=0.291068, validation/num_examples=3554, validation/ssim=0.720413
I0913 19:42:36.269724 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:42:38.102751 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:42:39.951383 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:42:41.786035 140624454674240 submission_runner.py:376] Time since start: 2047.27s, 	Step: 5843, 	{'train/ssim': 0.736884321485247, 'train/loss': 0.27581705365862164, 'validation/ssim': 0.7147157032568936, 'validation/loss': 0.2947708842589336, 'validation/num_examples': 3554, 'test/ssim': 0.7315846655962022, 'test/loss': 0.2966098609632435, 'test/num_examples': 3581, 'score': 1681.093552827835, 'total_duration': 2047.2707061767578, 'accumulated_submission_time': 1681.093552827835, 'accumulated_eval_time': 346.07404351234436, 'accumulated_logging_time': 0.5448422431945801}
I0913 19:42:41.804039 140561445017344 logging_writer.py:48] [5843] accumulated_eval_time=346.074044, accumulated_logging_time=0.544842, accumulated_submission_time=1681.093553, global_step=5843, preemption_count=0, score=1681.093553, test/loss=0.296610, test/num_examples=3581, test/ssim=0.731585, total_duration=2047.270706, train/loss=0.275817, train/ssim=0.736884, validation/loss=0.294771, validation/num_examples=3554, validation/ssim=0.714716
I0913 19:43:21.747964 140561453410048 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.436147, loss=0.220569
I0913 19:43:21.752028 140624454674240 pytorch_submission_base.py:86] 6000) loss = 0.221, grad_norm = 0.436
I0913 19:44:02.325500 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:44:04.348705 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:44:06.421392 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:44:08.253505 140624454674240 submission_runner.py:376] Time since start: 2133.74s, 	Step: 6151, 	{'train/ssim': 0.741410459790911, 'train/loss': 0.2702733107975551, 'validation/ssim': 0.7190236106104038, 'validation/loss': 0.28938831850995356, 'validation/num_examples': 3554, 'test/ssim': 0.7365850828373709, 'test/loss': 0.2907710753150307, 'test/num_examples': 3581, 'score': 1760.6165096759796, 'total_duration': 2133.738173007965, 'accumulated_submission_time': 1760.6165096759796, 'accumulated_eval_time': 352.0022325515747, 'accumulated_logging_time': 0.5718903541564941}
I0913 19:44:08.272001 140561445017344 logging_writer.py:48] [6151] accumulated_eval_time=352.002233, accumulated_logging_time=0.571890, accumulated_submission_time=1760.616510, global_step=6151, preemption_count=0, score=1760.616510, test/loss=0.290771, test/num_examples=3581, test/ssim=0.736585, total_duration=2133.738173, train/loss=0.270273, train/ssim=0.741410, validation/loss=0.289388, validation/num_examples=3554, validation/ssim=0.719024
I0913 19:45:29.015064 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:45:30.907354 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:45:32.844558 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:45:34.870546 140624454674240 submission_runner.py:376] Time since start: 2220.36s, 	Step: 6459, 	{'train/ssim': 0.7455499512808663, 'train/loss': 0.26953298704964773, 'validation/ssim': 0.7230374363613182, 'validation/loss': 0.2889066662893043, 'validation/num_examples': 3554, 'test/ssim': 0.7401407684436959, 'test/loss': 0.29037585520804243, 'test/num_examples': 3581, 'score': 1840.3476071357727, 'total_duration': 2220.3552284240723, 'accumulated_submission_time': 1840.3476071357727, 'accumulated_eval_time': 357.85781145095825, 'accumulated_logging_time': 0.5994911193847656}
I0913 19:45:34.886939 140561453410048 logging_writer.py:48] [6459] accumulated_eval_time=357.857811, accumulated_logging_time=0.599491, accumulated_submission_time=1840.347607, global_step=6459, preemption_count=0, score=1840.347607, test/loss=0.290376, test/num_examples=3581, test/ssim=0.740141, total_duration=2220.355228, train/loss=0.269533, train/ssim=0.745550, validation/loss=0.288907, validation/num_examples=3554, validation/ssim=0.723037
I0913 19:45:43.836526 140561445017344 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.215752, loss=0.291775
I0913 19:45:43.839888 140624454674240 pytorch_submission_base.py:86] 6500) loss = 0.292, grad_norm = 0.216
I0913 19:46:55.407815 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:46:57.261340 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:46:59.105154 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:47:00.936585 140624454674240 submission_runner.py:376] Time since start: 2306.42s, 	Step: 6771, 	{'train/ssim': 0.7444658279418945, 'train/loss': 0.2686554193496704, 'validation/ssim': 0.7217985978606851, 'validation/loss': 0.288065861501785, 'validation/num_examples': 3554, 'test/ssim': 0.7390641907812063, 'test/loss': 0.2894817523801836, 'test/num_examples': 3581, 'score': 1919.8285059928894, 'total_duration': 2306.4212551116943, 'accumulated_submission_time': 1919.8285059928894, 'accumulated_eval_time': 363.38677406311035, 'accumulated_logging_time': 0.6258382797241211}
I0913 19:47:00.955551 140561453410048 logging_writer.py:48] [6771] accumulated_eval_time=363.386774, accumulated_logging_time=0.625838, accumulated_submission_time=1919.828506, global_step=6771, preemption_count=0, score=1919.828506, test/loss=0.289482, test/num_examples=3581, test/ssim=0.739064, total_duration=2306.421255, train/loss=0.268655, train/ssim=0.744466, validation/loss=0.288066, validation/num_examples=3554, validation/ssim=0.721799
I0913 19:48:00.357257 140561445017344 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.429491, loss=0.223295
I0913 19:48:00.361023 140624454674240 pytorch_submission_base.py:86] 7000) loss = 0.223, grad_norm = 0.429
I0913 19:48:21.593313 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:48:23.579576 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:48:25.627073 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:48:27.648435 140624454674240 submission_runner.py:376] Time since start: 2393.13s, 	Step: 7079, 	{'train/ssim': 0.7423040526253837, 'train/loss': 0.26992688860212055, 'validation/ssim': 0.719702794166784, 'validation/loss': 0.2892800558129924, 'validation/num_examples': 3554, 'test/ssim': 0.7370093462021782, 'test/loss': 0.2907933009066951, 'test/num_examples': 3581, 'score': 1999.4033722877502, 'total_duration': 2393.1330904960632, 'accumulated_submission_time': 1999.4033722877502, 'accumulated_eval_time': 369.4419786930084, 'accumulated_logging_time': 0.6538686752319336}
I0913 19:48:27.664744 140561453410048 logging_writer.py:48] [7079] accumulated_eval_time=369.441979, accumulated_logging_time=0.653869, accumulated_submission_time=1999.403372, global_step=7079, preemption_count=0, score=1999.403372, test/loss=0.290793, test/num_examples=3581, test/ssim=0.737009, total_duration=2393.133090, train/loss=0.269927, train/ssim=0.742304, validation/loss=0.289280, validation/num_examples=3554, validation/ssim=0.719703
I0913 19:49:48.168528 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:49:50.068127 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:49:52.023986 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:49:53.957679 140624454674240 submission_runner.py:376] Time since start: 2479.44s, 	Step: 7389, 	{'train/ssim': 0.7438771384102958, 'train/loss': 0.26996987206595285, 'validation/ssim': 0.7210762053979671, 'validation/loss': 0.2891653358231922, 'validation/num_examples': 3554, 'test/ssim': 0.7382988395786791, 'test/loss': 0.29082984359728425, 'test/num_examples': 3581, 'score': 2078.963389158249, 'total_duration': 2479.4423944950104, 'accumulated_submission_time': 2078.963389158249, 'accumulated_eval_time': 375.2313406467438, 'accumulated_logging_time': 0.6784687042236328}
I0913 19:49:53.973839 140561445017344 logging_writer.py:48] [7389] accumulated_eval_time=375.231341, accumulated_logging_time=0.678469, accumulated_submission_time=2078.963389, global_step=7389, preemption_count=0, score=2078.963389, test/loss=0.290830, test/num_examples=3581, test/ssim=0.738299, total_duration=2479.442394, train/loss=0.269970, train/ssim=0.743877, validation/loss=0.289165, validation/num_examples=3554, validation/ssim=0.721076
I0913 19:50:22.198357 140561453410048 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.243891, loss=0.322139
I0913 19:50:22.201855 140624454674240 pytorch_submission_base.py:86] 7500) loss = 0.322, grad_norm = 0.244
I0913 19:51:14.487001 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:51:16.330776 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:51:18.189057 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:51:20.021019 140624454674240 submission_runner.py:376] Time since start: 2565.51s, 	Step: 7697, 	{'train/ssim': 0.7405800138201032, 'train/loss': 0.27401314462934223, 'validation/ssim': 0.7179149483856219, 'validation/loss': 0.29358789447189787, 'validation/num_examples': 3554, 'test/ssim': 0.7347750606499581, 'test/loss': 0.2953690798114179, 'test/num_examples': 3581, 'score': 2158.5110325813293, 'total_duration': 2565.505722284317, 'accumulated_submission_time': 2158.5110325813293, 'accumulated_eval_time': 380.76561093330383, 'accumulated_logging_time': 0.702869176864624}
I0913 19:51:20.039661 140561445017344 logging_writer.py:48] [7697] accumulated_eval_time=380.765611, accumulated_logging_time=0.702869, accumulated_submission_time=2158.511033, global_step=7697, preemption_count=0, score=2158.511033, test/loss=0.295369, test/num_examples=3581, test/ssim=0.734775, total_duration=2565.505722, train/loss=0.274013, train/ssim=0.740580, validation/loss=0.293588, validation/num_examples=3554, validation/ssim=0.717915
I0913 19:52:38.085279 140561453410048 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.169407, loss=0.224706
I0913 19:52:38.089138 140624454674240 pytorch_submission_base.py:86] 8000) loss = 0.225, grad_norm = 0.169
I0913 19:52:40.645535 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:52:42.659382 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:52:44.713821 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:52:46.740539 140624454674240 submission_runner.py:376] Time since start: 2652.23s, 	Step: 8009, 	{'train/ssim': 0.7442974363054548, 'train/loss': 0.26928649629865375, 'validation/ssim': 0.7214374703239308, 'validation/loss': 0.2887322850353475, 'validation/num_examples': 3554, 'test/ssim': 0.7388245498158684, 'test/loss': 0.29023783155674043, 'test/num_examples': 3581, 'score': 2238.0990495681763, 'total_duration': 2652.2252440452576, 'accumulated_submission_time': 2238.0990495681763, 'accumulated_eval_time': 386.8607077598572, 'accumulated_logging_time': 0.7306511402130127}
I0913 19:52:46.757205 140561445017344 logging_writer.py:48] [8009] accumulated_eval_time=386.860708, accumulated_logging_time=0.730651, accumulated_submission_time=2238.099050, global_step=8009, preemption_count=0, score=2238.099050, test/loss=0.290238, test/num_examples=3581, test/ssim=0.738825, total_duration=2652.225244, train/loss=0.269286, train/ssim=0.744297, validation/loss=0.288732, validation/num_examples=3554, validation/ssim=0.721437
I0913 19:54:07.374584 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:54:09.196137 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:54:11.035864 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:54:12.860722 140624454674240 submission_runner.py:376] Time since start: 2738.35s, 	Step: 8318, 	{'train/ssim': 0.7410651615687779, 'train/loss': 0.2717444896697998, 'validation/ssim': 0.7197819303513646, 'validation/loss': 0.29077862861388576, 'validation/num_examples': 3554, 'test/ssim': 0.7368259509826166, 'test/loss': 0.29214030129721097, 'test/num_examples': 3581, 'score': 2317.7767033576965, 'total_duration': 2738.345399618149, 'accumulated_submission_time': 2317.7767033576965, 'accumulated_eval_time': 392.3471648693085, 'accumulated_logging_time': 0.7555222511291504}
I0913 19:54:12.879709 140561453410048 logging_writer.py:48] [8318] accumulated_eval_time=392.347165, accumulated_logging_time=0.755522, accumulated_submission_time=2317.776703, global_step=8318, preemption_count=0, score=2317.776703, test/loss=0.292140, test/num_examples=3581, test/ssim=0.736826, total_duration=2738.345400, train/loss=0.271744, train/ssim=0.741065, validation/loss=0.290779, validation/num_examples=3554, validation/ssim=0.719782
I0913 19:54:59.885149 140561445017344 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.471259, loss=0.254549
I0913 19:54:59.888479 140624454674240 pytorch_submission_base.py:86] 8500) loss = 0.255, grad_norm = 0.471
I0913 19:55:33.511071 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:55:35.521036 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:55:37.569025 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:55:39.589283 140624454674240 submission_runner.py:376] Time since start: 2825.07s, 	Step: 8625, 	{'train/ssim': 0.7443616049630302, 'train/loss': 0.26848730019160677, 'validation/ssim': 0.7216371655397088, 'validation/loss': 0.2877900870113341, 'validation/num_examples': 3554, 'test/ssim': 0.7389441998568835, 'test/loss': 0.2892454520712615, 'test/num_examples': 3581, 'score': 2397.402399301529, 'total_duration': 2825.0740025043488, 'accumulated_submission_time': 2397.402399301529, 'accumulated_eval_time': 398.4255197048187, 'accumulated_logging_time': 0.7836117744445801}
I0913 19:55:39.606045 140561453410048 logging_writer.py:48] [8625] accumulated_eval_time=398.425520, accumulated_logging_time=0.783612, accumulated_submission_time=2397.402399, global_step=8625, preemption_count=0, score=2397.402399, test/loss=0.289245, test/num_examples=3581, test/ssim=0.738944, total_duration=2825.074003, train/loss=0.268487, train/ssim=0.744362, validation/loss=0.287790, validation/num_examples=3554, validation/ssim=0.721637
I0913 19:57:00.098211 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:57:01.999094 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:57:03.952484 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:57:05.886646 140624454674240 submission_runner.py:376] Time since start: 2911.37s, 	Step: 8934, 	{'train/ssim': 0.7458993366786412, 'train/loss': 0.26804510184696745, 'validation/ssim': 0.7229458664532921, 'validation/loss': 0.28764416249736213, 'validation/num_examples': 3554, 'test/ssim': 0.7400869770577353, 'test/loss': 0.2891295858349623, 'test/num_examples': 3581, 'score': 2476.94172167778, 'total_duration': 2911.371356010437, 'accumulated_submission_time': 2476.94172167778, 'accumulated_eval_time': 404.2142457962036, 'accumulated_logging_time': 0.8091602325439453}
I0913 19:57:05.904604 140561445017344 logging_writer.py:48] [8934] accumulated_eval_time=404.214246, accumulated_logging_time=0.809160, accumulated_submission_time=2476.941722, global_step=8934, preemption_count=0, score=2476.941722, test/loss=0.289130, test/num_examples=3581, test/ssim=0.740087, total_duration=2911.371356, train/loss=0.268045, train/ssim=0.745899, validation/loss=0.287644, validation/num_examples=3554, validation/ssim=0.722946
I0913 19:57:21.526388 140561453410048 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.397213, loss=0.320786
I0913 19:57:21.530318 140624454674240 pytorch_submission_base.py:86] 9000) loss = 0.321, grad_norm = 0.397
I0913 19:58:26.556544 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:58:28.443787 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:58:30.422834 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:58:32.364809 140624454674240 submission_runner.py:376] Time since start: 2997.85s, 	Step: 9245, 	{'train/ssim': 0.7458521979195731, 'train/loss': 0.2685942990439279, 'validation/ssim': 0.7231327844726013, 'validation/loss': 0.2881876055149128, 'validation/num_examples': 3554, 'test/ssim': 0.7403120963941636, 'test/loss': 0.28961561725513124, 'test/num_examples': 3581, 'score': 2556.6616587638855, 'total_duration': 2997.849468946457, 'accumulated_submission_time': 2556.6616587638855, 'accumulated_eval_time': 410.0227704048157, 'accumulated_logging_time': 0.8359005451202393}
I0913 19:58:32.382298 140561445017344 logging_writer.py:48] [9245] accumulated_eval_time=410.022770, accumulated_logging_time=0.835901, accumulated_submission_time=2556.661659, global_step=9245, preemption_count=0, score=2556.661659, test/loss=0.289616, test/num_examples=3581, test/ssim=0.740312, total_duration=2997.849469, train/loss=0.268594, train/ssim=0.745852, validation/loss=0.288188, validation/num_examples=3554, validation/ssim=0.723133
I0913 19:59:38.266501 140561453410048 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.391291, loss=0.271524
I0913 19:59:38.270122 140624454674240 pytorch_submission_base.py:86] 9500) loss = 0.272, grad_norm = 0.391
I0913 19:59:52.958798 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 19:59:54.858730 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 19:59:56.817261 140624454674240 spec.py:348] Evaluating on the test split.
I0913 19:59:58.743283 140624454674240 submission_runner.py:376] Time since start: 3084.23s, 	Step: 9555, 	{'train/ssim': 0.7455284936087472, 'train/loss': 0.26867711544036865, 'validation/ssim': 0.7228266126195836, 'validation/loss': 0.2879653097741981, 'validation/num_examples': 3554, 'test/ssim': 0.7400147097964954, 'test/loss': 0.2894689692561435, 'test/num_examples': 3581, 'score': 2636.2900590896606, 'total_duration': 3084.2279603481293, 'accumulated_submission_time': 2636.2900590896606, 'accumulated_eval_time': 415.8074870109558, 'accumulated_logging_time': 0.8618514537811279}
I0913 19:59:58.759862 140561445017344 logging_writer.py:48] [9555] accumulated_eval_time=415.807487, accumulated_logging_time=0.861851, accumulated_submission_time=2636.290059, global_step=9555, preemption_count=0, score=2636.290059, test/loss=0.289469, test/num_examples=3581, test/ssim=0.740015, total_duration=3084.227960, train/loss=0.268677, train/ssim=0.745528, validation/loss=0.287965, validation/num_examples=3554, validation/ssim=0.722827
I0913 20:01:19.267155 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:01:21.209668 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:01:23.102145 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:01:24.925808 140624454674240 submission_runner.py:376] Time since start: 3170.41s, 	Step: 9863, 	{'train/ssim': 0.7442137854439872, 'train/loss': 0.2694025380270822, 'validation/ssim': 0.7215104239940912, 'validation/loss': 0.28899383974263154, 'validation/num_examples': 3554, 'test/ssim': 0.7387430105286582, 'test/loss': 0.2904448159011973, 'test/num_examples': 3581, 'score': 2715.83766746521, 'total_duration': 3170.4104874134064, 'accumulated_submission_time': 2715.83766746521, 'accumulated_eval_time': 421.4662461280823, 'accumulated_logging_time': 0.8874399662017822}
I0913 20:01:24.945003 140561453410048 logging_writer.py:48] [9863] accumulated_eval_time=421.466246, accumulated_logging_time=0.887440, accumulated_submission_time=2715.837667, global_step=9863, preemption_count=0, score=2715.837667, test/loss=0.290445, test/num_examples=3581, test/ssim=0.738743, total_duration=3170.410487, train/loss=0.269403, train/ssim=0.744214, validation/loss=0.288994, validation/num_examples=3554, validation/ssim=0.721510
I0913 20:01:59.420697 140561445017344 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.360963, loss=0.261491
I0913 20:01:59.424370 140624454674240 pytorch_submission_base.py:86] 10000) loss = 0.261, grad_norm = 0.361
I0913 20:02:45.418401 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:02:47.243395 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:02:49.146631 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:02:50.967702 140624454674240 submission_runner.py:376] Time since start: 3256.45s, 	Step: 10172, 	{'train/ssim': 0.7373419489179339, 'train/loss': 0.2721356323787144, 'validation/ssim': 0.7169236165456176, 'validation/loss': 0.29107126762978336, 'validation/num_examples': 3554, 'test/ssim': 0.7337442295273666, 'test/loss': 0.2924061561889137, 'test/num_examples': 3581, 'score': 2795.3147897720337, 'total_duration': 3256.4523804187775, 'accumulated_submission_time': 2795.3147897720337, 'accumulated_eval_time': 427.01580834388733, 'accumulated_logging_time': 0.9152638912200928}
I0913 20:02:50.986744 140561453410048 logging_writer.py:48] [10172] accumulated_eval_time=427.015808, accumulated_logging_time=0.915264, accumulated_submission_time=2795.314790, global_step=10172, preemption_count=0, score=2795.314790, test/loss=0.292406, test/num_examples=3581, test/ssim=0.733744, total_duration=3256.452380, train/loss=0.272136, train/ssim=0.737342, validation/loss=0.291071, validation/num_examples=3554, validation/ssim=0.716924
I0913 20:04:11.609415 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:04:13.469721 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:04:15.374094 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:04:17.197813 140624454674240 submission_runner.py:376] Time since start: 3342.68s, 	Step: 10479, 	{'train/ssim': 0.7423101152692523, 'train/loss': 0.26954589571271625, 'validation/ssim': 0.7197944327694148, 'validation/loss': 0.2887804056059194, 'validation/num_examples': 3554, 'test/ssim': 0.7373214589587406, 'test/loss': 0.290187619445511, 'test/num_examples': 3581, 'score': 2874.9394323825836, 'total_duration': 3342.6825063228607, 'accumulated_submission_time': 2874.9394323825836, 'accumulated_eval_time': 432.6046464443207, 'accumulated_logging_time': 0.9428362846374512}
I0913 20:04:17.216338 140561445017344 logging_writer.py:48] [10479] accumulated_eval_time=432.604646, accumulated_logging_time=0.942836, accumulated_submission_time=2874.939432, global_step=10479, preemption_count=0, score=2874.939432, test/loss=0.290188, test/num_examples=3581, test/ssim=0.737321, total_duration=3342.682506, train/loss=0.269546, train/ssim=0.742310, validation/loss=0.288780, validation/num_examples=3554, validation/ssim=0.719794
I0913 20:04:20.774730 140561453410048 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.317136, loss=0.271529
I0913 20:04:20.778370 140624454674240 pytorch_submission_base.py:86] 10500) loss = 0.272, grad_norm = 0.317
I0913 20:05:37.868654 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:05:39.789078 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:05:41.702793 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:05:43.522540 140624454674240 submission_runner.py:376] Time since start: 3429.01s, 	Step: 10788, 	{'train/ssim': 0.7451863288879395, 'train/loss': 0.2679816314152309, 'validation/ssim': 0.7227734429955683, 'validation/loss': 0.28724083929968874, 'validation/num_examples': 3554, 'test/ssim': 0.7400273224788816, 'test/loss': 0.2886318280290073, 'test/num_examples': 3581, 'score': 2954.604351043701, 'total_duration': 3429.0071885585785, 'accumulated_submission_time': 2954.604351043701, 'accumulated_eval_time': 438.25855231285095, 'accumulated_logging_time': 0.9702644348144531}
I0913 20:05:43.540679 140561445017344 logging_writer.py:48] [10788] accumulated_eval_time=438.258552, accumulated_logging_time=0.970264, accumulated_submission_time=2954.604351, global_step=10788, preemption_count=0, score=2954.604351, test/loss=0.288632, test/num_examples=3581, test/ssim=0.740027, total_duration=3429.007189, train/loss=0.267982, train/ssim=0.745186, validation/loss=0.287241, validation/num_examples=3554, validation/ssim=0.722773
I0913 20:06:38.127120 140561453410048 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.223939, loss=0.295135
I0913 20:06:38.130664 140624454674240 pytorch_submission_base.py:86] 11000) loss = 0.295, grad_norm = 0.224
I0913 20:07:04.033294 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:07:05.963931 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:07:08.066166 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:07:10.086078 140624454674240 submission_runner.py:376] Time since start: 3515.57s, 	Step: 11097, 	{'train/ssim': 0.7454932076590401, 'train/loss': 0.26740224020821707, 'validation/ssim': 0.7221193329698931, 'validation/loss': 0.2872440335988059, 'validation/num_examples': 3554, 'test/ssim': 0.7394572974116866, 'test/loss': 0.2886622007317265, 'test/num_examples': 3581, 'score': 3034.1390216350555, 'total_duration': 3515.5707976818085, 'accumulated_submission_time': 3034.1390216350555, 'accumulated_eval_time': 444.3115622997284, 'accumulated_logging_time': 0.9972431659698486}
I0913 20:07:10.103398 140561445017344 logging_writer.py:48] [11097] accumulated_eval_time=444.311562, accumulated_logging_time=0.997243, accumulated_submission_time=3034.139022, global_step=11097, preemption_count=0, score=3034.139022, test/loss=0.288662, test/num_examples=3581, test/ssim=0.739457, total_duration=3515.570798, train/loss=0.267402, train/ssim=0.745493, validation/loss=0.287244, validation/num_examples=3554, validation/ssim=0.722119
I0913 20:08:30.554931 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:08:32.443296 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:08:34.410493 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:08:36.344394 140624454674240 submission_runner.py:376] Time since start: 3601.83s, 	Step: 11407, 	{'train/ssim': 0.7438053403581891, 'train/loss': 0.2682170016424997, 'validation/ssim': 0.7209619662703995, 'validation/loss': 0.2876438533716411, 'validation/num_examples': 3554, 'test/ssim': 0.7383730839631039, 'test/loss': 0.28903914949342013, 'test/num_examples': 3581, 'score': 3113.6554729938507, 'total_duration': 3601.829110622406, 'accumulated_submission_time': 3113.6554729938507, 'accumulated_eval_time': 450.1013078689575, 'accumulated_logging_time': 1.0226528644561768}
I0913 20:08:36.361213 140561453410048 logging_writer.py:48] [11407] accumulated_eval_time=450.101308, accumulated_logging_time=1.022653, accumulated_submission_time=3113.655473, global_step=11407, preemption_count=0, score=3113.655473, test/loss=0.289039, test/num_examples=3581, test/ssim=0.738373, total_duration=3601.829111, train/loss=0.268217, train/ssim=0.743805, validation/loss=0.287644, validation/num_examples=3554, validation/ssim=0.720962
I0913 20:08:59.076217 140561445017344 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.226032, loss=0.275121
I0913 20:08:59.079530 140624454674240 pytorch_submission_base.py:86] 11500) loss = 0.275, grad_norm = 0.226
I0913 20:09:57.039453 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:09:58.883648 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:10:00.730659 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:10:02.558910 140624454674240 submission_runner.py:376] Time since start: 3688.04s, 	Step: 11719, 	{'train/ssim': 0.7459876196725028, 'train/loss': 0.2685935156685965, 'validation/ssim': 0.7233722538644837, 'validation/loss': 0.28805492188599113, 'validation/num_examples': 3554, 'test/ssim': 0.7404616759895979, 'test/loss': 0.28955562179296984, 'test/num_examples': 3581, 'score': 3193.3718087673187, 'total_duration': 3688.0435876846313, 'accumulated_submission_time': 3193.3718087673187, 'accumulated_eval_time': 455.62085723876953, 'accumulated_logging_time': 1.048095464706421}
I0913 20:10:02.578094 140561453410048 logging_writer.py:48] [11719] accumulated_eval_time=455.620857, accumulated_logging_time=1.048095, accumulated_submission_time=3193.371809, global_step=11719, preemption_count=0, score=3193.371809, test/loss=0.289556, test/num_examples=3581, test/ssim=0.740462, total_duration=3688.043588, train/loss=0.268594, train/ssim=0.745988, validation/loss=0.288055, validation/num_examples=3554, validation/ssim=0.723372
I0913 20:11:16.165676 140561445017344 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.179062, loss=0.260813
I0913 20:11:16.170082 140624454674240 pytorch_submission_base.py:86] 12000) loss = 0.261, grad_norm = 0.179
I0913 20:11:23.014291 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:11:24.991715 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:11:27.066455 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:11:29.092691 140624454674240 submission_runner.py:376] Time since start: 3774.58s, 	Step: 12025, 	{'train/ssim': 0.7433530943734306, 'train/loss': 0.26904354776654926, 'validation/ssim': 0.7201315171945343, 'validation/loss': 0.28895320688396875, 'validation/num_examples': 3554, 'test/ssim': 0.7373912036835032, 'test/loss': 0.2903296655198443, 'test/num_examples': 3581, 'score': 3272.8409242630005, 'total_duration': 3774.5773737430573, 'accumulated_submission_time': 3272.8409242630005, 'accumulated_eval_time': 461.69947028160095, 'accumulated_logging_time': 1.0759556293487549}
I0913 20:11:29.109469 140561453410048 logging_writer.py:48] [12025] accumulated_eval_time=461.699470, accumulated_logging_time=1.075956, accumulated_submission_time=3272.840924, global_step=12025, preemption_count=0, score=3272.840924, test/loss=0.290330, test/num_examples=3581, test/ssim=0.737391, total_duration=3774.577374, train/loss=0.269044, train/ssim=0.743353, validation/loss=0.288953, validation/num_examples=3554, validation/ssim=0.720132
I0913 20:12:49.641530 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:12:51.490973 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:12:53.347043 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:12:55.176115 140624454674240 submission_runner.py:376] Time since start: 3860.66s, 	Step: 12337, 	{'train/ssim': 0.7430891990661621, 'train/loss': 0.2692720719746181, 'validation/ssim': 0.72075842415676, 'validation/loss': 0.2885815003780951, 'validation/num_examples': 3554, 'test/ssim': 0.738035745841769, 'test/loss': 0.28994511506038817, 'test/num_examples': 3581, 'score': 3352.425797700882, 'total_duration': 3860.6608130931854, 'accumulated_submission_time': 3352.425797700882, 'accumulated_eval_time': 467.2343444824219, 'accumulated_logging_time': 1.100759506225586}
I0913 20:12:55.195397 140561445017344 logging_writer.py:48] [12337] accumulated_eval_time=467.234344, accumulated_logging_time=1.100760, accumulated_submission_time=3352.425798, global_step=12337, preemption_count=0, score=3352.425798, test/loss=0.289945, test/num_examples=3581, test/ssim=0.738036, total_duration=3860.660813, train/loss=0.269272, train/ssim=0.743089, validation/loss=0.288582, validation/num_examples=3554, validation/ssim=0.720758
I0913 20:13:36.702157 140561453410048 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.383919, loss=0.343682
I0913 20:13:36.705850 140624454674240 pytorch_submission_base.py:86] 12500) loss = 0.344, grad_norm = 0.384
I0913 20:14:15.703229 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:14:17.703094 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:14:19.777781 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:14:21.801791 140624454674240 submission_runner.py:376] Time since start: 3947.29s, 	Step: 12645, 	{'train/ssim': 0.7434840883527484, 'train/loss': 0.26832214423588346, 'validation/ssim': 0.7214056647219682, 'validation/loss': 0.2876275727503341, 'validation/num_examples': 3554, 'test/ssim': 0.7385825226673765, 'test/loss': 0.2889664390838802, 'test/num_examples': 3581, 'score': 3431.963940858841, 'total_duration': 3947.2865085601807, 'accumulated_submission_time': 3431.963940858841, 'accumulated_eval_time': 473.33305740356445, 'accumulated_logging_time': 1.1286990642547607}
I0913 20:14:21.818981 140561445017344 logging_writer.py:48] [12645] accumulated_eval_time=473.333057, accumulated_logging_time=1.128699, accumulated_submission_time=3431.963941, global_step=12645, preemption_count=0, score=3431.963941, test/loss=0.288966, test/num_examples=3581, test/ssim=0.738583, total_duration=3947.286509, train/loss=0.268322, train/ssim=0.743484, validation/loss=0.287628, validation/num_examples=3554, validation/ssim=0.721406
I0913 20:15:42.247915 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:15:44.161931 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:15:46.112886 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:15:48.035416 140624454674240 submission_runner.py:376] Time since start: 4033.52s, 	Step: 12952, 	{'train/ssim': 0.7464030129568917, 'train/loss': 0.2683166095188686, 'validation/ssim': 0.7235317627365293, 'validation/loss': 0.28795167389517096, 'validation/num_examples': 3554, 'test/ssim': 0.7408640546460485, 'test/loss': 0.2892552354221935, 'test/num_examples': 3581, 'score': 3511.4471578598022, 'total_duration': 4033.5201275348663, 'accumulated_submission_time': 3511.4471578598022, 'accumulated_eval_time': 479.1207890510559, 'accumulated_logging_time': 1.1542394161224365}
I0913 20:15:48.052504 140561453410048 logging_writer.py:48] [12952] accumulated_eval_time=479.120789, accumulated_logging_time=1.154239, accumulated_submission_time=3511.447158, global_step=12952, preemption_count=0, score=3511.447158, test/loss=0.289255, test/num_examples=3581, test/ssim=0.740864, total_duration=4033.520128, train/loss=0.268317, train/ssim=0.746403, validation/loss=0.287952, validation/num_examples=3554, validation/ssim=0.723532
I0913 20:15:59.135235 140561445017344 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.411147, loss=0.314880
I0913 20:15:59.139209 140624454674240 pytorch_submission_base.py:86] 13000) loss = 0.315, grad_norm = 0.411
I0913 20:17:08.561963 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:17:10.451073 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:17:12.400303 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:17:14.342957 140624454674240 submission_runner.py:376] Time since start: 4119.83s, 	Step: 13259, 	{'train/ssim': 0.7465979712350028, 'train/loss': 0.2674778529575893, 'validation/ssim': 0.7233363265862408, 'validation/loss': 0.2872853018825619, 'validation/num_examples': 3554, 'test/ssim': 0.7405421244502234, 'test/loss': 0.28871677614929486, 'test/num_examples': 3581, 'score': 3590.9730904102325, 'total_duration': 4119.82766699791, 'accumulated_submission_time': 3590.9730904102325, 'accumulated_eval_time': 484.90194964408875, 'accumulated_logging_time': 1.1793885231018066}
I0913 20:17:14.360201 140561453410048 logging_writer.py:48] [13259] accumulated_eval_time=484.901950, accumulated_logging_time=1.179389, accumulated_submission_time=3590.973090, global_step=13259, preemption_count=0, score=3590.973090, test/loss=0.288717, test/num_examples=3581, test/ssim=0.740542, total_duration=4119.827667, train/loss=0.267478, train/ssim=0.746598, validation/loss=0.287285, validation/num_examples=3554, validation/ssim=0.723336
I0913 20:18:16.035608 140561445017344 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.201207, loss=0.230800
I0913 20:18:16.039088 140624454674240 pytorch_submission_base.py:86] 13500) loss = 0.231, grad_norm = 0.201
I0913 20:18:34.807073 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:18:36.685156 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:18:38.630784 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:18:40.564666 140624454674240 submission_runner.py:376] Time since start: 4206.05s, 	Step: 13570, 	{'train/ssim': 0.7459131649562291, 'train/loss': 0.2676959548677717, 'validation/ssim': 0.7231196638031092, 'validation/loss': 0.28724305470068934, 'validation/num_examples': 3554, 'test/ssim': 0.7403845000087266, 'test/loss': 0.28868555123830636, 'test/num_examples': 3581, 'score': 3670.472277402878, 'total_duration': 4206.049309492111, 'accumulated_submission_time': 3670.472277402878, 'accumulated_eval_time': 490.6596417427063, 'accumulated_logging_time': 1.2047319412231445}
I0913 20:18:40.582181 140561453410048 logging_writer.py:48] [13570] accumulated_eval_time=490.659642, accumulated_logging_time=1.204732, accumulated_submission_time=3670.472277, global_step=13570, preemption_count=0, score=3670.472277, test/loss=0.288686, test/num_examples=3581, test/ssim=0.740385, total_duration=4206.049309, train/loss=0.267696, train/ssim=0.745913, validation/loss=0.287243, validation/num_examples=3554, validation/ssim=0.723120
I0913 20:20:01.186427 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:20:03.096805 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:20:05.073564 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:20:06.999339 140624454674240 submission_runner.py:376] Time since start: 4292.48s, 	Step: 13879, 	{'train/ssim': 0.746532644544329, 'train/loss': 0.2674886328833444, 'validation/ssim': 0.7239132925708709, 'validation/loss': 0.2869240369565982, 'validation/num_examples': 3554, 'test/ssim': 0.7411158310571418, 'test/loss': 0.28829394449438006, 'test/num_examples': 3581, 'score': 3750.11217546463, 'total_duration': 4292.484053134918, 'accumulated_submission_time': 3750.11217546463, 'accumulated_eval_time': 496.4727256298065, 'accumulated_logging_time': 1.2311663627624512}
I0913 20:20:07.017024 140561445017344 logging_writer.py:48] [13879] accumulated_eval_time=496.472726, accumulated_logging_time=1.231166, accumulated_submission_time=3750.112175, global_step=13879, preemption_count=0, score=3750.112175, test/loss=0.288294, test/num_examples=3581, test/ssim=0.741116, total_duration=4292.484053, train/loss=0.267489, train/ssim=0.746533, validation/loss=0.286924, validation/num_examples=3554, validation/ssim=0.723913
I0913 20:20:37.675746 140561453410048 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.257229, loss=0.343163
I0913 20:20:37.679614 140624454674240 pytorch_submission_base.py:86] 14000) loss = 0.343, grad_norm = 0.257
I0913 20:21:27.469345 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:21:29.380790 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:21:31.344855 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:21:33.287769 140624454674240 submission_runner.py:376] Time since start: 4378.77s, 	Step: 14187, 	{'train/ssim': 0.7452981812613351, 'train/loss': 0.2685773032052176, 'validation/ssim': 0.7220276943672622, 'validation/loss': 0.28860204006489165, 'validation/num_examples': 3554, 'test/ssim': 0.7390033771991064, 'test/loss': 0.2900556976054175, 'test/num_examples': 3581, 'score': 3829.635034799576, 'total_duration': 4378.772463560104, 'accumulated_submission_time': 3829.635034799576, 'accumulated_eval_time': 502.2913980484009, 'accumulated_logging_time': 1.2570981979370117}
I0913 20:21:33.306268 140561445017344 logging_writer.py:48] [14187] accumulated_eval_time=502.291398, accumulated_logging_time=1.257098, accumulated_submission_time=3829.635035, global_step=14187, preemption_count=0, score=3829.635035, test/loss=0.290056, test/num_examples=3581, test/ssim=0.739003, total_duration=4378.772464, train/loss=0.268577, train/ssim=0.745298, validation/loss=0.288602, validation/num_examples=3554, validation/ssim=0.722028
I0913 20:22:53.879592 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:22:55.773844 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:22:57.732571 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:22:59.659247 140624454674240 submission_runner.py:376] Time since start: 4465.14s, 	Step: 14498, 	{'train/ssim': 0.746049063546317, 'train/loss': 0.2672664097377232, 'validation/ssim': 0.7230577012696962, 'validation/loss': 0.2868891744447278, 'validation/num_examples': 3554, 'test/ssim': 0.7403377308189053, 'test/loss': 0.2882701849278309, 'test/num_examples': 3581, 'score': 3909.278613090515, 'total_duration': 4465.1439690589905, 'accumulated_submission_time': 3909.278613090515, 'accumulated_eval_time': 508.0712606906891, 'accumulated_logging_time': 1.2835757732391357}
I0913 20:22:59.676917 140561453410048 logging_writer.py:48] [14498] accumulated_eval_time=508.071261, accumulated_logging_time=1.283576, accumulated_submission_time=3909.278613, global_step=14498, preemption_count=0, score=3909.278613, test/loss=0.288270, test/num_examples=3581, test/ssim=0.740338, total_duration=4465.143969, train/loss=0.267266, train/ssim=0.746049, validation/loss=0.286889, validation/num_examples=3554, validation/ssim=0.723058
I0913 20:23:00.351965 140561445017344 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.249774, loss=0.337230
I0913 20:23:00.355675 140624454674240 pytorch_submission_base.py:86] 14500) loss = 0.337, grad_norm = 0.250
I0913 20:24:20.270882 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:24:22.187476 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:24:24.155151 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:24:26.090211 140624454674240 submission_runner.py:376] Time since start: 4551.57s, 	Step: 14807, 	{'train/ssim': 0.7457357134137835, 'train/loss': 0.26792422362736296, 'validation/ssim': 0.7228009208374367, 'validation/loss': 0.28749835819894837, 'validation/num_examples': 3554, 'test/ssim': 0.7401610169121754, 'test/loss': 0.28879238406695057, 'test/num_examples': 3581, 'score': 3988.9324967861176, 'total_duration': 4551.5749180316925, 'accumulated_submission_time': 3988.9324967861176, 'accumulated_eval_time': 513.8907482624054, 'accumulated_logging_time': 1.3096263408660889}
I0913 20:24:26.108114 140561453410048 logging_writer.py:48] [14807] accumulated_eval_time=513.890748, accumulated_logging_time=1.309626, accumulated_submission_time=3988.932497, global_step=14807, preemption_count=0, score=3988.932497, test/loss=0.288792, test/num_examples=3581, test/ssim=0.740161, total_duration=4551.574918, train/loss=0.267924, train/ssim=0.745736, validation/loss=0.287498, validation/num_examples=3554, validation/ssim=0.722801
I0913 20:25:15.543728 140561445017344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.233579, loss=0.256595
I0913 20:25:15.547155 140624454674240 pytorch_submission_base.py:86] 15000) loss = 0.257, grad_norm = 0.234
I0913 20:25:46.679201 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:25:48.484720 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:25:50.337675 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:25:52.156657 140624454674240 submission_runner.py:376] Time since start: 4637.64s, 	Step: 15115, 	{'train/ssim': 0.7464144570486886, 'train/loss': 0.2668846164430891, 'validation/ssim': 0.7233416847654052, 'validation/loss': 0.2865295925365785, 'validation/num_examples': 3554, 'test/ssim': 0.7406838637295797, 'test/loss': 0.28784994398605485, 'test/num_examples': 3581, 'score': 4068.5602083206177, 'total_duration': 4637.6413397789, 'accumulated_submission_time': 4068.5602083206177, 'accumulated_eval_time': 519.3684697151184, 'accumulated_logging_time': 1.3356173038482666}
I0913 20:25:52.176462 140561453410048 logging_writer.py:48] [15115] accumulated_eval_time=519.368470, accumulated_logging_time=1.335617, accumulated_submission_time=4068.560208, global_step=15115, preemption_count=0, score=4068.560208, test/loss=0.287850, test/num_examples=3581, test/ssim=0.740684, total_duration=4637.641340, train/loss=0.266885, train/ssim=0.746414, validation/loss=0.286530, validation/num_examples=3554, validation/ssim=0.723342
I0913 20:27:12.718994 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:27:14.708081 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:27:16.761205 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:27:18.789016 140624454674240 submission_runner.py:376] Time since start: 4724.27s, 	Step: 15424, 	{'train/ssim': 0.7472506250653949, 'train/loss': 0.26673884051186697, 'validation/ssim': 0.7237581801535242, 'validation/loss': 0.2868198959359173, 'validation/num_examples': 3554, 'test/ssim': 0.7410028623289584, 'test/loss': 0.28818404371596623, 'test/num_examples': 3581, 'score': 4148.126308917999, 'total_duration': 4724.273729801178, 'accumulated_submission_time': 4148.126308917999, 'accumulated_eval_time': 525.4387454986572, 'accumulated_logging_time': 1.3653252124786377}
I0913 20:27:18.806977 140561445017344 logging_writer.py:48] [15424] accumulated_eval_time=525.438745, accumulated_logging_time=1.365325, accumulated_submission_time=4148.126309, global_step=15424, preemption_count=0, score=4148.126309, test/loss=0.288184, test/num_examples=3581, test/ssim=0.741003, total_duration=4724.273730, train/loss=0.266739, train/ssim=0.747251, validation/loss=0.286820, validation/num_examples=3554, validation/ssim=0.723758
I0913 20:27:36.994653 140561453410048 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.216878, loss=0.249421
I0913 20:27:36.998450 140624454674240 pytorch_submission_base.py:86] 15500) loss = 0.249, grad_norm = 0.217
I0913 20:28:39.452006 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:28:41.350167 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:28:43.314986 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:28:45.244610 140624454674240 submission_runner.py:376] Time since start: 4810.73s, 	Step: 15736, 	{'train/ssim': 0.7459214074271066, 'train/loss': 0.26725326265607563, 'validation/ssim': 0.7233034905652083, 'validation/loss': 0.2866978771443708, 'validation/num_examples': 3554, 'test/ssim': 0.7406325267034348, 'test/loss': 0.2880339187072396, 'test/num_examples': 3581, 'score': 4227.783855438232, 'total_duration': 4810.729299783707, 'accumulated_submission_time': 4227.783855438232, 'accumulated_eval_time': 531.2315909862518, 'accumulated_logging_time': 1.392350673675537}
I0913 20:28:45.263184 140561445017344 logging_writer.py:48] [15736] accumulated_eval_time=531.231591, accumulated_logging_time=1.392351, accumulated_submission_time=4227.783855, global_step=15736, preemption_count=0, score=4227.783855, test/loss=0.288034, test/num_examples=3581, test/ssim=0.740633, total_duration=4810.729300, train/loss=0.267253, train/ssim=0.745921, validation/loss=0.286698, validation/num_examples=3554, validation/ssim=0.723303
I0913 20:29:53.367652 140561453410048 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.269623, loss=0.227081
I0913 20:29:53.370971 140624454674240 pytorch_submission_base.py:86] 16000) loss = 0.227, grad_norm = 0.270
I0913 20:30:05.902557 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:30:07.762151 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:30:09.614293 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:30:11.437130 140624454674240 submission_runner.py:376] Time since start: 4896.92s, 	Step: 16047, 	{'train/ssim': 0.7475191525050572, 'train/loss': 0.2671656608581543, 'validation/ssim': 0.7245678834587789, 'validation/loss': 0.2868716229732344, 'validation/num_examples': 3554, 'test/ssim': 0.7417936434262427, 'test/loss': 0.2882191546966629, 'test/num_examples': 3581, 'score': 4307.437514066696, 'total_duration': 4896.921824455261, 'accumulated_submission_time': 4307.437514066696, 'accumulated_eval_time': 536.7663447856903, 'accumulated_logging_time': 1.4209182262420654}
I0913 20:30:11.456361 140561445017344 logging_writer.py:48] [16047] accumulated_eval_time=536.766345, accumulated_logging_time=1.420918, accumulated_submission_time=4307.437514, global_step=16047, preemption_count=0, score=4307.437514, test/loss=0.288219, test/num_examples=3581, test/ssim=0.741794, total_duration=4896.921824, train/loss=0.267166, train/ssim=0.747519, validation/loss=0.286872, validation/num_examples=3554, validation/ssim=0.724568
I0913 20:31:31.897450 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:31:33.851822 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:31:35.891606 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:31:37.906370 140624454674240 submission_runner.py:376] Time since start: 4983.39s, 	Step: 16355, 	{'train/ssim': 0.7468970843723842, 'train/loss': 0.2665720156260899, 'validation/ssim': 0.723749662022545, 'validation/loss': 0.2865244404412282, 'validation/num_examples': 3554, 'test/ssim': 0.7409775006108629, 'test/loss': 0.2878755784107966, 'test/num_examples': 3581, 'score': 4386.870840072632, 'total_duration': 4983.391057252884, 'accumulated_submission_time': 4386.870840072632, 'accumulated_eval_time': 542.7754430770874, 'accumulated_logging_time': 1.4489686489105225}
I0913 20:31:37.925127 140561453410048 logging_writer.py:48] [16355] accumulated_eval_time=542.775443, accumulated_logging_time=1.448969, accumulated_submission_time=4386.870840, global_step=16355, preemption_count=0, score=4386.870840, test/loss=0.287876, test/num_examples=3581, test/ssim=0.740978, total_duration=4983.391057, train/loss=0.266572, train/ssim=0.746897, validation/loss=0.286524, validation/num_examples=3554, validation/ssim=0.723750
I0913 20:32:14.296269 140561445017344 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.222619, loss=0.323982
I0913 20:32:14.299957 140624454674240 pytorch_submission_base.py:86] 16500) loss = 0.324, grad_norm = 0.223
I0913 20:32:58.486452 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:33:00.383250 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:33:02.358635 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:33:04.288889 140624454674240 submission_runner.py:376] Time since start: 5069.77s, 	Step: 16666, 	{'train/ssim': 0.7471059390476772, 'train/loss': 0.2673354319163731, 'validation/ssim': 0.7241452055562394, 'validation/loss': 0.2872957434624719, 'validation/num_examples': 3554, 'test/ssim': 0.7412302314952178, 'test/loss': 0.288708935833217, 'test/num_examples': 3581, 'score': 4466.468131065369, 'total_duration': 5069.773614883423, 'accumulated_submission_time': 4466.468131065369, 'accumulated_eval_time': 548.5780034065247, 'accumulated_logging_time': 1.4756381511688232}
I0913 20:33:04.307141 140561453410048 logging_writer.py:48] [16666] accumulated_eval_time=548.578003, accumulated_logging_time=1.475638, accumulated_submission_time=4466.468131, global_step=16666, preemption_count=0, score=4466.468131, test/loss=0.288709, test/num_examples=3581, test/ssim=0.741230, total_duration=5069.773615, train/loss=0.267335, train/ssim=0.747106, validation/loss=0.287296, validation/num_examples=3554, validation/ssim=0.724145
I0913 20:34:24.872544 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:34:26.798259 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:34:28.777059 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:34:30.720050 140624454674240 submission_runner.py:376] Time since start: 5156.20s, 	Step: 16976, 	{'train/ssim': 0.747286183493478, 'train/loss': 0.2676194225038801, 'validation/ssim': 0.7244386002127884, 'validation/loss': 0.2872222402354741, 'validation/num_examples': 3554, 'test/ssim': 0.7416266106054524, 'test/loss': 0.2885459936121195, 'test/num_examples': 3581, 'score': 4546.0840356349945, 'total_duration': 5156.204714536667, 'accumulated_submission_time': 4546.0840356349945, 'accumulated_eval_time': 554.4256772994995, 'accumulated_logging_time': 1.5024745464324951}
I0913 20:34:30.742118 140561445017344 logging_writer.py:48] [16976] accumulated_eval_time=554.425677, accumulated_logging_time=1.502475, accumulated_submission_time=4546.084036, global_step=16976, preemption_count=0, score=4546.084036, test/loss=0.288546, test/num_examples=3581, test/ssim=0.741627, total_duration=5156.204715, train/loss=0.267619, train/ssim=0.747286, validation/loss=0.287222, validation/num_examples=3554, validation/ssim=0.724439
I0913 20:34:35.189669 140561453410048 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.386992, loss=0.227269
I0913 20:34:35.193290 140624454674240 pytorch_submission_base.py:86] 17000) loss = 0.227, grad_norm = 0.387
I0913 20:35:51.218440 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:35:53.126031 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:35:55.112457 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:35:57.051822 140624454674240 submission_runner.py:376] Time since start: 5242.54s, 	Step: 17284, 	{'train/ssim': 0.7463902745928083, 'train/loss': 0.26667182786124094, 'validation/ssim': 0.7233096730796286, 'validation/loss': 0.2862843184506014, 'validation/num_examples': 3554, 'test/ssim': 0.74071031627426, 'test/loss': 0.28756922658213485, 'test/num_examples': 3581, 'score': 4625.585852384567, 'total_duration': 5242.536526918411, 'accumulated_submission_time': 4625.585852384567, 'accumulated_eval_time': 560.2594442367554, 'accumulated_logging_time': 1.5338432788848877}
I0913 20:35:57.070652 140561445017344 logging_writer.py:48] [17284] accumulated_eval_time=560.259444, accumulated_logging_time=1.533843, accumulated_submission_time=4625.585852, global_step=17284, preemption_count=0, score=4625.585852, test/loss=0.287569, test/num_examples=3581, test/ssim=0.740710, total_duration=5242.536527, train/loss=0.266672, train/ssim=0.746390, validation/loss=0.286284, validation/num_examples=3554, validation/ssim=0.723310
I0913 20:36:52.950709 140561453410048 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.225480, loss=0.254595
I0913 20:36:52.954004 140624454674240 pytorch_submission_base.py:86] 17500) loss = 0.255, grad_norm = 0.225
I0913 20:37:17.665997 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:37:19.573681 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:37:21.540216 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:37:23.481754 140624454674240 submission_runner.py:376] Time since start: 5328.97s, 	Step: 17592, 	{'train/ssim': 0.7479167665754046, 'train/loss': 0.26628707136426655, 'validation/ssim': 0.7245708373267797, 'validation/loss': 0.2862804543790887, 'validation/num_examples': 3554, 'test/ssim': 0.7417787809140254, 'test/loss': 0.28765710629886904, 'test/num_examples': 3581, 'score': 4705.243488550186, 'total_duration': 5328.9664669036865, 'accumulated_submission_time': 4705.243488550186, 'accumulated_eval_time': 566.0754532814026, 'accumulated_logging_time': 1.56105637550354}
I0913 20:37:23.500396 140561445017344 logging_writer.py:48] [17592] accumulated_eval_time=566.075453, accumulated_logging_time=1.561056, accumulated_submission_time=4705.243489, global_step=17592, preemption_count=0, score=4705.243489, test/loss=0.287657, test/num_examples=3581, test/ssim=0.741779, total_duration=5328.966467, train/loss=0.266287, train/ssim=0.747917, validation/loss=0.286280, validation/num_examples=3554, validation/ssim=0.724571
I0913 20:38:44.042657 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:38:45.933685 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:38:47.905621 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:38:49.834601 140624454674240 submission_runner.py:376] Time since start: 5415.32s, 	Step: 17902, 	{'train/ssim': 0.746361255645752, 'train/loss': 0.26691198348999023, 'validation/ssim': 0.7232250413266742, 'validation/loss': 0.28665715841745215, 'validation/num_examples': 3554, 'test/ssim': 0.7404264968322396, 'test/loss': 0.28802921451759284, 'test/num_examples': 3581, 'score': 4784.838724851608, 'total_duration': 5415.319300413132, 'accumulated_submission_time': 4784.838724851608, 'accumulated_eval_time': 571.8675920963287, 'accumulated_logging_time': 1.5877399444580078}
I0913 20:38:49.852653 140561453410048 logging_writer.py:48] [17902] accumulated_eval_time=571.867592, accumulated_logging_time=1.587740, accumulated_submission_time=4784.838725, global_step=17902, preemption_count=0, score=4784.838725, test/loss=0.288029, test/num_examples=3581, test/ssim=0.740426, total_duration=5415.319300, train/loss=0.266912, train/ssim=0.746361, validation/loss=0.286657, validation/num_examples=3554, validation/ssim=0.723225
I0913 20:39:13.799291 140561445017344 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.189911, loss=0.282374
I0913 20:39:13.803913 140624454674240 pytorch_submission_base.py:86] 18000) loss = 0.282, grad_norm = 0.190
I0913 20:40:10.397377 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:40:12.227071 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:40:14.110861 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:40:15.943732 140624454674240 submission_runner.py:376] Time since start: 5501.43s, 	Step: 18212, 	{'train/ssim': 0.7478054591587612, 'train/loss': 0.2665275165012905, 'validation/ssim': 0.7250058115635551, 'validation/loss': 0.2862256876055149, 'validation/num_examples': 3554, 'test/ssim': 0.7422586764346552, 'test/loss': 0.28744204302001886, 'test/num_examples': 3581, 'score': 4864.446467638016, 'total_duration': 5501.428427219391, 'accumulated_submission_time': 4864.446467638016, 'accumulated_eval_time': 577.414220571518, 'accumulated_logging_time': 1.613891839981079}
I0913 20:40:15.964276 140561453410048 logging_writer.py:48] [18212] accumulated_eval_time=577.414221, accumulated_logging_time=1.613892, accumulated_submission_time=4864.446468, global_step=18212, preemption_count=0, score=4864.446468, test/loss=0.287442, test/num_examples=3581, test/ssim=0.742259, total_duration=5501.428427, train/loss=0.266528, train/ssim=0.747805, validation/loss=0.286226, validation/num_examples=3554, validation/ssim=0.725006
I0913 20:41:30.815219 140561445017344 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.343478, loss=0.298790
I0913 20:41:30.818901 140624454674240 pytorch_submission_base.py:86] 18500) loss = 0.299, grad_norm = 0.343
I0913 20:41:36.524851 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:41:38.538729 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:41:40.615309 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:41:42.633075 140624454674240 submission_runner.py:376] Time since start: 5588.12s, 	Step: 18521, 	{'train/ssim': 0.7483962603977748, 'train/loss': 0.26638766697474886, 'validation/ssim': 0.7251937600019345, 'validation/loss': 0.28640336620049595, 'validation/num_examples': 3554, 'test/ssim': 0.7422457910456227, 'test/loss': 0.28782778657105207, 'test/num_examples': 3581, 'score': 4944.01176404953, 'total_duration': 5588.117795705795, 'accumulated_submission_time': 4944.01176404953, 'accumulated_eval_time': 583.5225822925568, 'accumulated_logging_time': 1.643033742904663}
I0913 20:41:42.651300 140561453410048 logging_writer.py:48] [18521] accumulated_eval_time=583.522582, accumulated_logging_time=1.643034, accumulated_submission_time=4944.011764, global_step=18521, preemption_count=0, score=4944.011764, test/loss=0.287828, test/num_examples=3581, test/ssim=0.742246, total_duration=5588.117796, train/loss=0.266388, train/ssim=0.748396, validation/loss=0.286403, validation/num_examples=3554, validation/ssim=0.725194
I0913 20:43:03.290729 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:43:05.167746 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:43:07.162176 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:43:09.095887 140624454674240 submission_runner.py:376] Time since start: 5674.58s, 	Step: 18834, 	{'train/ssim': 0.7462680680411202, 'train/loss': 0.26769535882132395, 'validation/ssim': 0.7236066398556205, 'validation/loss': 0.287207075901493, 'validation/num_examples': 3554, 'test/ssim': 0.7408335115016755, 'test/loss': 0.28860288703618053, 'test/num_examples': 3581, 'score': 5023.695832967758, 'total_duration': 5674.5805950164795, 'accumulated_submission_time': 5023.695832967758, 'accumulated_eval_time': 589.3280339241028, 'accumulated_logging_time': 1.669280767440796}
I0913 20:43:09.114400 140561445017344 logging_writer.py:48] [18834] accumulated_eval_time=589.328034, accumulated_logging_time=1.669281, accumulated_submission_time=5023.695833, global_step=18834, preemption_count=0, score=5023.695833, test/loss=0.288603, test/num_examples=3581, test/ssim=0.740834, total_duration=5674.580595, train/loss=0.267695, train/ssim=0.746268, validation/loss=0.287207, validation/num_examples=3554, validation/ssim=0.723607
I0913 20:43:51.281795 140561453410048 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.197876, loss=0.272517
I0913 20:43:51.285039 140624454674240 pytorch_submission_base.py:86] 19000) loss = 0.273, grad_norm = 0.198
I0913 20:44:29.790096 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:44:31.626499 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:44:33.488505 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:44:35.309672 140624454674240 submission_runner.py:376] Time since start: 5760.79s, 	Step: 19143, 	{'train/ssim': 0.7468204498291016, 'train/loss': 0.26704842703683035, 'validation/ssim': 0.7239463346757175, 'validation/loss': 0.2866950950128816, 'validation/num_examples': 3554, 'test/ssim': 0.7412553886833286, 'test/loss': 0.2879043489619694, 'test/num_examples': 3581, 'score': 5103.444320201874, 'total_duration': 5760.794360876083, 'accumulated_submission_time': 5103.444320201874, 'accumulated_eval_time': 594.8479006290436, 'accumulated_logging_time': 1.6969878673553467}
I0913 20:44:35.330532 140561445017344 logging_writer.py:48] [19143] accumulated_eval_time=594.847901, accumulated_logging_time=1.696988, accumulated_submission_time=5103.444320, global_step=19143, preemption_count=0, score=5103.444320, test/loss=0.287904, test/num_examples=3581, test/ssim=0.741255, total_duration=5760.794361, train/loss=0.267048, train/ssim=0.746820, validation/loss=0.286695, validation/num_examples=3554, validation/ssim=0.723946
I0913 20:45:55.899929 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:45:57.900832 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:45:59.966766 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:46:02.020066 140624454674240 submission_runner.py:376] Time since start: 5847.50s, 	Step: 19452, 	{'train/ssim': 0.7473120008196149, 'train/loss': 0.26621677194322857, 'validation/ssim': 0.724248590936269, 'validation/loss': 0.2860234850366664, 'validation/num_examples': 3554, 'test/ssim': 0.7415792278256772, 'test/loss': 0.2872748738459055, 'test/num_examples': 3581, 'score': 5183.024216890335, 'total_duration': 5847.504777431488, 'accumulated_submission_time': 5183.024216890335, 'accumulated_eval_time': 600.9682488441467, 'accumulated_logging_time': 1.7266974449157715}
I0913 20:46:02.039450 140561453410048 logging_writer.py:48] [19452] accumulated_eval_time=600.968249, accumulated_logging_time=1.726697, accumulated_submission_time=5183.024217, global_step=19452, preemption_count=0, score=5183.024217, test/loss=0.287275, test/num_examples=3581, test/ssim=0.741579, total_duration=5847.504777, train/loss=0.266217, train/ssim=0.747312, validation/loss=0.286023, validation/num_examples=3554, validation/ssim=0.724249
I0913 20:46:12.836503 140561445017344 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.462164, loss=0.361166
I0913 20:46:12.839983 140624454674240 pytorch_submission_base.py:86] 19500) loss = 0.361, grad_norm = 0.462
I0913 20:47:22.739847 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:47:24.632491 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:47:26.586567 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:47:28.522382 140624454674240 submission_runner.py:376] Time since start: 5934.01s, 	Step: 19762, 	{'train/ssim': 0.7486780030386788, 'train/loss': 0.26585989339011057, 'validation/ssim': 0.725319677212296, 'validation/loss': 0.2859349892121993, 'validation/num_examples': 3554, 'test/ssim': 0.7425679939480941, 'test/loss': 0.2872758624074979, 'test/num_examples': 3581, 'score': 5262.777628421783, 'total_duration': 5934.007062911987, 'accumulated_submission_time': 5262.777628421783, 'accumulated_eval_time': 606.7510075569153, 'accumulated_logging_time': 1.7542026042938232}
I0913 20:47:28.540217 140561453410048 logging_writer.py:48] [19762] accumulated_eval_time=606.751008, accumulated_logging_time=1.754203, accumulated_submission_time=5262.777628, global_step=19762, preemption_count=0, score=5262.777628, test/loss=0.287276, test/num_examples=3581, test/ssim=0.742568, total_duration=5934.007063, train/loss=0.265860, train/ssim=0.748678, validation/loss=0.285935, validation/num_examples=3554, validation/ssim=0.725320
I0913 20:48:29.318295 140561445017344 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.492875, loss=0.240643
I0913 20:48:29.322013 140624454674240 pytorch_submission_base.py:86] 20000) loss = 0.241, grad_norm = 0.493
I0913 20:48:49.031477 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:48:50.973032 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:48:52.920619 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:48:54.853342 140624454674240 submission_runner.py:376] Time since start: 6020.34s, 	Step: 20074, 	{'train/ssim': 0.7471725600106376, 'train/loss': 0.2661230223519461, 'validation/ssim': 0.7241128503974396, 'validation/loss': 0.28592739845838316, 'validation/num_examples': 3554, 'test/ssim': 0.7413708117713278, 'test/loss': 0.2872406150734781, 'test/num_examples': 3581, 'score': 5342.303017139435, 'total_duration': 6020.338069200516, 'accumulated_submission_time': 5342.303017139435, 'accumulated_eval_time': 612.57315325737, 'accumulated_logging_time': 1.7803120613098145}
I0913 20:48:54.872978 140561453410048 logging_writer.py:48] [20074] accumulated_eval_time=612.573153, accumulated_logging_time=1.780312, accumulated_submission_time=5342.303017, global_step=20074, preemption_count=0, score=5342.303017, test/loss=0.287241, test/num_examples=3581, test/ssim=0.741371, total_duration=6020.338069, train/loss=0.266123, train/ssim=0.747173, validation/loss=0.285927, validation/num_examples=3554, validation/ssim=0.724113
I0913 20:50:15.376178 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:50:17.288763 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:50:19.243489 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:50:21.195933 140624454674240 submission_runner.py:376] Time since start: 6106.68s, 	Step: 20385, 	{'train/ssim': 0.7481504167829242, 'train/loss': 0.266286952154977, 'validation/ssim': 0.7253547801552828, 'validation/loss': 0.2860922826832442, 'validation/num_examples': 3554, 'test/ssim': 0.7425520406092921, 'test/loss': 0.2873450617189682, 'test/num_examples': 3581, 'score': 5421.840210914612, 'total_duration': 6106.680652141571, 'accumulated_submission_time': 5421.840210914612, 'accumulated_eval_time': 618.3930585384369, 'accumulated_logging_time': 1.8078217506408691}
I0913 20:50:21.215163 140561445017344 logging_writer.py:48] [20385] accumulated_eval_time=618.393059, accumulated_logging_time=1.807822, accumulated_submission_time=5421.840211, global_step=20385, preemption_count=0, score=5421.840211, test/loss=0.287345, test/num_examples=3581, test/ssim=0.742552, total_duration=6106.680652, train/loss=0.266287, train/ssim=0.748150, validation/loss=0.286092, validation/num_examples=3554, validation/ssim=0.725355
I0913 20:50:50.300208 140561453410048 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.360909, loss=0.266171
I0913 20:50:50.304251 140624454674240 pytorch_submission_base.py:86] 20500) loss = 0.266, grad_norm = 0.361
I0913 20:51:41.886786 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:51:43.788434 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:51:45.745392 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:51:47.686338 140624454674240 submission_runner.py:376] Time since start: 6193.17s, 	Step: 20694, 	{'train/ssim': 0.7477902003696987, 'train/loss': 0.2659025362559727, 'validation/ssim': 0.7246776574370428, 'validation/loss': 0.28586205271569004, 'validation/num_examples': 3554, 'test/ssim': 0.7418751145367914, 'test/loss': 0.2871738360334927, 'test/num_examples': 3581, 'score': 5501.550112485886, 'total_duration': 6193.171048402786, 'accumulated_submission_time': 5501.550112485886, 'accumulated_eval_time': 624.1928188800812, 'accumulated_logging_time': 1.8353190422058105}
I0913 20:51:47.704888 140561445017344 logging_writer.py:48] [20694] accumulated_eval_time=624.192819, accumulated_logging_time=1.835319, accumulated_submission_time=5501.550112, global_step=20694, preemption_count=0, score=5501.550112, test/loss=0.287174, test/num_examples=3581, test/ssim=0.741875, total_duration=6193.171048, train/loss=0.265903, train/ssim=0.747790, validation/loss=0.285862, validation/num_examples=3554, validation/ssim=0.724678
I0913 20:53:07.066487 140561453410048 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.198860, loss=0.371352
I0913 20:53:07.070023 140624454674240 pytorch_submission_base.py:86] 21000) loss = 0.371, grad_norm = 0.199
I0913 20:53:08.313867 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:53:10.196819 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:53:12.141287 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:53:14.075859 140624454674240 submission_runner.py:376] Time since start: 6279.56s, 	Step: 21004, 	{'train/ssim': 0.7482043675013951, 'train/loss': 0.2658792734146118, 'validation/ssim': 0.7251773419914181, 'validation/loss': 0.2858192731506313, 'validation/num_examples': 3554, 'test/ssim': 0.7423757357625315, 'test/loss': 0.28716821145891513, 'test/num_examples': 3581, 'score': 5581.184408187866, 'total_duration': 6279.560573577881, 'accumulated_submission_time': 5581.184408187866, 'accumulated_eval_time': 629.9551358222961, 'accumulated_logging_time': 1.861687421798706}
I0913 20:53:14.095795 140561445017344 logging_writer.py:48] [21004] accumulated_eval_time=629.955136, accumulated_logging_time=1.861687, accumulated_submission_time=5581.184408, global_step=21004, preemption_count=0, score=5581.184408, test/loss=0.287168, test/num_examples=3581, test/ssim=0.742376, total_duration=6279.560574, train/loss=0.265879, train/ssim=0.748204, validation/loss=0.285819, validation/num_examples=3554, validation/ssim=0.725177
I0913 20:54:34.706193 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:54:36.595086 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:54:38.559888 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:54:40.503495 140624454674240 submission_runner.py:376] Time since start: 6365.99s, 	Step: 21315, 	{'train/ssim': 0.74720641544887, 'train/loss': 0.26581084728240967, 'validation/ssim': 0.7241199946363253, 'validation/loss': 0.28573680527772405, 'validation/num_examples': 3554, 'test/ssim': 0.7414712359937866, 'test/loss': 0.2869880887191078, 'test/num_examples': 3581, 'score': 5660.842807769775, 'total_duration': 6365.988186836243, 'accumulated_submission_time': 5660.842807769775, 'accumulated_eval_time': 635.7525928020477, 'accumulated_logging_time': 1.8901846408843994}
I0913 20:54:40.522069 140561453410048 logging_writer.py:48] [21315] accumulated_eval_time=635.752593, accumulated_logging_time=1.890185, accumulated_submission_time=5660.842808, global_step=21315, preemption_count=0, score=5660.842808, test/loss=0.286988, test/num_examples=3581, test/ssim=0.741471, total_duration=6365.988187, train/loss=0.265811, train/ssim=0.747206, validation/loss=0.285737, validation/num_examples=3554, validation/ssim=0.724120
I0913 20:55:28.111055 140561445017344 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.311048, loss=0.198488
I0913 20:55:28.114562 140624454674240 pytorch_submission_base.py:86] 21500) loss = 0.198, grad_norm = 0.311
I0913 20:56:01.195502 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:56:03.085221 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:56:05.057210 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:56:06.989106 140624454674240 submission_runner.py:376] Time since start: 6452.47s, 	Step: 21622, 	{'train/ssim': 0.7480870655604771, 'train/loss': 0.26618668011256624, 'validation/ssim': 0.7248988540640827, 'validation/loss': 0.2861559969290764, 'validation/num_examples': 3554, 'test/ssim': 0.7421755690842292, 'test/loss': 0.2874003529914828, 'test/num_examples': 3581, 'score': 5740.562634468079, 'total_duration': 6452.473796367645, 'accumulated_submission_time': 5740.562634468079, 'accumulated_eval_time': 641.5463263988495, 'accumulated_logging_time': 1.9170351028442383}
I0913 20:56:07.010745 140561453410048 logging_writer.py:48] [21622] accumulated_eval_time=641.546326, accumulated_logging_time=1.917035, accumulated_submission_time=5740.562634, global_step=21622, preemption_count=0, score=5740.562634, test/loss=0.287400, test/num_examples=3581, test/ssim=0.742176, total_duration=6452.473796, train/loss=0.266187, train/ssim=0.748087, validation/loss=0.286156, validation/num_examples=3554, validation/ssim=0.724899
I0913 20:57:27.525918 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:57:29.427028 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:57:31.395844 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:57:33.329181 140624454674240 submission_runner.py:376] Time since start: 6538.81s, 	Step: 21932, 	{'train/ssim': 0.7484736442565918, 'train/loss': 0.2652705907821655, 'validation/ssim': 0.7249953699836452, 'validation/loss': 0.28549494874832937, 'validation/num_examples': 3554, 'test/ssim': 0.7422237018072815, 'test/loss': 0.2868142382321628, 'test/num_examples': 3581, 'score': 5820.106271743774, 'total_duration': 6538.813883781433, 'accumulated_submission_time': 5820.106271743774, 'accumulated_eval_time': 647.3496873378754, 'accumulated_logging_time': 1.9472379684448242}
I0913 20:57:33.348735 140561445017344 logging_writer.py:48] [21932] accumulated_eval_time=647.349687, accumulated_logging_time=1.947238, accumulated_submission_time=5820.106272, global_step=21932, preemption_count=0, score=5820.106272, test/loss=0.286814, test/num_examples=3581, test/ssim=0.742224, total_duration=6538.813884, train/loss=0.265271, train/ssim=0.748474, validation/loss=0.285495, validation/num_examples=3554, validation/ssim=0.724995
I0913 20:57:49.428541 140561453410048 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.284374, loss=0.275186
I0913 20:57:49.432429 140624454674240 pytorch_submission_base.py:86] 22000) loss = 0.275, grad_norm = 0.284
I0913 20:58:54.038221 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 20:58:55.933297 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 20:58:57.879878 140624454674240 spec.py:348] Evaluating on the test split.
I0913 20:58:59.815336 140624454674240 submission_runner.py:376] Time since start: 6625.30s, 	Step: 22244, 	{'train/ssim': 0.7488002095903669, 'train/loss': 0.26557263306209017, 'validation/ssim': 0.7256000198939575, 'validation/loss': 0.2855803361419351, 'validation/num_examples': 3554, 'test/ssim': 0.7427972720608769, 'test/loss': 0.28690054988568137, 'test/num_examples': 3581, 'score': 5899.840828418732, 'total_duration': 6625.300048351288, 'accumulated_submission_time': 5899.840828418732, 'accumulated_eval_time': 653.1270537376404, 'accumulated_logging_time': 1.97556471824646}
I0913 20:58:59.835071 140561445017344 logging_writer.py:48] [22244] accumulated_eval_time=653.127054, accumulated_logging_time=1.975565, accumulated_submission_time=5899.840828, global_step=22244, preemption_count=0, score=5899.840828, test/loss=0.286901, test/num_examples=3581, test/ssim=0.742797, total_duration=6625.300048, train/loss=0.265573, train/ssim=0.748800, validation/loss=0.285580, validation/num_examples=3554, validation/ssim=0.725600
I0913 21:00:05.796231 140561453410048 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.344464, loss=0.299664
I0913 21:00:05.799614 140624454674240 pytorch_submission_base.py:86] 22500) loss = 0.300, grad_norm = 0.344
I0913 21:00:20.446610 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:00:22.340762 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:00:24.289917 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:00:26.237860 140624454674240 submission_runner.py:376] Time since start: 6711.72s, 	Step: 22554, 	{'train/ssim': 0.7480750765119281, 'train/loss': 0.2656329699925014, 'validation/ssim': 0.7250469596317529, 'validation/loss': 0.28557805204632986, 'validation/num_examples': 3554, 'test/ssim': 0.7422637896842712, 'test/loss': 0.28680428443957695, 'test/num_examples': 3581, 'score': 5979.526552915573, 'total_duration': 6711.722568035126, 'accumulated_submission_time': 5979.526552915573, 'accumulated_eval_time': 658.9185583591461, 'accumulated_logging_time': 2.003601551055908}
I0913 21:00:26.257310 140561445017344 logging_writer.py:48] [22554] accumulated_eval_time=658.918558, accumulated_logging_time=2.003602, accumulated_submission_time=5979.526553, global_step=22554, preemption_count=0, score=5979.526553, test/loss=0.286804, test/num_examples=3581, test/ssim=0.742264, total_duration=6711.722568, train/loss=0.265633, train/ssim=0.748075, validation/loss=0.285578, validation/num_examples=3554, validation/ssim=0.725047
I0913 21:01:46.924603 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:01:48.814926 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:01:50.773967 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:01:52.710635 140624454674240 submission_runner.py:376] Time since start: 6798.20s, 	Step: 22862, 	{'train/ssim': 0.7485605648585728, 'train/loss': 0.26537108421325684, 'validation/ssim': 0.7251298740195906, 'validation/loss': 0.2855709593283976, 'validation/num_examples': 3554, 'test/ssim': 0.742357873477206, 'test/loss': 0.286857257705599, 'test/num_examples': 3581, 'score': 6059.249801874161, 'total_duration': 6798.195337295532, 'accumulated_submission_time': 6059.249801874161, 'accumulated_eval_time': 664.7048542499542, 'accumulated_logging_time': 2.0317091941833496}
I0913 21:01:52.729749 140561453410048 logging_writer.py:48] [22862] accumulated_eval_time=664.704854, accumulated_logging_time=2.031709, accumulated_submission_time=6059.249802, global_step=22862, preemption_count=0, score=6059.249802, test/loss=0.286857, test/num_examples=3581, test/ssim=0.742358, total_duration=6798.195337, train/loss=0.265371, train/ssim=0.748561, validation/loss=0.285571, validation/num_examples=3554, validation/ssim=0.725130
I0913 21:02:27.446753 140561445017344 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.235544, loss=0.274577
I0913 21:02:27.450987 140624454674240 pytorch_submission_base.py:86] 23000) loss = 0.275, grad_norm = 0.236
I0913 21:03:13.169076 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:03:15.066397 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:03:17.023994 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:03:18.966089 140624454674240 submission_runner.py:376] Time since start: 6884.45s, 	Step: 23172, 	{'train/ssim': 0.7487709862845284, 'train/loss': 0.2652000870023455, 'validation/ssim': 0.7253795102129643, 'validation/loss': 0.28535484610210327, 'validation/num_examples': 3554, 'test/ssim': 0.7426281257635786, 'test/loss': 0.28666298830852066, 'test/num_examples': 3581, 'score': 6138.734473466873, 'total_duration': 6884.450774669647, 'accumulated_submission_time': 6138.734473466873, 'accumulated_eval_time': 670.501941204071, 'accumulated_logging_time': 2.0600852966308594}
I0913 21:03:18.985245 140561453410048 logging_writer.py:48] [23172] accumulated_eval_time=670.501941, accumulated_logging_time=2.060085, accumulated_submission_time=6138.734473, global_step=23172, preemption_count=0, score=6138.734473, test/loss=0.286663, test/num_examples=3581, test/ssim=0.742628, total_duration=6884.450775, train/loss=0.265200, train/ssim=0.748771, validation/loss=0.285355, validation/num_examples=3554, validation/ssim=0.725380
I0913 21:04:39.496582 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:04:41.409284 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:04:43.378987 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:04:45.308881 140624454674240 submission_runner.py:376] Time since start: 6970.79s, 	Step: 23482, 	{'train/ssim': 0.7492326327732631, 'train/loss': 0.26520969186510357, 'validation/ssim': 0.7258513047578433, 'validation/loss': 0.2853496253121483, 'validation/num_examples': 3554, 'test/ssim': 0.7431566312438914, 'test/loss': 0.28660599261946734, 'test/num_examples': 3581, 'score': 6218.294421195984, 'total_duration': 6970.79360127449, 'accumulated_submission_time': 6218.294421195984, 'accumulated_eval_time': 676.3145544528961, 'accumulated_logging_time': 2.0873494148254395}
I0913 21:04:45.328493 140561445017344 logging_writer.py:48] [23482] accumulated_eval_time=676.314554, accumulated_logging_time=2.087349, accumulated_submission_time=6218.294421, global_step=23482, preemption_count=0, score=6218.294421, test/loss=0.286606, test/num_examples=3581, test/ssim=0.743157, total_duration=6970.793601, train/loss=0.265210, train/ssim=0.749233, validation/loss=0.285350, validation/num_examples=3554, validation/ssim=0.725851
I0913 21:04:48.047541 140561453410048 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.300112, loss=0.187418
I0913 21:04:48.051107 140624454674240 pytorch_submission_base.py:86] 23500) loss = 0.187, grad_norm = 0.300
I0913 21:06:05.947914 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:06:07.857721 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:06:09.803079 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:06:11.733566 140624454674240 submission_runner.py:376] Time since start: 7057.22s, 	Step: 23793, 	{'train/ssim': 0.7490583147321429, 'train/loss': 0.2651618548801967, 'validation/ssim': 0.7258139348929024, 'validation/loss': 0.28524761382421215, 'validation/num_examples': 3554, 'test/ssim': 0.7430990219648841, 'test/loss': 0.2864280856211603, 'test/num_examples': 3581, 'score': 6298.002183437347, 'total_duration': 7057.218279838562, 'accumulated_submission_time': 6298.002183437347, 'accumulated_eval_time': 682.1003348827362, 'accumulated_logging_time': 2.116272211074829}
I0913 21:06:11.752873 140561445017344 logging_writer.py:48] [23793] accumulated_eval_time=682.100335, accumulated_logging_time=2.116272, accumulated_submission_time=6298.002183, global_step=23793, preemption_count=0, score=6298.002183, test/loss=0.286428, test/num_examples=3581, test/ssim=0.743099, total_duration=7057.218280, train/loss=0.265162, train/ssim=0.749058, validation/loss=0.285248, validation/num_examples=3554, validation/ssim=0.725814
I0913 21:07:04.879766 140561453410048 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.293759, loss=0.259901
I0913 21:07:04.884111 140624454674240 pytorch_submission_base.py:86] 24000) loss = 0.260, grad_norm = 0.294
I0913 21:07:32.236111 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:07:34.137544 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:07:36.104017 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:07:38.040027 140624454674240 submission_runner.py:376] Time since start: 7143.52s, 	Step: 24104, 	{'train/ssim': 0.7493989808218819, 'train/loss': 0.2646122489656721, 'validation/ssim': 0.7256988027354742, 'validation/loss': 0.28504266347117685, 'validation/num_examples': 3554, 'test/ssim': 0.7429555100923275, 'test/loss': 0.28636672662576795, 'test/num_examples': 3581, 'score': 6377.555734157562, 'total_duration': 7143.524742841721, 'accumulated_submission_time': 6377.555734157562, 'accumulated_eval_time': 687.9045672416687, 'accumulated_logging_time': 2.143724203109741}
I0913 21:07:38.059273 140561445017344 logging_writer.py:48] [24104] accumulated_eval_time=687.904567, accumulated_logging_time=2.143724, accumulated_submission_time=6377.555734, global_step=24104, preemption_count=0, score=6377.555734, test/loss=0.286367, test/num_examples=3581, test/ssim=0.742956, total_duration=7143.524743, train/loss=0.264612, train/ssim=0.749399, validation/loss=0.285043, validation/num_examples=3554, validation/ssim=0.725699
I0913 21:08:58.715532 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:09:00.549445 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:09:02.404598 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:09:04.228469 140624454674240 submission_runner.py:376] Time since start: 7229.71s, 	Step: 24414, 	{'train/ssim': 0.7492085184369769, 'train/loss': 0.26499567713056293, 'validation/ssim': 0.7257925708708497, 'validation/loss': 0.2851463064559739, 'validation/num_examples': 3554, 'test/ssim': 0.7430490484719702, 'test/loss': 0.2864555949040945, 'test/num_examples': 3581, 'score': 6457.212813615799, 'total_duration': 7229.71315741539, 'accumulated_submission_time': 6457.212813615799, 'accumulated_eval_time': 693.4177057743073, 'accumulated_logging_time': 2.171322822570801}
I0913 21:09:04.250226 140561453410048 logging_writer.py:48] [24414] accumulated_eval_time=693.417706, accumulated_logging_time=2.171323, accumulated_submission_time=6457.212814, global_step=24414, preemption_count=0, score=6457.212814, test/loss=0.286456, test/num_examples=3581, test/ssim=0.743049, total_duration=7229.713157, train/loss=0.264996, train/ssim=0.749209, validation/loss=0.285146, validation/num_examples=3554, validation/ssim=0.725793
I0913 21:09:24.981420 140561445017344 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.319490, loss=0.325725
I0913 21:09:24.984871 140624454674240 pytorch_submission_base.py:86] 24500) loss = 0.326, grad_norm = 0.319
I0913 21:10:24.749120 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:10:26.729982 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:10:28.819902 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:10:30.859827 140624454674240 submission_runner.py:376] Time since start: 7316.34s, 	Step: 24724, 	{'train/ssim': 0.7491113798958915, 'train/loss': 0.26487817083086285, 'validation/ssim': 0.7257345926245076, 'validation/loss': 0.2850529676618775, 'validation/num_examples': 3554, 'test/ssim': 0.7429647139416364, 'test/loss': 0.28626967714805573, 'test/num_examples': 3581, 'score': 6536.73136639595, 'total_duration': 7316.344540596008, 'accumulated_submission_time': 6536.73136639595, 'accumulated_eval_time': 699.5285453796387, 'accumulated_logging_time': 2.2020840644836426}
I0913 21:10:30.880568 140561453410048 logging_writer.py:48] [24724] accumulated_eval_time=699.528545, accumulated_logging_time=2.202084, accumulated_submission_time=6536.731366, global_step=24724, preemption_count=0, score=6536.731366, test/loss=0.286270, test/num_examples=3581, test/ssim=0.742965, total_duration=7316.344541, train/loss=0.264878, train/ssim=0.749111, validation/loss=0.285053, validation/num_examples=3554, validation/ssim=0.725735
I0913 21:11:42.503886 140561445017344 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.290020, loss=0.209330
I0913 21:11:42.507426 140624454674240 pytorch_submission_base.py:86] 25000) loss = 0.209, grad_norm = 0.290
I0913 21:11:51.604796 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:11:53.423830 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:11:55.291919 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:11:57.109084 140624454674240 submission_runner.py:376] Time since start: 7402.59s, 	Step: 25033, 	{'train/ssim': 0.7494565418788365, 'train/loss': 0.2645956448146275, 'validation/ssim': 0.7258850338087366, 'validation/loss': 0.2850158382280529, 'validation/num_examples': 3554, 'test/ssim': 0.7430658881073723, 'test/loss': 0.2863145033030229, 'test/num_examples': 3581, 'score': 6616.517788171768, 'total_duration': 7402.593783855438, 'accumulated_submission_time': 6616.517788171768, 'accumulated_eval_time': 705.0331559181213, 'accumulated_logging_time': 2.231322765350342}
I0913 21:11:57.130310 140561453410048 logging_writer.py:48] [25033] accumulated_eval_time=705.033156, accumulated_logging_time=2.231323, accumulated_submission_time=6616.517788, global_step=25033, preemption_count=0, score=6616.517788, test/loss=0.286315, test/num_examples=3581, test/ssim=0.743066, total_duration=7402.593784, train/loss=0.264596, train/ssim=0.749457, validation/loss=0.285016, validation/num_examples=3554, validation/ssim=0.725885
I0913 21:13:17.620720 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:13:19.598873 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:13:21.700764 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:13:23.737522 140624454674240 submission_runner.py:376] Time since start: 7489.22s, 	Step: 25342, 	{'train/ssim': 0.7493880816868373, 'train/loss': 0.264569844518389, 'validation/ssim': 0.7257581548739097, 'validation/loss': 0.2849689541603651, 'validation/num_examples': 3554, 'test/ssim': 0.7430480939987084, 'test/loss': 0.2862217148666574, 'test/num_examples': 3581, 'score': 6696.016061306, 'total_duration': 7489.222241640091, 'accumulated_submission_time': 6696.016061306, 'accumulated_eval_time': 711.1502118110657, 'accumulated_logging_time': 2.2611663341522217}
I0913 21:13:23.758378 140561445017344 logging_writer.py:48] [25342] accumulated_eval_time=711.150212, accumulated_logging_time=2.261166, accumulated_submission_time=6696.016061, global_step=25342, preemption_count=0, score=6696.016061, test/loss=0.286222, test/num_examples=3581, test/ssim=0.743048, total_duration=7489.222242, train/loss=0.264570, train/ssim=0.749388, validation/loss=0.284969, validation/num_examples=3554, validation/ssim=0.725758
I0913 21:14:04.040210 140561453410048 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.225243, loss=0.338968
I0913 21:14:04.043726 140624454674240 pytorch_submission_base.py:86] 25500) loss = 0.339, grad_norm = 0.225
I0913 21:14:44.334746 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:14:46.234783 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:14:48.241629 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:14:50.171894 140624454674240 submission_runner.py:376] Time since start: 7575.66s, 	Step: 25650, 	{'train/ssim': 0.7501136916024345, 'train/loss': 0.26442488602229525, 'validation/ssim': 0.7266153261509919, 'validation/loss': 0.2847477231860228, 'validation/num_examples': 3554, 'test/ssim': 0.7438383296827004, 'test/loss': 0.2860105376575154, 'test/num_examples': 3581, 'score': 6775.620117425919, 'total_duration': 7575.6566071510315, 'accumulated_submission_time': 6775.620117425919, 'accumulated_eval_time': 716.9875633716583, 'accumulated_logging_time': 2.290419578552246}
I0913 21:14:50.192937 140561445017344 logging_writer.py:48] [25650] accumulated_eval_time=716.987563, accumulated_logging_time=2.290420, accumulated_submission_time=6775.620117, global_step=25650, preemption_count=0, score=6775.620117, test/loss=0.286011, test/num_examples=3581, test/ssim=0.743838, total_duration=7575.656607, train/loss=0.264425, train/ssim=0.750114, validation/loss=0.284748, validation/num_examples=3554, validation/ssim=0.726615
I0913 21:16:10.726029 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:16:12.635940 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:16:14.621347 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:16:16.552940 140624454674240 submission_runner.py:376] Time since start: 7662.04s, 	Step: 25958, 	{'train/ssim': 0.7501708439418248, 'train/loss': 0.2642996140888759, 'validation/ssim': 0.7266605958954699, 'validation/loss': 0.2846713176119777, 'validation/num_examples': 3554, 'test/ssim': 0.7438678501771503, 'test/loss': 0.2859373840996754, 'test/num_examples': 3581, 'score': 6855.180752754211, 'total_duration': 7662.037658452988, 'accumulated_submission_time': 6855.180752754211, 'accumulated_eval_time': 722.8146538734436, 'accumulated_logging_time': 2.3198323249816895}
I0913 21:16:16.572862 140561453410048 logging_writer.py:48] [25958] accumulated_eval_time=722.814654, accumulated_logging_time=2.319832, accumulated_submission_time=6855.180753, global_step=25958, preemption_count=0, score=6855.180753, test/loss=0.285937, test/num_examples=3581, test/ssim=0.743868, total_duration=7662.037658, train/loss=0.264300, train/ssim=0.750171, validation/loss=0.284671, validation/num_examples=3554, validation/ssim=0.726661
I0913 21:16:25.858887 140561445017344 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.238585, loss=0.226309
I0913 21:16:25.862926 140624454674240 pytorch_submission_base.py:86] 26000) loss = 0.226, grad_norm = 0.239
I0913 21:17:37.139674 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:17:39.044452 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:17:41.029233 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:17:42.987325 140624454674240 submission_runner.py:376] Time since start: 7748.47s, 	Step: 26265, 	{'train/ssim': 0.7510219982692173, 'train/loss': 0.26386341026851107, 'validation/ssim': 0.7272205256181415, 'validation/loss': 0.28455625414915414, 'validation/num_examples': 3554, 'test/ssim': 0.7443772661922298, 'test/loss': 0.2858099789634093, 'test/num_examples': 3581, 'score': 6934.775408029556, 'total_duration': 7748.472028017044, 'accumulated_submission_time': 6934.775408029556, 'accumulated_eval_time': 728.6625134944916, 'accumulated_logging_time': 2.3479294776916504}
I0913 21:17:43.007346 140561453410048 logging_writer.py:48] [26265] accumulated_eval_time=728.662513, accumulated_logging_time=2.347929, accumulated_submission_time=6934.775408, global_step=26265, preemption_count=0, score=6934.775408, test/loss=0.285810, test/num_examples=3581, test/ssim=0.744377, total_duration=7748.472028, train/loss=0.263863, train/ssim=0.751022, validation/loss=0.284556, validation/num_examples=3554, validation/ssim=0.727221
I0913 21:18:43.573816 140561445017344 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.235654, loss=0.251977
I0913 21:18:43.577397 140624454674240 pytorch_submission_base.py:86] 26500) loss = 0.252, grad_norm = 0.236
I0913 21:19:03.585568 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:19:05.507668 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:19:07.522329 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:19:09.454310 140624454674240 submission_runner.py:376] Time since start: 7834.94s, 	Step: 26573, 	{'train/ssim': 0.7504673685346331, 'train/loss': 0.2638624225343977, 'validation/ssim': 0.7266487117288618, 'validation/loss': 0.28443938745295794, 'validation/num_examples': 3554, 'test/ssim': 0.743895120841769, 'test/loss': 0.28570087926076865, 'test/num_examples': 3581, 'score': 7014.378292798996, 'total_duration': 7834.939022302628, 'accumulated_submission_time': 7014.378292798996, 'accumulated_eval_time': 734.531521320343, 'accumulated_logging_time': 2.3762354850769043}
I0913 21:19:09.474785 140561453410048 logging_writer.py:48] [26573] accumulated_eval_time=734.531521, accumulated_logging_time=2.376235, accumulated_submission_time=7014.378293, global_step=26573, preemption_count=0, score=7014.378293, test/loss=0.285701, test/num_examples=3581, test/ssim=0.743895, total_duration=7834.939022, train/loss=0.263862, train/ssim=0.750467, validation/loss=0.284439, validation/num_examples=3554, validation/ssim=0.726649
I0913 21:20:29.977943 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:20:31.893495 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:20:33.850421 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:20:35.790069 140624454674240 submission_runner.py:376] Time since start: 7921.27s, 	Step: 26881, 	{'train/ssim': 0.7506608963012695, 'train/loss': 0.26375067234039307, 'validation/ssim': 0.7269144911543332, 'validation/loss': 0.2843712080578222, 'validation/num_examples': 3554, 'test/ssim': 0.7441134906887042, 'test/loss': 0.28558383697705775, 'test/num_examples': 3581, 'score': 7093.9232568740845, 'total_duration': 7921.2747802734375, 'accumulated_submission_time': 7093.9232568740845, 'accumulated_eval_time': 740.3439342975616, 'accumulated_logging_time': 2.4059293270111084}
I0913 21:20:35.810430 140561445017344 logging_writer.py:48] [26881] accumulated_eval_time=740.343934, accumulated_logging_time=2.405929, accumulated_submission_time=7093.923257, global_step=26881, preemption_count=0, score=7093.923257, test/loss=0.285584, test/num_examples=3581, test/ssim=0.744113, total_duration=7921.274780, train/loss=0.263751, train/ssim=0.750661, validation/loss=0.284371, validation/num_examples=3554, validation/ssim=0.726914
I0913 21:21:06.127643 140561453410048 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.283184, loss=0.273293
I0913 21:21:06.131453 140624454674240 pytorch_submission_base.py:86] 27000) loss = 0.273, grad_norm = 0.283
I0913 21:21:44.434463 140624454674240 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0913 21:21:46.360214 140624454674240 spec.py:332] Evaluating on the validation split.
I0913 21:21:48.334434 140624454674240 spec.py:348] Evaluating on the test split.
I0913 21:21:50.289831 140624454674240 submission_runner.py:376] Time since start: 7995.77s, 	Step: 27142, 	{'train/ssim': 0.7509222030639648, 'train/loss': 0.26384077753339497, 'validation/ssim': 0.7272265020487478, 'validation/loss': 0.28445913715180077, 'validation/num_examples': 3554, 'test/ssim': 0.7444088319865261, 'test/loss': 0.2857359902414654, 'test/num_examples': 3581, 'score': 7161.631084918976, 'total_duration': 7995.774512290955, 'accumulated_submission_time': 7161.631084918976, 'accumulated_eval_time': 746.1995131969452, 'accumulated_logging_time': 2.434459924697876}
I0913 21:21:50.310587 140561445017344 logging_writer.py:48] [27142] accumulated_eval_time=746.199513, accumulated_logging_time=2.434460, accumulated_submission_time=7161.631085, global_step=27142, preemption_count=0, score=7161.631085, test/loss=0.285736, test/num_examples=3581, test/ssim=0.744409, total_duration=7995.774512, train/loss=0.263841, train/ssim=0.750922, validation/loss=0.284459, validation/num_examples=3554, validation/ssim=0.727227
I0913 21:21:50.690864 140561453410048 logging_writer.py:48] [27142] global_step=27142, preemption_count=0, score=7161.631085
I0913 21:21:50.801871 140624454674240 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_pytorch/nesterov_run_0/fastmri_pytorch/trial_1/checkpoint_27142.
I0913 21:21:51.901569 140624454674240 submission_runner.py:540] Tuning trial 1/1
I0913 21:21:51.901790 140624454674240 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.028609, beta1=0.981543, beta2=0.9978504782314613, warmup_steps=1357, decay_steps_factor=0.984398, end_factor=0.01, weight_decay=0.000576)
I0913 21:21:51.908015 140624454674240 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/ssim': 0.14957191262926375, 'train/loss': 1.3122122628348214, 'validation/ssim': 0.14309939683389314, 'validation/loss': 1.3252357049275465, 'validation/num_examples': 3554, 'test/ssim': 0.16294639691797858, 'test/loss': 1.3201016977624964, 'test/num_examples': 3581, 'score': 89.2386724948883, 'total_duration': 317.08421993255615, 'accumulated_submission_time': 89.2386724948883, 'accumulated_eval_time': 227.39448761940002, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (304, {'train/ssim': 0.6713861737932477, 'train/loss': 0.33277484348842074, 'validation/ssim': 0.6462071922701533, 'validation/loss': 0.3543467610219119, 'validation/num_examples': 3554, 'test/ssim': 0.665678627761973, 'test/loss': 0.35553246109021575, 'test/num_examples': 3581, 'score': 168.76450395584106, 'total_duration': 403.6501896381378, 'accumulated_submission_time': 168.76450395584106, 'accumulated_eval_time': 233.36722874641418, 'accumulated_logging_time': 0.029778242111206055, 'global_step': 304, 'preemption_count': 0}), (509, {'train/ssim': 0.6984647342136928, 'train/loss': 0.3146581990378244, 'validation/ssim': 0.6773565546743107, 'validation/loss': 0.3343338244539603, 'validation/num_examples': 3554, 'test/ssim': 0.6959390435196174, 'test/loss': 0.33571146027864074, 'test/num_examples': 3581, 'score': 248.55980515480042, 'total_duration': 490.3173952102661, 'accumulated_submission_time': 248.55980515480042, 'accumulated_eval_time': 239.25912737846375, 'accumulated_logging_time': 0.06118941307067871, 'global_step': 509, 'preemption_count': 0}), (716, {'train/ssim': 0.7024536814008441, 'train/loss': 0.3062374251229422, 'validation/ssim': 0.6825161377365293, 'validation/loss': 0.3249259609550858, 'validation/num_examples': 3554, 'test/ssim': 0.7006145307918529, 'test/loss': 0.3266439302045518, 'test/num_examples': 3581, 'score': 328.0744700431824, 'total_duration': 576.7022390365601, 'accumulated_submission_time': 328.0744700431824, 'accumulated_eval_time': 245.15544176101685, 'accumulated_logging_time': 0.09543895721435547, 'global_step': 716, 'preemption_count': 0}), (918, {'train/ssim': 0.713130065373012, 'train/loss': 0.29831065450395855, 'validation/ssim': 0.6924108398987057, 'validation/loss': 0.3172396529438661, 'validation/num_examples': 3554, 'test/ssim': 0.7103617480932352, 'test/loss': 0.3189298091271642, 'test/num_examples': 3581, 'score': 407.9741961956024, 'total_duration': 665.933581829071, 'accumulated_submission_time': 407.9741961956024, 'accumulated_eval_time': 253.48493003845215, 'accumulated_logging_time': 0.13149571418762207, 'global_step': 918, 'preemption_count': 0}), (1213, {'train/ssim': 0.7194157327924456, 'train/loss': 0.28892179897853304, 'validation/ssim': 0.6983305974562113, 'validation/loss': 0.3073576936308561, 'validation/num_examples': 3554, 'test/ssim': 0.7159623244860025, 'test/loss': 0.30920222670430747, 'test/num_examples': 3581, 'score': 487.44238328933716, 'total_duration': 752.3367941379547, 'accumulated_submission_time': 487.44238328933716, 'accumulated_eval_time': 259.37974095344543, 'accumulated_logging_time': 0.17536449432373047, 'global_step': 1213, 'preemption_count': 0}), (1521, {'train/ssim': 0.7265268053327288, 'train/loss': 0.2833776814596994, 'validation/ssim': 0.705361696328081, 'validation/loss': 0.3018972282001794, 'validation/num_examples': 3554, 'test/ssim': 0.722781626880585, 'test/loss': 0.3035162590429524, 'test/num_examples': 3581, 'score': 566.9691789150238, 'total_duration': 838.6010181903839, 'accumulated_submission_time': 566.9691789150238, 'accumulated_eval_time': 265.1709449291229, 'accumulated_logging_time': 0.20132207870483398, 'global_step': 1521, 'preemption_count': 0}), (1829, {'train/ssim': 0.7199441364833287, 'train/loss': 0.2844561849321638, 'validation/ssim': 0.7010455456220104, 'validation/loss': 0.3022157994293402, 'validation/num_examples': 3554, 'test/ssim': 0.7176431518997836, 'test/loss': 0.30399468876535884, 'test/num_examples': 3581, 'score': 646.4586946964264, 'total_duration': 924.8027279376984, 'accumulated_submission_time': 646.4586946964264, 'accumulated_eval_time': 270.94679856300354, 'accumulated_logging_time': 0.2254931926727295, 'global_step': 1829, 'preemption_count': 0}), (2135, {'train/ssim': 0.7326239177158901, 'train/loss': 0.2802218198776245, 'validation/ssim': 0.7113897165825478, 'validation/loss': 0.29858432784802336, 'validation/num_examples': 3554, 'test/ssim': 0.7282336463278414, 'test/loss': 0.3008277805715059, 'test/num_examples': 3581, 'score': 726.1265122890472, 'total_duration': 1011.2302215099335, 'accumulated_submission_time': 726.1265122890472, 'accumulated_eval_time': 276.707816362381, 'accumulated_logging_time': 0.25042247772216797, 'global_step': 2135, 'preemption_count': 0}), (2444, {'train/ssim': 0.734095709664481, 'train/loss': 0.2759362118584769, 'validation/ssim': 0.712030705938731, 'validation/loss': 0.2945085395636958, 'validation/num_examples': 3554, 'test/ssim': 0.7293415852546425, 'test/loss': 0.29620390303206157, 'test/num_examples': 3581, 'score': 805.723030090332, 'total_duration': 1097.6036384105682, 'accumulated_submission_time': 805.723030090332, 'accumulated_eval_time': 282.5178198814392, 'accumulated_logging_time': 0.27398061752319336, 'global_step': 2444, 'preemption_count': 0}), (2753, {'train/ssim': 0.7337126731872559, 'train/loss': 0.2757058824811663, 'validation/ssim': 0.7117318157138084, 'validation/loss': 0.294372867719471, 'validation/num_examples': 3554, 'test/ssim': 0.729050266379852, 'test/loss': 0.2960281095102625, 'test/num_examples': 3581, 'score': 885.3843231201172, 'total_duration': 1184.0211849212646, 'accumulated_submission_time': 885.3843231201172, 'accumulated_eval_time': 288.2936661243439, 'accumulated_logging_time': 0.29852747917175293, 'global_step': 2753, 'preemption_count': 0}), (3064, {'train/ssim': 0.7348917552403041, 'train/loss': 0.27558508941105436, 'validation/ssim': 0.7122638554269837, 'validation/loss': 0.2941456946618247, 'validation/num_examples': 3554, 'test/ssim': 0.7296085650612608, 'test/loss': 0.2959964073626431, 'test/num_examples': 3581, 'score': 965.0553374290466, 'total_duration': 1270.5371618270874, 'accumulated_submission_time': 965.0553374290466, 'accumulated_eval_time': 294.1238112449646, 'accumulated_logging_time': 0.3228447437286377, 'global_step': 3064, 'preemption_count': 0}), (3371, {'train/ssim': 0.7375236238752093, 'train/loss': 0.2735832248415266, 'validation/ssim': 0.7148873710739659, 'validation/loss': 0.2927653452755698, 'validation/num_examples': 3554, 'test/ssim': 0.732084877761973, 'test/loss': 0.29453950619371333, 'test/num_examples': 3581, 'score': 1044.5425403118134, 'total_duration': 1356.7918014526367, 'accumulated_submission_time': 1044.5425403118134, 'accumulated_eval_time': 299.9197461605072, 'accumulated_logging_time': 0.34690427780151367, 'global_step': 3371, 'preemption_count': 0}), (3681, {'train/ssim': 0.7359226090567452, 'train/loss': 0.2734686817441668, 'validation/ssim': 0.7147759484251899, 'validation/loss': 0.2919789637886009, 'validation/num_examples': 3554, 'test/ssim': 0.7320963996177744, 'test/loss': 0.29348539274120355, 'test/num_examples': 3581, 'score': 1124.1040234565735, 'total_duration': 1443.1660206317902, 'accumulated_submission_time': 1124.1040234565735, 'accumulated_eval_time': 305.73155879974365, 'accumulated_logging_time': 0.3719668388366699, 'global_step': 3681, 'preemption_count': 0}), (3991, {'train/ssim': 0.7405587605067662, 'train/loss': 0.27273580006190706, 'validation/ssim': 0.7188544844937043, 'validation/loss': 0.2917125317643852, 'validation/num_examples': 3554, 'test/ssim': 0.7361104369196803, 'test/loss': 0.29318950603008936, 'test/num_examples': 3581, 'score': 1203.7507371902466, 'total_duration': 1529.6277277469635, 'accumulated_submission_time': 1203.7507371902466, 'accumulated_eval_time': 311.565477848053, 'accumulated_logging_time': 0.3964200019836426, 'global_step': 3991, 'preemption_count': 0}), (4298, {'train/ssim': 0.7356870515005929, 'train/loss': 0.2744684900556292, 'validation/ssim': 0.7158613918735931, 'validation/loss': 0.2925796294118423, 'validation/num_examples': 3554, 'test/ssim': 0.7326277685178721, 'test/loss': 0.2941692387448513, 'test/num_examples': 3581, 'score': 1283.2221932411194, 'total_duration': 1615.8767099380493, 'accumulated_submission_time': 1283.2221932411194, 'accumulated_eval_time': 317.35262274742126, 'accumulated_logging_time': 0.4210073947906494, 'global_step': 4298, 'preemption_count': 0}), (4609, {'train/ssim': 0.7426456723894391, 'train/loss': 0.2706260681152344, 'validation/ssim': 0.720315824818866, 'validation/loss': 0.2897289063599114, 'validation/num_examples': 3554, 'test/ssim': 0.7376476161075817, 'test/loss': 0.29124193742800547, 'test/num_examples': 3581, 'score': 1362.904296875, 'total_duration': 1702.3213410377502, 'accumulated_submission_time': 1362.904296875, 'accumulated_eval_time': 323.138956785202, 'accumulated_logging_time': 0.4463460445404053, 'global_step': 4609, 'preemption_count': 0}), (4919, {'train/ssim': 0.7413635935102191, 'train/loss': 0.27078393527439665, 'validation/ssim': 0.7193580159459412, 'validation/loss': 0.28967711062798956, 'validation/num_examples': 3554, 'test/ssim': 0.7366385333400237, 'test/loss': 0.29116915884180394, 'test/num_examples': 3581, 'score': 1442.6219186782837, 'total_duration': 1788.8227026462555, 'accumulated_submission_time': 1442.6219186782837, 'accumulated_eval_time': 328.9669449329376, 'accumulated_logging_time': 0.47068333625793457, 'global_step': 4919, 'preemption_count': 0}), (5227, {'train/ssim': 0.7392706189836774, 'train/loss': 0.2727593353816441, 'validation/ssim': 0.7171125954030669, 'validation/loss': 0.29157517690234597, 'validation/num_examples': 3554, 'test/ssim': 0.7342299882408894, 'test/loss': 0.2932301393203714, 'test/num_examples': 3581, 'score': 1522.107471704483, 'total_duration': 1875.049962759018, 'accumulated_submission_time': 1522.107471704483, 'accumulated_eval_time': 334.7532093524933, 'accumulated_logging_time': 0.4950675964355469, 'global_step': 5227, 'preemption_count': 0}), (5533, {'train/ssim': 0.742063045501709, 'train/loss': 0.2722414561680385, 'validation/ssim': 0.7204126155168472, 'validation/loss': 0.29106817637257315, 'validation/num_examples': 3554, 'test/ssim': 0.7373622286023457, 'test/loss': 0.29272505252330006, 'test/num_examples': 3581, 'score': 1601.614014863968, 'total_duration': 1961.308075428009, 'accumulated_submission_time': 1601.614014863968, 'accumulated_eval_time': 340.55744791030884, 'accumulated_logging_time': 0.520179033279419, 'global_step': 5533, 'preemption_count': 0}), (5843, {'train/ssim': 0.736884321485247, 'train/loss': 0.27581705365862164, 'validation/ssim': 0.7147157032568936, 'validation/loss': 0.2947708842589336, 'validation/num_examples': 3554, 'test/ssim': 0.7315846655962022, 'test/loss': 0.2966098609632435, 'test/num_examples': 3581, 'score': 1681.093552827835, 'total_duration': 2047.2707061767578, 'accumulated_submission_time': 1681.093552827835, 'accumulated_eval_time': 346.07404351234436, 'accumulated_logging_time': 0.5448422431945801, 'global_step': 5843, 'preemption_count': 0}), (6151, {'train/ssim': 0.741410459790911, 'train/loss': 0.2702733107975551, 'validation/ssim': 0.7190236106104038, 'validation/loss': 0.28938831850995356, 'validation/num_examples': 3554, 'test/ssim': 0.7365850828373709, 'test/loss': 0.2907710753150307, 'test/num_examples': 3581, 'score': 1760.6165096759796, 'total_duration': 2133.738173007965, 'accumulated_submission_time': 1760.6165096759796, 'accumulated_eval_time': 352.0022325515747, 'accumulated_logging_time': 0.5718903541564941, 'global_step': 6151, 'preemption_count': 0}), (6459, {'train/ssim': 0.7455499512808663, 'train/loss': 0.26953298704964773, 'validation/ssim': 0.7230374363613182, 'validation/loss': 0.2889066662893043, 'validation/num_examples': 3554, 'test/ssim': 0.7401407684436959, 'test/loss': 0.29037585520804243, 'test/num_examples': 3581, 'score': 1840.3476071357727, 'total_duration': 2220.3552284240723, 'accumulated_submission_time': 1840.3476071357727, 'accumulated_eval_time': 357.85781145095825, 'accumulated_logging_time': 0.5994911193847656, 'global_step': 6459, 'preemption_count': 0}), (6771, {'train/ssim': 0.7444658279418945, 'train/loss': 0.2686554193496704, 'validation/ssim': 0.7217985978606851, 'validation/loss': 0.288065861501785, 'validation/num_examples': 3554, 'test/ssim': 0.7390641907812063, 'test/loss': 0.2894817523801836, 'test/num_examples': 3581, 'score': 1919.8285059928894, 'total_duration': 2306.4212551116943, 'accumulated_submission_time': 1919.8285059928894, 'accumulated_eval_time': 363.38677406311035, 'accumulated_logging_time': 0.6258382797241211, 'global_step': 6771, 'preemption_count': 0}), (7079, {'train/ssim': 0.7423040526253837, 'train/loss': 0.26992688860212055, 'validation/ssim': 0.719702794166784, 'validation/loss': 0.2892800558129924, 'validation/num_examples': 3554, 'test/ssim': 0.7370093462021782, 'test/loss': 0.2907933009066951, 'test/num_examples': 3581, 'score': 1999.4033722877502, 'total_duration': 2393.1330904960632, 'accumulated_submission_time': 1999.4033722877502, 'accumulated_eval_time': 369.4419786930084, 'accumulated_logging_time': 0.6538686752319336, 'global_step': 7079, 'preemption_count': 0}), (7389, {'train/ssim': 0.7438771384102958, 'train/loss': 0.26996987206595285, 'validation/ssim': 0.7210762053979671, 'validation/loss': 0.2891653358231922, 'validation/num_examples': 3554, 'test/ssim': 0.7382988395786791, 'test/loss': 0.29082984359728425, 'test/num_examples': 3581, 'score': 2078.963389158249, 'total_duration': 2479.4423944950104, 'accumulated_submission_time': 2078.963389158249, 'accumulated_eval_time': 375.2313406467438, 'accumulated_logging_time': 0.6784687042236328, 'global_step': 7389, 'preemption_count': 0}), (7697, {'train/ssim': 0.7405800138201032, 'train/loss': 0.27401314462934223, 'validation/ssim': 0.7179149483856219, 'validation/loss': 0.29358789447189787, 'validation/num_examples': 3554, 'test/ssim': 0.7347750606499581, 'test/loss': 0.2953690798114179, 'test/num_examples': 3581, 'score': 2158.5110325813293, 'total_duration': 2565.505722284317, 'accumulated_submission_time': 2158.5110325813293, 'accumulated_eval_time': 380.76561093330383, 'accumulated_logging_time': 0.702869176864624, 'global_step': 7697, 'preemption_count': 0}), (8009, {'train/ssim': 0.7442974363054548, 'train/loss': 0.26928649629865375, 'validation/ssim': 0.7214374703239308, 'validation/loss': 0.2887322850353475, 'validation/num_examples': 3554, 'test/ssim': 0.7388245498158684, 'test/loss': 0.29023783155674043, 'test/num_examples': 3581, 'score': 2238.0990495681763, 'total_duration': 2652.2252440452576, 'accumulated_submission_time': 2238.0990495681763, 'accumulated_eval_time': 386.8607077598572, 'accumulated_logging_time': 0.7306511402130127, 'global_step': 8009, 'preemption_count': 0}), (8318, {'train/ssim': 0.7410651615687779, 'train/loss': 0.2717444896697998, 'validation/ssim': 0.7197819303513646, 'validation/loss': 0.29077862861388576, 'validation/num_examples': 3554, 'test/ssim': 0.7368259509826166, 'test/loss': 0.29214030129721097, 'test/num_examples': 3581, 'score': 2317.7767033576965, 'total_duration': 2738.345399618149, 'accumulated_submission_time': 2317.7767033576965, 'accumulated_eval_time': 392.3471648693085, 'accumulated_logging_time': 0.7555222511291504, 'global_step': 8318, 'preemption_count': 0}), (8625, {'train/ssim': 0.7443616049630302, 'train/loss': 0.26848730019160677, 'validation/ssim': 0.7216371655397088, 'validation/loss': 0.2877900870113341, 'validation/num_examples': 3554, 'test/ssim': 0.7389441998568835, 'test/loss': 0.2892454520712615, 'test/num_examples': 3581, 'score': 2397.402399301529, 'total_duration': 2825.0740025043488, 'accumulated_submission_time': 2397.402399301529, 'accumulated_eval_time': 398.4255197048187, 'accumulated_logging_time': 0.7836117744445801, 'global_step': 8625, 'preemption_count': 0}), (8934, {'train/ssim': 0.7458993366786412, 'train/loss': 0.26804510184696745, 'validation/ssim': 0.7229458664532921, 'validation/loss': 0.28764416249736213, 'validation/num_examples': 3554, 'test/ssim': 0.7400869770577353, 'test/loss': 0.2891295858349623, 'test/num_examples': 3581, 'score': 2476.94172167778, 'total_duration': 2911.371356010437, 'accumulated_submission_time': 2476.94172167778, 'accumulated_eval_time': 404.2142457962036, 'accumulated_logging_time': 0.8091602325439453, 'global_step': 8934, 'preemption_count': 0}), (9245, {'train/ssim': 0.7458521979195731, 'train/loss': 0.2685942990439279, 'validation/ssim': 0.7231327844726013, 'validation/loss': 0.2881876055149128, 'validation/num_examples': 3554, 'test/ssim': 0.7403120963941636, 'test/loss': 0.28961561725513124, 'test/num_examples': 3581, 'score': 2556.6616587638855, 'total_duration': 2997.849468946457, 'accumulated_submission_time': 2556.6616587638855, 'accumulated_eval_time': 410.0227704048157, 'accumulated_logging_time': 0.8359005451202393, 'global_step': 9245, 'preemption_count': 0}), (9555, {'train/ssim': 0.7455284936087472, 'train/loss': 0.26867711544036865, 'validation/ssim': 0.7228266126195836, 'validation/loss': 0.2879653097741981, 'validation/num_examples': 3554, 'test/ssim': 0.7400147097964954, 'test/loss': 0.2894689692561435, 'test/num_examples': 3581, 'score': 2636.2900590896606, 'total_duration': 3084.2279603481293, 'accumulated_submission_time': 2636.2900590896606, 'accumulated_eval_time': 415.8074870109558, 'accumulated_logging_time': 0.8618514537811279, 'global_step': 9555, 'preemption_count': 0}), (9863, {'train/ssim': 0.7442137854439872, 'train/loss': 0.2694025380270822, 'validation/ssim': 0.7215104239940912, 'validation/loss': 0.28899383974263154, 'validation/num_examples': 3554, 'test/ssim': 0.7387430105286582, 'test/loss': 0.2904448159011973, 'test/num_examples': 3581, 'score': 2715.83766746521, 'total_duration': 3170.4104874134064, 'accumulated_submission_time': 2715.83766746521, 'accumulated_eval_time': 421.4662461280823, 'accumulated_logging_time': 0.8874399662017822, 'global_step': 9863, 'preemption_count': 0}), (10172, {'train/ssim': 0.7373419489179339, 'train/loss': 0.2721356323787144, 'validation/ssim': 0.7169236165456176, 'validation/loss': 0.29107126762978336, 'validation/num_examples': 3554, 'test/ssim': 0.7337442295273666, 'test/loss': 0.2924061561889137, 'test/num_examples': 3581, 'score': 2795.3147897720337, 'total_duration': 3256.4523804187775, 'accumulated_submission_time': 2795.3147897720337, 'accumulated_eval_time': 427.01580834388733, 'accumulated_logging_time': 0.9152638912200928, 'global_step': 10172, 'preemption_count': 0}), (10479, {'train/ssim': 0.7423101152692523, 'train/loss': 0.26954589571271625, 'validation/ssim': 0.7197944327694148, 'validation/loss': 0.2887804056059194, 'validation/num_examples': 3554, 'test/ssim': 0.7373214589587406, 'test/loss': 0.290187619445511, 'test/num_examples': 3581, 'score': 2874.9394323825836, 'total_duration': 3342.6825063228607, 'accumulated_submission_time': 2874.9394323825836, 'accumulated_eval_time': 432.6046464443207, 'accumulated_logging_time': 0.9428362846374512, 'global_step': 10479, 'preemption_count': 0}), (10788, {'train/ssim': 0.7451863288879395, 'train/loss': 0.2679816314152309, 'validation/ssim': 0.7227734429955683, 'validation/loss': 0.28724083929968874, 'validation/num_examples': 3554, 'test/ssim': 0.7400273224788816, 'test/loss': 0.2886318280290073, 'test/num_examples': 3581, 'score': 2954.604351043701, 'total_duration': 3429.0071885585785, 'accumulated_submission_time': 2954.604351043701, 'accumulated_eval_time': 438.25855231285095, 'accumulated_logging_time': 0.9702644348144531, 'global_step': 10788, 'preemption_count': 0}), (11097, {'train/ssim': 0.7454932076590401, 'train/loss': 0.26740224020821707, 'validation/ssim': 0.7221193329698931, 'validation/loss': 0.2872440335988059, 'validation/num_examples': 3554, 'test/ssim': 0.7394572974116866, 'test/loss': 0.2886622007317265, 'test/num_examples': 3581, 'score': 3034.1390216350555, 'total_duration': 3515.5707976818085, 'accumulated_submission_time': 3034.1390216350555, 'accumulated_eval_time': 444.3115622997284, 'accumulated_logging_time': 0.9972431659698486, 'global_step': 11097, 'preemption_count': 0}), (11407, {'train/ssim': 0.7438053403581891, 'train/loss': 0.2682170016424997, 'validation/ssim': 0.7209619662703995, 'validation/loss': 0.2876438533716411, 'validation/num_examples': 3554, 'test/ssim': 0.7383730839631039, 'test/loss': 0.28903914949342013, 'test/num_examples': 3581, 'score': 3113.6554729938507, 'total_duration': 3601.829110622406, 'accumulated_submission_time': 3113.6554729938507, 'accumulated_eval_time': 450.1013078689575, 'accumulated_logging_time': 1.0226528644561768, 'global_step': 11407, 'preemption_count': 0}), (11719, {'train/ssim': 0.7459876196725028, 'train/loss': 0.2685935156685965, 'validation/ssim': 0.7233722538644837, 'validation/loss': 0.28805492188599113, 'validation/num_examples': 3554, 'test/ssim': 0.7404616759895979, 'test/loss': 0.28955562179296984, 'test/num_examples': 3581, 'score': 3193.3718087673187, 'total_duration': 3688.0435876846313, 'accumulated_submission_time': 3193.3718087673187, 'accumulated_eval_time': 455.62085723876953, 'accumulated_logging_time': 1.048095464706421, 'global_step': 11719, 'preemption_count': 0}), (12025, {'train/ssim': 0.7433530943734306, 'train/loss': 0.26904354776654926, 'validation/ssim': 0.7201315171945343, 'validation/loss': 0.28895320688396875, 'validation/num_examples': 3554, 'test/ssim': 0.7373912036835032, 'test/loss': 0.2903296655198443, 'test/num_examples': 3581, 'score': 3272.8409242630005, 'total_duration': 3774.5773737430573, 'accumulated_submission_time': 3272.8409242630005, 'accumulated_eval_time': 461.69947028160095, 'accumulated_logging_time': 1.0759556293487549, 'global_step': 12025, 'preemption_count': 0}), (12337, {'train/ssim': 0.7430891990661621, 'train/loss': 0.2692720719746181, 'validation/ssim': 0.72075842415676, 'validation/loss': 0.2885815003780951, 'validation/num_examples': 3554, 'test/ssim': 0.738035745841769, 'test/loss': 0.28994511506038817, 'test/num_examples': 3581, 'score': 3352.425797700882, 'total_duration': 3860.6608130931854, 'accumulated_submission_time': 3352.425797700882, 'accumulated_eval_time': 467.2343444824219, 'accumulated_logging_time': 1.100759506225586, 'global_step': 12337, 'preemption_count': 0}), (12645, {'train/ssim': 0.7434840883527484, 'train/loss': 0.26832214423588346, 'validation/ssim': 0.7214056647219682, 'validation/loss': 0.2876275727503341, 'validation/num_examples': 3554, 'test/ssim': 0.7385825226673765, 'test/loss': 0.2889664390838802, 'test/num_examples': 3581, 'score': 3431.963940858841, 'total_duration': 3947.2865085601807, 'accumulated_submission_time': 3431.963940858841, 'accumulated_eval_time': 473.33305740356445, 'accumulated_logging_time': 1.1286990642547607, 'global_step': 12645, 'preemption_count': 0}), (12952, {'train/ssim': 0.7464030129568917, 'train/loss': 0.2683166095188686, 'validation/ssim': 0.7235317627365293, 'validation/loss': 0.28795167389517096, 'validation/num_examples': 3554, 'test/ssim': 0.7408640546460485, 'test/loss': 0.2892552354221935, 'test/num_examples': 3581, 'score': 3511.4471578598022, 'total_duration': 4033.5201275348663, 'accumulated_submission_time': 3511.4471578598022, 'accumulated_eval_time': 479.1207890510559, 'accumulated_logging_time': 1.1542394161224365, 'global_step': 12952, 'preemption_count': 0}), (13259, {'train/ssim': 0.7465979712350028, 'train/loss': 0.2674778529575893, 'validation/ssim': 0.7233363265862408, 'validation/loss': 0.2872853018825619, 'validation/num_examples': 3554, 'test/ssim': 0.7405421244502234, 'test/loss': 0.28871677614929486, 'test/num_examples': 3581, 'score': 3590.9730904102325, 'total_duration': 4119.82766699791, 'accumulated_submission_time': 3590.9730904102325, 'accumulated_eval_time': 484.90194964408875, 'accumulated_logging_time': 1.1793885231018066, 'global_step': 13259, 'preemption_count': 0}), (13570, {'train/ssim': 0.7459131649562291, 'train/loss': 0.2676959548677717, 'validation/ssim': 0.7231196638031092, 'validation/loss': 0.28724305470068934, 'validation/num_examples': 3554, 'test/ssim': 0.7403845000087266, 'test/loss': 0.28868555123830636, 'test/num_examples': 3581, 'score': 3670.472277402878, 'total_duration': 4206.049309492111, 'accumulated_submission_time': 3670.472277402878, 'accumulated_eval_time': 490.6596417427063, 'accumulated_logging_time': 1.2047319412231445, 'global_step': 13570, 'preemption_count': 0}), (13879, {'train/ssim': 0.746532644544329, 'train/loss': 0.2674886328833444, 'validation/ssim': 0.7239132925708709, 'validation/loss': 0.2869240369565982, 'validation/num_examples': 3554, 'test/ssim': 0.7411158310571418, 'test/loss': 0.28829394449438006, 'test/num_examples': 3581, 'score': 3750.11217546463, 'total_duration': 4292.484053134918, 'accumulated_submission_time': 3750.11217546463, 'accumulated_eval_time': 496.4727256298065, 'accumulated_logging_time': 1.2311663627624512, 'global_step': 13879, 'preemption_count': 0}), (14187, {'train/ssim': 0.7452981812613351, 'train/loss': 0.2685773032052176, 'validation/ssim': 0.7220276943672622, 'validation/loss': 0.28860204006489165, 'validation/num_examples': 3554, 'test/ssim': 0.7390033771991064, 'test/loss': 0.2900556976054175, 'test/num_examples': 3581, 'score': 3829.635034799576, 'total_duration': 4378.772463560104, 'accumulated_submission_time': 3829.635034799576, 'accumulated_eval_time': 502.2913980484009, 'accumulated_logging_time': 1.2570981979370117, 'global_step': 14187, 'preemption_count': 0}), (14498, {'train/ssim': 0.746049063546317, 'train/loss': 0.2672664097377232, 'validation/ssim': 0.7230577012696962, 'validation/loss': 0.2868891744447278, 'validation/num_examples': 3554, 'test/ssim': 0.7403377308189053, 'test/loss': 0.2882701849278309, 'test/num_examples': 3581, 'score': 3909.278613090515, 'total_duration': 4465.1439690589905, 'accumulated_submission_time': 3909.278613090515, 'accumulated_eval_time': 508.0712606906891, 'accumulated_logging_time': 1.2835757732391357, 'global_step': 14498, 'preemption_count': 0}), (14807, {'train/ssim': 0.7457357134137835, 'train/loss': 0.26792422362736296, 'validation/ssim': 0.7228009208374367, 'validation/loss': 0.28749835819894837, 'validation/num_examples': 3554, 'test/ssim': 0.7401610169121754, 'test/loss': 0.28879238406695057, 'test/num_examples': 3581, 'score': 3988.9324967861176, 'total_duration': 4551.5749180316925, 'accumulated_submission_time': 3988.9324967861176, 'accumulated_eval_time': 513.8907482624054, 'accumulated_logging_time': 1.3096263408660889, 'global_step': 14807, 'preemption_count': 0}), (15115, {'train/ssim': 0.7464144570486886, 'train/loss': 0.2668846164430891, 'validation/ssim': 0.7233416847654052, 'validation/loss': 0.2865295925365785, 'validation/num_examples': 3554, 'test/ssim': 0.7406838637295797, 'test/loss': 0.28784994398605485, 'test/num_examples': 3581, 'score': 4068.5602083206177, 'total_duration': 4637.6413397789, 'accumulated_submission_time': 4068.5602083206177, 'accumulated_eval_time': 519.3684697151184, 'accumulated_logging_time': 1.3356173038482666, 'global_step': 15115, 'preemption_count': 0}), (15424, {'train/ssim': 0.7472506250653949, 'train/loss': 0.26673884051186697, 'validation/ssim': 0.7237581801535242, 'validation/loss': 0.2868198959359173, 'validation/num_examples': 3554, 'test/ssim': 0.7410028623289584, 'test/loss': 0.28818404371596623, 'test/num_examples': 3581, 'score': 4148.126308917999, 'total_duration': 4724.273729801178, 'accumulated_submission_time': 4148.126308917999, 'accumulated_eval_time': 525.4387454986572, 'accumulated_logging_time': 1.3653252124786377, 'global_step': 15424, 'preemption_count': 0}), (15736, {'train/ssim': 0.7459214074271066, 'train/loss': 0.26725326265607563, 'validation/ssim': 0.7233034905652083, 'validation/loss': 0.2866978771443708, 'validation/num_examples': 3554, 'test/ssim': 0.7406325267034348, 'test/loss': 0.2880339187072396, 'test/num_examples': 3581, 'score': 4227.783855438232, 'total_duration': 4810.729299783707, 'accumulated_submission_time': 4227.783855438232, 'accumulated_eval_time': 531.2315909862518, 'accumulated_logging_time': 1.392350673675537, 'global_step': 15736, 'preemption_count': 0}), (16047, {'train/ssim': 0.7475191525050572, 'train/loss': 0.2671656608581543, 'validation/ssim': 0.7245678834587789, 'validation/loss': 0.2868716229732344, 'validation/num_examples': 3554, 'test/ssim': 0.7417936434262427, 'test/loss': 0.2882191546966629, 'test/num_examples': 3581, 'score': 4307.437514066696, 'total_duration': 4896.921824455261, 'accumulated_submission_time': 4307.437514066696, 'accumulated_eval_time': 536.7663447856903, 'accumulated_logging_time': 1.4209182262420654, 'global_step': 16047, 'preemption_count': 0}), (16355, {'train/ssim': 0.7468970843723842, 'train/loss': 0.2665720156260899, 'validation/ssim': 0.723749662022545, 'validation/loss': 0.2865244404412282, 'validation/num_examples': 3554, 'test/ssim': 0.7409775006108629, 'test/loss': 0.2878755784107966, 'test/num_examples': 3581, 'score': 4386.870840072632, 'total_duration': 4983.391057252884, 'accumulated_submission_time': 4386.870840072632, 'accumulated_eval_time': 542.7754430770874, 'accumulated_logging_time': 1.4489686489105225, 'global_step': 16355, 'preemption_count': 0}), (16666, {'train/ssim': 0.7471059390476772, 'train/loss': 0.2673354319163731, 'validation/ssim': 0.7241452055562394, 'validation/loss': 0.2872957434624719, 'validation/num_examples': 3554, 'test/ssim': 0.7412302314952178, 'test/loss': 0.288708935833217, 'test/num_examples': 3581, 'score': 4466.468131065369, 'total_duration': 5069.773614883423, 'accumulated_submission_time': 4466.468131065369, 'accumulated_eval_time': 548.5780034065247, 'accumulated_logging_time': 1.4756381511688232, 'global_step': 16666, 'preemption_count': 0}), (16976, {'train/ssim': 0.747286183493478, 'train/loss': 0.2676194225038801, 'validation/ssim': 0.7244386002127884, 'validation/loss': 0.2872222402354741, 'validation/num_examples': 3554, 'test/ssim': 0.7416266106054524, 'test/loss': 0.2885459936121195, 'test/num_examples': 3581, 'score': 4546.0840356349945, 'total_duration': 5156.204714536667, 'accumulated_submission_time': 4546.0840356349945, 'accumulated_eval_time': 554.4256772994995, 'accumulated_logging_time': 1.5024745464324951, 'global_step': 16976, 'preemption_count': 0}), (17284, {'train/ssim': 0.7463902745928083, 'train/loss': 0.26667182786124094, 'validation/ssim': 0.7233096730796286, 'validation/loss': 0.2862843184506014, 'validation/num_examples': 3554, 'test/ssim': 0.74071031627426, 'test/loss': 0.28756922658213485, 'test/num_examples': 3581, 'score': 4625.585852384567, 'total_duration': 5242.536526918411, 'accumulated_submission_time': 4625.585852384567, 'accumulated_eval_time': 560.2594442367554, 'accumulated_logging_time': 1.5338432788848877, 'global_step': 17284, 'preemption_count': 0}), (17592, {'train/ssim': 0.7479167665754046, 'train/loss': 0.26628707136426655, 'validation/ssim': 0.7245708373267797, 'validation/loss': 0.2862804543790887, 'validation/num_examples': 3554, 'test/ssim': 0.7417787809140254, 'test/loss': 0.28765710629886904, 'test/num_examples': 3581, 'score': 4705.243488550186, 'total_duration': 5328.9664669036865, 'accumulated_submission_time': 4705.243488550186, 'accumulated_eval_time': 566.0754532814026, 'accumulated_logging_time': 1.56105637550354, 'global_step': 17592, 'preemption_count': 0}), (17902, {'train/ssim': 0.746361255645752, 'train/loss': 0.26691198348999023, 'validation/ssim': 0.7232250413266742, 'validation/loss': 0.28665715841745215, 'validation/num_examples': 3554, 'test/ssim': 0.7404264968322396, 'test/loss': 0.28802921451759284, 'test/num_examples': 3581, 'score': 4784.838724851608, 'total_duration': 5415.319300413132, 'accumulated_submission_time': 4784.838724851608, 'accumulated_eval_time': 571.8675920963287, 'accumulated_logging_time': 1.5877399444580078, 'global_step': 17902, 'preemption_count': 0}), (18212, {'train/ssim': 0.7478054591587612, 'train/loss': 0.2665275165012905, 'validation/ssim': 0.7250058115635551, 'validation/loss': 0.2862256876055149, 'validation/num_examples': 3554, 'test/ssim': 0.7422586764346552, 'test/loss': 0.28744204302001886, 'test/num_examples': 3581, 'score': 4864.446467638016, 'total_duration': 5501.428427219391, 'accumulated_submission_time': 4864.446467638016, 'accumulated_eval_time': 577.414220571518, 'accumulated_logging_time': 1.613891839981079, 'global_step': 18212, 'preemption_count': 0}), (18521, {'train/ssim': 0.7483962603977748, 'train/loss': 0.26638766697474886, 'validation/ssim': 0.7251937600019345, 'validation/loss': 0.28640336620049595, 'validation/num_examples': 3554, 'test/ssim': 0.7422457910456227, 'test/loss': 0.28782778657105207, 'test/num_examples': 3581, 'score': 4944.01176404953, 'total_duration': 5588.117795705795, 'accumulated_submission_time': 4944.01176404953, 'accumulated_eval_time': 583.5225822925568, 'accumulated_logging_time': 1.643033742904663, 'global_step': 18521, 'preemption_count': 0}), (18834, {'train/ssim': 0.7462680680411202, 'train/loss': 0.26769535882132395, 'validation/ssim': 0.7236066398556205, 'validation/loss': 0.287207075901493, 'validation/num_examples': 3554, 'test/ssim': 0.7408335115016755, 'test/loss': 0.28860288703618053, 'test/num_examples': 3581, 'score': 5023.695832967758, 'total_duration': 5674.5805950164795, 'accumulated_submission_time': 5023.695832967758, 'accumulated_eval_time': 589.3280339241028, 'accumulated_logging_time': 1.669280767440796, 'global_step': 18834, 'preemption_count': 0}), (19143, {'train/ssim': 0.7468204498291016, 'train/loss': 0.26704842703683035, 'validation/ssim': 0.7239463346757175, 'validation/loss': 0.2866950950128816, 'validation/num_examples': 3554, 'test/ssim': 0.7412553886833286, 'test/loss': 0.2879043489619694, 'test/num_examples': 3581, 'score': 5103.444320201874, 'total_duration': 5760.794360876083, 'accumulated_submission_time': 5103.444320201874, 'accumulated_eval_time': 594.8479006290436, 'accumulated_logging_time': 1.6969878673553467, 'global_step': 19143, 'preemption_count': 0}), (19452, {'train/ssim': 0.7473120008196149, 'train/loss': 0.26621677194322857, 'validation/ssim': 0.724248590936269, 'validation/loss': 0.2860234850366664, 'validation/num_examples': 3554, 'test/ssim': 0.7415792278256772, 'test/loss': 0.2872748738459055, 'test/num_examples': 3581, 'score': 5183.024216890335, 'total_duration': 5847.504777431488, 'accumulated_submission_time': 5183.024216890335, 'accumulated_eval_time': 600.9682488441467, 'accumulated_logging_time': 1.7266974449157715, 'global_step': 19452, 'preemption_count': 0}), (19762, {'train/ssim': 0.7486780030386788, 'train/loss': 0.26585989339011057, 'validation/ssim': 0.725319677212296, 'validation/loss': 0.2859349892121993, 'validation/num_examples': 3554, 'test/ssim': 0.7425679939480941, 'test/loss': 0.2872758624074979, 'test/num_examples': 3581, 'score': 5262.777628421783, 'total_duration': 5934.007062911987, 'accumulated_submission_time': 5262.777628421783, 'accumulated_eval_time': 606.7510075569153, 'accumulated_logging_time': 1.7542026042938232, 'global_step': 19762, 'preemption_count': 0}), (20074, {'train/ssim': 0.7471725600106376, 'train/loss': 0.2661230223519461, 'validation/ssim': 0.7241128503974396, 'validation/loss': 0.28592739845838316, 'validation/num_examples': 3554, 'test/ssim': 0.7413708117713278, 'test/loss': 0.2872406150734781, 'test/num_examples': 3581, 'score': 5342.303017139435, 'total_duration': 6020.338069200516, 'accumulated_submission_time': 5342.303017139435, 'accumulated_eval_time': 612.57315325737, 'accumulated_logging_time': 1.7803120613098145, 'global_step': 20074, 'preemption_count': 0}), (20385, {'train/ssim': 0.7481504167829242, 'train/loss': 0.266286952154977, 'validation/ssim': 0.7253547801552828, 'validation/loss': 0.2860922826832442, 'validation/num_examples': 3554, 'test/ssim': 0.7425520406092921, 'test/loss': 0.2873450617189682, 'test/num_examples': 3581, 'score': 5421.840210914612, 'total_duration': 6106.680652141571, 'accumulated_submission_time': 5421.840210914612, 'accumulated_eval_time': 618.3930585384369, 'accumulated_logging_time': 1.8078217506408691, 'global_step': 20385, 'preemption_count': 0}), (20694, {'train/ssim': 0.7477902003696987, 'train/loss': 0.2659025362559727, 'validation/ssim': 0.7246776574370428, 'validation/loss': 0.28586205271569004, 'validation/num_examples': 3554, 'test/ssim': 0.7418751145367914, 'test/loss': 0.2871738360334927, 'test/num_examples': 3581, 'score': 5501.550112485886, 'total_duration': 6193.171048402786, 'accumulated_submission_time': 5501.550112485886, 'accumulated_eval_time': 624.1928188800812, 'accumulated_logging_time': 1.8353190422058105, 'global_step': 20694, 'preemption_count': 0}), (21004, {'train/ssim': 0.7482043675013951, 'train/loss': 0.2658792734146118, 'validation/ssim': 0.7251773419914181, 'validation/loss': 0.2858192731506313, 'validation/num_examples': 3554, 'test/ssim': 0.7423757357625315, 'test/loss': 0.28716821145891513, 'test/num_examples': 3581, 'score': 5581.184408187866, 'total_duration': 6279.560573577881, 'accumulated_submission_time': 5581.184408187866, 'accumulated_eval_time': 629.9551358222961, 'accumulated_logging_time': 1.861687421798706, 'global_step': 21004, 'preemption_count': 0}), (21315, {'train/ssim': 0.74720641544887, 'train/loss': 0.26581084728240967, 'validation/ssim': 0.7241199946363253, 'validation/loss': 0.28573680527772405, 'validation/num_examples': 3554, 'test/ssim': 0.7414712359937866, 'test/loss': 0.2869880887191078, 'test/num_examples': 3581, 'score': 5660.842807769775, 'total_duration': 6365.988186836243, 'accumulated_submission_time': 5660.842807769775, 'accumulated_eval_time': 635.7525928020477, 'accumulated_logging_time': 1.8901846408843994, 'global_step': 21315, 'preemption_count': 0}), (21622, {'train/ssim': 0.7480870655604771, 'train/loss': 0.26618668011256624, 'validation/ssim': 0.7248988540640827, 'validation/loss': 0.2861559969290764, 'validation/num_examples': 3554, 'test/ssim': 0.7421755690842292, 'test/loss': 0.2874003529914828, 'test/num_examples': 3581, 'score': 5740.562634468079, 'total_duration': 6452.473796367645, 'accumulated_submission_time': 5740.562634468079, 'accumulated_eval_time': 641.5463263988495, 'accumulated_logging_time': 1.9170351028442383, 'global_step': 21622, 'preemption_count': 0}), (21932, {'train/ssim': 0.7484736442565918, 'train/loss': 0.2652705907821655, 'validation/ssim': 0.7249953699836452, 'validation/loss': 0.28549494874832937, 'validation/num_examples': 3554, 'test/ssim': 0.7422237018072815, 'test/loss': 0.2868142382321628, 'test/num_examples': 3581, 'score': 5820.106271743774, 'total_duration': 6538.813883781433, 'accumulated_submission_time': 5820.106271743774, 'accumulated_eval_time': 647.3496873378754, 'accumulated_logging_time': 1.9472379684448242, 'global_step': 21932, 'preemption_count': 0}), (22244, {'train/ssim': 0.7488002095903669, 'train/loss': 0.26557263306209017, 'validation/ssim': 0.7256000198939575, 'validation/loss': 0.2855803361419351, 'validation/num_examples': 3554, 'test/ssim': 0.7427972720608769, 'test/loss': 0.28690054988568137, 'test/num_examples': 3581, 'score': 5899.840828418732, 'total_duration': 6625.300048351288, 'accumulated_submission_time': 5899.840828418732, 'accumulated_eval_time': 653.1270537376404, 'accumulated_logging_time': 1.97556471824646, 'global_step': 22244, 'preemption_count': 0}), (22554, {'train/ssim': 0.7480750765119281, 'train/loss': 0.2656329699925014, 'validation/ssim': 0.7250469596317529, 'validation/loss': 0.28557805204632986, 'validation/num_examples': 3554, 'test/ssim': 0.7422637896842712, 'test/loss': 0.28680428443957695, 'test/num_examples': 3581, 'score': 5979.526552915573, 'total_duration': 6711.722568035126, 'accumulated_submission_time': 5979.526552915573, 'accumulated_eval_time': 658.9185583591461, 'accumulated_logging_time': 2.003601551055908, 'global_step': 22554, 'preemption_count': 0}), (22862, {'train/ssim': 0.7485605648585728, 'train/loss': 0.26537108421325684, 'validation/ssim': 0.7251298740195906, 'validation/loss': 0.2855709593283976, 'validation/num_examples': 3554, 'test/ssim': 0.742357873477206, 'test/loss': 0.286857257705599, 'test/num_examples': 3581, 'score': 6059.249801874161, 'total_duration': 6798.195337295532, 'accumulated_submission_time': 6059.249801874161, 'accumulated_eval_time': 664.7048542499542, 'accumulated_logging_time': 2.0317091941833496, 'global_step': 22862, 'preemption_count': 0}), (23172, {'train/ssim': 0.7487709862845284, 'train/loss': 0.2652000870023455, 'validation/ssim': 0.7253795102129643, 'validation/loss': 0.28535484610210327, 'validation/num_examples': 3554, 'test/ssim': 0.7426281257635786, 'test/loss': 0.28666298830852066, 'test/num_examples': 3581, 'score': 6138.734473466873, 'total_duration': 6884.450774669647, 'accumulated_submission_time': 6138.734473466873, 'accumulated_eval_time': 670.501941204071, 'accumulated_logging_time': 2.0600852966308594, 'global_step': 23172, 'preemption_count': 0}), (23482, {'train/ssim': 0.7492326327732631, 'train/loss': 0.26520969186510357, 'validation/ssim': 0.7258513047578433, 'validation/loss': 0.2853496253121483, 'validation/num_examples': 3554, 'test/ssim': 0.7431566312438914, 'test/loss': 0.28660599261946734, 'test/num_examples': 3581, 'score': 6218.294421195984, 'total_duration': 6970.79360127449, 'accumulated_submission_time': 6218.294421195984, 'accumulated_eval_time': 676.3145544528961, 'accumulated_logging_time': 2.0873494148254395, 'global_step': 23482, 'preemption_count': 0}), (23793, {'train/ssim': 0.7490583147321429, 'train/loss': 0.2651618548801967, 'validation/ssim': 0.7258139348929024, 'validation/loss': 0.28524761382421215, 'validation/num_examples': 3554, 'test/ssim': 0.7430990219648841, 'test/loss': 0.2864280856211603, 'test/num_examples': 3581, 'score': 6298.002183437347, 'total_duration': 7057.218279838562, 'accumulated_submission_time': 6298.002183437347, 'accumulated_eval_time': 682.1003348827362, 'accumulated_logging_time': 2.116272211074829, 'global_step': 23793, 'preemption_count': 0}), (24104, {'train/ssim': 0.7493989808218819, 'train/loss': 0.2646122489656721, 'validation/ssim': 0.7256988027354742, 'validation/loss': 0.28504266347117685, 'validation/num_examples': 3554, 'test/ssim': 0.7429555100923275, 'test/loss': 0.28636672662576795, 'test/num_examples': 3581, 'score': 6377.555734157562, 'total_duration': 7143.524742841721, 'accumulated_submission_time': 6377.555734157562, 'accumulated_eval_time': 687.9045672416687, 'accumulated_logging_time': 2.143724203109741, 'global_step': 24104, 'preemption_count': 0}), (24414, {'train/ssim': 0.7492085184369769, 'train/loss': 0.26499567713056293, 'validation/ssim': 0.7257925708708497, 'validation/loss': 0.2851463064559739, 'validation/num_examples': 3554, 'test/ssim': 0.7430490484719702, 'test/loss': 0.2864555949040945, 'test/num_examples': 3581, 'score': 6457.212813615799, 'total_duration': 7229.71315741539, 'accumulated_submission_time': 6457.212813615799, 'accumulated_eval_time': 693.4177057743073, 'accumulated_logging_time': 2.171322822570801, 'global_step': 24414, 'preemption_count': 0}), (24724, {'train/ssim': 0.7491113798958915, 'train/loss': 0.26487817083086285, 'validation/ssim': 0.7257345926245076, 'validation/loss': 0.2850529676618775, 'validation/num_examples': 3554, 'test/ssim': 0.7429647139416364, 'test/loss': 0.28626967714805573, 'test/num_examples': 3581, 'score': 6536.73136639595, 'total_duration': 7316.344540596008, 'accumulated_submission_time': 6536.73136639595, 'accumulated_eval_time': 699.5285453796387, 'accumulated_logging_time': 2.2020840644836426, 'global_step': 24724, 'preemption_count': 0}), (25033, {'train/ssim': 0.7494565418788365, 'train/loss': 0.2645956448146275, 'validation/ssim': 0.7258850338087366, 'validation/loss': 0.2850158382280529, 'validation/num_examples': 3554, 'test/ssim': 0.7430658881073723, 'test/loss': 0.2863145033030229, 'test/num_examples': 3581, 'score': 6616.517788171768, 'total_duration': 7402.593783855438, 'accumulated_submission_time': 6616.517788171768, 'accumulated_eval_time': 705.0331559181213, 'accumulated_logging_time': 2.231322765350342, 'global_step': 25033, 'preemption_count': 0}), (25342, {'train/ssim': 0.7493880816868373, 'train/loss': 0.264569844518389, 'validation/ssim': 0.7257581548739097, 'validation/loss': 0.2849689541603651, 'validation/num_examples': 3554, 'test/ssim': 0.7430480939987084, 'test/loss': 0.2862217148666574, 'test/num_examples': 3581, 'score': 6696.016061306, 'total_duration': 7489.222241640091, 'accumulated_submission_time': 6696.016061306, 'accumulated_eval_time': 711.1502118110657, 'accumulated_logging_time': 2.2611663341522217, 'global_step': 25342, 'preemption_count': 0}), (25650, {'train/ssim': 0.7501136916024345, 'train/loss': 0.26442488602229525, 'validation/ssim': 0.7266153261509919, 'validation/loss': 0.2847477231860228, 'validation/num_examples': 3554, 'test/ssim': 0.7438383296827004, 'test/loss': 0.2860105376575154, 'test/num_examples': 3581, 'score': 6775.620117425919, 'total_duration': 7575.6566071510315, 'accumulated_submission_time': 6775.620117425919, 'accumulated_eval_time': 716.9875633716583, 'accumulated_logging_time': 2.290419578552246, 'global_step': 25650, 'preemption_count': 0}), (25958, {'train/ssim': 0.7501708439418248, 'train/loss': 0.2642996140888759, 'validation/ssim': 0.7266605958954699, 'validation/loss': 0.2846713176119777, 'validation/num_examples': 3554, 'test/ssim': 0.7438678501771503, 'test/loss': 0.2859373840996754, 'test/num_examples': 3581, 'score': 6855.180752754211, 'total_duration': 7662.037658452988, 'accumulated_submission_time': 6855.180752754211, 'accumulated_eval_time': 722.8146538734436, 'accumulated_logging_time': 2.3198323249816895, 'global_step': 25958, 'preemption_count': 0}), (26265, {'train/ssim': 0.7510219982692173, 'train/loss': 0.26386341026851107, 'validation/ssim': 0.7272205256181415, 'validation/loss': 0.28455625414915414, 'validation/num_examples': 3554, 'test/ssim': 0.7443772661922298, 'test/loss': 0.2858099789634093, 'test/num_examples': 3581, 'score': 6934.775408029556, 'total_duration': 7748.472028017044, 'accumulated_submission_time': 6934.775408029556, 'accumulated_eval_time': 728.6625134944916, 'accumulated_logging_time': 2.3479294776916504, 'global_step': 26265, 'preemption_count': 0}), (26573, {'train/ssim': 0.7504673685346331, 'train/loss': 0.2638624225343977, 'validation/ssim': 0.7266487117288618, 'validation/loss': 0.28443938745295794, 'validation/num_examples': 3554, 'test/ssim': 0.743895120841769, 'test/loss': 0.28570087926076865, 'test/num_examples': 3581, 'score': 7014.378292798996, 'total_duration': 7834.939022302628, 'accumulated_submission_time': 7014.378292798996, 'accumulated_eval_time': 734.531521320343, 'accumulated_logging_time': 2.3762354850769043, 'global_step': 26573, 'preemption_count': 0}), (26881, {'train/ssim': 0.7506608963012695, 'train/loss': 0.26375067234039307, 'validation/ssim': 0.7269144911543332, 'validation/loss': 0.2843712080578222, 'validation/num_examples': 3554, 'test/ssim': 0.7441134906887042, 'test/loss': 0.28558383697705775, 'test/num_examples': 3581, 'score': 7093.9232568740845, 'total_duration': 7921.2747802734375, 'accumulated_submission_time': 7093.9232568740845, 'accumulated_eval_time': 740.3439342975616, 'accumulated_logging_time': 2.4059293270111084, 'global_step': 26881, 'preemption_count': 0}), (27142, {'train/ssim': 0.7509222030639648, 'train/loss': 0.26384077753339497, 'validation/ssim': 0.7272265020487478, 'validation/loss': 0.28445913715180077, 'validation/num_examples': 3554, 'test/ssim': 0.7444088319865261, 'test/loss': 0.2857359902414654, 'test/num_examples': 3581, 'score': 7161.631084918976, 'total_duration': 7995.774512290955, 'accumulated_submission_time': 7161.631084918976, 'accumulated_eval_time': 746.1995131969452, 'accumulated_logging_time': 2.434459924697876, 'global_step': 27142, 'preemption_count': 0})], 'global_step': 27142}
I0913 21:21:51.908226 140624454674240 submission_runner.py:543] Timing: 7161.631084918976
I0913 21:21:51.908279 140624454674240 submission_runner.py:545] Total number of evals: 90
I0913 21:21:51.908342 140624454674240 submission_runner.py:546] ====================
I0913 21:21:51.908592 140624454674240 submission_runner.py:614] Final fastmri score: 7161.631084918976
