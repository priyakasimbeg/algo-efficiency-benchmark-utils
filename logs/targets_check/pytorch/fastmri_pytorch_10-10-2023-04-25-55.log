torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nesterov.py --tuning_search_space=reference_algorithms/target_setting_algorithms/fastmri/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=fastmri_targets_check/nesterov_run_2 --overwrite=true --save_checkpoints=false --max_global_steps=27142 --torch_compile=true 2>&1 | tee -a /logs/fastmri_pytorch_10-10-2023-04-25-55.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-10-10 04:26:05.589785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 04:26:05.589807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1010 04:26:20.542469 139627708704576 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I1010 04:26:20.542484 139936525821760 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I1010 04:26:20.542513 139791811921728 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I1010 04:26:20.542546 139690322835264 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I1010 04:26:20.542584 140412964316992 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I1010 04:26:20.542600 139846818621248 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I1010 04:26:20.542801 139990339839808 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I1010 04:26:20.552409 140204662740800 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I1010 04:26:20.552754 140204662740800 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553072 139627708704576 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553113 139791811921728 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553157 139936525821760 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553187 140412964316992 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553199 139846818621248 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553363 139690322835264 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.553495 139990339839808 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 04:26:20.881723 140204662740800 logger_utils.py:76] Creating experiment directory at /experiment_runs/fastmri_targets_check/nesterov_run_2/fastmri_pytorch.
W1010 04:26:21.667385 139990339839808 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.667569 139936525821760 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.667651 140204662740800 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.667756 139846818621248 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.668185 139791811921728 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.668321 139690322835264 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.668513 139627708704576 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 04:26:21.670222 140412964316992 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1010 04:26:21.674463 140204662740800 submission_runner.py:507] Using RNG seed 1223169239
I1010 04:26:21.676617 140204662740800 submission_runner.py:516] --- Tuning run 1/1 ---
I1010 04:26:21.676762 140204662740800 submission_runner.py:521] Creating tuning directory at /experiment_runs/fastmri_targets_check/nesterov_run_2/fastmri_pytorch/trial_1.
I1010 04:26:21.676971 140204662740800 logger_utils.py:92] Saving hparams to /experiment_runs/fastmri_targets_check/nesterov_run_2/fastmri_pytorch/trial_1/hparams.json.
I1010 04:26:21.677891 140204662740800 submission_runner.py:191] Initializing dataset.
I1010 04:26:21.678032 140204662740800 submission_runner.py:198] Initializing model.
I1010 04:26:26.074841 140204662740800 submission_runner.py:229] Performing `torch.compile`.
I1010 04:26:26.349203 140204662740800 submission_runner.py:232] Initializing optimizer.
I1010 04:26:26.887107 140204662740800 submission_runner.py:239] Initializing metrics bundle.
I1010 04:26:26.887303 140204662740800 submission_runner.py:257] Initializing checkpoint and logger.
I1010 04:26:26.887997 140204662740800 submission_runner.py:277] Saving meta data to /experiment_runs/fastmri_targets_check/nesterov_run_2/fastmri_pytorch/trial_1/meta_data_0.json.
I1010 04:26:26.888276 140204662740800 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1010 04:26:26.888365 140204662740800 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1010 04:26:27.100718 140204662740800 logger_utils.py:220] Unable to record git information. Continuing without it.
I1010 04:26:27.283230 140204662740800 submission_runner.py:280] Saving flags to /experiment_runs/fastmri_targets_check/nesterov_run_2/fastmri_pytorch/trial_1/flags_0.json.
I1010 04:26:27.375315 140204662740800 submission_runner.py:290] Starting training loop.
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:26:27,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:26:27,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,743] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,743] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,743] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:27,743] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:27,743] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,744] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:27,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,745] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,745] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,745] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:27,746] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,747] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,747] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:27,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:26:27,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:27,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:26:27,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:26:27,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:26:33,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:26:33,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:26:33,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:26:33,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:26:33,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:26:33,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:26:33,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:27:14,104] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:14,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,139] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,141] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,142] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,142] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,143] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,143] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:14,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:14,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 04:27:14,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:14,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:14,656] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:14,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:14,657] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:17,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:17,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:17,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:17,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:17,934] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:17,934] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:17,935] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:17,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:17,957] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:17,957] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:17,958] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:17,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:17,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:17,970] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:17,970] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:17,971] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:18,002] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:18,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:18,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:18,018] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:18,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:18,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:18,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:18,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:18,056] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:18,057] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:18,057] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:18,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:18,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:18,066] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:18,066] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:18,067] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:18,067] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:18,067] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:18,067] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:18,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:18,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:18,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:18,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:18,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:18,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:18,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:19,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,625] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,639] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,682] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,696] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,734] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,736] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,737] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:19,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,747] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,747] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,748] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:19,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,790] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,790] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,790] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:19,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,805] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,838] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:19,849] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:19,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,915] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:19,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:19,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,960] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,960] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,961] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:19,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,964] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,964] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,965] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:19,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:19,967] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:19,968] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:19,968] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:20,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:20,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:20,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:20,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:20,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:20,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:20,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:21,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,118] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,130] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,190] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,192] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,192] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,192] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,232] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,232] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,242] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,242] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,286] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,353] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,400] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,400] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,400] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,403] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:21,407] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:21,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,461] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:21,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,511] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:21,513] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:21,514] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:21,514] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:21,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:21,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 04:27:22,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:22,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:22,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:22,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:22,711] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:22,721] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:22,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:22,757] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:22,757] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:22,757] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:22,763] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:22,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:22,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,770] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:22,770] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:22,771] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:22,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,788] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:22,811] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:22,813] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:22,814] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:22,814] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:22,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,898] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:22,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:22,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,983] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:22,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:22,988] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:23,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:23,003] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:23,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:23,004] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:23,004] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:23,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,041] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:23,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:23,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:23,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:23,088] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:23,088] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:23,088] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:23,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:23,101] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:23,101] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:23,102] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:23,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:23,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:23,112] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:23,112] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:23,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:23,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 04:27:23,931] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:24,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:24,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:24,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:24,783] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:24,788] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:24,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,842] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:24,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:24,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:24,866] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:24,866] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:24,866] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:24,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:24,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:24,872] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:24,872] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:24,873] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:24,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:24,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:24,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:24,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:25,000] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,004] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,004] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:25,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:25,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:25,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,087] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,087] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,088] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,088] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,095] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,108] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,189] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,190] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,191] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,191] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,191] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,191] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,192] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,192] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,199] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,199] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,199] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,309] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,347] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,394] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,442] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,442] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,442] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:25,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,451] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,532] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,561] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,563] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,563] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,563] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,625] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:25,653] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:25,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,665] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:25,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:25,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:25,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,738] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,738] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,751] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:25,768] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:25,768] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:25,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:25,805] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:25,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:25,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:25,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:25,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:25,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:26,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:26,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:26,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:26,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:26,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:26,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:26,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,290] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,312] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,371] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,372] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,372] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,394] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,394] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,395] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,407] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:26,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,491] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,491] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,516] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,597] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,597] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,597] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:26,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,617] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,699] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,722] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,722] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,722] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,722] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,724] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,724] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,725] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:26,818] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:26,840] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:26,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,927] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,927] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,928] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:26,930] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:26,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:26,949] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:26,949] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:26,949] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:26,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:26,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,039] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,039] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,040] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:27,042] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,150] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,151] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,151] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:27,154] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:27,155] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:27,171] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,288] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,288] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,289] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,289] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,289] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,289] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,290] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,290] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:27,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:27,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:27,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,751] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:27,760] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,816] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,838] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,841] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,841] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,841] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:27,903] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:27,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,923] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,923] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,924] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:27,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:27,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:27,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:27,997] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:27,997] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:27,997] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:28,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:28,035] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:28,053] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:28,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,119] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,119] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,119] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:28,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,150] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,150] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,151] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,164] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,165] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,165] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,222] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:28,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,260] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,303] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,341] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,351] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,351] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,352] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,389] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,390] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:28,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:28,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:28,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,464] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,466] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,466] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,467] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,545] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,590] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,590] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,590] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,593] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,596] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 04:27:28,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:28,620] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:28,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,699] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:28,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,721] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,723] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,724] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,724] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:28,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:28,743] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:28,743] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:28,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:28,831] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:28,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:29,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:29,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,060] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,105] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,133] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,134] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,134] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:29,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,180] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,180] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,180] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,180] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:29,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:29,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,253] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,255] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,344] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,422] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,423] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,466] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,469] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:29,497] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,555] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,555] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,555] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,556] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,556] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,557] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,590] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,590] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,591] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:29,619] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:29,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:29,735] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:29,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,786] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,816] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,816] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,816] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,842] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,842] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,843] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:29,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:29,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:29,928] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:29,929] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:29,929] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:29,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:29,934] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:30,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:30,037] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:30,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:30,053] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:30,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:30,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:30,118] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:30,129] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:30,131] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:30,132] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:30,132] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:30,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:30,189] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:30,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:30,226] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:30,226] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:30,226] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:30,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:30,244] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:30,245] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:30,245] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:30,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:30,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:30,311] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:30,311] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:30,312] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:30,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 04:27:30,519] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:30,576] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:30,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:30,646] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:30,646] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:30,647] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:30,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:30,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:30,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:30,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:31,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:31,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:31,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 04:27:31,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:31,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,348] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:31,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:31,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:31,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,492] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:31,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:31,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:31,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,711] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:31,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:31,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:31,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,793] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:31,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:31,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:31,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:31,965] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:32,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:32,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 04:27:32,636] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:32,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:32,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:32,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:32,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:32,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:33,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 04:27:34,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 04:27:34,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 04:27:35,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 04:27:35,179] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:35,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:35,291] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:35,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:35,294] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:35,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:35,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 04:27:37,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 04:27:37,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 04:27:37,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 04:27:37,741] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:37,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:37,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:37,821] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:37,821] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:37,821] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:38,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 04:27:38,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 04:27:38,222] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:38,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:38,330] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:38,331] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:38,332] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:38,332] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:38,566] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 04:27:38,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 04:27:38,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 04:27:39,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 04:27:39,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:39,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:39,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:39,203] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:39,203] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:39,203] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:39,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 04:27:39,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 04:27:39,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:39,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:39,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:39,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:39,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:39,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:40,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 04:27:40,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 04:27:40,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:40,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:40,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:40,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:40,494] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:40,494] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:40,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 04:27:40,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 04:27:40,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:41,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:41,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:41,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:41,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:41,062] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:41,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 04:27:41,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 04:27:41,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:41,806] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:41,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:41,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:41,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:41,828] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:42,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 04:27:42,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 04:27:42,290] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:42,420] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:27:42,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:27:42,471] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:27:42,472] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:27:42,472] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:27:42,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 04:27:44,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 04:27:44,020] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:27:44,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 04:27:44,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:44,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:44,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:44,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:44,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:44,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:44,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:44,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:45,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 04:27:45,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:45,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:45,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 04:27:46,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:46,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:46,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:46,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:46,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:46,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:46,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:46,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:46,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:46,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:46,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:46,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:46,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:47,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:47,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:47,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:47,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 04:27:47,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 04:27:47,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:47,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:47,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:47,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:47,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:47,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:47,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:48,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:48,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:48,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 04:27:48,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:48,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:48,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:48,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:48,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:48,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:48,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:48,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 04:27:48,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:48,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:48,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:48,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:48,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 04:27:48,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:48,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:48,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:48,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:48,805] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 04:27:48,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:48,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:48,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:48,877] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:48,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:48,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:49,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:49,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:49,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:49,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:49,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:49,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:49,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:49,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:49,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:49,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:49,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 04:27:49,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:49,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:49,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:50,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:50,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:50,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 04:27:50,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 04:27:50,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:50,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 04:27:50,184] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:50,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:50,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,323] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:50,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:50,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:50,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:50,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:50,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:50,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:50,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:50,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:50,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:50,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:50,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 04:27:50,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:50,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:50,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:50,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:50,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 04:27:50,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:50,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:51,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:51,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:51,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:51,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 04:27:51,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:51,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:51,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 04:27:51,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:51,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:51,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:51,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:51,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:51,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 04:27:51,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 04:27:51,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:51,375] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 04:27:52,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 04:27:52,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 04:27:52,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 04:27:52,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 04:27:52,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 04:27:52,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 04:27:53,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 04:27:53,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 04:27:53,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 04:27:53,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 04:27:53,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 04:27:53,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 04:27:53,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I1010 04:27:53.835419 140162558322432 logging_writer.py:48] [0] global_step=0, grad_norm=5.200974, loss=1.047605
I1010 04:27:53.847739 140204662740800 pytorch_submission_base.py:86] 0) loss = 1.048, grad_norm = 5.201
I1010 04:27:54.272918 140204662740800 spec.py:321] Evaluating on the training split.
[2023-10-10 04:28:48,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:48,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,363] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,363] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,363] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,364] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,364] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,364] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,365] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,365] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,365] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,365] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,365] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,365] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,366] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,366] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,366] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,367] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,367] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,367] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,368] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,368] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,368] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:48,408] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:48,409] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:48,411] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:48,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:48,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 04:28:49,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,159] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,169] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,183] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,200] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,244] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,356] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,407] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,408] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,408] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,428] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,428] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,429] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,430] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,442] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,442] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,442] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,456] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,459] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,459] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,460] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,480] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,480] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,481] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,506] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,506] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:49,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 04:28:49,704] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:49,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:49,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:49,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:49,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:49,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:49,976] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,218] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 04:28:50,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,261] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,271] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,297] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,309] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,311] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:50,315] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:50,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,451] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,453] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,453] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,477] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,522] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,522] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,523] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,527] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,529] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,529] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,544] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,544] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,545] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,547] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,556] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:50,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,586] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,587] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,587] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,590] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,593] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,593] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,593] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:50,601] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:50,601] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:50,601] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:50,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:50,721] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:50,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:50,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:50,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:50,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:50,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:51,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 04:28:51,188] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,266] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,334] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,365] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,416] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,423] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,425] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:51,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:51,467] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,469] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,470] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,470] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,539] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,540] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,540] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:51,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,602] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,602] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,603] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,607] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:51,655] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,657] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,657] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,658] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:51,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 04:28:51,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,702] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,702] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,702] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:51,704] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,705] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:51,705] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,705] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:51,705] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,705] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:51,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:51,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:51,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:51,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:51,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:52,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,136] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,351] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:52,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,417] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,466] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,484] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:52,508] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,541] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,563] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,564] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,564] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,570] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,586] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:52,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,626] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,627] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,627] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,639] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 04:28:52,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:52,676] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:52,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,725] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:52,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,768] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,769] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,769] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,832] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:52,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,841] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:52,842] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,842] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,842] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,844] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,845] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,845] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:52,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,902] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,903] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,903] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:52,918] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:52,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:52,919] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:52,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:52,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:53,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:53,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 04:28:53,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:53,670] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:53,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:53,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,739] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:53,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:53,832] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:53,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:53,857] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:53,857] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:53,858] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:53,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:53,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:53,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:53,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:53,904] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:53,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:53,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:53,925] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:53,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:53,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:53,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:53,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 04:28:53,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:53,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:54,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:54,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:54,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,068] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,080] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,099] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,100] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,149] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 04:28:54,176] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:54,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,200] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:54,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,226] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,228] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,228] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,228] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,260] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,260] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,260] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,261] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,268] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,268] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,268] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 04:28:54,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,302] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:54,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,306] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,306] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,306] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:54,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:54,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,421] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,468] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:54,493] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,499] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,499] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,499] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,574] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,577] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,578] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,580] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,584] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,584] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:54,655] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,657] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,658] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:54,658] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,662] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,662] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,663] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:54,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 04:28:54,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,743] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:54,772] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:54,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:54,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:54,893] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:54,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:54,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,928] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:54,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:54,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,947] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,947] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,947] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:54,982] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:54,984] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:54,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:54,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:54,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:55,007] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:55,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:55,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:55,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:55,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,123] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,144] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,144] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,145] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:55,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,188] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,199] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,212] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,216] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,216] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,269] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,295] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,295] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,322] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,372] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,374] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,374] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,375] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,375] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,376] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,393] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,393] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,393] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,445] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,533] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:55,570] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,573] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,573] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:55,574] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,575] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,579] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:55,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,624] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,624] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,624] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:55,705] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,707] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,757] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:55,775] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:55,799] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:55,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:55,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:55,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,878] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,878] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,878] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,921] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:55,956] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:55,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:55,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:55,979] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:55,979] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:55,980] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:55,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,005] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 04:28:56,006] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,006] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,007] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:56,012] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:56,033] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:56,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:56,134] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 04:28:56,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:56,181] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,206] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,216] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,216] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,217] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,267] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,268] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,268] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:56,271] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 04:28:56,314] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,350] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,350] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,351] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,394] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,418] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,418] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,418] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,450] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,483] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,484] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,484] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,485] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,495] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,496] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,496] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,496] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,496] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,496] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,498] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,543] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,573] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,575] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,575] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,576] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 04:28:56,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,630] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,667] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,669] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:56,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:56,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,714] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:56,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 04:28:56,717] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,738] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,739] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 04:28:56,839] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,876] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,876] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,877] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:56,891] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:56,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:56,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:56,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,915] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,915] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:56,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:56,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:56,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:56,979] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:56,980] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:56,980] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,011] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,012] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,012] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:57,104] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:57,130] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,131] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,132] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:57,132] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,133] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,134] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,134] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:57,139] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 04:28:57,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:57,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 04:28:57,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:57,308] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:57,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,320] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,321] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,322] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,322] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,359] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,368] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:57,378] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,420] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,420] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,421] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 04:28:57,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 04:28:57,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,539] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,545] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,548] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,561] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,563] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,563] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,563] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,592] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,593] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,594] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,594] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,623] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,623] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,624] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:28:57,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,760] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,761] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,762] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,775] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,794] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:28:57,861] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:57,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:57,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 04:28:57,916] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:57,918] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:57,919] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:57,919] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:57,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:57,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:58,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:58,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:58,027] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:58,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:58,028] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:58,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 04:28:58,034] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:58,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:58,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:58,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:58,146] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:58,148] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:58,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:58,149] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:58,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:58,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:58,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:58,188] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:58,188] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:58,189] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:58,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:58,223] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:58,223] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:58,223] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:58,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:58,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:58,232] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:58,232] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:58,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:58,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,347] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:58,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:58,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,391] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:58,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:58,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:28:58,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 04:28:58,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,513] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:58,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,627] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 04:28:58,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 04:28:58,711] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 04:28:58,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,757] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:58,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,823] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:58,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:28:58,829] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 04:28:58,906] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:58,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:58,935] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:58,935] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:58,936] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 04:28:59,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 04:28:59,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 04:28:59,251] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:28:59,432] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:28:59,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:28:59,491] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:28:59,491] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:28:59,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:28:59,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 04:28:59,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 04:28:59,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:29:00,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:29:00,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:29:00,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:29:00,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:29:00,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:29:00,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 04:29:00,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 04:29:00,509] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 04:29:00,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 04:29:00,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 04:29:00,889] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 04:29:00,890] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 04:29:00,891] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 04:29:01,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 04:29:02,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 04:29:02,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:29:32.470761 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:30:35.317364 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:31:40.364853 140204662740800 submission_runner.py:381] Time since start: 312.99s, 	Step: 1, 	{'train/ssim': 0.16792333126068115, 'train/loss': 1.030285154070173, 'validation/ssim': 0.16574210025785208, 'validation/loss': 1.0420777809774198, 'validation/num_examples': 3554, 'test/ssim': 0.18604843365483628, 'test/loss': 1.0386019666512496, 'test/num_examples': 3581, 'score': 86.47392249107361, 'total_duration': 312.9899561405182, 'accumulated_submission_time': 86.47392249107361, 'accumulated_eval_time': 226.091970205307, 'accumulated_logging_time': 0}
I1010 04:31:40.386194 140142937356032 logging_writer.py:48] [1] accumulated_eval_time=226.091970, accumulated_logging_time=0, accumulated_submission_time=86.473922, global_step=1, preemption_count=0, score=86.473922, test/loss=1.038602, test/num_examples=3581, test/ssim=0.186048, total_duration=312.989956, train/loss=1.030285, train/ssim=0.167923, validation/loss=1.042078, validation/num_examples=3554, validation/ssim=0.165742
I1010 04:31:40.883336 139627708704576 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.883334 139990339839808 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.883417 140412964316992 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.883419 139936525821760 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.883437 139846818621248 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.883343 139791811921728 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.883746 139690322835264 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:40.884303 140204662740800 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 04:31:41.053581 140142928963328 logging_writer.py:48] [1] global_step=1, grad_norm=4.342120, loss=0.998687
I1010 04:31:41.061080 140204662740800 pytorch_submission_base.py:86] 1) loss = 0.999, grad_norm = 4.342
I1010 04:31:41.123357 140142937356032 logging_writer.py:48] [2] global_step=2, grad_norm=5.351116, loss=1.007467
I1010 04:31:41.127348 140204662740800 pytorch_submission_base.py:86] 2) loss = 1.007, grad_norm = 5.351
I1010 04:31:41.204257 140142928963328 logging_writer.py:48] [3] global_step=3, grad_norm=5.215786, loss=1.045463
I1010 04:31:41.210043 140204662740800 pytorch_submission_base.py:86] 3) loss = 1.045, grad_norm = 5.216
I1010 04:31:41.296922 140142937356032 logging_writer.py:48] [4] global_step=4, grad_norm=4.961077, loss=0.973518
I1010 04:31:41.303028 140204662740800 pytorch_submission_base.py:86] 4) loss = 0.974, grad_norm = 4.961
I1010 04:31:41.364577 140142928963328 logging_writer.py:48] [5] global_step=5, grad_norm=4.174770, loss=1.118503
I1010 04:31:41.370737 140204662740800 pytorch_submission_base.py:86] 5) loss = 1.119, grad_norm = 4.175
I1010 04:31:41.444408 140142937356032 logging_writer.py:48] [6] global_step=6, grad_norm=4.835948, loss=0.962226
I1010 04:31:41.450164 140204662740800 pytorch_submission_base.py:86] 6) loss = 0.962, grad_norm = 4.836
I1010 04:31:41.529786 140142928963328 logging_writer.py:48] [7] global_step=7, grad_norm=4.428434, loss=0.919492
I1010 04:31:41.535296 140204662740800 pytorch_submission_base.py:86] 7) loss = 0.919, grad_norm = 4.428
I1010 04:31:41.607367 140142937356032 logging_writer.py:48] [8] global_step=8, grad_norm=4.730531, loss=0.916428
I1010 04:31:41.614336 140204662740800 pytorch_submission_base.py:86] 8) loss = 0.916, grad_norm = 4.731
I1010 04:31:41.693747 140142928963328 logging_writer.py:48] [9] global_step=9, grad_norm=4.177740, loss=0.863882
I1010 04:31:41.698967 140204662740800 pytorch_submission_base.py:86] 9) loss = 0.864, grad_norm = 4.178
I1010 04:31:41.785387 140142937356032 logging_writer.py:48] [10] global_step=10, grad_norm=3.912259, loss=0.842682
I1010 04:31:41.789318 140204662740800 pytorch_submission_base.py:86] 10) loss = 0.843, grad_norm = 3.912
I1010 04:31:41.863100 140142928963328 logging_writer.py:48] [11] global_step=11, grad_norm=3.735568, loss=0.871240
I1010 04:31:41.868442 140204662740800 pytorch_submission_base.py:86] 11) loss = 0.871, grad_norm = 3.736
I1010 04:31:41.949162 140142937356032 logging_writer.py:48] [12] global_step=12, grad_norm=3.410396, loss=0.802352
I1010 04:31:41.956358 140204662740800 pytorch_submission_base.py:86] 12) loss = 0.802, grad_norm = 3.410
I1010 04:31:42.043134 140142928963328 logging_writer.py:48] [13] global_step=13, grad_norm=2.891013, loss=0.716027
I1010 04:31:42.051060 140204662740800 pytorch_submission_base.py:86] 13) loss = 0.716, grad_norm = 2.891
I1010 04:31:42.131154 140142937356032 logging_writer.py:48] [14] global_step=14, grad_norm=2.189776, loss=0.731282
I1010 04:31:42.137602 140204662740800 pytorch_submission_base.py:86] 14) loss = 0.731, grad_norm = 2.190
I1010 04:31:42.267913 140142928963328 logging_writer.py:48] [15] global_step=15, grad_norm=1.635518, loss=0.641280
I1010 04:31:42.275164 140204662740800 pytorch_submission_base.py:86] 15) loss = 0.641, grad_norm = 1.636
I1010 04:31:42.565375 140142937356032 logging_writer.py:48] [16] global_step=16, grad_norm=1.369325, loss=0.653891
I1010 04:31:42.572334 140204662740800 pytorch_submission_base.py:86] 16) loss = 0.654, grad_norm = 1.369
I1010 04:31:42.826596 140142928963328 logging_writer.py:48] [17] global_step=17, grad_norm=1.510952, loss=0.641917
I1010 04:31:42.830492 140204662740800 pytorch_submission_base.py:86] 17) loss = 0.642, grad_norm = 1.511
I1010 04:31:43.064882 140142937356032 logging_writer.py:48] [18] global_step=18, grad_norm=1.820419, loss=0.727813
I1010 04:31:43.071154 140204662740800 pytorch_submission_base.py:86] 18) loss = 0.728, grad_norm = 1.820
I1010 04:31:43.371182 140142928963328 logging_writer.py:48] [19] global_step=19, grad_norm=1.941095, loss=0.667094
I1010 04:31:43.378657 140204662740800 pytorch_submission_base.py:86] 19) loss = 0.667, grad_norm = 1.941
I1010 04:31:43.650223 140142937356032 logging_writer.py:48] [20] global_step=20, grad_norm=2.264918, loss=0.663336
I1010 04:31:43.656237 140204662740800 pytorch_submission_base.py:86] 20) loss = 0.663, grad_norm = 2.265
I1010 04:31:43.965772 140142928963328 logging_writer.py:48] [21] global_step=21, grad_norm=2.263929, loss=0.730248
I1010 04:31:43.969773 140204662740800 pytorch_submission_base.py:86] 21) loss = 0.730, grad_norm = 2.264
I1010 04:31:44.208337 140142937356032 logging_writer.py:48] [22] global_step=22, grad_norm=2.330612, loss=0.766490
I1010 04:31:44.214828 140204662740800 pytorch_submission_base.py:86] 22) loss = 0.766, grad_norm = 2.331
I1010 04:31:44.527200 140142928963328 logging_writer.py:48] [23] global_step=23, grad_norm=2.494526, loss=0.663841
I1010 04:31:44.533659 140204662740800 pytorch_submission_base.py:86] 23) loss = 0.664, grad_norm = 2.495
I1010 04:31:44.738615 140142937356032 logging_writer.py:48] [24] global_step=24, grad_norm=2.548435, loss=0.635178
I1010 04:31:44.744714 140204662740800 pytorch_submission_base.py:86] 24) loss = 0.635, grad_norm = 2.548
I1010 04:31:45.054094 140142928963328 logging_writer.py:48] [25] global_step=25, grad_norm=2.545217, loss=0.726988
I1010 04:31:45.059960 140204662740800 pytorch_submission_base.py:86] 25) loss = 0.727, grad_norm = 2.545
I1010 04:31:45.251122 140142937356032 logging_writer.py:48] [26] global_step=26, grad_norm=2.614539, loss=0.619781
I1010 04:31:45.257962 140204662740800 pytorch_submission_base.py:86] 26) loss = 0.620, grad_norm = 2.615
I1010 04:31:45.526881 140142928963328 logging_writer.py:48] [27] global_step=27, grad_norm=2.603157, loss=0.645450
I1010 04:31:45.530544 140204662740800 pytorch_submission_base.py:86] 27) loss = 0.645, grad_norm = 2.603
I1010 04:31:45.830952 140142937356032 logging_writer.py:48] [28] global_step=28, grad_norm=2.585233, loss=0.701240
I1010 04:31:45.835495 140204662740800 pytorch_submission_base.py:86] 28) loss = 0.701, grad_norm = 2.585
I1010 04:31:46.101100 140142928963328 logging_writer.py:48] [29] global_step=29, grad_norm=2.442819, loss=0.626174
I1010 04:31:46.105205 140204662740800 pytorch_submission_base.py:86] 29) loss = 0.626, grad_norm = 2.443
I1010 04:31:46.343030 140142937356032 logging_writer.py:48] [30] global_step=30, grad_norm=2.446642, loss=0.588274
I1010 04:31:46.346736 140204662740800 pytorch_submission_base.py:86] 30) loss = 0.588, grad_norm = 2.447
I1010 04:31:46.617559 140142928963328 logging_writer.py:48] [31] global_step=31, grad_norm=2.345779, loss=0.622921
I1010 04:31:46.621509 140204662740800 pytorch_submission_base.py:86] 31) loss = 0.623, grad_norm = 2.346
I1010 04:31:46.909296 140142937356032 logging_writer.py:48] [32] global_step=32, grad_norm=2.013390, loss=0.504568
I1010 04:31:46.912889 140204662740800 pytorch_submission_base.py:86] 32) loss = 0.505, grad_norm = 2.013
I1010 04:31:47.158540 140142928963328 logging_writer.py:48] [33] global_step=33, grad_norm=1.687209, loss=0.639115
I1010 04:31:47.169128 140204662740800 pytorch_submission_base.py:86] 33) loss = 0.639, grad_norm = 1.687
I1010 04:31:47.416643 140142937356032 logging_writer.py:48] [34] global_step=34, grad_norm=1.481556, loss=0.573893
I1010 04:31:47.426161 140204662740800 pytorch_submission_base.py:86] 34) loss = 0.574, grad_norm = 1.482
I1010 04:31:47.747728 140142928963328 logging_writer.py:48] [35] global_step=35, grad_norm=1.212660, loss=0.534587
I1010 04:31:47.753566 140204662740800 pytorch_submission_base.py:86] 35) loss = 0.535, grad_norm = 1.213
I1010 04:31:48.034800 140142937356032 logging_writer.py:48] [36] global_step=36, grad_norm=0.829305, loss=0.476540
I1010 04:31:48.041481 140204662740800 pytorch_submission_base.py:86] 36) loss = 0.477, grad_norm = 0.829
I1010 04:31:48.283294 140142928963328 logging_writer.py:48] [37] global_step=37, grad_norm=0.712735, loss=0.459466
I1010 04:31:48.290359 140204662740800 pytorch_submission_base.py:86] 37) loss = 0.459, grad_norm = 0.713
I1010 04:31:48.576158 140142937356032 logging_writer.py:48] [38] global_step=38, grad_norm=0.896088, loss=0.399033
I1010 04:31:48.580981 140204662740800 pytorch_submission_base.py:86] 38) loss = 0.399, grad_norm = 0.896
I1010 04:31:48.866847 140142928963328 logging_writer.py:48] [39] global_step=39, grad_norm=1.162247, loss=0.427736
I1010 04:31:48.872809 140204662740800 pytorch_submission_base.py:86] 39) loss = 0.428, grad_norm = 1.162
I1010 04:31:49.199676 140142937356032 logging_writer.py:48] [40] global_step=40, grad_norm=1.315188, loss=0.424989
I1010 04:31:49.204550 140204662740800 pytorch_submission_base.py:86] 40) loss = 0.425, grad_norm = 1.315
I1010 04:31:49.484354 140142928963328 logging_writer.py:48] [41] global_step=41, grad_norm=1.355810, loss=0.444657
I1010 04:31:49.489886 140204662740800 pytorch_submission_base.py:86] 41) loss = 0.445, grad_norm = 1.356
I1010 04:31:49.797819 140142937356032 logging_writer.py:48] [42] global_step=42, grad_norm=1.356671, loss=0.471662
I1010 04:31:49.803714 140204662740800 pytorch_submission_base.py:86] 42) loss = 0.472, grad_norm = 1.357
I1010 04:31:50.104471 140142928963328 logging_writer.py:48] [43] global_step=43, grad_norm=1.551687, loss=0.434330
I1010 04:31:50.108511 140204662740800 pytorch_submission_base.py:86] 43) loss = 0.434, grad_norm = 1.552
I1010 04:31:50.370808 140142937356032 logging_writer.py:48] [44] global_step=44, grad_norm=1.476322, loss=0.481931
I1010 04:31:50.374691 140204662740800 pytorch_submission_base.py:86] 44) loss = 0.482, grad_norm = 1.476
I1010 04:31:50.669537 140142928963328 logging_writer.py:48] [45] global_step=45, grad_norm=1.313498, loss=0.573676
I1010 04:31:50.673437 140204662740800 pytorch_submission_base.py:86] 45) loss = 0.574, grad_norm = 1.313
I1010 04:31:50.923505 140142937356032 logging_writer.py:48] [46] global_step=46, grad_norm=1.432649, loss=0.544971
I1010 04:31:50.927320 140204662740800 pytorch_submission_base.py:86] 46) loss = 0.545, grad_norm = 1.433
I1010 04:31:51.199674 140142928963328 logging_writer.py:48] [47] global_step=47, grad_norm=1.392221, loss=0.542058
I1010 04:31:51.203589 140204662740800 pytorch_submission_base.py:86] 47) loss = 0.542, grad_norm = 1.392
I1010 04:31:51.481246 140142937356032 logging_writer.py:48] [48] global_step=48, grad_norm=1.614233, loss=0.485117
I1010 04:31:51.485522 140204662740800 pytorch_submission_base.py:86] 48) loss = 0.485, grad_norm = 1.614
I1010 04:31:51.733125 140142928963328 logging_writer.py:48] [49] global_step=49, grad_norm=1.362755, loss=0.527836
I1010 04:31:51.738927 140204662740800 pytorch_submission_base.py:86] 49) loss = 0.528, grad_norm = 1.363
I1010 04:31:52.056687 140142937356032 logging_writer.py:48] [50] global_step=50, grad_norm=1.277713, loss=0.551111
I1010 04:31:52.060840 140204662740800 pytorch_submission_base.py:86] 50) loss = 0.551, grad_norm = 1.278
I1010 04:31:52.336753 140142928963328 logging_writer.py:48] [51] global_step=51, grad_norm=1.274401, loss=0.498700
I1010 04:31:52.341709 140204662740800 pytorch_submission_base.py:86] 51) loss = 0.499, grad_norm = 1.274
I1010 04:31:52.572773 140142937356032 logging_writer.py:48] [52] global_step=52, grad_norm=1.142316, loss=0.539231
I1010 04:31:52.579204 140204662740800 pytorch_submission_base.py:86] 52) loss = 0.539, grad_norm = 1.142
I1010 04:31:52.838646 140142928963328 logging_writer.py:48] [53] global_step=53, grad_norm=1.204131, loss=0.474522
I1010 04:31:52.844286 140204662740800 pytorch_submission_base.py:86] 53) loss = 0.475, grad_norm = 1.204
I1010 04:31:53.113698 140142937356032 logging_writer.py:48] [54] global_step=54, grad_norm=1.117955, loss=0.445529
I1010 04:31:53.120679 140204662740800 pytorch_submission_base.py:86] 54) loss = 0.446, grad_norm = 1.118
I1010 04:31:53.439711 140142928963328 logging_writer.py:48] [55] global_step=55, grad_norm=1.150810, loss=0.457253
I1010 04:31:53.444734 140204662740800 pytorch_submission_base.py:86] 55) loss = 0.457, grad_norm = 1.151
I1010 04:31:53.723451 140142937356032 logging_writer.py:48] [56] global_step=56, grad_norm=0.947349, loss=0.478865
I1010 04:31:53.729756 140204662740800 pytorch_submission_base.py:86] 56) loss = 0.479, grad_norm = 0.947
I1010 04:31:54.029021 140142928963328 logging_writer.py:48] [57] global_step=57, grad_norm=0.970488, loss=0.431357
I1010 04:31:54.035183 140204662740800 pytorch_submission_base.py:86] 57) loss = 0.431, grad_norm = 0.970
I1010 04:31:54.302898 140142937356032 logging_writer.py:48] [58] global_step=58, grad_norm=0.880067, loss=0.466150
I1010 04:31:54.310050 140204662740800 pytorch_submission_base.py:86] 58) loss = 0.466, grad_norm = 0.880
I1010 04:31:54.559528 140142928963328 logging_writer.py:48] [59] global_step=59, grad_norm=0.791737, loss=0.419062
I1010 04:31:54.566112 140204662740800 pytorch_submission_base.py:86] 59) loss = 0.419, grad_norm = 0.792
I1010 04:31:54.830258 140142937356032 logging_writer.py:48] [60] global_step=60, grad_norm=0.951027, loss=0.391998
I1010 04:31:54.836769 140204662740800 pytorch_submission_base.py:86] 60) loss = 0.392, grad_norm = 0.951
I1010 04:31:55.135813 140142928963328 logging_writer.py:48] [61] global_step=61, grad_norm=0.933762, loss=0.404632
I1010 04:31:55.142801 140204662740800 pytorch_submission_base.py:86] 61) loss = 0.405, grad_norm = 0.934
I1010 04:31:55.344479 140142937356032 logging_writer.py:48] [62] global_step=62, grad_norm=0.758299, loss=0.453092
I1010 04:31:55.350131 140204662740800 pytorch_submission_base.py:86] 62) loss = 0.453, grad_norm = 0.758
I1010 04:31:55.624165 140142928963328 logging_writer.py:48] [63] global_step=63, grad_norm=0.921310, loss=0.466323
I1010 04:31:55.628299 140204662740800 pytorch_submission_base.py:86] 63) loss = 0.466, grad_norm = 0.921
I1010 04:31:55.884218 140142937356032 logging_writer.py:48] [64] global_step=64, grad_norm=0.762654, loss=0.422639
I1010 04:31:55.887691 140204662740800 pytorch_submission_base.py:86] 64) loss = 0.423, grad_norm = 0.763
I1010 04:31:56.111770 140142928963328 logging_writer.py:48] [65] global_step=65, grad_norm=0.846068, loss=0.382060
I1010 04:31:56.117609 140204662740800 pytorch_submission_base.py:86] 65) loss = 0.382, grad_norm = 0.846
I1010 04:31:56.382496 140142937356032 logging_writer.py:48] [66] global_step=66, grad_norm=0.707797, loss=0.465070
I1010 04:31:56.388111 140204662740800 pytorch_submission_base.py:86] 66) loss = 0.465, grad_norm = 0.708
I1010 04:31:56.692237 140142928963328 logging_writer.py:48] [67] global_step=67, grad_norm=0.838975, loss=0.349702
I1010 04:31:56.696813 140204662740800 pytorch_submission_base.py:86] 67) loss = 0.350, grad_norm = 0.839
I1010 04:31:56.943579 140142937356032 logging_writer.py:48] [68] global_step=68, grad_norm=0.751601, loss=0.440767
I1010 04:31:56.948509 140204662740800 pytorch_submission_base.py:86] 68) loss = 0.441, grad_norm = 0.752
I1010 04:31:57.203447 140142928963328 logging_writer.py:48] [69] global_step=69, grad_norm=0.779414, loss=0.367054
I1010 04:31:57.209811 140204662740800 pytorch_submission_base.py:86] 69) loss = 0.367, grad_norm = 0.779
I1010 04:31:57.460209 140142937356032 logging_writer.py:48] [70] global_step=70, grad_norm=0.816999, loss=0.378921
I1010 04:31:57.464205 140204662740800 pytorch_submission_base.py:86] 70) loss = 0.379, grad_norm = 0.817
I1010 04:31:57.710197 140142928963328 logging_writer.py:48] [71] global_step=71, grad_norm=0.741275, loss=0.394131
I1010 04:31:57.713957 140204662740800 pytorch_submission_base.py:86] 71) loss = 0.394, grad_norm = 0.741
I1010 04:31:57.995405 140142937356032 logging_writer.py:48] [72] global_step=72, grad_norm=0.747435, loss=0.318504
I1010 04:31:58.000450 140204662740800 pytorch_submission_base.py:86] 72) loss = 0.319, grad_norm = 0.747
I1010 04:31:58.305745 140142928963328 logging_writer.py:48] [73] global_step=73, grad_norm=0.750126, loss=0.332101
I1010 04:31:58.312827 140204662740800 pytorch_submission_base.py:86] 73) loss = 0.332, grad_norm = 0.750
I1010 04:31:58.605237 140142937356032 logging_writer.py:48] [74] global_step=74, grad_norm=0.757025, loss=0.398540
I1010 04:31:58.611187 140204662740800 pytorch_submission_base.py:86] 74) loss = 0.399, grad_norm = 0.757
I1010 04:31:58.856625 140142928963328 logging_writer.py:48] [75] global_step=75, grad_norm=0.724380, loss=0.363965
I1010 04:31:58.862601 140204662740800 pytorch_submission_base.py:86] 75) loss = 0.364, grad_norm = 0.724
I1010 04:31:59.111725 140142937356032 logging_writer.py:48] [76] global_step=76, grad_norm=0.687358, loss=0.328558
I1010 04:31:59.116946 140204662740800 pytorch_submission_base.py:86] 76) loss = 0.329, grad_norm = 0.687
I1010 04:31:59.372873 140142928963328 logging_writer.py:48] [77] global_step=77, grad_norm=0.647529, loss=0.278554
I1010 04:31:59.379081 140204662740800 pytorch_submission_base.py:86] 77) loss = 0.279, grad_norm = 0.648
I1010 04:31:59.648089 140142937356032 logging_writer.py:48] [78] global_step=78, grad_norm=0.679646, loss=0.284974
I1010 04:31:59.653744 140204662740800 pytorch_submission_base.py:86] 78) loss = 0.285, grad_norm = 0.680
I1010 04:31:59.899139 140142928963328 logging_writer.py:48] [79] global_step=79, grad_norm=0.623344, loss=0.306598
I1010 04:31:59.904987 140204662740800 pytorch_submission_base.py:86] 79) loss = 0.307, grad_norm = 0.623
I1010 04:32:00.128474 140142937356032 logging_writer.py:48] [80] global_step=80, grad_norm=0.666750, loss=0.327309
I1010 04:32:00.132160 140204662740800 pytorch_submission_base.py:86] 80) loss = 0.327, grad_norm = 0.667
I1010 04:32:00.402744 140142928963328 logging_writer.py:48] [81] global_step=81, grad_norm=0.631129, loss=0.320524
I1010 04:32:00.406775 140204662740800 pytorch_submission_base.py:86] 81) loss = 0.321, grad_norm = 0.631
I1010 04:32:00.697636 140142937356032 logging_writer.py:48] [82] global_step=82, grad_norm=0.596534, loss=0.364170
I1010 04:32:00.701223 140204662740800 pytorch_submission_base.py:86] 82) loss = 0.364, grad_norm = 0.597
I1010 04:32:00.949206 140142928963328 logging_writer.py:48] [83] global_step=83, grad_norm=0.627910, loss=0.299972
I1010 04:32:00.956973 140204662740800 pytorch_submission_base.py:86] 83) loss = 0.300, grad_norm = 0.628
I1010 04:32:01.352510 140142937356032 logging_writer.py:48] [84] global_step=84, grad_norm=0.581311, loss=0.361421
I1010 04:32:01.362395 140204662740800 pytorch_submission_base.py:86] 84) loss = 0.361, grad_norm = 0.581
I1010 04:32:01.665431 140142928963328 logging_writer.py:48] [85] global_step=85, grad_norm=0.693158, loss=0.321665
I1010 04:32:01.672100 140204662740800 pytorch_submission_base.py:86] 85) loss = 0.322, grad_norm = 0.693
I1010 04:32:01.951692 140142937356032 logging_writer.py:48] [86] global_step=86, grad_norm=0.637613, loss=0.306250
I1010 04:32:01.955576 140204662740800 pytorch_submission_base.py:86] 86) loss = 0.306, grad_norm = 0.638
I1010 04:32:02.182579 140142928963328 logging_writer.py:48] [87] global_step=87, grad_norm=0.650898, loss=0.379712
I1010 04:32:02.185966 140204662740800 pytorch_submission_base.py:86] 87) loss = 0.380, grad_norm = 0.651
I1010 04:32:02.463550 140142937356032 logging_writer.py:48] [88] global_step=88, grad_norm=0.665640, loss=0.423372
I1010 04:32:02.467423 140204662740800 pytorch_submission_base.py:86] 88) loss = 0.423, grad_norm = 0.666
I1010 04:32:02.773901 140142928963328 logging_writer.py:48] [89] global_step=89, grad_norm=0.766893, loss=0.395316
I1010 04:32:02.777646 140204662740800 pytorch_submission_base.py:86] 89) loss = 0.395, grad_norm = 0.767
I1010 04:32:03.083071 140142937356032 logging_writer.py:48] [90] global_step=90, grad_norm=0.578907, loss=0.387120
I1010 04:32:03.086942 140204662740800 pytorch_submission_base.py:86] 90) loss = 0.387, grad_norm = 0.579
I1010 04:32:03.383548 140142928963328 logging_writer.py:48] [91] global_step=91, grad_norm=0.599459, loss=0.320004
I1010 04:32:03.389807 140204662740800 pytorch_submission_base.py:86] 91) loss = 0.320, grad_norm = 0.599
I1010 04:32:03.635537 140142937356032 logging_writer.py:48] [92] global_step=92, grad_norm=0.569937, loss=0.345918
I1010 04:32:03.640554 140204662740800 pytorch_submission_base.py:86] 92) loss = 0.346, grad_norm = 0.570
I1010 04:32:03.882591 140142928963328 logging_writer.py:48] [93] global_step=93, grad_norm=0.530888, loss=0.314489
I1010 04:32:03.886192 140204662740800 pytorch_submission_base.py:86] 93) loss = 0.314, grad_norm = 0.531
I1010 04:32:04.135778 140142937356032 logging_writer.py:48] [94] global_step=94, grad_norm=0.506011, loss=0.267196
I1010 04:32:04.139283 140204662740800 pytorch_submission_base.py:86] 94) loss = 0.267, grad_norm = 0.506
I1010 04:32:04.416270 140142928963328 logging_writer.py:48] [95] global_step=95, grad_norm=0.558454, loss=0.342984
I1010 04:32:04.419748 140204662740800 pytorch_submission_base.py:86] 95) loss = 0.343, grad_norm = 0.558
I1010 04:32:04.698759 140142937356032 logging_writer.py:48] [96] global_step=96, grad_norm=0.489708, loss=0.318687
I1010 04:32:04.702373 140204662740800 pytorch_submission_base.py:86] 96) loss = 0.319, grad_norm = 0.490
I1010 04:32:04.967343 140142928963328 logging_writer.py:48] [97] global_step=97, grad_norm=0.420641, loss=0.318928
I1010 04:32:04.974736 140204662740800 pytorch_submission_base.py:86] 97) loss = 0.319, grad_norm = 0.421
I1010 04:32:05.256991 140142937356032 logging_writer.py:48] [98] global_step=98, grad_norm=0.482685, loss=0.320109
I1010 04:32:05.261534 140204662740800 pytorch_submission_base.py:86] 98) loss = 0.320, grad_norm = 0.483
I1010 04:32:05.538266 140142928963328 logging_writer.py:48] [99] global_step=99, grad_norm=0.534149, loss=0.326973
I1010 04:32:05.542958 140204662740800 pytorch_submission_base.py:86] 99) loss = 0.327, grad_norm = 0.534
I1010 04:32:05.754988 140142937356032 logging_writer.py:48] [100] global_step=100, grad_norm=0.451840, loss=0.255783
I1010 04:32:05.760923 140204662740800 pytorch_submission_base.py:86] 100) loss = 0.256, grad_norm = 0.452
I1010 04:33:00.867876 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:33:03.043025 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:33:05.089426 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:33:07.126536 140204662740800 submission_runner.py:381] Time since start: 399.75s, 	Step: 305, 	{'train/ssim': 0.6801228523254395, 'train/loss': 0.32252434321812223, 'validation/ssim': 0.6584233599025746, 'validation/loss': 0.34896474482976925, 'validation/num_examples': 3554, 'test/ssim': 0.6772323220643675, 'test/loss': 0.3500391947627234, 'test/num_examples': 3581, 'score': 165.85433745384216, 'total_duration': 399.7517354488373, 'accumulated_submission_time': 165.85433745384216, 'accumulated_eval_time': 232.35104131698608, 'accumulated_logging_time': 0.030467987060546875}
I1010 04:33:07.148021 140142928963328 logging_writer.py:48] [305] accumulated_eval_time=232.351041, accumulated_logging_time=0.030468, accumulated_submission_time=165.854337, global_step=305, preemption_count=0, score=165.854337, test/loss=0.350039, test/num_examples=3581, test/ssim=0.677232, total_duration=399.751735, train/loss=0.322524, train/ssim=0.680123, validation/loss=0.348965, validation/num_examples=3554, validation/ssim=0.658423
I1010 04:34:20.644816 140142937356032 logging_writer.py:48] [500] global_step=500, grad_norm=0.071388, loss=0.328941
I1010 04:34:20.650644 140204662740800 pytorch_submission_base.py:86] 500) loss = 0.329, grad_norm = 0.071
I1010 04:34:27.650365 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:34:29.712376 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:34:31.837533 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:34:33.853825 140204662740800 submission_runner.py:381] Time since start: 486.48s, 	Step: 516, 	{'train/ssim': 0.6987625530787877, 'train/loss': 0.30547291891915457, 'validation/ssim': 0.6788700342044176, 'validation/loss': 0.3299729192129467, 'validation/num_examples': 3554, 'test/ssim': 0.6977442251640603, 'test/loss': 0.3311899159081786, 'test/num_examples': 3581, 'score': 245.3850381374359, 'total_duration': 486.4789927005768, 'accumulated_submission_time': 245.3850381374359, 'accumulated_eval_time': 238.5548391342163, 'accumulated_logging_time': 0.06409859657287598}
I1010 04:34:33.876240 140142928963328 logging_writer.py:48] [516] accumulated_eval_time=238.554839, accumulated_logging_time=0.064099, accumulated_submission_time=245.385038, global_step=516, preemption_count=0, score=245.385038, test/loss=0.331190, test/num_examples=3581, test/ssim=0.697744, total_duration=486.478993, train/loss=0.305473, train/ssim=0.698763, validation/loss=0.329973, validation/num_examples=3554, validation/ssim=0.678870
I1010 04:35:54.312886 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:35:56.309149 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:35:58.398818 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:36:00.450902 140204662740800 submission_runner.py:381] Time since start: 573.07s, 	Step: 735, 	{'train/ssim': 0.7080442564828056, 'train/loss': 0.2958430562700544, 'validation/ssim': 0.6887974349984173, 'validation/loss': 0.31898799932734245, 'validation/num_examples': 3554, 'test/ssim': 0.7071918742146048, 'test/loss': 0.32055739056827703, 'test/num_examples': 3581, 'score': 324.8076493740082, 'total_duration': 573.0747888088226, 'accumulated_submission_time': 324.8076493740082, 'accumulated_eval_time': 244.6919298171997, 'accumulated_logging_time': 0.10406279563903809}
I1010 04:36:00.476669 140142937356032 logging_writer.py:48] [735] accumulated_eval_time=244.691930, accumulated_logging_time=0.104063, accumulated_submission_time=324.807649, global_step=735, preemption_count=0, score=324.807649, test/loss=0.320557, test/num_examples=3581, test/ssim=0.707192, total_duration=573.074789, train/loss=0.295843, train/ssim=0.708044, validation/loss=0.318988, validation/num_examples=3554, validation/ssim=0.688797
I1010 04:37:21.077781 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:37:23.108138 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:37:25.386302 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:37:27.411033 140204662740800 submission_runner.py:381] Time since start: 660.04s, 	Step: 945, 	{'train/ssim': 0.7137081963675362, 'train/loss': 0.28919478825160433, 'validation/ssim': 0.6943492642533061, 'validation/loss': 0.31186134626037565, 'validation/num_examples': 3554, 'test/ssim': 0.712264626893675, 'test/loss': 0.31363207346498884, 'test/num_examples': 3581, 'score': 404.4168653488159, 'total_duration': 660.0350699424744, 'accumulated_submission_time': 404.4168653488159, 'accumulated_eval_time': 251.0242133140564, 'accumulated_logging_time': 0.14265918731689453}
I1010 04:37:27.436052 140142928963328 logging_writer.py:48] [945] accumulated_eval_time=251.024213, accumulated_logging_time=0.142659, accumulated_submission_time=404.416865, global_step=945, preemption_count=0, score=404.416865, test/loss=0.313632, test/num_examples=3581, test/ssim=0.712265, total_duration=660.035070, train/loss=0.289195, train/ssim=0.713708, validation/loss=0.311861, validation/num_examples=3554, validation/ssim=0.694349
I1010 04:37:40.294614 140142937356032 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.458018, loss=0.208302
I1010 04:37:40.301524 140204662740800 pytorch_submission_base.py:86] 1000) loss = 0.208, grad_norm = 0.458
I1010 04:38:47.870815 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:38:49.820210 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:38:51.779274 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:38:53.652166 140204662740800 submission_runner.py:381] Time since start: 746.28s, 	Step: 1251, 	{'train/ssim': 0.7215217181614467, 'train/loss': 0.2828573499407087, 'validation/ssim': 0.7010443091191263, 'validation/loss': 0.306146504708603, 'validation/num_examples': 3554, 'test/ssim': 0.7183888000471237, 'test/loss': 0.3079361860993787, 'test/num_examples': 3581, 'score': 483.8129186630249, 'total_duration': 746.2773327827454, 'accumulated_submission_time': 483.8129186630249, 'accumulated_eval_time': 256.80575919151306, 'accumulated_logging_time': 0.18888568878173828}
I1010 04:38:53.670122 140142928963328 logging_writer.py:48] [1251] accumulated_eval_time=256.805759, accumulated_logging_time=0.188886, accumulated_submission_time=483.812919, global_step=1251, preemption_count=0, score=483.812919, test/loss=0.307936, test/num_examples=3581, test/ssim=0.718389, total_duration=746.277333, train/loss=0.282857, train/ssim=0.721522, validation/loss=0.306147, validation/num_examples=3554, validation/ssim=0.701044
I1010 04:39:58.376701 140142937356032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.342312, loss=0.309225
I1010 04:39:58.381186 140204662740800 pytorch_submission_base.py:86] 1500) loss = 0.309, grad_norm = 0.342
I1010 04:40:14.318580 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:40:16.380243 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:40:18.452611 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:40:20.479956 140204662740800 submission_runner.py:381] Time since start: 833.11s, 	Step: 1558, 	{'train/ssim': 0.728050776890346, 'train/loss': 0.27642972128731863, 'validation/ssim': 0.7083639940252181, 'validation/loss': 0.2987849504409644, 'validation/num_examples': 3554, 'test/ssim': 0.7256676131405334, 'test/loss': 0.30052040609292097, 'test/num_examples': 3581, 'score': 563.4335632324219, 'total_duration': 833.105167388916, 'accumulated_submission_time': 563.4335632324219, 'accumulated_eval_time': 262.9674036502838, 'accumulated_logging_time': 0.21651482582092285}
I1010 04:40:20.495599 140142928963328 logging_writer.py:48] [1558] accumulated_eval_time=262.967404, accumulated_logging_time=0.216515, accumulated_submission_time=563.433563, global_step=1558, preemption_count=0, score=563.433563, test/loss=0.300520, test/num_examples=3581, test/ssim=0.725668, total_duration=833.105167, train/loss=0.276430, train/ssim=0.728051, validation/loss=0.298785, validation/num_examples=3554, validation/ssim=0.708364
I1010 04:41:40.994392 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:41:42.906764 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:41:44.836863 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:41:46.713976 140204662740800 submission_runner.py:381] Time since start: 919.34s, 	Step: 1866, 	{'train/ssim': 0.7198609624590192, 'train/loss': 0.28334368978227886, 'validation/ssim': 0.7011858200047482, 'validation/loss': 0.3059248615666327, 'validation/num_examples': 3554, 'test/ssim': 0.7175343419479545, 'test/loss': 0.30798537556068484, 'test/num_examples': 3581, 'score': 642.8614251613617, 'total_duration': 919.3391571044922, 'accumulated_submission_time': 642.8614251613617, 'accumulated_eval_time': 268.68727827072144, 'accumulated_logging_time': 0.24095916748046875}
I1010 04:41:46.732473 140142937356032 logging_writer.py:48] [1866] accumulated_eval_time=268.687278, accumulated_logging_time=0.240959, accumulated_submission_time=642.861425, global_step=1866, preemption_count=0, score=642.861425, test/loss=0.307985, test/num_examples=3581, test/ssim=0.717534, total_duration=919.339157, train/loss=0.283344, train/ssim=0.719861, validation/loss=0.305925, validation/num_examples=3554, validation/ssim=0.701186
I1010 04:42:21.373609 140142928963328 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.126633, loss=0.254044
I1010 04:42:21.377501 140204662740800 pytorch_submission_base.py:86] 2000) loss = 0.254, grad_norm = 0.127
I1010 04:43:07.202064 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:43:09.286686 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:43:11.392758 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:43:13.423788 140204662740800 submission_runner.py:381] Time since start: 1006.05s, 	Step: 2169, 	{'train/ssim': 0.7334461212158203, 'train/loss': 0.272248591695513, 'validation/ssim': 0.7133662664427406, 'validation/loss': 0.2948755061418472, 'validation/num_examples': 3554, 'test/ssim': 0.7304469334683049, 'test/loss': 0.29674771417289164, 'test/num_examples': 3581, 'score': 722.2912240028381, 'total_duration': 1006.0489928722382, 'accumulated_submission_time': 722.2912240028381, 'accumulated_eval_time': 274.9092185497284, 'accumulated_logging_time': 0.2700538635253906}
I1010 04:43:13.439858 140142937356032 logging_writer.py:48] [2169] accumulated_eval_time=274.909219, accumulated_logging_time=0.270054, accumulated_submission_time=722.291224, global_step=2169, preemption_count=0, score=722.291224, test/loss=0.296748, test/num_examples=3581, test/ssim=0.730447, total_duration=1006.048993, train/loss=0.272249, train/ssim=0.733446, validation/loss=0.294876, validation/num_examples=3554, validation/ssim=0.713366
I1010 04:44:33.901587 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:44:35.876738 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:44:37.837021 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:44:39.738322 140204662740800 submission_runner.py:381] Time since start: 1092.36s, 	Step: 2472, 	{'train/ssim': 0.733506270817348, 'train/loss': 0.27514166491372244, 'validation/ssim': 0.7130587206976294, 'validation/loss': 0.2987488514262099, 'validation/num_examples': 3554, 'test/ssim': 0.7298015731901005, 'test/loss': 0.3005153610199665, 'test/num_examples': 3581, 'score': 800.2384705543518, 'total_duration': 1092.3634963035583, 'accumulated_submission_time': 800.2384705543518, 'accumulated_eval_time': 280.74616861343384, 'accumulated_logging_time': 1.8322670459747314}
I1010 04:44:39.754324 140142928963328 logging_writer.py:48] [2472] accumulated_eval_time=280.746169, accumulated_logging_time=1.832267, accumulated_submission_time=800.238471, global_step=2472, preemption_count=0, score=800.238471, test/loss=0.300515, test/num_examples=3581, test/ssim=0.729802, total_duration=1092.363496, train/loss=0.275142, train/ssim=0.733506, validation/loss=0.298749, validation/num_examples=3554, validation/ssim=0.713059
I1010 04:44:45.424554 140142937356032 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.457903, loss=0.283919
I1010 04:44:45.428139 140204662740800 pytorch_submission_base.py:86] 2500) loss = 0.284, grad_norm = 0.458
I1010 04:46:00.340904 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:46:02.344962 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:46:04.303193 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:46:06.201790 140204662740800 submission_runner.py:381] Time since start: 1178.83s, 	Step: 2781, 	{'train/ssim': 0.7368245806012835, 'train/loss': 0.2693100316183908, 'validation/ssim': 0.7164774450882808, 'validation/loss': 0.2920083650794, 'validation/num_examples': 3554, 'test/ssim': 0.7337744999650936, 'test/loss': 0.29355902353567437, 'test/num_examples': 3581, 'score': 879.8399803638458, 'total_duration': 1178.8269805908203, 'accumulated_submission_time': 879.8399803638458, 'accumulated_eval_time': 286.607310295105, 'accumulated_logging_time': 1.8570406436920166}
I1010 04:46:06.217774 140142928963328 logging_writer.py:48] [2781] accumulated_eval_time=286.607310, accumulated_logging_time=1.857041, accumulated_submission_time=879.839980, global_step=2781, preemption_count=0, score=879.839980, test/loss=0.293559, test/num_examples=3581, test/ssim=0.733774, total_duration=1178.826981, train/loss=0.269310, train/ssim=0.736825, validation/loss=0.292008, validation/num_examples=3554, validation/ssim=0.716477
I1010 04:47:02.834576 140142937356032 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.163297, loss=0.291375
I1010 04:47:02.838196 140204662740800 pytorch_submission_base.py:86] 3000) loss = 0.291, grad_norm = 0.163
I1010 04:47:26.739049 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:47:28.694832 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:47:30.667020 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:47:32.569379 140204662740800 submission_runner.py:381] Time since start: 1265.19s, 	Step: 3089, 	{'train/ssim': 0.7309059415544782, 'train/loss': 0.2714355332510812, 'validation/ssim': 0.7117228167205966, 'validation/loss': 0.2934708388655388, 'validation/num_examples': 3554, 'test/ssim': 0.7289881574411826, 'test/loss': 0.29502376502068206, 'test/num_examples': 3581, 'score': 959.372216463089, 'total_duration': 1265.1945691108704, 'accumulated_submission_time': 959.372216463089, 'accumulated_eval_time': 292.4378583431244, 'accumulated_logging_time': 1.8813128471374512}
I1010 04:47:32.586245 140142928963328 logging_writer.py:48] [3089] accumulated_eval_time=292.437858, accumulated_logging_time=1.881313, accumulated_submission_time=959.372216, global_step=3089, preemption_count=0, score=959.372216, test/loss=0.295024, test/num_examples=3581, test/ssim=0.728988, total_duration=1265.194569, train/loss=0.271436, train/ssim=0.730906, validation/loss=0.293471, validation/num_examples=3554, validation/ssim=0.711723
I1010 04:48:53.191335 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:48:55.159188 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:48:57.110638 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:48:58.998237 140204662740800 submission_runner.py:381] Time since start: 1351.62s, 	Step: 3396, 	{'train/ssim': 0.7398200035095215, 'train/loss': 0.26771460260663715, 'validation/ssim': 0.7189482526290799, 'validation/loss': 0.2908463958413935, 'validation/num_examples': 3554, 'test/ssim': 0.7362282461908336, 'test/loss': 0.29240936049200644, 'test/num_examples': 3581, 'score': 1039.0017628669739, 'total_duration': 1351.6234259605408, 'accumulated_submission_time': 1039.0017628669739, 'accumulated_eval_time': 298.2449851036072, 'accumulated_logging_time': 1.9081621170043945}
I1010 04:48:59.014147 140142937356032 logging_writer.py:48] [3396] accumulated_eval_time=298.244985, accumulated_logging_time=1.908162, accumulated_submission_time=1039.001763, global_step=3396, preemption_count=0, score=1039.001763, test/loss=0.292409, test/num_examples=3581, test/ssim=0.736228, total_duration=1351.623426, train/loss=0.267715, train/ssim=0.739820, validation/loss=0.290846, validation/num_examples=3554, validation/ssim=0.718948
I1010 04:49:25.081892 140142928963328 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.345742, loss=0.313923
I1010 04:49:25.085530 140204662740800 pytorch_submission_base.py:86] 3500) loss = 0.314, grad_norm = 0.346
I1010 04:50:19.518071 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:50:21.483471 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:50:23.434539 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:50:25.326699 140204662740800 submission_runner.py:381] Time since start: 1437.95s, 	Step: 3705, 	{'train/ssim': 0.7395520210266113, 'train/loss': 0.2700499636786325, 'validation/ssim': 0.7186436607519696, 'validation/loss': 0.2934485131190208, 'validation/num_examples': 3554, 'test/ssim': 0.7358480931260472, 'test/loss': 0.29500024407244835, 'test/num_examples': 3581, 'score': 1118.5210628509521, 'total_duration': 1437.9518880844116, 'accumulated_submission_time': 1118.5210628509521, 'accumulated_eval_time': 304.05380749702454, 'accumulated_logging_time': 1.9326221942901611}
I1010 04:50:25.343094 140142937356032 logging_writer.py:48] [3705] accumulated_eval_time=304.053807, accumulated_logging_time=1.932622, accumulated_submission_time=1118.521063, global_step=3705, preemption_count=0, score=1118.521063, test/loss=0.295000, test/num_examples=3581, test/ssim=0.735848, total_duration=1437.951888, train/loss=0.270050, train/ssim=0.739552, validation/loss=0.293449, validation/num_examples=3554, validation/ssim=0.718644
I1010 04:51:41.737930 140142928963328 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.929279, loss=0.185347
I1010 04:51:41.742208 140204662740800 pytorch_submission_base.py:86] 4000) loss = 0.185, grad_norm = 0.929
I1010 04:51:45.807132 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:51:47.815617 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:51:49.795845 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:51:51.713381 140204662740800 submission_runner.py:381] Time since start: 1524.34s, 	Step: 4015, 	{'train/ssim': 0.7385118348257882, 'train/loss': 0.2696098600115095, 'validation/ssim': 0.7181648593574141, 'validation/loss': 0.292431214718451, 'validation/num_examples': 3554, 'test/ssim': 0.7351044902785535, 'test/loss': 0.2942182918528344, 'test/num_examples': 3581, 'score': 1198.004730939865, 'total_duration': 1524.3385648727417, 'accumulated_submission_time': 1198.004730939865, 'accumulated_eval_time': 309.9602212905884, 'accumulated_logging_time': 1.958364725112915}
I1010 04:51:51.730220 140142937356032 logging_writer.py:48] [4015] accumulated_eval_time=309.960221, accumulated_logging_time=1.958365, accumulated_submission_time=1198.004731, global_step=4015, preemption_count=0, score=1198.004731, test/loss=0.294218, test/num_examples=3581, test/ssim=0.735104, total_duration=1524.338565, train/loss=0.269610, train/ssim=0.738512, validation/loss=0.292431, validation/num_examples=3554, validation/ssim=0.718165
I1010 04:53:12.385448 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:53:14.363030 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:53:16.320952 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:53:18.216039 140204662740800 submission_runner.py:381] Time since start: 1610.84s, 	Step: 4321, 	{'train/ssim': 0.7231118338448661, 'train/loss': 0.2743037939071655, 'validation/ssim': 0.7062113798932541, 'validation/loss': 0.29636387779394696, 'validation/num_examples': 3554, 'test/ssim': 0.7224049508255376, 'test/loss': 0.29784024517418317, 'test/num_examples': 3581, 'score': 1277.661129951477, 'total_duration': 1610.841249704361, 'accumulated_submission_time': 1277.661129951477, 'accumulated_eval_time': 315.79105830192566, 'accumulated_logging_time': 1.9837493896484375}
I1010 04:53:18.232186 140142928963328 logging_writer.py:48] [4321] accumulated_eval_time=315.791058, accumulated_logging_time=1.983749, accumulated_submission_time=1277.661130, global_step=4321, preemption_count=0, score=1277.661130, test/loss=0.297840, test/num_examples=3581, test/ssim=0.722405, total_duration=1610.841250, train/loss=0.274304, train/ssim=0.723112, validation/loss=0.296364, validation/num_examples=3554, validation/ssim=0.706211
I1010 04:54:03.943689 140142937356032 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.316467, loss=0.268540
I1010 04:54:03.948011 140204662740800 pytorch_submission_base.py:86] 4500) loss = 0.269, grad_norm = 0.316
I1010 04:54:38.784900 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:54:40.767919 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:54:42.733901 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:54:44.626937 140204662740800 submission_runner.py:381] Time since start: 1697.25s, 	Step: 4631, 	{'train/ssim': 0.740626403263637, 'train/loss': 0.267188480922154, 'validation/ssim': 0.7193851503147861, 'validation/loss': 0.2901319032582126, 'validation/num_examples': 3554, 'test/ssim': 0.7369220118987364, 'test/loss': 0.29163518041180886, 'test/num_examples': 3581, 'score': 1357.2343275547028, 'total_duration': 1697.252140045166, 'accumulated_submission_time': 1357.2343275547028, 'accumulated_eval_time': 321.6333043575287, 'accumulated_logging_time': 2.009265422821045}
I1010 04:54:44.642892 140142928963328 logging_writer.py:48] [4631] accumulated_eval_time=321.633304, accumulated_logging_time=2.009265, accumulated_submission_time=1357.234328, global_step=4631, preemption_count=0, score=1357.234328, test/loss=0.291635, test/num_examples=3581, test/ssim=0.736922, total_duration=1697.252140, train/loss=0.267188, train/ssim=0.740626, validation/loss=0.290132, validation/num_examples=3554, validation/ssim=0.719385
I1010 04:56:05.335155 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:56:07.300482 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:56:09.257267 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:56:11.152767 140204662740800 submission_runner.py:381] Time since start: 1783.78s, 	Step: 4940, 	{'train/ssim': 0.7359288079398019, 'train/loss': 0.27013741220746723, 'validation/ssim': 0.7157025012529896, 'validation/loss': 0.2929488629119478, 'validation/num_examples': 3554, 'test/ssim': 0.7328671367765638, 'test/loss': 0.2945947292895665, 'test/num_examples': 3581, 'score': 1436.9232141971588, 'total_duration': 1783.7779281139374, 'accumulated_submission_time': 1436.9232141971588, 'accumulated_eval_time': 327.45118594169617, 'accumulated_logging_time': 2.0347211360931396}
I1010 04:56:11.169564 140142937356032 logging_writer.py:48] [4940] accumulated_eval_time=327.451186, accumulated_logging_time=2.034721, accumulated_submission_time=1436.923214, global_step=4940, preemption_count=0, score=1436.923214, test/loss=0.294595, test/num_examples=3581, test/ssim=0.732867, total_duration=1783.777928, train/loss=0.270137, train/ssim=0.735929, validation/loss=0.292949, validation/num_examples=3554, validation/ssim=0.715703
I1010 04:56:25.192200 140142928963328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.282448, loss=0.227877
I1010 04:56:25.195780 140204662740800 pytorch_submission_base.py:86] 5000) loss = 0.228, grad_norm = 0.282
I1010 04:57:31.631388 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:57:33.645364 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:57:35.614052 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:57:37.514524 140204662740800 submission_runner.py:381] Time since start: 1870.14s, 	Step: 5249, 	{'train/ssim': 0.7402444566999163, 'train/loss': 0.26728527886526926, 'validation/ssim': 0.7195401253429234, 'validation/loss': 0.290283409208814, 'validation/num_examples': 3554, 'test/ssim': 0.7367175500907568, 'test/loss': 0.29188692273457134, 'test/num_examples': 3581, 'score': 1516.4129123687744, 'total_duration': 1870.1397202014923, 'accumulated_submission_time': 1516.4129123687744, 'accumulated_eval_time': 333.33461689949036, 'accumulated_logging_time': 2.060382127761841}
I1010 04:57:37.530601 140142937356032 logging_writer.py:48] [5249] accumulated_eval_time=333.334617, accumulated_logging_time=2.060382, accumulated_submission_time=1516.412912, global_step=5249, preemption_count=0, score=1516.412912, test/loss=0.291887, test/num_examples=3581, test/ssim=0.736718, total_duration=1870.139720, train/loss=0.267285, train/ssim=0.740244, validation/loss=0.290283, validation/num_examples=3554, validation/ssim=0.719540
I1010 04:58:43.018287 140142928963328 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.438683, loss=0.287077
I1010 04:58:43.022181 140204662740800 pytorch_submission_base.py:86] 5500) loss = 0.287, grad_norm = 0.439
I1010 04:58:58.171224 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:59:00.121282 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 04:59:02.160225 140204662740800 spec.py:349] Evaluating on the test split.
I1010 04:59:04.075504 140204662740800 submission_runner.py:381] Time since start: 1956.70s, 	Step: 5555, 	{'train/ssim': 0.7407815796988351, 'train/loss': 0.26683548518589567, 'validation/ssim': 0.7201292502725802, 'validation/loss': 0.2901017119794598, 'validation/num_examples': 3554, 'test/ssim': 0.7372907794610444, 'test/loss': 0.2916151023849832, 'test/num_examples': 3581, 'score': 1596.0226624011993, 'total_duration': 1956.700663805008, 'accumulated_submission_time': 1596.0226624011993, 'accumulated_eval_time': 339.23907566070557, 'accumulated_logging_time': 2.0855679512023926}
I1010 04:59:04.093924 140142937356032 logging_writer.py:48] [5555] accumulated_eval_time=339.239076, accumulated_logging_time=2.085568, accumulated_submission_time=1596.022662, global_step=5555, preemption_count=0, score=1596.022662, test/loss=0.291615, test/num_examples=3581, test/ssim=0.737291, total_duration=1956.700664, train/loss=0.266835, train/ssim=0.740782, validation/loss=0.290102, validation/num_examples=3554, validation/ssim=0.720129
I1010 05:00:24.704139 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:00:26.770264 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:00:28.800985 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:00:30.777729 140204662740800 submission_runner.py:381] Time since start: 2043.40s, 	Step: 5864, 	{'train/ssim': 0.7418274198259626, 'train/loss': 0.26806613377162386, 'validation/ssim': 0.7216104433384919, 'validation/loss': 0.2913272237267867, 'validation/num_examples': 3554, 'test/ssim': 0.7385354125942474, 'test/loss': 0.2930547548585416, 'test/num_examples': 3581, 'score': 1675.6175727844238, 'total_duration': 2043.4029126167297, 'accumulated_submission_time': 1675.6175727844238, 'accumulated_eval_time': 345.3128430843353, 'accumulated_logging_time': 2.113248586654663}
I1010 05:00:30.793911 140142928963328 logging_writer.py:48] [5864] accumulated_eval_time=345.312843, accumulated_logging_time=2.113249, accumulated_submission_time=1675.617573, global_step=5864, preemption_count=0, score=1675.617573, test/loss=0.293055, test/num_examples=3581, test/ssim=0.738535, total_duration=2043.402913, train/loss=0.268066, train/ssim=0.741827, validation/loss=0.291327, validation/num_examples=3554, validation/ssim=0.721610
I1010 05:01:05.239608 140142937356032 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.210846, loss=0.265961
I1010 05:01:05.243261 140204662740800 pytorch_submission_base.py:86] 6000) loss = 0.266, grad_norm = 0.211
I1010 05:01:51.447108 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:01:53.430816 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:01:55.381360 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:01:57.275776 140204662740800 submission_runner.py:381] Time since start: 2129.90s, 	Step: 6172, 	{'train/ssim': 0.7416880471365792, 'train/loss': 0.26659904207502094, 'validation/ssim': 0.7208092581642164, 'validation/loss': 0.2895858154983821, 'validation/num_examples': 3554, 'test/ssim': 0.7381549868228149, 'test/loss': 0.29101450008508445, 'test/num_examples': 3581, 'score': 1755.2498984336853, 'total_duration': 2129.9009618759155, 'accumulated_submission_time': 1755.2498984336853, 'accumulated_eval_time': 351.1417167186737, 'accumulated_logging_time': 2.137796401977539}
I1010 05:01:57.292018 140142928963328 logging_writer.py:48] [6172] accumulated_eval_time=351.141717, accumulated_logging_time=2.137796, accumulated_submission_time=1755.249898, global_step=6172, preemption_count=0, score=1755.249898, test/loss=0.291015, test/num_examples=3581, test/ssim=0.738155, total_duration=2129.900962, train/loss=0.266599, train/ssim=0.741688, validation/loss=0.289586, validation/num_examples=3554, validation/ssim=0.720809
I1010 05:03:17.721297 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:03:19.707207 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:03:21.666231 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:03:23.560515 140204662740800 submission_runner.py:381] Time since start: 2216.19s, 	Step: 6478, 	{'train/ssim': 0.7379164014543805, 'train/loss': 0.2682357685906546, 'validation/ssim': 0.7172490228879431, 'validation/loss': 0.29111832343398286, 'validation/num_examples': 3554, 'test/ssim': 0.7343105048781765, 'test/loss': 0.29271479193573724, 'test/num_examples': 3581, 'score': 1834.7129101753235, 'total_duration': 2216.1857285499573, 'accumulated_submission_time': 1834.7129101753235, 'accumulated_eval_time': 356.9811520576477, 'accumulated_logging_time': 2.1638283729553223}
I1010 05:03:23.577088 140142937356032 logging_writer.py:48] [6478] accumulated_eval_time=356.981152, accumulated_logging_time=2.163828, accumulated_submission_time=1834.712910, global_step=6478, preemption_count=0, score=1834.712910, test/loss=0.292715, test/num_examples=3581, test/ssim=0.734311, total_duration=2216.185729, train/loss=0.268236, train/ssim=0.737916, validation/loss=0.291118, validation/num_examples=3554, validation/ssim=0.717249
I1010 05:03:27.517557 140142928963328 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.332564, loss=0.268587
I1010 05:03:27.521717 140204662740800 pytorch_submission_base.py:86] 6500) loss = 0.269, grad_norm = 0.333
I1010 05:04:44.290421 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:04:46.253521 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:04:48.226811 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:04:50.124304 140204662740800 submission_runner.py:381] Time since start: 2302.75s, 	Step: 6786, 	{'train/ssim': 0.741504601069859, 'train/loss': 0.265310423714774, 'validation/ssim': 0.7200476410822313, 'validation/loss': 0.28864287900736846, 'validation/num_examples': 3554, 'test/ssim': 0.7374265873708461, 'test/loss': 0.29004939126422435, 'test/num_examples': 3581, 'score': 1914.4597845077515, 'total_duration': 2302.74951672554, 'accumulated_submission_time': 1914.4597845077515, 'accumulated_eval_time': 362.8152663707733, 'accumulated_logging_time': 2.189122438430786}
I1010 05:04:50.141200 140142937356032 logging_writer.py:48] [6786] accumulated_eval_time=362.815266, accumulated_logging_time=2.189122, accumulated_submission_time=1914.459785, global_step=6786, preemption_count=0, score=1914.459785, test/loss=0.290049, test/num_examples=3581, test/ssim=0.737427, total_duration=2302.749517, train/loss=0.265310, train/ssim=0.741505, validation/loss=0.288643, validation/num_examples=3554, validation/ssim=0.720048
I1010 05:05:45.532040 140142928963328 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.240579, loss=0.255646
I1010 05:05:45.536020 140204662740800 pytorch_submission_base.py:86] 7000) loss = 0.256, grad_norm = 0.241
I1010 05:06:10.725177 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:06:12.699207 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:06:14.656407 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:06:16.546413 140204662740800 submission_runner.py:381] Time since start: 2389.17s, 	Step: 7093, 	{'train/ssim': 0.7261455399649483, 'train/loss': 0.2726601873125349, 'validation/ssim': 0.7072191297437747, 'validation/loss': 0.2951184789585678, 'validation/num_examples': 3554, 'test/ssim': 0.7243525535159523, 'test/loss': 0.29655129721097456, 'test/num_examples': 3581, 'score': 1994.0795505046844, 'total_duration': 2389.1716210842133, 'accumulated_submission_time': 1994.0795505046844, 'accumulated_eval_time': 368.6366057395935, 'accumulated_logging_time': 2.2144317626953125}
I1010 05:06:16.562923 140142937356032 logging_writer.py:48] [7093] accumulated_eval_time=368.636606, accumulated_logging_time=2.214432, accumulated_submission_time=1994.079551, global_step=7093, preemption_count=0, score=1994.079551, test/loss=0.296551, test/num_examples=3581, test/ssim=0.724353, total_duration=2389.171621, train/loss=0.272660, train/ssim=0.726146, validation/loss=0.295118, validation/num_examples=3554, validation/ssim=0.707219
I1010 05:07:37.171450 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:07:39.163171 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:07:41.112339 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:07:43.002998 140204662740800 submission_runner.py:381] Time since start: 2475.63s, 	Step: 7402, 	{'train/ssim': 0.7395340374537877, 'train/loss': 0.2657548018864223, 'validation/ssim': 0.7188987925137169, 'validation/loss': 0.2887144244381331, 'validation/num_examples': 3554, 'test/ssim': 0.7362722201375315, 'test/loss': 0.29014971322169086, 'test/num_examples': 3581, 'score': 2073.6813666820526, 'total_duration': 2475.6282069683075, 'accumulated_submission_time': 2073.6813666820526, 'accumulated_eval_time': 374.4683928489685, 'accumulated_logging_time': 2.2392570972442627}
I1010 05:07:43.020241 140142928963328 logging_writer.py:48] [7402] accumulated_eval_time=374.468393, accumulated_logging_time=2.239257, accumulated_submission_time=2073.681367, global_step=7402, preemption_count=0, score=2073.681367, test/loss=0.290150, test/num_examples=3581, test/ssim=0.736272, total_duration=2475.628207, train/loss=0.265755, train/ssim=0.739534, validation/loss=0.288714, validation/num_examples=3554, validation/ssim=0.718899
I1010 05:08:07.842559 140142937356032 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.259533, loss=0.262323
I1010 05:08:07.846909 140204662740800 pytorch_submission_base.py:86] 7500) loss = 0.262, grad_norm = 0.260
I1010 05:09:03.632838 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:09:05.596442 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:09:07.570920 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:09:09.480598 140204662740800 submission_runner.py:381] Time since start: 2562.11s, 	Step: 7708, 	{'train/ssim': 0.7388514110020229, 'train/loss': 0.26898867743355886, 'validation/ssim': 0.7175751161763154, 'validation/loss': 0.2925260819675014, 'validation/num_examples': 3554, 'test/ssim': 0.7345956878534278, 'test/loss': 0.2941942595796391, 'test/num_examples': 3581, 'score': 2153.2630984783173, 'total_duration': 2562.1057510375977, 'accumulated_submission_time': 2153.2630984783173, 'accumulated_eval_time': 380.31637144088745, 'accumulated_logging_time': 2.2650411128997803}
I1010 05:09:09.500036 140142928963328 logging_writer.py:48] [7708] accumulated_eval_time=380.316371, accumulated_logging_time=2.265041, accumulated_submission_time=2153.263098, global_step=7708, preemption_count=0, score=2153.263098, test/loss=0.294194, test/num_examples=3581, test/ssim=0.734596, total_duration=2562.105751, train/loss=0.268989, train/ssim=0.738851, validation/loss=0.292526, validation/num_examples=3554, validation/ssim=0.717575
I1010 05:10:25.173133 140142937356032 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.248036, loss=0.309755
I1010 05:10:25.177460 140204662740800 pytorch_submission_base.py:86] 8000) loss = 0.310, grad_norm = 0.248
I1010 05:10:30.175029 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:10:32.279039 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:10:34.310909 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:10:36.300807 140204662740800 submission_runner.py:381] Time since start: 2648.93s, 	Step: 8018, 	{'train/ssim': 0.7405543327331543, 'train/loss': 0.2702175889696394, 'validation/ssim': 0.7202724098287141, 'validation/loss': 0.2933752159758371, 'validation/num_examples': 3554, 'test/ssim': 0.7366512823757331, 'test/loss': 0.29520794427185143, 'test/num_examples': 3581, 'score': 2232.9535706043243, 'total_duration': 2648.9260125160217, 'accumulated_submission_time': 2232.9535706043243, 'accumulated_eval_time': 386.44236040115356, 'accumulated_logging_time': 2.293684482574463}
I1010 05:10:36.318139 140142928963328 logging_writer.py:48] [8018] accumulated_eval_time=386.442360, accumulated_logging_time=2.293684, accumulated_submission_time=2232.953571, global_step=8018, preemption_count=0, score=2232.953571, test/loss=0.295208, test/num_examples=3581, test/ssim=0.736651, total_duration=2648.926013, train/loss=0.270218, train/ssim=0.740554, validation/loss=0.293375, validation/num_examples=3554, validation/ssim=0.720272
I1010 05:11:56.978803 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:11:58.936668 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:12:00.874733 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:12:02.824864 140204662740800 submission_runner.py:381] Time since start: 2735.45s, 	Step: 8327, 	{'train/ssim': 0.742305074419294, 'train/loss': 0.26552157742636545, 'validation/ssim': 0.7211831628974396, 'validation/loss': 0.28879569015545864, 'validation/num_examples': 3554, 'test/ssim': 0.7385761822378526, 'test/loss': 0.29016218955075396, 'test/num_examples': 3581, 'score': 2312.624679327011, 'total_duration': 2735.4500720500946, 'accumulated_submission_time': 2312.624679327011, 'accumulated_eval_time': 392.2888216972351, 'accumulated_logging_time': 2.3204901218414307}
I1010 05:12:02.842238 140142937356032 logging_writer.py:48] [8327] accumulated_eval_time=392.288822, accumulated_logging_time=2.320490, accumulated_submission_time=2312.624679, global_step=8327, preemption_count=0, score=2312.624679, test/loss=0.290162, test/num_examples=3581, test/ssim=0.738576, total_duration=2735.450072, train/loss=0.265522, train/ssim=0.742305, validation/loss=0.288796, validation/num_examples=3554, validation/ssim=0.721183
I1010 05:12:47.251769 140142928963328 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.276181, loss=0.328797
I1010 05:12:47.255848 140204662740800 pytorch_submission_base.py:86] 8500) loss = 0.329, grad_norm = 0.276
I1010 05:13:23.298051 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:13:25.242821 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:13:27.197641 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:13:29.095867 140204662740800 submission_runner.py:381] Time since start: 2821.72s, 	Step: 8633, 	{'train/ssim': 0.7432466915675572, 'train/loss': 0.2651015179497855, 'validation/ssim': 0.7222014230224747, 'validation/loss': 0.2882895483082442, 'validation/num_examples': 3554, 'test/ssim': 0.7394887268526599, 'test/loss': 0.2897077580132121, 'test/num_examples': 3581, 'score': 2392.075304746628, 'total_duration': 2821.721071958542, 'accumulated_submission_time': 2392.075304746628, 'accumulated_eval_time': 398.0868935585022, 'accumulated_logging_time': 2.347553253173828}
I1010 05:13:29.112832 140142937356032 logging_writer.py:48] [8633] accumulated_eval_time=398.086894, accumulated_logging_time=2.347553, accumulated_submission_time=2392.075305, global_step=8633, preemption_count=0, score=2392.075305, test/loss=0.289708, test/num_examples=3581, test/ssim=0.739489, total_duration=2821.721072, train/loss=0.265102, train/ssim=0.743247, validation/loss=0.288290, validation/num_examples=3554, validation/ssim=0.722201
I1010 05:14:49.575872 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:14:51.549527 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:14:53.504263 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:14:55.392740 140204662740800 submission_runner.py:381] Time since start: 2908.02s, 	Step: 8942, 	{'train/ssim': 0.7437271390642438, 'train/loss': 0.2644360065460205, 'validation/ssim': 0.7222374876899268, 'validation/loss': 0.288108469330332, 'validation/num_examples': 3554, 'test/ssim': 0.7394019379625105, 'test/loss': 0.28959557331663643, 'test/num_examples': 3581, 'score': 2471.5909333229065, 'total_duration': 2908.0179464817047, 'accumulated_submission_time': 2471.5909333229065, 'accumulated_eval_time': 403.9040672779083, 'accumulated_logging_time': 2.372897148132324}
I1010 05:14:55.409996 140142928963328 logging_writer.py:48] [8942] accumulated_eval_time=403.904067, accumulated_logging_time=2.372897, accumulated_submission_time=2471.590933, global_step=8942, preemption_count=0, score=2471.590933, test/loss=0.289596, test/num_examples=3581, test/ssim=0.739402, total_duration=2908.017946, train/loss=0.264436, train/ssim=0.743727, validation/loss=0.288108, validation/num_examples=3554, validation/ssim=0.722237
I1010 05:15:09.055273 140142937356032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.524811, loss=0.257853
I1010 05:15:09.058749 140204662740800 pytorch_submission_base.py:86] 9000) loss = 0.258, grad_norm = 0.525
I1010 05:16:16.109191 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:16:18.084937 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:16:20.039670 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:16:21.928199 140204662740800 submission_runner.py:381] Time since start: 2994.55s, 	Step: 9251, 	{'train/ssim': 0.7425071171351841, 'train/loss': 0.26476895809173584, 'validation/ssim': 0.7213926814416854, 'validation/loss': 0.2879573755473586, 'validation/num_examples': 3554, 'test/ssim': 0.7387488737215513, 'test/loss': 0.2893701130969003, 'test/num_examples': 3581, 'score': 2551.3116743564606, 'total_duration': 2994.5534007549286, 'accumulated_submission_time': 2551.3116743564606, 'accumulated_eval_time': 409.72331953048706, 'accumulated_logging_time': 2.399118661880493}
I1010 05:16:21.945358 140142928963328 logging_writer.py:48] [9251] accumulated_eval_time=409.723320, accumulated_logging_time=2.399119, accumulated_submission_time=2551.311674, global_step=9251, preemption_count=0, score=2551.311674, test/loss=0.289370, test/num_examples=3581, test/ssim=0.738749, total_duration=2994.553401, train/loss=0.264769, train/ssim=0.742507, validation/loss=0.287957, validation/num_examples=3554, validation/ssim=0.721393
I1010 05:17:26.428180 140142937356032 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.157712, loss=0.296804
I1010 05:17:26.432202 140204662740800 pytorch_submission_base.py:86] 9500) loss = 0.297, grad_norm = 0.158
I1010 05:17:42.562288 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:17:44.578100 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:17:46.531787 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:17:48.427204 140204662740800 submission_runner.py:381] Time since start: 3081.05s, 	Step: 9560, 	{'train/ssim': 0.7433134487697056, 'train/loss': 0.26483518736703054, 'validation/ssim': 0.7227133352164814, 'validation/loss': 0.28785933117284224, 'validation/num_examples': 3554, 'test/ssim': 0.7399288753796076, 'test/loss': 0.2892544854789165, 'test/num_examples': 3581, 'score': 2630.9752430915833, 'total_duration': 3081.052417039871, 'accumulated_submission_time': 2630.9752430915833, 'accumulated_eval_time': 415.5883586406708, 'accumulated_logging_time': 2.4256081581115723}
I1010 05:17:48.444542 140142928963328 logging_writer.py:48] [9560] accumulated_eval_time=415.588359, accumulated_logging_time=2.425608, accumulated_submission_time=2630.975243, global_step=9560, preemption_count=0, score=2630.975243, test/loss=0.289254, test/num_examples=3581, test/ssim=0.739929, total_duration=3081.052417, train/loss=0.264835, train/ssim=0.743313, validation/loss=0.287859, validation/num_examples=3554, validation/ssim=0.722713
I1010 05:19:09.067439 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:19:11.040530 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:19:13.007220 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:19:14.912268 140204662740800 submission_runner.py:381] Time since start: 3167.54s, 	Step: 9866, 	{'train/ssim': 0.7405358723231724, 'train/loss': 0.2659291369574411, 'validation/ssim': 0.7198089273310003, 'validation/loss': 0.2891314693830895, 'validation/num_examples': 3554, 'test/ssim': 0.7368423815580494, 'test/loss': 0.2906959105456751, 'test/num_examples': 3581, 'score': 2710.6089477539062, 'total_duration': 3167.537455558777, 'accumulated_submission_time': 2710.6089477539062, 'accumulated_eval_time': 421.4334752559662, 'accumulated_logging_time': 2.4510579109191895}
I1010 05:19:14.932019 140142937356032 logging_writer.py:48] [9866] accumulated_eval_time=421.433475, accumulated_logging_time=2.451058, accumulated_submission_time=2710.608948, global_step=9866, preemption_count=0, score=2710.608948, test/loss=0.290696, test/num_examples=3581, test/ssim=0.736842, total_duration=3167.537456, train/loss=0.265929, train/ssim=0.740536, validation/loss=0.289131, validation/num_examples=3554, validation/ssim=0.719809
I1010 05:19:48.671628 140142928963328 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.146224, loss=0.288470
I1010 05:19:48.675557 140204662740800 pytorch_submission_base.py:86] 10000) loss = 0.288, grad_norm = 0.146
I1010 05:20:35.372960 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:20:37.431981 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:20:39.472116 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:20:41.462294 140204662740800 submission_runner.py:381] Time since start: 3254.09s, 	Step: 10173, 	{'train/ssim': 0.7426485334123883, 'train/loss': 0.26604740960257395, 'validation/ssim': 0.7218587056397721, 'validation/loss': 0.2891095558041995, 'validation/num_examples': 3554, 'test/ssim': 0.7390568958784208, 'test/loss': 0.2905985883613167, 'test/num_examples': 3581, 'score': 2790.0488142967224, 'total_duration': 3254.0874865055084, 'accumulated_submission_time': 2790.0488142967224, 'accumulated_eval_time': 427.5229561328888, 'accumulated_logging_time': 2.4809067249298096}
I1010 05:20:41.480060 140142937356032 logging_writer.py:48] [10173] accumulated_eval_time=427.522956, accumulated_logging_time=2.480907, accumulated_submission_time=2790.048814, global_step=10173, preemption_count=0, score=2790.048814, test/loss=0.290599, test/num_examples=3581, test/ssim=0.739057, total_duration=3254.087487, train/loss=0.266047, train/ssim=0.742649, validation/loss=0.289110, validation/num_examples=3554, validation/ssim=0.721859
I1010 05:22:02.280446 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:22:04.238286 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:22:06.188557 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:22:08.075235 140204662740800 submission_runner.py:381] Time since start: 3340.70s, 	Step: 10478, 	{'train/ssim': 0.7429479190281459, 'train/loss': 0.26432931423187256, 'validation/ssim': 0.7217718756594682, 'validation/loss': 0.28757290901866733, 'validation/num_examples': 3554, 'test/ssim': 0.7391050286014731, 'test/loss': 0.2889906417987294, 'test/num_examples': 3581, 'score': 2869.7241768836975, 'total_duration': 3340.700449705124, 'accumulated_submission_time': 2869.7241768836975, 'accumulated_eval_time': 433.3181622028351, 'accumulated_logging_time': 2.506944417953491}
I1010 05:22:08.092292 140142928963328 logging_writer.py:48] [10478] accumulated_eval_time=433.318162, accumulated_logging_time=2.506944, accumulated_submission_time=2869.724177, global_step=10478, preemption_count=0, score=2869.724177, test/loss=0.288991, test/num_examples=3581, test/ssim=0.739105, total_duration=3340.700450, train/loss=0.264329, train/ssim=0.742948, validation/loss=0.287573, validation/num_examples=3554, validation/ssim=0.721772
I1010 05:22:11.872853 140142937356032 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.661973, loss=0.269539
I1010 05:22:11.876379 140204662740800 pytorch_submission_base.py:86] 10500) loss = 0.270, grad_norm = 0.662
I1010 05:23:28.727214 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:23:30.673738 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:23:32.637128 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:23:34.538523 140204662740800 submission_runner.py:381] Time since start: 3427.16s, 	Step: 10784, 	{'train/ssim': 0.7438717569623675, 'train/loss': 0.2645560162408011, 'validation/ssim': 0.7227788011747327, 'validation/loss': 0.2878263234152979, 'validation/num_examples': 3554, 'test/ssim': 0.7401943552996719, 'test/loss': 0.289197967026494, 'test/num_examples': 3581, 'score': 2949.3888173103333, 'total_duration': 3427.1637194156647, 'accumulated_submission_time': 2949.3888173103333, 'accumulated_eval_time': 439.1296989917755, 'accumulated_logging_time': 2.532369613647461}
I1010 05:23:34.555807 140142928963328 logging_writer.py:48] [10784] accumulated_eval_time=439.129699, accumulated_logging_time=2.532370, accumulated_submission_time=2949.388817, global_step=10784, preemption_count=0, score=2949.388817, test/loss=0.289198, test/num_examples=3581, test/ssim=0.740194, total_duration=3427.163719, train/loss=0.264556, train/ssim=0.743872, validation/loss=0.287826, validation/num_examples=3554, validation/ssim=0.722779
I1010 05:24:30.354575 140142937356032 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.156583, loss=0.342303
I1010 05:24:30.358935 140204662740800 pytorch_submission_base.py:86] 11000) loss = 0.342, grad_norm = 0.157
I1010 05:24:55.186244 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:24:57.133146 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:24:59.106761 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:25:01.016490 140204662740800 submission_runner.py:381] Time since start: 3513.64s, 	Step: 11093, 	{'train/ssim': 0.7453657558986119, 'train/loss': 0.26422129358564106, 'validation/ssim': 0.7239784837507034, 'validation/loss': 0.28789215002022367, 'validation/num_examples': 3554, 'test/ssim': 0.7410098163484362, 'test/loss': 0.28940911014730525, 'test/num_examples': 3581, 'score': 3029.025542497635, 'total_duration': 3513.6416630744934, 'accumulated_submission_time': 3029.025542497635, 'accumulated_eval_time': 444.9601626396179, 'accumulated_logging_time': 2.559365749359131}
I1010 05:25:01.035352 140142928963328 logging_writer.py:48] [11093] accumulated_eval_time=444.960163, accumulated_logging_time=2.559366, accumulated_submission_time=3029.025542, global_step=11093, preemption_count=0, score=3029.025542, test/loss=0.289409, test/num_examples=3581, test/ssim=0.741010, total_duration=3513.641663, train/loss=0.264221, train/ssim=0.745366, validation/loss=0.287892, validation/num_examples=3554, validation/ssim=0.723978
I1010 05:26:21.677828 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:26:23.739701 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:26:25.797732 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:26:27.784788 140204662740800 submission_runner.py:381] Time since start: 3600.41s, 	Step: 11401, 	{'train/ssim': 0.7411413873944964, 'train/loss': 0.2648871626172747, 'validation/ssim': 0.7201837937886888, 'validation/loss': 0.287964073271314, 'validation/num_examples': 3554, 'test/ssim': 0.7376043239274993, 'test/loss': 0.28931598082763194, 'test/num_examples': 3581, 'score': 3108.681938648224, 'total_duration': 3600.4099621772766, 'accumulated_submission_time': 3108.681938648224, 'accumulated_eval_time': 451.0672836303711, 'accumulated_logging_time': 2.5875611305236816}
I1010 05:26:27.803183 140142937356032 logging_writer.py:48] [11401] accumulated_eval_time=451.067284, accumulated_logging_time=2.587561, accumulated_submission_time=3108.681939, global_step=11401, preemption_count=0, score=3108.681939, test/loss=0.289316, test/num_examples=3581, test/ssim=0.737604, total_duration=3600.409962, train/loss=0.264887, train/ssim=0.741141, validation/loss=0.287964, validation/num_examples=3554, validation/ssim=0.720184
I1010 05:26:52.130056 140142928963328 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.143151, loss=0.314406
I1010 05:26:52.134307 140204662740800 pytorch_submission_base.py:86] 11500) loss = 0.314, grad_norm = 0.143
I1010 05:27:48.389207 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:27:50.331162 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:27:52.293808 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:27:54.191836 140204662740800 submission_runner.py:381] Time since start: 3686.82s, 	Step: 11711, 	{'train/ssim': 0.7440343584333148, 'train/loss': 0.2646040064947946, 'validation/ssim': 0.7228662494064786, 'validation/loss': 0.287836370001231, 'validation/num_examples': 3554, 'test/ssim': 0.7400071421870636, 'test/loss': 0.2893998722096656, 'test/num_examples': 3581, 'score': 3188.2996957302094, 'total_duration': 3686.817044019699, 'accumulated_submission_time': 3188.2996957302094, 'accumulated_eval_time': 456.8700966835022, 'accumulated_logging_time': 2.6155266761779785}
I1010 05:27:54.209649 140142937356032 logging_writer.py:48] [11711] accumulated_eval_time=456.870097, accumulated_logging_time=2.615527, accumulated_submission_time=3188.299696, global_step=11711, preemption_count=0, score=3188.299696, test/loss=0.289400, test/num_examples=3581, test/ssim=0.740007, total_duration=3686.817044, train/loss=0.264604, train/ssim=0.744034, validation/loss=0.287836, validation/num_examples=3554, validation/ssim=0.722866
I1010 05:29:10.005687 140142928963328 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.203657, loss=0.307565
I1010 05:29:10.009307 140204662740800 pytorch_submission_base.py:86] 12000) loss = 0.308, grad_norm = 0.204
I1010 05:29:14.710376 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:29:16.700005 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:29:18.671909 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:29:20.569905 140204662740800 submission_runner.py:381] Time since start: 3773.20s, 	Step: 12017, 	{'train/ssim': 0.7446763856070382, 'train/loss': 0.2642279693058559, 'validation/ssim': 0.7230840799978897, 'validation/loss': 0.2877823073473551, 'validation/num_examples': 3554, 'test/ssim': 0.7402127629982895, 'test/loss': 0.2892108865038572, 'test/num_examples': 3581, 'score': 3267.820540189743, 'total_duration': 3773.195077896118, 'accumulated_submission_time': 3267.820540189743, 'accumulated_eval_time': 462.7298436164856, 'accumulated_logging_time': 2.641876459121704}
I1010 05:29:20.587580 140142937356032 logging_writer.py:48] [12017] accumulated_eval_time=462.729844, accumulated_logging_time=2.641876, accumulated_submission_time=3267.820540, global_step=12017, preemption_count=0, score=3267.820540, test/loss=0.289211, test/num_examples=3581, test/ssim=0.740213, total_duration=3773.195078, train/loss=0.264228, train/ssim=0.744676, validation/loss=0.287782, validation/num_examples=3554, validation/ssim=0.723084
I1010 05:30:41.066112 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:30:43.099503 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:30:45.054236 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:30:46.944787 140204662740800 submission_runner.py:381] Time since start: 3859.57s, 	Step: 12325, 	{'train/ssim': 0.7447128977094378, 'train/loss': 0.2646752766200474, 'validation/ssim': 0.7237226650429094, 'validation/loss': 0.28781306535659645, 'validation/num_examples': 3554, 'test/ssim': 0.7408643955293563, 'test/loss': 0.28931393552778556, 'test/num_examples': 3581, 'score': 3347.294772386551, 'total_duration': 3859.569970369339, 'accumulated_submission_time': 3347.294772386551, 'accumulated_eval_time': 468.6086890697479, 'accumulated_logging_time': 2.6689863204956055}
I1010 05:30:46.962711 140142928963328 logging_writer.py:48] [12325] accumulated_eval_time=468.608689, accumulated_logging_time=2.668986, accumulated_submission_time=3347.294772, global_step=12325, preemption_count=0, score=3347.294772, test/loss=0.289314, test/num_examples=3581, test/ssim=0.740864, total_duration=3859.569970, train/loss=0.264675, train/ssim=0.744713, validation/loss=0.287813, validation/num_examples=3554, validation/ssim=0.723723
I1010 05:31:31.854035 140142937356032 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.184703, loss=0.322871
I1010 05:31:31.858534 140204662740800 pytorch_submission_base.py:86] 12500) loss = 0.323, grad_norm = 0.185
I1010 05:32:07.449222 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:32:09.419431 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:32:11.361718 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:32:13.248809 140204662740800 submission_runner.py:381] Time since start: 3945.87s, 	Step: 12631, 	{'train/ssim': 0.7449334008353097, 'train/loss': 0.26370700768062044, 'validation/ssim': 0.7236994462665307, 'validation/loss': 0.2869125993049205, 'validation/num_examples': 3554, 'test/ssim': 0.7410447227991482, 'test/loss': 0.2882688895712615, 'test/num_examples': 3581, 'score': 3426.822010755539, 'total_duration': 3945.8740181922913, 'accumulated_submission_time': 3426.822010755539, 'accumulated_eval_time': 474.4083797931671, 'accumulated_logging_time': 2.6954219341278076}
I1010 05:32:13.266817 140142928963328 logging_writer.py:48] [12631] accumulated_eval_time=474.408380, accumulated_logging_time=2.695422, accumulated_submission_time=3426.822011, global_step=12631, preemption_count=0, score=3426.822011, test/loss=0.288269, test/num_examples=3581, test/ssim=0.741045, total_duration=3945.874018, train/loss=0.263707, train/ssim=0.744933, validation/loss=0.286913, validation/num_examples=3554, validation/ssim=0.723699
I1010 05:33:33.847080 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:33:35.841536 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:33:37.803370 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:33:39.706071 140204662740800 submission_runner.py:381] Time since start: 4032.33s, 	Step: 12937, 	{'train/ssim': 0.7438997541155133, 'train/loss': 0.26406238760266987, 'validation/ssim': 0.7227297532269977, 'validation/loss': 0.28731248777236035, 'validation/num_examples': 3554, 'test/ssim': 0.7401613577954831, 'test/loss': 0.28871759426923344, 'test/num_examples': 3581, 'score': 3506.44699382782, 'total_duration': 4032.3312780857086, 'accumulated_submission_time': 3506.44699382782, 'accumulated_eval_time': 480.2675156593323, 'accumulated_logging_time': 2.722045421600342}
I1010 05:33:39.723965 140142937356032 logging_writer.py:48] [12937] accumulated_eval_time=480.267516, accumulated_logging_time=2.722045, accumulated_submission_time=3506.446994, global_step=12937, preemption_count=0, score=3506.446994, test/loss=0.288718, test/num_examples=3581, test/ssim=0.740161, total_duration=4032.331278, train/loss=0.264062, train/ssim=0.743900, validation/loss=0.287312, validation/num_examples=3554, validation/ssim=0.722730
I1010 05:33:54.532725 140142928963328 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.208130, loss=0.290161
I1010 05:33:54.536917 140204662740800 pytorch_submission_base.py:86] 13000) loss = 0.290, grad_norm = 0.208
I1010 05:35:00.415413 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:35:02.417386 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:35:04.404876 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:35:06.303302 140204662740800 submission_runner.py:381] Time since start: 4118.93s, 	Step: 13247, 	{'train/ssim': 0.7463790348597935, 'train/loss': 0.2635846308299473, 'validation/ssim': 0.7249723572910804, 'validation/loss': 0.28736938407867896, 'validation/num_examples': 3554, 'test/ssim': 0.7419568583539863, 'test/loss': 0.2888660830380829, 'test/num_examples': 3581, 'score': 3586.143978834152, 'total_duration': 4118.928502559662, 'accumulated_submission_time': 3586.143978834152, 'accumulated_eval_time': 486.1556911468506, 'accumulated_logging_time': 2.7483441829681396}
I1010 05:35:06.321476 140142937356032 logging_writer.py:48] [13247] accumulated_eval_time=486.155691, accumulated_logging_time=2.748344, accumulated_submission_time=3586.143979, global_step=13247, preemption_count=0, score=3586.143979, test/loss=0.288866, test/num_examples=3581, test/ssim=0.741957, total_duration=4118.928503, train/loss=0.263585, train/ssim=0.746379, validation/loss=0.287369, validation/num_examples=3554, validation/ssim=0.724972
I1010 05:36:12.077184 140142928963328 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.199014, loss=0.287322
I1010 05:36:12.081290 140204662740800 pytorch_submission_base.py:86] 13500) loss = 0.287, grad_norm = 0.199
I1010 05:36:26.897256 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:36:28.881684 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:36:30.821631 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:36:32.721735 140204662740800 submission_runner.py:381] Time since start: 4205.35s, 	Step: 13555, 	{'train/ssim': 0.7455594880240304, 'train/loss': 0.263953583581107, 'validation/ssim': 0.724471436233821, 'validation/loss': 0.2871713203597619, 'validation/num_examples': 3554, 'test/ssim': 0.7415424806051033, 'test/loss': 0.28858563834080914, 'test/num_examples': 3581, 'score': 3665.777436733246, 'total_duration': 4205.346914291382, 'accumulated_submission_time': 3665.777436733246, 'accumulated_eval_time': 491.98027324676514, 'accumulated_logging_time': 2.7749133110046387}
I1010 05:36:32.739821 140142937356032 logging_writer.py:48] [13555] accumulated_eval_time=491.980273, accumulated_logging_time=2.774913, accumulated_submission_time=3665.777437, global_step=13555, preemption_count=0, score=3665.777437, test/loss=0.288586, test/num_examples=3581, test/ssim=0.741542, total_duration=4205.346914, train/loss=0.263954, train/ssim=0.745559, validation/loss=0.287171, validation/num_examples=3554, validation/ssim=0.724471
I1010 05:37:53.347905 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:37:55.287370 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:37:57.249318 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:37:59.142822 140204662740800 submission_runner.py:381] Time since start: 4291.77s, 	Step: 13864, 	{'train/ssim': 0.7440620149884906, 'train/loss': 0.263454794883728, 'validation/ssim': 0.7228357490020048, 'validation/loss': 0.2867525408760376, 'validation/num_examples': 3554, 'test/ssim': 0.7400878633543354, 'test/loss': 0.28811467396284207, 'test/num_examples': 3581, 'score': 3745.4199805259705, 'total_duration': 4291.768035411835, 'accumulated_submission_time': 3745.4199805259705, 'accumulated_eval_time': 497.7755160331726, 'accumulated_logging_time': 2.802415609359741}
I1010 05:37:59.160705 140142928963328 logging_writer.py:48] [13864] accumulated_eval_time=497.775516, accumulated_logging_time=2.802416, accumulated_submission_time=3745.419981, global_step=13864, preemption_count=0, score=3745.419981, test/loss=0.288115, test/num_examples=3581, test/ssim=0.740088, total_duration=4291.768035, train/loss=0.263455, train/ssim=0.744062, validation/loss=0.286753, validation/num_examples=3554, validation/ssim=0.722836
I1010 05:38:34.134887 140142937356032 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.389834, loss=0.246546
I1010 05:38:34.139028 140204662740800 pytorch_submission_base.py:86] 14000) loss = 0.247, grad_norm = 0.390
I1010 05:39:19.871275 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:39:21.835004 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:39:23.805681 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:39:25.711732 140204662740800 submission_runner.py:381] Time since start: 4378.34s, 	Step: 14169, 	{'train/ssim': 0.7441503661019462, 'train/loss': 0.26373861517224995, 'validation/ssim': 0.7223456129976786, 'validation/loss': 0.2873325637705754, 'validation/num_examples': 3554, 'test/ssim': 0.7396045590006283, 'test/loss': 0.2887801122678721, 'test/num_examples': 3581, 'score': 3825.150273323059, 'total_duration': 4378.336943626404, 'accumulated_submission_time': 3825.150273323059, 'accumulated_eval_time': 503.6161382198334, 'accumulated_logging_time': 2.82869029045105}
I1010 05:39:25.729564 140142928963328 logging_writer.py:48] [14169] accumulated_eval_time=503.616138, accumulated_logging_time=2.828690, accumulated_submission_time=3825.150273, global_step=14169, preemption_count=0, score=3825.150273, test/loss=0.288780, test/num_examples=3581, test/ssim=0.739605, total_duration=4378.336944, train/loss=0.263739, train/ssim=0.744150, validation/loss=0.287333, validation/num_examples=3554, validation/ssim=0.722346
I1010 05:40:46.338246 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:40:48.304407 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:40:50.262158 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:40:52.168330 140204662740800 submission_runner.py:381] Time since start: 4464.79s, 	Step: 14480, 	{'train/ssim': 0.7452069691249302, 'train/loss': 0.2635948828288487, 'validation/ssim': 0.7240870212260833, 'validation/loss': 0.2869385830391372, 'validation/num_examples': 3554, 'test/ssim': 0.7412888634241482, 'test/loss': 0.2883943346285081, 'test/num_examples': 3581, 'score': 3904.799750328064, 'total_duration': 4464.793534994125, 'accumulated_submission_time': 3904.799750328064, 'accumulated_eval_time': 509.44630885124207, 'accumulated_logging_time': 2.855097532272339}
I1010 05:40:52.186581 140142937356032 logging_writer.py:48] [14480] accumulated_eval_time=509.446309, accumulated_logging_time=2.855098, accumulated_submission_time=3904.799750, global_step=14480, preemption_count=0, score=3904.799750, test/loss=0.288394, test/num_examples=3581, test/ssim=0.741289, total_duration=4464.793535, train/loss=0.263595, train/ssim=0.745207, validation/loss=0.286939, validation/num_examples=3554, validation/ssim=0.724087
I1010 05:40:55.562725 140142928963328 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.213667, loss=0.339901
I1010 05:40:55.566219 140204662740800 pytorch_submission_base.py:86] 14500) loss = 0.340, grad_norm = 0.214
I1010 05:42:12.677851 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:42:14.651617 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:42:16.622063 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:42:18.530586 140204662740800 submission_runner.py:381] Time since start: 4551.16s, 	Step: 14786, 	{'train/ssim': 0.7435030937194824, 'train/loss': 0.26358212743486675, 'validation/ssim': 0.7221164477964969, 'validation/loss': 0.2869454009786508, 'validation/num_examples': 3554, 'test/ssim': 0.7394701828007191, 'test/loss': 0.2882743437041853, 'test/num_examples': 3581, 'score': 3984.3336300849915, 'total_duration': 4551.155753135681, 'accumulated_submission_time': 3984.3336300849915, 'accumulated_eval_time': 515.2992980480194, 'accumulated_logging_time': 2.8820011615753174}
I1010 05:42:18.552735 140142937356032 logging_writer.py:48] [14786] accumulated_eval_time=515.299298, accumulated_logging_time=2.882001, accumulated_submission_time=3984.333630, global_step=14786, preemption_count=0, score=3984.333630, test/loss=0.288274, test/num_examples=3581, test/ssim=0.739470, total_duration=4551.155753, train/loss=0.263582, train/ssim=0.743503, validation/loss=0.286945, validation/num_examples=3554, validation/ssim=0.722116
I1010 05:43:13.613453 140142928963328 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.335231, loss=0.223544
I1010 05:43:13.617611 140204662740800 pytorch_submission_base.py:86] 15000) loss = 0.224, grad_norm = 0.335
I1010 05:43:39.066939 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:43:41.096715 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:43:43.037979 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:43:44.944105 140204662740800 submission_runner.py:381] Time since start: 4637.57s, 	Step: 15092, 	{'train/ssim': 0.7447686195373535, 'train/loss': 0.26374360493251253, 'validation/ssim': 0.7233442951603827, 'validation/loss': 0.28717236795248313, 'validation/num_examples': 3554, 'test/ssim': 0.7406549568250838, 'test/loss': 0.2885999213514032, 'test/num_examples': 3581, 'score': 4063.8503143787384, 'total_duration': 4637.569295406342, 'accumulated_submission_time': 4063.8503143787384, 'accumulated_eval_time': 521.1766204833984, 'accumulated_logging_time': 2.9149417877197266}
I1010 05:43:44.964955 140142937356032 logging_writer.py:48] [15092] accumulated_eval_time=521.176620, accumulated_logging_time=2.914942, accumulated_submission_time=4063.850314, global_step=15092, preemption_count=0, score=4063.850314, test/loss=0.288600, test/num_examples=3581, test/ssim=0.740655, total_duration=4637.569295, train/loss=0.263744, train/ssim=0.744769, validation/loss=0.287172, validation/num_examples=3554, validation/ssim=0.723344
I1010 05:45:05.656057 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:45:07.635008 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:45:09.693831 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:45:11.678017 140204662740800 submission_runner.py:381] Time since start: 4724.30s, 	Step: 15398, 	{'train/ssim': 0.7463897977556501, 'train/loss': 0.26315672057015554, 'validation/ssim': 0.7247934078459131, 'validation/loss': 0.28690756742512835, 'validation/num_examples': 3554, 'test/ssim': 0.7418886816924393, 'test/loss': 0.28833232795483105, 'test/num_examples': 3581, 'score': 4143.545258760452, 'total_duration': 4724.303222417831, 'accumulated_submission_time': 4143.545258760452, 'accumulated_eval_time': 527.198808670044, 'accumulated_logging_time': 2.9450130462646484}
I1010 05:45:11.696882 140142928963328 logging_writer.py:48] [15398] accumulated_eval_time=527.198809, accumulated_logging_time=2.945013, accumulated_submission_time=4143.545259, global_step=15398, preemption_count=0, score=4143.545259, test/loss=0.288332, test/num_examples=3581, test/ssim=0.741889, total_duration=4724.303222, train/loss=0.263157, train/ssim=0.746390, validation/loss=0.286908, validation/num_examples=3554, validation/ssim=0.724793
I1010 05:45:36.703127 140142937356032 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.457886, loss=0.305582
I1010 05:45:36.706932 140204662740800 pytorch_submission_base.py:86] 15500) loss = 0.306, grad_norm = 0.458
I1010 05:46:32.323874 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:46:34.315341 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:46:36.277326 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:46:38.197883 140204662740800 submission_runner.py:381] Time since start: 4810.82s, 	Step: 15709, 	{'train/ssim': 0.7432845660618373, 'train/loss': 0.2642319883619036, 'validation/ssim': 0.7222652403102139, 'validation/loss': 0.28732933512415587, 'validation/num_examples': 3554, 'test/ssim': 0.7395440181251746, 'test/loss': 0.28876000015271575, 'test/num_examples': 3581, 'score': 4223.227202177048, 'total_duration': 4810.823029518127, 'accumulated_submission_time': 4223.227202177048, 'accumulated_eval_time': 533.0728666782379, 'accumulated_logging_time': 2.9728827476501465}
I1010 05:46:38.218549 140142928963328 logging_writer.py:48] [15709] accumulated_eval_time=533.072867, accumulated_logging_time=2.972883, accumulated_submission_time=4223.227202, global_step=15709, preemption_count=0, score=4223.227202, test/loss=0.288760, test/num_examples=3581, test/ssim=0.739544, total_duration=4810.823030, train/loss=0.264232, train/ssim=0.743285, validation/loss=0.287329, validation/num_examples=3554, validation/ssim=0.722265
I1010 05:47:54.249536 140142937356032 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.342372, loss=0.271411
I1010 05:47:54.253442 140204662740800 pytorch_submission_base.py:86] 16000) loss = 0.271, grad_norm = 0.342
I1010 05:47:58.818344 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:48:00.819154 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:48:02.906971 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:48:04.894612 140204662740800 submission_runner.py:381] Time since start: 4897.52s, 	Step: 16016, 	{'train/ssim': 0.7431158338274274, 'train/loss': 0.26372127873556955, 'validation/ssim': 0.7220782535963, 'validation/loss': 0.28694423317037143, 'validation/num_examples': 3554, 'test/ssim': 0.7393683950450293, 'test/loss': 0.2882672192430536, 'test/num_examples': 3581, 'score': 4302.822021484375, 'total_duration': 4897.519806623459, 'accumulated_submission_time': 4302.822021484375, 'accumulated_eval_time': 539.1492757797241, 'accumulated_logging_time': 3.0026814937591553}
I1010 05:48:04.913088 140142928963328 logging_writer.py:48] [16016] accumulated_eval_time=539.149276, accumulated_logging_time=3.002681, accumulated_submission_time=4302.822021, global_step=16016, preemption_count=0, score=4302.822021, test/loss=0.288267, test/num_examples=3581, test/ssim=0.739368, total_duration=4897.519807, train/loss=0.263721, train/ssim=0.743116, validation/loss=0.286944, validation/num_examples=3554, validation/ssim=0.722078
I1010 05:49:25.444830 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:49:27.400821 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:49:29.371891 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:49:31.265885 140204662740800 submission_runner.py:381] Time since start: 4983.89s, 	Step: 16323, 	{'train/ssim': 0.7440167835780552, 'train/loss': 0.26401967661721365, 'validation/ssim': 0.7221713347856289, 'validation/loss': 0.28767136556081174, 'validation/num_examples': 3554, 'test/ssim': 0.7394296858637601, 'test/loss': 0.2891141097327911, 'test/num_examples': 3581, 'score': 4382.375616788864, 'total_duration': 4983.891037940979, 'accumulated_submission_time': 4382.375616788864, 'accumulated_eval_time': 544.9705655574799, 'accumulated_logging_time': 3.0305049419403076}
I1010 05:49:31.284533 140142937356032 logging_writer.py:48] [16323] accumulated_eval_time=544.970566, accumulated_logging_time=3.030505, accumulated_submission_time=4382.375617, global_step=16323, preemption_count=0, score=4382.375617, test/loss=0.289114, test/num_examples=3581, test/ssim=0.739430, total_duration=4983.891038, train/loss=0.264020, train/ssim=0.744017, validation/loss=0.287671, validation/num_examples=3554, validation/ssim=0.722171
I1010 05:50:16.600539 140142928963328 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.207968, loss=0.294219
I1010 05:50:16.604841 140204662740800 pytorch_submission_base.py:86] 16500) loss = 0.294, grad_norm = 0.208
I1010 05:50:51.881863 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:50:53.834078 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:50:55.798375 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:50:57.709475 140204662740800 submission_runner.py:381] Time since start: 5070.33s, 	Step: 16631, 	{'train/ssim': 0.7450784274509975, 'train/loss': 0.26303240231105257, 'validation/ssim': 0.7235767577025887, 'validation/loss': 0.28655854731244723, 'validation/num_examples': 3554, 'test/ssim': 0.7409015518098995, 'test/loss': 0.2879321991282114, 'test/num_examples': 3581, 'score': 4461.961856603622, 'total_duration': 5070.334650039673, 'accumulated_submission_time': 4461.961856603622, 'accumulated_eval_time': 550.7984764575958, 'accumulated_logging_time': 3.05747652053833}
I1010 05:50:57.730745 140142937356032 logging_writer.py:48] [16631] accumulated_eval_time=550.798476, accumulated_logging_time=3.057477, accumulated_submission_time=4461.961857, global_step=16631, preemption_count=0, score=4461.961857, test/loss=0.287932, test/num_examples=3581, test/ssim=0.740902, total_duration=5070.334650, train/loss=0.263032, train/ssim=0.745078, validation/loss=0.286559, validation/num_examples=3554, validation/ssim=0.723577
I1010 05:52:18.299616 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:52:20.374268 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:52:22.412767 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:52:24.397673 140204662740800 submission_runner.py:381] Time since start: 5157.02s, 	Step: 16938, 	{'train/ssim': 0.7457123483930316, 'train/loss': 0.26295930998665945, 'validation/ssim': 0.7242161670828644, 'validation/loss': 0.2864360133113657, 'validation/num_examples': 3554, 'test/ssim': 0.7414781900132644, 'test/loss': 0.28780907207745743, 'test/num_examples': 3581, 'score': 4541.54456114769, 'total_duration': 5157.022856473923, 'accumulated_submission_time': 4541.54456114769, 'accumulated_eval_time': 556.8966844081879, 'accumulated_logging_time': 3.0879549980163574}
I1010 05:52:24.416633 140142928963328 logging_writer.py:48] [16938] accumulated_eval_time=556.896684, accumulated_logging_time=3.087955, accumulated_submission_time=4541.544561, global_step=16938, preemption_count=0, score=4541.544561, test/loss=0.287809, test/num_examples=3581, test/ssim=0.741478, total_duration=5157.022856, train/loss=0.262959, train/ssim=0.745712, validation/loss=0.286436, validation/num_examples=3554, validation/ssim=0.724216
I1010 05:52:38.855975 140142937356032 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.487144, loss=0.234675
I1010 05:52:38.860095 140204662740800 pytorch_submission_base.py:86] 17000) loss = 0.235, grad_norm = 0.487
I1010 05:53:44.923638 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:53:46.903149 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:53:48.867774 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:53:50.774030 140204662740800 submission_runner.py:381] Time since start: 5243.40s, 	Step: 17244, 	{'train/ssim': 0.746013709477016, 'train/loss': 0.26317875725882395, 'validation/ssim': 0.7245230945765335, 'validation/loss': 0.28655698451019096, 'validation/num_examples': 3554, 'test/ssim': 0.7417712133045937, 'test/loss': 0.2879459367255131, 'test/num_examples': 3581, 'score': 4621.06239938736, 'total_duration': 5243.3992438316345, 'accumulated_submission_time': 4621.06239938736, 'accumulated_eval_time': 562.7472848892212, 'accumulated_logging_time': 3.1151349544525146}
I1010 05:53:50.793270 140142928963328 logging_writer.py:48] [17244] accumulated_eval_time=562.747285, accumulated_logging_time=3.115135, accumulated_submission_time=4621.062399, global_step=17244, preemption_count=0, score=4621.062399, test/loss=0.287946, test/num_examples=3581, test/ssim=0.741771, total_duration=5243.399244, train/loss=0.263179, train/ssim=0.746014, validation/loss=0.286557, validation/num_examples=3554, validation/ssim=0.724523
I1010 05:54:57.328360 140142937356032 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.151813, loss=0.243631
I1010 05:54:57.331994 140204662740800 pytorch_submission_base.py:86] 17500) loss = 0.244, grad_norm = 0.152
I1010 05:55:11.329383 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:55:13.303795 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:55:15.268638 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:55:17.169297 140204662740800 submission_runner.py:381] Time since start: 5329.79s, 	Step: 17551, 	{'train/ssim': 0.7463387761797223, 'train/loss': 0.26255290848868235, 'validation/ssim': 0.7245207589599747, 'validation/loss': 0.2864756672719119, 'validation/num_examples': 3554, 'test/ssim': 0.7416914466105836, 'test/loss': 0.287917507057648, 'test/num_examples': 3581, 'score': 4700.621969461441, 'total_duration': 5329.79449057579, 'accumulated_submission_time': 4700.621969461441, 'accumulated_eval_time': 568.5874304771423, 'accumulated_logging_time': 3.1428062915802}
I1010 05:55:17.188813 140142928963328 logging_writer.py:48] [17551] accumulated_eval_time=568.587430, accumulated_logging_time=3.142806, accumulated_submission_time=4700.621969, global_step=17551, preemption_count=0, score=4700.621969, test/loss=0.287918, test/num_examples=3581, test/ssim=0.741691, total_duration=5329.794491, train/loss=0.262553, train/ssim=0.746339, validation/loss=0.286476, validation/num_examples=3554, validation/ssim=0.724521
I1010 05:56:37.648242 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:56:39.600683 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:56:41.588028 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:56:43.497914 140204662740800 submission_runner.py:381] Time since start: 5416.12s, 	Step: 17858, 	{'train/ssim': 0.7459402765546527, 'train/loss': 0.2630037409918649, 'validation/ssim': 0.7246268234295864, 'validation/loss': 0.2864508341723234, 'validation/num_examples': 3554, 'test/ssim': 0.7418222094474309, 'test/loss': 0.2878442171464849, 'test/num_examples': 3581, 'score': 4780.104678869247, 'total_duration': 5416.1230754852295, 'accumulated_submission_time': 4780.104678869247, 'accumulated_eval_time': 574.4372298717499, 'accumulated_logging_time': 3.1708579063415527}
I1010 05:56:43.520169 140142937356032 logging_writer.py:48] [17858] accumulated_eval_time=574.437230, accumulated_logging_time=3.170858, accumulated_submission_time=4780.104679, global_step=17858, preemption_count=0, score=4780.104679, test/loss=0.287844, test/num_examples=3581, test/ssim=0.741822, total_duration=5416.123075, train/loss=0.263004, train/ssim=0.745940, validation/loss=0.286451, validation/num_examples=3554, validation/ssim=0.724627
I1010 05:57:19.345276 140142928963328 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.322239, loss=0.305295
I1010 05:57:19.348839 140204662740800 pytorch_submission_base.py:86] 18000) loss = 0.305, grad_norm = 0.322
I1010 05:58:04.167342 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:58:06.213240 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:58:08.249131 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:58:10.237818 140204662740800 submission_runner.py:381] Time since start: 5502.86s, 	Step: 18166, 	{'train/ssim': 0.7443195751735142, 'train/loss': 0.2629752499716623, 'validation/ssim': 0.7229475151238042, 'validation/loss': 0.28638043937618707, 'validation/num_examples': 3554, 'test/ssim': 0.7403038470181165, 'test/loss': 0.2877339073081018, 'test/num_examples': 3581, 'score': 4859.739594697952, 'total_duration': 5502.862973690033, 'accumulated_submission_time': 4859.739594697952, 'accumulated_eval_time': 580.5079565048218, 'accumulated_logging_time': 3.202439785003662}
I1010 05:58:10.258147 140142937356032 logging_writer.py:48] [18166] accumulated_eval_time=580.507957, accumulated_logging_time=3.202440, accumulated_submission_time=4859.739595, global_step=18166, preemption_count=0, score=4859.739595, test/loss=0.287734, test/num_examples=3581, test/ssim=0.740304, total_duration=5502.862974, train/loss=0.262975, train/ssim=0.744320, validation/loss=0.286380, validation/num_examples=3554, validation/ssim=0.722948
I1010 05:59:30.926132 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 05:59:32.877873 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 05:59:34.839602 140204662740800 spec.py:349] Evaluating on the test split.
I1010 05:59:36.732558 140204662740800 submission_runner.py:381] Time since start: 5589.36s, 	Step: 18472, 	{'train/ssim': 0.744185038975307, 'train/loss': 0.26515139852251324, 'validation/ssim': 0.7224475557910102, 'validation/loss': 0.2887891298207126, 'validation/num_examples': 3554, 'test/ssim': 0.7396445105242949, 'test/loss': 0.2903313358480522, 'test/num_examples': 3581, 'score': 4939.421260595322, 'total_duration': 5589.357748508453, 'accumulated_submission_time': 4939.421260595322, 'accumulated_eval_time': 586.3145625591278, 'accumulated_logging_time': 3.2310616970062256}
I1010 05:59:36.752425 140142928963328 logging_writer.py:48] [18472] accumulated_eval_time=586.314563, accumulated_logging_time=3.231062, accumulated_submission_time=4939.421261, global_step=18472, preemption_count=0, score=4939.421261, test/loss=0.290331, test/num_examples=3581, test/ssim=0.739645, total_duration=5589.357749, train/loss=0.265151, train/ssim=0.744185, validation/loss=0.288789, validation/num_examples=3554, validation/ssim=0.722448
I1010 05:59:42.418343 140142937356032 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.350693, loss=0.265513
I1010 05:59:42.422818 140204662740800 pytorch_submission_base.py:86] 18500) loss = 0.266, grad_norm = 0.351
I1010 06:00:57.388771 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:00:59.346136 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:01:01.304693 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:01:03.269114 140204662740800 submission_runner.py:381] Time since start: 5675.89s, 	Step: 18775, 	{'train/ssim': 0.7457596915108817, 'train/loss': 0.2635413578578404, 'validation/ssim': 0.7250343198244935, 'validation/loss': 0.2869968875848516, 'validation/num_examples': 3554, 'test/ssim': 0.7417599641554384, 'test/loss': 0.28847001072282535, 'test/num_examples': 3581, 'score': 5019.038831949234, 'total_duration': 5675.894321203232, 'accumulated_submission_time': 5019.038831949234, 'accumulated_eval_time': 592.1952176094055, 'accumulated_logging_time': 3.2596919536590576}
I1010 06:01:03.288484 140142928963328 logging_writer.py:48] [18775] accumulated_eval_time=592.195218, accumulated_logging_time=3.259692, accumulated_submission_time=5019.038832, global_step=18775, preemption_count=0, score=5019.038832, test/loss=0.288470, test/num_examples=3581, test/ssim=0.741760, total_duration=5675.894321, train/loss=0.263541, train/ssim=0.745760, validation/loss=0.286997, validation/num_examples=3554, validation/ssim=0.725034
I1010 06:02:01.541909 140142937356032 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.314376, loss=0.311719
I1010 06:02:01.570641 140204662740800 pytorch_submission_base.py:86] 19000) loss = 0.312, grad_norm = 0.314
I1010 06:02:23.957918 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:02:25.974610 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:02:27.936825 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:02:29.844450 140204662740800 submission_runner.py:381] Time since start: 5762.47s, 	Step: 19082, 	{'train/ssim': 0.7468434061322894, 'train/loss': 0.26240226200648714, 'validation/ssim': 0.7255028170283483, 'validation/loss': 0.285969783029465, 'validation/num_examples': 3554, 'test/ssim': 0.7426817807962162, 'test/loss': 0.2872967244659313, 'test/num_examples': 3581, 'score': 5098.7637894153595, 'total_duration': 5762.46964263916, 'accumulated_submission_time': 5098.7637894153595, 'accumulated_eval_time': 598.0819914340973, 'accumulated_logging_time': 3.287536144256592}
I1010 06:02:29.864119 140142928963328 logging_writer.py:48] [19082] accumulated_eval_time=598.081991, accumulated_logging_time=3.287536, accumulated_submission_time=5098.763789, global_step=19082, preemption_count=0, score=5098.763789, test/loss=0.287297, test/num_examples=3581, test/ssim=0.742682, total_duration=5762.469643, train/loss=0.262402, train/ssim=0.746843, validation/loss=0.285970, validation/num_examples=3554, validation/ssim=0.725503
I1010 06:03:50.602585 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:03:52.555591 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:03:54.502133 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:03:56.407677 140204662740800 submission_runner.py:381] Time since start: 5849.03s, 	Step: 19389, 	{'train/ssim': 0.7443292481558663, 'train/loss': 0.26278761454990934, 'validation/ssim': 0.7229194190304938, 'validation/loss': 0.28624105802331, 'validation/num_examples': 3554, 'test/ssim': 0.7402972338819463, 'test/loss': 0.2875300931784069, 'test/num_examples': 3581, 'score': 5178.509335517883, 'total_duration': 5849.032888174057, 'accumulated_submission_time': 5178.509335517883, 'accumulated_eval_time': 603.8873469829559, 'accumulated_logging_time': 3.3157379627227783}
I1010 06:03:56.426622 140142937356032 logging_writer.py:48] [19389] accumulated_eval_time=603.887347, accumulated_logging_time=3.315738, accumulated_submission_time=5178.509336, global_step=19389, preemption_count=0, score=5178.509336, test/loss=0.287530, test/num_examples=3581, test/ssim=0.740297, total_duration=5849.032888, train/loss=0.262788, train/ssim=0.744329, validation/loss=0.286241, validation/num_examples=3554, validation/ssim=0.722919
I1010 06:04:24.221074 140142928963328 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.616335, loss=0.245042
I1010 06:04:24.224434 140204662740800 pytorch_submission_base.py:86] 19500) loss = 0.245, grad_norm = 0.616
I1010 06:05:16.992626 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:05:18.935784 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:05:20.910256 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:05:22.819484 140204662740800 submission_runner.py:381] Time since start: 5935.44s, 	Step: 19696, 	{'train/ssim': 0.7462765829903739, 'train/loss': 0.2621185098375593, 'validation/ssim': 0.7243356270003869, 'validation/loss': 0.2860813430674504, 'validation/num_examples': 3554, 'test/ssim': 0.741581068595539, 'test/loss': 0.2874615415452213, 'test/num_examples': 3581, 'score': 5258.099138259888, 'total_duration': 5935.444639921188, 'accumulated_submission_time': 5258.099138259888, 'accumulated_eval_time': 609.714390039444, 'accumulated_logging_time': 3.343081474304199}
I1010 06:05:22.841329 140142937356032 logging_writer.py:48] [19696] accumulated_eval_time=609.714390, accumulated_logging_time=3.343081, accumulated_submission_time=5258.099138, global_step=19696, preemption_count=0, score=5258.099138, test/loss=0.287462, test/num_examples=3581, test/ssim=0.741581, total_duration=5935.444640, train/loss=0.262119, train/ssim=0.746277, validation/loss=0.286081, validation/num_examples=3554, validation/ssim=0.724336
I1010 06:06:41.970542 140142928963328 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.385153, loss=0.203606
I1010 06:06:41.974877 140204662740800 pytorch_submission_base.py:86] 20000) loss = 0.204, grad_norm = 0.385
I1010 06:06:43.445437 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:06:45.499499 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:06:47.537462 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:06:49.519616 140204662740800 submission_runner.py:381] Time since start: 6022.14s, 	Step: 20005, 	{'train/ssim': 0.745769909449986, 'train/loss': 0.26284922872270855, 'validation/ssim': 0.7243343904975028, 'validation/loss': 0.2863809889330244, 'validation/num_examples': 3554, 'test/ssim': 0.7415886362049707, 'test/loss': 0.28776690481229056, 'test/num_examples': 3581, 'score': 5337.687087774277, 'total_duration': 6022.144808292389, 'accumulated_submission_time': 5337.687087774277, 'accumulated_eval_time': 615.7887609004974, 'accumulated_logging_time': 3.373713254928589}
I1010 06:06:49.538424 140142937356032 logging_writer.py:48] [20005] accumulated_eval_time=615.788761, accumulated_logging_time=3.373713, accumulated_submission_time=5337.687088, global_step=20005, preemption_count=0, score=5337.687088, test/loss=0.287767, test/num_examples=3581, test/ssim=0.741589, total_duration=6022.144808, train/loss=0.262849, train/ssim=0.745770, validation/loss=0.286381, validation/num_examples=3554, validation/ssim=0.724334
I1010 06:08:10.036526 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:08:11.985218 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:08:13.954800 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:08:15.857666 140204662740800 submission_runner.py:381] Time since start: 6108.48s, 	Step: 20313, 	{'train/ssim': 0.7457877567836216, 'train/loss': 0.2624605553490775, 'validation/ssim': 0.7245521523943093, 'validation/loss': 0.2859537428392744, 'validation/num_examples': 3554, 'test/ssim': 0.741810755768291, 'test/loss': 0.2872609317186191, 'test/num_examples': 3581, 'score': 5417.196914434433, 'total_duration': 6108.482858181, 'accumulated_submission_time': 5417.196914434433, 'accumulated_eval_time': 621.6101474761963, 'accumulated_logging_time': 3.4022324085235596}
I1010 06:08:15.876634 140142928963328 logging_writer.py:48] [20313] accumulated_eval_time=621.610147, accumulated_logging_time=3.402232, accumulated_submission_time=5417.196914, global_step=20313, preemption_count=0, score=5417.196914, test/loss=0.287261, test/num_examples=3581, test/ssim=0.741811, total_duration=6108.482858, train/loss=0.262461, train/ssim=0.745788, validation/loss=0.285954, validation/num_examples=3554, validation/ssim=0.724552
I1010 06:09:04.503928 140142937356032 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.267995, loss=0.310691
I1010 06:09:04.507561 140204662740800 pytorch_submission_base.py:86] 20500) loss = 0.311, grad_norm = 0.268
I1010 06:09:36.416749 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:09:38.361302 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:09:40.336408 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:09:42.233036 140204662740800 submission_runner.py:381] Time since start: 6194.86s, 	Step: 20618, 	{'train/ssim': 0.7447492054530552, 'train/loss': 0.26309006554739817, 'validation/ssim': 0.7232288882245357, 'validation/loss': 0.2866473694362866, 'validation/num_examples': 3554, 'test/ssim': 0.7406678422141162, 'test/loss': 0.2880393387518326, 'test/num_examples': 3581, 'score': 5496.766225814819, 'total_duration': 6194.858247280121, 'accumulated_submission_time': 5496.766225814819, 'accumulated_eval_time': 627.4265532493591, 'accumulated_logging_time': 3.430436134338379}
I1010 06:09:42.252668 140142928963328 logging_writer.py:48] [20618] accumulated_eval_time=627.426553, accumulated_logging_time=3.430436, accumulated_submission_time=5496.766226, global_step=20618, preemption_count=0, score=5496.766226, test/loss=0.288039, test/num_examples=3581, test/ssim=0.740668, total_duration=6194.858247, train/loss=0.263090, train/ssim=0.744749, validation/loss=0.286647, validation/num_examples=3554, validation/ssim=0.723229
I1010 06:11:02.983206 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:11:04.952568 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:11:06.911717 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:11:08.817603 140204662740800 submission_runner.py:381] Time since start: 6281.44s, 	Step: 20926, 	{'train/ssim': 0.7461234501429966, 'train/loss': 0.2620681013379778, 'validation/ssim': 0.7239641952729319, 'validation/loss': 0.28603586723915836, 'validation/num_examples': 3554, 'test/ssim': 0.7413451091699246, 'test/loss': 0.2873622763260088, 'test/num_examples': 3581, 'score': 5576.531705617905, 'total_duration': 6281.442793130875, 'accumulated_submission_time': 5576.531705617905, 'accumulated_eval_time': 633.2610561847687, 'accumulated_logging_time': 3.4589145183563232}
I1010 06:11:08.836866 140142937356032 logging_writer.py:48] [20926] accumulated_eval_time=633.261056, accumulated_logging_time=3.458915, accumulated_submission_time=5576.531706, global_step=20926, preemption_count=0, score=5576.531706, test/loss=0.287362, test/num_examples=3581, test/ssim=0.741345, total_duration=6281.442793, train/loss=0.262068, train/ssim=0.746123, validation/loss=0.286036, validation/num_examples=3554, validation/ssim=0.723964
I1010 06:11:26.612239 140142928963328 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.243359, loss=0.365925
I1010 06:11:26.616424 140204662740800 pytorch_submission_base.py:86] 21000) loss = 0.366, grad_norm = 0.243
I1010 06:12:29.308805 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:12:31.273940 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:12:33.241060 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:12:35.150110 140204662740800 submission_runner.py:381] Time since start: 6367.78s, 	Step: 21233, 	{'train/ssim': 0.7469345501491002, 'train/loss': 0.2620537962232317, 'validation/ssim': 0.7254315120287, 'validation/loss': 0.28567224952298464, 'validation/num_examples': 3554, 'test/ssim': 0.7426571690213977, 'test/loss': 0.28699899698495535, 'test/num_examples': 3581, 'score': 5656.052699327469, 'total_duration': 6367.775305509567, 'accumulated_submission_time': 5656.052699327469, 'accumulated_eval_time': 639.1026182174683, 'accumulated_logging_time': 3.4876201152801514}
I1010 06:12:35.169193 140142937356032 logging_writer.py:48] [21233] accumulated_eval_time=639.102618, accumulated_logging_time=3.487620, accumulated_submission_time=5656.052699, global_step=21233, preemption_count=0, score=5656.052699, test/loss=0.286999, test/num_examples=3581, test/ssim=0.742657, total_duration=6367.775306, train/loss=0.262054, train/ssim=0.746935, validation/loss=0.285672, validation/num_examples=3554, validation/ssim=0.725432
I1010 06:13:43.987638 140142928963328 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.275547, loss=0.223708
I1010 06:13:43.991711 140204662740800 pytorch_submission_base.py:86] 21500) loss = 0.224, grad_norm = 0.276
I1010 06:13:55.646729 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:13:57.607564 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:13:59.573466 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:14:01.537468 140204662740800 submission_runner.py:381] Time since start: 6454.16s, 	Step: 21543, 	{'train/ssim': 0.7464140483311245, 'train/loss': 0.2621051583971296, 'validation/ssim': 0.7248294038187606, 'validation/loss': 0.28575688127593907, 'validation/num_examples': 3554, 'test/ssim': 0.7421052107695127, 'test/loss': 0.28706260581017873, 'test/num_examples': 3581, 'score': 5735.501694917679, 'total_duration': 6454.1626060009, 'accumulated_submission_time': 5735.501694917679, 'accumulated_eval_time': 644.9934492111206, 'accumulated_logging_time': 3.516158103942871}
I1010 06:14:01.562096 140142937356032 logging_writer.py:48] [21543] accumulated_eval_time=644.993449, accumulated_logging_time=3.516158, accumulated_submission_time=5735.501695, global_step=21543, preemption_count=0, score=5735.501695, test/loss=0.287063, test/num_examples=3581, test/ssim=0.742105, total_duration=6454.162606, train/loss=0.262105, train/ssim=0.746414, validation/loss=0.285757, validation/num_examples=3554, validation/ssim=0.724829
I1010 06:15:22.207434 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:15:24.261932 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:15:26.301006 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:15:28.286101 140204662740800 submission_runner.py:381] Time since start: 6540.91s, 	Step: 21850, 	{'train/ssim': 0.7471836635044643, 'train/loss': 0.26160201004573275, 'validation/ssim': 0.725101846620885, 'validation/loss': 0.28575561042575265, 'validation/num_examples': 3554, 'test/ssim': 0.7423863031450713, 'test/loss': 0.2870889560898667, 'test/num_examples': 3581, 'score': 5815.112927436829, 'total_duration': 6540.911270856857, 'accumulated_submission_time': 5815.112927436829, 'accumulated_eval_time': 651.0722677707672, 'accumulated_logging_time': 3.5507688522338867}
I1010 06:15:28.306279 140142928963328 logging_writer.py:48] [21850] accumulated_eval_time=651.072268, accumulated_logging_time=3.550769, accumulated_submission_time=5815.112927, global_step=21850, preemption_count=0, score=5815.112927, test/loss=0.287089, test/num_examples=3581, test/ssim=0.742386, total_duration=6540.911271, train/loss=0.261602, train/ssim=0.747184, validation/loss=0.285756, validation/num_examples=3554, validation/ssim=0.725102
I1010 06:16:06.567496 140142937356032 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.540397, loss=0.243662
I1010 06:16:06.571686 140204662740800 pytorch_submission_base.py:86] 22000) loss = 0.244, grad_norm = 0.540
I1010 06:16:48.905665 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:16:50.924042 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:16:52.879334 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:16:54.785339 140204662740800 submission_runner.py:381] Time since start: 6627.41s, 	Step: 22159, 	{'train/ssim': 0.7451338768005371, 'train/loss': 0.2624304975782122, 'validation/ssim': 0.7232994375835327, 'validation/loss': 0.28618429910620075, 'validation/num_examples': 3554, 'test/ssim': 0.7405994610225844, 'test/loss': 0.28751710552438214, 'test/num_examples': 3581, 'score': 5894.767419338226, 'total_duration': 6627.410543203354, 'accumulated_submission_time': 5894.767419338226, 'accumulated_eval_time': 656.9522054195404, 'accumulated_logging_time': 3.580577850341797}
I1010 06:16:54.805063 140142928963328 logging_writer.py:48] [22159] accumulated_eval_time=656.952205, accumulated_logging_time=3.580578, accumulated_submission_time=5894.767419, global_step=22159, preemption_count=0, score=5894.767419, test/loss=0.287517, test/num_examples=3581, test/ssim=0.740599, total_duration=6627.410543, train/loss=0.262430, train/ssim=0.745134, validation/loss=0.286184, validation/num_examples=3554, validation/ssim=0.723299
I1010 06:18:15.248198 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:18:17.188156 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:18:19.134568 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:18:21.035242 140204662740800 submission_runner.py:381] Time since start: 6713.66s, 	Step: 22467, 	{'train/ssim': 0.7475487845284599, 'train/loss': 0.26196428707667757, 'validation/ssim': 0.7258721192230585, 'validation/loss': 0.2856694845651467, 'validation/num_examples': 3554, 'test/ssim': 0.7431370645420273, 'test/loss': 0.2869915998171775, 'test/num_examples': 3581, 'score': 5974.2579870224, 'total_duration': 6713.660455465317, 'accumulated_submission_time': 5974.2579870224, 'accumulated_eval_time': 662.739494562149, 'accumulated_logging_time': 3.609541893005371}
I1010 06:18:21.054535 140142937356032 logging_writer.py:48] [22467] accumulated_eval_time=662.739495, accumulated_logging_time=3.609542, accumulated_submission_time=5974.257987, global_step=22467, preemption_count=0, score=5974.257987, test/loss=0.286992, test/num_examples=3581, test/ssim=0.743137, total_duration=6713.660455, train/loss=0.261964, train/ssim=0.747549, validation/loss=0.285669, validation/num_examples=3554, validation/ssim=0.725872
I1010 06:18:27.987073 140142928963328 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.229092, loss=0.250025
I1010 06:18:27.990750 140204662740800 pytorch_submission_base.py:86] 22500) loss = 0.250, grad_norm = 0.229
I1010 06:19:41.573777 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:19:43.529019 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:19:45.498254 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:19:47.402868 140204662740800 submission_runner.py:381] Time since start: 6800.03s, 	Step: 22772, 	{'train/ssim': 0.7463259015764508, 'train/loss': 0.26202118396759033, 'validation/ssim': 0.7247678534529756, 'validation/loss': 0.28566333639802866, 'validation/num_examples': 3554, 'test/ssim': 0.7420236033056409, 'test/loss': 0.2870305627792516, 'test/num_examples': 3581, 'score': 6053.788078546524, 'total_duration': 6800.0280747413635, 'accumulated_submission_time': 6053.788078546524, 'accumulated_eval_time': 668.5687086582184, 'accumulated_logging_time': 3.6376843452453613}
I1010 06:19:47.422657 140142937356032 logging_writer.py:48] [22772] accumulated_eval_time=668.568709, accumulated_logging_time=3.637684, accumulated_submission_time=6053.788079, global_step=22772, preemption_count=0, score=6053.788079, test/loss=0.287031, test/num_examples=3581, test/ssim=0.742024, total_duration=6800.028075, train/loss=0.262021, train/ssim=0.746326, validation/loss=0.285663, validation/num_examples=3554, validation/ssim=0.724768
I1010 06:20:46.282133 140142928963328 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.344308, loss=0.321622
I1010 06:20:46.286391 140204662740800 pytorch_submission_base.py:86] 23000) loss = 0.322, grad_norm = 0.344
I1010 06:21:07.961982 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:21:09.904568 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:21:11.864989 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:21:13.765008 140204662740800 submission_runner.py:381] Time since start: 6886.39s, 	Step: 23080, 	{'train/ssim': 0.7471797125680106, 'train/loss': 0.2612269265311105, 'validation/ssim': 0.7250651637019907, 'validation/loss': 0.2853362642115398, 'validation/num_examples': 3554, 'test/ssim': 0.7423631230801452, 'test/loss': 0.28666786293982127, 'test/num_examples': 3581, 'score': 6133.37411904335, 'total_duration': 6886.390181541443, 'accumulated_submission_time': 6133.37411904335, 'accumulated_eval_time': 674.3718917369843, 'accumulated_logging_time': 3.6659483909606934}
I1010 06:21:13.785445 140142937356032 logging_writer.py:48] [23080] accumulated_eval_time=674.371892, accumulated_logging_time=3.665948, accumulated_submission_time=6133.374119, global_step=23080, preemption_count=0, score=6133.374119, test/loss=0.286668, test/num_examples=3581, test/ssim=0.742363, total_duration=6886.390182, train/loss=0.261227, train/ssim=0.747180, validation/loss=0.285336, validation/num_examples=3554, validation/ssim=0.725065
I1010 06:22:34.266161 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:22:36.268051 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:22:38.241348 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:22:40.147492 140204662740800 submission_runner.py:381] Time since start: 6972.77s, 	Step: 23389, 	{'train/ssim': 0.7469581876482282, 'train/loss': 0.26153632572719027, 'validation/ssim': 0.7252407471115293, 'validation/loss': 0.28531709841683667, 'validation/num_examples': 3554, 'test/ssim': 0.742448343907079, 'test/loss': 0.2866669084665596, 'test/num_examples': 3581, 'score': 6212.9061233997345, 'total_duration': 6972.772681236267, 'accumulated_submission_time': 6212.9061233997345, 'accumulated_eval_time': 680.253331899643, 'accumulated_logging_time': 3.6957108974456787}
I1010 06:22:40.168205 140142928963328 logging_writer.py:48] [23389] accumulated_eval_time=680.253332, accumulated_logging_time=3.695711, accumulated_submission_time=6212.906123, global_step=23389, preemption_count=0, score=6212.906123, test/loss=0.286667, test/num_examples=3581, test/ssim=0.742448, total_duration=6972.772681, train/loss=0.261536, train/ssim=0.746958, validation/loss=0.285317, validation/num_examples=3554, validation/ssim=0.725241
I1010 06:23:08.070621 140142937356032 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.376115, loss=0.266319
I1010 06:23:08.074319 140204662740800 pytorch_submission_base.py:86] 23500) loss = 0.266, grad_norm = 0.376
I1010 06:24:00.808134 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:24:02.832406 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:24:04.808557 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:24:06.715932 140204662740800 submission_runner.py:381] Time since start: 7059.34s, 	Step: 23697, 	{'train/ssim': 0.7469154085431781, 'train/loss': 0.2614326477050781, 'validation/ssim': 0.7251507571794106, 'validation/loss': 0.2852404695853264, 'validation/num_examples': 3554, 'test/ssim': 0.742470774028728, 'test/loss': 0.28652656680876504, 'test/num_examples': 3581, 'score': 6292.5494956970215, 'total_duration': 7059.341114521027, 'accumulated_submission_time': 6292.5494956970215, 'accumulated_eval_time': 686.161438703537, 'accumulated_logging_time': 3.7250707149505615}
I1010 06:24:06.736437 140142928963328 logging_writer.py:48] [23697] accumulated_eval_time=686.161439, accumulated_logging_time=3.725071, accumulated_submission_time=6292.549496, global_step=23697, preemption_count=0, score=6292.549496, test/loss=0.286527, test/num_examples=3581, test/ssim=0.742471, total_duration=7059.341115, train/loss=0.261433, train/ssim=0.746915, validation/loss=0.285240, validation/num_examples=3554, validation/ssim=0.725151
I1010 06:25:26.738817 140142937356032 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.391358, loss=0.213693
I1010 06:25:26.743722 140204662740800 pytorch_submission_base.py:86] 24000) loss = 0.214, grad_norm = 0.391
I1010 06:25:27.185599 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:25:29.166801 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:25:31.147627 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:25:33.056742 140204662740800 submission_runner.py:381] Time since start: 7145.68s, 	Step: 24001, 	{'train/ssim': 0.7474967411586216, 'train/loss': 0.2611254964556013, 'validation/ssim': 0.7253958595288759, 'validation/loss': 0.28532589132623454, 'validation/num_examples': 3554, 'test/ssim': 0.742609854418284, 'test/loss': 0.2866861342851159, 'test/num_examples': 3581, 'score': 6372.017669200897, 'total_duration': 7145.681921243668, 'accumulated_submission_time': 6372.017669200897, 'accumulated_eval_time': 692.0328688621521, 'accumulated_logging_time': 3.7542219161987305}
I1010 06:25:33.078440 140142928963328 logging_writer.py:48] [24001] accumulated_eval_time=692.032869, accumulated_logging_time=3.754222, accumulated_submission_time=6372.017669, global_step=24001, preemption_count=0, score=6372.017669, test/loss=0.286686, test/num_examples=3581, test/ssim=0.742610, total_duration=7145.681921, train/loss=0.261125, train/ssim=0.747497, validation/loss=0.285326, validation/num_examples=3554, validation/ssim=0.725396
I1010 06:26:53.692224 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:26:55.672816 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:26:57.634250 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:26:59.529880 140204662740800 submission_runner.py:381] Time since start: 7232.16s, 	Step: 24310, 	{'train/ssim': 0.7471049172537667, 'train/loss': 0.2611891371863229, 'validation/ssim': 0.7250921606816263, 'validation/loss': 0.2852296501850907, 'validation/num_examples': 3554, 'test/ssim': 0.7423481242146048, 'test/loss': 0.2865496446086987, 'test/num_examples': 3581, 'score': 6451.636000394821, 'total_duration': 7232.155041694641, 'accumulated_submission_time': 6451.636000394821, 'accumulated_eval_time': 697.8706994056702, 'accumulated_logging_time': 3.7841715812683105}
I1010 06:26:59.550768 140142937356032 logging_writer.py:48] [24310] accumulated_eval_time=697.870699, accumulated_logging_time=3.784172, accumulated_submission_time=6451.636000, global_step=24310, preemption_count=0, score=6451.636000, test/loss=0.286550, test/num_examples=3581, test/ssim=0.742348, total_duration=7232.155042, train/loss=0.261189, train/ssim=0.747105, validation/loss=0.285230, validation/num_examples=3554, validation/ssim=0.725092
I1010 06:27:48.391226 140142928963328 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.301011, loss=0.381813
I1010 06:27:48.394748 140204662740800 pytorch_submission_base.py:86] 24500) loss = 0.382, grad_norm = 0.301
I1010 06:28:20.230327 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:28:22.177463 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:28:24.136364 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:28:26.033883 140204662740800 submission_runner.py:381] Time since start: 7318.66s, 	Step: 24617, 	{'train/ssim': 0.7481568200247628, 'train/loss': 0.2611300264086042, 'validation/ssim': 0.7263775741242262, 'validation/loss': 0.2850739538636044, 'validation/num_examples': 3554, 'test/ssim': 0.7435450336847249, 'test/loss': 0.28636103387452877, 'test/num_examples': 3581, 'score': 6531.296428442001, 'total_duration': 7318.659018993378, 'accumulated_submission_time': 6531.296428442001, 'accumulated_eval_time': 703.6744980812073, 'accumulated_logging_time': 3.813491106033325}
I1010 06:28:26.058512 140142937356032 logging_writer.py:48] [24617] accumulated_eval_time=703.674498, accumulated_logging_time=3.813491, accumulated_submission_time=6531.296428, global_step=24617, preemption_count=0, score=6531.296428, test/loss=0.286361, test/num_examples=3581, test/ssim=0.743545, total_duration=7318.659019, train/loss=0.261130, train/ssim=0.748157, validation/loss=0.285074, validation/num_examples=3554, validation/ssim=0.726378
I1010 06:29:46.512609 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:29:48.544707 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:29:50.573744 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:29:52.552771 140204662740800 submission_runner.py:381] Time since start: 7405.18s, 	Step: 24924, 	{'train/ssim': 0.7476644515991211, 'train/loss': 0.2610858167920794, 'validation/ssim': 0.725747026347953, 'validation/loss': 0.28504005307619934, 'validation/num_examples': 3554, 'test/ssim': 0.7430084833583496, 'test/loss': 0.28638578200267034, 'test/num_examples': 3581, 'score': 6610.758322954178, 'total_duration': 7405.177961349487, 'accumulated_submission_time': 6610.758322954178, 'accumulated_eval_time': 709.7149248123169, 'accumulated_logging_time': 3.8477327823638916}
I1010 06:29:52.572643 140142928963328 logging_writer.py:48] [24924] accumulated_eval_time=709.714925, accumulated_logging_time=3.847733, accumulated_submission_time=6610.758323, global_step=24924, preemption_count=0, score=6610.758323, test/loss=0.286386, test/num_examples=3581, test/ssim=0.743008, total_duration=7405.177961, train/loss=0.261086, train/ssim=0.747664, validation/loss=0.285040, validation/num_examples=3554, validation/ssim=0.725747
I1010 06:30:10.999036 140142937356032 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.272407, loss=0.302706
I1010 06:30:11.002543 140204662740800 pytorch_submission_base.py:86] 25000) loss = 0.303, grad_norm = 0.272
I1010 06:31:13.096495 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:31:15.032237 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:31:16.981863 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:31:18.876900 140204662740800 submission_runner.py:381] Time since start: 7491.50s, 	Step: 25231, 	{'train/ssim': 0.7486874716622489, 'train/loss': 0.2604835203715733, 'validation/ssim': 0.7263664455982696, 'validation/loss': 0.2848612925411948, 'validation/num_examples': 3554, 'test/ssim': 0.7435075365208741, 'test/loss': 0.2862691317347633, 'test/num_examples': 3581, 'score': 6690.337490081787, 'total_duration': 7491.502110242844, 'accumulated_submission_time': 6690.337490081787, 'accumulated_eval_time': 715.4954693317413, 'accumulated_logging_time': 3.877150297164917}
I1010 06:31:18.897761 140142928963328 logging_writer.py:48] [25231] accumulated_eval_time=715.495469, accumulated_logging_time=3.877150, accumulated_submission_time=6690.337490, global_step=25231, preemption_count=0, score=6690.337490, test/loss=0.286269, test/num_examples=3581, test/ssim=0.743508, total_duration=7491.502110, train/loss=0.260484, train/ssim=0.748687, validation/loss=0.284861, validation/num_examples=3554, validation/ssim=0.726366
I1010 06:32:29.265675 140142937356032 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.266452, loss=0.231003
I1010 06:32:29.270644 140204662740800 pytorch_submission_base.py:86] 25500) loss = 0.231, grad_norm = 0.266
I1010 06:32:39.441009 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:32:41.417960 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:32:43.382624 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:32:45.290802 140204662740800 submission_runner.py:381] Time since start: 7577.92s, 	Step: 25538, 	{'train/ssim': 0.7481859752110073, 'train/loss': 0.2606891393661499, 'validation/ssim': 0.7260428253156654, 'validation/loss': 0.2848250561372309, 'validation/num_examples': 3554, 'test/ssim': 0.7433212778815275, 'test/loss': 0.28612861963531483, 'test/num_examples': 3581, 'score': 6769.9041385650635, 'total_duration': 7577.916015148163, 'accumulated_submission_time': 6769.9041385650635, 'accumulated_eval_time': 721.3455908298492, 'accumulated_logging_time': 3.9066436290740967}
I1010 06:32:45.311327 140142928963328 logging_writer.py:48] [25538] accumulated_eval_time=721.345591, accumulated_logging_time=3.906644, accumulated_submission_time=6769.904139, global_step=25538, preemption_count=0, score=6769.904139, test/loss=0.286129, test/num_examples=3581, test/ssim=0.743321, total_duration=7577.916015, train/loss=0.260689, train/ssim=0.748186, validation/loss=0.284825, validation/num_examples=3554, validation/ssim=0.726043
I1010 06:34:05.781971 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:34:07.767499 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:34:09.738684 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:34:11.643195 140204662740800 submission_runner.py:381] Time since start: 7664.27s, 	Step: 25844, 	{'train/ssim': 0.7483651978628976, 'train/loss': 0.26065937110355925, 'validation/ssim': 0.7263425398758441, 'validation/loss': 0.28474755144951114, 'validation/num_examples': 3554, 'test/ssim': 0.7435469426312482, 'test/loss': 0.2860300702710486, 'test/num_examples': 3581, 'score': 6849.424270868301, 'total_duration': 7664.268394708633, 'accumulated_submission_time': 6849.424270868301, 'accumulated_eval_time': 727.2069861888885, 'accumulated_logging_time': 3.9364676475524902}
I1010 06:34:11.664149 140142937356032 logging_writer.py:48] [25844] accumulated_eval_time=727.206986, accumulated_logging_time=3.936468, accumulated_submission_time=6849.424271, global_step=25844, preemption_count=0, score=6849.424271, test/loss=0.286030, test/num_examples=3581, test/ssim=0.743547, total_duration=7664.268395, train/loss=0.260659, train/ssim=0.748365, validation/loss=0.284748, validation/num_examples=3554, validation/ssim=0.726343
I1010 06:34:51.893121 140142928963328 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.258146, loss=0.189223
I1010 06:34:51.896859 140204662740800 pytorch_submission_base.py:86] 26000) loss = 0.189, grad_norm = 0.258
I1010 06:35:32.140551 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:35:34.139119 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:35:36.105001 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:35:38.012366 140204662740800 submission_runner.py:381] Time since start: 7750.64s, 	Step: 26147, 	{'train/ssim': 0.7490249361310687, 'train/loss': 0.2603274754115513, 'validation/ssim': 0.7267319695897229, 'validation/loss': 0.28476008821486354, 'validation/num_examples': 3554, 'test/ssim': 0.7438592599177953, 'test/loss': 0.2861611399028728, 'test/num_examples': 3581, 'score': 6928.9466807842255, 'total_duration': 7750.637572050095, 'accumulated_submission_time': 6928.9466807842255, 'accumulated_eval_time': 733.079066991806, 'accumulated_logging_time': 3.96708607673645}
I1010 06:35:38.033210 140142937356032 logging_writer.py:48] [26147] accumulated_eval_time=733.079067, accumulated_logging_time=3.967086, accumulated_submission_time=6928.946681, global_step=26147, preemption_count=0, score=6928.946681, test/loss=0.286161, test/num_examples=3581, test/ssim=0.743859, total_duration=7750.637572, train/loss=0.260327, train/ssim=0.749025, validation/loss=0.284760, validation/num_examples=3554, validation/ssim=0.726732
I1010 06:36:58.646326 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:37:00.618813 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:37:02.643890 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:37:04.549001 140204662740800 submission_runner.py:381] Time since start: 7837.17s, 	Step: 26454, 	{'train/ssim': 0.7490370614188058, 'train/loss': 0.26014011246817453, 'validation/ssim': 0.7266109983908976, 'validation/loss': 0.2845534548440138, 'validation/num_examples': 3554, 'test/ssim': 0.7438636232241344, 'test/loss': 0.2858977905034819, 'test/num_examples': 3581, 'score': 7008.612009525299, 'total_duration': 7837.174219369888, 'accumulated_submission_time': 7008.612009525299, 'accumulated_eval_time': 738.9819419384003, 'accumulated_logging_time': 3.996572494506836}
I1010 06:37:04.570154 140142928963328 logging_writer.py:48] [26454] accumulated_eval_time=738.981942, accumulated_logging_time=3.996572, accumulated_submission_time=7008.612010, global_step=26454, preemption_count=0, score=7008.612010, test/loss=0.285898, test/num_examples=3581, test/ssim=0.743864, total_duration=7837.174219, train/loss=0.260140, train/ssim=0.749037, validation/loss=0.284553, validation/num_examples=3554, validation/ssim=0.726611
I1010 06:37:15.087228 140142937356032 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.245453, loss=0.256711
I1010 06:37:15.090885 140204662740800 pytorch_submission_base.py:86] 26500) loss = 0.257, grad_norm = 0.245
I1010 06:38:25.185662 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:38:27.136871 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:38:29.107413 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:38:31.008971 140204662740800 submission_runner.py:381] Time since start: 7923.63s, 	Step: 26759, 	{'train/ssim': 0.7491796357291085, 'train/loss': 0.25998570237840923, 'validation/ssim': 0.7268591233029685, 'validation/loss': 0.28432683134320486, 'validation/num_examples': 3554, 'test/ssim': 0.7440969237599483, 'test/loss': 0.28561923770856606, 'test/num_examples': 3581, 'score': 7088.215101242065, 'total_duration': 7923.634158849716, 'accumulated_submission_time': 7088.215101242065, 'accumulated_eval_time': 744.8053612709045, 'accumulated_logging_time': 4.027449607849121}
I1010 06:38:31.029925 140142928963328 logging_writer.py:48] [26759] accumulated_eval_time=744.805361, accumulated_logging_time=4.027450, accumulated_submission_time=7088.215101, global_step=26759, preemption_count=0, score=7088.215101, test/loss=0.285619, test/num_examples=3581, test/ssim=0.744097, total_duration=7923.634159, train/loss=0.259986, train/ssim=0.749180, validation/loss=0.284327, validation/num_examples=3554, validation/ssim=0.726859
I1010 06:39:34.105489 140142937356032 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.219695, loss=0.309395
I1010 06:39:34.109291 140204662740800 pytorch_submission_base.py:86] 27000) loss = 0.309, grad_norm = 0.220
I1010 06:39:51.709135 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:39:53.689324 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:39:55.653328 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:39:57.551213 140204662740800 submission_runner.py:381] Time since start: 8010.18s, 	Step: 27065, 	{'train/ssim': 0.7490682601928711, 'train/loss': 0.25998946598597933, 'validation/ssim': 0.7266760521815209, 'validation/loss': 0.28437393866835786, 'validation/num_examples': 3554, 'test/ssim': 0.7439459806312831, 'test/loss': 0.2856325662458985, 'test/num_examples': 3581, 'score': 7167.906291246414, 'total_duration': 8010.176422357559, 'accumulated_submission_time': 7167.906291246414, 'accumulated_eval_time': 750.6477189064026, 'accumulated_logging_time': 4.057188034057617}
I1010 06:39:57.572775 140142928963328 logging_writer.py:48] [27065] accumulated_eval_time=750.647719, accumulated_logging_time=4.057188, accumulated_submission_time=7167.906291, global_step=27065, preemption_count=0, score=7167.906291, test/loss=0.285633, test/num_examples=3581, test/ssim=0.743946, total_duration=8010.176422, train/loss=0.259989, train/ssim=0.749068, validation/loss=0.284374, validation/num_examples=3554, validation/ssim=0.726676
I1010 06:40:17.029748 140204662740800 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 06:40:18.952220 140204662740800 spec.py:333] Evaluating on the validation split.
I1010 06:40:20.893321 140204662740800 spec.py:349] Evaluating on the test split.
I1010 06:40:22.792181 140204662740800 submission_runner.py:381] Time since start: 8035.42s, 	Step: 27142, 	{'train/ssim': 0.7495369229997907, 'train/loss': 0.26024251324789865, 'validation/ssim': 0.7272361879880065, 'validation/loss': 0.284661923624789, 'validation/num_examples': 3554, 'test/ssim': 0.7443930150010472, 'test/loss': 0.28602894535613305, 'test/num_examples': 3581, 'score': 7186.474029302597, 'total_duration': 8035.417387008667, 'accumulated_submission_time': 7186.474029302597, 'accumulated_eval_time': 756.4104130268097, 'accumulated_logging_time': 4.087508201599121}
I1010 06:40:22.812379 140142937356032 logging_writer.py:48] [27142] accumulated_eval_time=756.410413, accumulated_logging_time=4.087508, accumulated_submission_time=7186.474029, global_step=27142, preemption_count=0, score=7186.474029, test/loss=0.286029, test/num_examples=3581, test/ssim=0.744393, total_duration=8035.417387, train/loss=0.260243, train/ssim=0.749537, validation/loss=0.284662, validation/num_examples=3554, validation/ssim=0.727236
I1010 06:40:23.253445 140142928963328 logging_writer.py:48] [27142] global_step=27142, preemption_count=0, score=7186.474029
I1010 06:40:23.357775 140204662740800 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/fastmri_targets_check/nesterov_run_2/fastmri_pytorch/trial_1/checkpoint_27142.
I1010 06:40:24.593141 140204662740800 submission_runner.py:549] Tuning trial 1/1
I1010 06:40:24.593355 140204662740800 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.028609, beta1=0.981543, beta2=0.9978504782314613, warmup_steps=1357, decay_steps_factor=0.984398, end_factor=0.01, weight_decay=0.000576)
I1010 06:40:24.600047 140204662740800 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ssim': 0.16792333126068115, 'train/loss': 1.030285154070173, 'validation/ssim': 0.16574210025785208, 'validation/loss': 1.0420777809774198, 'validation/num_examples': 3554, 'test/ssim': 0.18604843365483628, 'test/loss': 1.0386019666512496, 'test/num_examples': 3581, 'score': 86.47392249107361, 'total_duration': 312.9899561405182, 'accumulated_submission_time': 86.47392249107361, 'accumulated_eval_time': 226.091970205307, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (305, {'train/ssim': 0.6801228523254395, 'train/loss': 0.32252434321812223, 'validation/ssim': 0.6584233599025746, 'validation/loss': 0.34896474482976925, 'validation/num_examples': 3554, 'test/ssim': 0.6772323220643675, 'test/loss': 0.3500391947627234, 'test/num_examples': 3581, 'score': 165.85433745384216, 'total_duration': 399.7517354488373, 'accumulated_submission_time': 165.85433745384216, 'accumulated_eval_time': 232.35104131698608, 'accumulated_logging_time': 0.030467987060546875, 'global_step': 305, 'preemption_count': 0}), (516, {'train/ssim': 0.6987625530787877, 'train/loss': 0.30547291891915457, 'validation/ssim': 0.6788700342044176, 'validation/loss': 0.3299729192129467, 'validation/num_examples': 3554, 'test/ssim': 0.6977442251640603, 'test/loss': 0.3311899159081786, 'test/num_examples': 3581, 'score': 245.3850381374359, 'total_duration': 486.4789927005768, 'accumulated_submission_time': 245.3850381374359, 'accumulated_eval_time': 238.5548391342163, 'accumulated_logging_time': 0.06409859657287598, 'global_step': 516, 'preemption_count': 0}), (735, {'train/ssim': 0.7080442564828056, 'train/loss': 0.2958430562700544, 'validation/ssim': 0.6887974349984173, 'validation/loss': 0.31898799932734245, 'validation/num_examples': 3554, 'test/ssim': 0.7071918742146048, 'test/loss': 0.32055739056827703, 'test/num_examples': 3581, 'score': 324.8076493740082, 'total_duration': 573.0747888088226, 'accumulated_submission_time': 324.8076493740082, 'accumulated_eval_time': 244.6919298171997, 'accumulated_logging_time': 0.10406279563903809, 'global_step': 735, 'preemption_count': 0}), (945, {'train/ssim': 0.7137081963675362, 'train/loss': 0.28919478825160433, 'validation/ssim': 0.6943492642533061, 'validation/loss': 0.31186134626037565, 'validation/num_examples': 3554, 'test/ssim': 0.712264626893675, 'test/loss': 0.31363207346498884, 'test/num_examples': 3581, 'score': 404.4168653488159, 'total_duration': 660.0350699424744, 'accumulated_submission_time': 404.4168653488159, 'accumulated_eval_time': 251.0242133140564, 'accumulated_logging_time': 0.14265918731689453, 'global_step': 945, 'preemption_count': 0}), (1251, {'train/ssim': 0.7215217181614467, 'train/loss': 0.2828573499407087, 'validation/ssim': 0.7010443091191263, 'validation/loss': 0.306146504708603, 'validation/num_examples': 3554, 'test/ssim': 0.7183888000471237, 'test/loss': 0.3079361860993787, 'test/num_examples': 3581, 'score': 483.8129186630249, 'total_duration': 746.2773327827454, 'accumulated_submission_time': 483.8129186630249, 'accumulated_eval_time': 256.80575919151306, 'accumulated_logging_time': 0.18888568878173828, 'global_step': 1251, 'preemption_count': 0}), (1558, {'train/ssim': 0.728050776890346, 'train/loss': 0.27642972128731863, 'validation/ssim': 0.7083639940252181, 'validation/loss': 0.2987849504409644, 'validation/num_examples': 3554, 'test/ssim': 0.7256676131405334, 'test/loss': 0.30052040609292097, 'test/num_examples': 3581, 'score': 563.4335632324219, 'total_duration': 833.105167388916, 'accumulated_submission_time': 563.4335632324219, 'accumulated_eval_time': 262.9674036502838, 'accumulated_logging_time': 0.21651482582092285, 'global_step': 1558, 'preemption_count': 0}), (1866, {'train/ssim': 0.7198609624590192, 'train/loss': 0.28334368978227886, 'validation/ssim': 0.7011858200047482, 'validation/loss': 0.3059248615666327, 'validation/num_examples': 3554, 'test/ssim': 0.7175343419479545, 'test/loss': 0.30798537556068484, 'test/num_examples': 3581, 'score': 642.8614251613617, 'total_duration': 919.3391571044922, 'accumulated_submission_time': 642.8614251613617, 'accumulated_eval_time': 268.68727827072144, 'accumulated_logging_time': 0.24095916748046875, 'global_step': 1866, 'preemption_count': 0}), (2169, {'train/ssim': 0.7334461212158203, 'train/loss': 0.272248591695513, 'validation/ssim': 0.7133662664427406, 'validation/loss': 0.2948755061418472, 'validation/num_examples': 3554, 'test/ssim': 0.7304469334683049, 'test/loss': 0.29674771417289164, 'test/num_examples': 3581, 'score': 722.2912240028381, 'total_duration': 1006.0489928722382, 'accumulated_submission_time': 722.2912240028381, 'accumulated_eval_time': 274.9092185497284, 'accumulated_logging_time': 0.2700538635253906, 'global_step': 2169, 'preemption_count': 0}), (2472, {'train/ssim': 0.733506270817348, 'train/loss': 0.27514166491372244, 'validation/ssim': 0.7130587206976294, 'validation/loss': 0.2987488514262099, 'validation/num_examples': 3554, 'test/ssim': 0.7298015731901005, 'test/loss': 0.3005153610199665, 'test/num_examples': 3581, 'score': 800.2384705543518, 'total_duration': 1092.3634963035583, 'accumulated_submission_time': 800.2384705543518, 'accumulated_eval_time': 280.74616861343384, 'accumulated_logging_time': 1.8322670459747314, 'global_step': 2472, 'preemption_count': 0}), (2781, {'train/ssim': 0.7368245806012835, 'train/loss': 0.2693100316183908, 'validation/ssim': 0.7164774450882808, 'validation/loss': 0.2920083650794, 'validation/num_examples': 3554, 'test/ssim': 0.7337744999650936, 'test/loss': 0.29355902353567437, 'test/num_examples': 3581, 'score': 879.8399803638458, 'total_duration': 1178.8269805908203, 'accumulated_submission_time': 879.8399803638458, 'accumulated_eval_time': 286.607310295105, 'accumulated_logging_time': 1.8570406436920166, 'global_step': 2781, 'preemption_count': 0}), (3089, {'train/ssim': 0.7309059415544782, 'train/loss': 0.2714355332510812, 'validation/ssim': 0.7117228167205966, 'validation/loss': 0.2934708388655388, 'validation/num_examples': 3554, 'test/ssim': 0.7289881574411826, 'test/loss': 0.29502376502068206, 'test/num_examples': 3581, 'score': 959.372216463089, 'total_duration': 1265.1945691108704, 'accumulated_submission_time': 959.372216463089, 'accumulated_eval_time': 292.4378583431244, 'accumulated_logging_time': 1.8813128471374512, 'global_step': 3089, 'preemption_count': 0}), (3396, {'train/ssim': 0.7398200035095215, 'train/loss': 0.26771460260663715, 'validation/ssim': 0.7189482526290799, 'validation/loss': 0.2908463958413935, 'validation/num_examples': 3554, 'test/ssim': 0.7362282461908336, 'test/loss': 0.29240936049200644, 'test/num_examples': 3581, 'score': 1039.0017628669739, 'total_duration': 1351.6234259605408, 'accumulated_submission_time': 1039.0017628669739, 'accumulated_eval_time': 298.2449851036072, 'accumulated_logging_time': 1.9081621170043945, 'global_step': 3396, 'preemption_count': 0}), (3705, {'train/ssim': 0.7395520210266113, 'train/loss': 0.2700499636786325, 'validation/ssim': 0.7186436607519696, 'validation/loss': 0.2934485131190208, 'validation/num_examples': 3554, 'test/ssim': 0.7358480931260472, 'test/loss': 0.29500024407244835, 'test/num_examples': 3581, 'score': 1118.5210628509521, 'total_duration': 1437.9518880844116, 'accumulated_submission_time': 1118.5210628509521, 'accumulated_eval_time': 304.05380749702454, 'accumulated_logging_time': 1.9326221942901611, 'global_step': 3705, 'preemption_count': 0}), (4015, {'train/ssim': 0.7385118348257882, 'train/loss': 0.2696098600115095, 'validation/ssim': 0.7181648593574141, 'validation/loss': 0.292431214718451, 'validation/num_examples': 3554, 'test/ssim': 0.7351044902785535, 'test/loss': 0.2942182918528344, 'test/num_examples': 3581, 'score': 1198.004730939865, 'total_duration': 1524.3385648727417, 'accumulated_submission_time': 1198.004730939865, 'accumulated_eval_time': 309.9602212905884, 'accumulated_logging_time': 1.958364725112915, 'global_step': 4015, 'preemption_count': 0}), (4321, {'train/ssim': 0.7231118338448661, 'train/loss': 0.2743037939071655, 'validation/ssim': 0.7062113798932541, 'validation/loss': 0.29636387779394696, 'validation/num_examples': 3554, 'test/ssim': 0.7224049508255376, 'test/loss': 0.29784024517418317, 'test/num_examples': 3581, 'score': 1277.661129951477, 'total_duration': 1610.841249704361, 'accumulated_submission_time': 1277.661129951477, 'accumulated_eval_time': 315.79105830192566, 'accumulated_logging_time': 1.9837493896484375, 'global_step': 4321, 'preemption_count': 0}), (4631, {'train/ssim': 0.740626403263637, 'train/loss': 0.267188480922154, 'validation/ssim': 0.7193851503147861, 'validation/loss': 0.2901319032582126, 'validation/num_examples': 3554, 'test/ssim': 0.7369220118987364, 'test/loss': 0.29163518041180886, 'test/num_examples': 3581, 'score': 1357.2343275547028, 'total_duration': 1697.252140045166, 'accumulated_submission_time': 1357.2343275547028, 'accumulated_eval_time': 321.6333043575287, 'accumulated_logging_time': 2.009265422821045, 'global_step': 4631, 'preemption_count': 0}), (4940, {'train/ssim': 0.7359288079398019, 'train/loss': 0.27013741220746723, 'validation/ssim': 0.7157025012529896, 'validation/loss': 0.2929488629119478, 'validation/num_examples': 3554, 'test/ssim': 0.7328671367765638, 'test/loss': 0.2945947292895665, 'test/num_examples': 3581, 'score': 1436.9232141971588, 'total_duration': 1783.7779281139374, 'accumulated_submission_time': 1436.9232141971588, 'accumulated_eval_time': 327.45118594169617, 'accumulated_logging_time': 2.0347211360931396, 'global_step': 4940, 'preemption_count': 0}), (5249, {'train/ssim': 0.7402444566999163, 'train/loss': 0.26728527886526926, 'validation/ssim': 0.7195401253429234, 'validation/loss': 0.290283409208814, 'validation/num_examples': 3554, 'test/ssim': 0.7367175500907568, 'test/loss': 0.29188692273457134, 'test/num_examples': 3581, 'score': 1516.4129123687744, 'total_duration': 1870.1397202014923, 'accumulated_submission_time': 1516.4129123687744, 'accumulated_eval_time': 333.33461689949036, 'accumulated_logging_time': 2.060382127761841, 'global_step': 5249, 'preemption_count': 0}), (5555, {'train/ssim': 0.7407815796988351, 'train/loss': 0.26683548518589567, 'validation/ssim': 0.7201292502725802, 'validation/loss': 0.2901017119794598, 'validation/num_examples': 3554, 'test/ssim': 0.7372907794610444, 'test/loss': 0.2916151023849832, 'test/num_examples': 3581, 'score': 1596.0226624011993, 'total_duration': 1956.700663805008, 'accumulated_submission_time': 1596.0226624011993, 'accumulated_eval_time': 339.23907566070557, 'accumulated_logging_time': 2.0855679512023926, 'global_step': 5555, 'preemption_count': 0}), (5864, {'train/ssim': 0.7418274198259626, 'train/loss': 0.26806613377162386, 'validation/ssim': 0.7216104433384919, 'validation/loss': 0.2913272237267867, 'validation/num_examples': 3554, 'test/ssim': 0.7385354125942474, 'test/loss': 0.2930547548585416, 'test/num_examples': 3581, 'score': 1675.6175727844238, 'total_duration': 2043.4029126167297, 'accumulated_submission_time': 1675.6175727844238, 'accumulated_eval_time': 345.3128430843353, 'accumulated_logging_time': 2.113248586654663, 'global_step': 5864, 'preemption_count': 0}), (6172, {'train/ssim': 0.7416880471365792, 'train/loss': 0.26659904207502094, 'validation/ssim': 0.7208092581642164, 'validation/loss': 0.2895858154983821, 'validation/num_examples': 3554, 'test/ssim': 0.7381549868228149, 'test/loss': 0.29101450008508445, 'test/num_examples': 3581, 'score': 1755.2498984336853, 'total_duration': 2129.9009618759155, 'accumulated_submission_time': 1755.2498984336853, 'accumulated_eval_time': 351.1417167186737, 'accumulated_logging_time': 2.137796401977539, 'global_step': 6172, 'preemption_count': 0}), (6478, {'train/ssim': 0.7379164014543805, 'train/loss': 0.2682357685906546, 'validation/ssim': 0.7172490228879431, 'validation/loss': 0.29111832343398286, 'validation/num_examples': 3554, 'test/ssim': 0.7343105048781765, 'test/loss': 0.29271479193573724, 'test/num_examples': 3581, 'score': 1834.7129101753235, 'total_duration': 2216.1857285499573, 'accumulated_submission_time': 1834.7129101753235, 'accumulated_eval_time': 356.9811520576477, 'accumulated_logging_time': 2.1638283729553223, 'global_step': 6478, 'preemption_count': 0}), (6786, {'train/ssim': 0.741504601069859, 'train/loss': 0.265310423714774, 'validation/ssim': 0.7200476410822313, 'validation/loss': 0.28864287900736846, 'validation/num_examples': 3554, 'test/ssim': 0.7374265873708461, 'test/loss': 0.29004939126422435, 'test/num_examples': 3581, 'score': 1914.4597845077515, 'total_duration': 2302.74951672554, 'accumulated_submission_time': 1914.4597845077515, 'accumulated_eval_time': 362.8152663707733, 'accumulated_logging_time': 2.189122438430786, 'global_step': 6786, 'preemption_count': 0}), (7093, {'train/ssim': 0.7261455399649483, 'train/loss': 0.2726601873125349, 'validation/ssim': 0.7072191297437747, 'validation/loss': 0.2951184789585678, 'validation/num_examples': 3554, 'test/ssim': 0.7243525535159523, 'test/loss': 0.29655129721097456, 'test/num_examples': 3581, 'score': 1994.0795505046844, 'total_duration': 2389.1716210842133, 'accumulated_submission_time': 1994.0795505046844, 'accumulated_eval_time': 368.6366057395935, 'accumulated_logging_time': 2.2144317626953125, 'global_step': 7093, 'preemption_count': 0}), (7402, {'train/ssim': 0.7395340374537877, 'train/loss': 0.2657548018864223, 'validation/ssim': 0.7188987925137169, 'validation/loss': 0.2887144244381331, 'validation/num_examples': 3554, 'test/ssim': 0.7362722201375315, 'test/loss': 0.29014971322169086, 'test/num_examples': 3581, 'score': 2073.6813666820526, 'total_duration': 2475.6282069683075, 'accumulated_submission_time': 2073.6813666820526, 'accumulated_eval_time': 374.4683928489685, 'accumulated_logging_time': 2.2392570972442627, 'global_step': 7402, 'preemption_count': 0}), (7708, {'train/ssim': 0.7388514110020229, 'train/loss': 0.26898867743355886, 'validation/ssim': 0.7175751161763154, 'validation/loss': 0.2925260819675014, 'validation/num_examples': 3554, 'test/ssim': 0.7345956878534278, 'test/loss': 0.2941942595796391, 'test/num_examples': 3581, 'score': 2153.2630984783173, 'total_duration': 2562.1057510375977, 'accumulated_submission_time': 2153.2630984783173, 'accumulated_eval_time': 380.31637144088745, 'accumulated_logging_time': 2.2650411128997803, 'global_step': 7708, 'preemption_count': 0}), (8018, {'train/ssim': 0.7405543327331543, 'train/loss': 0.2702175889696394, 'validation/ssim': 0.7202724098287141, 'validation/loss': 0.2933752159758371, 'validation/num_examples': 3554, 'test/ssim': 0.7366512823757331, 'test/loss': 0.29520794427185143, 'test/num_examples': 3581, 'score': 2232.9535706043243, 'total_duration': 2648.9260125160217, 'accumulated_submission_time': 2232.9535706043243, 'accumulated_eval_time': 386.44236040115356, 'accumulated_logging_time': 2.293684482574463, 'global_step': 8018, 'preemption_count': 0}), (8327, {'train/ssim': 0.742305074419294, 'train/loss': 0.26552157742636545, 'validation/ssim': 0.7211831628974396, 'validation/loss': 0.28879569015545864, 'validation/num_examples': 3554, 'test/ssim': 0.7385761822378526, 'test/loss': 0.29016218955075396, 'test/num_examples': 3581, 'score': 2312.624679327011, 'total_duration': 2735.4500720500946, 'accumulated_submission_time': 2312.624679327011, 'accumulated_eval_time': 392.2888216972351, 'accumulated_logging_time': 2.3204901218414307, 'global_step': 8327, 'preemption_count': 0}), (8633, {'train/ssim': 0.7432466915675572, 'train/loss': 0.2651015179497855, 'validation/ssim': 0.7222014230224747, 'validation/loss': 0.2882895483082442, 'validation/num_examples': 3554, 'test/ssim': 0.7394887268526599, 'test/loss': 0.2897077580132121, 'test/num_examples': 3581, 'score': 2392.075304746628, 'total_duration': 2821.721071958542, 'accumulated_submission_time': 2392.075304746628, 'accumulated_eval_time': 398.0868935585022, 'accumulated_logging_time': 2.347553253173828, 'global_step': 8633, 'preemption_count': 0}), (8942, {'train/ssim': 0.7437271390642438, 'train/loss': 0.2644360065460205, 'validation/ssim': 0.7222374876899268, 'validation/loss': 0.288108469330332, 'validation/num_examples': 3554, 'test/ssim': 0.7394019379625105, 'test/loss': 0.28959557331663643, 'test/num_examples': 3581, 'score': 2471.5909333229065, 'total_duration': 2908.0179464817047, 'accumulated_submission_time': 2471.5909333229065, 'accumulated_eval_time': 403.9040672779083, 'accumulated_logging_time': 2.372897148132324, 'global_step': 8942, 'preemption_count': 0}), (9251, {'train/ssim': 0.7425071171351841, 'train/loss': 0.26476895809173584, 'validation/ssim': 0.7213926814416854, 'validation/loss': 0.2879573755473586, 'validation/num_examples': 3554, 'test/ssim': 0.7387488737215513, 'test/loss': 0.2893701130969003, 'test/num_examples': 3581, 'score': 2551.3116743564606, 'total_duration': 2994.5534007549286, 'accumulated_submission_time': 2551.3116743564606, 'accumulated_eval_time': 409.72331953048706, 'accumulated_logging_time': 2.399118661880493, 'global_step': 9251, 'preemption_count': 0}), (9560, {'train/ssim': 0.7433134487697056, 'train/loss': 0.26483518736703054, 'validation/ssim': 0.7227133352164814, 'validation/loss': 0.28785933117284224, 'validation/num_examples': 3554, 'test/ssim': 0.7399288753796076, 'test/loss': 0.2892544854789165, 'test/num_examples': 3581, 'score': 2630.9752430915833, 'total_duration': 3081.052417039871, 'accumulated_submission_time': 2630.9752430915833, 'accumulated_eval_time': 415.5883586406708, 'accumulated_logging_time': 2.4256081581115723, 'global_step': 9560, 'preemption_count': 0}), (9866, {'train/ssim': 0.7405358723231724, 'train/loss': 0.2659291369574411, 'validation/ssim': 0.7198089273310003, 'validation/loss': 0.2891314693830895, 'validation/num_examples': 3554, 'test/ssim': 0.7368423815580494, 'test/loss': 0.2906959105456751, 'test/num_examples': 3581, 'score': 2710.6089477539062, 'total_duration': 3167.537455558777, 'accumulated_submission_time': 2710.6089477539062, 'accumulated_eval_time': 421.4334752559662, 'accumulated_logging_time': 2.4510579109191895, 'global_step': 9866, 'preemption_count': 0}), (10173, {'train/ssim': 0.7426485334123883, 'train/loss': 0.26604740960257395, 'validation/ssim': 0.7218587056397721, 'validation/loss': 0.2891095558041995, 'validation/num_examples': 3554, 'test/ssim': 0.7390568958784208, 'test/loss': 0.2905985883613167, 'test/num_examples': 3581, 'score': 2790.0488142967224, 'total_duration': 3254.0874865055084, 'accumulated_submission_time': 2790.0488142967224, 'accumulated_eval_time': 427.5229561328888, 'accumulated_logging_time': 2.4809067249298096, 'global_step': 10173, 'preemption_count': 0}), (10478, {'train/ssim': 0.7429479190281459, 'train/loss': 0.26432931423187256, 'validation/ssim': 0.7217718756594682, 'validation/loss': 0.28757290901866733, 'validation/num_examples': 3554, 'test/ssim': 0.7391050286014731, 'test/loss': 0.2889906417987294, 'test/num_examples': 3581, 'score': 2869.7241768836975, 'total_duration': 3340.700449705124, 'accumulated_submission_time': 2869.7241768836975, 'accumulated_eval_time': 433.3181622028351, 'accumulated_logging_time': 2.506944417953491, 'global_step': 10478, 'preemption_count': 0}), (10784, {'train/ssim': 0.7438717569623675, 'train/loss': 0.2645560162408011, 'validation/ssim': 0.7227788011747327, 'validation/loss': 0.2878263234152979, 'validation/num_examples': 3554, 'test/ssim': 0.7401943552996719, 'test/loss': 0.289197967026494, 'test/num_examples': 3581, 'score': 2949.3888173103333, 'total_duration': 3427.1637194156647, 'accumulated_submission_time': 2949.3888173103333, 'accumulated_eval_time': 439.1296989917755, 'accumulated_logging_time': 2.532369613647461, 'global_step': 10784, 'preemption_count': 0}), (11093, {'train/ssim': 0.7453657558986119, 'train/loss': 0.26422129358564106, 'validation/ssim': 0.7239784837507034, 'validation/loss': 0.28789215002022367, 'validation/num_examples': 3554, 'test/ssim': 0.7410098163484362, 'test/loss': 0.28940911014730525, 'test/num_examples': 3581, 'score': 3029.025542497635, 'total_duration': 3513.6416630744934, 'accumulated_submission_time': 3029.025542497635, 'accumulated_eval_time': 444.9601626396179, 'accumulated_logging_time': 2.559365749359131, 'global_step': 11093, 'preemption_count': 0}), (11401, {'train/ssim': 0.7411413873944964, 'train/loss': 0.2648871626172747, 'validation/ssim': 0.7201837937886888, 'validation/loss': 0.287964073271314, 'validation/num_examples': 3554, 'test/ssim': 0.7376043239274993, 'test/loss': 0.28931598082763194, 'test/num_examples': 3581, 'score': 3108.681938648224, 'total_duration': 3600.4099621772766, 'accumulated_submission_time': 3108.681938648224, 'accumulated_eval_time': 451.0672836303711, 'accumulated_logging_time': 2.5875611305236816, 'global_step': 11401, 'preemption_count': 0}), (11711, {'train/ssim': 0.7440343584333148, 'train/loss': 0.2646040064947946, 'validation/ssim': 0.7228662494064786, 'validation/loss': 0.287836370001231, 'validation/num_examples': 3554, 'test/ssim': 0.7400071421870636, 'test/loss': 0.2893998722096656, 'test/num_examples': 3581, 'score': 3188.2996957302094, 'total_duration': 3686.817044019699, 'accumulated_submission_time': 3188.2996957302094, 'accumulated_eval_time': 456.8700966835022, 'accumulated_logging_time': 2.6155266761779785, 'global_step': 11711, 'preemption_count': 0}), (12017, {'train/ssim': 0.7446763856070382, 'train/loss': 0.2642279693058559, 'validation/ssim': 0.7230840799978897, 'validation/loss': 0.2877823073473551, 'validation/num_examples': 3554, 'test/ssim': 0.7402127629982895, 'test/loss': 0.2892108865038572, 'test/num_examples': 3581, 'score': 3267.820540189743, 'total_duration': 3773.195077896118, 'accumulated_submission_time': 3267.820540189743, 'accumulated_eval_time': 462.7298436164856, 'accumulated_logging_time': 2.641876459121704, 'global_step': 12017, 'preemption_count': 0}), (12325, {'train/ssim': 0.7447128977094378, 'train/loss': 0.2646752766200474, 'validation/ssim': 0.7237226650429094, 'validation/loss': 0.28781306535659645, 'validation/num_examples': 3554, 'test/ssim': 0.7408643955293563, 'test/loss': 0.28931393552778556, 'test/num_examples': 3581, 'score': 3347.294772386551, 'total_duration': 3859.569970369339, 'accumulated_submission_time': 3347.294772386551, 'accumulated_eval_time': 468.6086890697479, 'accumulated_logging_time': 2.6689863204956055, 'global_step': 12325, 'preemption_count': 0}), (12631, {'train/ssim': 0.7449334008353097, 'train/loss': 0.26370700768062044, 'validation/ssim': 0.7236994462665307, 'validation/loss': 0.2869125993049205, 'validation/num_examples': 3554, 'test/ssim': 0.7410447227991482, 'test/loss': 0.2882688895712615, 'test/num_examples': 3581, 'score': 3426.822010755539, 'total_duration': 3945.8740181922913, 'accumulated_submission_time': 3426.822010755539, 'accumulated_eval_time': 474.4083797931671, 'accumulated_logging_time': 2.6954219341278076, 'global_step': 12631, 'preemption_count': 0}), (12937, {'train/ssim': 0.7438997541155133, 'train/loss': 0.26406238760266987, 'validation/ssim': 0.7227297532269977, 'validation/loss': 0.28731248777236035, 'validation/num_examples': 3554, 'test/ssim': 0.7401613577954831, 'test/loss': 0.28871759426923344, 'test/num_examples': 3581, 'score': 3506.44699382782, 'total_duration': 4032.3312780857086, 'accumulated_submission_time': 3506.44699382782, 'accumulated_eval_time': 480.2675156593323, 'accumulated_logging_time': 2.722045421600342, 'global_step': 12937, 'preemption_count': 0}), (13247, {'train/ssim': 0.7463790348597935, 'train/loss': 0.2635846308299473, 'validation/ssim': 0.7249723572910804, 'validation/loss': 0.28736938407867896, 'validation/num_examples': 3554, 'test/ssim': 0.7419568583539863, 'test/loss': 0.2888660830380829, 'test/num_examples': 3581, 'score': 3586.143978834152, 'total_duration': 4118.928502559662, 'accumulated_submission_time': 3586.143978834152, 'accumulated_eval_time': 486.1556911468506, 'accumulated_logging_time': 2.7483441829681396, 'global_step': 13247, 'preemption_count': 0}), (13555, {'train/ssim': 0.7455594880240304, 'train/loss': 0.263953583581107, 'validation/ssim': 0.724471436233821, 'validation/loss': 0.2871713203597619, 'validation/num_examples': 3554, 'test/ssim': 0.7415424806051033, 'test/loss': 0.28858563834080914, 'test/num_examples': 3581, 'score': 3665.777436733246, 'total_duration': 4205.346914291382, 'accumulated_submission_time': 3665.777436733246, 'accumulated_eval_time': 491.98027324676514, 'accumulated_logging_time': 2.7749133110046387, 'global_step': 13555, 'preemption_count': 0}), (13864, {'train/ssim': 0.7440620149884906, 'train/loss': 0.263454794883728, 'validation/ssim': 0.7228357490020048, 'validation/loss': 0.2867525408760376, 'validation/num_examples': 3554, 'test/ssim': 0.7400878633543354, 'test/loss': 0.28811467396284207, 'test/num_examples': 3581, 'score': 3745.4199805259705, 'total_duration': 4291.768035411835, 'accumulated_submission_time': 3745.4199805259705, 'accumulated_eval_time': 497.7755160331726, 'accumulated_logging_time': 2.802415609359741, 'global_step': 13864, 'preemption_count': 0}), (14169, {'train/ssim': 0.7441503661019462, 'train/loss': 0.26373861517224995, 'validation/ssim': 0.7223456129976786, 'validation/loss': 0.2873325637705754, 'validation/num_examples': 3554, 'test/ssim': 0.7396045590006283, 'test/loss': 0.2887801122678721, 'test/num_examples': 3581, 'score': 3825.150273323059, 'total_duration': 4378.336943626404, 'accumulated_submission_time': 3825.150273323059, 'accumulated_eval_time': 503.6161382198334, 'accumulated_logging_time': 2.82869029045105, 'global_step': 14169, 'preemption_count': 0}), (14480, {'train/ssim': 0.7452069691249302, 'train/loss': 0.2635948828288487, 'validation/ssim': 0.7240870212260833, 'validation/loss': 0.2869385830391372, 'validation/num_examples': 3554, 'test/ssim': 0.7412888634241482, 'test/loss': 0.2883943346285081, 'test/num_examples': 3581, 'score': 3904.799750328064, 'total_duration': 4464.793534994125, 'accumulated_submission_time': 3904.799750328064, 'accumulated_eval_time': 509.44630885124207, 'accumulated_logging_time': 2.855097532272339, 'global_step': 14480, 'preemption_count': 0}), (14786, {'train/ssim': 0.7435030937194824, 'train/loss': 0.26358212743486675, 'validation/ssim': 0.7221164477964969, 'validation/loss': 0.2869454009786508, 'validation/num_examples': 3554, 'test/ssim': 0.7394701828007191, 'test/loss': 0.2882743437041853, 'test/num_examples': 3581, 'score': 3984.3336300849915, 'total_duration': 4551.155753135681, 'accumulated_submission_time': 3984.3336300849915, 'accumulated_eval_time': 515.2992980480194, 'accumulated_logging_time': 2.8820011615753174, 'global_step': 14786, 'preemption_count': 0}), (15092, {'train/ssim': 0.7447686195373535, 'train/loss': 0.26374360493251253, 'validation/ssim': 0.7233442951603827, 'validation/loss': 0.28717236795248313, 'validation/num_examples': 3554, 'test/ssim': 0.7406549568250838, 'test/loss': 0.2885999213514032, 'test/num_examples': 3581, 'score': 4063.8503143787384, 'total_duration': 4637.569295406342, 'accumulated_submission_time': 4063.8503143787384, 'accumulated_eval_time': 521.1766204833984, 'accumulated_logging_time': 2.9149417877197266, 'global_step': 15092, 'preemption_count': 0}), (15398, {'train/ssim': 0.7463897977556501, 'train/loss': 0.26315672057015554, 'validation/ssim': 0.7247934078459131, 'validation/loss': 0.28690756742512835, 'validation/num_examples': 3554, 'test/ssim': 0.7418886816924393, 'test/loss': 0.28833232795483105, 'test/num_examples': 3581, 'score': 4143.545258760452, 'total_duration': 4724.303222417831, 'accumulated_submission_time': 4143.545258760452, 'accumulated_eval_time': 527.198808670044, 'accumulated_logging_time': 2.9450130462646484, 'global_step': 15398, 'preemption_count': 0}), (15709, {'train/ssim': 0.7432845660618373, 'train/loss': 0.2642319883619036, 'validation/ssim': 0.7222652403102139, 'validation/loss': 0.28732933512415587, 'validation/num_examples': 3554, 'test/ssim': 0.7395440181251746, 'test/loss': 0.28876000015271575, 'test/num_examples': 3581, 'score': 4223.227202177048, 'total_duration': 4810.823029518127, 'accumulated_submission_time': 4223.227202177048, 'accumulated_eval_time': 533.0728666782379, 'accumulated_logging_time': 2.9728827476501465, 'global_step': 15709, 'preemption_count': 0}), (16016, {'train/ssim': 0.7431158338274274, 'train/loss': 0.26372127873556955, 'validation/ssim': 0.7220782535963, 'validation/loss': 0.28694423317037143, 'validation/num_examples': 3554, 'test/ssim': 0.7393683950450293, 'test/loss': 0.2882672192430536, 'test/num_examples': 3581, 'score': 4302.822021484375, 'total_duration': 4897.519806623459, 'accumulated_submission_time': 4302.822021484375, 'accumulated_eval_time': 539.1492757797241, 'accumulated_logging_time': 3.0026814937591553, 'global_step': 16016, 'preemption_count': 0}), (16323, {'train/ssim': 0.7440167835780552, 'train/loss': 0.26401967661721365, 'validation/ssim': 0.7221713347856289, 'validation/loss': 0.28767136556081174, 'validation/num_examples': 3554, 'test/ssim': 0.7394296858637601, 'test/loss': 0.2891141097327911, 'test/num_examples': 3581, 'score': 4382.375616788864, 'total_duration': 4983.891037940979, 'accumulated_submission_time': 4382.375616788864, 'accumulated_eval_time': 544.9705655574799, 'accumulated_logging_time': 3.0305049419403076, 'global_step': 16323, 'preemption_count': 0}), (16631, {'train/ssim': 0.7450784274509975, 'train/loss': 0.26303240231105257, 'validation/ssim': 0.7235767577025887, 'validation/loss': 0.28655854731244723, 'validation/num_examples': 3554, 'test/ssim': 0.7409015518098995, 'test/loss': 0.2879321991282114, 'test/num_examples': 3581, 'score': 4461.961856603622, 'total_duration': 5070.334650039673, 'accumulated_submission_time': 4461.961856603622, 'accumulated_eval_time': 550.7984764575958, 'accumulated_logging_time': 3.05747652053833, 'global_step': 16631, 'preemption_count': 0}), (16938, {'train/ssim': 0.7457123483930316, 'train/loss': 0.26295930998665945, 'validation/ssim': 0.7242161670828644, 'validation/loss': 0.2864360133113657, 'validation/num_examples': 3554, 'test/ssim': 0.7414781900132644, 'test/loss': 0.28780907207745743, 'test/num_examples': 3581, 'score': 4541.54456114769, 'total_duration': 5157.022856473923, 'accumulated_submission_time': 4541.54456114769, 'accumulated_eval_time': 556.8966844081879, 'accumulated_logging_time': 3.0879549980163574, 'global_step': 16938, 'preemption_count': 0}), (17244, {'train/ssim': 0.746013709477016, 'train/loss': 0.26317875725882395, 'validation/ssim': 0.7245230945765335, 'validation/loss': 0.28655698451019096, 'validation/num_examples': 3554, 'test/ssim': 0.7417712133045937, 'test/loss': 0.2879459367255131, 'test/num_examples': 3581, 'score': 4621.06239938736, 'total_duration': 5243.3992438316345, 'accumulated_submission_time': 4621.06239938736, 'accumulated_eval_time': 562.7472848892212, 'accumulated_logging_time': 3.1151349544525146, 'global_step': 17244, 'preemption_count': 0}), (17551, {'train/ssim': 0.7463387761797223, 'train/loss': 0.26255290848868235, 'validation/ssim': 0.7245207589599747, 'validation/loss': 0.2864756672719119, 'validation/num_examples': 3554, 'test/ssim': 0.7416914466105836, 'test/loss': 0.287917507057648, 'test/num_examples': 3581, 'score': 4700.621969461441, 'total_duration': 5329.79449057579, 'accumulated_submission_time': 4700.621969461441, 'accumulated_eval_time': 568.5874304771423, 'accumulated_logging_time': 3.1428062915802, 'global_step': 17551, 'preemption_count': 0}), (17858, {'train/ssim': 0.7459402765546527, 'train/loss': 0.2630037409918649, 'validation/ssim': 0.7246268234295864, 'validation/loss': 0.2864508341723234, 'validation/num_examples': 3554, 'test/ssim': 0.7418222094474309, 'test/loss': 0.2878442171464849, 'test/num_examples': 3581, 'score': 4780.104678869247, 'total_duration': 5416.1230754852295, 'accumulated_submission_time': 4780.104678869247, 'accumulated_eval_time': 574.4372298717499, 'accumulated_logging_time': 3.1708579063415527, 'global_step': 17858, 'preemption_count': 0}), (18166, {'train/ssim': 0.7443195751735142, 'train/loss': 0.2629752499716623, 'validation/ssim': 0.7229475151238042, 'validation/loss': 0.28638043937618707, 'validation/num_examples': 3554, 'test/ssim': 0.7403038470181165, 'test/loss': 0.2877339073081018, 'test/num_examples': 3581, 'score': 4859.739594697952, 'total_duration': 5502.862973690033, 'accumulated_submission_time': 4859.739594697952, 'accumulated_eval_time': 580.5079565048218, 'accumulated_logging_time': 3.202439785003662, 'global_step': 18166, 'preemption_count': 0}), (18472, {'train/ssim': 0.744185038975307, 'train/loss': 0.26515139852251324, 'validation/ssim': 0.7224475557910102, 'validation/loss': 0.2887891298207126, 'validation/num_examples': 3554, 'test/ssim': 0.7396445105242949, 'test/loss': 0.2903313358480522, 'test/num_examples': 3581, 'score': 4939.421260595322, 'total_duration': 5589.357748508453, 'accumulated_submission_time': 4939.421260595322, 'accumulated_eval_time': 586.3145625591278, 'accumulated_logging_time': 3.2310616970062256, 'global_step': 18472, 'preemption_count': 0}), (18775, {'train/ssim': 0.7457596915108817, 'train/loss': 0.2635413578578404, 'validation/ssim': 0.7250343198244935, 'validation/loss': 0.2869968875848516, 'validation/num_examples': 3554, 'test/ssim': 0.7417599641554384, 'test/loss': 0.28847001072282535, 'test/num_examples': 3581, 'score': 5019.038831949234, 'total_duration': 5675.894321203232, 'accumulated_submission_time': 5019.038831949234, 'accumulated_eval_time': 592.1952176094055, 'accumulated_logging_time': 3.2596919536590576, 'global_step': 18775, 'preemption_count': 0}), (19082, {'train/ssim': 0.7468434061322894, 'train/loss': 0.26240226200648714, 'validation/ssim': 0.7255028170283483, 'validation/loss': 0.285969783029465, 'validation/num_examples': 3554, 'test/ssim': 0.7426817807962162, 'test/loss': 0.2872967244659313, 'test/num_examples': 3581, 'score': 5098.7637894153595, 'total_duration': 5762.46964263916, 'accumulated_submission_time': 5098.7637894153595, 'accumulated_eval_time': 598.0819914340973, 'accumulated_logging_time': 3.287536144256592, 'global_step': 19082, 'preemption_count': 0}), (19389, {'train/ssim': 0.7443292481558663, 'train/loss': 0.26278761454990934, 'validation/ssim': 0.7229194190304938, 'validation/loss': 0.28624105802331, 'validation/num_examples': 3554, 'test/ssim': 0.7402972338819463, 'test/loss': 0.2875300931784069, 'test/num_examples': 3581, 'score': 5178.509335517883, 'total_duration': 5849.032888174057, 'accumulated_submission_time': 5178.509335517883, 'accumulated_eval_time': 603.8873469829559, 'accumulated_logging_time': 3.3157379627227783, 'global_step': 19389, 'preemption_count': 0}), (19696, {'train/ssim': 0.7462765829903739, 'train/loss': 0.2621185098375593, 'validation/ssim': 0.7243356270003869, 'validation/loss': 0.2860813430674504, 'validation/num_examples': 3554, 'test/ssim': 0.741581068595539, 'test/loss': 0.2874615415452213, 'test/num_examples': 3581, 'score': 5258.099138259888, 'total_duration': 5935.444639921188, 'accumulated_submission_time': 5258.099138259888, 'accumulated_eval_time': 609.714390039444, 'accumulated_logging_time': 3.343081474304199, 'global_step': 19696, 'preemption_count': 0}), (20005, {'train/ssim': 0.745769909449986, 'train/loss': 0.26284922872270855, 'validation/ssim': 0.7243343904975028, 'validation/loss': 0.2863809889330244, 'validation/num_examples': 3554, 'test/ssim': 0.7415886362049707, 'test/loss': 0.28776690481229056, 'test/num_examples': 3581, 'score': 5337.687087774277, 'total_duration': 6022.144808292389, 'accumulated_submission_time': 5337.687087774277, 'accumulated_eval_time': 615.7887609004974, 'accumulated_logging_time': 3.373713254928589, 'global_step': 20005, 'preemption_count': 0}), (20313, {'train/ssim': 0.7457877567836216, 'train/loss': 0.2624605553490775, 'validation/ssim': 0.7245521523943093, 'validation/loss': 0.2859537428392744, 'validation/num_examples': 3554, 'test/ssim': 0.741810755768291, 'test/loss': 0.2872609317186191, 'test/num_examples': 3581, 'score': 5417.196914434433, 'total_duration': 6108.482858181, 'accumulated_submission_time': 5417.196914434433, 'accumulated_eval_time': 621.6101474761963, 'accumulated_logging_time': 3.4022324085235596, 'global_step': 20313, 'preemption_count': 0}), (20618, {'train/ssim': 0.7447492054530552, 'train/loss': 0.26309006554739817, 'validation/ssim': 0.7232288882245357, 'validation/loss': 0.2866473694362866, 'validation/num_examples': 3554, 'test/ssim': 0.7406678422141162, 'test/loss': 0.2880393387518326, 'test/num_examples': 3581, 'score': 5496.766225814819, 'total_duration': 6194.858247280121, 'accumulated_submission_time': 5496.766225814819, 'accumulated_eval_time': 627.4265532493591, 'accumulated_logging_time': 3.430436134338379, 'global_step': 20618, 'preemption_count': 0}), (20926, {'train/ssim': 0.7461234501429966, 'train/loss': 0.2620681013379778, 'validation/ssim': 0.7239641952729319, 'validation/loss': 0.28603586723915836, 'validation/num_examples': 3554, 'test/ssim': 0.7413451091699246, 'test/loss': 0.2873622763260088, 'test/num_examples': 3581, 'score': 5576.531705617905, 'total_duration': 6281.442793130875, 'accumulated_submission_time': 5576.531705617905, 'accumulated_eval_time': 633.2610561847687, 'accumulated_logging_time': 3.4589145183563232, 'global_step': 20926, 'preemption_count': 0}), (21233, {'train/ssim': 0.7469345501491002, 'train/loss': 0.2620537962232317, 'validation/ssim': 0.7254315120287, 'validation/loss': 0.28567224952298464, 'validation/num_examples': 3554, 'test/ssim': 0.7426571690213977, 'test/loss': 0.28699899698495535, 'test/num_examples': 3581, 'score': 5656.052699327469, 'total_duration': 6367.775305509567, 'accumulated_submission_time': 5656.052699327469, 'accumulated_eval_time': 639.1026182174683, 'accumulated_logging_time': 3.4876201152801514, 'global_step': 21233, 'preemption_count': 0}), (21543, {'train/ssim': 0.7464140483311245, 'train/loss': 0.2621051583971296, 'validation/ssim': 0.7248294038187606, 'validation/loss': 0.28575688127593907, 'validation/num_examples': 3554, 'test/ssim': 0.7421052107695127, 'test/loss': 0.28706260581017873, 'test/num_examples': 3581, 'score': 5735.501694917679, 'total_duration': 6454.1626060009, 'accumulated_submission_time': 5735.501694917679, 'accumulated_eval_time': 644.9934492111206, 'accumulated_logging_time': 3.516158103942871, 'global_step': 21543, 'preemption_count': 0}), (21850, {'train/ssim': 0.7471836635044643, 'train/loss': 0.26160201004573275, 'validation/ssim': 0.725101846620885, 'validation/loss': 0.28575561042575265, 'validation/num_examples': 3554, 'test/ssim': 0.7423863031450713, 'test/loss': 0.2870889560898667, 'test/num_examples': 3581, 'score': 5815.112927436829, 'total_duration': 6540.911270856857, 'accumulated_submission_time': 5815.112927436829, 'accumulated_eval_time': 651.0722677707672, 'accumulated_logging_time': 3.5507688522338867, 'global_step': 21850, 'preemption_count': 0}), (22159, {'train/ssim': 0.7451338768005371, 'train/loss': 0.2624304975782122, 'validation/ssim': 0.7232994375835327, 'validation/loss': 0.28618429910620075, 'validation/num_examples': 3554, 'test/ssim': 0.7405994610225844, 'test/loss': 0.28751710552438214, 'test/num_examples': 3581, 'score': 5894.767419338226, 'total_duration': 6627.410543203354, 'accumulated_submission_time': 5894.767419338226, 'accumulated_eval_time': 656.9522054195404, 'accumulated_logging_time': 3.580577850341797, 'global_step': 22159, 'preemption_count': 0}), (22467, {'train/ssim': 0.7475487845284599, 'train/loss': 0.26196428707667757, 'validation/ssim': 0.7258721192230585, 'validation/loss': 0.2856694845651467, 'validation/num_examples': 3554, 'test/ssim': 0.7431370645420273, 'test/loss': 0.2869915998171775, 'test/num_examples': 3581, 'score': 5974.2579870224, 'total_duration': 6713.660455465317, 'accumulated_submission_time': 5974.2579870224, 'accumulated_eval_time': 662.739494562149, 'accumulated_logging_time': 3.609541893005371, 'global_step': 22467, 'preemption_count': 0}), (22772, {'train/ssim': 0.7463259015764508, 'train/loss': 0.26202118396759033, 'validation/ssim': 0.7247678534529756, 'validation/loss': 0.28566333639802866, 'validation/num_examples': 3554, 'test/ssim': 0.7420236033056409, 'test/loss': 0.2870305627792516, 'test/num_examples': 3581, 'score': 6053.788078546524, 'total_duration': 6800.0280747413635, 'accumulated_submission_time': 6053.788078546524, 'accumulated_eval_time': 668.5687086582184, 'accumulated_logging_time': 3.6376843452453613, 'global_step': 22772, 'preemption_count': 0}), (23080, {'train/ssim': 0.7471797125680106, 'train/loss': 0.2612269265311105, 'validation/ssim': 0.7250651637019907, 'validation/loss': 0.2853362642115398, 'validation/num_examples': 3554, 'test/ssim': 0.7423631230801452, 'test/loss': 0.28666786293982127, 'test/num_examples': 3581, 'score': 6133.37411904335, 'total_duration': 6886.390181541443, 'accumulated_submission_time': 6133.37411904335, 'accumulated_eval_time': 674.3718917369843, 'accumulated_logging_time': 3.6659483909606934, 'global_step': 23080, 'preemption_count': 0}), (23389, {'train/ssim': 0.7469581876482282, 'train/loss': 0.26153632572719027, 'validation/ssim': 0.7252407471115293, 'validation/loss': 0.28531709841683667, 'validation/num_examples': 3554, 'test/ssim': 0.742448343907079, 'test/loss': 0.2866669084665596, 'test/num_examples': 3581, 'score': 6212.9061233997345, 'total_duration': 6972.772681236267, 'accumulated_submission_time': 6212.9061233997345, 'accumulated_eval_time': 680.253331899643, 'accumulated_logging_time': 3.6957108974456787, 'global_step': 23389, 'preemption_count': 0}), (23697, {'train/ssim': 0.7469154085431781, 'train/loss': 0.2614326477050781, 'validation/ssim': 0.7251507571794106, 'validation/loss': 0.2852404695853264, 'validation/num_examples': 3554, 'test/ssim': 0.742470774028728, 'test/loss': 0.28652656680876504, 'test/num_examples': 3581, 'score': 6292.5494956970215, 'total_duration': 7059.341114521027, 'accumulated_submission_time': 6292.5494956970215, 'accumulated_eval_time': 686.161438703537, 'accumulated_logging_time': 3.7250707149505615, 'global_step': 23697, 'preemption_count': 0}), (24001, {'train/ssim': 0.7474967411586216, 'train/loss': 0.2611254964556013, 'validation/ssim': 0.7253958595288759, 'validation/loss': 0.28532589132623454, 'validation/num_examples': 3554, 'test/ssim': 0.742609854418284, 'test/loss': 0.2866861342851159, 'test/num_examples': 3581, 'score': 6372.017669200897, 'total_duration': 7145.681921243668, 'accumulated_submission_time': 6372.017669200897, 'accumulated_eval_time': 692.0328688621521, 'accumulated_logging_time': 3.7542219161987305, 'global_step': 24001, 'preemption_count': 0}), (24310, {'train/ssim': 0.7471049172537667, 'train/loss': 0.2611891371863229, 'validation/ssim': 0.7250921606816263, 'validation/loss': 0.2852296501850907, 'validation/num_examples': 3554, 'test/ssim': 0.7423481242146048, 'test/loss': 0.2865496446086987, 'test/num_examples': 3581, 'score': 6451.636000394821, 'total_duration': 7232.155041694641, 'accumulated_submission_time': 6451.636000394821, 'accumulated_eval_time': 697.8706994056702, 'accumulated_logging_time': 3.7841715812683105, 'global_step': 24310, 'preemption_count': 0}), (24617, {'train/ssim': 0.7481568200247628, 'train/loss': 0.2611300264086042, 'validation/ssim': 0.7263775741242262, 'validation/loss': 0.2850739538636044, 'validation/num_examples': 3554, 'test/ssim': 0.7435450336847249, 'test/loss': 0.28636103387452877, 'test/num_examples': 3581, 'score': 6531.296428442001, 'total_duration': 7318.659018993378, 'accumulated_submission_time': 6531.296428442001, 'accumulated_eval_time': 703.6744980812073, 'accumulated_logging_time': 3.813491106033325, 'global_step': 24617, 'preemption_count': 0}), (24924, {'train/ssim': 0.7476644515991211, 'train/loss': 0.2610858167920794, 'validation/ssim': 0.725747026347953, 'validation/loss': 0.28504005307619934, 'validation/num_examples': 3554, 'test/ssim': 0.7430084833583496, 'test/loss': 0.28638578200267034, 'test/num_examples': 3581, 'score': 6610.758322954178, 'total_duration': 7405.177961349487, 'accumulated_submission_time': 6610.758322954178, 'accumulated_eval_time': 709.7149248123169, 'accumulated_logging_time': 3.8477327823638916, 'global_step': 24924, 'preemption_count': 0}), (25231, {'train/ssim': 0.7486874716622489, 'train/loss': 0.2604835203715733, 'validation/ssim': 0.7263664455982696, 'validation/loss': 0.2848612925411948, 'validation/num_examples': 3554, 'test/ssim': 0.7435075365208741, 'test/loss': 0.2862691317347633, 'test/num_examples': 3581, 'score': 6690.337490081787, 'total_duration': 7491.502110242844, 'accumulated_submission_time': 6690.337490081787, 'accumulated_eval_time': 715.4954693317413, 'accumulated_logging_time': 3.877150297164917, 'global_step': 25231, 'preemption_count': 0}), (25538, {'train/ssim': 0.7481859752110073, 'train/loss': 0.2606891393661499, 'validation/ssim': 0.7260428253156654, 'validation/loss': 0.2848250561372309, 'validation/num_examples': 3554, 'test/ssim': 0.7433212778815275, 'test/loss': 0.28612861963531483, 'test/num_examples': 3581, 'score': 6769.9041385650635, 'total_duration': 7577.916015148163, 'accumulated_submission_time': 6769.9041385650635, 'accumulated_eval_time': 721.3455908298492, 'accumulated_logging_time': 3.9066436290740967, 'global_step': 25538, 'preemption_count': 0}), (25844, {'train/ssim': 0.7483651978628976, 'train/loss': 0.26065937110355925, 'validation/ssim': 0.7263425398758441, 'validation/loss': 0.28474755144951114, 'validation/num_examples': 3554, 'test/ssim': 0.7435469426312482, 'test/loss': 0.2860300702710486, 'test/num_examples': 3581, 'score': 6849.424270868301, 'total_duration': 7664.268394708633, 'accumulated_submission_time': 6849.424270868301, 'accumulated_eval_time': 727.2069861888885, 'accumulated_logging_time': 3.9364676475524902, 'global_step': 25844, 'preemption_count': 0}), (26147, {'train/ssim': 0.7490249361310687, 'train/loss': 0.2603274754115513, 'validation/ssim': 0.7267319695897229, 'validation/loss': 0.28476008821486354, 'validation/num_examples': 3554, 'test/ssim': 0.7438592599177953, 'test/loss': 0.2861611399028728, 'test/num_examples': 3581, 'score': 6928.9466807842255, 'total_duration': 7750.637572050095, 'accumulated_submission_time': 6928.9466807842255, 'accumulated_eval_time': 733.079066991806, 'accumulated_logging_time': 3.96708607673645, 'global_step': 26147, 'preemption_count': 0}), (26454, {'train/ssim': 0.7490370614188058, 'train/loss': 0.26014011246817453, 'validation/ssim': 0.7266109983908976, 'validation/loss': 0.2845534548440138, 'validation/num_examples': 3554, 'test/ssim': 0.7438636232241344, 'test/loss': 0.2858977905034819, 'test/num_examples': 3581, 'score': 7008.612009525299, 'total_duration': 7837.174219369888, 'accumulated_submission_time': 7008.612009525299, 'accumulated_eval_time': 738.9819419384003, 'accumulated_logging_time': 3.996572494506836, 'global_step': 26454, 'preemption_count': 0}), (26759, {'train/ssim': 0.7491796357291085, 'train/loss': 0.25998570237840923, 'validation/ssim': 0.7268591233029685, 'validation/loss': 0.28432683134320486, 'validation/num_examples': 3554, 'test/ssim': 0.7440969237599483, 'test/loss': 0.28561923770856606, 'test/num_examples': 3581, 'score': 7088.215101242065, 'total_duration': 7923.634158849716, 'accumulated_submission_time': 7088.215101242065, 'accumulated_eval_time': 744.8053612709045, 'accumulated_logging_time': 4.027449607849121, 'global_step': 26759, 'preemption_count': 0}), (27065, {'train/ssim': 0.7490682601928711, 'train/loss': 0.25998946598597933, 'validation/ssim': 0.7266760521815209, 'validation/loss': 0.28437393866835786, 'validation/num_examples': 3554, 'test/ssim': 0.7439459806312831, 'test/loss': 0.2856325662458985, 'test/num_examples': 3581, 'score': 7167.906291246414, 'total_duration': 8010.176422357559, 'accumulated_submission_time': 7167.906291246414, 'accumulated_eval_time': 750.6477189064026, 'accumulated_logging_time': 4.057188034057617, 'global_step': 27065, 'preemption_count': 0}), (27142, {'train/ssim': 0.7495369229997907, 'train/loss': 0.26024251324789865, 'validation/ssim': 0.7272361879880065, 'validation/loss': 0.284661923624789, 'validation/num_examples': 3554, 'test/ssim': 0.7443930150010472, 'test/loss': 0.28602894535613305, 'test/num_examples': 3581, 'score': 7186.474029302597, 'total_duration': 8035.417387008667, 'accumulated_submission_time': 7186.474029302597, 'accumulated_eval_time': 756.4104130268097, 'accumulated_logging_time': 4.087508201599121, 'global_step': 27142, 'preemption_count': 0})], 'global_step': 27142}
I1010 06:40:24.600301 140204662740800 submission_runner.py:552] Timing: 7186.474029302597
I1010 06:40:24.600380 140204662740800 submission_runner.py:554] Total number of evals: 91
I1010 06:40:24.600438 140204662740800 submission_runner.py:555] ====================
I1010 06:40:24.600702 140204662740800 submission_runner.py:625] Final fastmri score: 7186.474029302597
