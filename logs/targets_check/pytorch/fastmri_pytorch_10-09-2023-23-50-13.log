torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nesterov.py --tuning_search_space=reference_algorithms/target_setting_algorithms/fastmri/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=fastmri_targets_check/nesterov_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=27142 --torch_compile=true 2>&1 | tee -a /logs/fastmri_pytorch_10-09-2023-23-50-13.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-10-09 23:50:23.233272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-09 23:50:23.233323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1009 23:50:38.012094 139660997596992 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I1009 23:50:38.012189 139853920515904 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I1009 23:50:38.012220 139905728698176 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I1009 23:50:38.012272 140171823425344 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I1009 23:50:38.012296 140056467248960 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I1009 23:50:38.012327 140176211519296 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I1009 23:50:38.012549 139853920515904 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.012607 139905728698176 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.012516 140471065859904 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I1009 23:50:38.012642 140056467248960 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.012555 140427052750656 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I1009 23:50:38.012701 140171823425344 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.012733 140176211519296 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.012866 140471065859904 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.012921 140427052750656 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.022719 139660997596992 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1009 23:50:38.327238 139660997596992 logger_utils.py:76] Creating experiment directory at /experiment_runs/fastmri_targets_check/nesterov_run_0/fastmri_pytorch.
W1009 23:50:39.111495 140427052750656 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.111490 140176211519296 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.111491 139660997596992 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.111507 140171823425344 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.112170 140471065859904 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.112283 139853920515904 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.112776 139905728698176 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1009 23:50:39.112789 140056467248960 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1009 23:50:39.118875 139660997596992 submission_runner.py:507] Using RNG seed 3454700388
I1009 23:50:39.120759 139660997596992 submission_runner.py:516] --- Tuning run 1/1 ---
I1009 23:50:39.120885 139660997596992 submission_runner.py:521] Creating tuning directory at /experiment_runs/fastmri_targets_check/nesterov_run_0/fastmri_pytorch/trial_1.
I1009 23:50:39.121066 139660997596992 logger_utils.py:92] Saving hparams to /experiment_runs/fastmri_targets_check/nesterov_run_0/fastmri_pytorch/trial_1/hparams.json.
I1009 23:50:39.121947 139660997596992 submission_runner.py:191] Initializing dataset.
I1009 23:50:39.122067 139660997596992 submission_runner.py:198] Initializing model.
I1009 23:50:43.469982 139660997596992 submission_runner.py:229] Performing `torch.compile`.
I1009 23:50:43.743021 139660997596992 submission_runner.py:232] Initializing optimizer.
I1009 23:50:44.260746 139660997596992 submission_runner.py:239] Initializing metrics bundle.
I1009 23:50:44.260941 139660997596992 submission_runner.py:257] Initializing checkpoint and logger.
I1009 23:50:44.261554 139660997596992 submission_runner.py:277] Saving meta data to /experiment_runs/fastmri_targets_check/nesterov_run_0/fastmri_pytorch/trial_1/meta_data_0.json.
I1009 23:50:44.261837 139660997596992 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1009 23:50:44.261921 139660997596992 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1009 23:50:44.469925 139660997596992 logger_utils.py:220] Unable to record git information. Continuing without it.
I1009 23:50:44.648761 139660997596992 submission_runner.py:280] Saving flags to /experiment_runs/fastmri_targets_check/nesterov_run_0/fastmri_pytorch/trial_1/flags_0.json.
I1009 23:50:44.739165 139660997596992 submission_runner.py:290] Starting training loop.
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:44,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:50:45,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:50:45,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,101] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,101] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,101] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:50:45,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:45,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:45,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,104] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:50:45,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:45,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:50:45,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:50:50,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:50:50,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:50:50,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:50:50,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:50:50,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:50:50,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:50:50,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:51:30,289] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:30,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,329] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,329] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:30,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:30,680] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-09 23:51:30,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:30,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:30,749] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:30,749] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:30,750] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:33,909] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:33,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:33,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:33,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:33,961] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:33,961] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:33,961] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:33,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:33,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:33,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:33,975] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:33,975] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:33,975] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:33,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:33,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:33,992] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:33,992] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:33,993] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:34,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:34,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:34,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:34,017] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:34,017] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:34,017] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:34,018] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:34,018] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:34,018] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:34,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:34,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:34,043] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:34,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:34,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:34,060] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:34,060] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:34,060] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:34,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:34,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:34,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:34,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:34,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:34,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:34,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:35,606] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,607] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,648] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,661] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,662] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,667] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,683] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,717] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:35,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,732] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:35,750] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:35,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,761] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,762] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:35,776] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,778] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:35,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,780] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:35,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,800] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,800] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:35,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,821] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:35,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,848] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,848] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:35,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:35,867] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:35,867] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:35,868] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:36,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:36,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:36,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:36,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:36,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:36,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:36,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:37,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,104] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,111] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,123] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,126] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,148] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,203] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,216] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,216] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,217] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,229] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,230] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:37,247] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:37,252] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,253] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,255] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,258] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,258] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,258] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:37,305] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,307] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,307] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,308] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:37,342] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:37,342] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:37,342] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:37,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-09 23:51:37,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:37,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:37,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:37,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:37,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:37,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:37,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:38,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:38,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:38,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,687] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,701] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,747] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,747] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,766] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,776] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:38,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,809] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:38,811] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,811] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,812] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:38,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,824] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,825] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,825] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,826] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:38,832] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,848] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,848] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:38,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,869] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,869] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,870] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:38,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:38,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,874] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,874] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,875] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:38,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,882] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,883] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:38,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:38,920] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:38,920] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:38,920] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:39,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:39,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-09 23:51:39,529] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:40,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,550] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:40,591] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,607] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,629] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,633] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,646] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,673] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,697] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,697] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,698] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,708] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,719] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,738] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,739] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,739] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,739] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,739] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,739] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,740] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,740] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:40,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:40,776] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,781] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,805] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,806] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:40,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:40,843] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:40,843] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:40,843] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:40,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:40,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:40,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:40,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:41,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:41,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:41,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:41,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,121] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,161] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,176] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,206] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,233] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,246] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,251] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,251] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,251] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,264] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:41,281] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:41,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,309] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,310] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,310] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,311] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,311] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,311] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,311] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,317] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,318] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,318] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:41,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,352] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,352] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,352] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,360] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,360] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,361] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:41,394] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:41,394] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:41,395] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:41,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:41,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:41,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,118] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:42,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,175] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,187] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,202] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,202] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,202] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,244] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,245] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,267] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:42,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,312] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,314] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,314] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,314] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,315] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,335] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,337] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,337] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,338] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,376] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,376] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,376] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,378] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,378] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,378] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,415] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,417] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,417] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,417] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:42,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,667] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,784] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,785] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,785] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,825] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,841] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,841] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,851] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:42,857] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:42,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,925] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:42,961] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:42,963] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:42,963] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:42,964] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:42,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,001] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,001] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,002] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,002] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,003] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,005] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,005] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,005] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,005] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,006] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,006] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,368] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,432] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,435] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,435] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,436] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:43,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,574] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,649] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,649] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,650] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,695] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,717] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,727] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,760] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,764] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:43,784] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:43,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,791] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,792] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,792] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,815] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,815] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,833] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,838] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,838] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,838] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,838] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,839] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,839] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,840] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:43,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,857] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,857] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,857] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,857] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,859] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,859] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,859] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:43,869] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:43,869] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:43,870] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:43,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-09 23:51:44,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,102] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:44,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,106] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,230] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,231] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,233] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,265] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,269] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,281] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,285] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:44,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:44,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,357] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,358] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:44,393] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,393] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,395] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,395] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,396] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,396] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,396] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,396] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,410] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,410] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,410] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,410] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,410] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,411] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,420] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:44,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:44,422] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:44,423] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:44,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:44,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:44,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,026] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,026] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,027] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:45,086] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:45,132] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,139] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:45,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:45,151] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,152] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:45,163] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,164] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,164] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,165] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:45,187] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,188] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,212] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,212] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,226] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,247] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,248] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,248] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,249] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,249] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,250] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,250] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,272] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,274] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:45,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,650] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,706] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,757] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,770] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,776] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:45,797] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,797] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,862] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,862] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,863] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,900] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,900] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,901] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:45,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-09 23:51:45,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:45,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,970] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,970] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:45,973] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:45,974] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:45,974] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:45,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:46,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:46,000] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:46,001] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:46,002] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:46,005] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:46,005] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:46,005] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:46,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:46,071] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:46,073] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:46,073] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:46,074] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:46,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-09 23:51:46,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:46,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:46,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:46,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:46,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:46,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:46,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:51:47,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,362] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,363] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,426] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,540] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:51:47,579] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:51:47,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-09 23:51:47,915] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:47,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:48,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:48,069] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:48,070] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:48,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:48,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-09 23:51:49,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-09 23:51:50,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-09 23:51:50,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-09 23:51:50,824] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:50,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:50,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:50,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:50,943] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:50,944] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:51,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-09 23:51:52,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-09 23:51:53,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-09 23:51:53,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-09 23:51:53,453] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:53,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:53,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:53,542] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:53,542] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:53,542] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:53,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-09 23:51:54,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-09 23:51:54,020] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:54,089] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:54,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:54,150] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:54,150] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:54,151] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:54,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-09 23:51:54,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-09 23:51:54,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-09 23:51:55,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-09 23:51:55,101] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:55,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:55,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:55,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:55,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:55,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:55,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-09 23:51:55,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-09 23:51:55,682] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:55,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:55,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:55,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:55,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:55,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:56,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-09 23:51:56,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-09 23:51:56,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:56,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:56,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:56,715] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:56,715] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:56,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:56,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-09 23:51:57,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-09 23:51:57,133] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:57,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:57,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:57,256] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:57,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:57,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:57,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-09 23:51:57,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-09 23:51:57,953] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:58,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:58,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:58,026] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:58,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:58,027] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:58,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-09 23:51:58,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-09 23:51:58,484] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:51:58,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:51:58,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:51:58,667] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:51:58,667] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:51:58,667] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:51:59,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-09 23:52:00,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-09 23:52:00,142] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:52:00,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-09 23:52:00,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:00,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:00,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:00,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:00,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:00,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:00,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:00,936] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:01,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-09 23:52:01,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:01,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:01,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:01,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:01,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:01,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:01,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:01,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:02,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-09 23:52:02,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:02,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:02,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:02,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:02,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:02,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:02,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:02,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:03,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:03,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:03,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:03,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:03,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:03,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:03,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:03,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:03,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-09 23:52:03,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-09 23:52:03,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:03,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,018] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-09 23:52:04,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:04,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:04,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:04,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:04,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:04,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-09 23:52:04,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:04,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:04,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:04,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:04,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:04,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:04,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-09 23:52:04,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:04,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:04,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:04,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-09 23:52:04,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:04,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:04,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:04,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:04,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:04,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:05,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:05,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:05,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:05,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:05,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:05,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:05,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:05,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:05,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:05,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:05,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:05,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:05,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:05,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-09 23:52:05,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:05,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:05,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:05,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:05,893] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:05,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:05,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:05,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-09 23:52:06,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:06,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:06,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:06,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:06,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-09 23:52:06,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-09 23:52:06,136] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:06,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:06,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:06,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:06,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:06,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:06,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:06,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:06,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,305] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:06,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:06,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:06,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:06,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:06,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:06,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:06,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:06,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:06,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:06,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-09 23:52:06,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:06,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:06,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:06,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:06,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-09 23:52:06,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:06,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:06,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:06,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:06,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:06,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:06,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-09 23:52:07,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:07,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:07,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-09 23:52:07,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:07,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:07,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:07,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:07,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:07,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-09 23:52:07,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:07,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-09 23:52:07,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:07,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-09 23:52:08,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-09 23:52:08,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-09 23:52:08,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-09 23:52:08,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-09 23:52:08,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-09 23:52:08,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-09 23:52:08,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-09 23:52:08,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-09 23:52:09,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-09 23:52:09,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-09 23:52:09,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-09 23:52:09,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-09 23:52:09,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I1009 23:52:09.562172 139619471447808 logging_writer.py:48] [0] global_step=0, grad_norm=4.097008, loss=1.096054
I1009 23:52:09.576110 139660997596992 pytorch_submission_base.py:86] 0) loss = 1.096, grad_norm = 4.097
I1009 23:52:10.023928 139660997596992 spec.py:321] Evaluating on the training split.
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:03,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,074] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,074] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,075] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,076] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,076] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,076] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,076] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,076] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,076] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,093] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:03,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:03,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:03,097] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:03,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-09 23:53:03,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,865] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:03,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:03,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,908] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:03,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:03,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,928] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:03,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,946] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:03,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:03,960] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:04,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,074] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,100] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,100] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,100] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,141] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,141] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,142] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,152] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,152] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,152] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,154] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,193] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,198] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,198] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,199] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,203] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,204] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,204] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,216] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,217] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,217] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,245] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,246] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,246] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-09 23:53:04,385] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:04,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:04,649] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:04,651] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:04,652] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:04,652] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:04,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-09 23:53:04,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:04,974] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:04,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:04,989] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:04,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:04,991] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:05,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:05,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:05,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:05,006] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:05,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:05,049] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:05,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:05,085] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:05,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,215] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,216] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,279] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,296] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,303] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,303] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,303] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,303] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,304] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,304] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,317] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:05,317] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,318] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,324] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,326] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,327] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,327] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,346] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,346] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,347] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:05,367] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:05,368] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:05,368] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:05,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:05,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-09 23:53:05,958] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,061] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,123] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,151] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,176] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,215] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:06,217] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:06,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,226] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,227] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,227] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,279] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,280] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,280] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,331] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,336] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,377] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,377] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,378] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,386] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,420] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,423] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,423] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,423] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,423] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,424] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,487] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:06,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-09 23:53:06,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,533] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,533] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:06,534] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,536] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:06,536] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:06,536] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:06,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:06,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:06,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:07,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:07,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:07,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:07,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,185] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:07,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,245] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:07,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,307] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,360] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:07,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,394] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,394] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,394] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,481] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,482] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,482] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,486] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-09 23:53:07,503] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,503] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:07,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:07,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,535] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,535] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,536] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,567] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,609] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,613] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,615] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,615] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,616] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:07,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,756] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,756] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,757] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,761] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:07,767] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:07,767] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:07,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:07,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:07,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-09 23:53:08,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,520] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:08,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,669] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,677] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:08,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:08,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:08,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:08,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:08,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-09 23:53:08,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,770] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:08,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:08,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:08,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:08,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:08,876] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:08,877] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:08,877] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:08,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:08,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:08,888] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:08,888] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:08,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:08,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:08,916] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:08,916] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,918] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:08,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:08,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:08,919] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-09 23:53:08,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:08,934] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:08,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:08,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:08,980] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:08,982] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:08,982] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:08,982] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:09,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:09,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-09 23:53:09,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,068] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:09,092] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,094] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,095] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,095] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:09,096] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,116] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:09,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,118] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,118] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,119] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,124] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,124] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,125] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:09,131] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:09,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:09,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:09,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,257] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,257] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,258] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,275] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,275] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,276] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,316] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:09,344] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:09,350] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,359] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,359] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,359] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,364] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,365] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,380] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,381] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,381] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,515] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,518] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,518] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,518] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-09 23:53:09,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,526] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,526] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:09,567] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:09,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:09,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,712] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:09,736] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:09,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,771] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,771] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,771] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:09,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:09,816] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:09,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:09,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:09,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:09,919] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,919] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,919] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:09,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:09,944] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:09,944] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:09,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:09,985] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:10,014] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,015] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,022] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:10,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:10,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,062] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,062] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,062] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,063] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,155] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:10,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:10,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,182] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,199] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,199] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,199] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,207] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,207] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,207] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,234] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,234] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,295] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,295] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:10,330] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,332] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,332] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,332] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:10,377] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,379] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,380] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,380] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,436] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:10,459] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,491] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,503] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,503] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,504] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:10,605] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,612] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,612] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,612] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,633] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,633] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,634] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-09 23:53:10,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:10,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,784] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:10,801] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,806] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,808] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,808] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,809] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:10,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:10,897] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:10,911] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:10,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:10,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-09 23:53:10,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:10,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:10,961] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:10,961] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:10,961] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:10,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,020] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,021] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,022] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,022] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:11,030] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:11,033] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-09 23:53:11,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,050] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:11,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,116] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:11,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,133] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,133] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,134] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,139] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,141] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,141] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,141] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:11,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:11,252] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,258] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,258] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,258] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,260] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,276] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,276] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,277] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,310] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,310] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,312] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,334] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,334] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,334] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:11,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,365] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,373] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-09 23:53:11,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,436] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,438] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,438] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,439] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-09 23:53:11,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,483] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,490] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,490] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,491] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:11,503] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-09 23:53:11,562] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,567] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,607] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,609] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,609] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,609] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,613] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,615] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,615] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,616] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:11,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,727] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,727] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,730] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,730] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,731] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:11,816] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-09 23:53:11,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:11,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:11,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:11,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:11,945] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:11,945] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:11,945] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:11,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:11,987] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:11,992] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:11,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-09 23:53:12,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,070] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,072] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,072] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,072] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:12,098] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,108] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,108] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,109] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:12,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-09 23:53:12,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,171] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,208] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,232] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,247] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,247] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-09 23:53:12,285] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,330] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,403] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,458] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,505] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:12,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,533] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,534] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,534] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,539] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,539] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,612] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,631] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:12,633] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,675] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-09 23:53:12,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,689] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,729] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,730] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,730] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,745] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:12,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,778] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-09 23:53:12,808] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:12,832] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:12,834] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:12,835] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:12,835] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:12,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:12,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:12,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:12,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:13,001] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:13,002] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:13,002] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:13,005] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:13,005] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:13,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:13,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:13,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:13,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:13,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:13,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:13,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:13,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:13,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,136] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:13,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:13,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-09 23:53:13,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,300] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,317] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-09 23:53:13,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-09 23:53:13,570] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,571] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:13,583] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:13,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:13,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:13,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:13,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:13,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-09 23:53:13,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-09 23:53:14,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-09 23:53:14,001] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:14,188] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:14,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:14,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:14,275] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:14,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:14,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-09 23:53:14,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-09 23:53:14,733] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:14,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:14,968] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:14,970] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:14,970] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:14,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:15,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-09 23:53:15,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-09 23:53:15,255] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-09 23:53:15,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-09 23:53:15,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-09 23:53:15,634] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-09 23:53:15,635] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-09 23:53:15,635] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-09 23:53:15,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-09 23:53:16,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-09 23:53:16,676] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1009 23:53:44.247775 139660997596992 spec.py:333] Evaluating on the validation split.
I1009 23:54:45.936196 139660997596992 spec.py:349] Evaluating on the test split.
I1009 23:55:49.821804 139660997596992 submission_runner.py:381] Time since start: 305.08s, 	Step: 1, 	{'train/ssim': 0.1878136055810111, 'train/loss': 1.106189250946045, 'validation/ssim': 0.1798226566346898, 'validation/loss': 1.1171398946389632, 'validation/num_examples': 3554, 'test/ssim': 0.20116746056116483, 'test/loss': 1.1132284130873011, 'test/num_examples': 3581, 'score': 84.83852958679199, 'total_duration': 305.0831594467163, 'accumulated_submission_time': 84.83852958679199, 'accumulated_eval_time': 219.79801106452942, 'accumulated_logging_time': 0}
I1009 23:55:49.843714 139598139205376 logging_writer.py:48] [1] accumulated_eval_time=219.798011, accumulated_logging_time=0, accumulated_submission_time=84.838530, global_step=1, preemption_count=0, score=84.838530, test/loss=1.113228, test/num_examples=3581, test/ssim=0.201167, total_duration=305.083159, train/loss=1.106189, train/ssim=0.187814, validation/loss=1.117140, validation/num_examples=3554, validation/ssim=0.179823
I1009 23:55:50.321244 140427052750656 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321266 140176211519296 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321266 139853920515904 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321265 140171823425344 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321391 140471065859904 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321382 140056467248960 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321387 139905728698176 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.321238 139660997596992 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1009 23:55:50.501075 139598130812672 logging_writer.py:48] [1] global_step=1, grad_norm=4.272449, loss=1.063859
I1009 23:55:50.504710 139660997596992 pytorch_submission_base.py:86] 1) loss = 1.064, grad_norm = 4.272
I1009 23:55:50.566970 139598139205376 logging_writer.py:48] [2] global_step=2, grad_norm=3.916240, loss=1.106826
I1009 23:55:50.571295 139660997596992 pytorch_submission_base.py:86] 2) loss = 1.107, grad_norm = 3.916
I1009 23:55:50.631678 139598130812672 logging_writer.py:48] [3] global_step=3, grad_norm=3.681620, loss=1.062873
I1009 23:55:50.637139 139660997596992 pytorch_submission_base.py:86] 3) loss = 1.063, grad_norm = 3.682
I1009 23:55:50.707483 139598139205376 logging_writer.py:48] [4] global_step=4, grad_norm=4.193063, loss=1.075754
I1009 23:55:50.712004 139660997596992 pytorch_submission_base.py:86] 4) loss = 1.076, grad_norm = 4.193
I1009 23:55:50.779340 139598130812672 logging_writer.py:48] [5] global_step=5, grad_norm=3.813848, loss=1.100201
I1009 23:55:50.783111 139660997596992 pytorch_submission_base.py:86] 5) loss = 1.100, grad_norm = 3.814
I1009 23:55:50.863475 139598139205376 logging_writer.py:48] [6] global_step=6, grad_norm=3.834663, loss=1.068443
I1009 23:55:50.870180 139660997596992 pytorch_submission_base.py:86] 6) loss = 1.068, grad_norm = 3.835
I1009 23:55:50.947765 139598130812672 logging_writer.py:48] [7] global_step=7, grad_norm=3.400948, loss=1.109922
I1009 23:55:50.953064 139660997596992 pytorch_submission_base.py:86] 7) loss = 1.110, grad_norm = 3.401
I1009 23:55:51.030604 139598139205376 logging_writer.py:48] [8] global_step=8, grad_norm=3.950598, loss=1.125829
I1009 23:55:51.036752 139660997596992 pytorch_submission_base.py:86] 8) loss = 1.126, grad_norm = 3.951
I1009 23:55:51.123801 139598130812672 logging_writer.py:48] [9] global_step=9, grad_norm=4.028996, loss=0.985964
I1009 23:55:51.128513 139660997596992 pytorch_submission_base.py:86] 9) loss = 0.986, grad_norm = 4.029
I1009 23:55:51.213742 139598139205376 logging_writer.py:48] [10] global_step=10, grad_norm=3.778094, loss=0.987356
I1009 23:55:51.217725 139660997596992 pytorch_submission_base.py:86] 10) loss = 0.987, grad_norm = 3.778
I1009 23:55:51.294956 139598130812672 logging_writer.py:48] [11] global_step=11, grad_norm=3.340118, loss=0.981601
I1009 23:55:51.300818 139660997596992 pytorch_submission_base.py:86] 11) loss = 0.982, grad_norm = 3.340
I1009 23:55:51.386765 139598139205376 logging_writer.py:48] [12] global_step=12, grad_norm=3.159564, loss=0.949569
I1009 23:55:51.391381 139660997596992 pytorch_submission_base.py:86] 12) loss = 0.950, grad_norm = 3.160
I1009 23:55:51.489976 139598130812672 logging_writer.py:48] [13] global_step=13, grad_norm=2.942321, loss=0.885891
I1009 23:55:51.495684 139660997596992 pytorch_submission_base.py:86] 13) loss = 0.886, grad_norm = 2.942
I1009 23:55:51.562202 139598139205376 logging_writer.py:48] [14] global_step=14, grad_norm=3.000143, loss=0.824179
I1009 23:55:51.567892 139660997596992 pytorch_submission_base.py:86] 14) loss = 0.824, grad_norm = 3.000
I1009 23:55:51.754067 139598130812672 logging_writer.py:48] [15] global_step=15, grad_norm=2.598559, loss=0.773610
I1009 23:55:51.759899 139660997596992 pytorch_submission_base.py:86] 15) loss = 0.774, grad_norm = 2.599
I1009 23:55:52.020354 139598139205376 logging_writer.py:48] [16] global_step=16, grad_norm=2.176729, loss=0.755731
I1009 23:55:52.025260 139660997596992 pytorch_submission_base.py:86] 16) loss = 0.756, grad_norm = 2.177
I1009 23:55:52.330630 139598130812672 logging_writer.py:48] [17] global_step=17, grad_norm=1.957555, loss=0.695213
I1009 23:55:52.336471 139660997596992 pytorch_submission_base.py:86] 17) loss = 0.695, grad_norm = 1.958
I1009 23:55:52.590301 139598139205376 logging_writer.py:48] [18] global_step=18, grad_norm=1.584215, loss=0.785507
I1009 23:55:52.597278 139660997596992 pytorch_submission_base.py:86] 18) loss = 0.786, grad_norm = 1.584
I1009 23:55:52.901969 139598130812672 logging_writer.py:48] [19] global_step=19, grad_norm=1.398888, loss=0.792137
I1009 23:55:52.907928 139660997596992 pytorch_submission_base.py:86] 19) loss = 0.792, grad_norm = 1.399
I1009 23:55:53.172792 139598139205376 logging_writer.py:48] [20] global_step=20, grad_norm=1.324452, loss=0.724573
I1009 23:55:53.178266 139660997596992 pytorch_submission_base.py:86] 20) loss = 0.725, grad_norm = 1.324
I1009 23:55:53.436379 139598130812672 logging_writer.py:48] [21] global_step=21, grad_norm=1.382073, loss=0.687605
I1009 23:55:53.442958 139660997596992 pytorch_submission_base.py:86] 21) loss = 0.688, grad_norm = 1.382
I1009 23:55:53.714431 139598139205376 logging_writer.py:48] [22] global_step=22, grad_norm=1.534978, loss=0.673223
I1009 23:55:53.719780 139660997596992 pytorch_submission_base.py:86] 22) loss = 0.673, grad_norm = 1.535
I1009 23:55:53.970664 139598130812672 logging_writer.py:48] [23] global_step=23, grad_norm=1.548275, loss=0.727681
I1009 23:55:53.976921 139660997596992 pytorch_submission_base.py:86] 23) loss = 0.728, grad_norm = 1.548
I1009 23:55:54.236872 139598139205376 logging_writer.py:48] [24] global_step=24, grad_norm=1.738721, loss=0.619490
I1009 23:55:54.244078 139660997596992 pytorch_submission_base.py:86] 24) loss = 0.619, grad_norm = 1.739
I1009 23:55:54.532562 139598130812672 logging_writer.py:48] [25] global_step=25, grad_norm=1.651693, loss=0.713480
I1009 23:55:54.538053 139660997596992 pytorch_submission_base.py:86] 25) loss = 0.713, grad_norm = 1.652
I1009 23:55:54.762593 139598139205376 logging_writer.py:48] [26] global_step=26, grad_norm=2.004477, loss=0.601717
I1009 23:55:54.766188 139660997596992 pytorch_submission_base.py:86] 26) loss = 0.602, grad_norm = 2.004
I1009 23:55:55.065407 139598130812672 logging_writer.py:48] [27] global_step=27, grad_norm=2.081798, loss=0.568111
I1009 23:55:55.069102 139660997596992 pytorch_submission_base.py:86] 27) loss = 0.568, grad_norm = 2.082
I1009 23:55:55.307851 139598139205376 logging_writer.py:48] [28] global_step=28, grad_norm=2.060204, loss=0.647924
I1009 23:55:55.311796 139660997596992 pytorch_submission_base.py:86] 28) loss = 0.648, grad_norm = 2.060
I1009 23:55:55.601413 139598130812672 logging_writer.py:48] [29] global_step=29, grad_norm=2.128104, loss=0.670403
I1009 23:55:55.605684 139660997596992 pytorch_submission_base.py:86] 29) loss = 0.670, grad_norm = 2.128
I1009 23:55:55.863996 139598139205376 logging_writer.py:48] [30] global_step=30, grad_norm=2.186153, loss=0.612301
I1009 23:55:55.867721 139660997596992 pytorch_submission_base.py:86] 30) loss = 0.612, grad_norm = 2.186
I1009 23:55:56.130416 139598130812672 logging_writer.py:48] [31] global_step=31, grad_norm=2.113553, loss=0.624206
I1009 23:55:56.134154 139660997596992 pytorch_submission_base.py:86] 31) loss = 0.624, grad_norm = 2.114
I1009 23:55:56.375136 139598139205376 logging_writer.py:48] [32] global_step=32, grad_norm=2.202298, loss=0.606153
I1009 23:55:56.378549 139660997596992 pytorch_submission_base.py:86] 32) loss = 0.606, grad_norm = 2.202
I1009 23:55:56.664467 139598130812672 logging_writer.py:48] [33] global_step=33, grad_norm=2.120587, loss=0.571298
I1009 23:55:56.670668 139660997596992 pytorch_submission_base.py:86] 33) loss = 0.571, grad_norm = 2.121
I1009 23:55:56.880834 139598139205376 logging_writer.py:48] [34] global_step=34, grad_norm=2.049982, loss=0.535696
I1009 23:55:56.886234 139660997596992 pytorch_submission_base.py:86] 34) loss = 0.536, grad_norm = 2.050
I1009 23:55:57.195673 139598130812672 logging_writer.py:48] [35] global_step=35, grad_norm=1.772626, loss=0.649959
I1009 23:55:57.201689 139660997596992 pytorch_submission_base.py:86] 35) loss = 0.650, grad_norm = 1.773
I1009 23:55:57.454303 139598139205376 logging_writer.py:48] [36] global_step=36, grad_norm=1.717329, loss=0.480839
I1009 23:55:57.458612 139660997596992 pytorch_submission_base.py:86] 36) loss = 0.481, grad_norm = 1.717
I1009 23:55:57.739695 139598130812672 logging_writer.py:48] [37] global_step=37, grad_norm=1.523826, loss=0.435626
I1009 23:55:57.745552 139660997596992 pytorch_submission_base.py:86] 37) loss = 0.436, grad_norm = 1.524
I1009 23:55:58.045240 139598139205376 logging_writer.py:48] [38] global_step=38, grad_norm=1.259855, loss=0.475457
I1009 23:55:58.051176 139660997596992 pytorch_submission_base.py:86] 38) loss = 0.475, grad_norm = 1.260
I1009 23:55:58.267777 139598130812672 logging_writer.py:48] [39] global_step=39, grad_norm=0.858808, loss=0.446916
I1009 23:55:58.272718 139660997596992 pytorch_submission_base.py:86] 39) loss = 0.447, grad_norm = 0.859
I1009 23:55:58.519264 139598139205376 logging_writer.py:48] [40] global_step=40, grad_norm=0.672487, loss=0.494668
I1009 23:55:58.524159 139660997596992 pytorch_submission_base.py:86] 40) loss = 0.495, grad_norm = 0.672
I1009 23:55:58.869897 139598130812672 logging_writer.py:48] [41] global_step=41, grad_norm=0.527978, loss=0.522681
I1009 23:55:58.876036 139660997596992 pytorch_submission_base.py:86] 41) loss = 0.523, grad_norm = 0.528
I1009 23:55:59.118490 139598139205376 logging_writer.py:48] [42] global_step=42, grad_norm=0.698012, loss=0.397492
I1009 23:55:59.125143 139660997596992 pytorch_submission_base.py:86] 42) loss = 0.397, grad_norm = 0.698
I1009 23:55:59.367756 139598130812672 logging_writer.py:48] [43] global_step=43, grad_norm=0.738823, loss=0.490571
I1009 23:55:59.374501 139660997596992 pytorch_submission_base.py:86] 43) loss = 0.491, grad_norm = 0.739
I1009 23:55:59.649052 139598139205376 logging_writer.py:48] [44] global_step=44, grad_norm=1.032962, loss=0.432792
I1009 23:55:59.652935 139660997596992 pytorch_submission_base.py:86] 44) loss = 0.433, grad_norm = 1.033
I1009 23:55:59.921118 139598130812672 logging_writer.py:48] [45] global_step=45, grad_norm=0.872208, loss=0.425675
I1009 23:55:59.924901 139660997596992 pytorch_submission_base.py:86] 45) loss = 0.426, grad_norm = 0.872
I1009 23:56:00.181474 139598139205376 logging_writer.py:48] [46] global_step=46, grad_norm=0.903869, loss=0.549697
I1009 23:56:00.184967 139660997596992 pytorch_submission_base.py:86] 46) loss = 0.550, grad_norm = 0.904
I1009 23:56:00.437486 139598130812672 logging_writer.py:48] [47] global_step=47, grad_norm=1.196701, loss=0.469542
I1009 23:56:00.441164 139660997596992 pytorch_submission_base.py:86] 47) loss = 0.470, grad_norm = 1.197
I1009 23:56:00.692843 139598139205376 logging_writer.py:48] [48] global_step=48, grad_norm=1.148053, loss=0.484389
I1009 23:56:00.697597 139660997596992 pytorch_submission_base.py:86] 48) loss = 0.484, grad_norm = 1.148
I1009 23:56:00.975130 139598130812672 logging_writer.py:48] [49] global_step=49, grad_norm=1.232095, loss=0.474149
I1009 23:56:00.979145 139660997596992 pytorch_submission_base.py:86] 49) loss = 0.474, grad_norm = 1.232
I1009 23:56:01.230114 139598139205376 logging_writer.py:48] [50] global_step=50, grad_norm=1.375578, loss=0.536522
I1009 23:56:01.250048 139660997596992 pytorch_submission_base.py:86] 50) loss = 0.537, grad_norm = 1.376
I1009 23:56:01.646568 139598130812672 logging_writer.py:48] [51] global_step=51, grad_norm=1.130140, loss=0.504007
I1009 23:56:01.654164 139660997596992 pytorch_submission_base.py:86] 51) loss = 0.504, grad_norm = 1.130
I1009 23:56:01.935611 139598139205376 logging_writer.py:48] [52] global_step=52, grad_norm=1.384107, loss=0.463981
I1009 23:56:01.939982 139660997596992 pytorch_submission_base.py:86] 52) loss = 0.464, grad_norm = 1.384
I1009 23:56:02.291176 139598130812672 logging_writer.py:48] [53] global_step=53, grad_norm=1.044802, loss=0.502698
I1009 23:56:02.298837 139660997596992 pytorch_submission_base.py:86] 53) loss = 0.503, grad_norm = 1.045
I1009 23:56:02.551718 139598139205376 logging_writer.py:48] [54] global_step=54, grad_norm=1.184464, loss=0.452034
I1009 23:56:02.559583 139660997596992 pytorch_submission_base.py:86] 54) loss = 0.452, grad_norm = 1.184
I1009 23:56:02.807914 139598130812672 logging_writer.py:48] [55] global_step=55, grad_norm=1.040224, loss=0.503985
I1009 23:56:02.813376 139660997596992 pytorch_submission_base.py:86] 55) loss = 0.504, grad_norm = 1.040
I1009 23:56:03.068668 139598139205376 logging_writer.py:48] [56] global_step=56, grad_norm=0.984421, loss=0.534701
I1009 23:56:03.075759 139660997596992 pytorch_submission_base.py:86] 56) loss = 0.535, grad_norm = 0.984
I1009 23:56:03.353283 139598130812672 logging_writer.py:48] [57] global_step=57, grad_norm=0.837120, loss=0.515353
I1009 23:56:03.359764 139660997596992 pytorch_submission_base.py:86] 57) loss = 0.515, grad_norm = 0.837
I1009 23:56:03.605044 139598139205376 logging_writer.py:48] [58] global_step=58, grad_norm=1.027654, loss=0.498126
I1009 23:56:03.610634 139660997596992 pytorch_submission_base.py:86] 58) loss = 0.498, grad_norm = 1.028
I1009 23:56:03.891577 139598130812672 logging_writer.py:48] [59] global_step=59, grad_norm=0.961478, loss=0.532523
I1009 23:56:03.896823 139660997596992 pytorch_submission_base.py:86] 59) loss = 0.533, grad_norm = 0.961
I1009 23:56:04.145813 139598139205376 logging_writer.py:48] [60] global_step=60, grad_norm=0.992697, loss=0.482282
I1009 23:56:04.151540 139660997596992 pytorch_submission_base.py:86] 60) loss = 0.482, grad_norm = 0.993
I1009 23:56:04.370565 139598130812672 logging_writer.py:48] [61] global_step=61, grad_norm=0.997679, loss=0.552451
I1009 23:56:04.375828 139660997596992 pytorch_submission_base.py:86] 61) loss = 0.552, grad_norm = 0.998
I1009 23:56:04.664380 139598139205376 logging_writer.py:48] [62] global_step=62, grad_norm=1.074359, loss=0.452454
I1009 23:56:04.668024 139660997596992 pytorch_submission_base.py:86] 62) loss = 0.452, grad_norm = 1.074
I1009 23:56:04.935561 139598130812672 logging_writer.py:48] [63] global_step=63, grad_norm=0.963985, loss=0.445435
I1009 23:56:04.939627 139660997596992 pytorch_submission_base.py:86] 63) loss = 0.445, grad_norm = 0.964
I1009 23:56:05.165195 139598139205376 logging_writer.py:48] [64] global_step=64, grad_norm=1.083236, loss=0.452594
I1009 23:56:05.170211 139660997596992 pytorch_submission_base.py:86] 64) loss = 0.453, grad_norm = 1.083
I1009 23:56:05.460623 139598130812672 logging_writer.py:48] [65] global_step=65, grad_norm=1.096896, loss=0.425218
I1009 23:56:05.464312 139660997596992 pytorch_submission_base.py:86] 65) loss = 0.425, grad_norm = 1.097
I1009 23:56:05.778896 139598139205376 logging_writer.py:48] [66] global_step=66, grad_norm=1.076802, loss=0.422630
I1009 23:56:05.782748 139660997596992 pytorch_submission_base.py:86] 66) loss = 0.423, grad_norm = 1.077
I1009 23:56:06.026550 139598130812672 logging_writer.py:48] [67] global_step=67, grad_norm=0.998260, loss=0.399545
I1009 23:56:06.033012 139660997596992 pytorch_submission_base.py:86] 67) loss = 0.400, grad_norm = 0.998
I1009 23:56:06.337084 139598139205376 logging_writer.py:48] [68] global_step=68, grad_norm=1.067323, loss=0.375873
I1009 23:56:06.342436 139660997596992 pytorch_submission_base.py:86] 68) loss = 0.376, grad_norm = 1.067
I1009 23:56:06.552132 139598130812672 logging_writer.py:48] [69] global_step=69, grad_norm=0.866996, loss=0.430824
I1009 23:56:06.555749 139660997596992 pytorch_submission_base.py:86] 69) loss = 0.431, grad_norm = 0.867
I1009 23:56:06.837811 139598139205376 logging_writer.py:48] [70] global_step=70, grad_norm=0.817067, loss=0.419682
I1009 23:56:06.841186 139660997596992 pytorch_submission_base.py:86] 70) loss = 0.420, grad_norm = 0.817
I1009 23:56:07.084843 139598130812672 logging_writer.py:48] [71] global_step=71, grad_norm=0.705841, loss=0.342187
I1009 23:56:07.088732 139660997596992 pytorch_submission_base.py:86] 71) loss = 0.342, grad_norm = 0.706
I1009 23:56:07.347084 139598139205376 logging_writer.py:48] [72] global_step=72, grad_norm=0.476039, loss=0.496114
I1009 23:56:07.350366 139660997596992 pytorch_submission_base.py:86] 72) loss = 0.496, grad_norm = 0.476
I1009 23:56:07.609507 139598130812672 logging_writer.py:48] [73] global_step=73, grad_norm=0.660336, loss=0.338396
I1009 23:56:07.615108 139660997596992 pytorch_submission_base.py:86] 73) loss = 0.338, grad_norm = 0.660
I1009 23:56:07.931127 139598139205376 logging_writer.py:48] [74] global_step=74, grad_norm=0.567355, loss=0.348190
I1009 23:56:07.937315 139660997596992 pytorch_submission_base.py:86] 74) loss = 0.348, grad_norm = 0.567
I1009 23:56:08.227224 139598130812672 logging_writer.py:48] [75] global_step=75, grad_norm=0.441724, loss=0.378355
I1009 23:56:08.233705 139660997596992 pytorch_submission_base.py:86] 75) loss = 0.378, grad_norm = 0.442
I1009 23:56:08.476493 139598139205376 logging_writer.py:48] [76] global_step=76, grad_norm=0.535568, loss=0.396627
I1009 23:56:08.481835 139660997596992 pytorch_submission_base.py:86] 76) loss = 0.397, grad_norm = 0.536
I1009 23:56:08.706438 139598130812672 logging_writer.py:48] [77] global_step=77, grad_norm=0.574666, loss=0.403855
I1009 23:56:08.710997 139660997596992 pytorch_submission_base.py:86] 77) loss = 0.404, grad_norm = 0.575
I1009 23:56:09.078271 139598139205376 logging_writer.py:48] [78] global_step=78, grad_norm=0.695800, loss=0.327358
I1009 23:56:09.083218 139660997596992 pytorch_submission_base.py:86] 78) loss = 0.327, grad_norm = 0.696
I1009 23:56:09.352857 139598130812672 logging_writer.py:48] [79] global_step=79, grad_norm=0.758627, loss=0.270834
I1009 23:56:09.358537 139660997596992 pytorch_submission_base.py:86] 79) loss = 0.271, grad_norm = 0.759
I1009 23:56:09.629065 139598139205376 logging_writer.py:48] [80] global_step=80, grad_norm=0.885348, loss=0.305741
I1009 23:56:09.634654 139660997596992 pytorch_submission_base.py:86] 80) loss = 0.306, grad_norm = 0.885
I1009 23:56:09.886796 139598130812672 logging_writer.py:48] [81] global_step=81, grad_norm=0.806001, loss=0.323938
I1009 23:56:09.890451 139660997596992 pytorch_submission_base.py:86] 81) loss = 0.324, grad_norm = 0.806
I1009 23:56:10.150326 139598139205376 logging_writer.py:48] [82] global_step=82, grad_norm=0.887066, loss=0.342554
I1009 23:56:10.155597 139660997596992 pytorch_submission_base.py:86] 82) loss = 0.343, grad_norm = 0.887
I1009 23:56:10.432992 139598130812672 logging_writer.py:48] [83] global_step=83, grad_norm=0.828017, loss=0.353100
I1009 23:56:10.437729 139660997596992 pytorch_submission_base.py:86] 83) loss = 0.353, grad_norm = 0.828
I1009 23:56:10.726203 139598139205376 logging_writer.py:48] [84] global_step=84, grad_norm=0.800474, loss=0.380406
I1009 23:56:10.731520 139660997596992 pytorch_submission_base.py:86] 84) loss = 0.380, grad_norm = 0.800
I1009 23:56:10.940630 139598130812672 logging_writer.py:48] [85] global_step=85, grad_norm=0.889630, loss=0.364127
I1009 23:56:10.944298 139660997596992 pytorch_submission_base.py:86] 85) loss = 0.364, grad_norm = 0.890
I1009 23:56:11.254755 139598139205376 logging_writer.py:48] [86] global_step=86, grad_norm=0.886389, loss=0.383411
I1009 23:56:11.259709 139660997596992 pytorch_submission_base.py:86] 86) loss = 0.383, grad_norm = 0.886
I1009 23:56:11.484585 139598130812672 logging_writer.py:48] [87] global_step=87, grad_norm=0.872091, loss=0.325808
I1009 23:56:11.488024 139660997596992 pytorch_submission_base.py:86] 87) loss = 0.326, grad_norm = 0.872
I1009 23:56:11.815351 139598139205376 logging_writer.py:48] [88] global_step=88, grad_norm=0.807737, loss=0.427456
I1009 23:56:11.818815 139660997596992 pytorch_submission_base.py:86] 88) loss = 0.427, grad_norm = 0.808
I1009 23:56:12.078913 139598130812672 logging_writer.py:48] [89] global_step=89, grad_norm=0.820933, loss=0.311837
I1009 23:56:12.082211 139660997596992 pytorch_submission_base.py:86] 89) loss = 0.312, grad_norm = 0.821
I1009 23:56:12.418913 139598139205376 logging_writer.py:48] [90] global_step=90, grad_norm=0.692168, loss=0.436738
I1009 23:56:12.422154 139660997596992 pytorch_submission_base.py:86] 90) loss = 0.437, grad_norm = 0.692
I1009 23:56:12.628839 139598130812672 logging_writer.py:48] [91] global_step=91, grad_norm=0.687130, loss=0.330957
I1009 23:56:12.633734 139660997596992 pytorch_submission_base.py:86] 91) loss = 0.331, grad_norm = 0.687
I1009 23:56:12.920419 139598139205376 logging_writer.py:48] [92] global_step=92, grad_norm=0.668882, loss=0.293983
I1009 23:56:12.924934 139660997596992 pytorch_submission_base.py:86] 92) loss = 0.294, grad_norm = 0.669
I1009 23:56:13.220444 139598130812672 logging_writer.py:48] [93] global_step=93, grad_norm=0.658794, loss=0.287596
I1009 23:56:13.226182 139660997596992 pytorch_submission_base.py:86] 93) loss = 0.288, grad_norm = 0.659
I1009 23:56:13.507436 139598139205376 logging_writer.py:48] [94] global_step=94, grad_norm=0.615435, loss=0.422354
I1009 23:56:13.510814 139660997596992 pytorch_submission_base.py:86] 94) loss = 0.422, grad_norm = 0.615
I1009 23:56:13.808573 139598130812672 logging_writer.py:48] [95] global_step=95, grad_norm=0.580581, loss=0.273509
I1009 23:56:13.812056 139660997596992 pytorch_submission_base.py:86] 95) loss = 0.274, grad_norm = 0.581
I1009 23:56:14.081423 139598139205376 logging_writer.py:48] [96] global_step=96, grad_norm=0.512701, loss=0.379224
I1009 23:56:14.084626 139660997596992 pytorch_submission_base.py:86] 96) loss = 0.379, grad_norm = 0.513
I1009 23:56:14.425943 139598130812672 logging_writer.py:48] [97] global_step=97, grad_norm=0.582856, loss=0.311515
I1009 23:56:14.433528 139660997596992 pytorch_submission_base.py:86] 97) loss = 0.312, grad_norm = 0.583
I1009 23:56:14.687461 139598139205376 logging_writer.py:48] [98] global_step=98, grad_norm=0.510837, loss=0.366577
I1009 23:56:14.692577 139660997596992 pytorch_submission_base.py:86] 98) loss = 0.367, grad_norm = 0.511
I1009 23:56:14.989285 139598130812672 logging_writer.py:48] [99] global_step=99, grad_norm=0.564537, loss=0.318777
I1009 23:56:14.994896 139660997596992 pytorch_submission_base.py:86] 99) loss = 0.319, grad_norm = 0.565
I1009 23:56:15.302516 139598139205376 logging_writer.py:48] [100] global_step=100, grad_norm=0.604566, loss=0.292381
I1009 23:56:15.308336 139660997596992 pytorch_submission_base.py:86] 100) loss = 0.292, grad_norm = 0.605
I1009 23:57:10.468619 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1009 23:57:12.364219 139660997596992 spec.py:333] Evaluating on the validation split.
I1009 23:57:14.303617 139660997596992 spec.py:349] Evaluating on the test split.
I1009 23:57:16.268071 139660997596992 submission_runner.py:381] Time since start: 391.53s, 	Step: 302, 	{'train/ssim': 0.673194340297154, 'train/loss': 0.32891099793570383, 'validation/ssim': 0.6480368043709552, 'validation/loss': 0.35065342994908905, 'validation/num_examples': 3554, 'test/ssim': 0.6679116179576235, 'test/loss': 0.3517704388177185, 'test/num_examples': 3581, 'score': 164.36816549301147, 'total_duration': 391.5294907093048, 'accumulated_submission_time': 164.36816549301147, 'accumulated_eval_time': 225.59786081314087, 'accumulated_logging_time': 0.030315637588500977}
I1009 23:57:16.288431 139598130812672 logging_writer.py:48] [302] accumulated_eval_time=225.597861, accumulated_logging_time=0.030316, accumulated_submission_time=164.368165, global_step=302, preemption_count=0, score=164.368165, test/loss=0.351770, test/num_examples=3581, test/ssim=0.667912, total_duration=391.529491, train/loss=0.328911, train/ssim=0.673194, validation/loss=0.350653, validation/num_examples=3554, validation/ssim=0.648037
I1009 23:58:28.146560 139598139205376 logging_writer.py:48] [500] global_step=500, grad_norm=0.154763, loss=0.215680
I1009 23:58:28.151625 139660997596992 pytorch_submission_base.py:86] 500) loss = 0.216, grad_norm = 0.155
I1009 23:58:36.710432 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1009 23:58:38.578969 139660997596992 spec.py:333] Evaluating on the validation split.
I1009 23:58:40.498060 139660997596992 spec.py:349] Evaluating on the test split.
I1009 23:58:42.383419 139660997596992 submission_runner.py:381] Time since start: 477.64s, 	Step: 522, 	{'train/ssim': 0.7009882245744977, 'train/loss': 0.3085228034428188, 'validation/ssim': 0.6795110235606008, 'validation/loss': 0.3280221641768078, 'validation/num_examples': 3554, 'test/ssim': 0.6983904035622033, 'test/loss': 0.3293236820633203, 'test/num_examples': 3581, 'score': 243.81787967681885, 'total_duration': 477.64457869529724, 'accumulated_submission_time': 243.81787967681885, 'accumulated_eval_time': 231.2707793712616, 'accumulated_logging_time': 0.0634005069732666}
I1009 23:58:42.402014 139598130812672 logging_writer.py:48] [522] accumulated_eval_time=231.270779, accumulated_logging_time=0.063401, accumulated_submission_time=243.817880, global_step=522, preemption_count=0, score=243.817880, test/loss=0.329324, test/num_examples=3581, test/ssim=0.698390, total_duration=477.644579, train/loss=0.308523, train/ssim=0.700988, validation/loss=0.328022, validation/num_examples=3554, validation/ssim=0.679511
I1010 00:00:02.896376 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:00:04.758885 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:00:06.675605 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:00:08.556602 139660997596992 submission_runner.py:381] Time since start: 563.82s, 	Step: 741, 	{'train/ssim': 0.7108234677995954, 'train/loss': 0.2994429383959089, 'validation/ssim': 0.6900917787396244, 'validation/loss': 0.3182765980013717, 'validation/num_examples': 3554, 'test/ssim': 0.7084422341873778, 'test/loss': 0.3198583411691043, 'test/num_examples': 3581, 'score': 323.333943605423, 'total_duration': 563.8180117607117, 'accumulated_submission_time': 323.333943605423, 'accumulated_eval_time': 236.93174743652344, 'accumulated_logging_time': 0.0981447696685791}
I1010 00:00:08.576815 139598139205376 logging_writer.py:48] [741] accumulated_eval_time=236.931747, accumulated_logging_time=0.098145, accumulated_submission_time=323.333944, global_step=741, preemption_count=0, score=323.333944, test/loss=0.319858, test/num_examples=3581, test/ssim=0.708442, total_duration=563.818012, train/loss=0.299443, train/ssim=0.710823, validation/loss=0.318277, validation/num_examples=3554, validation/ssim=0.690092
I1010 00:01:29.106673 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:01:30.995137 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:01:33.949887 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:01:35.845039 139660997596992 submission_runner.py:381] Time since start: 651.11s, 	Step: 947, 	{'train/ssim': 0.7176779338291713, 'train/loss': 0.293111971446446, 'validation/ssim': 0.6969130842888295, 'validation/loss': 0.3116147326296075, 'validation/num_examples': 3554, 'test/ssim': 0.7148627712885717, 'test/loss': 0.3134358951213872, 'test/num_examples': 3581, 'score': 402.8794529438019, 'total_duration': 651.1056900024414, 'accumulated_submission_time': 402.8794529438019, 'accumulated_eval_time': 243.669615983963, 'accumulated_logging_time': 0.12829375267028809}
I1010 00:01:35.881273 139598130812672 logging_writer.py:48] [947] accumulated_eval_time=243.669616, accumulated_logging_time=0.128294, accumulated_submission_time=402.879453, global_step=947, preemption_count=0, score=402.879453, test/loss=0.313436, test/num_examples=3581, test/ssim=0.714863, total_duration=651.105690, train/loss=0.293112, train/ssim=0.717678, validation/loss=0.311615, validation/num_examples=3554, validation/ssim=0.696913
I1010 00:01:48.073501 139598139205376 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.286942, loss=0.270322
I1010 00:01:48.077713 139660997596992 pytorch_submission_base.py:86] 1000) loss = 0.270, grad_norm = 0.287
I1010 00:02:56.366528 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:02:58.225304 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:03:00.055365 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:03:01.903055 139660997596992 submission_runner.py:381] Time since start: 737.16s, 	Step: 1252, 	{'train/ssim': 0.7250095094953265, 'train/loss': 0.28699493408203125, 'validation/ssim': 0.7034201120216658, 'validation/loss': 0.3057656274730058, 'validation/num_examples': 3554, 'test/ssim': 0.7210843007147095, 'test/loss': 0.30741804347162105, 'test/num_examples': 3581, 'score': 482.38947224617004, 'total_duration': 737.1644275188446, 'accumulated_submission_time': 482.38947224617004, 'accumulated_eval_time': 249.20625352859497, 'accumulated_logging_time': 0.1834704875946045}
I1010 00:03:01.919736 139598130812672 logging_writer.py:48] [1252] accumulated_eval_time=249.206254, accumulated_logging_time=0.183470, accumulated_submission_time=482.389472, global_step=1252, preemption_count=0, score=482.389472, test/loss=0.307418, test/num_examples=3581, test/ssim=0.721084, total_duration=737.164428, train/loss=0.286995, train/ssim=0.725010, validation/loss=0.305766, validation/num_examples=3554, validation/ssim=0.703420
I1010 00:04:06.794304 139598139205376 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.233141, loss=0.251981
I1010 00:04:06.797808 139660997596992 pytorch_submission_base.py:86] 1500) loss = 0.252, grad_norm = 0.233
I1010 00:04:22.410895 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:04:24.235696 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:04:26.075107 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:04:27.870528 139660997596992 submission_runner.py:381] Time since start: 823.13s, 	Step: 1558, 	{'train/ssim': 0.7293124880109515, 'train/loss': 0.28075305053165983, 'validation/ssim': 0.7083720312939645, 'validation/loss': 0.299299610419158, 'validation/num_examples': 3554, 'test/ssim': 0.7254220408056409, 'test/loss': 0.3010486047782568, 'test/num_examples': 3581, 'score': 561.9244515895844, 'total_duration': 823.1319580078125, 'accumulated_submission_time': 561.9244515895844, 'accumulated_eval_time': 254.66623258590698, 'accumulated_logging_time': 0.2086184024810791}
I1010 00:04:27.886583 139598130812672 logging_writer.py:48] [1558] accumulated_eval_time=254.666233, accumulated_logging_time=0.208618, accumulated_submission_time=561.924452, global_step=1558, preemption_count=0, score=561.924452, test/loss=0.301049, test/num_examples=3581, test/ssim=0.725422, total_duration=823.131958, train/loss=0.280753, train/ssim=0.729312, validation/loss=0.299300, validation/num_examples=3554, validation/ssim=0.708372
I1010 00:05:48.374201 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:05:50.300305 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:05:52.294497 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:05:54.262575 139660997596992 submission_runner.py:381] Time since start: 909.52s, 	Step: 1865, 	{'train/ssim': 0.7222302300589425, 'train/loss': 0.2828918525150844, 'validation/ssim': 0.7044938086926702, 'validation/loss': 0.3003139549517269, 'validation/num_examples': 3554, 'test/ssim': 0.7210478943774434, 'test/loss': 0.3021586912698967, 'test/num_examples': 3581, 'score': 641.470006942749, 'total_duration': 909.5239601135254, 'accumulated_submission_time': 641.470006942749, 'accumulated_eval_time': 260.5547978878021, 'accumulated_logging_time': 0.23319268226623535}
I1010 00:05:54.279794 139598139205376 logging_writer.py:48] [1865] accumulated_eval_time=260.554798, accumulated_logging_time=0.233193, accumulated_submission_time=641.470007, global_step=1865, preemption_count=0, score=641.470007, test/loss=0.302159, test/num_examples=3581, test/ssim=0.721048, total_duration=909.523960, train/loss=0.282892, train/ssim=0.722230, validation/loss=0.300314, validation/num_examples=3554, validation/ssim=0.704494
I1010 00:06:28.675860 139598130812672 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.178755, loss=0.280721
I1010 00:06:28.679702 139660997596992 pytorch_submission_base.py:86] 2000) loss = 0.281, grad_norm = 0.179
I1010 00:07:14.799332 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:07:16.710682 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:07:18.673261 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:07:20.591872 139660997596992 submission_runner.py:381] Time since start: 995.85s, 	Step: 2170, 	{'train/ssim': 0.734670775277274, 'train/loss': 0.2769376720700945, 'validation/ssim': 0.7132202217132104, 'validation/loss': 0.2955353178197102, 'validation/num_examples': 3554, 'test/ssim': 0.7303017853558713, 'test/loss': 0.2972627547625489, 'test/num_examples': 3581, 'score': 721.0053427219391, 'total_duration': 995.8532979488373, 'accumulated_submission_time': 721.0053427219391, 'accumulated_eval_time': 266.3475959300995, 'accumulated_logging_time': 0.2587299346923828}
I1010 00:07:20.607631 139598139205376 logging_writer.py:48] [2170] accumulated_eval_time=266.347596, accumulated_logging_time=0.258730, accumulated_submission_time=721.005343, global_step=2170, preemption_count=0, score=721.005343, test/loss=0.297263, test/num_examples=3581, test/ssim=0.730302, total_duration=995.853298, train/loss=0.276938, train/ssim=0.734671, validation/loss=0.295535, validation/num_examples=3554, validation/ssim=0.713220
I1010 00:08:41.123295 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:08:42.957996 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:08:44.795258 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:08:46.594044 139660997596992 submission_runner.py:381] Time since start: 1081.86s, 	Step: 2470, 	{'train/ssim': 0.7319746017456055, 'train/loss': 0.27826389244624544, 'validation/ssim': 0.7124390953634989, 'validation/loss': 0.2967478806340567, 'validation/num_examples': 3554, 'test/ssim': 0.7290621291189612, 'test/loss': 0.29847701519303266, 'test/num_examples': 3581, 'score': 798.9338841438293, 'total_duration': 1081.8554601669312, 'accumulated_submission_time': 798.9338841438293, 'accumulated_eval_time': 271.81862139701843, 'accumulated_logging_time': 1.919734239578247}
I1010 00:08:46.610286 139598130812672 logging_writer.py:48] [2470] accumulated_eval_time=271.818621, accumulated_logging_time=1.919734, accumulated_submission_time=798.933884, global_step=2470, preemption_count=0, score=798.933884, test/loss=0.298477, test/num_examples=3581, test/ssim=0.729062, total_duration=1081.855460, train/loss=0.278264, train/ssim=0.731975, validation/loss=0.296748, validation/num_examples=3554, validation/ssim=0.712439
I1010 00:08:52.598592 139598139205376 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.567482, loss=0.221842
I1010 00:08:52.602018 139660997596992 pytorch_submission_base.py:86] 2500) loss = 0.222, grad_norm = 0.567
I1010 00:10:07.114837 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:10:08.921645 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:10:10.760897 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:10:12.560492 139660997596992 submission_runner.py:381] Time since start: 1167.82s, 	Step: 2776, 	{'train/ssim': 0.7390833582196917, 'train/loss': 0.2733630963734218, 'validation/ssim': 0.7173482178970878, 'validation/loss': 0.2926767979301491, 'validation/num_examples': 3554, 'test/ssim': 0.7346614783318207, 'test/loss': 0.2941477290081332, 'test/num_examples': 3581, 'score': 878.4453988075256, 'total_duration': 1167.8219242095947, 'accumulated_submission_time': 878.4453988075256, 'accumulated_eval_time': 277.2645103931427, 'accumulated_logging_time': 1.94419527053833}
I1010 00:10:12.576489 139598130812672 logging_writer.py:48] [2776] accumulated_eval_time=277.264510, accumulated_logging_time=1.944195, accumulated_submission_time=878.445399, global_step=2776, preemption_count=0, score=878.445399, test/loss=0.294148, test/num_examples=3581, test/ssim=0.734661, total_duration=1167.821924, train/loss=0.273363, train/ssim=0.739083, validation/loss=0.292677, validation/num_examples=3554, validation/ssim=0.717348
I1010 00:11:10.935655 139598139205376 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.270568, loss=0.283982
I1010 00:11:10.939548 139660997596992 pytorch_submission_base.py:86] 3000) loss = 0.284, grad_norm = 0.271
I1010 00:11:33.250840 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:11:35.081470 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:11:37.052045 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:11:39.019296 139660997596992 submission_runner.py:381] Time since start: 1254.28s, 	Step: 3081, 	{'train/ssim': 0.7399342400687081, 'train/loss': 0.2727991853441511, 'validation/ssim': 0.7182097856288688, 'validation/loss': 0.29207541101355866, 'validation/num_examples': 3554, 'test/ssim': 0.7355446388055012, 'test/loss': 0.29353962727546423, 'test/num_examples': 3581, 'score': 958.1526997089386, 'total_duration': 1254.2806787490845, 'accumulated_submission_time': 958.1526997089386, 'accumulated_eval_time': 283.033194065094, 'accumulated_logging_time': 1.9688525199890137}
I1010 00:11:39.036918 139598130812672 logging_writer.py:48] [3081] accumulated_eval_time=283.033194, accumulated_logging_time=1.968853, accumulated_submission_time=958.152700, global_step=3081, preemption_count=0, score=958.152700, test/loss=0.293540, test/num_examples=3581, test/ssim=0.735545, total_duration=1254.280679, train/loss=0.272799, train/ssim=0.739934, validation/loss=0.292075, validation/num_examples=3554, validation/ssim=0.718210
I1010 00:12:59.681120 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:13:01.552289 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:13:03.514576 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:13:05.431647 139660997596992 submission_runner.py:381] Time since start: 1340.69s, 	Step: 3386, 	{'train/ssim': 0.7397701399666923, 'train/loss': 0.2718642098563058, 'validation/ssim': 0.7178768228800295, 'validation/loss': 0.2910674550792241, 'validation/num_examples': 3554, 'test/ssim': 0.7351424646790352, 'test/loss': 0.2925954827780299, 'test/num_examples': 3581, 'score': 1037.7781476974487, 'total_duration': 1340.6930603981018, 'accumulated_submission_time': 1037.7781476974487, 'accumulated_eval_time': 288.7838685512543, 'accumulated_logging_time': 1.9955394268035889}
I1010 00:13:05.447830 139598139205376 logging_writer.py:48] [3386] accumulated_eval_time=288.783869, accumulated_logging_time=1.995539, accumulated_submission_time=1037.778148, global_step=3386, preemption_count=0, score=1037.778148, test/loss=0.292595, test/num_examples=3581, test/ssim=0.735142, total_duration=1340.693060, train/loss=0.271864, train/ssim=0.739770, validation/loss=0.291067, validation/num_examples=3554, validation/ssim=0.717877
I1010 00:13:34.201330 139598130812672 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.308337, loss=0.396395
I1010 00:13:34.204789 139660997596992 pytorch_submission_base.py:86] 3500) loss = 0.396, grad_norm = 0.308
I1010 00:14:25.906930 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:14:27.732601 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:14:29.557630 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:14:31.353091 139660997596992 submission_runner.py:381] Time since start: 1426.61s, 	Step: 3692, 	{'train/ssim': 0.7360054424830845, 'train/loss': 0.27480460916246685, 'validation/ssim': 0.715559685169879, 'validation/loss': 0.2933773798558842, 'validation/num_examples': 3554, 'test/ssim': 0.7328384344020525, 'test/loss': 0.2947410023169157, 'test/num_examples': 3581, 'score': 1117.2578999996185, 'total_duration': 1426.6145117282867, 'accumulated_submission_time': 1117.2578999996185, 'accumulated_eval_time': 294.2301528453827, 'accumulated_logging_time': 2.0197577476501465}
I1010 00:14:31.369473 139598139205376 logging_writer.py:48] [3692] accumulated_eval_time=294.230153, accumulated_logging_time=2.019758, accumulated_submission_time=1117.257900, global_step=3692, preemption_count=0, score=1117.257900, test/loss=0.294741, test/num_examples=3581, test/ssim=0.732838, total_duration=1426.614512, train/loss=0.274805, train/ssim=0.736005, validation/loss=0.293377, validation/num_examples=3554, validation/ssim=0.715560
I1010 00:15:51.798471 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:15:53.648068 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:15:55.490980 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:15:57.294545 139660997596992 submission_runner.py:381] Time since start: 1512.56s, 	Step: 3999, 	{'train/ssim': 0.737025260925293, 'train/loss': 0.27403736114501953, 'validation/ssim': 0.7165397510947172, 'validation/loss': 0.29271097349597286, 'validation/num_examples': 3554, 'test/ssim': 0.7335206782541539, 'test/loss': 0.29427487848191847, 'test/num_examples': 3581, 'score': 1196.7379038333893, 'total_duration': 1512.555954694748, 'accumulated_submission_time': 1196.7379038333893, 'accumulated_eval_time': 299.726514339447, 'accumulated_logging_time': 2.0453457832336426}
I1010 00:15:57.311260 139598130812672 logging_writer.py:48] [3999] accumulated_eval_time=299.726514, accumulated_logging_time=2.045346, accumulated_submission_time=1196.737904, global_step=3999, preemption_count=0, score=1196.737904, test/loss=0.294275, test/num_examples=3581, test/ssim=0.733521, total_duration=1512.555955, train/loss=0.274037, train/ssim=0.737025, validation/loss=0.292711, validation/num_examples=3554, validation/ssim=0.716540
I1010 00:15:57.936569 139598139205376 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.526172, loss=0.268128
I1010 00:15:57.940586 139660997596992 pytorch_submission_base.py:86] 4000) loss = 0.268, grad_norm = 0.526
I1010 00:17:17.892793 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:17:19.716461 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:17:21.548558 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:17:23.344780 139660997596992 submission_runner.py:381] Time since start: 1598.61s, 	Step: 4304, 	{'train/ssim': 0.7433297293526786, 'train/loss': 0.2719092539378575, 'validation/ssim': 0.7218965563669457, 'validation/loss': 0.2913017380284538, 'validation/num_examples': 3554, 'test/ssim': 0.7389514265830075, 'test/loss': 0.29285728115837056, 'test/num_examples': 3581, 'score': 1276.3680992126465, 'total_duration': 1598.6061973571777, 'accumulated_submission_time': 1276.3680992126465, 'accumulated_eval_time': 305.1786015033722, 'accumulated_logging_time': 2.0715436935424805}
I1010 00:17:23.361650 139598130812672 logging_writer.py:48] [4304] accumulated_eval_time=305.178602, accumulated_logging_time=2.071544, accumulated_submission_time=1276.368099, global_step=4304, preemption_count=0, score=1276.368099, test/loss=0.292857, test/num_examples=3581, test/ssim=0.738951, total_duration=1598.606197, train/loss=0.271909, train/ssim=0.743330, validation/loss=0.291302, validation/num_examples=3554, validation/ssim=0.721897
I1010 00:18:14.525596 139598139205376 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.175067, loss=0.294923
I1010 00:18:14.529252 139660997596992 pytorch_submission_base.py:86] 4500) loss = 0.295, grad_norm = 0.175
I1010 00:18:44.027233 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:18:45.926286 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:18:47.916686 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:18:49.893527 139660997596992 submission_runner.py:381] Time since start: 1685.15s, 	Step: 4609, 	{'train/ssim': 0.7428179468427386, 'train/loss': 0.2697267362049648, 'validation/ssim': 0.7206678846678038, 'validation/loss': 0.2891256303416925, 'validation/num_examples': 3554, 'test/ssim': 0.7379392076890184, 'test/loss': 0.29058147601926837, 'test/num_examples': 3581, 'score': 1356.0617468357086, 'total_duration': 1685.1548902988434, 'accumulated_submission_time': 1356.0617468357086, 'accumulated_eval_time': 311.0451056957245, 'accumulated_logging_time': 2.097012758255005}
I1010 00:18:49.911536 139598130812672 logging_writer.py:48] [4609] accumulated_eval_time=311.045106, accumulated_logging_time=2.097013, accumulated_submission_time=1356.061747, global_step=4609, preemption_count=0, score=1356.061747, test/loss=0.290581, test/num_examples=3581, test/ssim=0.737939, total_duration=1685.154890, train/loss=0.269727, train/ssim=0.742818, validation/loss=0.289126, validation/num_examples=3554, validation/ssim=0.720668
I1010 00:20:10.366630 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:20:12.251786 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:20:14.224112 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:20:16.150228 139660997596992 submission_runner.py:381] Time since start: 1771.41s, 	Step: 4913, 	{'train/ssim': 0.7426370212009975, 'train/loss': 0.27249326024736675, 'validation/ssim': 0.7211405722425436, 'validation/loss': 0.29187025457670934, 'validation/num_examples': 3554, 'test/ssim': 0.7380553125436331, 'test/loss': 0.2934393734946593, 'test/num_examples': 3581, 'score': 1435.53444814682, 'total_duration': 1771.411648273468, 'accumulated_submission_time': 1435.53444814682, 'accumulated_eval_time': 316.8288424015045, 'accumulated_logging_time': 2.123643636703491}
I1010 00:20:16.167273 139598139205376 logging_writer.py:48] [4913] accumulated_eval_time=316.828842, accumulated_logging_time=2.123644, accumulated_submission_time=1435.534448, global_step=4913, preemption_count=0, score=1435.534448, test/loss=0.293439, test/num_examples=3581, test/ssim=0.738055, total_duration=1771.411648, train/loss=0.272493, train/ssim=0.742637, validation/loss=0.291870, validation/num_examples=3554, validation/ssim=0.721141
I1010 00:20:37.523346 139598130812672 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.317790, loss=0.219615
I1010 00:20:37.527655 139660997596992 pytorch_submission_base.py:86] 5000) loss = 0.220, grad_norm = 0.318
I1010 00:21:36.674380 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:21:38.501944 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:21:40.325091 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:21:42.112527 139660997596992 submission_runner.py:381] Time since start: 1857.37s, 	Step: 5222, 	{'train/ssim': 0.7392873764038086, 'train/loss': 0.2727458817618234, 'validation/ssim': 0.7170412904034187, 'validation/loss': 0.2920683354692776, 'validation/num_examples': 3554, 'test/ssim': 0.7344005662480801, 'test/loss': 0.29343824857974377, 'test/num_examples': 3581, 'score': 1515.08047580719, 'total_duration': 1857.3739607334137, 'accumulated_submission_time': 1515.08047580719, 'accumulated_eval_time': 322.2671821117401, 'accumulated_logging_time': 2.1490907669067383}
I1010 00:21:42.129105 139598139205376 logging_writer.py:48] [5222] accumulated_eval_time=322.267182, accumulated_logging_time=2.149091, accumulated_submission_time=1515.080476, global_step=5222, preemption_count=0, score=1515.080476, test/loss=0.293438, test/num_examples=3581, test/ssim=0.734401, total_duration=1857.373961, train/loss=0.272746, train/ssim=0.739287, validation/loss=0.292068, validation/num_examples=3554, validation/ssim=0.717041
I1010 00:22:55.707053 139598130812672 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.170670, loss=0.221473
I1010 00:22:55.710965 139660997596992 pytorch_submission_base.py:86] 5500) loss = 0.221, grad_norm = 0.171
I1010 00:23:02.673734 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:23:04.493809 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:23:06.328469 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:23:08.123454 139660997596992 submission_runner.py:381] Time since start: 1943.38s, 	Step: 5524, 	{'train/ssim': 0.7408336230686733, 'train/loss': 0.27089851243155344, 'validation/ssim': 0.7186393329918753, 'validation/loss': 0.2903538555259039, 'validation/num_examples': 3554, 'test/ssim': 0.7360359880052709, 'test/loss': 0.29179894075284485, 'test/num_examples': 3581, 'score': 1594.6307199001312, 'total_duration': 1943.3848676681519, 'accumulated_submission_time': 1594.6307199001312, 'accumulated_eval_time': 327.71722078323364, 'accumulated_logging_time': 2.1737678050994873}
I1010 00:23:08.139758 139598139205376 logging_writer.py:48] [5524] accumulated_eval_time=327.717221, accumulated_logging_time=2.173768, accumulated_submission_time=1594.630720, global_step=5524, preemption_count=0, score=1594.630720, test/loss=0.291799, test/num_examples=3581, test/ssim=0.736036, total_duration=1943.384868, train/loss=0.270899, train/ssim=0.740834, validation/loss=0.290354, validation/num_examples=3554, validation/ssim=0.718639
I1010 00:24:28.823013 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:24:30.655580 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:24:32.483725 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:24:34.277879 139660997596992 submission_runner.py:381] Time since start: 2029.54s, 	Step: 5830, 	{'train/ssim': 0.7426271438598633, 'train/loss': 0.2698540006365095, 'validation/ssim': 0.7207706517963914, 'validation/loss': 0.28905010062385694, 'validation/num_examples': 3554, 'test/ssim': 0.7381483055099832, 'test/loss': 0.29042518102267173, 'test/num_examples': 3581, 'score': 1674.3439302444458, 'total_duration': 2029.5392787456512, 'accumulated_submission_time': 1674.3439302444458, 'accumulated_eval_time': 333.1722204685211, 'accumulated_logging_time': 2.198284387588501}
I1010 00:24:34.293882 139598130812672 logging_writer.py:48] [5830] accumulated_eval_time=333.172220, accumulated_logging_time=2.198284, accumulated_submission_time=1674.343930, global_step=5830, preemption_count=0, score=1674.343930, test/loss=0.290425, test/num_examples=3581, test/ssim=0.738148, total_duration=2029.539279, train/loss=0.269854, train/ssim=0.742627, validation/loss=0.289050, validation/num_examples=3554, validation/ssim=0.720771
I1010 00:25:18.079464 139598139205376 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.205340, loss=0.297624
I1010 00:25:18.083317 139660997596992 pytorch_submission_base.py:86] 6000) loss = 0.298, grad_norm = 0.205
I1010 00:25:54.829268 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:25:56.643301 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:25:58.455482 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:26:00.245335 139660997596992 submission_runner.py:381] Time since start: 2115.51s, 	Step: 6136, 	{'train/ssim': 0.7442991392953056, 'train/loss': 0.2695438861846924, 'validation/ssim': 0.7223965843943444, 'validation/loss': 0.2890330643618986, 'validation/num_examples': 3554, 'test/ssim': 0.7396124674933677, 'test/loss': 0.29048851714124896, 'test/num_examples': 3581, 'score': 1753.9117028713226, 'total_duration': 2115.5067443847656, 'accumulated_submission_time': 1753.9117028713226, 'accumulated_eval_time': 338.58848214149475, 'accumulated_logging_time': 2.223477840423584}
I1010 00:26:00.261417 139598130812672 logging_writer.py:48] [6136] accumulated_eval_time=338.588482, accumulated_logging_time=2.223478, accumulated_submission_time=1753.911703, global_step=6136, preemption_count=0, score=1753.911703, test/loss=0.290489, test/num_examples=3581, test/ssim=0.739612, total_duration=2115.506744, train/loss=0.269544, train/ssim=0.744299, validation/loss=0.289033, validation/num_examples=3554, validation/ssim=0.722397
I1010 00:27:20.765187 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:27:22.608625 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:27:24.437462 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:27:26.235010 139660997596992 submission_runner.py:381] Time since start: 2201.50s, 	Step: 6437, 	{'train/ssim': 0.744375501360212, 'train/loss': 0.26940609727587017, 'validation/ssim': 0.7226136593451041, 'validation/loss': 0.28857658871386116, 'validation/num_examples': 3554, 'test/ssim': 0.7399255347231919, 'test/loss': 0.2899933159601019, 'test/num_examples': 3581, 'score': 1833.455997467041, 'total_duration': 2201.496433019638, 'accumulated_submission_time': 1833.455997467041, 'accumulated_eval_time': 344.0585124492645, 'accumulated_logging_time': 2.2476155757904053}
I1010 00:27:26.251712 139598139205376 logging_writer.py:48] [6437] accumulated_eval_time=344.058512, accumulated_logging_time=2.247616, accumulated_submission_time=1833.455997, global_step=6437, preemption_count=0, score=1833.455997, test/loss=0.289993, test/num_examples=3581, test/ssim=0.739926, total_duration=2201.496433, train/loss=0.269406, train/ssim=0.744376, validation/loss=0.288577, validation/num_examples=3554, validation/ssim=0.722614
I1010 00:27:41.361795 139598130812672 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.253169, loss=0.297073
I1010 00:27:41.365086 139660997596992 pytorch_submission_base.py:86] 6500) loss = 0.297, grad_norm = 0.253
I1010 00:28:46.767399 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:28:48.605903 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:28:50.432881 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:28:52.240850 139660997596992 submission_runner.py:381] Time since start: 2287.50s, 	Step: 6743, 	{'train/ssim': 0.7445532935006278, 'train/loss': 0.26857779707227436, 'validation/ssim': 0.7221429639138999, 'validation/loss': 0.2883123205696926, 'validation/num_examples': 3554, 'test/ssim': 0.7393975746561715, 'test/loss': 0.289766014970504, 'test/num_examples': 3581, 'score': 1912.9912633895874, 'total_duration': 2287.502226114273, 'accumulated_submission_time': 1912.9912633895874, 'accumulated_eval_time': 349.53205132484436, 'accumulated_logging_time': 2.2725353240966797}
I1010 00:28:52.257552 139598139205376 logging_writer.py:48] [6743] accumulated_eval_time=349.532051, accumulated_logging_time=2.272535, accumulated_submission_time=1912.991263, global_step=6743, preemption_count=0, score=1912.991263, test/loss=0.289766, test/num_examples=3581, test/ssim=0.739398, total_duration=2287.502226, train/loss=0.268578, train/ssim=0.744553, validation/loss=0.288312, validation/num_examples=3554, validation/ssim=0.722143
I1010 00:29:59.425007 139598130812672 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.239311, loss=0.235028
I1010 00:29:59.428697 139660997596992 pytorch_submission_base.py:86] 7000) loss = 0.235, grad_norm = 0.239
I1010 00:30:12.924458 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:30:14.768689 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:30:16.608093 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:30:18.421377 139660997596992 submission_runner.py:381] Time since start: 2373.68s, 	Step: 7048, 	{'train/ssim': 0.7416153635297503, 'train/loss': 0.26922340052468435, 'validation/ssim': 0.7194549440331317, 'validation/loss': 0.2884597048440138, 'validation/num_examples': 3554, 'test/ssim': 0.7369528277497557, 'test/loss': 0.28984929276258375, 'test/num_examples': 3581, 'score': 1992.6915755271912, 'total_duration': 2373.6827714443207, 'accumulated_submission_time': 1992.6915755271912, 'accumulated_eval_time': 355.0292468070984, 'accumulated_logging_time': 2.298259735107422}
I1010 00:30:18.437750 139598139205376 logging_writer.py:48] [7048] accumulated_eval_time=355.029247, accumulated_logging_time=2.298260, accumulated_submission_time=1992.691576, global_step=7048, preemption_count=0, score=1992.691576, test/loss=0.289849, test/num_examples=3581, test/ssim=0.736953, total_duration=2373.682771, train/loss=0.269223, train/ssim=0.741615, validation/loss=0.288460, validation/num_examples=3554, validation/ssim=0.719455
I1010 00:31:38.855530 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:31:40.643749 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:31:42.483385 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:31:44.285768 139660997596992 submission_runner.py:381] Time since start: 2459.55s, 	Step: 7355, 	{'train/ssim': 0.7300920486450195, 'train/loss': 0.2805758033479963, 'validation/ssim': 0.7093407626090321, 'validation/loss': 0.29951654798070837, 'validation/num_examples': 3554, 'test/ssim': 0.7263529931190659, 'test/loss': 0.3014505061980767, 'test/num_examples': 3581, 'score': 2072.1570131778717, 'total_duration': 2459.547152519226, 'accumulated_submission_time': 2072.1570131778717, 'accumulated_eval_time': 360.459623336792, 'accumulated_logging_time': 2.322638988494873}
I1010 00:31:44.302289 139598130812672 logging_writer.py:48] [7355] accumulated_eval_time=360.459623, accumulated_logging_time=2.322639, accumulated_submission_time=2072.157013, global_step=7355, preemption_count=0, score=2072.157013, test/loss=0.301451, test/num_examples=3581, test/ssim=0.726353, total_duration=2459.547153, train/loss=0.280576, train/ssim=0.730092, validation/loss=0.299517, validation/num_examples=3554, validation/ssim=0.709341
I1010 00:32:22.680616 139598139205376 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.230548, loss=0.220517
I1010 00:32:22.684235 139660997596992 pytorch_submission_base.py:86] 7500) loss = 0.221, grad_norm = 0.231
I1010 00:33:04.936477 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:33:06.860438 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:33:08.841696 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:33:10.807452 139660997596992 submission_runner.py:381] Time since start: 2546.07s, 	Step: 7656, 	{'train/ssim': 0.7367579596383231, 'train/loss': 0.2735166549682617, 'validation/ssim': 0.7152510403110931, 'validation/loss': 0.29251653341745215, 'validation/num_examples': 3554, 'test/ssim': 0.7323884684358419, 'test/loss': 0.29405299753691355, 'test/num_examples': 3581, 'score': 2151.8514437675476, 'total_duration': 2546.068855047226, 'accumulated_submission_time': 2151.8514437675476, 'accumulated_eval_time': 366.33084774017334, 'accumulated_logging_time': 2.347259759902954}
I1010 00:33:10.826014 139598130812672 logging_writer.py:48] [7656] accumulated_eval_time=366.330848, accumulated_logging_time=2.347260, accumulated_submission_time=2151.851444, global_step=7656, preemption_count=0, score=2151.851444, test/loss=0.294053, test/num_examples=3581, test/ssim=0.732388, total_duration=2546.068855, train/loss=0.273517, train/ssim=0.736758, validation/loss=0.292517, validation/num_examples=3554, validation/ssim=0.715251
I1010 00:34:31.490727 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:34:33.392654 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:34:35.352666 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:34:37.286782 139660997596992 submission_runner.py:381] Time since start: 2632.55s, 	Step: 7963, 	{'train/ssim': 0.7334636960710798, 'train/loss': 0.27386721542903353, 'validation/ssim': 0.7117426694613463, 'validation/loss': 0.29263743592167274, 'validation/num_examples': 3554, 'test/ssim': 0.729336403828365, 'test/loss': 0.2941151405639137, 'test/num_examples': 3581, 'score': 2231.5210666656494, 'total_duration': 2632.54821228981, 'accumulated_submission_time': 2231.5210666656494, 'accumulated_eval_time': 372.12716698646545, 'accumulated_logging_time': 2.3744521141052246}
I1010 00:34:37.304641 139598139205376 logging_writer.py:48] [7963] accumulated_eval_time=372.127167, accumulated_logging_time=2.374452, accumulated_submission_time=2231.521067, global_step=7963, preemption_count=0, score=2231.521067, test/loss=0.294115, test/num_examples=3581, test/ssim=0.729336, total_duration=2632.548212, train/loss=0.273867, train/ssim=0.733464, validation/loss=0.292637, validation/num_examples=3554, validation/ssim=0.711743
I1010 00:34:45.282555 139598130812672 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.270736, loss=0.339432
I1010 00:34:45.286056 139660997596992 pytorch_submission_base.py:86] 8000) loss = 0.339, grad_norm = 0.271
I1010 00:35:57.945828 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:35:59.789509 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:36:01.684689 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:36:03.529032 139660997596992 submission_runner.py:381] Time since start: 2718.79s, 	Step: 8270, 	{'train/ssim': 0.7420095716203962, 'train/loss': 0.271261419568743, 'validation/ssim': 0.7201556290007738, 'validation/loss': 0.2909583336997046, 'validation/num_examples': 3554, 'test/ssim': 0.7370135731551941, 'test/loss': 0.29261494721490156, 'test/num_examples': 3581, 'score': 2311.2081706523895, 'total_duration': 2718.7904584407806, 'accumulated_submission_time': 2311.2081706523895, 'accumulated_eval_time': 377.7106475830078, 'accumulated_logging_time': 2.400472402572632}
I1010 00:36:03.546351 139598139205376 logging_writer.py:48] [8270] accumulated_eval_time=377.710648, accumulated_logging_time=2.400472, accumulated_submission_time=2311.208171, global_step=8270, preemption_count=0, score=2311.208171, test/loss=0.292615, test/num_examples=3581, test/ssim=0.737014, total_duration=2718.790458, train/loss=0.271261, train/ssim=0.742010, validation/loss=0.290958, validation/num_examples=3554, validation/ssim=0.720156
I1010 00:37:03.580941 139598130812672 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.159852, loss=0.325966
I1010 00:37:03.585178 139660997596992 pytorch_submission_base.py:86] 8500) loss = 0.326, grad_norm = 0.160
I1010 00:37:24.004963 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:37:25.844928 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:37:27.715594 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:37:29.520783 139660997596992 submission_runner.py:381] Time since start: 2804.78s, 	Step: 8574, 	{'train/ssim': 0.7410568509783063, 'train/loss': 0.27044616426740375, 'validation/ssim': 0.7194318626459623, 'validation/loss': 0.28954401483143993, 'validation/num_examples': 3554, 'test/ssim': 0.7367372531459438, 'test/loss': 0.29104916791748114, 'test/num_examples': 3581, 'score': 2390.693242073059, 'total_duration': 2804.7822086811066, 'accumulated_submission_time': 2390.693242073059, 'accumulated_eval_time': 383.22669315338135, 'accumulated_logging_time': 2.4268853664398193}
I1010 00:37:29.537841 139598139205376 logging_writer.py:48] [8574] accumulated_eval_time=383.226693, accumulated_logging_time=2.426885, accumulated_submission_time=2390.693242, global_step=8574, preemption_count=0, score=2390.693242, test/loss=0.291049, test/num_examples=3581, test/ssim=0.736737, total_duration=2804.782209, train/loss=0.270446, train/ssim=0.741057, validation/loss=0.289544, validation/num_examples=3554, validation/ssim=0.719432
I1010 00:38:50.028422 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:38:51.846545 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:38:53.687213 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:38:55.506420 139660997596992 submission_runner.py:381] Time since start: 2890.77s, 	Step: 8881, 	{'train/ssim': 0.7423738070896694, 'train/loss': 0.27148147991725374, 'validation/ssim': 0.7203205647465883, 'validation/loss': 0.29121820538917415, 'validation/num_examples': 3554, 'test/ssim': 0.7371027482284976, 'test/loss': 0.2929939412764416, 'test/num_examples': 3581, 'score': 2470.2215464115143, 'total_duration': 2890.7677960395813, 'accumulated_submission_time': 2470.2215464115143, 'accumulated_eval_time': 388.7048466205597, 'accumulated_logging_time': 2.4529144763946533}
I1010 00:38:55.523621 139598130812672 logging_writer.py:48] [8881] accumulated_eval_time=388.704847, accumulated_logging_time=2.452914, accumulated_submission_time=2470.221546, global_step=8881, preemption_count=0, score=2470.221546, test/loss=0.292994, test/num_examples=3581, test/ssim=0.737103, total_duration=2890.767796, train/loss=0.271481, train/ssim=0.742374, validation/loss=0.291218, validation/num_examples=3554, validation/ssim=0.720321
I1010 00:39:25.434175 139598139205376 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.432349, loss=0.248717
I1010 00:39:25.437734 139660997596992 pytorch_submission_base.py:86] 9000) loss = 0.249, grad_norm = 0.432
I1010 00:40:16.082735 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:40:17.886293 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:40:19.722107 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:40:21.523985 139660997596992 submission_runner.py:381] Time since start: 2976.79s, 	Step: 9186, 	{'train/ssim': 0.7413781029837472, 'train/loss': 0.27125138895852224, 'validation/ssim': 0.7195531773178109, 'validation/loss': 0.2904476236612795, 'validation/num_examples': 3554, 'test/ssim': 0.736561289182491, 'test/loss': 0.2922420208762392, 'test/num_examples': 3581, 'score': 2549.824760198593, 'total_duration': 2976.78538274765, 'accumulated_submission_time': 2549.824760198593, 'accumulated_eval_time': 394.1461937427521, 'accumulated_logging_time': 2.478292465209961}
I1010 00:40:21.541606 139598130812672 logging_writer.py:48] [9186] accumulated_eval_time=394.146194, accumulated_logging_time=2.478292, accumulated_submission_time=2549.824760, global_step=9186, preemption_count=0, score=2549.824760, test/loss=0.292242, test/num_examples=3581, test/ssim=0.736561, total_duration=2976.785383, train/loss=0.271251, train/ssim=0.741378, validation/loss=0.290448, validation/num_examples=3554, validation/ssim=0.719553
I1010 00:41:42.194901 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:41:44.015134 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:41:45.867445 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:41:47.678249 139660997596992 submission_runner.py:381] Time since start: 3062.94s, 	Step: 9492, 	{'train/ssim': 0.7427920613970075, 'train/loss': 0.2703582389014108, 'validation/ssim': 0.7209653323060284, 'validation/loss': 0.2894901239140757, 'validation/num_examples': 3554, 'test/ssim': 0.7380324051853533, 'test/loss': 0.2910683255593759, 'test/num_examples': 3581, 'score': 2629.5283744335175, 'total_duration': 3062.9396636486053, 'accumulated_submission_time': 2629.5283744335175, 'accumulated_eval_time': 399.6296634674072, 'accumulated_logging_time': 2.5044710636138916}
I1010 00:41:47.696923 139598139205376 logging_writer.py:48] [9492] accumulated_eval_time=399.629663, accumulated_logging_time=2.504471, accumulated_submission_time=2629.528374, global_step=9492, preemption_count=0, score=2629.528374, test/loss=0.291068, test/num_examples=3581, test/ssim=0.738032, total_duration=3062.939664, train/loss=0.270358, train/ssim=0.742792, validation/loss=0.289490, validation/num_examples=3554, validation/ssim=0.720965
I1010 00:41:48.676742 139598130812672 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.268958, loss=0.339408
I1010 00:41:48.680221 139660997596992 pytorch_submission_base.py:86] 9500) loss = 0.339, grad_norm = 0.269
I1010 00:43:08.257671 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:43:10.098104 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:43:11.919548 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:43:13.739056 139660997596992 submission_runner.py:381] Time since start: 3149.00s, 	Step: 9793, 	{'train/ssim': 0.7423223086765834, 'train/loss': 0.2698291369846889, 'validation/ssim': 0.7203698187781373, 'validation/loss': 0.2891008315894063, 'validation/num_examples': 3554, 'test/ssim': 0.7374906734327004, 'test/loss': 0.2907038872150761, 'test/num_examples': 3581, 'score': 2709.126131296158, 'total_duration': 3149.0004789829254, 'accumulated_submission_time': 2709.126131296158, 'accumulated_eval_time': 405.11137318611145, 'accumulated_logging_time': 2.5312657356262207}
I1010 00:43:13.755898 139598139205376 logging_writer.py:48] [9793] accumulated_eval_time=405.111373, accumulated_logging_time=2.531266, accumulated_submission_time=2709.126131, global_step=9793, preemption_count=0, score=2709.126131, test/loss=0.290704, test/num_examples=3581, test/ssim=0.737491, total_duration=3149.000479, train/loss=0.269829, train/ssim=0.742322, validation/loss=0.289101, validation/num_examples=3554, validation/ssim=0.720370
I1010 00:44:07.394628 139598130812672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.356049, loss=0.286425
I1010 00:44:07.398265 139660997596992 pytorch_submission_base.py:86] 10000) loss = 0.286, grad_norm = 0.356
I1010 00:44:34.426139 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:44:36.355925 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:44:38.338123 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:44:40.300593 139660997596992 submission_runner.py:381] Time since start: 3235.56s, 	Step: 10101, 	{'train/ssim': 0.7382858140127999, 'train/loss': 0.271142772265843, 'validation/ssim': 0.7166901235843416, 'validation/loss': 0.29029780072849254, 'validation/num_examples': 3554, 'test/ssim': 0.7338843325668458, 'test/loss': 0.291783123767366, 'test/num_examples': 3581, 'score': 2788.757292032242, 'total_duration': 3235.5619950294495, 'accumulated_submission_time': 2788.757292032242, 'accumulated_eval_time': 410.9862003326416, 'accumulated_logging_time': 2.55649995803833}
I1010 00:44:40.319896 139598139205376 logging_writer.py:48] [10101] accumulated_eval_time=410.986200, accumulated_logging_time=2.556500, accumulated_submission_time=2788.757292, global_step=10101, preemption_count=0, score=2788.757292, test/loss=0.291783, test/num_examples=3581, test/ssim=0.733884, total_duration=3235.561995, train/loss=0.271143, train/ssim=0.738286, validation/loss=0.290298, validation/num_examples=3554, validation/ssim=0.716690
I1010 00:46:00.994834 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:46:02.964004 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:46:04.906477 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:46:06.837816 139660997596992 submission_runner.py:381] Time since start: 3322.10s, 	Step: 10406, 	{'train/ssim': 0.7445270674569267, 'train/loss': 0.2690940925053188, 'validation/ssim': 0.7225427665130838, 'validation/loss': 0.28861066123777784, 'validation/num_examples': 3554, 'test/ssim': 0.7396599184498045, 'test/loss': 0.290103932593462, 'test/num_examples': 3581, 'score': 2868.419425725937, 'total_duration': 3322.099216938019, 'accumulated_submission_time': 2868.419425725937, 'accumulated_eval_time': 416.82938718795776, 'accumulated_logging_time': 2.584564208984375}
I1010 00:46:06.855349 139598130812672 logging_writer.py:48] [10406] accumulated_eval_time=416.829387, accumulated_logging_time=2.584564, accumulated_submission_time=2868.419426, global_step=10406, preemption_count=0, score=2868.419426, test/loss=0.290104, test/num_examples=3581, test/ssim=0.739660, total_duration=3322.099217, train/loss=0.269094, train/ssim=0.744527, validation/loss=0.288611, validation/num_examples=3554, validation/ssim=0.722543
I1010 00:46:29.914701 139598139205376 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.253578, loss=0.245213
I1010 00:46:29.920723 139660997596992 pytorch_submission_base.py:86] 10500) loss = 0.245, grad_norm = 0.254
I1010 00:47:27.303351 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:47:29.123518 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:47:31.098017 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:47:33.064782 139660997596992 submission_runner.py:381] Time since start: 3408.33s, 	Step: 10710, 	{'train/ssim': 0.7402200698852539, 'train/loss': 0.2710904393877302, 'validation/ssim': 0.7187054172015687, 'validation/loss': 0.2899298380785734, 'validation/num_examples': 3554, 'test/ssim': 0.7358622056949874, 'test/loss': 0.2914105383120113, 'test/num_examples': 3581, 'score': 2947.9170067310333, 'total_duration': 3408.3262116909027, 'accumulated_submission_time': 2947.9170067310333, 'accumulated_eval_time': 422.5909914970398, 'accumulated_logging_time': 2.6108925342559814}
I1010 00:47:33.084248 139598130812672 logging_writer.py:48] [10710] accumulated_eval_time=422.590991, accumulated_logging_time=2.610893, accumulated_submission_time=2947.917007, global_step=10710, preemption_count=0, score=2947.917007, test/loss=0.291411, test/num_examples=3581, test/ssim=0.735862, total_duration=3408.326212, train/loss=0.271090, train/ssim=0.740220, validation/loss=0.289930, validation/num_examples=3554, validation/ssim=0.718705
I1010 00:48:49.693138 139598139205376 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.405102, loss=0.334983
I1010 00:48:49.697835 139660997596992 pytorch_submission_base.py:86] 11000) loss = 0.335, grad_norm = 0.405
I1010 00:48:53.604318 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:48:55.451764 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:48:57.392323 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:48:59.324658 139660997596992 submission_runner.py:381] Time since start: 3494.59s, 	Step: 11014, 	{'train/ssim': 0.7441794531685966, 'train/loss': 0.26878951277051655, 'validation/ssim': 0.7218553396041432, 'validation/loss': 0.2883972271010657, 'validation/num_examples': 3554, 'test/ssim': 0.7391236408300754, 'test/loss': 0.2898805176735723, 'test/num_examples': 3581, 'score': 3027.478718519211, 'total_duration': 3494.58607673645, 'accumulated_submission_time': 3027.478718519211, 'accumulated_eval_time': 428.31161999702454, 'accumulated_logging_time': 2.6388399600982666}
I1010 00:48:59.342974 139598130812672 logging_writer.py:48] [11014] accumulated_eval_time=428.311620, accumulated_logging_time=2.638840, accumulated_submission_time=3027.478719, global_step=11014, preemption_count=0, score=3027.478719, test/loss=0.289881, test/num_examples=3581, test/ssim=0.739124, total_duration=3494.586077, train/loss=0.268790, train/ssim=0.744179, validation/loss=0.288397, validation/num_examples=3554, validation/ssim=0.721855
I1010 00:50:19.881050 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:50:21.802636 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:50:23.621815 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:50:25.433783 139660997596992 submission_runner.py:381] Time since start: 3580.70s, 	Step: 11319, 	{'train/ssim': 0.7435727119445801, 'train/loss': 0.2689156191689627, 'validation/ssim': 0.7217505803320202, 'validation/loss': 0.28824379770153347, 'validation/num_examples': 3554, 'test/ssim': 0.7389075889896328, 'test/loss': 0.2897215637871754, 'test/num_examples': 3581, 'score': 3107.0510540008545, 'total_duration': 3580.6951711177826, 'accumulated_submission_time': 3107.0510540008545, 'accumulated_eval_time': 433.86473083496094, 'accumulated_logging_time': 2.6659834384918213}
I1010 00:50:25.452731 139598139205376 logging_writer.py:48] [11319] accumulated_eval_time=433.864731, accumulated_logging_time=2.665983, accumulated_submission_time=3107.051054, global_step=11319, preemption_count=0, score=3107.051054, test/loss=0.289722, test/num_examples=3581, test/ssim=0.738908, total_duration=3580.695171, train/loss=0.268916, train/ssim=0.743573, validation/loss=0.288244, validation/num_examples=3554, validation/ssim=0.721751
I1010 00:51:12.316995 139598130812672 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.512049, loss=0.284484
I1010 00:51:12.321203 139660997596992 pytorch_submission_base.py:86] 11500) loss = 0.284, grad_norm = 0.512
I1010 00:51:45.968111 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:51:47.943339 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:51:49.948552 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:51:51.931769 139660997596992 submission_runner.py:381] Time since start: 3667.19s, 	Step: 11625, 	{'train/ssim': 0.7432649476187569, 'train/loss': 0.2694261074066162, 'validation/ssim': 0.7212863421936551, 'validation/loss': 0.2886983155533378, 'validation/num_examples': 3554, 'test/ssim': 0.7385802728375453, 'test/loss': 0.29013737324595085, 'test/num_examples': 3581, 'score': 3186.6024153232574, 'total_duration': 3667.1931533813477, 'accumulated_submission_time': 3186.6024153232574, 'accumulated_eval_time': 439.8286702632904, 'accumulated_logging_time': 2.694730281829834}
I1010 00:51:51.951868 139598139205376 logging_writer.py:48] [11625] accumulated_eval_time=439.828670, accumulated_logging_time=2.694730, accumulated_submission_time=3186.602415, global_step=11625, preemption_count=0, score=3186.602415, test/loss=0.290137, test/num_examples=3581, test/ssim=0.738580, total_duration=3667.193153, train/loss=0.269426, train/ssim=0.743265, validation/loss=0.288698, validation/num_examples=3554, validation/ssim=0.721286
I1010 00:53:12.519086 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:53:14.467230 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:53:16.469507 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:53:18.454153 139660997596992 submission_runner.py:381] Time since start: 3753.72s, 	Step: 11928, 	{'train/ssim': 0.7427441052028111, 'train/loss': 0.27012550830841064, 'validation/ssim': 0.7204972472698016, 'validation/loss': 0.28967329807743036, 'validation/num_examples': 3554, 'test/ssim': 0.7376360942517802, 'test/loss': 0.29115678477773316, 'test/num_examples': 3581, 'score': 3266.139392375946, 'total_duration': 3753.7155635356903, 'accumulated_submission_time': 3266.139392375946, 'accumulated_eval_time': 445.7641804218292, 'accumulated_logging_time': 2.724207878112793}
I1010 00:53:18.474460 139598130812672 logging_writer.py:48] [11928] accumulated_eval_time=445.764180, accumulated_logging_time=2.724208, accumulated_submission_time=3266.139392, global_step=11928, preemption_count=0, score=3266.139392, test/loss=0.291157, test/num_examples=3581, test/ssim=0.737636, total_duration=3753.715564, train/loss=0.270126, train/ssim=0.742744, validation/loss=0.289673, validation/num_examples=3554, validation/ssim=0.720497
I1010 00:53:35.734876 139598139205376 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.223777, loss=0.258536
I1010 00:53:35.738795 139660997596992 pytorch_submission_base.py:86] 12000) loss = 0.259, grad_norm = 0.224
I1010 00:54:39.020517 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:54:40.929385 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:54:42.894068 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:54:44.822461 139660997596992 submission_runner.py:381] Time since start: 3840.08s, 	Step: 12234, 	{'train/ssim': 0.7430727141244071, 'train/loss': 0.26856000082833426, 'validation/ssim': 0.7210674811831739, 'validation/loss': 0.2880525347484788, 'validation/num_examples': 3554, 'test/ssim': 0.7382499569123498, 'test/loss': 0.2895450884987608, 'test/num_examples': 3581, 'score': 3345.680385351181, 'total_duration': 3840.083880186081, 'accumulated_submission_time': 3345.680385351181, 'accumulated_eval_time': 451.56626772880554, 'accumulated_logging_time': 2.7534189224243164}
I1010 00:54:44.840201 139598130812672 logging_writer.py:48] [12234] accumulated_eval_time=451.566268, accumulated_logging_time=2.753419, accumulated_submission_time=3345.680385, global_step=12234, preemption_count=0, score=3345.680385, test/loss=0.289545, test/num_examples=3581, test/ssim=0.738250, total_duration=3840.083880, train/loss=0.268560, train/ssim=0.743073, validation/loss=0.288053, validation/num_examples=3554, validation/ssim=0.721067
I1010 00:55:54.431731 139598139205376 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.157229, loss=0.330325
I1010 00:55:54.435382 139660997596992 pytorch_submission_base.py:86] 12500) loss = 0.330, grad_norm = 0.157
I1010 00:56:05.272924 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:56:07.111309 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:56:08.929732 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:56:10.732491 139660997596992 submission_runner.py:381] Time since start: 3925.99s, 	Step: 12540, 	{'train/ssim': 0.74507965360369, 'train/loss': 0.2683646168027605, 'validation/ssim': 0.7228972306731851, 'validation/loss': 0.28797910021608575, 'validation/num_examples': 3554, 'test/ssim': 0.7401541992460207, 'test/loss': 0.2894209387980836, 'test/num_examples': 3581, 'score': 3425.16584277153, 'total_duration': 3925.993871450424, 'accumulated_submission_time': 3425.16584277153, 'accumulated_eval_time': 457.0259292125702, 'accumulated_logging_time': 2.7801856994628906}
I1010 00:56:10.749677 139598130812672 logging_writer.py:48] [12540] accumulated_eval_time=457.025929, accumulated_logging_time=2.780186, accumulated_submission_time=3425.165843, global_step=12540, preemption_count=0, score=3425.165843, test/loss=0.289421, test/num_examples=3581, test/ssim=0.740154, total_duration=3925.993871, train/loss=0.268365, train/ssim=0.745080, validation/loss=0.287979, validation/num_examples=3554, validation/ssim=0.722897
I1010 00:57:31.179772 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:57:33.149460 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:57:35.146938 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:57:37.114928 139660997596992 submission_runner.py:381] Time since start: 4012.38s, 	Step: 12844, 	{'train/ssim': 0.7456983838762555, 'train/loss': 0.268803460257394, 'validation/ssim': 0.7237162764446751, 'validation/loss': 0.2885054211034222, 'validation/num_examples': 3554, 'test/ssim': 0.7408583278064786, 'test/loss': 0.2899693177752374, 'test/num_examples': 3581, 'score': 3504.6441464424133, 'total_duration': 4012.37633228302, 'accumulated_submission_time': 3504.6441464424133, 'accumulated_eval_time': 462.9613928794861, 'accumulated_logging_time': 2.805645704269409}
I1010 00:57:37.133919 139598139205376 logging_writer.py:48] [12844] accumulated_eval_time=462.961393, accumulated_logging_time=2.805646, accumulated_submission_time=3504.644146, global_step=12844, preemption_count=0, score=3504.644146, test/loss=0.289969, test/num_examples=3581, test/ssim=0.740858, total_duration=4012.376332, train/loss=0.268803, train/ssim=0.745698, validation/loss=0.288505, validation/num_examples=3554, validation/ssim=0.723716
I1010 00:58:17.115020 139598130812672 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.417141, loss=0.323089
I1010 00:58:17.118506 139660997596992 pytorch_submission_base.py:86] 13000) loss = 0.323, grad_norm = 0.417
I1010 00:58:57.607173 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 00:58:59.518646 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 00:59:01.483103 139660997596992 spec.py:349] Evaluating on the test split.
I1010 00:59:03.465574 139660997596992 submission_runner.py:381] Time since start: 4098.73s, 	Step: 13149, 	{'train/ssim': 0.744255610874721, 'train/loss': 0.26796075275966097, 'validation/ssim': 0.7216271361274268, 'validation/loss': 0.2878519636764913, 'validation/num_examples': 3554, 'test/ssim': 0.7388769776685982, 'test/loss': 0.2892889146929978, 'test/num_examples': 3581, 'score': 3584.1353940963745, 'total_duration': 4098.7269587516785, 'accumulated_submission_time': 3584.1353940963745, 'accumulated_eval_time': 468.8199112415314, 'accumulated_logging_time': 2.833308219909668}
I1010 00:59:03.483678 139598139205376 logging_writer.py:48] [13149] accumulated_eval_time=468.819911, accumulated_logging_time=2.833308, accumulated_submission_time=3584.135394, global_step=13149, preemption_count=0, score=3584.135394, test/loss=0.289289, test/num_examples=3581, test/ssim=0.738877, total_duration=4098.726959, train/loss=0.267961, train/ssim=0.744256, validation/loss=0.287852, validation/num_examples=3554, validation/ssim=0.721627
I1010 01:00:24.044047 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:00:25.877500 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:00:27.710668 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:00:29.501513 139660997596992 submission_runner.py:381] Time since start: 4184.76s, 	Step: 13455, 	{'train/ssim': 0.7455732481820243, 'train/loss': 0.26829707622528076, 'validation/ssim': 0.7233232059167487, 'validation/loss': 0.2879369732497714, 'validation/num_examples': 3554, 'test/ssim': 0.7404644712327213, 'test/loss': 0.2893712039234851, 'test/num_examples': 3581, 'score': 3663.742796897888, 'total_duration': 4184.762919187546, 'accumulated_submission_time': 3663.742796897888, 'accumulated_eval_time': 474.2775752544403, 'accumulated_logging_time': 2.8592922687530518}
I1010 01:00:29.519212 139598130812672 logging_writer.py:48] [13455] accumulated_eval_time=474.277575, accumulated_logging_time=2.859292, accumulated_submission_time=3663.742797, global_step=13455, preemption_count=0, score=3663.742797, test/loss=0.289371, test/num_examples=3581, test/ssim=0.740464, total_duration=4184.762919, train/loss=0.268297, train/ssim=0.745573, validation/loss=0.287937, validation/num_examples=3554, validation/ssim=0.723323
I1010 01:00:39.582112 139598139205376 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.418031, loss=0.250917
I1010 01:00:39.586257 139660997596992 pytorch_submission_base.py:86] 13500) loss = 0.251, grad_norm = 0.418
I1010 01:01:50.047486 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:01:51.872001 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:01:53.701489 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:01:55.497470 139660997596992 submission_runner.py:381] Time since start: 4270.76s, 	Step: 13761, 	{'train/ssim': 0.7417126383100238, 'train/loss': 0.2690096242087228, 'validation/ssim': 0.7201379744873734, 'validation/loss': 0.28831682006629855, 'validation/num_examples': 3554, 'test/ssim': 0.7374621755881737, 'test/loss': 0.28971692777419017, 'test/num_examples': 3581, 'score': 3743.281441450119, 'total_duration': 4270.758888959885, 'accumulated_submission_time': 3743.281441450119, 'accumulated_eval_time': 479.7278366088867, 'accumulated_logging_time': 2.8850831985473633}
I1010 01:01:55.515335 139598130812672 logging_writer.py:48] [13761] accumulated_eval_time=479.727837, accumulated_logging_time=2.885083, accumulated_submission_time=3743.281441, global_step=13761, preemption_count=0, score=3743.281441, test/loss=0.289717, test/num_examples=3581, test/ssim=0.737462, total_duration=4270.758889, train/loss=0.269010, train/ssim=0.741713, validation/loss=0.288317, validation/num_examples=3554, validation/ssim=0.720138
I1010 01:02:58.141674 139598139205376 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.230853, loss=0.256622
I1010 01:02:58.145175 139660997596992 pytorch_submission_base.py:86] 14000) loss = 0.257, grad_norm = 0.231
I1010 01:03:16.179420 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:03:18.000215 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:03:19.825517 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:03:21.624825 139660997596992 submission_runner.py:381] Time since start: 4356.89s, 	Step: 14066, 	{'train/ssim': 0.7469384329659599, 'train/loss': 0.26976278850010466, 'validation/ssim': 0.7247166759724958, 'validation/loss': 0.28990582931424097, 'validation/num_examples': 3554, 'test/ssim': 0.7416990142200154, 'test/loss': 0.29146818167934935, 'test/num_examples': 3581, 'score': 3822.989084005356, 'total_duration': 4356.886250734329, 'accumulated_submission_time': 3822.989084005356, 'accumulated_eval_time': 485.17335534095764, 'accumulated_logging_time': 2.9106526374816895}
I1010 01:03:21.643130 139598130812672 logging_writer.py:48] [14066] accumulated_eval_time=485.173355, accumulated_logging_time=2.910653, accumulated_submission_time=3822.989084, global_step=14066, preemption_count=0, score=3822.989084, test/loss=0.291468, test/num_examples=3581, test/ssim=0.741699, total_duration=4356.886251, train/loss=0.269763, train/ssim=0.746938, validation/loss=0.289906, validation/num_examples=3554, validation/ssim=0.724717
I1010 01:04:42.302228 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:04:44.233613 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:04:46.224291 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:04:48.205033 139660997596992 submission_runner.py:381] Time since start: 4443.47s, 	Step: 14371, 	{'train/ssim': 0.7456225667681012, 'train/loss': 0.26739002977098736, 'validation/ssim': 0.7232266899971863, 'validation/loss': 0.2873235819510147, 'validation/num_examples': 3554, 'test/ssim': 0.7404136796198687, 'test/loss': 0.2887887707038886, 'test/num_examples': 3581, 'score': 3902.6783130168915, 'total_duration': 4443.466443538666, 'accumulated_submission_time': 3902.6783130168915, 'accumulated_eval_time': 491.0763490200043, 'accumulated_logging_time': 2.9384188652038574}
I1010 01:04:48.225470 139598139205376 logging_writer.py:48] [14371] accumulated_eval_time=491.076349, accumulated_logging_time=2.938419, accumulated_submission_time=3902.678313, global_step=14371, preemption_count=0, score=3902.678313, test/loss=0.288789, test/num_examples=3581, test/ssim=0.740414, total_duration=4443.466444, train/loss=0.267390, train/ssim=0.745623, validation/loss=0.287324, validation/num_examples=3554, validation/ssim=0.723227
I1010 01:05:21.150755 139598130812672 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.241347, loss=0.335973
I1010 01:05:21.154388 139660997596992 pytorch_submission_base.py:86] 14500) loss = 0.336, grad_norm = 0.241
I1010 01:06:08.854533 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:06:10.754871 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:06:12.732159 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:06:14.657256 139660997596992 submission_runner.py:381] Time since start: 4529.92s, 	Step: 14676, 	{'train/ssim': 0.7464823041643415, 'train/loss': 0.2678264890398298, 'validation/ssim': 0.7241860788460186, 'validation/loss': 0.28746432002233396, 'validation/num_examples': 3554, 'test/ssim': 0.7413907875331611, 'test/loss': 0.288878116218846, 'test/num_examples': 3581, 'score': 3982.3136689662933, 'total_duration': 4529.918646335602, 'accumulated_submission_time': 3982.3136689662933, 'accumulated_eval_time': 496.87917709350586, 'accumulated_logging_time': 2.967838764190674}
I1010 01:06:14.675057 139598139205376 logging_writer.py:48] [14676] accumulated_eval_time=496.879177, accumulated_logging_time=2.967839, accumulated_submission_time=3982.313669, global_step=14676, preemption_count=0, score=3982.313669, test/loss=0.288878, test/num_examples=3581, test/ssim=0.741391, total_duration=4529.918646, train/loss=0.267826, train/ssim=0.746482, validation/loss=0.287464, validation/num_examples=3554, validation/ssim=0.724186
I1010 01:07:35.344213 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:07:37.170129 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:07:39.012921 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:07:40.816449 139660997596992 submission_runner.py:381] Time since start: 4616.08s, 	Step: 14981, 	{'train/ssim': 0.7413056918552944, 'train/loss': 0.2694911275591169, 'validation/ssim': 0.7206393077122608, 'validation/loss': 0.28855099997362127, 'validation/num_examples': 3554, 'test/ssim': 0.7375194439838733, 'test/loss': 0.28994453555876504, 'test/num_examples': 3581, 'score': 4062.038354873657, 'total_duration': 4616.077848672867, 'accumulated_submission_time': 4062.038354873657, 'accumulated_eval_time': 502.35155606269836, 'accumulated_logging_time': 2.993614435195923}
I1010 01:07:40.834308 139598130812672 logging_writer.py:48] [14981] accumulated_eval_time=502.351556, accumulated_logging_time=2.993614, accumulated_submission_time=4062.038355, global_step=14981, preemption_count=0, score=4062.038355, test/loss=0.289945, test/num_examples=3581, test/ssim=0.737519, total_duration=4616.077849, train/loss=0.269491, train/ssim=0.741306, validation/loss=0.288551, validation/num_examples=3554, validation/ssim=0.720639
I1010 01:07:43.903275 139598139205376 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.296928, loss=0.226586
I1010 01:07:43.906576 139660997596992 pytorch_submission_base.py:86] 15000) loss = 0.227, grad_norm = 0.297
I1010 01:09:01.407460 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:09:03.377534 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:09:05.199194 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:09:07.009155 139660997596992 submission_runner.py:381] Time since start: 4702.27s, 	Step: 15284, 	{'train/ssim': 0.74451630456107, 'train/loss': 0.2681370632989066, 'validation/ssim': 0.7216035051834201, 'validation/loss': 0.28840172659767166, 'validation/num_examples': 3554, 'test/ssim': 0.7387580093941986, 'test/loss': 0.2898277830258657, 'test/num_examples': 3581, 'score': 4141.665055990219, 'total_duration': 4702.270579338074, 'accumulated_submission_time': 4141.665055990219, 'accumulated_eval_time': 507.9534640312195, 'accumulated_logging_time': 3.019418478012085}
I1010 01:09:07.027345 139598130812672 logging_writer.py:48] [15284] accumulated_eval_time=507.953464, accumulated_logging_time=3.019418, accumulated_submission_time=4141.665056, global_step=15284, preemption_count=0, score=4141.665056, test/loss=0.289828, test/num_examples=3581, test/ssim=0.738758, total_duration=4702.270579, train/loss=0.268137, train/ssim=0.744516, validation/loss=0.288402, validation/num_examples=3554, validation/ssim=0.721604
I1010 01:10:02.843664 139598139205376 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.178902, loss=0.308762
I1010 01:10:02.847382 139660997596992 pytorch_submission_base.py:86] 15500) loss = 0.309, grad_norm = 0.179
I1010 01:10:27.444910 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:10:29.254430 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:10:31.101643 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:10:32.901032 139660997596992 submission_runner.py:381] Time since start: 4788.16s, 	Step: 15591, 	{'train/ssim': 0.7450955254690987, 'train/loss': 0.2675144502094814, 'validation/ssim': 0.7227013823552687, 'validation/loss': 0.2871259132260745, 'validation/num_examples': 3554, 'test/ssim': 0.7400002563442474, 'test/loss': 0.28851943880244696, 'test/num_examples': 3581, 'score': 4221.113683462143, 'total_duration': 4788.162426233292, 'accumulated_submission_time': 4221.113683462143, 'accumulated_eval_time': 513.4097089767456, 'accumulated_logging_time': 3.0456342697143555}
I1010 01:10:32.920495 139598130812672 logging_writer.py:48] [15591] accumulated_eval_time=513.409709, accumulated_logging_time=3.045634, accumulated_submission_time=4221.113683, global_step=15591, preemption_count=0, score=4221.113683, test/loss=0.288519, test/num_examples=3581, test/ssim=0.740000, total_duration=4788.162426, train/loss=0.267514, train/ssim=0.745096, validation/loss=0.287126, validation/num_examples=3554, validation/ssim=0.722701
I1010 01:11:53.412244 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:11:55.230848 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:11:57.054864 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:11:58.851923 139660997596992 submission_runner.py:381] Time since start: 4874.11s, 	Step: 15896, 	{'train/ssim': 0.743556295122419, 'train/loss': 0.26878511905670166, 'validation/ssim': 0.7219262324361635, 'validation/loss': 0.288113999246008, 'validation/num_examples': 3554, 'test/ssim': 0.7390444877260193, 'test/loss': 0.2894575155770036, 'test/num_examples': 3581, 'score': 4300.635037660599, 'total_duration': 4874.11335849762, 'accumulated_submission_time': 4300.635037660599, 'accumulated_eval_time': 518.8496088981628, 'accumulated_logging_time': 3.073381185531616}
I1010 01:11:58.870529 139598139205376 logging_writer.py:48] [15896] accumulated_eval_time=518.849609, accumulated_logging_time=3.073381, accumulated_submission_time=4300.635038, global_step=15896, preemption_count=0, score=4300.635038, test/loss=0.289458, test/num_examples=3581, test/ssim=0.739044, total_duration=4874.113358, train/loss=0.268785, train/ssim=0.743556, validation/loss=0.288114, validation/num_examples=3554, validation/ssim=0.721926
I1010 01:12:24.909030 139598130812672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.219092, loss=0.248467
I1010 01:12:24.912573 139660997596992 pytorch_submission_base.py:86] 16000) loss = 0.248, grad_norm = 0.219
I1010 01:13:19.324389 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:13:21.134703 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:13:22.974735 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:13:24.772574 139660997596992 submission_runner.py:381] Time since start: 4960.03s, 	Step: 16199, 	{'train/ssim': 0.744654655456543, 'train/loss': 0.26743718555995394, 'validation/ssim': 0.7224630807716658, 'validation/loss': 0.286956477983654, 'validation/num_examples': 3554, 'test/ssim': 0.7397725462946803, 'test/loss': 0.2882901606896642, 'test/num_examples': 3581, 'score': 4380.129357814789, 'total_duration': 4960.033996105194, 'accumulated_submission_time': 4380.129357814789, 'accumulated_eval_time': 524.2980237007141, 'accumulated_logging_time': 3.099935293197632}
I1010 01:13:24.790925 139598139205376 logging_writer.py:48] [16199] accumulated_eval_time=524.298024, accumulated_logging_time=3.099935, accumulated_submission_time=4380.129358, global_step=16199, preemption_count=0, score=4380.129358, test/loss=0.288290, test/num_examples=3581, test/ssim=0.739773, total_duration=4960.033996, train/loss=0.267437, train/ssim=0.744655, validation/loss=0.286956, validation/num_examples=3554, validation/ssim=0.722463
I1010 01:14:43.698783 139598130812672 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.213584, loss=0.268362
I1010 01:14:43.702224 139660997596992 pytorch_submission_base.py:86] 16500) loss = 0.268, grad_norm = 0.214
I1010 01:14:45.479454 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:14:47.307140 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:14:49.145749 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:14:50.951836 139660997596992 submission_runner.py:381] Time since start: 5046.21s, 	Step: 16506, 	{'train/ssim': 0.7473114558628627, 'train/loss': 0.2667367288044521, 'validation/ssim': 0.7247202480919387, 'validation/loss': 0.286781770430325, 'validation/num_examples': 3554, 'test/ssim': 0.7418561614248813, 'test/loss': 0.28819331574193663, 'test/num_examples': 3581, 'score': 4459.870715856552, 'total_duration': 5046.213260173798, 'accumulated_submission_time': 4459.870715856552, 'accumulated_eval_time': 529.7706182003021, 'accumulated_logging_time': 3.1263413429260254}
I1010 01:14:50.970341 139598139205376 logging_writer.py:48] [16506] accumulated_eval_time=529.770618, accumulated_logging_time=3.126341, accumulated_submission_time=4459.870716, global_step=16506, preemption_count=0, score=4459.870716, test/loss=0.288193, test/num_examples=3581, test/ssim=0.741856, total_duration=5046.213260, train/loss=0.266737, train/ssim=0.747311, validation/loss=0.286782, validation/num_examples=3554, validation/ssim=0.724720
I1010 01:16:11.406715 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:16:13.240319 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:16:15.072342 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:16:16.881872 139660997596992 submission_runner.py:381] Time since start: 5132.14s, 	Step: 16813, 	{'train/ssim': 0.7459908212934222, 'train/loss': 0.26730593613215853, 'validation/ssim': 0.7238091515501899, 'validation/loss': 0.2870320763960942, 'validation/num_examples': 3554, 'test/ssim': 0.7409432759267662, 'test/loss': 0.28845719351045446, 'test/num_examples': 3581, 'score': 4539.347900867462, 'total_duration': 5132.143274784088, 'accumulated_submission_time': 4539.347900867462, 'accumulated_eval_time': 535.2460277080536, 'accumulated_logging_time': 3.1539697647094727}
I1010 01:16:16.900586 139598130812672 logging_writer.py:48] [16813] accumulated_eval_time=535.246028, accumulated_logging_time=3.153970, accumulated_submission_time=4539.347901, global_step=16813, preemption_count=0, score=4539.347901, test/loss=0.288457, test/num_examples=3581, test/ssim=0.740943, total_duration=5132.143275, train/loss=0.267306, train/ssim=0.745991, validation/loss=0.287032, validation/num_examples=3554, validation/ssim=0.723809
I1010 01:17:05.185034 139598139205376 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.653748, loss=0.324999
I1010 01:17:05.189148 139660997596992 pytorch_submission_base.py:86] 17000) loss = 0.325, grad_norm = 0.654
I1010 01:17:37.507492 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:17:39.327758 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:17:41.168854 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:17:42.979315 139660997596992 submission_runner.py:381] Time since start: 5218.24s, 	Step: 17119, 	{'train/ssim': 0.7451875550406319, 'train/loss': 0.2672051021030971, 'validation/ssim': 0.7229054740257457, 'validation/loss': 0.286712749526282, 'validation/num_examples': 3554, 'test/ssim': 0.7402188988978288, 'test/loss': 0.2880518491692265, 'test/num_examples': 3581, 'score': 4619.013758182526, 'total_duration': 5218.240732908249, 'accumulated_submission_time': 4619.013758182526, 'accumulated_eval_time': 540.7179870605469, 'accumulated_logging_time': 3.1814231872558594}
I1010 01:17:42.998348 139598130812672 logging_writer.py:48] [17119] accumulated_eval_time=540.717987, accumulated_logging_time=3.181423, accumulated_submission_time=4619.013758, global_step=17119, preemption_count=0, score=4619.013758, test/loss=0.288052, test/num_examples=3581, test/ssim=0.740219, total_duration=5218.240733, train/loss=0.267205, train/ssim=0.745188, validation/loss=0.286713, validation/num_examples=3554, validation/ssim=0.722905
I1010 01:19:03.579206 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:19:05.397718 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:19:07.238933 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:19:09.053322 139660997596992 submission_runner.py:381] Time since start: 5304.31s, 	Step: 17422, 	{'train/ssim': 0.7448293822152274, 'train/loss': 0.267853992325919, 'validation/ssim': 0.7218951824748523, 'validation/loss': 0.2877952734539867, 'validation/num_examples': 3554, 'test/ssim': 0.739165092240296, 'test/loss': 0.2892288510541748, 'test/num_examples': 3581, 'score': 4698.644377231598, 'total_duration': 5304.314746856689, 'accumulated_submission_time': 4698.644377231598, 'accumulated_eval_time': 546.1922211647034, 'accumulated_logging_time': 3.209461212158203}
I1010 01:19:09.072067 139598139205376 logging_writer.py:48] [17422] accumulated_eval_time=546.192221, accumulated_logging_time=3.209461, accumulated_submission_time=4698.644377, global_step=17422, preemption_count=0, score=4698.644377, test/loss=0.289229, test/num_examples=3581, test/ssim=0.739165, total_duration=5304.314747, train/loss=0.267854, train/ssim=0.744829, validation/loss=0.287795, validation/num_examples=3554, validation/ssim=0.721895
I1010 01:19:28.094957 139598130812672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.242437, loss=0.316109
I1010 01:19:28.098675 139660997596992 pytorch_submission_base.py:86] 17500) loss = 0.316, grad_norm = 0.242
I1010 01:20:29.497668 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:20:31.320653 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:20:33.143535 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:20:34.953911 139660997596992 submission_runner.py:381] Time since start: 5390.22s, 	Step: 17728, 	{'train/ssim': 0.7443248203822544, 'train/loss': 0.2673984595707485, 'validation/ssim': 0.7215969105013716, 'validation/loss': 0.28727506638646594, 'validation/num_examples': 3554, 'test/ssim': 0.7389689479850251, 'test/loss': 0.2886345210071384, 'test/num_examples': 3581, 'score': 4778.120165109634, 'total_duration': 5390.215332508087, 'accumulated_submission_time': 4778.120165109634, 'accumulated_eval_time': 551.6486639976501, 'accumulated_logging_time': 3.236421585083008}
I1010 01:20:34.972283 139598139205376 logging_writer.py:48] [17728] accumulated_eval_time=551.648664, accumulated_logging_time=3.236422, accumulated_submission_time=4778.120165, global_step=17728, preemption_count=0, score=4778.120165, test/loss=0.288635, test/num_examples=3581, test/ssim=0.738969, total_duration=5390.215333, train/loss=0.267398, train/ssim=0.744325, validation/loss=0.287275, validation/num_examples=3554, validation/ssim=0.721597
I1010 01:21:45.642827 139598130812672 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.167662, loss=0.348455
I1010 01:21:45.646271 139660997596992 pytorch_submission_base.py:86] 18000) loss = 0.348, grad_norm = 0.168
I1010 01:21:55.442753 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:21:57.283191 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:21:59.125669 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:22:00.926918 139660997596992 submission_runner.py:381] Time since start: 5476.19s, 	Step: 18034, 	{'train/ssim': 0.7464590072631836, 'train/loss': 0.2673988342285156, 'validation/ssim': 0.7242340276800788, 'validation/loss': 0.28714501032617296, 'validation/num_examples': 3554, 'test/ssim': 0.74141805819778, 'test/loss': 0.2884813280486421, 'test/num_examples': 3581, 'score': 4857.591336250305, 'total_duration': 5476.188336610794, 'accumulated_submission_time': 4857.591336250305, 'accumulated_eval_time': 557.1330442428589, 'accumulated_logging_time': 3.2637789249420166}
I1010 01:22:00.945176 139598139205376 logging_writer.py:48] [18034] accumulated_eval_time=557.133044, accumulated_logging_time=3.263779, accumulated_submission_time=4857.591336, global_step=18034, preemption_count=0, score=4857.591336, test/loss=0.288481, test/num_examples=3581, test/ssim=0.741418, total_duration=5476.188337, train/loss=0.267399, train/ssim=0.746459, validation/loss=0.287145, validation/num_examples=3554, validation/ssim=0.724234
I1010 01:23:21.458987 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:23:23.280880 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:23:25.137006 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:23:26.942136 139660997596992 submission_runner.py:381] Time since start: 5562.20s, 	Step: 18339, 	{'train/ssim': 0.7461464064461845, 'train/loss': 0.26695222514016287, 'validation/ssim': 0.7237971299943725, 'validation/loss': 0.28658566450764106, 'validation/num_examples': 3554, 'test/ssim': 0.7410681755707205, 'test/loss': 0.2880024892662664, 'test/num_examples': 3581, 'score': 4937.138226270676, 'total_duration': 5562.203558683395, 'accumulated_submission_time': 4937.138226270676, 'accumulated_eval_time': 562.6163387298584, 'accumulated_logging_time': 3.2896788120269775}
I1010 01:23:26.962202 139598130812672 logging_writer.py:48] [18339] accumulated_eval_time=562.616339, accumulated_logging_time=3.289679, accumulated_submission_time=4937.138226, global_step=18339, preemption_count=0, score=4937.138226, test/loss=0.288002, test/num_examples=3581, test/ssim=0.741068, total_duration=5562.203559, train/loss=0.266952, train/ssim=0.746146, validation/loss=0.286586, validation/num_examples=3554, validation/ssim=0.723797
I1010 01:24:08.671916 139598139205376 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.243819, loss=0.274256
I1010 01:24:08.675719 139660997596992 pytorch_submission_base.py:86] 18500) loss = 0.274, grad_norm = 0.244
I1010 01:24:47.602289 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:24:49.427208 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:24:51.279488 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:24:53.087806 139660997596992 submission_runner.py:381] Time since start: 5648.35s, 	Step: 18645, 	{'train/ssim': 0.745931693485805, 'train/loss': 0.26716104575565885, 'validation/ssim': 0.7233712921400183, 'validation/loss': 0.286998158435038, 'validation/num_examples': 3554, 'test/ssim': 0.7405681679349344, 'test/loss': 0.2884285934009355, 'test/num_examples': 3581, 'score': 5016.8269464969635, 'total_duration': 5648.349233388901, 'accumulated_submission_time': 5016.8269464969635, 'accumulated_eval_time': 568.1021540164948, 'accumulated_logging_time': 3.318765878677368}
I1010 01:24:53.106505 139598130812672 logging_writer.py:48] [18645] accumulated_eval_time=568.102154, accumulated_logging_time=3.318766, accumulated_submission_time=5016.826946, global_step=18645, preemption_count=0, score=5016.826946, test/loss=0.288429, test/num_examples=3581, test/ssim=0.740568, total_duration=5648.349233, train/loss=0.267161, train/ssim=0.745932, validation/loss=0.286998, validation/num_examples=3554, validation/ssim=0.723371
I1010 01:26:13.764290 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:26:15.689987 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:26:17.669246 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:26:19.645194 139660997596992 submission_runner.py:381] Time since start: 5734.91s, 	Step: 18953, 	{'train/ssim': 0.7464321000235421, 'train/loss': 0.26677760056086947, 'validation/ssim': 0.7241322222759566, 'validation/loss': 0.2864446173106007, 'validation/num_examples': 3554, 'test/ssim': 0.7413291558311226, 'test/loss': 0.28782925236927537, 'test/num_examples': 3581, 'score': 5096.485533237457, 'total_duration': 5734.906534671783, 'accumulated_submission_time': 5096.485533237457, 'accumulated_eval_time': 573.983553647995, 'accumulated_logging_time': 3.3463873863220215}
I1010 01:26:19.668398 139598139205376 logging_writer.py:48] [18953] accumulated_eval_time=573.983554, accumulated_logging_time=3.346387, accumulated_submission_time=5096.485533, global_step=18953, preemption_count=0, score=5096.485533, test/loss=0.287829, test/num_examples=3581, test/ssim=0.741329, total_duration=5734.906535, train/loss=0.266778, train/ssim=0.746432, validation/loss=0.286445, validation/num_examples=3554, validation/ssim=0.724132
I1010 01:26:30.295162 139598130812672 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.191386, loss=0.248079
I1010 01:26:30.298902 139660997596992 pytorch_submission_base.py:86] 19000) loss = 0.248, grad_norm = 0.191
I1010 01:27:40.241434 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:27:42.181143 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:27:44.152724 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:27:46.079421 139660997596992 submission_runner.py:381] Time since start: 5821.34s, 	Step: 19260, 	{'train/ssim': 0.7460182734898159, 'train/loss': 0.26684517519814627, 'validation/ssim': 0.7235738725291925, 'validation/loss': 0.2865671684853334, 'validation/num_examples': 3554, 'test/ssim': 0.7407747432194219, 'test/loss': 0.28790042880393046, 'test/num_examples': 3581, 'score': 5176.060472011566, 'total_duration': 5821.340844154358, 'accumulated_submission_time': 5176.060472011566, 'accumulated_eval_time': 579.8217279911041, 'accumulated_logging_time': 3.3792102336883545}
I1010 01:27:46.098139 139598139205376 logging_writer.py:48] [19260] accumulated_eval_time=579.821728, accumulated_logging_time=3.379210, accumulated_submission_time=5176.060472, global_step=19260, preemption_count=0, score=5176.060472, test/loss=0.287900, test/num_examples=3581, test/ssim=0.740775, total_duration=5821.340844, train/loss=0.266845, train/ssim=0.746018, validation/loss=0.286567, validation/num_examples=3554, validation/ssim=0.723574
I1010 01:28:49.009967 139598130812672 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.635225, loss=0.285021
I1010 01:28:49.015330 139660997596992 pytorch_submission_base.py:86] 19500) loss = 0.285, grad_norm = 0.635
I1010 01:29:06.657267 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:29:08.485708 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:29:10.328839 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:29:12.134598 139660997596992 submission_runner.py:381] Time since start: 5907.40s, 	Step: 19564, 	{'train/ssim': 0.7463537624904087, 'train/loss': 0.2666154078074864, 'validation/ssim': 0.7237041174996482, 'validation/loss': 0.286454784112092, 'validation/num_examples': 3554, 'test/ssim': 0.7409394580337196, 'test/loss': 0.28780706086594177, 'test/num_examples': 3581, 'score': 5255.67177438736, 'total_duration': 5907.396010160446, 'accumulated_submission_time': 5255.67177438736, 'accumulated_eval_time': 585.299188375473, 'accumulated_logging_time': 3.4067423343658447}
I1010 01:29:12.152627 139598139205376 logging_writer.py:48] [19564] accumulated_eval_time=585.299188, accumulated_logging_time=3.406742, accumulated_submission_time=5255.671774, global_step=19564, preemption_count=0, score=5255.671774, test/loss=0.287807, test/num_examples=3581, test/ssim=0.740939, total_duration=5907.396010, train/loss=0.266615, train/ssim=0.746354, validation/loss=0.286455, validation/num_examples=3554, validation/ssim=0.723704
I1010 01:30:32.608166 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:30:34.430827 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:30:36.278813 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:30:38.091499 139660997596992 submission_runner.py:381] Time since start: 5993.35s, 	Step: 19870, 	{'train/ssim': 0.7456118719918388, 'train/loss': 0.2670260838099888, 'validation/ssim': 0.722839664594471, 'validation/loss': 0.28712136220851503, 'validation/num_examples': 3554, 'test/ssim': 0.7400972717336288, 'test/loss': 0.2884945202326515, 'test/num_examples': 3581, 'score': 5335.137010097504, 'total_duration': 5993.352916717529, 'accumulated_submission_time': 5335.137010097504, 'accumulated_eval_time': 590.7827792167664, 'accumulated_logging_time': 3.4327847957611084}
I1010 01:30:38.110363 139598130812672 logging_writer.py:48] [19870] accumulated_eval_time=590.782779, accumulated_logging_time=3.432785, accumulated_submission_time=5335.137010, global_step=19870, preemption_count=0, score=5335.137010, test/loss=0.288495, test/num_examples=3581, test/ssim=0.740097, total_duration=5993.352917, train/loss=0.267026, train/ssim=0.745612, validation/loss=0.287121, validation/num_examples=3554, validation/ssim=0.722840
I1010 01:31:11.220829 139598139205376 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.251268, loss=0.280809
I1010 01:31:11.224275 139660997596992 pytorch_submission_base.py:86] 20000) loss = 0.281, grad_norm = 0.251
I1010 01:31:58.564269 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:32:00.388879 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:32:02.294121 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:32:04.114834 139660997596992 submission_runner.py:381] Time since start: 6079.38s, 	Step: 20176, 	{'train/ssim': 0.7475668362208775, 'train/loss': 0.26695774282727924, 'validation/ssim': 0.7253485976408625, 'validation/loss': 0.2867360026499631, 'validation/num_examples': 3554, 'test/ssim': 0.7423813262487783, 'test/loss': 0.28819580419008306, 'test/num_examples': 3581, 'score': 5414.630995750427, 'total_duration': 6079.376256227493, 'accumulated_submission_time': 5414.630995750427, 'accumulated_eval_time': 596.3335919380188, 'accumulated_logging_time': 3.4608311653137207}
I1010 01:32:04.133417 139598130812672 logging_writer.py:48] [20176] accumulated_eval_time=596.333592, accumulated_logging_time=3.460831, accumulated_submission_time=5414.630996, global_step=20176, preemption_count=0, score=5414.630996, test/loss=0.288196, test/num_examples=3581, test/ssim=0.742381, total_duration=6079.376256, train/loss=0.266958, train/ssim=0.747567, validation/loss=0.286736, validation/num_examples=3554, validation/ssim=0.725349
I1010 01:33:24.725386 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:33:26.536117 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:33:28.374923 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:33:30.184007 139660997596992 submission_runner.py:381] Time since start: 6165.45s, 	Step: 20482, 	{'train/ssim': 0.7448517254420689, 'train/loss': 0.2674118791307722, 'validation/ssim': 0.7221007854266319, 'validation/loss': 0.2872424707965497, 'validation/num_examples': 3554, 'test/ssim': 0.7394292086271292, 'test/loss': 0.28850870097825326, 'test/num_examples': 3581, 'score': 5494.289910078049, 'total_duration': 6165.445397853851, 'accumulated_submission_time': 5494.289910078049, 'accumulated_eval_time': 601.7922656536102, 'accumulated_logging_time': 3.48738169670105}
I1010 01:33:30.203332 139598139205376 logging_writer.py:48] [20482] accumulated_eval_time=601.792266, accumulated_logging_time=3.487382, accumulated_submission_time=5494.289910, global_step=20482, preemption_count=0, score=5494.289910, test/loss=0.288509, test/num_examples=3581, test/ssim=0.739429, total_duration=6165.445398, train/loss=0.267412, train/ssim=0.744852, validation/loss=0.287242, validation/num_examples=3554, validation/ssim=0.722101
I1010 01:33:33.021369 139598130812672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.192805, loss=0.205667
I1010 01:33:33.024950 139660997596992 pytorch_submission_base.py:86] 20500) loss = 0.206, grad_norm = 0.193
I1010 01:34:50.756567 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:34:52.563982 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:34:54.399345 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:34:56.202651 139660997596992 submission_runner.py:381] Time since start: 6251.46s, 	Step: 20790, 	{'train/ssim': 0.7468810762677874, 'train/loss': 0.26609039306640625, 'validation/ssim': 0.724243782313942, 'validation/loss': 0.28611012610680747, 'validation/num_examples': 3554, 'test/ssim': 0.741470417873848, 'test/loss': 0.28746883644800686, 'test/num_examples': 3581, 'score': 5573.882079839706, 'total_duration': 6251.464052915573, 'accumulated_submission_time': 5573.882079839706, 'accumulated_eval_time': 607.2385025024414, 'accumulated_logging_time': 3.515974760055542}
I1010 01:34:56.221323 139598139205376 logging_writer.py:48] [20790] accumulated_eval_time=607.238503, accumulated_logging_time=3.515975, accumulated_submission_time=5573.882080, global_step=20790, preemption_count=0, score=5573.882080, test/loss=0.287469, test/num_examples=3581, test/ssim=0.741470, total_duration=6251.464053, train/loss=0.266090, train/ssim=0.746881, validation/loss=0.286110, validation/num_examples=3554, validation/ssim=0.724244
I1010 01:35:50.654022 139598130812672 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.272535, loss=0.319375
I1010 01:35:50.657799 139660997596992 pytorch_submission_base.py:86] 21000) loss = 0.319, grad_norm = 0.273
I1010 01:36:16.743602 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:36:18.573966 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:36:20.399240 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:36:22.202245 139660997596992 submission_runner.py:381] Time since start: 6337.46s, 	Step: 21096, 	{'train/ssim': 0.7474692208426339, 'train/loss': 0.26601089750017437, 'validation/ssim': 0.7247561066755768, 'validation/loss': 0.28596232966485824, 'validation/num_examples': 3554, 'test/ssim': 0.7420016504206227, 'test/loss': 0.2873374600212057, 'test/num_examples': 3581, 'score': 5653.413482427597, 'total_duration': 6337.463673830032, 'accumulated_submission_time': 5653.413482427597, 'accumulated_eval_time': 612.6973757743835, 'accumulated_logging_time': 3.5425899028778076}
I1010 01:36:22.221532 139598139205376 logging_writer.py:48] [21096] accumulated_eval_time=612.697376, accumulated_logging_time=3.542590, accumulated_submission_time=5653.413482, global_step=21096, preemption_count=0, score=5653.413482, test/loss=0.287337, test/num_examples=3581, test/ssim=0.742002, total_duration=6337.463674, train/loss=0.266011, train/ssim=0.747469, validation/loss=0.285962, validation/num_examples=3554, validation/ssim=0.724756
I1010 01:37:42.722522 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:37:44.586900 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:37:46.431012 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:37:48.231175 139660997596992 submission_runner.py:381] Time since start: 6423.49s, 	Step: 21404, 	{'train/ssim': 0.7462524005344936, 'train/loss': 0.2663341590336391, 'validation/ssim': 0.723722321569886, 'validation/loss': 0.28620056255385656, 'validation/num_examples': 3554, 'test/ssim': 0.7410956507653239, 'test/loss': 0.2874490652161582, 'test/num_examples': 3581, 'score': 5732.928864479065, 'total_duration': 6423.492595434189, 'accumulated_submission_time': 5732.928864479065, 'accumulated_eval_time': 618.2063145637512, 'accumulated_logging_time': 3.570712089538574}
I1010 01:37:48.250139 139598130812672 logging_writer.py:48] [21404] accumulated_eval_time=618.206315, accumulated_logging_time=3.570712, accumulated_submission_time=5732.928864, global_step=21404, preemption_count=0, score=5732.928864, test/loss=0.287449, test/num_examples=3581, test/ssim=0.741096, total_duration=6423.492595, train/loss=0.266334, train/ssim=0.746252, validation/loss=0.286201, validation/num_examples=3554, validation/ssim=0.723722
I1010 01:38:11.950664 139598139205376 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.395621, loss=0.276625
I1010 01:38:11.954191 139660997596992 pytorch_submission_base.py:86] 21500) loss = 0.277, grad_norm = 0.396
I1010 01:39:08.670924 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:39:10.495041 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:39:12.320918 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:39:14.120608 139660997596992 submission_runner.py:381] Time since start: 6509.38s, 	Step: 21709, 	{'train/ssim': 0.7477067538670131, 'train/loss': 0.26630641732897076, 'validation/ssim': 0.725130629660242, 'validation/loss': 0.28622599673123594, 'validation/num_examples': 3554, 'test/ssim': 0.7423608050736527, 'test/loss': 0.28757379441845854, 'test/num_examples': 3581, 'score': 5812.379476070404, 'total_duration': 6509.382015943527, 'accumulated_submission_time': 5812.379476070404, 'accumulated_eval_time': 623.6561231613159, 'accumulated_logging_time': 3.5986313819885254}
I1010 01:39:14.140244 139598130812672 logging_writer.py:48] [21709] accumulated_eval_time=623.656123, accumulated_logging_time=3.598631, accumulated_submission_time=5812.379476, global_step=21709, preemption_count=0, score=5812.379476, test/loss=0.287574, test/num_examples=3581, test/ssim=0.742361, total_duration=6509.382016, train/loss=0.266306, train/ssim=0.747707, validation/loss=0.286226, validation/num_examples=3554, validation/ssim=0.725131
I1010 01:40:30.298880 139598139205376 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.348827, loss=0.255888
I1010 01:40:30.303375 139660997596992 pytorch_submission_base.py:86] 22000) loss = 0.256, grad_norm = 0.349
I1010 01:40:34.791560 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:40:36.660061 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:40:38.481121 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:40:40.282533 139660997596992 submission_runner.py:381] Time since start: 6595.54s, 	Step: 22016, 	{'train/ssim': 0.7475725582667759, 'train/loss': 0.26567951270512175, 'validation/ssim': 0.7247592666273917, 'validation/loss': 0.28580495032555747, 'validation/num_examples': 3554, 'test/ssim': 0.7419255652663362, 'test/loss': 0.28715413297830567, 'test/num_examples': 3581, 'score': 5892.081765413284, 'total_duration': 6595.543939828873, 'accumulated_submission_time': 5892.081765413284, 'accumulated_eval_time': 629.1475946903229, 'accumulated_logging_time': 3.6270670890808105}
I1010 01:40:40.302636 139598130812672 logging_writer.py:48] [22016] accumulated_eval_time=629.147595, accumulated_logging_time=3.627067, accumulated_submission_time=5892.081765, global_step=22016, preemption_count=0, score=5892.081765, test/loss=0.287154, test/num_examples=3581, test/ssim=0.741926, total_duration=6595.543940, train/loss=0.265680, train/ssim=0.747573, validation/loss=0.285805, validation/num_examples=3554, validation/ssim=0.724759
I1010 01:42:00.928424 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:42:02.886703 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:42:04.718025 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:42:06.534138 139660997596992 submission_runner.py:381] Time since start: 6681.80s, 	Step: 22322, 	{'train/ssim': 0.7461707932608468, 'train/loss': 0.2659803628921509, 'validation/ssim': 0.7235878175339406, 'validation/loss': 0.28586706742183104, 'validation/num_examples': 3554, 'test/ssim': 0.7408354204481988, 'test/loss': 0.28714925834700505, 'test/num_examples': 3581, 'score': 5971.721819400787, 'total_duration': 6681.795519351959, 'accumulated_submission_time': 5971.721819400787, 'accumulated_eval_time': 634.7534976005554, 'accumulated_logging_time': 3.656085968017578}
I1010 01:42:06.553460 139598139205376 logging_writer.py:48] [22322] accumulated_eval_time=634.753498, accumulated_logging_time=3.656086, accumulated_submission_time=5971.721819, global_step=22322, preemption_count=0, score=5971.721819, test/loss=0.287149, test/num_examples=3581, test/ssim=0.740835, total_duration=6681.795519, train/loss=0.265980, train/ssim=0.746171, validation/loss=0.285867, validation/num_examples=3554, validation/ssim=0.723588
I1010 01:42:52.240150 139598130812672 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.369215, loss=0.253407
I1010 01:42:52.243909 139660997596992 pytorch_submission_base.py:86] 22500) loss = 0.253, grad_norm = 0.369
I1010 01:43:27.206271 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:43:29.111966 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:43:31.089369 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:43:33.054265 139660997596992 submission_runner.py:381] Time since start: 6768.32s, 	Step: 22629, 	{'train/ssim': 0.7475331170218331, 'train/loss': 0.2657494374683925, 'validation/ssim': 0.7247872940260973, 'validation/loss': 0.2857416482473533, 'validation/num_examples': 3554, 'test/ssim': 0.7419881514416364, 'test/loss': 0.28704286866666084, 'test/num_examples': 3581, 'score': 6051.434379339218, 'total_duration': 6768.315627813339, 'accumulated_submission_time': 6051.434379339218, 'accumulated_eval_time': 640.6016840934753, 'accumulated_logging_time': 3.6837246417999268}
I1010 01:43:33.076338 139598139205376 logging_writer.py:48] [22629] accumulated_eval_time=640.601684, accumulated_logging_time=3.683725, accumulated_submission_time=6051.434379, global_step=22629, preemption_count=0, score=6051.434379, test/loss=0.287043, test/num_examples=3581, test/ssim=0.741988, total_duration=6768.315628, train/loss=0.265749, train/ssim=0.747533, validation/loss=0.285742, validation/num_examples=3554, validation/ssim=0.724787
I1010 01:44:53.784801 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:44:55.711894 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:44:57.667415 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:44:59.588322 139660997596992 submission_runner.py:381] Time since start: 6854.85s, 	Step: 22935, 	{'train/ssim': 0.7470641817365374, 'train/loss': 0.2654778446469988, 'validation/ssim': 0.7240030764191756, 'validation/loss': 0.2858085739659538, 'validation/num_examples': 3554, 'test/ssim': 0.7413221336349832, 'test/loss': 0.28711755619938567, 'test/num_examples': 3581, 'score': 6131.139020681381, 'total_duration': 6854.84974861145, 'accumulated_submission_time': 6131.139020681381, 'accumulated_eval_time': 646.4053626060486, 'accumulated_logging_time': 3.7143096923828125}
I1010 01:44:59.607574 139598130812672 logging_writer.py:48] [22935] accumulated_eval_time=646.405363, accumulated_logging_time=3.714310, accumulated_submission_time=6131.139021, global_step=22935, preemption_count=0, score=6131.139021, test/loss=0.287118, test/num_examples=3581, test/ssim=0.741322, total_duration=6854.849749, train/loss=0.265478, train/ssim=0.747064, validation/loss=0.285809, validation/num_examples=3554, validation/ssim=0.724003
I1010 01:45:15.336467 139598139205376 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.246848, loss=0.313995
I1010 01:45:15.340070 139660997596992 pytorch_submission_base.py:86] 23000) loss = 0.314, grad_norm = 0.247
I1010 01:46:20.309976 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:46:22.147473 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:46:23.988816 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:46:25.799092 139660997596992 submission_runner.py:381] Time since start: 6941.06s, 	Step: 23238, 	{'train/ssim': 0.7480191503252301, 'train/loss': 0.26546263694763184, 'validation/ssim': 0.7253986760076674, 'validation/loss': 0.2855343966250615, 'validation/num_examples': 3554, 'test/ssim': 0.7425673803581402, 'test/loss': 0.2868483947395979, 'test/num_examples': 3581, 'score': 6210.86292052269, 'total_duration': 6941.060522556305, 'accumulated_submission_time': 6210.86292052269, 'accumulated_eval_time': 651.8946976661682, 'accumulated_logging_time': 3.741570234298706}
I1010 01:46:25.819277 139598130812672 logging_writer.py:48] [23238] accumulated_eval_time=651.894698, accumulated_logging_time=3.741570, accumulated_submission_time=6210.862921, global_step=23238, preemption_count=0, score=6210.862921, test/loss=0.286848, test/num_examples=3581, test/ssim=0.742567, total_duration=6941.060523, train/loss=0.265463, train/ssim=0.748019, validation/loss=0.285534, validation/num_examples=3554, validation/ssim=0.725399
I1010 01:47:34.233788 139598139205376 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.244637, loss=0.281576
I1010 01:47:34.237282 139660997596992 pytorch_submission_base.py:86] 23500) loss = 0.282, grad_norm = 0.245
I1010 01:47:46.387957 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:47:48.314056 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:47:50.300703 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:47:52.264859 139660997596992 submission_runner.py:381] Time since start: 7027.53s, 	Step: 23545, 	{'train/ssim': 0.7479383604867118, 'train/loss': 0.2654566935130528, 'validation/ssim': 0.7254155748804164, 'validation/loss': 0.2854663717927863, 'validation/num_examples': 3554, 'test/ssim': 0.7425720845477869, 'test/loss': 0.28673811898954554, 'test/num_examples': 3581, 'score': 6290.48442363739, 'total_duration': 7027.526257514954, 'accumulated_submission_time': 6290.48442363739, 'accumulated_eval_time': 657.7718698978424, 'accumulated_logging_time': 3.7699053287506104}
I1010 01:47:52.286695 139598130812672 logging_writer.py:48] [23545] accumulated_eval_time=657.771870, accumulated_logging_time=3.769905, accumulated_submission_time=6290.484424, global_step=23545, preemption_count=0, score=6290.484424, test/loss=0.286738, test/num_examples=3581, test/ssim=0.742572, total_duration=7027.526258, train/loss=0.265457, train/ssim=0.747938, validation/loss=0.285466, validation/num_examples=3554, validation/ssim=0.725416
I1010 01:49:12.720307 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:49:14.656807 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:49:16.609650 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:49:18.532277 139660997596992 submission_runner.py:381] Time since start: 7113.79s, 	Step: 23847, 	{'train/ssim': 0.7476138387407575, 'train/loss': 0.2658113070896694, 'validation/ssim': 0.7249831423440138, 'validation/loss': 0.28573225426016463, 'validation/num_examples': 3554, 'test/ssim': 0.742188863533231, 'test/loss': 0.2871232830389556, 'test/num_examples': 3581, 'score': 6369.924151659012, 'total_duration': 7113.793695926666, 'accumulated_submission_time': 6369.924151659012, 'accumulated_eval_time': 663.5839703083038, 'accumulated_logging_time': 3.8012688159942627}
I1010 01:49:18.552134 139598139205376 logging_writer.py:48] [23847] accumulated_eval_time=663.583970, accumulated_logging_time=3.801269, accumulated_submission_time=6369.924152, global_step=23847, preemption_count=0, score=6369.924152, test/loss=0.287123, test/num_examples=3581, test/ssim=0.742189, total_duration=7113.793696, train/loss=0.265811, train/ssim=0.747614, validation/loss=0.285732, validation/num_examples=3554, validation/ssim=0.724983
I1010 01:49:57.675909 139598130812672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.353163, loss=0.251954
I1010 01:49:57.679421 139660997596992 pytorch_submission_base.py:86] 24000) loss = 0.252, grad_norm = 0.353
I1010 01:50:39.253770 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:50:41.215607 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:50:43.189944 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:50:45.149191 139660997596992 submission_runner.py:381] Time since start: 7200.41s, 	Step: 24155, 	{'train/ssim': 0.748699256352016, 'train/loss': 0.26490938663482666, 'validation/ssim': 0.7256678901633723, 'validation/loss': 0.28529393116141144, 'validation/num_examples': 3554, 'test/ssim': 0.7429206036416155, 'test/loss': 0.2865771879799637, 'test/num_examples': 3581, 'score': 6449.646962881088, 'total_duration': 7200.410577058792, 'accumulated_submission_time': 6449.646962881088, 'accumulated_eval_time': 669.4797310829163, 'accumulated_logging_time': 3.829319477081299}
I1010 01:50:45.172188 139598139205376 logging_writer.py:48] [24155] accumulated_eval_time=669.479731, accumulated_logging_time=3.829319, accumulated_submission_time=6449.646963, global_step=24155, preemption_count=0, score=6449.646963, test/loss=0.286577, test/num_examples=3581, test/ssim=0.742921, total_duration=7200.410577, train/loss=0.264909, train/ssim=0.748699, validation/loss=0.285294, validation/num_examples=3554, validation/ssim=0.725668
I1010 01:52:05.612643 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:52:07.526927 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:52:09.522291 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:52:11.456712 139660997596992 submission_runner.py:381] Time since start: 7286.72s, 	Step: 24460, 	{'train/ssim': 0.747868537902832, 'train/loss': 0.2651862416948591, 'validation/ssim': 0.7249457724790729, 'validation/loss': 0.2853731188669457, 'validation/num_examples': 3554, 'test/ssim': 0.7422227473340198, 'test/loss': 0.28668449804523877, 'test/num_examples': 3581, 'score': 6529.101427555084, 'total_duration': 7286.71812748909, 'accumulated_submission_time': 6529.101427555084, 'accumulated_eval_time': 675.3239223957062, 'accumulated_logging_time': 3.8611960411071777}
I1010 01:52:11.478001 139598130812672 logging_writer.py:48] [24460] accumulated_eval_time=675.323922, accumulated_logging_time=3.861196, accumulated_submission_time=6529.101428, global_step=24460, preemption_count=0, score=6529.101428, test/loss=0.286684, test/num_examples=3581, test/ssim=0.742223, total_duration=7286.718127, train/loss=0.265186, train/ssim=0.747869, validation/loss=0.285373, validation/num_examples=3554, validation/ssim=0.724946
I1010 01:52:20.092420 139598139205376 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.225628, loss=0.405700
I1010 01:52:20.095831 139660997596992 pytorch_submission_base.py:86] 24500) loss = 0.406, grad_norm = 0.226
I1010 01:53:32.183667 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:53:34.117316 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:53:36.098670 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:53:38.059143 139660997596992 submission_runner.py:381] Time since start: 7373.32s, 	Step: 24769, 	{'train/ssim': 0.7486545698983329, 'train/loss': 0.26505277838025776, 'validation/ssim': 0.7259173202729319, 'validation/loss': 0.2851830924167751, 'validation/num_examples': 3554, 'test/ssim': 0.743067728877234, 'test/loss': 0.2864458115531625, 'test/num_examples': 3581, 'score': 6608.810528516769, 'total_duration': 7373.320537567139, 'accumulated_submission_time': 6608.810528516769, 'accumulated_eval_time': 681.1995906829834, 'accumulated_logging_time': 3.8907217979431152}
I1010 01:53:38.082598 139598130812672 logging_writer.py:48] [24769] accumulated_eval_time=681.199591, accumulated_logging_time=3.890722, accumulated_submission_time=6608.810529, global_step=24769, preemption_count=0, score=6608.810529, test/loss=0.286446, test/num_examples=3581, test/ssim=0.743068, total_duration=7373.320538, train/loss=0.265053, train/ssim=0.748655, validation/loss=0.285183, validation/num_examples=3554, validation/ssim=0.725917
I1010 01:54:38.169371 139598139205376 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.318132, loss=0.207553
I1010 01:54:38.172925 139660997596992 pytorch_submission_base.py:86] 25000) loss = 0.208, grad_norm = 0.318
I1010 01:54:58.761465 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:55:00.692865 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:55:02.748061 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:55:04.718304 139660997596992 submission_runner.py:381] Time since start: 7459.98s, 	Step: 25077, 	{'train/ssim': 0.749190194266183, 'train/loss': 0.2647651093346732, 'validation/ssim': 0.7261166720156865, 'validation/loss': 0.28522559720341517, 'validation/num_examples': 3554, 'test/ssim': 0.7432428747207483, 'test/loss': 0.28661158310571416, 'test/num_examples': 3581, 'score': 6688.482065677643, 'total_duration': 7459.979671955109, 'accumulated_submission_time': 6688.482065677643, 'accumulated_eval_time': 687.1564803123474, 'accumulated_logging_time': 3.9229960441589355}
I1010 01:55:04.741096 139598130812672 logging_writer.py:48] [25077] accumulated_eval_time=687.156480, accumulated_logging_time=3.922996, accumulated_submission_time=6688.482066, global_step=25077, preemption_count=0, score=6688.482066, test/loss=0.286612, test/num_examples=3581, test/ssim=0.743243, total_duration=7459.979672, train/loss=0.264765, train/ssim=0.749190, validation/loss=0.285226, validation/num_examples=3554, validation/ssim=0.726117
I1010 01:56:25.265187 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:56:27.091365 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:56:28.971529 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:56:30.912213 139660997596992 submission_runner.py:381] Time since start: 7546.17s, 	Step: 25387, 	{'train/ssim': 0.748274530683245, 'train/loss': 0.2648131847381592, 'validation/ssim': 0.7252469983205543, 'validation/loss': 0.2851313138585045, 'validation/num_examples': 3554, 'test/ssim': 0.7425107937290562, 'test/loss': 0.28644206183677745, 'test/num_examples': 3581, 'score': 6768.029294490814, 'total_duration': 7546.173633813858, 'accumulated_submission_time': 6768.029294490814, 'accumulated_eval_time': 692.8036894798279, 'accumulated_logging_time': 3.9547364711761475}
I1010 01:56:30.931952 139598139205376 logging_writer.py:48] [25387] accumulated_eval_time=692.803689, accumulated_logging_time=3.954736, accumulated_submission_time=6768.029294, global_step=25387, preemption_count=0, score=6768.029294, test/loss=0.286442, test/num_examples=3581, test/ssim=0.742511, total_duration=7546.173634, train/loss=0.264813, train/ssim=0.748275, validation/loss=0.285131, validation/num_examples=3554, validation/ssim=0.725247
I1010 01:56:59.018754 139598130812672 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.264831, loss=0.262098
I1010 01:56:59.022722 139660997596992 pytorch_submission_base.py:86] 25500) loss = 0.262, grad_norm = 0.265
I1010 01:57:51.387079 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:57:53.209965 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:57:55.049685 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:57:56.859774 139660997596992 submission_runner.py:381] Time since start: 7632.12s, 	Step: 25694, 	{'train/ssim': 0.7489894458225795, 'train/loss': 0.2646549769810268, 'validation/ssim': 0.7260206369583568, 'validation/loss': 0.28497742077039073, 'validation/num_examples': 3554, 'test/ssim': 0.7432368751745323, 'test/loss': 0.28628235800710344, 'test/num_examples': 3581, 'score': 6847.542138338089, 'total_duration': 7632.121198415756, 'accumulated_submission_time': 6847.542138338089, 'accumulated_eval_time': 698.2766332626343, 'accumulated_logging_time': 3.9824976921081543}
I1010 01:57:56.879556 139598139205376 logging_writer.py:48] [25694] accumulated_eval_time=698.276633, accumulated_logging_time=3.982498, accumulated_submission_time=6847.542138, global_step=25694, preemption_count=0, score=6847.542138, test/loss=0.286282, test/num_examples=3581, test/ssim=0.743237, total_duration=7632.121198, train/loss=0.264655, train/ssim=0.748989, validation/loss=0.284977, validation/num_examples=3554, validation/ssim=0.726021
I1010 01:59:17.091594 139598130812672 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.259804, loss=0.287485
I1010 01:59:17.095186 139660997596992 pytorch_submission_base.py:86] 26000) loss = 0.287, grad_norm = 0.260
I1010 01:59:17.522173 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 01:59:19.335962 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 01:59:21.181830 139660997596992 spec.py:349] Evaluating on the test split.
I1010 01:59:22.987270 139660997596992 submission_runner.py:381] Time since start: 7718.25s, 	Step: 26001, 	{'train/ssim': 0.7492356300354004, 'train/loss': 0.26446800572531565, 'validation/ssim': 0.7262895076410383, 'validation/loss': 0.28476688898072594, 'validation/num_examples': 3554, 'test/ssim': 0.7435014006213349, 'test/loss': 0.28605604557909803, 'test/num_examples': 3581, 'score': 6927.24715256691, 'total_duration': 7718.248688220978, 'accumulated_submission_time': 6927.24715256691, 'accumulated_eval_time': 703.7418665885925, 'accumulated_logging_time': 4.0101752281188965}
I1010 01:59:23.007229 139598139205376 logging_writer.py:48] [26001] accumulated_eval_time=703.741867, accumulated_logging_time=4.010175, accumulated_submission_time=6927.247153, global_step=26001, preemption_count=0, score=6927.247153, test/loss=0.286056, test/num_examples=3581, test/ssim=0.743501, total_duration=7718.248688, train/loss=0.264468, train/ssim=0.749236, validation/loss=0.284767, validation/num_examples=3554, validation/ssim=0.726290
I1010 02:00:43.734469 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:00:45.568830 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 02:00:47.397641 139660997596992 spec.py:349] Evaluating on the test split.
I1010 02:00:49.201354 139660997596992 submission_runner.py:381] Time since start: 7804.46s, 	Step: 26309, 	{'train/ssim': 0.7496715273175921, 'train/loss': 0.2641012328011649, 'validation/ssim': 0.7263163672314645, 'validation/loss': 0.28474016677950903, 'validation/num_examples': 3554, 'test/ssim': 0.7435410794383552, 'test/loss': 0.2860657266650377, 'test/num_examples': 3581, 'score': 7007.040945529938, 'total_duration': 7804.462784767151, 'accumulated_submission_time': 7007.040945529938, 'accumulated_eval_time': 709.2088804244995, 'accumulated_logging_time': 4.038287878036499}
I1010 02:00:49.221669 139598130812672 logging_writer.py:48] [26309] accumulated_eval_time=709.208880, accumulated_logging_time=4.038288, accumulated_submission_time=7007.040946, global_step=26309, preemption_count=0, score=7007.040946, test/loss=0.286066, test/num_examples=3581, test/ssim=0.743541, total_duration=7804.462785, train/loss=0.264101, train/ssim=0.749672, validation/loss=0.284740, validation/num_examples=3554, validation/ssim=0.726316
I1010 02:01:38.524734 139598139205376 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.264473, loss=0.226532
I1010 02:01:38.528183 139660997596992 pytorch_submission_base.py:86] 26500) loss = 0.227, grad_norm = 0.264
I1010 02:02:09.807598 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:02:11.751369 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 02:02:13.739119 139660997596992 spec.py:349] Evaluating on the test split.
I1010 02:02:15.702847 139660997596992 submission_runner.py:381] Time since start: 7890.96s, 	Step: 26615, 	{'train/ssim': 0.7495265007019043, 'train/loss': 0.2640659638813564, 'validation/ssim': 0.7262778982528489, 'validation/loss': 0.28460322408509775, 'validation/num_examples': 3554, 'test/ssim': 0.7434862654024714, 'test/loss': 0.28591797079529985, 'test/num_examples': 3581, 'score': 7086.673682451248, 'total_duration': 7890.964240789413, 'accumulated_submission_time': 7086.673682451248, 'accumulated_eval_time': 715.1043362617493, 'accumulated_logging_time': 4.066661596298218}
I1010 02:02:15.726063 139598130812672 logging_writer.py:48] [26615] accumulated_eval_time=715.104336, accumulated_logging_time=4.066662, accumulated_submission_time=7086.673682, global_step=26615, preemption_count=0, score=7086.673682, test/loss=0.285918, test/num_examples=3581, test/ssim=0.743486, total_duration=7890.964241, train/loss=0.264066, train/ssim=0.749527, validation/loss=0.284603, validation/num_examples=3554, validation/ssim=0.726278
I1010 02:03:36.253155 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:03:38.156603 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 02:03:40.132722 139660997596992 spec.py:349] Evaluating on the test split.
I1010 02:03:42.074819 139660997596992 submission_runner.py:381] Time since start: 7977.34s, 	Step: 26921, 	{'train/ssim': 0.7497388294764927, 'train/loss': 0.26397292954581125, 'validation/ssim': 0.7264971027363534, 'validation/loss': 0.28449350162778736, 'validation/num_examples': 3554, 'test/ssim': 0.74368936367722, 'test/loss': 0.2857849410844562, 'test/num_examples': 3581, 'score': 7166.205990314484, 'total_duration': 7977.336238622665, 'accumulated_submission_time': 7166.205990314484, 'accumulated_eval_time': 720.9261713027954, 'accumulated_logging_time': 4.098658800125122}
I1010 02:03:42.095226 139598139205376 logging_writer.py:48] [26921] accumulated_eval_time=720.926171, accumulated_logging_time=4.098659, accumulated_submission_time=7166.205990, global_step=26921, preemption_count=0, score=7166.205990, test/loss=0.285785, test/num_examples=3581, test/ssim=0.743689, total_duration=7977.336239, train/loss=0.263973, train/ssim=0.749739, validation/loss=0.284494, validation/num_examples=3554, validation/ssim=0.726497
I1010 02:04:01.695323 139598130812672 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.247445, loss=0.280977
I1010 02:04:01.698891 139660997596992 pytorch_submission_base.py:86] 27000) loss = 0.281, grad_norm = 0.247
I1010 02:04:40.288599 139660997596992 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:04:42.114000 139660997596992 spec.py:333] Evaluating on the validation split.
I1010 02:04:43.956313 139660997596992 spec.py:349] Evaluating on the test split.
I1010 02:04:45.761471 139660997596992 submission_runner.py:381] Time since start: 8041.02s, 	Step: 27142, 	{'train/ssim': 0.7498985699244908, 'train/loss': 0.2640864849090576, 'validation/ssim': 0.7267003013769696, 'validation/loss': 0.2845645661963193, 'validation/num_examples': 3554, 'test/ssim': 0.7438857124624756, 'test/loss': 0.2858898649665771, 'test/num_examples': 3581, 'score': 7223.4665603637695, 'total_duration': 8041.022883415222, 'accumulated_submission_time': 7223.4665603637695, 'accumulated_eval_time': 726.3992280960083, 'accumulated_logging_time': 4.127046585083008}
I1010 02:04:45.781340 139598139205376 logging_writer.py:48] [27142] accumulated_eval_time=726.399228, accumulated_logging_time=4.127047, accumulated_submission_time=7223.466560, global_step=27142, preemption_count=0, score=7223.466560, test/loss=0.285890, test/num_examples=3581, test/ssim=0.743886, total_duration=8041.022883, train/loss=0.264086, train/ssim=0.749899, validation/loss=0.284565, validation/num_examples=3554, validation/ssim=0.726700
I1010 02:04:46.192798 139598130812672 logging_writer.py:48] [27142] global_step=27142, preemption_count=0, score=7223.466560
I1010 02:04:46.297107 139660997596992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/fastmri_targets_check/nesterov_run_0/fastmri_pytorch/trial_1/checkpoint_27142.
I1010 02:04:47.165801 139660997596992 submission_runner.py:549] Tuning trial 1/1
I1010 02:04:47.166010 139660997596992 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.028609, beta1=0.981543, beta2=0.9978504782314613, warmup_steps=1357, decay_steps_factor=0.984398, end_factor=0.01, weight_decay=0.000576)
I1010 02:04:47.176345 139660997596992 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ssim': 0.1878136055810111, 'train/loss': 1.106189250946045, 'validation/ssim': 0.1798226566346898, 'validation/loss': 1.1171398946389632, 'validation/num_examples': 3554, 'test/ssim': 0.20116746056116483, 'test/loss': 1.1132284130873011, 'test/num_examples': 3581, 'score': 84.83852958679199, 'total_duration': 305.0831594467163, 'accumulated_submission_time': 84.83852958679199, 'accumulated_eval_time': 219.79801106452942, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (302, {'train/ssim': 0.673194340297154, 'train/loss': 0.32891099793570383, 'validation/ssim': 0.6480368043709552, 'validation/loss': 0.35065342994908905, 'validation/num_examples': 3554, 'test/ssim': 0.6679116179576235, 'test/loss': 0.3517704388177185, 'test/num_examples': 3581, 'score': 164.36816549301147, 'total_duration': 391.5294907093048, 'accumulated_submission_time': 164.36816549301147, 'accumulated_eval_time': 225.59786081314087, 'accumulated_logging_time': 0.030315637588500977, 'global_step': 302, 'preemption_count': 0}), (522, {'train/ssim': 0.7009882245744977, 'train/loss': 0.3085228034428188, 'validation/ssim': 0.6795110235606008, 'validation/loss': 0.3280221641768078, 'validation/num_examples': 3554, 'test/ssim': 0.6983904035622033, 'test/loss': 0.3293236820633203, 'test/num_examples': 3581, 'score': 243.81787967681885, 'total_duration': 477.64457869529724, 'accumulated_submission_time': 243.81787967681885, 'accumulated_eval_time': 231.2707793712616, 'accumulated_logging_time': 0.0634005069732666, 'global_step': 522, 'preemption_count': 0}), (741, {'train/ssim': 0.7108234677995954, 'train/loss': 0.2994429383959089, 'validation/ssim': 0.6900917787396244, 'validation/loss': 0.3182765980013717, 'validation/num_examples': 3554, 'test/ssim': 0.7084422341873778, 'test/loss': 0.3198583411691043, 'test/num_examples': 3581, 'score': 323.333943605423, 'total_duration': 563.8180117607117, 'accumulated_submission_time': 323.333943605423, 'accumulated_eval_time': 236.93174743652344, 'accumulated_logging_time': 0.0981447696685791, 'global_step': 741, 'preemption_count': 0}), (947, {'train/ssim': 0.7176779338291713, 'train/loss': 0.293111971446446, 'validation/ssim': 0.6969130842888295, 'validation/loss': 0.3116147326296075, 'validation/num_examples': 3554, 'test/ssim': 0.7148627712885717, 'test/loss': 0.3134358951213872, 'test/num_examples': 3581, 'score': 402.8794529438019, 'total_duration': 651.1056900024414, 'accumulated_submission_time': 402.8794529438019, 'accumulated_eval_time': 243.669615983963, 'accumulated_logging_time': 0.12829375267028809, 'global_step': 947, 'preemption_count': 0}), (1252, {'train/ssim': 0.7250095094953265, 'train/loss': 0.28699493408203125, 'validation/ssim': 0.7034201120216658, 'validation/loss': 0.3057656274730058, 'validation/num_examples': 3554, 'test/ssim': 0.7210843007147095, 'test/loss': 0.30741804347162105, 'test/num_examples': 3581, 'score': 482.38947224617004, 'total_duration': 737.1644275188446, 'accumulated_submission_time': 482.38947224617004, 'accumulated_eval_time': 249.20625352859497, 'accumulated_logging_time': 0.1834704875946045, 'global_step': 1252, 'preemption_count': 0}), (1558, {'train/ssim': 0.7293124880109515, 'train/loss': 0.28075305053165983, 'validation/ssim': 0.7083720312939645, 'validation/loss': 0.299299610419158, 'validation/num_examples': 3554, 'test/ssim': 0.7254220408056409, 'test/loss': 0.3010486047782568, 'test/num_examples': 3581, 'score': 561.9244515895844, 'total_duration': 823.1319580078125, 'accumulated_submission_time': 561.9244515895844, 'accumulated_eval_time': 254.66623258590698, 'accumulated_logging_time': 0.2086184024810791, 'global_step': 1558, 'preemption_count': 0}), (1865, {'train/ssim': 0.7222302300589425, 'train/loss': 0.2828918525150844, 'validation/ssim': 0.7044938086926702, 'validation/loss': 0.3003139549517269, 'validation/num_examples': 3554, 'test/ssim': 0.7210478943774434, 'test/loss': 0.3021586912698967, 'test/num_examples': 3581, 'score': 641.470006942749, 'total_duration': 909.5239601135254, 'accumulated_submission_time': 641.470006942749, 'accumulated_eval_time': 260.5547978878021, 'accumulated_logging_time': 0.23319268226623535, 'global_step': 1865, 'preemption_count': 0}), (2170, {'train/ssim': 0.734670775277274, 'train/loss': 0.2769376720700945, 'validation/ssim': 0.7132202217132104, 'validation/loss': 0.2955353178197102, 'validation/num_examples': 3554, 'test/ssim': 0.7303017853558713, 'test/loss': 0.2972627547625489, 'test/num_examples': 3581, 'score': 721.0053427219391, 'total_duration': 995.8532979488373, 'accumulated_submission_time': 721.0053427219391, 'accumulated_eval_time': 266.3475959300995, 'accumulated_logging_time': 0.2587299346923828, 'global_step': 2170, 'preemption_count': 0}), (2470, {'train/ssim': 0.7319746017456055, 'train/loss': 0.27826389244624544, 'validation/ssim': 0.7124390953634989, 'validation/loss': 0.2967478806340567, 'validation/num_examples': 3554, 'test/ssim': 0.7290621291189612, 'test/loss': 0.29847701519303266, 'test/num_examples': 3581, 'score': 798.9338841438293, 'total_duration': 1081.8554601669312, 'accumulated_submission_time': 798.9338841438293, 'accumulated_eval_time': 271.81862139701843, 'accumulated_logging_time': 1.919734239578247, 'global_step': 2470, 'preemption_count': 0}), (2776, {'train/ssim': 0.7390833582196917, 'train/loss': 0.2733630963734218, 'validation/ssim': 0.7173482178970878, 'validation/loss': 0.2926767979301491, 'validation/num_examples': 3554, 'test/ssim': 0.7346614783318207, 'test/loss': 0.2941477290081332, 'test/num_examples': 3581, 'score': 878.4453988075256, 'total_duration': 1167.8219242095947, 'accumulated_submission_time': 878.4453988075256, 'accumulated_eval_time': 277.2645103931427, 'accumulated_logging_time': 1.94419527053833, 'global_step': 2776, 'preemption_count': 0}), (3081, {'train/ssim': 0.7399342400687081, 'train/loss': 0.2727991853441511, 'validation/ssim': 0.7182097856288688, 'validation/loss': 0.29207541101355866, 'validation/num_examples': 3554, 'test/ssim': 0.7355446388055012, 'test/loss': 0.29353962727546423, 'test/num_examples': 3581, 'score': 958.1526997089386, 'total_duration': 1254.2806787490845, 'accumulated_submission_time': 958.1526997089386, 'accumulated_eval_time': 283.033194065094, 'accumulated_logging_time': 1.9688525199890137, 'global_step': 3081, 'preemption_count': 0}), (3386, {'train/ssim': 0.7397701399666923, 'train/loss': 0.2718642098563058, 'validation/ssim': 0.7178768228800295, 'validation/loss': 0.2910674550792241, 'validation/num_examples': 3554, 'test/ssim': 0.7351424646790352, 'test/loss': 0.2925954827780299, 'test/num_examples': 3581, 'score': 1037.7781476974487, 'total_duration': 1340.6930603981018, 'accumulated_submission_time': 1037.7781476974487, 'accumulated_eval_time': 288.7838685512543, 'accumulated_logging_time': 1.9955394268035889, 'global_step': 3386, 'preemption_count': 0}), (3692, {'train/ssim': 0.7360054424830845, 'train/loss': 0.27480460916246685, 'validation/ssim': 0.715559685169879, 'validation/loss': 0.2933773798558842, 'validation/num_examples': 3554, 'test/ssim': 0.7328384344020525, 'test/loss': 0.2947410023169157, 'test/num_examples': 3581, 'score': 1117.2578999996185, 'total_duration': 1426.6145117282867, 'accumulated_submission_time': 1117.2578999996185, 'accumulated_eval_time': 294.2301528453827, 'accumulated_logging_time': 2.0197577476501465, 'global_step': 3692, 'preemption_count': 0}), (3999, {'train/ssim': 0.737025260925293, 'train/loss': 0.27403736114501953, 'validation/ssim': 0.7165397510947172, 'validation/loss': 0.29271097349597286, 'validation/num_examples': 3554, 'test/ssim': 0.7335206782541539, 'test/loss': 0.29427487848191847, 'test/num_examples': 3581, 'score': 1196.7379038333893, 'total_duration': 1512.555954694748, 'accumulated_submission_time': 1196.7379038333893, 'accumulated_eval_time': 299.726514339447, 'accumulated_logging_time': 2.0453457832336426, 'global_step': 3999, 'preemption_count': 0}), (4304, {'train/ssim': 0.7433297293526786, 'train/loss': 0.2719092539378575, 'validation/ssim': 0.7218965563669457, 'validation/loss': 0.2913017380284538, 'validation/num_examples': 3554, 'test/ssim': 0.7389514265830075, 'test/loss': 0.29285728115837056, 'test/num_examples': 3581, 'score': 1276.3680992126465, 'total_duration': 1598.6061973571777, 'accumulated_submission_time': 1276.3680992126465, 'accumulated_eval_time': 305.1786015033722, 'accumulated_logging_time': 2.0715436935424805, 'global_step': 4304, 'preemption_count': 0}), (4609, {'train/ssim': 0.7428179468427386, 'train/loss': 0.2697267362049648, 'validation/ssim': 0.7206678846678038, 'validation/loss': 0.2891256303416925, 'validation/num_examples': 3554, 'test/ssim': 0.7379392076890184, 'test/loss': 0.29058147601926837, 'test/num_examples': 3581, 'score': 1356.0617468357086, 'total_duration': 1685.1548902988434, 'accumulated_submission_time': 1356.0617468357086, 'accumulated_eval_time': 311.0451056957245, 'accumulated_logging_time': 2.097012758255005, 'global_step': 4609, 'preemption_count': 0}), (4913, {'train/ssim': 0.7426370212009975, 'train/loss': 0.27249326024736675, 'validation/ssim': 0.7211405722425436, 'validation/loss': 0.29187025457670934, 'validation/num_examples': 3554, 'test/ssim': 0.7380553125436331, 'test/loss': 0.2934393734946593, 'test/num_examples': 3581, 'score': 1435.53444814682, 'total_duration': 1771.411648273468, 'accumulated_submission_time': 1435.53444814682, 'accumulated_eval_time': 316.8288424015045, 'accumulated_logging_time': 2.123643636703491, 'global_step': 4913, 'preemption_count': 0}), (5222, {'train/ssim': 0.7392873764038086, 'train/loss': 0.2727458817618234, 'validation/ssim': 0.7170412904034187, 'validation/loss': 0.2920683354692776, 'validation/num_examples': 3554, 'test/ssim': 0.7344005662480801, 'test/loss': 0.29343824857974377, 'test/num_examples': 3581, 'score': 1515.08047580719, 'total_duration': 1857.3739607334137, 'accumulated_submission_time': 1515.08047580719, 'accumulated_eval_time': 322.2671821117401, 'accumulated_logging_time': 2.1490907669067383, 'global_step': 5222, 'preemption_count': 0}), (5524, {'train/ssim': 0.7408336230686733, 'train/loss': 0.27089851243155344, 'validation/ssim': 0.7186393329918753, 'validation/loss': 0.2903538555259039, 'validation/num_examples': 3554, 'test/ssim': 0.7360359880052709, 'test/loss': 0.29179894075284485, 'test/num_examples': 3581, 'score': 1594.6307199001312, 'total_duration': 1943.3848676681519, 'accumulated_submission_time': 1594.6307199001312, 'accumulated_eval_time': 327.71722078323364, 'accumulated_logging_time': 2.1737678050994873, 'global_step': 5524, 'preemption_count': 0}), (5830, {'train/ssim': 0.7426271438598633, 'train/loss': 0.2698540006365095, 'validation/ssim': 0.7207706517963914, 'validation/loss': 0.28905010062385694, 'validation/num_examples': 3554, 'test/ssim': 0.7381483055099832, 'test/loss': 0.29042518102267173, 'test/num_examples': 3581, 'score': 1674.3439302444458, 'total_duration': 2029.5392787456512, 'accumulated_submission_time': 1674.3439302444458, 'accumulated_eval_time': 333.1722204685211, 'accumulated_logging_time': 2.198284387588501, 'global_step': 5830, 'preemption_count': 0}), (6136, {'train/ssim': 0.7442991392953056, 'train/loss': 0.2695438861846924, 'validation/ssim': 0.7223965843943444, 'validation/loss': 0.2890330643618986, 'validation/num_examples': 3554, 'test/ssim': 0.7396124674933677, 'test/loss': 0.29048851714124896, 'test/num_examples': 3581, 'score': 1753.9117028713226, 'total_duration': 2115.5067443847656, 'accumulated_submission_time': 1753.9117028713226, 'accumulated_eval_time': 338.58848214149475, 'accumulated_logging_time': 2.223477840423584, 'global_step': 6136, 'preemption_count': 0}), (6437, {'train/ssim': 0.744375501360212, 'train/loss': 0.26940609727587017, 'validation/ssim': 0.7226136593451041, 'validation/loss': 0.28857658871386116, 'validation/num_examples': 3554, 'test/ssim': 0.7399255347231919, 'test/loss': 0.2899933159601019, 'test/num_examples': 3581, 'score': 1833.455997467041, 'total_duration': 2201.496433019638, 'accumulated_submission_time': 1833.455997467041, 'accumulated_eval_time': 344.0585124492645, 'accumulated_logging_time': 2.2476155757904053, 'global_step': 6437, 'preemption_count': 0}), (6743, {'train/ssim': 0.7445532935006278, 'train/loss': 0.26857779707227436, 'validation/ssim': 0.7221429639138999, 'validation/loss': 0.2883123205696926, 'validation/num_examples': 3554, 'test/ssim': 0.7393975746561715, 'test/loss': 0.289766014970504, 'test/num_examples': 3581, 'score': 1912.9912633895874, 'total_duration': 2287.502226114273, 'accumulated_submission_time': 1912.9912633895874, 'accumulated_eval_time': 349.53205132484436, 'accumulated_logging_time': 2.2725353240966797, 'global_step': 6743, 'preemption_count': 0}), (7048, {'train/ssim': 0.7416153635297503, 'train/loss': 0.26922340052468435, 'validation/ssim': 0.7194549440331317, 'validation/loss': 0.2884597048440138, 'validation/num_examples': 3554, 'test/ssim': 0.7369528277497557, 'test/loss': 0.28984929276258375, 'test/num_examples': 3581, 'score': 1992.6915755271912, 'total_duration': 2373.6827714443207, 'accumulated_submission_time': 1992.6915755271912, 'accumulated_eval_time': 355.0292468070984, 'accumulated_logging_time': 2.298259735107422, 'global_step': 7048, 'preemption_count': 0}), (7355, {'train/ssim': 0.7300920486450195, 'train/loss': 0.2805758033479963, 'validation/ssim': 0.7093407626090321, 'validation/loss': 0.29951654798070837, 'validation/num_examples': 3554, 'test/ssim': 0.7263529931190659, 'test/loss': 0.3014505061980767, 'test/num_examples': 3581, 'score': 2072.1570131778717, 'total_duration': 2459.547152519226, 'accumulated_submission_time': 2072.1570131778717, 'accumulated_eval_time': 360.459623336792, 'accumulated_logging_time': 2.322638988494873, 'global_step': 7355, 'preemption_count': 0}), (7656, {'train/ssim': 0.7367579596383231, 'train/loss': 0.2735166549682617, 'validation/ssim': 0.7152510403110931, 'validation/loss': 0.29251653341745215, 'validation/num_examples': 3554, 'test/ssim': 0.7323884684358419, 'test/loss': 0.29405299753691355, 'test/num_examples': 3581, 'score': 2151.8514437675476, 'total_duration': 2546.068855047226, 'accumulated_submission_time': 2151.8514437675476, 'accumulated_eval_time': 366.33084774017334, 'accumulated_logging_time': 2.347259759902954, 'global_step': 7656, 'preemption_count': 0}), (7963, {'train/ssim': 0.7334636960710798, 'train/loss': 0.27386721542903353, 'validation/ssim': 0.7117426694613463, 'validation/loss': 0.29263743592167274, 'validation/num_examples': 3554, 'test/ssim': 0.729336403828365, 'test/loss': 0.2941151405639137, 'test/num_examples': 3581, 'score': 2231.5210666656494, 'total_duration': 2632.54821228981, 'accumulated_submission_time': 2231.5210666656494, 'accumulated_eval_time': 372.12716698646545, 'accumulated_logging_time': 2.3744521141052246, 'global_step': 7963, 'preemption_count': 0}), (8270, {'train/ssim': 0.7420095716203962, 'train/loss': 0.271261419568743, 'validation/ssim': 0.7201556290007738, 'validation/loss': 0.2909583336997046, 'validation/num_examples': 3554, 'test/ssim': 0.7370135731551941, 'test/loss': 0.29261494721490156, 'test/num_examples': 3581, 'score': 2311.2081706523895, 'total_duration': 2718.7904584407806, 'accumulated_submission_time': 2311.2081706523895, 'accumulated_eval_time': 377.7106475830078, 'accumulated_logging_time': 2.400472402572632, 'global_step': 8270, 'preemption_count': 0}), (8574, {'train/ssim': 0.7410568509783063, 'train/loss': 0.27044616426740375, 'validation/ssim': 0.7194318626459623, 'validation/loss': 0.28954401483143993, 'validation/num_examples': 3554, 'test/ssim': 0.7367372531459438, 'test/loss': 0.29104916791748114, 'test/num_examples': 3581, 'score': 2390.693242073059, 'total_duration': 2804.7822086811066, 'accumulated_submission_time': 2390.693242073059, 'accumulated_eval_time': 383.22669315338135, 'accumulated_logging_time': 2.4268853664398193, 'global_step': 8574, 'preemption_count': 0}), (8881, {'train/ssim': 0.7423738070896694, 'train/loss': 0.27148147991725374, 'validation/ssim': 0.7203205647465883, 'validation/loss': 0.29121820538917415, 'validation/num_examples': 3554, 'test/ssim': 0.7371027482284976, 'test/loss': 0.2929939412764416, 'test/num_examples': 3581, 'score': 2470.2215464115143, 'total_duration': 2890.7677960395813, 'accumulated_submission_time': 2470.2215464115143, 'accumulated_eval_time': 388.7048466205597, 'accumulated_logging_time': 2.4529144763946533, 'global_step': 8881, 'preemption_count': 0}), (9186, {'train/ssim': 0.7413781029837472, 'train/loss': 0.27125138895852224, 'validation/ssim': 0.7195531773178109, 'validation/loss': 0.2904476236612795, 'validation/num_examples': 3554, 'test/ssim': 0.736561289182491, 'test/loss': 0.2922420208762392, 'test/num_examples': 3581, 'score': 2549.824760198593, 'total_duration': 2976.78538274765, 'accumulated_submission_time': 2549.824760198593, 'accumulated_eval_time': 394.1461937427521, 'accumulated_logging_time': 2.478292465209961, 'global_step': 9186, 'preemption_count': 0}), (9492, {'train/ssim': 0.7427920613970075, 'train/loss': 0.2703582389014108, 'validation/ssim': 0.7209653323060284, 'validation/loss': 0.2894901239140757, 'validation/num_examples': 3554, 'test/ssim': 0.7380324051853533, 'test/loss': 0.2910683255593759, 'test/num_examples': 3581, 'score': 2629.5283744335175, 'total_duration': 3062.9396636486053, 'accumulated_submission_time': 2629.5283744335175, 'accumulated_eval_time': 399.6296634674072, 'accumulated_logging_time': 2.5044710636138916, 'global_step': 9492, 'preemption_count': 0}), (9793, {'train/ssim': 0.7423223086765834, 'train/loss': 0.2698291369846889, 'validation/ssim': 0.7203698187781373, 'validation/loss': 0.2891008315894063, 'validation/num_examples': 3554, 'test/ssim': 0.7374906734327004, 'test/loss': 0.2907038872150761, 'test/num_examples': 3581, 'score': 2709.126131296158, 'total_duration': 3149.0004789829254, 'accumulated_submission_time': 2709.126131296158, 'accumulated_eval_time': 405.11137318611145, 'accumulated_logging_time': 2.5312657356262207, 'global_step': 9793, 'preemption_count': 0}), (10101, {'train/ssim': 0.7382858140127999, 'train/loss': 0.271142772265843, 'validation/ssim': 0.7166901235843416, 'validation/loss': 0.29029780072849254, 'validation/num_examples': 3554, 'test/ssim': 0.7338843325668458, 'test/loss': 0.291783123767366, 'test/num_examples': 3581, 'score': 2788.757292032242, 'total_duration': 3235.5619950294495, 'accumulated_submission_time': 2788.757292032242, 'accumulated_eval_time': 410.9862003326416, 'accumulated_logging_time': 2.55649995803833, 'global_step': 10101, 'preemption_count': 0}), (10406, {'train/ssim': 0.7445270674569267, 'train/loss': 0.2690940925053188, 'validation/ssim': 0.7225427665130838, 'validation/loss': 0.28861066123777784, 'validation/num_examples': 3554, 'test/ssim': 0.7396599184498045, 'test/loss': 0.290103932593462, 'test/num_examples': 3581, 'score': 2868.419425725937, 'total_duration': 3322.099216938019, 'accumulated_submission_time': 2868.419425725937, 'accumulated_eval_time': 416.82938718795776, 'accumulated_logging_time': 2.584564208984375, 'global_step': 10406, 'preemption_count': 0}), (10710, {'train/ssim': 0.7402200698852539, 'train/loss': 0.2710904393877302, 'validation/ssim': 0.7187054172015687, 'validation/loss': 0.2899298380785734, 'validation/num_examples': 3554, 'test/ssim': 0.7358622056949874, 'test/loss': 0.2914105383120113, 'test/num_examples': 3581, 'score': 2947.9170067310333, 'total_duration': 3408.3262116909027, 'accumulated_submission_time': 2947.9170067310333, 'accumulated_eval_time': 422.5909914970398, 'accumulated_logging_time': 2.6108925342559814, 'global_step': 10710, 'preemption_count': 0}), (11014, {'train/ssim': 0.7441794531685966, 'train/loss': 0.26878951277051655, 'validation/ssim': 0.7218553396041432, 'validation/loss': 0.2883972271010657, 'validation/num_examples': 3554, 'test/ssim': 0.7391236408300754, 'test/loss': 0.2898805176735723, 'test/num_examples': 3581, 'score': 3027.478718519211, 'total_duration': 3494.58607673645, 'accumulated_submission_time': 3027.478718519211, 'accumulated_eval_time': 428.31161999702454, 'accumulated_logging_time': 2.6388399600982666, 'global_step': 11014, 'preemption_count': 0}), (11319, {'train/ssim': 0.7435727119445801, 'train/loss': 0.2689156191689627, 'validation/ssim': 0.7217505803320202, 'validation/loss': 0.28824379770153347, 'validation/num_examples': 3554, 'test/ssim': 0.7389075889896328, 'test/loss': 0.2897215637871754, 'test/num_examples': 3581, 'score': 3107.0510540008545, 'total_duration': 3580.6951711177826, 'accumulated_submission_time': 3107.0510540008545, 'accumulated_eval_time': 433.86473083496094, 'accumulated_logging_time': 2.6659834384918213, 'global_step': 11319, 'preemption_count': 0}), (11625, {'train/ssim': 0.7432649476187569, 'train/loss': 0.2694261074066162, 'validation/ssim': 0.7212863421936551, 'validation/loss': 0.2886983155533378, 'validation/num_examples': 3554, 'test/ssim': 0.7385802728375453, 'test/loss': 0.29013737324595085, 'test/num_examples': 3581, 'score': 3186.6024153232574, 'total_duration': 3667.1931533813477, 'accumulated_submission_time': 3186.6024153232574, 'accumulated_eval_time': 439.8286702632904, 'accumulated_logging_time': 2.694730281829834, 'global_step': 11625, 'preemption_count': 0}), (11928, {'train/ssim': 0.7427441052028111, 'train/loss': 0.27012550830841064, 'validation/ssim': 0.7204972472698016, 'validation/loss': 0.28967329807743036, 'validation/num_examples': 3554, 'test/ssim': 0.7376360942517802, 'test/loss': 0.29115678477773316, 'test/num_examples': 3581, 'score': 3266.139392375946, 'total_duration': 3753.7155635356903, 'accumulated_submission_time': 3266.139392375946, 'accumulated_eval_time': 445.7641804218292, 'accumulated_logging_time': 2.724207878112793, 'global_step': 11928, 'preemption_count': 0}), (12234, {'train/ssim': 0.7430727141244071, 'train/loss': 0.26856000082833426, 'validation/ssim': 0.7210674811831739, 'validation/loss': 0.2880525347484788, 'validation/num_examples': 3554, 'test/ssim': 0.7382499569123498, 'test/loss': 0.2895450884987608, 'test/num_examples': 3581, 'score': 3345.680385351181, 'total_duration': 3840.083880186081, 'accumulated_submission_time': 3345.680385351181, 'accumulated_eval_time': 451.56626772880554, 'accumulated_logging_time': 2.7534189224243164, 'global_step': 12234, 'preemption_count': 0}), (12540, {'train/ssim': 0.74507965360369, 'train/loss': 0.2683646168027605, 'validation/ssim': 0.7228972306731851, 'validation/loss': 0.28797910021608575, 'validation/num_examples': 3554, 'test/ssim': 0.7401541992460207, 'test/loss': 0.2894209387980836, 'test/num_examples': 3581, 'score': 3425.16584277153, 'total_duration': 3925.993871450424, 'accumulated_submission_time': 3425.16584277153, 'accumulated_eval_time': 457.0259292125702, 'accumulated_logging_time': 2.7801856994628906, 'global_step': 12540, 'preemption_count': 0}), (12844, {'train/ssim': 0.7456983838762555, 'train/loss': 0.268803460257394, 'validation/ssim': 0.7237162764446751, 'validation/loss': 0.2885054211034222, 'validation/num_examples': 3554, 'test/ssim': 0.7408583278064786, 'test/loss': 0.2899693177752374, 'test/num_examples': 3581, 'score': 3504.6441464424133, 'total_duration': 4012.37633228302, 'accumulated_submission_time': 3504.6441464424133, 'accumulated_eval_time': 462.9613928794861, 'accumulated_logging_time': 2.805645704269409, 'global_step': 12844, 'preemption_count': 0}), (13149, {'train/ssim': 0.744255610874721, 'train/loss': 0.26796075275966097, 'validation/ssim': 0.7216271361274268, 'validation/loss': 0.2878519636764913, 'validation/num_examples': 3554, 'test/ssim': 0.7388769776685982, 'test/loss': 0.2892889146929978, 'test/num_examples': 3581, 'score': 3584.1353940963745, 'total_duration': 4098.7269587516785, 'accumulated_submission_time': 3584.1353940963745, 'accumulated_eval_time': 468.8199112415314, 'accumulated_logging_time': 2.833308219909668, 'global_step': 13149, 'preemption_count': 0}), (13455, {'train/ssim': 0.7455732481820243, 'train/loss': 0.26829707622528076, 'validation/ssim': 0.7233232059167487, 'validation/loss': 0.2879369732497714, 'validation/num_examples': 3554, 'test/ssim': 0.7404644712327213, 'test/loss': 0.2893712039234851, 'test/num_examples': 3581, 'score': 3663.742796897888, 'total_duration': 4184.762919187546, 'accumulated_submission_time': 3663.742796897888, 'accumulated_eval_time': 474.2775752544403, 'accumulated_logging_time': 2.8592922687530518, 'global_step': 13455, 'preemption_count': 0}), (13761, {'train/ssim': 0.7417126383100238, 'train/loss': 0.2690096242087228, 'validation/ssim': 0.7201379744873734, 'validation/loss': 0.28831682006629855, 'validation/num_examples': 3554, 'test/ssim': 0.7374621755881737, 'test/loss': 0.28971692777419017, 'test/num_examples': 3581, 'score': 3743.281441450119, 'total_duration': 4270.758888959885, 'accumulated_submission_time': 3743.281441450119, 'accumulated_eval_time': 479.7278366088867, 'accumulated_logging_time': 2.8850831985473633, 'global_step': 13761, 'preemption_count': 0}), (14066, {'train/ssim': 0.7469384329659599, 'train/loss': 0.26976278850010466, 'validation/ssim': 0.7247166759724958, 'validation/loss': 0.28990582931424097, 'validation/num_examples': 3554, 'test/ssim': 0.7416990142200154, 'test/loss': 0.29146818167934935, 'test/num_examples': 3581, 'score': 3822.989084005356, 'total_duration': 4356.886250734329, 'accumulated_submission_time': 3822.989084005356, 'accumulated_eval_time': 485.17335534095764, 'accumulated_logging_time': 2.9106526374816895, 'global_step': 14066, 'preemption_count': 0}), (14371, {'train/ssim': 0.7456225667681012, 'train/loss': 0.26739002977098736, 'validation/ssim': 0.7232266899971863, 'validation/loss': 0.2873235819510147, 'validation/num_examples': 3554, 'test/ssim': 0.7404136796198687, 'test/loss': 0.2887887707038886, 'test/num_examples': 3581, 'score': 3902.6783130168915, 'total_duration': 4443.466443538666, 'accumulated_submission_time': 3902.6783130168915, 'accumulated_eval_time': 491.0763490200043, 'accumulated_logging_time': 2.9384188652038574, 'global_step': 14371, 'preemption_count': 0}), (14676, {'train/ssim': 0.7464823041643415, 'train/loss': 0.2678264890398298, 'validation/ssim': 0.7241860788460186, 'validation/loss': 0.28746432002233396, 'validation/num_examples': 3554, 'test/ssim': 0.7413907875331611, 'test/loss': 0.288878116218846, 'test/num_examples': 3581, 'score': 3982.3136689662933, 'total_duration': 4529.918646335602, 'accumulated_submission_time': 3982.3136689662933, 'accumulated_eval_time': 496.87917709350586, 'accumulated_logging_time': 2.967838764190674, 'global_step': 14676, 'preemption_count': 0}), (14981, {'train/ssim': 0.7413056918552944, 'train/loss': 0.2694911275591169, 'validation/ssim': 0.7206393077122608, 'validation/loss': 0.28855099997362127, 'validation/num_examples': 3554, 'test/ssim': 0.7375194439838733, 'test/loss': 0.28994453555876504, 'test/num_examples': 3581, 'score': 4062.038354873657, 'total_duration': 4616.077848672867, 'accumulated_submission_time': 4062.038354873657, 'accumulated_eval_time': 502.35155606269836, 'accumulated_logging_time': 2.993614435195923, 'global_step': 14981, 'preemption_count': 0}), (15284, {'train/ssim': 0.74451630456107, 'train/loss': 0.2681370632989066, 'validation/ssim': 0.7216035051834201, 'validation/loss': 0.28840172659767166, 'validation/num_examples': 3554, 'test/ssim': 0.7387580093941986, 'test/loss': 0.2898277830258657, 'test/num_examples': 3581, 'score': 4141.665055990219, 'total_duration': 4702.270579338074, 'accumulated_submission_time': 4141.665055990219, 'accumulated_eval_time': 507.9534640312195, 'accumulated_logging_time': 3.019418478012085, 'global_step': 15284, 'preemption_count': 0}), (15591, {'train/ssim': 0.7450955254690987, 'train/loss': 0.2675144502094814, 'validation/ssim': 0.7227013823552687, 'validation/loss': 0.2871259132260745, 'validation/num_examples': 3554, 'test/ssim': 0.7400002563442474, 'test/loss': 0.28851943880244696, 'test/num_examples': 3581, 'score': 4221.113683462143, 'total_duration': 4788.162426233292, 'accumulated_submission_time': 4221.113683462143, 'accumulated_eval_time': 513.4097089767456, 'accumulated_logging_time': 3.0456342697143555, 'global_step': 15591, 'preemption_count': 0}), (15896, {'train/ssim': 0.743556295122419, 'train/loss': 0.26878511905670166, 'validation/ssim': 0.7219262324361635, 'validation/loss': 0.288113999246008, 'validation/num_examples': 3554, 'test/ssim': 0.7390444877260193, 'test/loss': 0.2894575155770036, 'test/num_examples': 3581, 'score': 4300.635037660599, 'total_duration': 4874.11335849762, 'accumulated_submission_time': 4300.635037660599, 'accumulated_eval_time': 518.8496088981628, 'accumulated_logging_time': 3.073381185531616, 'global_step': 15896, 'preemption_count': 0}), (16199, {'train/ssim': 0.744654655456543, 'train/loss': 0.26743718555995394, 'validation/ssim': 0.7224630807716658, 'validation/loss': 0.286956477983654, 'validation/num_examples': 3554, 'test/ssim': 0.7397725462946803, 'test/loss': 0.2882901606896642, 'test/num_examples': 3581, 'score': 4380.129357814789, 'total_duration': 4960.033996105194, 'accumulated_submission_time': 4380.129357814789, 'accumulated_eval_time': 524.2980237007141, 'accumulated_logging_time': 3.099935293197632, 'global_step': 16199, 'preemption_count': 0}), (16506, {'train/ssim': 0.7473114558628627, 'train/loss': 0.2667367288044521, 'validation/ssim': 0.7247202480919387, 'validation/loss': 0.286781770430325, 'validation/num_examples': 3554, 'test/ssim': 0.7418561614248813, 'test/loss': 0.28819331574193663, 'test/num_examples': 3581, 'score': 4459.870715856552, 'total_duration': 5046.213260173798, 'accumulated_submission_time': 4459.870715856552, 'accumulated_eval_time': 529.7706182003021, 'accumulated_logging_time': 3.1263413429260254, 'global_step': 16506, 'preemption_count': 0}), (16813, {'train/ssim': 0.7459908212934222, 'train/loss': 0.26730593613215853, 'validation/ssim': 0.7238091515501899, 'validation/loss': 0.2870320763960942, 'validation/num_examples': 3554, 'test/ssim': 0.7409432759267662, 'test/loss': 0.28845719351045446, 'test/num_examples': 3581, 'score': 4539.347900867462, 'total_duration': 5132.143274784088, 'accumulated_submission_time': 4539.347900867462, 'accumulated_eval_time': 535.2460277080536, 'accumulated_logging_time': 3.1539697647094727, 'global_step': 16813, 'preemption_count': 0}), (17119, {'train/ssim': 0.7451875550406319, 'train/loss': 0.2672051021030971, 'validation/ssim': 0.7229054740257457, 'validation/loss': 0.286712749526282, 'validation/num_examples': 3554, 'test/ssim': 0.7402188988978288, 'test/loss': 0.2880518491692265, 'test/num_examples': 3581, 'score': 4619.013758182526, 'total_duration': 5218.240732908249, 'accumulated_submission_time': 4619.013758182526, 'accumulated_eval_time': 540.7179870605469, 'accumulated_logging_time': 3.1814231872558594, 'global_step': 17119, 'preemption_count': 0}), (17422, {'train/ssim': 0.7448293822152274, 'train/loss': 0.267853992325919, 'validation/ssim': 0.7218951824748523, 'validation/loss': 0.2877952734539867, 'validation/num_examples': 3554, 'test/ssim': 0.739165092240296, 'test/loss': 0.2892288510541748, 'test/num_examples': 3581, 'score': 4698.644377231598, 'total_duration': 5304.314746856689, 'accumulated_submission_time': 4698.644377231598, 'accumulated_eval_time': 546.1922211647034, 'accumulated_logging_time': 3.209461212158203, 'global_step': 17422, 'preemption_count': 0}), (17728, {'train/ssim': 0.7443248203822544, 'train/loss': 0.2673984595707485, 'validation/ssim': 0.7215969105013716, 'validation/loss': 0.28727506638646594, 'validation/num_examples': 3554, 'test/ssim': 0.7389689479850251, 'test/loss': 0.2886345210071384, 'test/num_examples': 3581, 'score': 4778.120165109634, 'total_duration': 5390.215332508087, 'accumulated_submission_time': 4778.120165109634, 'accumulated_eval_time': 551.6486639976501, 'accumulated_logging_time': 3.236421585083008, 'global_step': 17728, 'preemption_count': 0}), (18034, {'train/ssim': 0.7464590072631836, 'train/loss': 0.2673988342285156, 'validation/ssim': 0.7242340276800788, 'validation/loss': 0.28714501032617296, 'validation/num_examples': 3554, 'test/ssim': 0.74141805819778, 'test/loss': 0.2884813280486421, 'test/num_examples': 3581, 'score': 4857.591336250305, 'total_duration': 5476.188336610794, 'accumulated_submission_time': 4857.591336250305, 'accumulated_eval_time': 557.1330442428589, 'accumulated_logging_time': 3.2637789249420166, 'global_step': 18034, 'preemption_count': 0}), (18339, {'train/ssim': 0.7461464064461845, 'train/loss': 0.26695222514016287, 'validation/ssim': 0.7237971299943725, 'validation/loss': 0.28658566450764106, 'validation/num_examples': 3554, 'test/ssim': 0.7410681755707205, 'test/loss': 0.2880024892662664, 'test/num_examples': 3581, 'score': 4937.138226270676, 'total_duration': 5562.203558683395, 'accumulated_submission_time': 4937.138226270676, 'accumulated_eval_time': 562.6163387298584, 'accumulated_logging_time': 3.2896788120269775, 'global_step': 18339, 'preemption_count': 0}), (18645, {'train/ssim': 0.745931693485805, 'train/loss': 0.26716104575565885, 'validation/ssim': 0.7233712921400183, 'validation/loss': 0.286998158435038, 'validation/num_examples': 3554, 'test/ssim': 0.7405681679349344, 'test/loss': 0.2884285934009355, 'test/num_examples': 3581, 'score': 5016.8269464969635, 'total_duration': 5648.349233388901, 'accumulated_submission_time': 5016.8269464969635, 'accumulated_eval_time': 568.1021540164948, 'accumulated_logging_time': 3.318765878677368, 'global_step': 18645, 'preemption_count': 0}), (18953, {'train/ssim': 0.7464321000235421, 'train/loss': 0.26677760056086947, 'validation/ssim': 0.7241322222759566, 'validation/loss': 0.2864446173106007, 'validation/num_examples': 3554, 'test/ssim': 0.7413291558311226, 'test/loss': 0.28782925236927537, 'test/num_examples': 3581, 'score': 5096.485533237457, 'total_duration': 5734.906534671783, 'accumulated_submission_time': 5096.485533237457, 'accumulated_eval_time': 573.983553647995, 'accumulated_logging_time': 3.3463873863220215, 'global_step': 18953, 'preemption_count': 0}), (19260, {'train/ssim': 0.7460182734898159, 'train/loss': 0.26684517519814627, 'validation/ssim': 0.7235738725291925, 'validation/loss': 0.2865671684853334, 'validation/num_examples': 3554, 'test/ssim': 0.7407747432194219, 'test/loss': 0.28790042880393046, 'test/num_examples': 3581, 'score': 5176.060472011566, 'total_duration': 5821.340844154358, 'accumulated_submission_time': 5176.060472011566, 'accumulated_eval_time': 579.8217279911041, 'accumulated_logging_time': 3.3792102336883545, 'global_step': 19260, 'preemption_count': 0}), (19564, {'train/ssim': 0.7463537624904087, 'train/loss': 0.2666154078074864, 'validation/ssim': 0.7237041174996482, 'validation/loss': 0.286454784112092, 'validation/num_examples': 3554, 'test/ssim': 0.7409394580337196, 'test/loss': 0.28780706086594177, 'test/num_examples': 3581, 'score': 5255.67177438736, 'total_duration': 5907.396010160446, 'accumulated_submission_time': 5255.67177438736, 'accumulated_eval_time': 585.299188375473, 'accumulated_logging_time': 3.4067423343658447, 'global_step': 19564, 'preemption_count': 0}), (19870, {'train/ssim': 0.7456118719918388, 'train/loss': 0.2670260838099888, 'validation/ssim': 0.722839664594471, 'validation/loss': 0.28712136220851503, 'validation/num_examples': 3554, 'test/ssim': 0.7400972717336288, 'test/loss': 0.2884945202326515, 'test/num_examples': 3581, 'score': 5335.137010097504, 'total_duration': 5993.352916717529, 'accumulated_submission_time': 5335.137010097504, 'accumulated_eval_time': 590.7827792167664, 'accumulated_logging_time': 3.4327847957611084, 'global_step': 19870, 'preemption_count': 0}), (20176, {'train/ssim': 0.7475668362208775, 'train/loss': 0.26695774282727924, 'validation/ssim': 0.7253485976408625, 'validation/loss': 0.2867360026499631, 'validation/num_examples': 3554, 'test/ssim': 0.7423813262487783, 'test/loss': 0.28819580419008306, 'test/num_examples': 3581, 'score': 5414.630995750427, 'total_duration': 6079.376256227493, 'accumulated_submission_time': 5414.630995750427, 'accumulated_eval_time': 596.3335919380188, 'accumulated_logging_time': 3.4608311653137207, 'global_step': 20176, 'preemption_count': 0}), (20482, {'train/ssim': 0.7448517254420689, 'train/loss': 0.2674118791307722, 'validation/ssim': 0.7221007854266319, 'validation/loss': 0.2872424707965497, 'validation/num_examples': 3554, 'test/ssim': 0.7394292086271292, 'test/loss': 0.28850870097825326, 'test/num_examples': 3581, 'score': 5494.289910078049, 'total_duration': 6165.445397853851, 'accumulated_submission_time': 5494.289910078049, 'accumulated_eval_time': 601.7922656536102, 'accumulated_logging_time': 3.48738169670105, 'global_step': 20482, 'preemption_count': 0}), (20790, {'train/ssim': 0.7468810762677874, 'train/loss': 0.26609039306640625, 'validation/ssim': 0.724243782313942, 'validation/loss': 0.28611012610680747, 'validation/num_examples': 3554, 'test/ssim': 0.741470417873848, 'test/loss': 0.28746883644800686, 'test/num_examples': 3581, 'score': 5573.882079839706, 'total_duration': 6251.464052915573, 'accumulated_submission_time': 5573.882079839706, 'accumulated_eval_time': 607.2385025024414, 'accumulated_logging_time': 3.515974760055542, 'global_step': 20790, 'preemption_count': 0}), (21096, {'train/ssim': 0.7474692208426339, 'train/loss': 0.26601089750017437, 'validation/ssim': 0.7247561066755768, 'validation/loss': 0.28596232966485824, 'validation/num_examples': 3554, 'test/ssim': 0.7420016504206227, 'test/loss': 0.2873374600212057, 'test/num_examples': 3581, 'score': 5653.413482427597, 'total_duration': 6337.463673830032, 'accumulated_submission_time': 5653.413482427597, 'accumulated_eval_time': 612.6973757743835, 'accumulated_logging_time': 3.5425899028778076, 'global_step': 21096, 'preemption_count': 0}), (21404, {'train/ssim': 0.7462524005344936, 'train/loss': 0.2663341590336391, 'validation/ssim': 0.723722321569886, 'validation/loss': 0.28620056255385656, 'validation/num_examples': 3554, 'test/ssim': 0.7410956507653239, 'test/loss': 0.2874490652161582, 'test/num_examples': 3581, 'score': 5732.928864479065, 'total_duration': 6423.492595434189, 'accumulated_submission_time': 5732.928864479065, 'accumulated_eval_time': 618.2063145637512, 'accumulated_logging_time': 3.570712089538574, 'global_step': 21404, 'preemption_count': 0}), (21709, {'train/ssim': 0.7477067538670131, 'train/loss': 0.26630641732897076, 'validation/ssim': 0.725130629660242, 'validation/loss': 0.28622599673123594, 'validation/num_examples': 3554, 'test/ssim': 0.7423608050736527, 'test/loss': 0.28757379441845854, 'test/num_examples': 3581, 'score': 5812.379476070404, 'total_duration': 6509.382015943527, 'accumulated_submission_time': 5812.379476070404, 'accumulated_eval_time': 623.6561231613159, 'accumulated_logging_time': 3.5986313819885254, 'global_step': 21709, 'preemption_count': 0}), (22016, {'train/ssim': 0.7475725582667759, 'train/loss': 0.26567951270512175, 'validation/ssim': 0.7247592666273917, 'validation/loss': 0.28580495032555747, 'validation/num_examples': 3554, 'test/ssim': 0.7419255652663362, 'test/loss': 0.28715413297830567, 'test/num_examples': 3581, 'score': 5892.081765413284, 'total_duration': 6595.543939828873, 'accumulated_submission_time': 5892.081765413284, 'accumulated_eval_time': 629.1475946903229, 'accumulated_logging_time': 3.6270670890808105, 'global_step': 22016, 'preemption_count': 0}), (22322, {'train/ssim': 0.7461707932608468, 'train/loss': 0.2659803628921509, 'validation/ssim': 0.7235878175339406, 'validation/loss': 0.28586706742183104, 'validation/num_examples': 3554, 'test/ssim': 0.7408354204481988, 'test/loss': 0.28714925834700505, 'test/num_examples': 3581, 'score': 5971.721819400787, 'total_duration': 6681.795519351959, 'accumulated_submission_time': 5971.721819400787, 'accumulated_eval_time': 634.7534976005554, 'accumulated_logging_time': 3.656085968017578, 'global_step': 22322, 'preemption_count': 0}), (22629, {'train/ssim': 0.7475331170218331, 'train/loss': 0.2657494374683925, 'validation/ssim': 0.7247872940260973, 'validation/loss': 0.2857416482473533, 'validation/num_examples': 3554, 'test/ssim': 0.7419881514416364, 'test/loss': 0.28704286866666084, 'test/num_examples': 3581, 'score': 6051.434379339218, 'total_duration': 6768.315627813339, 'accumulated_submission_time': 6051.434379339218, 'accumulated_eval_time': 640.6016840934753, 'accumulated_logging_time': 3.6837246417999268, 'global_step': 22629, 'preemption_count': 0}), (22935, {'train/ssim': 0.7470641817365374, 'train/loss': 0.2654778446469988, 'validation/ssim': 0.7240030764191756, 'validation/loss': 0.2858085739659538, 'validation/num_examples': 3554, 'test/ssim': 0.7413221336349832, 'test/loss': 0.28711755619938567, 'test/num_examples': 3581, 'score': 6131.139020681381, 'total_duration': 6854.84974861145, 'accumulated_submission_time': 6131.139020681381, 'accumulated_eval_time': 646.4053626060486, 'accumulated_logging_time': 3.7143096923828125, 'global_step': 22935, 'preemption_count': 0}), (23238, {'train/ssim': 0.7480191503252301, 'train/loss': 0.26546263694763184, 'validation/ssim': 0.7253986760076674, 'validation/loss': 0.2855343966250615, 'validation/num_examples': 3554, 'test/ssim': 0.7425673803581402, 'test/loss': 0.2868483947395979, 'test/num_examples': 3581, 'score': 6210.86292052269, 'total_duration': 6941.060522556305, 'accumulated_submission_time': 6210.86292052269, 'accumulated_eval_time': 651.8946976661682, 'accumulated_logging_time': 3.741570234298706, 'global_step': 23238, 'preemption_count': 0}), (23545, {'train/ssim': 0.7479383604867118, 'train/loss': 0.2654566935130528, 'validation/ssim': 0.7254155748804164, 'validation/loss': 0.2854663717927863, 'validation/num_examples': 3554, 'test/ssim': 0.7425720845477869, 'test/loss': 0.28673811898954554, 'test/num_examples': 3581, 'score': 6290.48442363739, 'total_duration': 7027.526257514954, 'accumulated_submission_time': 6290.48442363739, 'accumulated_eval_time': 657.7718698978424, 'accumulated_logging_time': 3.7699053287506104, 'global_step': 23545, 'preemption_count': 0}), (23847, {'train/ssim': 0.7476138387407575, 'train/loss': 0.2658113070896694, 'validation/ssim': 0.7249831423440138, 'validation/loss': 0.28573225426016463, 'validation/num_examples': 3554, 'test/ssim': 0.742188863533231, 'test/loss': 0.2871232830389556, 'test/num_examples': 3581, 'score': 6369.924151659012, 'total_duration': 7113.793695926666, 'accumulated_submission_time': 6369.924151659012, 'accumulated_eval_time': 663.5839703083038, 'accumulated_logging_time': 3.8012688159942627, 'global_step': 23847, 'preemption_count': 0}), (24155, {'train/ssim': 0.748699256352016, 'train/loss': 0.26490938663482666, 'validation/ssim': 0.7256678901633723, 'validation/loss': 0.28529393116141144, 'validation/num_examples': 3554, 'test/ssim': 0.7429206036416155, 'test/loss': 0.2865771879799637, 'test/num_examples': 3581, 'score': 6449.646962881088, 'total_duration': 7200.410577058792, 'accumulated_submission_time': 6449.646962881088, 'accumulated_eval_time': 669.4797310829163, 'accumulated_logging_time': 3.829319477081299, 'global_step': 24155, 'preemption_count': 0}), (24460, {'train/ssim': 0.747868537902832, 'train/loss': 0.2651862416948591, 'validation/ssim': 0.7249457724790729, 'validation/loss': 0.2853731188669457, 'validation/num_examples': 3554, 'test/ssim': 0.7422227473340198, 'test/loss': 0.28668449804523877, 'test/num_examples': 3581, 'score': 6529.101427555084, 'total_duration': 7286.71812748909, 'accumulated_submission_time': 6529.101427555084, 'accumulated_eval_time': 675.3239223957062, 'accumulated_logging_time': 3.8611960411071777, 'global_step': 24460, 'preemption_count': 0}), (24769, {'train/ssim': 0.7486545698983329, 'train/loss': 0.26505277838025776, 'validation/ssim': 0.7259173202729319, 'validation/loss': 0.2851830924167751, 'validation/num_examples': 3554, 'test/ssim': 0.743067728877234, 'test/loss': 0.2864458115531625, 'test/num_examples': 3581, 'score': 6608.810528516769, 'total_duration': 7373.320537567139, 'accumulated_submission_time': 6608.810528516769, 'accumulated_eval_time': 681.1995906829834, 'accumulated_logging_time': 3.8907217979431152, 'global_step': 24769, 'preemption_count': 0}), (25077, {'train/ssim': 0.749190194266183, 'train/loss': 0.2647651093346732, 'validation/ssim': 0.7261166720156865, 'validation/loss': 0.28522559720341517, 'validation/num_examples': 3554, 'test/ssim': 0.7432428747207483, 'test/loss': 0.28661158310571416, 'test/num_examples': 3581, 'score': 6688.482065677643, 'total_duration': 7459.979671955109, 'accumulated_submission_time': 6688.482065677643, 'accumulated_eval_time': 687.1564803123474, 'accumulated_logging_time': 3.9229960441589355, 'global_step': 25077, 'preemption_count': 0}), (25387, {'train/ssim': 0.748274530683245, 'train/loss': 0.2648131847381592, 'validation/ssim': 0.7252469983205543, 'validation/loss': 0.2851313138585045, 'validation/num_examples': 3554, 'test/ssim': 0.7425107937290562, 'test/loss': 0.28644206183677745, 'test/num_examples': 3581, 'score': 6768.029294490814, 'total_duration': 7546.173633813858, 'accumulated_submission_time': 6768.029294490814, 'accumulated_eval_time': 692.8036894798279, 'accumulated_logging_time': 3.9547364711761475, 'global_step': 25387, 'preemption_count': 0}), (25694, {'train/ssim': 0.7489894458225795, 'train/loss': 0.2646549769810268, 'validation/ssim': 0.7260206369583568, 'validation/loss': 0.28497742077039073, 'validation/num_examples': 3554, 'test/ssim': 0.7432368751745323, 'test/loss': 0.28628235800710344, 'test/num_examples': 3581, 'score': 6847.542138338089, 'total_duration': 7632.121198415756, 'accumulated_submission_time': 6847.542138338089, 'accumulated_eval_time': 698.2766332626343, 'accumulated_logging_time': 3.9824976921081543, 'global_step': 25694, 'preemption_count': 0}), (26001, {'train/ssim': 0.7492356300354004, 'train/loss': 0.26446800572531565, 'validation/ssim': 0.7262895076410383, 'validation/loss': 0.28476688898072594, 'validation/num_examples': 3554, 'test/ssim': 0.7435014006213349, 'test/loss': 0.28605604557909803, 'test/num_examples': 3581, 'score': 6927.24715256691, 'total_duration': 7718.248688220978, 'accumulated_submission_time': 6927.24715256691, 'accumulated_eval_time': 703.7418665885925, 'accumulated_logging_time': 4.0101752281188965, 'global_step': 26001, 'preemption_count': 0}), (26309, {'train/ssim': 0.7496715273175921, 'train/loss': 0.2641012328011649, 'validation/ssim': 0.7263163672314645, 'validation/loss': 0.28474016677950903, 'validation/num_examples': 3554, 'test/ssim': 0.7435410794383552, 'test/loss': 0.2860657266650377, 'test/num_examples': 3581, 'score': 7007.040945529938, 'total_duration': 7804.462784767151, 'accumulated_submission_time': 7007.040945529938, 'accumulated_eval_time': 709.2088804244995, 'accumulated_logging_time': 4.038287878036499, 'global_step': 26309, 'preemption_count': 0}), (26615, {'train/ssim': 0.7495265007019043, 'train/loss': 0.2640659638813564, 'validation/ssim': 0.7262778982528489, 'validation/loss': 0.28460322408509775, 'validation/num_examples': 3554, 'test/ssim': 0.7434862654024714, 'test/loss': 0.28591797079529985, 'test/num_examples': 3581, 'score': 7086.673682451248, 'total_duration': 7890.964240789413, 'accumulated_submission_time': 7086.673682451248, 'accumulated_eval_time': 715.1043362617493, 'accumulated_logging_time': 4.066661596298218, 'global_step': 26615, 'preemption_count': 0}), (26921, {'train/ssim': 0.7497388294764927, 'train/loss': 0.26397292954581125, 'validation/ssim': 0.7264971027363534, 'validation/loss': 0.28449350162778736, 'validation/num_examples': 3554, 'test/ssim': 0.74368936367722, 'test/loss': 0.2857849410844562, 'test/num_examples': 3581, 'score': 7166.205990314484, 'total_duration': 7977.336238622665, 'accumulated_submission_time': 7166.205990314484, 'accumulated_eval_time': 720.9261713027954, 'accumulated_logging_time': 4.098658800125122, 'global_step': 26921, 'preemption_count': 0}), (27142, {'train/ssim': 0.7498985699244908, 'train/loss': 0.2640864849090576, 'validation/ssim': 0.7267003013769696, 'validation/loss': 0.2845645661963193, 'validation/num_examples': 3554, 'test/ssim': 0.7438857124624756, 'test/loss': 0.2858898649665771, 'test/num_examples': 3581, 'score': 7223.4665603637695, 'total_duration': 8041.022883415222, 'accumulated_submission_time': 7223.4665603637695, 'accumulated_eval_time': 726.3992280960083, 'accumulated_logging_time': 4.127046585083008, 'global_step': 27142, 'preemption_count': 0})], 'global_step': 27142}
I1010 02:04:47.176546 139660997596992 submission_runner.py:552] Timing: 7223.4665603637695
I1010 02:04:47.176605 139660997596992 submission_runner.py:554] Total number of evals: 91
I1010 02:04:47.176651 139660997596992 submission_runner.py:555] ====================
I1010 02:04:47.176888 139660997596992 submission_runner.py:625] Final fastmri score: 7223.4665603637695
