torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nesterov.py --tuning_search_space=reference_algorithms/target_setting_algorithms/fastmri/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=fastmri_targets_check/nesterov_run_1 --overwrite=true --save_checkpoints=false --max_global_steps=27142 --torch_compile=true 2>&1 | tee -a /logs/fastmri_pytorch_10-10-2023-02-10-34.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-10-10 02:10:44.457838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 02:10:44.457848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1010 02:10:59.884478 139701982922560 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I1010 02:10:59.884567 140282542974784 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I1010 02:10:59.884570 139709814679360 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I1010 02:10:59.884654 139765329717056 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I1010 02:11:00.874315 140018763888448 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I1010 02:11:00.874422 140201452693312 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I1010 02:11:00.881235 140339418064704 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I1010 02:11:00.889388 140216147011392 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I1010 02:11:00.889795 140216147011392 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.892061 140339418064704 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.895339 140018763888448 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.895367 140201452693312 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.898556 140282542974784 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.898576 139701982922560 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.898540 139709814679360 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:00.898689 139765329717056 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 02:11:01.214052 140216147011392 logger_utils.py:76] Creating experiment directory at /experiment_runs/fastmri_targets_check/nesterov_run_1/fastmri_pytorch.
W1010 02:11:02.008873 140201452693312 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.008872 139765329717056 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.008886 140339418064704 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.009540 140018763888448 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.009530 140216147011392 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.009868 139701982922560 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.009831 139709814679360 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 02:11:02.012728 140282542974784 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1010 02:11:02.017412 140216147011392 submission_runner.py:507] Using RNG seed 3956072187
I1010 02:11:02.019388 140216147011392 submission_runner.py:516] --- Tuning run 1/1 ---
I1010 02:11:02.019548 140216147011392 submission_runner.py:521] Creating tuning directory at /experiment_runs/fastmri_targets_check/nesterov_run_1/fastmri_pytorch/trial_1.
I1010 02:11:02.019820 140216147011392 logger_utils.py:92] Saving hparams to /experiment_runs/fastmri_targets_check/nesterov_run_1/fastmri_pytorch/trial_1/hparams.json.
I1010 02:11:02.020788 140216147011392 submission_runner.py:191] Initializing dataset.
I1010 02:11:02.020945 140216147011392 submission_runner.py:198] Initializing model.
I1010 02:11:06.592951 140216147011392 submission_runner.py:229] Performing `torch.compile`.
I1010 02:11:06.868088 140216147011392 submission_runner.py:232] Initializing optimizer.
I1010 02:11:07.384804 140216147011392 submission_runner.py:239] Initializing metrics bundle.
I1010 02:11:07.385054 140216147011392 submission_runner.py:257] Initializing checkpoint and logger.
I1010 02:11:07.386012 140216147011392 submission_runner.py:277] Saving meta data to /experiment_runs/fastmri_targets_check/nesterov_run_1/fastmri_pytorch/trial_1/meta_data_0.json.
I1010 02:11:07.386361 140216147011392 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1010 02:11:07.386481 140216147011392 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1010 02:11:07.607315 140216147011392 logger_utils.py:220] Unable to record git information. Continuing without it.
I1010 02:11:07.811650 140216147011392 submission_runner.py:280] Saving flags to /experiment_runs/fastmri_targets_check/nesterov_run_1/fastmri_pytorch/trial_1/flags_0.json.
I1010 02:11:07.910540 140216147011392 submission_runner.py:290] Starting training loop.
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:07,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:08,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:08,269] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,271] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,271] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,272] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:08,272] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,272] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,272] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,272] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,272] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,272] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:08,272] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:08,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,274] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:08,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:08,276] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,276] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,276] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:08,277] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,277] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,277] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:08,278] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:08,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:08,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:13,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:13,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:13,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:13,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:13,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:13,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:13,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:11:55,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:55,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,968] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:55,968] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:55,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,970] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:55,970] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:55,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,971] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:55,972] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:55,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:11:55,973] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:11:56,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-10-10 02:11:56,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:56,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:56,417] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:56,417] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:56,418] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,667] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,713] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,717] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,717] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,718] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,743] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,744] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,765] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,766] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,789] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:11:59,835] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,838] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,838] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,838] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:11:59,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:11:59,868] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:11:59,868] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:11:59,869] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:00,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:00,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:00,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:00,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:00,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:00,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:00,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:01,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,400] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,474] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,474] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,489] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,489] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,506] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,508] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,509] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,509] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,522] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,522] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,522] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,524] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,526] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:01,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:01,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,606] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,607] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,607] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,609] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,609] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,609] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,652] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:01,697] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,700] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,700] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,700] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:01,709] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:01,709] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:01,709] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:01,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:02,004] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:02,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:02,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:02,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:02,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:02,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:03,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,118] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,127] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,150] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,206] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,211] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,229] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,229] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,229] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,229] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,241] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,241] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:03,248] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:03,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,260] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,260] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,260] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,329] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:03,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,344] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,344] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,345] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,345] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,345] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,346] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,346] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,347] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,347] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,372] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:03,374] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:03,374] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:03,375] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:03,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:03,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-10-10 02:12:04,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:04,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:04,677] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,690] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:04,799] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:04,800] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:04,800] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:04,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:04,803] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:04,803] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:04,804] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:04,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,877] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,888] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:04,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:04,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:04,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:04,914] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:04,915] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:04,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:04,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:04,996] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:04,997] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:04,997] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:05,005] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:05,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:05,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:05,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:05,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:05,019] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:05,020] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:05,020] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:05,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:05,031] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:05,031] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:05,032] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:05,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:05,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:05,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:05,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:05,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:05,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:05,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:06,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-10-10 02:12:06,051] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:06,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:06,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:06,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:06,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:06,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:06,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:06,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:06,837] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:06,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:06,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:06,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:06,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:06,913] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:06,916] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:06,920] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:06,921] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:06,921] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:06,922] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:06,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:06,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:06,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:06,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:06,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:07,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,001] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,002] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,002] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,002] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,002] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,003] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:07,033] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,039] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,039] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,040] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,092] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:07,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,115] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,116] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,116] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,116] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:07,177] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,202] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,202] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,203] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,259] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,259] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,260] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,345] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,430] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,459] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,491] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,494] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,494] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,494] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:07,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,529] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,545] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,547] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,548] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,550] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,551] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,573] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,576] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,577] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,612] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,639] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,640] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,659] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,660] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,660] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:07,684] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:07,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:07,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,751] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:07,752] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:07,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:07,798] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:07,798] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:07,798] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:07,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:07,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:07,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:07,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:07,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:08,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:08,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:08,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:08,058] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:08,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:08,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:08,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:08,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,385] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,414] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,421] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,489] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,489] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,490] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,508] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,509] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,509] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,510] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:08,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,523] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,523] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,523] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,523] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,523] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,524] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,544] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,623] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,623] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,623] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:08,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,658] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,680] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,680] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,680] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:08,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:08,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:08,737] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:08,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:08,737] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:08,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:08,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:08,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:08,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:08,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:08,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:08,940] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:08,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:08,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:08,959] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:08,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:09,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:09,060] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,066] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,066] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,066] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,088] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,090] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,090] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,090] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,091] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,092] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,093] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,093] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,093] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,094] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,094] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,095] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:09,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,170] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,181] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:09,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,227] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,227] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,227] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,300] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,300] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,301] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,552] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,570] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,673] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,742] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,742] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,743] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:09,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:09,819] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:09,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:09,846] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:09,877] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,888] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,912] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,913] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,913] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:09,923] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:09,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,930] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,938] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:09,954] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:09,954] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:09,954] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:09,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:09,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:09,999] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,002] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,004] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,004] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,005] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,080] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,080] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,081] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:10,088] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,170] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,184] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,396] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,405] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,432] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,439] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:10,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,487] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,524] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,524] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,525] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,537] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,537] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,538] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-10-10 02:12:10,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,562] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,562] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,562] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,567] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,569] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,569] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,570] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,650] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,651] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,651] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:10,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:10,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,695] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,734] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:10,778] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:10,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:10,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:10,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:10,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:10,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:10,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:11,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:11,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:11,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:11,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:11,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,245] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,263] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,281] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,305] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,331] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,345] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,345] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,346] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,357] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,357] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,373] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,382] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,382] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,383] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,512] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,512] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,512] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:11,528] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,603] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:11,604] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:11,605] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:11,605] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:11,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,745] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:11,839] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:11,853] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:11,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:11,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:11,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:11,986] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:11,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:11,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:11,998] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:12,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:12,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,031] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,031] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,032] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:12,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,050] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,050] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,051] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,084] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,084] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,084] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,106] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,107] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:12,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:12,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:12,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:12,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,179] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,180] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,180] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,197] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,197] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,198] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:12,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,356] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-10-10 02:12:12,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:12,534] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:12,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:12,627] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:12,628] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:12,630] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:12,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:12,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:13,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:13,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:13,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:13,108] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:13,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-10-10 02:12:13,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:13,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,620] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:13,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,667] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:13,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,682] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:13,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:13,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:13,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:13,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,770] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:13,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,774] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:13,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,806] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:13,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:13,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:13,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:13,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:13,974] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:14,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:14,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-10-10 02:12:14,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:14,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:14,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:14,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:14,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:14,778] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:15,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-10-10 02:12:16,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-10-10 02:12:16,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-10-10 02:12:17,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-10-10 02:12:17,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:17,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:17,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:17,221] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:17,222] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:17,222] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:17,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-10-10 02:12:19,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-10-10 02:12:19,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-10-10 02:12:19,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-10-10 02:12:19,702] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:19,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:19,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:19,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:19,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:19,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:20,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-10-10 02:12:20,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-10-10 02:12:20,284] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:20,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:20,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:20,419] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:20,419] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:20,420] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:20,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-10-10 02:12:20,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-10-10 02:12:21,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-10-10 02:12:21,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-10-10 02:12:21,405] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:21,461] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:21,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:21,490] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:21,490] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:21,495] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:21,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-10-10 02:12:21,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-10-10 02:12:21,975] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:22,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:22,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:22,081] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:22,082] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:22,082] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:22,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-10-10 02:12:22,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-10-10 02:12:22,787] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:22,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:22,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:22,863] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:22,864] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:22,864] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:23,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-10-10 02:12:23,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-10-10 02:12:23,287] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:23,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:23,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:23,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:23,411] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:23,412] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:23,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-10-10 02:12:24,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-10-10 02:12:24,120] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:24,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:24,190] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:24,191] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:24,191] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:24,192] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:24,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-10-10 02:12:24,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-10-10 02:12:24,672] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:24,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:12:24,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:12:24,855] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:12:24,855] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:12:24,855] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:12:25,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-10-10 02:12:26,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-10-10 02:12:26,417] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:12:26,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-10-10 02:12:26,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:26,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:26,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:26,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:26,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:26,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:26,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:27,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:27,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-10-10 02:12:27,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:27,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:27,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:27,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:27,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:27,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:27,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:28,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:28,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:28,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:28,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:28,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:28,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:28,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:28,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-10-10 02:12:28,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:28,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,004] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:29,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:29,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:29,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-10-10 02:12:29,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-10-10 02:12:29,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,761] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:29,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:30,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,498] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-10-10 02:12:30,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:30,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:30,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:30,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:30,893] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-10-10 02:12:30,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:30,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:30,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:30,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-10-10 02:12:31,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:31,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:31,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:31,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-10-10 02:12:31,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:31,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:31,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:31,108] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:31,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:31,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:31,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:31,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:31,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:31,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:31,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:31,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:31,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:31,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:31,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:31,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:31,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:31,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:31,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-10-10 02:12:31,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:31,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:32,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:32,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:32,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:32,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:32,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:32,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-10-10 02:12:32,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:32,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:32,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:32,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:32,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:32,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:32,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-10-10 02:12:32,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-10-10 02:12:32,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:32,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:32,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:32,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:32,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:32,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:32,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:32,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:32,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:32,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:32,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:32,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:32,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:32,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:32,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,855] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:32,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:32,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:32,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:32,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:32,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:32,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:33,018] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-10-10 02:12:33,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:33,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:33,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:33,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:33,104] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:33,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:33,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-10-10 02:12:33,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:33,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:33,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:33,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:33,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:33,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:33,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-10-10 02:12:33,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-10-10 02:12:33,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,411] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-10-10 02:12:33,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-10-10 02:12:33,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-10-10 02:12:34,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-10-10 02:12:34,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-10-10 02:12:34,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-10-10 02:12:34,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-10-10 02:12:35,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-10-10 02:12:35,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-10-10 02:12:35,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-10-10 02:12:35,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-10-10 02:12:35,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-10-10 02:12:35,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-10-10 02:12:35,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-10-10 02:12:35,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-10-10 02:12:35,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I1010 02:12:35.893272 140173874558720 logging_writer.py:48] [0] global_step=0, grad_norm=4.013832, loss=0.782463
I1010 02:12:35.905330 140216147011392 pytorch_submission_base.py:86] 0) loss = 0.782, grad_norm = 4.014
I1010 02:12:36.328180 140216147011392 spec.py:321] Evaluating on the training split.
[2023-10-10 02:13:31,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:31,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,586] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,586] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,587] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,587] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,587] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,587] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,588] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,588] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,588] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,589] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,589] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,589] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,590] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:31,629] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:31,629] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:31,630] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:31,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,788] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:31,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-10-10 02:13:32,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,383] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,386] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,625] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,642] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,646] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,647] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,647] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,657] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,657] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,663] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,664] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,664] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,675] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,675] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,676] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,680] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,682] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,683] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,683] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,695] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:32,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:32,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:32,752] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:32,752] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:32,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:32,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:32,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:32,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:32,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-10-10 02:13:32,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:32,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:32,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:32,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:33,090] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,158] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,158] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,158] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-10-10 02:13:33,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,471] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,487] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,489] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,507] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:33,560] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:33,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,675] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,692] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,692] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,763] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,763] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,764] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,780] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,783] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:33,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,796] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,805] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,807] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,808] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:33,831] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:33,831] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:33,831] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:33,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:33,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:33,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:33,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:33,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:34,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:34,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:34,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-10-10 02:13:34,433] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,530] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,623] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:34,660] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:34,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,691] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,691] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,753] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,771] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,800] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,800] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,815] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,817] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,817] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,818] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:34,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:34,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:34,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,967] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,967] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,968] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,980] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,980] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,982] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,982] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,983] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,983] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,983] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,983] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,986] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,986] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,986] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:34,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:34,992] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:34,992] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:34,993] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:35,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-10-10 02:13:35,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:35,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:35,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:35,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:35,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:35,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,553] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:35,691] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:35,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:35,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,761] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:35,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:35,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:35,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:35,869] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:35,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:35,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:35,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:35,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:35,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:35,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:35,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:35,900] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:35,900] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:35,900] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:35,902] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:35,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:35,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:35,968] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:35,968] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:35,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:35,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-10-10 02:13:35,977] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:36,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:36,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:36,034] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:36,077] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:36,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,079] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:36,079] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:36,079] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:36,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:36,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:36,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:36,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:36,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:36,152] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:36,152] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:36,152] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:36,153] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:36,153] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:36,153] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:36,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:36,176] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:36,178] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:36,178] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:36,179] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:36,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:36,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:36,232] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:36,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:36,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:36,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:36,277] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:36,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:36,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:36,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-10-10 02:13:36,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:36,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:36,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:36,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:36,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:36,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:37,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:37,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:37,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:37,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:37,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,124] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:37,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:37,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:37,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-10-10 02:13:37,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:37,252] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,254] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:37,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,287] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,289] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,312] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,314] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,314] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,314] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,381] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:37,407] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,410] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,434] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,435] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-10-10 02:13:37,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,490] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,491] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,511] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,512] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,512] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,540] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-10-10 02:13:37,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,584] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,605] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,605] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,605] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,645] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,647] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,648] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,648] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,709] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:37,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,737] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:37,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,776] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,822] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,823] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,823] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,824] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:37,829] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:37,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:37,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,843] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,843] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,844] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:37,891] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,891] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,891] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,906] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,906] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,907] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:37,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:37,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:37,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,990] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,991] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:37,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:37,998] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:37,999] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:37,999] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-10-10 02:13:38,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:38,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:38,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:38,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:38,108] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:38,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:38,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:38,137] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,184] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:38,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:38,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,286] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,323] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:38,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,351] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,369] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:38,448] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,458] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,526] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,528] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,528] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,528] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,543] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,566] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,567] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,567] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,568] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,584] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,588] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,588] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,589] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,628] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,689] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,690] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,690] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,690] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,690] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,763] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,771] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,772] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,772] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:38,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,800] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,800] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,814] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,827] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,918] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:38,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:38,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,950] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:38,950] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,950] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,950] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:38,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,990] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,991] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:38,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:38,995] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:38,996] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:38,996] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:39,044] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,087] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,090] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,090] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,090] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,111] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,112] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:39,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,178] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:39,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:39,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,202] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-10-10 02:13:39,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:39,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:39,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,357] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,388] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,392] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,395] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-10-10 02:13:39,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,414] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,414] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,491] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:39,510] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-10-10 02:13:39,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,549] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,566] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,566] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,635] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,635] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,635] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,640] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,640] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,641] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,649] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,684] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,711] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,712] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,750] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,750] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,750] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,809] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,842] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:39,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,867] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,876] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,889] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,889] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,890] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-10-10 02:13:39,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,936] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:39,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:39,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:39,951] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:39,951] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:39,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:39,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:39,991] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-10-10 02:13:40,021] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,035] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,036] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,066] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,112] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,112] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,113] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,114] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,162] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,175] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,187] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,221] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,222] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,222] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,283] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-10-10 02:13:40,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,345] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:40,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,430] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-10-10 02:13:40,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,511] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,511] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,515] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,515] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,516] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,559] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:40,621] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,621] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,622] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-10-10 02:13:40,641] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:40,642] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,664] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,664] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,665] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:40,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-10-10 02:13:40,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:40,744] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,752] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:40,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,824] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,824] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,824] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,825] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,826] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,826] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,826] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:40,848] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,851] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,853] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,854] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,854] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:40,884] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:40,884] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:40,884] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:40,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:40,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:40,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:40,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:40,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:40,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:40,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:41,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,009] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,010] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,010] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:41,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:41,063] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:41,087] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:41,113] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-10-10 02:13:41,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,164] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,165] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,165] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,202] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,203] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,203] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,246] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,247] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-10-10 02:13:41,272] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:41,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,422] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,422] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,425] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,431] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,453] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,455] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,456] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,456] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:41,472] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:41,473] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:41,473] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:41,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:41,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-10-10 02:13:41,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:41,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:41,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:41,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:41,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-10-10 02:13:41,929] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:42,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:42,012] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:42,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:42,023] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:42,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:42,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:42,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:42,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:42,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:42,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:42,205] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:42,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:42,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 02:13:42,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-10-10 02:13:42,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-10-10 02:13:42,469] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-10-10 02:13:42,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:42,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:42,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:42,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:42,715] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:42,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-10-10 02:13:43,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-10-10 02:13:43,181] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:43,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:43,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:43,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:43,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:43,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:43,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-10-10 02:13:43,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-10-10 02:13:43,687] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 02:13:43,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 02:13:44,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 02:13:44,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 02:13:44,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 02:13:44,063] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 02:13:44,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-10-10 02:13:45,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-10-10 02:13:45,107] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:14:16.759092 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:15:21.282384 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:16:26.658491 140216147011392 submission_runner.py:381] Time since start: 318.75s, 	Step: 1, 	{'train/ssim': 0.2844876561846052, 'train/loss': 0.8128893715994698, 'validation/ssim': 0.2764096064183842, 'validation/loss': 0.8214740570703785, 'validation/num_examples': 3554, 'test/ssim': 0.2970230797088802, 'test/loss': 0.8232938654094527, 'test/num_examples': 3581, 'score': 87.99606323242188, 'total_duration': 318.74846863746643, 'accumulated_submission_time': 87.99606323242188, 'accumulated_eval_time': 230.33046627044678, 'accumulated_logging_time': 0}
I1010 02:16:26.682878 140158296893184 logging_writer.py:48] [1] accumulated_eval_time=230.330466, accumulated_logging_time=0, accumulated_submission_time=87.996063, global_step=1, preemption_count=0, score=87.996063, test/loss=0.823294, test/num_examples=3581, test/ssim=0.297023, total_duration=318.748469, train/loss=0.812889, train/ssim=0.284488, validation/loss=0.821474, validation/num_examples=3554, validation/ssim=0.276410
I1010 02:16:27.178709 139765329717056 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178738 140018763888448 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178710 139701982922560 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178706 140201452693312 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178721 140282542974784 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178931 139709814679360 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178940 140339418064704 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.178923 140216147011392 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 02:16:27.324831 140158288500480 logging_writer.py:48] [1] global_step=1, grad_norm=3.666744, loss=0.795918
I1010 02:16:27.331193 140216147011392 pytorch_submission_base.py:86] 1) loss = 0.796, grad_norm = 3.667
I1010 02:16:27.405489 140158296893184 logging_writer.py:48] [2] global_step=2, grad_norm=3.618864, loss=0.857340
I1010 02:16:27.409128 140216147011392 pytorch_submission_base.py:86] 2) loss = 0.857, grad_norm = 3.619
I1010 02:16:27.473134 140158288500480 logging_writer.py:48] [3] global_step=3, grad_norm=3.982584, loss=0.796281
I1010 02:16:27.477248 140216147011392 pytorch_submission_base.py:86] 3) loss = 0.796, grad_norm = 3.983
I1010 02:16:27.545805 140158296893184 logging_writer.py:48] [4] global_step=4, grad_norm=3.370624, loss=0.855616
I1010 02:16:27.551911 140216147011392 pytorch_submission_base.py:86] 4) loss = 0.856, grad_norm = 3.371
I1010 02:16:27.633187 140158288500480 logging_writer.py:48] [5] global_step=5, grad_norm=3.881812, loss=0.780694
I1010 02:16:27.639506 140216147011392 pytorch_submission_base.py:86] 5) loss = 0.781, grad_norm = 3.882
I1010 02:16:27.721053 140158296893184 logging_writer.py:48] [6] global_step=6, grad_norm=3.591528, loss=0.803249
I1010 02:16:27.726111 140216147011392 pytorch_submission_base.py:86] 6) loss = 0.803, grad_norm = 3.592
I1010 02:16:27.805503 140158288500480 logging_writer.py:48] [7] global_step=7, grad_norm=3.225242, loss=0.783051
I1010 02:16:27.811394 140216147011392 pytorch_submission_base.py:86] 7) loss = 0.783, grad_norm = 3.225
I1010 02:16:27.883429 140158296893184 logging_writer.py:48] [8] global_step=8, grad_norm=3.561773, loss=0.810894
I1010 02:16:27.890813 140216147011392 pytorch_submission_base.py:86] 8) loss = 0.811, grad_norm = 3.562
I1010 02:16:27.962337 140158288500480 logging_writer.py:48] [9] global_step=9, grad_norm=3.214020, loss=0.717018
I1010 02:16:27.968685 140216147011392 pytorch_submission_base.py:86] 9) loss = 0.717, grad_norm = 3.214
I1010 02:16:28.042533 140158296893184 logging_writer.py:48] [10] global_step=10, grad_norm=3.154774, loss=0.736043
I1010 02:16:28.048422 140216147011392 pytorch_submission_base.py:86] 10) loss = 0.736, grad_norm = 3.155
I1010 02:16:28.125229 140158288500480 logging_writer.py:48] [11] global_step=11, grad_norm=2.968580, loss=0.667053
I1010 02:16:28.130630 140216147011392 pytorch_submission_base.py:86] 11) loss = 0.667, grad_norm = 2.969
I1010 02:16:28.207307 140158296893184 logging_writer.py:48] [12] global_step=12, grad_norm=2.829755, loss=0.671584
I1010 02:16:28.214297 140216147011392 pytorch_submission_base.py:86] 12) loss = 0.672, grad_norm = 2.830
I1010 02:16:28.285032 140158288500480 logging_writer.py:48] [13] global_step=13, grad_norm=2.250900, loss=0.629862
I1010 02:16:28.290059 140216147011392 pytorch_submission_base.py:86] 13) loss = 0.630, grad_norm = 2.251
I1010 02:16:28.366019 140158296893184 logging_writer.py:48] [14] global_step=14, grad_norm=2.060888, loss=0.643596
I1010 02:16:28.372520 140216147011392 pytorch_submission_base.py:86] 14) loss = 0.644, grad_norm = 2.061
I1010 02:16:28.640355 140158288500480 logging_writer.py:48] [15] global_step=15, grad_norm=1.809563, loss=0.601432
I1010 02:16:28.646494 140216147011392 pytorch_submission_base.py:86] 15) loss = 0.601, grad_norm = 1.810
I1010 02:16:28.934107 140158296893184 logging_writer.py:48] [16] global_step=16, grad_norm=1.378097, loss=0.545837
I1010 02:16:28.938019 140216147011392 pytorch_submission_base.py:86] 16) loss = 0.546, grad_norm = 1.378
I1010 02:16:29.207026 140158288500480 logging_writer.py:48] [17] global_step=17, grad_norm=1.183648, loss=0.586847
I1010 02:16:29.212743 140216147011392 pytorch_submission_base.py:86] 17) loss = 0.587, grad_norm = 1.184
I1010 02:16:29.524247 140158296893184 logging_writer.py:48] [18] global_step=18, grad_norm=1.046283, loss=0.488406
I1010 02:16:29.530668 140216147011392 pytorch_submission_base.py:86] 18) loss = 0.488, grad_norm = 1.046
I1010 02:16:29.849463 140158288500480 logging_writer.py:48] [19] global_step=19, grad_norm=0.952617, loss=0.528460
I1010 02:16:29.855335 140216147011392 pytorch_submission_base.py:86] 19) loss = 0.528, grad_norm = 0.953
I1010 02:16:30.066760 140158296893184 logging_writer.py:48] [20] global_step=20, grad_norm=1.033104, loss=0.507926
I1010 02:16:30.073218 140216147011392 pytorch_submission_base.py:86] 20) loss = 0.508, grad_norm = 1.033
I1010 02:16:30.336591 140158288500480 logging_writer.py:48] [21] global_step=21, grad_norm=1.063933, loss=0.512889
I1010 02:16:30.341032 140216147011392 pytorch_submission_base.py:86] 21) loss = 0.513, grad_norm = 1.064
I1010 02:16:30.613471 140158296893184 logging_writer.py:48] [22] global_step=22, grad_norm=1.224703, loss=0.518209
I1010 02:16:30.619181 140216147011392 pytorch_submission_base.py:86] 22) loss = 0.518, grad_norm = 1.225
I1010 02:16:30.827359 140158288500480 logging_writer.py:48] [23] global_step=23, grad_norm=1.283334, loss=0.530513
I1010 02:16:30.834192 140216147011392 pytorch_submission_base.py:86] 23) loss = 0.531, grad_norm = 1.283
I1010 02:16:31.125137 140158296893184 logging_writer.py:48] [24] global_step=24, grad_norm=1.420179, loss=0.534765
I1010 02:16:31.130059 140216147011392 pytorch_submission_base.py:86] 24) loss = 0.535, grad_norm = 1.420
I1010 02:16:31.344324 140158288500480 logging_writer.py:48] [25] global_step=25, grad_norm=1.484938, loss=0.562819
I1010 02:16:31.349789 140216147011392 pytorch_submission_base.py:86] 25) loss = 0.563, grad_norm = 1.485
I1010 02:16:31.623227 140158296893184 logging_writer.py:48] [26] global_step=26, grad_norm=1.630722, loss=0.503493
I1010 02:16:31.627419 140216147011392 pytorch_submission_base.py:86] 26) loss = 0.503, grad_norm = 1.631
I1010 02:16:31.878288 140158288500480 logging_writer.py:48] [27] global_step=27, grad_norm=1.777296, loss=0.506933
I1010 02:16:31.882045 140216147011392 pytorch_submission_base.py:86] 27) loss = 0.507, grad_norm = 1.777
I1010 02:16:32.145407 140158296893184 logging_writer.py:48] [28] global_step=28, grad_norm=1.865296, loss=0.516776
I1010 02:16:32.149373 140216147011392 pytorch_submission_base.py:86] 28) loss = 0.517, grad_norm = 1.865
I1010 02:16:32.393509 140158288500480 logging_writer.py:48] [29] global_step=29, grad_norm=1.800892, loss=0.532845
I1010 02:16:32.397352 140216147011392 pytorch_submission_base.py:86] 29) loss = 0.533, grad_norm = 1.801
I1010 02:16:32.704952 140158296893184 logging_writer.py:48] [30] global_step=30, grad_norm=1.880210, loss=0.511685
I1010 02:16:32.708672 140216147011392 pytorch_submission_base.py:86] 30) loss = 0.512, grad_norm = 1.880
I1010 02:16:32.918693 140158288500480 logging_writer.py:48] [31] global_step=31, grad_norm=1.784843, loss=0.554244
I1010 02:16:32.924132 140216147011392 pytorch_submission_base.py:86] 31) loss = 0.554, grad_norm = 1.785
I1010 02:16:33.197518 140158296893184 logging_writer.py:48] [32] global_step=32, grad_norm=1.822100, loss=0.571260
I1010 02:16:33.203344 140216147011392 pytorch_submission_base.py:86] 32) loss = 0.571, grad_norm = 1.822
I1010 02:16:33.509726 140158288500480 logging_writer.py:48] [33] global_step=33, grad_norm=1.851807, loss=0.477915
I1010 02:16:33.513149 140216147011392 pytorch_submission_base.py:86] 33) loss = 0.478, grad_norm = 1.852
I1010 02:16:33.783186 140158296893184 logging_writer.py:48] [34] global_step=34, grad_norm=1.612675, loss=0.539741
I1010 02:16:33.787214 140216147011392 pytorch_submission_base.py:86] 34) loss = 0.540, grad_norm = 1.613
I1010 02:16:34.011769 140158288500480 logging_writer.py:48] [35] global_step=35, grad_norm=1.522051, loss=0.487242
I1010 02:16:34.018638 140216147011392 pytorch_submission_base.py:86] 35) loss = 0.487, grad_norm = 1.522
I1010 02:16:34.302513 140158296893184 logging_writer.py:48] [36] global_step=36, grad_norm=1.234183, loss=0.499433
I1010 02:16:34.309081 140216147011392 pytorch_submission_base.py:86] 36) loss = 0.499, grad_norm = 1.234
I1010 02:16:34.574963 140158288500480 logging_writer.py:48] [37] global_step=37, grad_norm=1.033281, loss=0.466556
I1010 02:16:34.581550 140216147011392 pytorch_submission_base.py:86] 37) loss = 0.467, grad_norm = 1.033
I1010 02:16:34.832016 140158296893184 logging_writer.py:48] [38] global_step=38, grad_norm=0.737232, loss=0.425170
I1010 02:16:34.838357 140216147011392 pytorch_submission_base.py:86] 38) loss = 0.425, grad_norm = 0.737
I1010 02:16:35.145676 140158288500480 logging_writer.py:48] [39] global_step=39, grad_norm=0.616674, loss=0.401028
I1010 02:16:35.151529 140216147011392 pytorch_submission_base.py:86] 39) loss = 0.401, grad_norm = 0.617
I1010 02:16:35.414779 140158296893184 logging_writer.py:48] [40] global_step=40, grad_norm=0.507198, loss=0.436540
I1010 02:16:35.420376 140216147011392 pytorch_submission_base.py:86] 40) loss = 0.437, grad_norm = 0.507
I1010 02:16:35.648982 140158288500480 logging_writer.py:48] [41] global_step=41, grad_norm=0.620485, loss=0.423531
I1010 02:16:35.653088 140216147011392 pytorch_submission_base.py:86] 41) loss = 0.424, grad_norm = 0.620
I1010 02:16:35.889425 140158296893184 logging_writer.py:48] [42] global_step=42, grad_norm=0.720112, loss=0.450642
I1010 02:16:35.894303 140216147011392 pytorch_submission_base.py:86] 42) loss = 0.451, grad_norm = 0.720
I1010 02:16:36.188544 140158288500480 logging_writer.py:48] [43] global_step=43, grad_norm=1.214965, loss=0.389685
I1010 02:16:36.195899 140216147011392 pytorch_submission_base.py:86] 43) loss = 0.390, grad_norm = 1.215
I1010 02:16:36.475172 140158296893184 logging_writer.py:48] [44] global_step=44, grad_norm=1.066624, loss=0.442353
I1010 02:16:36.479101 140216147011392 pytorch_submission_base.py:86] 44) loss = 0.442, grad_norm = 1.067
I1010 02:16:36.789497 140158288500480 logging_writer.py:48] [45] global_step=45, grad_norm=0.981671, loss=0.426665
I1010 02:16:36.794764 140216147011392 pytorch_submission_base.py:86] 45) loss = 0.427, grad_norm = 0.982
I1010 02:16:37.054728 140158296893184 logging_writer.py:48] [46] global_step=46, grad_norm=1.204031, loss=0.448983
I1010 02:16:37.058661 140216147011392 pytorch_submission_base.py:86] 46) loss = 0.449, grad_norm = 1.204
I1010 02:16:37.332548 140158288500480 logging_writer.py:48] [47] global_step=47, grad_norm=1.091593, loss=0.540081
I1010 02:16:37.336423 140216147011392 pytorch_submission_base.py:86] 47) loss = 0.540, grad_norm = 1.092
I1010 02:16:37.588888 140158296893184 logging_writer.py:48] [48] global_step=48, grad_norm=1.069310, loss=0.511887
I1010 02:16:37.592565 140216147011392 pytorch_submission_base.py:86] 48) loss = 0.512, grad_norm = 1.069
I1010 02:16:37.915781 140158288500480 logging_writer.py:48] [49] global_step=49, grad_norm=1.514848, loss=0.464810
I1010 02:16:37.919798 140216147011392 pytorch_submission_base.py:86] 49) loss = 0.465, grad_norm = 1.515
I1010 02:16:38.216179 140158296893184 logging_writer.py:48] [50] global_step=50, grad_norm=1.358704, loss=0.441028
I1010 02:16:38.219774 140216147011392 pytorch_submission_base.py:86] 50) loss = 0.441, grad_norm = 1.359
I1010 02:16:38.453932 140158288500480 logging_writer.py:48] [51] global_step=51, grad_norm=1.205070, loss=0.439164
I1010 02:16:38.460009 140216147011392 pytorch_submission_base.py:86] 51) loss = 0.439, grad_norm = 1.205
I1010 02:16:38.675352 140158296893184 logging_writer.py:48] [52] global_step=52, grad_norm=1.267288, loss=0.477792
I1010 02:16:38.680744 140216147011392 pytorch_submission_base.py:86] 52) loss = 0.478, grad_norm = 1.267
I1010 02:16:39.008683 140158288500480 logging_writer.py:48] [53] global_step=53, grad_norm=1.060751, loss=0.541393
I1010 02:16:39.014739 140216147011392 pytorch_submission_base.py:86] 53) loss = 0.541, grad_norm = 1.061
I1010 02:16:39.267923 140158296893184 logging_writer.py:48] [54] global_step=54, grad_norm=1.283716, loss=0.399498
I1010 02:16:39.274001 140216147011392 pytorch_submission_base.py:86] 54) loss = 0.399, grad_norm = 1.284
I1010 02:16:39.563206 140158288500480 logging_writer.py:48] [55] global_step=55, grad_norm=1.074550, loss=0.438316
I1010 02:16:39.569428 140216147011392 pytorch_submission_base.py:86] 55) loss = 0.438, grad_norm = 1.075
I1010 02:16:39.841316 140158296893184 logging_writer.py:48] [56] global_step=56, grad_norm=0.832688, loss=0.470615
I1010 02:16:39.846338 140216147011392 pytorch_submission_base.py:86] 56) loss = 0.471, grad_norm = 0.833
I1010 02:16:40.046133 140158288500480 logging_writer.py:48] [57] global_step=57, grad_norm=0.768984, loss=0.461047
I1010 02:16:40.051878 140216147011392 pytorch_submission_base.py:86] 57) loss = 0.461, grad_norm = 0.769
I1010 02:16:40.339046 140158296893184 logging_writer.py:48] [58] global_step=58, grad_norm=0.897751, loss=0.390482
I1010 02:16:40.345317 140216147011392 pytorch_submission_base.py:86] 58) loss = 0.390, grad_norm = 0.898
I1010 02:16:40.634453 140158288500480 logging_writer.py:48] [59] global_step=59, grad_norm=0.789342, loss=0.427277
I1010 02:16:40.640377 140216147011392 pytorch_submission_base.py:86] 59) loss = 0.427, grad_norm = 0.789
I1010 02:16:40.917964 140158296893184 logging_writer.py:48] [60] global_step=60, grad_norm=0.801250, loss=0.419096
I1010 02:16:40.922188 140216147011392 pytorch_submission_base.py:86] 60) loss = 0.419, grad_norm = 0.801
I1010 02:16:41.196288 140158288500480 logging_writer.py:48] [61] global_step=61, grad_norm=0.826309, loss=0.373490
I1010 02:16:41.202190 140216147011392 pytorch_submission_base.py:86] 61) loss = 0.373, grad_norm = 0.826
I1010 02:16:41.448189 140158296893184 logging_writer.py:48] [62] global_step=62, grad_norm=0.806955, loss=0.375507
I1010 02:16:41.451878 140216147011392 pytorch_submission_base.py:86] 62) loss = 0.376, grad_norm = 0.807
I1010 02:16:41.674387 140158288500480 logging_writer.py:48] [63] global_step=63, grad_norm=0.727792, loss=0.356494
I1010 02:16:41.678062 140216147011392 pytorch_submission_base.py:86] 63) loss = 0.356, grad_norm = 0.728
I1010 02:16:41.941551 140158296893184 logging_writer.py:48] [64] global_step=64, grad_norm=0.781539, loss=0.353262
I1010 02:16:41.944947 140216147011392 pytorch_submission_base.py:86] 64) loss = 0.353, grad_norm = 0.782
I1010 02:16:42.202319 140158288500480 logging_writer.py:48] [65] global_step=65, grad_norm=0.730604, loss=0.349434
I1010 02:16:42.206876 140216147011392 pytorch_submission_base.py:86] 65) loss = 0.349, grad_norm = 0.731
I1010 02:16:42.472254 140158296893184 logging_writer.py:48] [66] global_step=66, grad_norm=0.595014, loss=0.405050
I1010 02:16:42.478034 140216147011392 pytorch_submission_base.py:86] 66) loss = 0.405, grad_norm = 0.595
I1010 02:16:42.767342 140158288500480 logging_writer.py:48] [67] global_step=67, grad_norm=0.646263, loss=0.314399
I1010 02:16:42.772562 140216147011392 pytorch_submission_base.py:86] 67) loss = 0.314, grad_norm = 0.646
I1010 02:16:43.034425 140158296893184 logging_writer.py:48] [68] global_step=68, grad_norm=0.536142, loss=0.350028
I1010 02:16:43.039695 140216147011392 pytorch_submission_base.py:86] 68) loss = 0.350, grad_norm = 0.536
I1010 02:16:43.329646 140158288500480 logging_writer.py:48] [69] global_step=69, grad_norm=0.411638, loss=0.407633
I1010 02:16:43.335040 140216147011392 pytorch_submission_base.py:86] 69) loss = 0.408, grad_norm = 0.412
I1010 02:16:43.567730 140158296893184 logging_writer.py:48] [70] global_step=70, grad_norm=0.359968, loss=0.445402
I1010 02:16:43.572059 140216147011392 pytorch_submission_base.py:86] 70) loss = 0.445, grad_norm = 0.360
I1010 02:16:43.861427 140158288500480 logging_writer.py:48] [71] global_step=71, grad_norm=0.394680, loss=0.302032
I1010 02:16:43.865346 140216147011392 pytorch_submission_base.py:86] 71) loss = 0.302, grad_norm = 0.395
I1010 02:16:44.097006 140158296893184 logging_writer.py:48] [72] global_step=72, grad_norm=0.362771, loss=0.319184
I1010 02:16:44.101319 140216147011392 pytorch_submission_base.py:86] 72) loss = 0.319, grad_norm = 0.363
I1010 02:16:44.372292 140158288500480 logging_writer.py:48] [73] global_step=73, grad_norm=0.366253, loss=0.349826
I1010 02:16:44.380754 140216147011392 pytorch_submission_base.py:86] 73) loss = 0.350, grad_norm = 0.366
I1010 02:16:44.639066 140158296893184 logging_writer.py:48] [74] global_step=74, grad_norm=0.477078, loss=0.337086
I1010 02:16:44.645065 140216147011392 pytorch_submission_base.py:86] 74) loss = 0.337, grad_norm = 0.477
I1010 02:16:44.894687 140158288500480 logging_writer.py:48] [75] global_step=75, grad_norm=0.517698, loss=0.404113
I1010 02:16:44.901363 140216147011392 pytorch_submission_base.py:86] 75) loss = 0.404, grad_norm = 0.518
I1010 02:16:45.166971 140158296893184 logging_writer.py:48] [76] global_step=76, grad_norm=0.592697, loss=0.411652
I1010 02:16:45.172026 140216147011392 pytorch_submission_base.py:86] 76) loss = 0.412, grad_norm = 0.593
I1010 02:16:45.435212 140158288500480 logging_writer.py:48] [77] global_step=77, grad_norm=0.678548, loss=0.370697
I1010 02:16:45.441599 140216147011392 pytorch_submission_base.py:86] 77) loss = 0.371, grad_norm = 0.679
I1010 02:16:45.665273 140158296893184 logging_writer.py:48] [78] global_step=78, grad_norm=0.668442, loss=0.354229
I1010 02:16:45.672483 140216147011392 pytorch_submission_base.py:86] 78) loss = 0.354, grad_norm = 0.668
I1010 02:16:45.998656 140158288500480 logging_writer.py:48] [79] global_step=79, grad_norm=0.711984, loss=0.323498
I1010 02:16:46.004992 140216147011392 pytorch_submission_base.py:86] 79) loss = 0.323, grad_norm = 0.712
I1010 02:16:46.251041 140158296893184 logging_writer.py:48] [80] global_step=80, grad_norm=0.685300, loss=0.480822
I1010 02:16:46.255102 140216147011392 pytorch_submission_base.py:86] 80) loss = 0.481, grad_norm = 0.685
I1010 02:16:46.501448 140158288500480 logging_writer.py:48] [81] global_step=81, grad_norm=0.804264, loss=0.393698
I1010 02:16:46.505362 140216147011392 pytorch_submission_base.py:86] 81) loss = 0.394, grad_norm = 0.804
I1010 02:16:46.784303 140158296893184 logging_writer.py:48] [82] global_step=82, grad_norm=0.818648, loss=0.273170
I1010 02:16:46.789271 140216147011392 pytorch_submission_base.py:86] 82) loss = 0.273, grad_norm = 0.819
I1010 02:16:47.044716 140158288500480 logging_writer.py:48] [83] global_step=83, grad_norm=0.752976, loss=0.278322
I1010 02:16:47.050410 140216147011392 pytorch_submission_base.py:86] 83) loss = 0.278, grad_norm = 0.753
I1010 02:16:47.319160 140158296893184 logging_writer.py:48] [84] global_step=84, grad_norm=0.680049, loss=0.356802
I1010 02:16:47.324302 140216147011392 pytorch_submission_base.py:86] 84) loss = 0.357, grad_norm = 0.680
I1010 02:16:47.583532 140158288500480 logging_writer.py:48] [85] global_step=85, grad_norm=0.630495, loss=0.303183
I1010 02:16:47.590174 140216147011392 pytorch_submission_base.py:86] 85) loss = 0.303, grad_norm = 0.630
I1010 02:16:47.873596 140158296893184 logging_writer.py:48] [86] global_step=86, grad_norm=0.651630, loss=0.365522
I1010 02:16:47.877434 140216147011392 pytorch_submission_base.py:86] 86) loss = 0.366, grad_norm = 0.652
I1010 02:16:48.182171 140158288500480 logging_writer.py:48] [87] global_step=87, grad_norm=0.531374, loss=0.355096
I1010 02:16:48.187474 140216147011392 pytorch_submission_base.py:86] 87) loss = 0.355, grad_norm = 0.531
I1010 02:16:48.438440 140158296893184 logging_writer.py:48] [88] global_step=88, grad_norm=0.428029, loss=0.416122
I1010 02:16:48.442028 140216147011392 pytorch_submission_base.py:86] 88) loss = 0.416, grad_norm = 0.428
I1010 02:16:48.698225 140158288500480 logging_writer.py:48] [89] global_step=89, grad_norm=0.396416, loss=0.329019
I1010 02:16:48.701825 140216147011392 pytorch_submission_base.py:86] 89) loss = 0.329, grad_norm = 0.396
I1010 02:16:49.003566 140158296893184 logging_writer.py:48] [90] global_step=90, grad_norm=0.334390, loss=0.303931
I1010 02:16:49.009567 140216147011392 pytorch_submission_base.py:86] 90) loss = 0.304, grad_norm = 0.334
I1010 02:16:49.266760 140158288500480 logging_writer.py:48] [91] global_step=91, grad_norm=0.341110, loss=0.367778
I1010 02:16:49.274544 140216147011392 pytorch_submission_base.py:86] 91) loss = 0.368, grad_norm = 0.341
I1010 02:16:49.467749 140158296893184 logging_writer.py:48] [92] global_step=92, grad_norm=0.328462, loss=0.255181
I1010 02:16:49.475977 140216147011392 pytorch_submission_base.py:86] 92) loss = 0.255, grad_norm = 0.328
I1010 02:16:49.797829 140158288500480 logging_writer.py:48] [93] global_step=93, grad_norm=0.274704, loss=0.322882
I1010 02:16:49.804120 140216147011392 pytorch_submission_base.py:86] 93) loss = 0.323, grad_norm = 0.275
I1010 02:16:50.012132 140158296893184 logging_writer.py:48] [94] global_step=94, grad_norm=0.355307, loss=0.250524
I1010 02:16:50.016306 140216147011392 pytorch_submission_base.py:86] 94) loss = 0.251, grad_norm = 0.355
I1010 02:16:50.287761 140158288500480 logging_writer.py:48] [95] global_step=95, grad_norm=0.410452, loss=0.261775
I1010 02:16:50.291242 140216147011392 pytorch_submission_base.py:86] 95) loss = 0.262, grad_norm = 0.410
I1010 02:16:50.568809 140158296893184 logging_writer.py:48] [96] global_step=96, grad_norm=0.424933, loss=0.250986
I1010 02:16:50.572654 140216147011392 pytorch_submission_base.py:86] 96) loss = 0.251, grad_norm = 0.425
I1010 02:16:50.831033 140158288500480 logging_writer.py:48] [97] global_step=97, grad_norm=0.478528, loss=0.349585
I1010 02:16:50.838940 140216147011392 pytorch_submission_base.py:86] 97) loss = 0.350, grad_norm = 0.479
I1010 02:16:51.108420 140158296893184 logging_writer.py:48] [98] global_step=98, grad_norm=0.475339, loss=0.351342
I1010 02:16:51.113693 140216147011392 pytorch_submission_base.py:86] 98) loss = 0.351, grad_norm = 0.475
I1010 02:16:51.375299 140158288500480 logging_writer.py:48] [99] global_step=99, grad_norm=0.478017, loss=0.328188
I1010 02:16:51.380965 140216147011392 pytorch_submission_base.py:86] 99) loss = 0.328, grad_norm = 0.478
I1010 02:16:51.644016 140158296893184 logging_writer.py:48] [100] global_step=100, grad_norm=0.514700, loss=0.310775
I1010 02:16:51.649348 140216147011392 pytorch_submission_base.py:86] 100) loss = 0.311, grad_norm = 0.515
I1010 02:17:47.184044 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:17:49.218806 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:17:51.298901 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:17:53.376789 140216147011392 submission_runner.py:381] Time since start: 405.47s, 	Step: 303, 	{'train/ssim': 0.6871239798409599, 'train/loss': 0.3260907105037144, 'validation/ssim': 0.6650636551684721, 'validation/loss': 0.3454317813115504, 'validation/num_examples': 3554, 'test/ssim': 0.6836658809079168, 'test/loss': 0.34659116010498114, 'test/num_examples': 3581, 'score': 167.40180373191833, 'total_duration': 405.46679639816284, 'accumulated_submission_time': 167.40180373191833, 'accumulated_eval_time': 236.52397465705872, 'accumulated_logging_time': 0.03457927703857422}
I1010 02:17:53.399986 140158288500480 logging_writer.py:48] [303] accumulated_eval_time=236.523975, accumulated_logging_time=0.034579, accumulated_submission_time=167.401804, global_step=303, preemption_count=0, score=167.401804, test/loss=0.346591, test/num_examples=3581, test/ssim=0.683666, total_duration=405.466796, train/loss=0.326091, train/ssim=0.687124, validation/loss=0.345432, validation/num_examples=3554, validation/ssim=0.665064
I1010 02:19:09.161153 140158296893184 logging_writer.py:48] [500] global_step=500, grad_norm=0.068500, loss=0.243743
I1010 02:19:09.166103 140216147011392 pytorch_submission_base.py:86] 500) loss = 0.244, grad_norm = 0.068
I1010 02:19:13.986503 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:19:15.921368 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:19:17.981434 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:19:20.003833 140216147011392 submission_runner.py:381] Time since start: 492.09s, 	Step: 511, 	{'train/ssim': 0.7030360358101981, 'train/loss': 0.309774569102696, 'validation/ssim': 0.6823016044861424, 'validation/loss': 0.3276162477578081, 'validation/num_examples': 3554, 'test/ssim': 0.7008656936129922, 'test/loss': 0.3290475665840547, 'test/num_examples': 3581, 'score': 247.0248794555664, 'total_duration': 492.0938313007355, 'accumulated_submission_time': 247.0248794555664, 'accumulated_eval_time': 242.54185032844543, 'accumulated_logging_time': 0.07579469680786133}
I1010 02:19:20.020951 140158288500480 logging_writer.py:48] [511] accumulated_eval_time=242.541850, accumulated_logging_time=0.075795, accumulated_submission_time=247.024879, global_step=511, preemption_count=0, score=247.024879, test/loss=0.329048, test/num_examples=3581, test/ssim=0.700866, total_duration=492.093831, train/loss=0.309775, train/ssim=0.703036, validation/loss=0.327616, validation/num_examples=3554, validation/ssim=0.682302
I1010 02:20:40.936515 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:20:42.849399 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:20:44.865886 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:20:46.882636 140216147011392 submission_runner.py:381] Time since start: 578.97s, 	Step: 715, 	{'train/ssim': 0.7100772857666016, 'train/loss': 0.30013860974993023, 'validation/ssim': 0.6900099634654615, 'validation/loss': 0.31704710196697383, 'validation/num_examples': 3554, 'test/ssim': 0.7081830946968375, 'test/loss': 0.31867980530927115, 'test/num_examples': 3581, 'score': 326.9715027809143, 'total_duration': 578.9726641178131, 'accumulated_submission_time': 326.9715027809143, 'accumulated_eval_time': 248.48882460594177, 'accumulated_logging_time': 0.10609912872314453}
I1010 02:20:46.901384 140158296893184 logging_writer.py:48] [715] accumulated_eval_time=248.488825, accumulated_logging_time=0.106099, accumulated_submission_time=326.971503, global_step=715, preemption_count=0, score=326.971503, test/loss=0.318680, test/num_examples=3581, test/ssim=0.708183, total_duration=578.972664, train/loss=0.300139, train/ssim=0.710077, validation/loss=0.317047, validation/num_examples=3554, validation/ssim=0.690010
I1010 02:22:07.552610 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:22:09.619935 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:22:12.376151 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:22:14.402821 140216147011392 submission_runner.py:381] Time since start: 666.49s, 	Step: 925, 	{'train/ssim': 0.7171282087053571, 'train/loss': 0.29336401394435335, 'validation/ssim': 0.6966693558314575, 'validation/loss': 0.31036036914831877, 'validation/num_examples': 3554, 'test/ssim': 0.7143259482555502, 'test/loss': 0.312050852153728, 'test/num_examples': 3581, 'score': 406.6623592376709, 'total_duration': 666.492814540863, 'accumulated_submission_time': 406.6623592376709, 'accumulated_eval_time': 255.3394331932068, 'accumulated_logging_time': 0.13835358619689941}
I1010 02:22:14.422543 140158288500480 logging_writer.py:48] [925] accumulated_eval_time=255.339433, accumulated_logging_time=0.138354, accumulated_submission_time=406.662359, global_step=925, preemption_count=0, score=406.662359, test/loss=0.312051, test/num_examples=3581, test/ssim=0.714326, total_duration=666.492815, train/loss=0.293364, train/ssim=0.717128, validation/loss=0.310360, validation/num_examples=3554, validation/ssim=0.696669
I1010 02:22:33.953606 140158296893184 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.149534, loss=0.301333
I1010 02:22:33.957499 140216147011392 pytorch_submission_base.py:86] 1000) loss = 0.301, grad_norm = 0.150
I1010 02:23:34.844767 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:23:36.804495 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:23:38.786960 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:23:40.724994 140216147011392 submission_runner.py:381] Time since start: 752.82s, 	Step: 1225, 	{'train/ssim': 0.7261755807059151, 'train/loss': 0.28606932503836496, 'validation/ssim': 0.7053904106728335, 'validation/loss': 0.30326352953977914, 'validation/num_examples': 3554, 'test/ssim': 0.7225815283789444, 'test/loss': 0.305056267562308, 'test/num_examples': 3581, 'score': 486.0998725891113, 'total_duration': 752.815033197403, 'accumulated_submission_time': 486.0998725891113, 'accumulated_eval_time': 261.2199556827545, 'accumulated_logging_time': 0.1702287197113037}
I1010 02:23:40.741917 140158288500480 logging_writer.py:48] [1225] accumulated_eval_time=261.219956, accumulated_logging_time=0.170229, accumulated_submission_time=486.099873, global_step=1225, preemption_count=0, score=486.099873, test/loss=0.305056, test/num_examples=3581, test/ssim=0.722582, total_duration=752.815033, train/loss=0.286069, train/ssim=0.726176, validation/loss=0.303264, validation/num_examples=3554, validation/ssim=0.705390
I1010 02:24:51.968802 140158296893184 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.551619, loss=0.339150
I1010 02:24:51.972470 140216147011392 pytorch_submission_base.py:86] 1500) loss = 0.339, grad_norm = 0.552
I1010 02:25:01.381141 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:25:03.389650 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:25:05.383361 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:25:07.333040 140216147011392 submission_runner.py:381] Time since start: 839.42s, 	Step: 1535, 	{'train/ssim': 0.7169576372419085, 'train/loss': 0.28833866119384766, 'validation/ssim': 0.6994413205191333, 'validation/loss': 0.30437538606367825, 'validation/num_examples': 3554, 'test/ssim': 0.7161155174444988, 'test/loss': 0.3060349094504852, 'test/num_examples': 3581, 'score': 565.7556653022766, 'total_duration': 839.4230749607086, 'accumulated_submission_time': 565.7556653022766, 'accumulated_eval_time': 267.1721751689911, 'accumulated_logging_time': 0.19649291038513184}
I1010 02:25:07.348371 140158288500480 logging_writer.py:48] [1535] accumulated_eval_time=267.172175, accumulated_logging_time=0.196493, accumulated_submission_time=565.755665, global_step=1535, preemption_count=0, score=565.755665, test/loss=0.306035, test/num_examples=3581, test/ssim=0.716116, total_duration=839.423075, train/loss=0.288339, train/ssim=0.716958, validation/loss=0.304375, validation/num_examples=3554, validation/ssim=0.699441
I1010 02:26:28.027634 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:26:29.972608 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:26:31.946742 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:26:33.896720 140216147011392 submission_runner.py:381] Time since start: 925.99s, 	Step: 1845, 	{'train/ssim': 0.7319788251604352, 'train/loss': 0.28032668999263216, 'validation/ssim': 0.7118229047596019, 'validation/loss': 0.2970356079857203, 'validation/num_examples': 3554, 'test/ssim': 0.7287340630235968, 'test/loss': 0.29878882115461813, 'test/num_examples': 3581, 'score': 645.4322462081909, 'total_duration': 925.9867289066315, 'accumulated_submission_time': 645.4322462081909, 'accumulated_eval_time': 273.041535615921, 'accumulated_logging_time': 0.22025728225708008}
I1010 02:26:33.911831 140158296893184 logging_writer.py:48] [1845] accumulated_eval_time=273.041536, accumulated_logging_time=0.220257, accumulated_submission_time=645.432246, global_step=1845, preemption_count=0, score=645.432246, test/loss=0.298789, test/num_examples=3581, test/ssim=0.728734, total_duration=925.986729, train/loss=0.280327, train/ssim=0.731979, validation/loss=0.297036, validation/num_examples=3554, validation/ssim=0.711823
I1010 02:27:13.819454 140158288500480 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.167295, loss=0.276846
I1010 02:27:13.823100 140216147011392 pytorch_submission_base.py:86] 2000) loss = 0.277, grad_norm = 0.167
I1010 02:27:54.450914 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:27:56.264986 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:27:58.204571 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:28:00.040377 140216147011392 submission_runner.py:381] Time since start: 1012.13s, 	Step: 2151, 	{'train/ssim': 0.733088493347168, 'train/loss': 0.27777103015354704, 'validation/ssim': 0.7120736400666503, 'validation/loss': 0.2949073117438098, 'validation/num_examples': 3554, 'test/ssim': 0.7291287377172927, 'test/loss': 0.29670673999930186, 'test/num_examples': 3581, 'score': 724.9572834968567, 'total_duration': 1012.1303761005402, 'accumulated_submission_time': 724.9572834968567, 'accumulated_eval_time': 278.63119864463806, 'accumulated_logging_time': 0.24489331245422363}
I1010 02:28:00.057695 140158296893184 logging_writer.py:48] [2151] accumulated_eval_time=278.631199, accumulated_logging_time=0.244893, accumulated_submission_time=724.957283, global_step=2151, preemption_count=0, score=724.957283, test/loss=0.296707, test/num_examples=3581, test/ssim=0.729129, total_duration=1012.130376, train/loss=0.277771, train/ssim=0.733088, validation/loss=0.294907, validation/num_examples=3554, validation/ssim=0.712074
I1010 02:29:20.618861 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:29:22.670881 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:29:24.722732 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:29:26.764572 140216147011392 submission_runner.py:381] Time since start: 1098.85s, 	Step: 2452, 	{'train/ssim': 0.733717509678432, 'train/loss': 0.2761002097811018, 'validation/ssim': 0.712606229336663, 'validation/loss': 0.29347729615837786, 'validation/num_examples': 3554, 'test/ssim': 0.7297807111316671, 'test/loss': 0.2951334612691113, 'test/num_examples': 3581, 'score': 802.7604639530182, 'total_duration': 1098.8546028137207, 'accumulated_submission_time': 802.7604639530182, 'accumulated_eval_time': 284.7772524356842, 'accumulated_logging_time': 2.008593797683716}
I1010 02:29:26.781731 140158288500480 logging_writer.py:48] [2452] accumulated_eval_time=284.777252, accumulated_logging_time=2.008594, accumulated_submission_time=802.760464, global_step=2452, preemption_count=0, score=802.760464, test/loss=0.295133, test/num_examples=3581, test/ssim=0.729781, total_duration=1098.854603, train/loss=0.276100, train/ssim=0.733718, validation/loss=0.293477, validation/num_examples=3554, validation/ssim=0.712606
I1010 02:29:37.562705 140158296893184 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.299994, loss=0.275732
I1010 02:29:37.566785 140216147011392 pytorch_submission_base.py:86] 2500) loss = 0.276, grad_norm = 0.300
I1010 02:30:47.413540 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:30:49.352519 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:30:51.315774 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:30:53.254128 140216147011392 submission_runner.py:381] Time since start: 1185.34s, 	Step: 2761, 	{'train/ssim': 0.7319229670933315, 'train/loss': 0.27739221709115164, 'validation/ssim': 0.7104972362786649, 'validation/loss': 0.2946034068127462, 'validation/num_examples': 3554, 'test/ssim': 0.7279895057028414, 'test/loss': 0.29615737246055573, 'test/num_examples': 3581, 'score': 882.4432985782623, 'total_duration': 1185.344166278839, 'accumulated_submission_time': 882.4432985782623, 'accumulated_eval_time': 290.6179518699646, 'accumulated_logging_time': 2.0351693630218506}
I1010 02:30:53.270347 140158288500480 logging_writer.py:48] [2761] accumulated_eval_time=290.617952, accumulated_logging_time=2.035169, accumulated_submission_time=882.443299, global_step=2761, preemption_count=0, score=882.443299, test/loss=0.296157, test/num_examples=3581, test/ssim=0.727990, total_duration=1185.344166, train/loss=0.277392, train/ssim=0.731923, validation/loss=0.294603, validation/num_examples=3554, validation/ssim=0.710497
I1010 02:31:55.439776 140158296893184 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.237763, loss=0.242680
I1010 02:31:55.443817 140216147011392 pytorch_submission_base.py:86] 3000) loss = 0.243, grad_norm = 0.238
I1010 02:32:13.954499 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:32:15.891727 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:32:17.871803 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:32:19.803510 140216147011392 submission_runner.py:381] Time since start: 1271.89s, 	Step: 3068, 	{'train/ssim': 0.7356076921735492, 'train/loss': 0.2748680966241019, 'validation/ssim': 0.7135930960273635, 'validation/loss': 0.2924621272905529, 'validation/num_examples': 3554, 'test/ssim': 0.7311548799218095, 'test/loss': 0.29407181429550056, 'test/num_examples': 3581, 'score': 962.1247160434723, 'total_duration': 1271.893544435501, 'accumulated_submission_time': 962.1247160434723, 'accumulated_eval_time': 296.4672088623047, 'accumulated_logging_time': 2.0600008964538574}
I1010 02:32:19.819162 140158288500480 logging_writer.py:48] [3068] accumulated_eval_time=296.467209, accumulated_logging_time=2.060001, accumulated_submission_time=962.124716, global_step=3068, preemption_count=0, score=962.124716, test/loss=0.294072, test/num_examples=3581, test/ssim=0.731155, total_duration=1271.893544, train/loss=0.274868, train/ssim=0.735608, validation/loss=0.292462, validation/num_examples=3554, validation/ssim=0.713593
I1010 02:33:40.400291 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:33:42.349496 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:33:44.325070 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:33:46.262144 140216147011392 submission_runner.py:381] Time since start: 1358.35s, 	Step: 3376, 	{'train/ssim': 0.7398606709071568, 'train/loss': 0.27318453788757324, 'validation/ssim': 0.7182415912308314, 'validation/loss': 0.2912077294619619, 'validation/num_examples': 3554, 'test/ssim': 0.7352981119973471, 'test/loss': 0.2928267380139975, 'test/num_examples': 3581, 'score': 1041.7301037311554, 'total_duration': 1358.3521766662598, 'accumulated_submission_time': 1041.7301037311554, 'accumulated_eval_time': 302.3292465209961, 'accumulated_logging_time': 2.085174083709717}
I1010 02:33:46.278362 140158296893184 logging_writer.py:48] [3376] accumulated_eval_time=302.329247, accumulated_logging_time=2.085174, accumulated_submission_time=1041.730104, global_step=3376, preemption_count=0, score=1041.730104, test/loss=0.292827, test/num_examples=3581, test/ssim=0.735298, total_duration=1358.352177, train/loss=0.273185, train/ssim=0.739861, validation/loss=0.291208, validation/num_examples=3554, validation/ssim=0.718242
I1010 02:34:17.291213 140158288500480 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.350056, loss=0.252666
I1010 02:34:17.295178 140216147011392 pytorch_submission_base.py:86] 3500) loss = 0.253, grad_norm = 0.350
I1010 02:35:06.961919 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:35:08.802663 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:35:10.697341 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:35:12.567989 140216147011392 submission_runner.py:381] Time since start: 1444.66s, 	Step: 3686, 	{'train/ssim': 0.7288509096418109, 'train/loss': 0.2772975819451468, 'validation/ssim': 0.7103600531531373, 'validation/loss': 0.2941648948038302, 'validation/num_examples': 3554, 'test/ssim': 0.726968287489528, 'test/loss': 0.2957079178193068, 'test/num_examples': 3581, 'score': 1121.4139938354492, 'total_duration': 1444.6579933166504, 'accumulated_submission_time': 1121.4139938354492, 'accumulated_eval_time': 307.9354507923126, 'accumulated_logging_time': 2.110544443130493}
I1010 02:35:12.587181 140158296893184 logging_writer.py:48] [3686] accumulated_eval_time=307.935451, accumulated_logging_time=2.110544, accumulated_submission_time=1121.413994, global_step=3686, preemption_count=0, score=1121.413994, test/loss=0.295708, test/num_examples=3581, test/ssim=0.726968, total_duration=1444.657993, train/loss=0.277298, train/ssim=0.728851, validation/loss=0.294165, validation/num_examples=3554, validation/ssim=0.710360
I1010 02:36:33.253583 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:36:35.264521 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:36:37.324527 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:36:39.353311 140216147011392 submission_runner.py:381] Time since start: 1531.44s, 	Step: 3995, 	{'train/ssim': 0.7355198178972516, 'train/loss': 0.2743499619620187, 'validation/ssim': 0.7150340340549382, 'validation/loss': 0.2916196566588703, 'validation/num_examples': 3554, 'test/ssim': 0.7321354648448408, 'test/loss': 0.2932032095390603, 'test/num_examples': 3581, 'score': 1201.070006608963, 'total_duration': 1531.4433393478394, 'accumulated_submission_time': 1201.070006608963, 'accumulated_eval_time': 314.03532671928406, 'accumulated_logging_time': 2.13865327835083}
I1010 02:36:39.370173 140158288500480 logging_writer.py:48] [3995] accumulated_eval_time=314.035327, accumulated_logging_time=2.138653, accumulated_submission_time=1201.070007, global_step=3995, preemption_count=0, score=1201.070007, test/loss=0.293203, test/num_examples=3581, test/ssim=0.732135, total_duration=1531.443339, train/loss=0.274350, train/ssim=0.735520, validation/loss=0.291620, validation/num_examples=3554, validation/ssim=0.715034
I1010 02:36:40.209061 140158296893184 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.367368, loss=0.247561
I1010 02:36:40.212465 140216147011392 pytorch_submission_base.py:86] 4000) loss = 0.248, grad_norm = 0.367
I1010 02:37:59.932855 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:38:01.889504 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:38:03.866019 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:38:05.793110 140216147011392 submission_runner.py:381] Time since start: 1617.88s, 	Step: 4302, 	{'train/ssim': 0.7404168673924038, 'train/loss': 0.27310214723859516, 'validation/ssim': 0.7188229536701604, 'validation/loss': 0.29081228897017447, 'validation/num_examples': 3554, 'test/ssim': 0.7358126412620427, 'test/loss': 0.2925010581017872, 'test/num_examples': 3581, 'score': 1280.64071059227, 'total_duration': 1617.88312458992, 'accumulated_submission_time': 1280.64071059227, 'accumulated_eval_time': 319.89588379859924, 'accumulated_logging_time': 2.1639182567596436}
I1010 02:38:05.808861 140158288500480 logging_writer.py:48] [4302] accumulated_eval_time=319.895884, accumulated_logging_time=2.163918, accumulated_submission_time=1280.640711, global_step=4302, preemption_count=0, score=1280.640711, test/loss=0.292501, test/num_examples=3581, test/ssim=0.735813, total_duration=1617.883125, train/loss=0.273102, train/ssim=0.740417, validation/loss=0.290812, validation/num_examples=3554, validation/ssim=0.718823
I1010 02:38:56.785554 140158296893184 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.294849, loss=0.210518
I1010 02:38:56.790701 140216147011392 pytorch_submission_base.py:86] 4500) loss = 0.211, grad_norm = 0.295
I1010 02:39:26.373765 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:39:28.339760 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:39:30.314842 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:39:32.250664 140216147011392 submission_runner.py:381] Time since start: 1704.34s, 	Step: 4610, 	{'train/ssim': 0.7407129832676479, 'train/loss': 0.2717583349772862, 'validation/ssim': 0.7195261803381753, 'validation/loss': 0.28963235609304655, 'validation/num_examples': 3554, 'test/ssim': 0.7365850828373709, 'test/loss': 0.29113544548266895, 'test/num_examples': 3581, 'score': 1360.1626873016357, 'total_duration': 1704.3407008647919, 'accumulated_submission_time': 1360.1626873016357, 'accumulated_eval_time': 325.7730760574341, 'accumulated_logging_time': 2.1881043910980225}
I1010 02:39:32.266928 140158288500480 logging_writer.py:48] [4610] accumulated_eval_time=325.773076, accumulated_logging_time=2.188104, accumulated_submission_time=1360.162687, global_step=4610, preemption_count=0, score=1360.162687, test/loss=0.291135, test/num_examples=3581, test/ssim=0.736585, total_duration=1704.340701, train/loss=0.271758, train/ssim=0.740713, validation/loss=0.289632, validation/num_examples=3554, validation/ssim=0.719526
I1010 02:40:52.869877 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:40:54.823237 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:40:56.788711 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:40:58.727221 140216147011392 submission_runner.py:381] Time since start: 1790.82s, 	Step: 4921, 	{'train/ssim': 0.7348274503435407, 'train/loss': 0.27647205761500765, 'validation/ssim': 0.7131163554709482, 'validation/loss': 0.29422208306221864, 'validation/num_examples': 3554, 'test/ssim': 0.7304922709482338, 'test/loss': 0.2956590692413083, 'test/num_examples': 3581, 'score': 1439.8118708133698, 'total_duration': 1790.817242860794, 'accumulated_submission_time': 1439.8118708133698, 'accumulated_eval_time': 331.63064885139465, 'accumulated_logging_time': 2.2127513885498047}
I1010 02:40:58.744796 140158296893184 logging_writer.py:48] [4921] accumulated_eval_time=331.630649, accumulated_logging_time=2.212751, accumulated_submission_time=1439.811871, global_step=4921, preemption_count=0, score=1439.811871, test/loss=0.295659, test/num_examples=3581, test/ssim=0.730492, total_duration=1790.817243, train/loss=0.276472, train/ssim=0.734827, validation/loss=0.294222, validation/num_examples=3554, validation/ssim=0.713116
I1010 02:41:17.907852 140158288500480 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.318401, loss=0.293347
I1010 02:41:17.911420 140216147011392 pytorch_submission_base.py:86] 5000) loss = 0.293, grad_norm = 0.318
I1010 02:42:19.331345 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:42:21.284314 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:42:23.259004 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:42:25.203950 140216147011392 submission_runner.py:381] Time since start: 1877.29s, 	Step: 5227, 	{'train/ssim': 0.7347612380981445, 'train/loss': 0.27582904270717074, 'validation/ssim': 0.7149407467817952, 'validation/loss': 0.2928777983434159, 'validation/num_examples': 3554, 'test/ssim': 0.7320965359710975, 'test/loss': 0.2943809613672857, 'test/num_examples': 3581, 'score': 1519.4112832546234, 'total_duration': 1877.2939748764038, 'accumulated_submission_time': 1519.4112832546234, 'accumulated_eval_time': 337.5035355091095, 'accumulated_logging_time': 2.2394347190856934}
I1010 02:42:25.219809 140158296893184 logging_writer.py:48] [5227] accumulated_eval_time=337.503536, accumulated_logging_time=2.239435, accumulated_submission_time=1519.411283, global_step=5227, preemption_count=0, score=1519.411283, test/loss=0.294381, test/num_examples=3581, test/ssim=0.732097, total_duration=1877.293975, train/loss=0.275829, train/ssim=0.734761, validation/loss=0.292878, validation/num_examples=3554, validation/ssim=0.714941
I1010 02:43:36.663582 140158288500480 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.252967, loss=0.265718
I1010 02:43:36.667126 140216147011392 pytorch_submission_base.py:86] 5500) loss = 0.266, grad_norm = 0.253
I1010 02:43:45.744163 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:43:47.612034 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:43:49.509681 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:43:51.369760 140216147011392 submission_runner.py:381] Time since start: 1963.46s, 	Step: 5534, 	{'train/ssim': 0.7369174957275391, 'train/loss': 0.2733560289655413, 'validation/ssim': 0.7157085463782006, 'validation/loss': 0.29101617455683737, 'validation/num_examples': 3554, 'test/ssim': 0.7329879458208252, 'test/loss': 0.2925143184624581, 'test/num_examples': 3581, 'score': 1598.9534223079681, 'total_duration': 1963.4597458839417, 'accumulated_submission_time': 1598.9534223079681, 'accumulated_eval_time': 343.1292185783386, 'accumulated_logging_time': 2.264946699142456}
I1010 02:43:51.388158 140158296893184 logging_writer.py:48] [5534] accumulated_eval_time=343.129219, accumulated_logging_time=2.264947, accumulated_submission_time=1598.953422, global_step=5534, preemption_count=0, score=1598.953422, test/loss=0.292514, test/num_examples=3581, test/ssim=0.732988, total_duration=1963.459746, train/loss=0.273356, train/ssim=0.736917, validation/loss=0.291016, validation/num_examples=3554, validation/ssim=0.715709
I1010 02:45:11.943072 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:45:13.941988 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:45:15.995668 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:45:18.026818 140216147011392 submission_runner.py:381] Time since start: 2050.12s, 	Step: 5842, 	{'train/ssim': 0.7367862973894391, 'train/loss': 0.27314138412475586, 'validation/ssim': 0.7160769555430501, 'validation/loss': 0.2908402820215778, 'validation/num_examples': 3554, 'test/ssim': 0.7331502062753071, 'test/loss': 0.29232148077527226, 'test/num_examples': 3581, 'score': 1678.5088427066803, 'total_duration': 2050.116842508316, 'accumulated_submission_time': 1678.5088427066803, 'accumulated_eval_time': 349.21320366859436, 'accumulated_logging_time': 2.293583631515503}
I1010 02:45:18.044276 140158288500480 logging_writer.py:48] [5842] accumulated_eval_time=349.213204, accumulated_logging_time=2.293584, accumulated_submission_time=1678.508843, global_step=5842, preemption_count=0, score=1678.508843, test/loss=0.292321, test/num_examples=3581, test/ssim=0.733150, total_duration=2050.116843, train/loss=0.273141, train/ssim=0.736786, validation/loss=0.290840, validation/num_examples=3554, validation/ssim=0.716077
I1010 02:45:58.256486 140158296893184 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.198131, loss=0.278458
I1010 02:45:58.259777 140216147011392 pytorch_submission_base.py:86] 6000) loss = 0.278, grad_norm = 0.198
I1010 02:46:38.609843 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:46:40.480203 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:46:42.392345 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:46:44.258035 140216147011392 submission_runner.py:381] Time since start: 2136.35s, 	Step: 6149, 	{'train/ssim': 0.7390224593026298, 'train/loss': 0.27243811743600027, 'validation/ssim': 0.7183370080367192, 'validation/loss': 0.2901400779161684, 'validation/num_examples': 3554, 'test/ssim': 0.7351779847197012, 'test/loss': 0.2915490391999441, 'test/num_examples': 3581, 'score': 1758.1025536060333, 'total_duration': 2136.3480501174927, 'accumulated_submission_time': 1758.1025536060333, 'accumulated_eval_time': 354.8616609573364, 'accumulated_logging_time': 2.3204290866851807}
I1010 02:46:44.276343 140158288500480 logging_writer.py:48] [6149] accumulated_eval_time=354.861661, accumulated_logging_time=2.320429, accumulated_submission_time=1758.102554, global_step=6149, preemption_count=0, score=1758.102554, test/loss=0.291549, test/num_examples=3581, test/ssim=0.735178, total_duration=2136.348050, train/loss=0.272438, train/ssim=0.739022, validation/loss=0.290140, validation/num_examples=3554, validation/ssim=0.718337
I1010 02:48:04.944553 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:48:06.964321 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:48:09.027920 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:48:11.060165 140216147011392 submission_runner.py:381] Time since start: 2223.15s, 	Step: 6456, 	{'train/ssim': 0.742260047367641, 'train/loss': 0.27071663311549593, 'validation/ssim': 0.7203011241734665, 'validation/loss': 0.28857421875, 'validation/num_examples': 3554, 'test/ssim': 0.7377084978663432, 'test/loss': 0.2901110229662629, 'test/num_examples': 3581, 'score': 1837.7527384757996, 'total_duration': 2223.150191783905, 'accumulated_submission_time': 1837.7527384757996, 'accumulated_eval_time': 360.9774954319, 'accumulated_logging_time': 2.3480262756347656}
I1010 02:48:11.077155 140158296893184 logging_writer.py:48] [6456] accumulated_eval_time=360.977495, accumulated_logging_time=2.348026, accumulated_submission_time=1837.752738, global_step=6456, preemption_count=0, score=1837.752738, test/loss=0.290111, test/num_examples=3581, test/ssim=0.737708, total_duration=2223.150192, train/loss=0.270717, train/ssim=0.742260, validation/loss=0.288574, validation/num_examples=3554, validation/ssim=0.720301
I1010 02:48:20.762825 140158288500480 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.440233, loss=0.223755
I1010 02:48:20.766736 140216147011392 pytorch_submission_base.py:86] 6500) loss = 0.224, grad_norm = 0.440
I1010 02:49:31.649882 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:49:33.580454 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:49:35.542356 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:49:37.480139 140216147011392 submission_runner.py:381] Time since start: 2309.57s, 	Step: 6763, 	{'train/ssim': 0.7443150111607143, 'train/loss': 0.27010384627750944, 'validation/ssim': 0.7219010902108539, 'validation/loss': 0.28841330163855866, 'validation/num_examples': 3554, 'test/ssim': 0.7391327083260611, 'test/loss': 0.2899065611582833, 'test/num_examples': 3581, 'score': 1917.3343152999878, 'total_duration': 2309.5701735019684, 'accumulated_submission_time': 1917.3343152999878, 'accumulated_eval_time': 366.80816197395325, 'accumulated_logging_time': 2.37479305267334}
I1010 02:49:37.496423 140158296893184 logging_writer.py:48] [6763] accumulated_eval_time=366.808162, accumulated_logging_time=2.374793, accumulated_submission_time=1917.334315, global_step=6763, preemption_count=0, score=1917.334315, test/loss=0.289907, test/num_examples=3581, test/ssim=0.739133, total_duration=2309.570174, train/loss=0.270104, train/ssim=0.744315, validation/loss=0.288413, validation/num_examples=3554, validation/ssim=0.721901
I1010 02:50:38.743036 140158288500480 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.233862, loss=0.236973
I1010 02:50:38.747204 140216147011392 pytorch_submission_base.py:86] 7000) loss = 0.237, grad_norm = 0.234
I1010 02:50:58.005298 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:50:59.960128 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:51:02.018978 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:51:03.904535 140216147011392 submission_runner.py:381] Time since start: 2395.99s, 	Step: 7071, 	{'train/ssim': 0.7409170695713588, 'train/loss': 0.2706665311540876, 'validation/ssim': 0.7189129436022791, 'validation/loss': 0.28869148044017306, 'validation/num_examples': 3554, 'test/ssim': 0.7363670538737433, 'test/loss': 0.29013566882941216, 'test/num_examples': 3581, 'score': 1996.8287000656128, 'total_duration': 2395.994550704956, 'accumulated_submission_time': 1996.8287000656128, 'accumulated_eval_time': 372.70765590667725, 'accumulated_logging_time': 2.3995752334594727}
I1010 02:51:03.922521 140158296893184 logging_writer.py:48] [7071] accumulated_eval_time=372.707656, accumulated_logging_time=2.399575, accumulated_submission_time=1996.828700, global_step=7071, preemption_count=0, score=1996.828700, test/loss=0.290136, test/num_examples=3581, test/ssim=0.736367, total_duration=2395.994551, train/loss=0.270667, train/ssim=0.740917, validation/loss=0.288691, validation/num_examples=3554, validation/ssim=0.718913
I1010 02:52:24.499535 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:52:26.485758 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:52:28.450774 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:52:30.479573 140216147011392 submission_runner.py:381] Time since start: 2482.57s, 	Step: 7380, 	{'train/ssim': 0.744725227355957, 'train/loss': 0.2702400003160749, 'validation/ssim': 0.7230981623918472, 'validation/loss': 0.28831218318048324, 'validation/num_examples': 3554, 'test/ssim': 0.7401801063774085, 'test/loss': 0.2897879678555222, 'test/num_examples': 3581, 'score': 2076.401529312134, 'total_duration': 2482.569599866867, 'accumulated_submission_time': 2076.401529312134, 'accumulated_eval_time': 378.6879720687866, 'accumulated_logging_time': 2.426312208175659}
I1010 02:52:30.496146 140158288500480 logging_writer.py:48] [7380] accumulated_eval_time=378.687972, accumulated_logging_time=2.426312, accumulated_submission_time=2076.401529, global_step=7380, preemption_count=0, score=2076.401529, test/loss=0.289788, test/num_examples=3581, test/ssim=0.740180, total_duration=2482.569600, train/loss=0.270240, train/ssim=0.744725, validation/loss=0.288312, validation/num_examples=3554, validation/ssim=0.723098
I1010 02:53:00.918623 140158296893184 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.668513, loss=0.256278
I1010 02:53:00.921984 140216147011392 pytorch_submission_base.py:86] 7500) loss = 0.256, grad_norm = 0.669
I1010 02:53:51.024718 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:53:52.979588 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:53:54.958016 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:53:56.902568 140216147011392 submission_runner.py:381] Time since start: 2568.99s, 	Step: 7688, 	{'train/ssim': 0.7427986008780343, 'train/loss': 0.2706306832177298, 'validation/ssim': 0.7204409863885762, 'validation/loss': 0.28889488516460327, 'validation/num_examples': 3554, 'test/ssim': 0.7377329051111771, 'test/loss': 0.2903244159169052, 'test/num_examples': 3581, 'score': 2155.9663047790527, 'total_duration': 2568.992602586746, 'accumulated_submission_time': 2155.9663047790527, 'accumulated_eval_time': 384.56602334976196, 'accumulated_logging_time': 2.4513158798217773}
I1010 02:53:56.919128 140158288500480 logging_writer.py:48] [7688] accumulated_eval_time=384.566023, accumulated_logging_time=2.451316, accumulated_submission_time=2155.966305, global_step=7688, preemption_count=0, score=2155.966305, test/loss=0.290324, test/num_examples=3581, test/ssim=0.737733, total_duration=2568.992603, train/loss=0.270631, train/ssim=0.742799, validation/loss=0.288895, validation/num_examples=3554, validation/ssim=0.720441
I1010 02:55:17.429762 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:55:19.375817 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:55:21.350509 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:55:23.289635 140216147011392 submission_runner.py:381] Time since start: 2655.38s, 	Step: 7996, 	{'train/ssim': 0.7406712940761021, 'train/loss': 0.27117202963147846, 'validation/ssim': 0.7190298618194289, 'validation/loss': 0.2891658510327272, 'validation/num_examples': 3554, 'test/ssim': 0.7364460024478149, 'test/loss': 0.2905314343496928, 'test/num_examples': 3581, 'score': 2235.521714925766, 'total_duration': 2655.379640340805, 'accumulated_submission_time': 2235.521714925766, 'accumulated_eval_time': 390.4260365962982, 'accumulated_logging_time': 2.4765377044677734}
I1010 02:55:23.307023 140158296893184 logging_writer.py:48] [7996] accumulated_eval_time=390.426037, accumulated_logging_time=2.476538, accumulated_submission_time=2235.521715, global_step=7996, preemption_count=0, score=2235.521715, test/loss=0.290531, test/num_examples=3581, test/ssim=0.736446, total_duration=2655.379640, train/loss=0.271172, train/ssim=0.740671, validation/loss=0.289166, validation/num_examples=3554, validation/ssim=0.719030
I1010 02:55:24.092866 140158288500480 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.220833, loss=0.281551
I1010 02:55:24.096409 140216147011392 pytorch_submission_base.py:86] 8000) loss = 0.282, grad_norm = 0.221
I1010 02:56:43.982097 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:56:45.813430 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:56:47.713388 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:56:49.588646 140216147011392 submission_runner.py:381] Time since start: 2741.68s, 	Step: 8305, 	{'train/ssim': 0.7359423637390137, 'train/loss': 0.27750461442129953, 'validation/ssim': 0.7150202951340039, 'validation/loss': 0.2951681451577448, 'validation/num_examples': 3554, 'test/ssim': 0.7319636596577422, 'test/loss': 0.2967454302547298, 'test/num_examples': 3581, 'score': 2315.2203829288483, 'total_duration': 2741.678628206253, 'accumulated_submission_time': 2315.2203829288483, 'accumulated_eval_time': 396.0329291820526, 'accumulated_logging_time': 2.502425193786621}
I1010 02:56:49.607076 140158296893184 logging_writer.py:48] [8305] accumulated_eval_time=396.032929, accumulated_logging_time=2.502425, accumulated_submission_time=2315.220383, global_step=8305, preemption_count=0, score=2315.220383, test/loss=0.296745, test/num_examples=3581, test/ssim=0.731964, total_duration=2741.678628, train/loss=0.277505, train/ssim=0.735942, validation/loss=0.295168, validation/num_examples=3554, validation/ssim=0.715020
I1010 02:57:39.729562 140158288500480 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.196381, loss=0.281960
I1010 02:57:39.733182 140216147011392 pytorch_submission_base.py:86] 8500) loss = 0.282, grad_norm = 0.196
I1010 02:58:10.232119 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:58:12.251257 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:58:14.308665 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:58:16.345748 140216147011392 submission_runner.py:381] Time since start: 2828.44s, 	Step: 8612, 	{'train/ssim': 0.7435787064688546, 'train/loss': 0.27013492584228516, 'validation/ssim': 0.7219633275226857, 'validation/loss': 0.2880846494761624, 'validation/num_examples': 3554, 'test/ssim': 0.7392561762601229, 'test/loss': 0.289554394613062, 'test/num_examples': 3581, 'score': 2394.7811086177826, 'total_duration': 2828.4357776641846, 'accumulated_submission_time': 2394.7811086177826, 'accumulated_eval_time': 402.1468152999878, 'accumulated_logging_time': 2.530094623565674}
I1010 02:58:16.363134 140158296893184 logging_writer.py:48] [8612] accumulated_eval_time=402.146815, accumulated_logging_time=2.530095, accumulated_submission_time=2394.781109, global_step=8612, preemption_count=0, score=2394.781109, test/loss=0.289554, test/num_examples=3581, test/ssim=0.739256, total_duration=2828.435778, train/loss=0.270135, train/ssim=0.743579, validation/loss=0.288085, validation/num_examples=3554, validation/ssim=0.721963
I1010 02:59:36.924556 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 02:59:38.887096 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 02:59:40.853276 140216147011392 spec.py:349] Evaluating on the test split.
I1010 02:59:42.789025 140216147011392 submission_runner.py:381] Time since start: 2914.88s, 	Step: 8922, 	{'train/ssim': 0.7445036343165806, 'train/loss': 0.2693431888307844, 'validation/ssim': 0.7221773112162352, 'validation/loss': 0.2877102638807066, 'validation/num_examples': 3554, 'test/ssim': 0.7395213152968794, 'test/loss': 0.2891255975002618, 'test/num_examples': 3581, 'score': 2474.375424146652, 'total_duration': 2914.8790657520294, 'accumulated_submission_time': 2474.375424146652, 'accumulated_eval_time': 408.01150131225586, 'accumulated_logging_time': 2.5558865070343018}
I1010 02:59:42.806257 140158288500480 logging_writer.py:48] [8922] accumulated_eval_time=408.011501, accumulated_logging_time=2.555887, accumulated_submission_time=2474.375424, global_step=8922, preemption_count=0, score=2474.375424, test/loss=0.289126, test/num_examples=3581, test/ssim=0.739521, total_duration=2914.879066, train/loss=0.269343, train/ssim=0.744504, validation/loss=0.287710, validation/num_examples=3554, validation/ssim=0.722177
I1010 03:00:01.628303 140158296893184 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.333061, loss=0.291024
I1010 03:00:01.633314 140216147011392 pytorch_submission_base.py:86] 9000) loss = 0.291, grad_norm = 1.333
I1010 03:01:03.282900 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:01:05.213669 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:01:07.204751 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:01:09.137228 140216147011392 submission_runner.py:381] Time since start: 3001.23s, 	Step: 9230, 	{'train/ssim': 0.74365234375, 'train/loss': 0.269491229738508, 'validation/ssim': 0.7217590984629995, 'validation/loss': 0.2875861327300665, 'validation/num_examples': 3554, 'test/ssim': 0.7390269663240017, 'test/loss': 0.28899057362206787, 'test/num_examples': 3581, 'score': 2553.899953842163, 'total_duration': 3001.2272350788116, 'accumulated_submission_time': 2553.899953842163, 'accumulated_eval_time': 413.8660433292389, 'accumulated_logging_time': 2.5818021297454834}
I1010 03:01:09.153498 140158288500480 logging_writer.py:48] [9230] accumulated_eval_time=413.866043, accumulated_logging_time=2.581802, accumulated_submission_time=2553.899954, global_step=9230, preemption_count=0, score=2553.899954, test/loss=0.288991, test/num_examples=3581, test/ssim=0.739027, total_duration=3001.227235, train/loss=0.269491, train/ssim=0.743652, validation/loss=0.287586, validation/num_examples=3554, validation/ssim=0.721759
I1010 03:02:19.055762 140158296893184 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.210067, loss=0.316130
I1010 03:02:19.059344 140216147011392 pytorch_submission_base.py:86] 9500) loss = 0.316, grad_norm = 0.210
I1010 03:02:29.615559 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:02:31.461176 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:02:33.358006 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:02:35.229099 140216147011392 submission_runner.py:381] Time since start: 3087.32s, 	Step: 9539, 	{'train/ssim': 0.7448873519897461, 'train/loss': 0.2696940728596279, 'validation/ssim': 0.7227241202694148, 'validation/loss': 0.28791583248518393, 'validation/num_examples': 3554, 'test/ssim': 0.739924443896607, 'test/loss': 0.28938487334412527, 'test/num_examples': 3581, 'score': 2633.387009382248, 'total_duration': 3087.3191092014313, 'accumulated_submission_time': 2633.387009382248, 'accumulated_eval_time': 419.47976183891296, 'accumulated_logging_time': 2.6076576709747314}
I1010 03:02:35.247955 140158288500480 logging_writer.py:48] [9539] accumulated_eval_time=419.479762, accumulated_logging_time=2.607658, accumulated_submission_time=2633.387009, global_step=9539, preemption_count=0, score=2633.387009, test/loss=0.289385, test/num_examples=3581, test/ssim=0.739924, total_duration=3087.319109, train/loss=0.269694, train/ssim=0.744887, validation/loss=0.287916, validation/num_examples=3554, validation/ssim=0.722724
I1010 03:03:55.842898 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:03:57.833610 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:03:59.883820 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:04:01.910076 140216147011392 submission_runner.py:381] Time since start: 3174.00s, 	Step: 9846, 	{'train/ssim': 0.7445345606122699, 'train/loss': 0.2717437744140625, 'validation/ssim': 0.7221051131867262, 'validation/loss': 0.29073562579136186, 'validation/num_examples': 3554, 'test/ssim': 0.7390494646223122, 'test/loss': 0.2922214656127827, 'test/num_examples': 3581, 'score': 2712.936015367508, 'total_duration': 3173.9999208450317, 'accumulated_submission_time': 2712.936015367508, 'accumulated_eval_time': 425.5468873977661, 'accumulated_logging_time': 2.6360297203063965}
I1010 03:04:01.932074 140158296893184 logging_writer.py:48] [9846] accumulated_eval_time=425.546887, accumulated_logging_time=2.636030, accumulated_submission_time=2712.936015, global_step=9846, preemption_count=0, score=2712.936015, test/loss=0.292221, test/num_examples=3581, test/ssim=0.739049, total_duration=3173.999921, train/loss=0.271744, train/ssim=0.744535, validation/loss=0.290736, validation/num_examples=3554, validation/ssim=0.722105
I1010 03:04:40.989574 140158288500480 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.251474, loss=0.283420
I1010 03:04:40.992908 140216147011392 pytorch_submission_base.py:86] 10000) loss = 0.283, grad_norm = 0.251
I1010 03:05:22.570368 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:05:24.539000 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:05:26.513450 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:05:28.457582 140216147011392 submission_runner.py:381] Time since start: 3260.55s, 	Step: 10156, 	{'train/ssim': 0.7451366697038923, 'train/loss': 0.2704284531729562, 'validation/ssim': 0.7233288388743317, 'validation/loss': 0.28862735402671286, 'validation/num_examples': 3554, 'test/ssim': 0.7403834091821418, 'test/loss': 0.29010236453024646, 'test/num_examples': 3581, 'score': 2792.441019296646, 'total_duration': 3260.5476138591766, 'accumulated_submission_time': 2792.441019296646, 'accumulated_eval_time': 431.4342589378357, 'accumulated_logging_time': 2.67303729057312}
I1010 03:05:28.475032 140158296893184 logging_writer.py:48] [10156] accumulated_eval_time=431.434259, accumulated_logging_time=2.673037, accumulated_submission_time=2792.441019, global_step=10156, preemption_count=0, score=2792.441019, test/loss=0.290102, test/num_examples=3581, test/ssim=0.740383, total_duration=3260.547614, train/loss=0.270428, train/ssim=0.745137, validation/loss=0.288627, validation/num_examples=3554, validation/ssim=0.723329
I1010 03:06:49.088682 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:06:51.028254 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:06:52.994923 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:06:54.917186 140216147011392 submission_runner.py:381] Time since start: 3347.01s, 	Step: 10469, 	{'train/ssim': 0.7449252264840263, 'train/loss': 0.2709404570715768, 'validation/ssim': 0.7238126549750281, 'validation/loss': 0.2888565192278946, 'validation/num_examples': 3554, 'test/ssim': 0.7407451545483106, 'test/loss': 0.29029073664610094, 'test/num_examples': 3581, 'score': 2872.029524087906, 'total_duration': 3347.007192850113, 'accumulated_submission_time': 2872.029524087906, 'accumulated_eval_time': 437.26291847229004, 'accumulated_logging_time': 2.699338436126709}
I1010 03:06:54.934953 140158288500480 logging_writer.py:48] [10469] accumulated_eval_time=437.262918, accumulated_logging_time=2.699338, accumulated_submission_time=2872.029524, global_step=10469, preemption_count=0, score=2872.029524, test/loss=0.290291, test/num_examples=3581, test/ssim=0.740745, total_duration=3347.007193, train/loss=0.270940, train/ssim=0.744925, validation/loss=0.288857, validation/num_examples=3554, validation/ssim=0.723813
I1010 03:07:01.015044 140158296893184 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.293937, loss=0.308096
I1010 03:07:01.018905 140216147011392 pytorch_submission_base.py:86] 10500) loss = 0.308, grad_norm = 0.294
I1010 03:08:15.583233 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:08:17.417325 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:08:19.331478 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:08:21.198178 140216147011392 submission_runner.py:381] Time since start: 3433.29s, 	Step: 10779, 	{'train/ssim': 0.7457284927368164, 'train/loss': 0.2693310635430472, 'validation/ssim': 0.7233291136527504, 'validation/loss': 0.2877661297679551, 'validation/num_examples': 3554, 'test/ssim': 0.7407147477572605, 'test/loss': 0.2891107349880445, 'test/num_examples': 3581, 'score': 2951.710850954056, 'total_duration': 3433.288172483444, 'accumulated_submission_time': 2951.710850954056, 'accumulated_eval_time': 442.8781199455261, 'accumulated_logging_time': 2.7260024547576904}
I1010 03:08:21.216956 140158288500480 logging_writer.py:48] [10779] accumulated_eval_time=442.878120, accumulated_logging_time=2.726002, accumulated_submission_time=2951.710851, global_step=10779, preemption_count=0, score=2951.710851, test/loss=0.289111, test/num_examples=3581, test/ssim=0.740715, total_duration=3433.288172, train/loss=0.269331, train/ssim=0.745728, validation/loss=0.287766, validation/num_examples=3554, validation/ssim=0.723329
I1010 03:09:18.069125 140158296893184 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.193650, loss=0.324020
I1010 03:09:18.073166 140216147011392 pytorch_submission_base.py:86] 11000) loss = 0.324, grad_norm = 0.194
I1010 03:09:41.767245 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:09:43.770737 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:09:45.836385 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:09:47.851780 140216147011392 submission_runner.py:381] Time since start: 3519.94s, 	Step: 11089, 	{'train/ssim': 0.7450030871800014, 'train/loss': 0.2688075133732387, 'validation/ssim': 0.7225435221537352, 'validation/loss': 0.28730048339019415, 'validation/num_examples': 3554, 'test/ssim': 0.7398610396013683, 'test/loss': 0.28871278781459436, 'test/num_examples': 3581, 'score': 3031.259932756424, 'total_duration': 3519.941817522049, 'accumulated_submission_time': 3031.259932756424, 'accumulated_eval_time': 448.96289348602295, 'accumulated_logging_time': 2.7537286281585693}
I1010 03:09:47.869107 140158288500480 logging_writer.py:48] [11089] accumulated_eval_time=448.962893, accumulated_logging_time=2.753729, accumulated_submission_time=3031.259933, global_step=11089, preemption_count=0, score=3031.259933, test/loss=0.288713, test/num_examples=3581, test/ssim=0.739861, total_duration=3519.941818, train/loss=0.268808, train/ssim=0.745003, validation/loss=0.287300, validation/num_examples=3554, validation/ssim=0.722544
I1010 03:11:08.456051 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:11:10.413877 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:11:12.387873 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:11:14.318930 140216147011392 submission_runner.py:381] Time since start: 3606.41s, 	Step: 11400, 	{'train/ssim': 0.7452663694109235, 'train/loss': 0.26884065355573383, 'validation/ssim': 0.7228128736986494, 'validation/loss': 0.28721116323047097, 'validation/num_examples': 3554, 'test/ssim': 0.7401323827143256, 'test/loss': 0.2886229309746754, 'test/num_examples': 3581, 'score': 3110.8493280410767, 'total_duration': 3606.408946752548, 'accumulated_submission_time': 3110.8493280410767, 'accumulated_eval_time': 454.8260598182678, 'accumulated_logging_time': 2.7796761989593506}
I1010 03:11:14.336852 140158296893184 logging_writer.py:48] [11400] accumulated_eval_time=454.826060, accumulated_logging_time=2.779676, accumulated_submission_time=3110.849328, global_step=11400, preemption_count=0, score=3110.849328, test/loss=0.288623, test/num_examples=3581, test/ssim=0.740132, total_duration=3606.408947, train/loss=0.268841, train/ssim=0.745266, validation/loss=0.287211, validation/num_examples=3554, validation/ssim=0.722813
I1010 03:11:38.807442 140158288500480 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.317167, loss=0.313887
I1010 03:11:38.811308 140216147011392 pytorch_submission_base.py:86] 11500) loss = 0.314, grad_norm = 0.317
I1010 03:12:35.014787 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:12:36.968643 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:12:38.964321 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:12:40.914220 140216147011392 submission_runner.py:381] Time since start: 3693.00s, 	Step: 11711, 	{'train/ssim': 0.7456013815743583, 'train/loss': 0.2692469528743199, 'validation/ssim': 0.7234306442784538, 'validation/loss': 0.2875077006651836, 'validation/num_examples': 3554, 'test/ssim': 0.7406137099448478, 'test/loss': 0.2889252944686366, 'test/num_examples': 3581, 'score': 3190.555389404297, 'total_duration': 3693.00421500206, 'accumulated_submission_time': 3190.555389404297, 'accumulated_eval_time': 460.725745677948, 'accumulated_logging_time': 2.8059895038604736}
I1010 03:12:40.932830 140158296893184 logging_writer.py:48] [11711] accumulated_eval_time=460.725746, accumulated_logging_time=2.805990, accumulated_submission_time=3190.555389, global_step=11711, preemption_count=0, score=3190.555389, test/loss=0.288925, test/num_examples=3581, test/ssim=0.740614, total_duration=3693.004215, train/loss=0.269247, train/ssim=0.745601, validation/loss=0.287508, validation/num_examples=3554, validation/ssim=0.723431
I1010 03:13:57.006312 140158288500480 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.270756, loss=0.271919
I1010 03:13:57.009759 140216147011392 pytorch_submission_base.py:86] 12000) loss = 0.272, grad_norm = 0.271
I1010 03:14:01.482198 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:14:03.443357 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:14:05.417188 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:14:07.345227 140216147011392 submission_runner.py:381] Time since start: 3779.44s, 	Step: 12016, 	{'train/ssim': 0.744403498513358, 'train/loss': 0.2687079395566668, 'validation/ssim': 0.722090343846722, 'validation/loss': 0.28727425922486105, 'validation/num_examples': 3554, 'test/ssim': 0.7392697434157708, 'test/loss': 0.288701709107093, 'test/num_examples': 3581, 'score': 3270.0436816215515, 'total_duration': 3779.4352633953094, 'accumulated_submission_time': 3270.0436816215515, 'accumulated_eval_time': 466.5889983177185, 'accumulated_logging_time': 2.8333652019500732}
I1010 03:14:07.362432 140158296893184 logging_writer.py:48] [12016] accumulated_eval_time=466.588998, accumulated_logging_time=2.833365, accumulated_submission_time=3270.043682, global_step=12016, preemption_count=0, score=3270.043682, test/loss=0.288702, test/num_examples=3581, test/ssim=0.739270, total_duration=3779.435263, train/loss=0.268708, train/ssim=0.744403, validation/loss=0.287274, validation/num_examples=3554, validation/ssim=0.722090
I1010 03:15:27.929350 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:15:29.865959 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:15:31.828881 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:15:33.767707 140216147011392 submission_runner.py:381] Time since start: 3865.86s, 	Step: 12326, 	{'train/ssim': 0.7438523428780692, 'train/loss': 0.2701730898448399, 'validation/ssim': 0.7217950257412422, 'validation/loss': 0.28835226648230866, 'validation/num_examples': 3554, 'test/ssim': 0.7389368367774365, 'test/loss': 0.2898826311500803, 'test/num_examples': 3581, 'score': 3349.625075817108, 'total_duration': 3865.857735157013, 'accumulated_submission_time': 3349.625075817108, 'accumulated_eval_time': 472.42758297920227, 'accumulated_logging_time': 2.859830617904663}
I1010 03:15:33.785522 140158288500480 logging_writer.py:48] [12326] accumulated_eval_time=472.427583, accumulated_logging_time=2.859831, accumulated_submission_time=3349.625076, global_step=12326, preemption_count=0, score=3349.625076, test/loss=0.289883, test/num_examples=3581, test/ssim=0.738937, total_duration=3865.857735, train/loss=0.270173, train/ssim=0.743852, validation/loss=0.288352, validation/num_examples=3554, validation/ssim=0.721795
I1010 03:16:18.055586 140158296893184 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.419079, loss=0.336315
I1010 03:16:18.059256 140216147011392 pytorch_submission_base.py:86] 12500) loss = 0.336, grad_norm = 0.419
I1010 03:16:54.489955 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:16:56.434434 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:16:58.396579 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:17:00.322804 140216147011392 submission_runner.py:381] Time since start: 3952.41s, 	Step: 12636, 	{'train/ssim': 0.7447174617222377, 'train/loss': 0.2693060466221401, 'validation/ssim': 0.7227802437614308, 'validation/loss': 0.28751666531109316, 'validation/num_examples': 3554, 'test/ssim': 0.7399409426487015, 'test/loss': 0.2888523454407812, 'test/num_examples': 3581, 'score': 3429.299788951874, 'total_duration': 3952.412826061249, 'accumulated_submission_time': 3429.299788951874, 'accumulated_eval_time': 478.2607867717743, 'accumulated_logging_time': 2.8856914043426514}
I1010 03:17:00.340399 140158288500480 logging_writer.py:48] [12636] accumulated_eval_time=478.260787, accumulated_logging_time=2.885691, accumulated_submission_time=3429.299789, global_step=12636, preemption_count=0, score=3429.299789, test/loss=0.288852, test/num_examples=3581, test/ssim=0.739941, total_duration=3952.412826, train/loss=0.269306, train/ssim=0.744717, validation/loss=0.287517, validation/num_examples=3554, validation/ssim=0.722780
I1010 03:18:20.927119 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:18:22.869691 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:18:24.828768 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:18:26.761529 140216147011392 submission_runner.py:381] Time since start: 4038.85s, 	Step: 12941, 	{'train/ssim': 0.7354416847229004, 'train/loss': 0.27136618750435965, 'validation/ssim': 0.7152502846704417, 'validation/loss': 0.28929300474597286, 'validation/num_examples': 3554, 'test/ssim': 0.7323037930222005, 'test/loss': 0.29060005415953993, 'test/num_examples': 3581, 'score': 3508.915581703186, 'total_duration': 4038.851531982422, 'accumulated_submission_time': 3508.915581703186, 'accumulated_eval_time': 484.09526443481445, 'accumulated_logging_time': 2.911729097366333}
I1010 03:18:26.779138 140158296893184 logging_writer.py:48] [12941] accumulated_eval_time=484.095264, accumulated_logging_time=2.911729, accumulated_submission_time=3508.915582, global_step=12941, preemption_count=0, score=3508.915582, test/loss=0.290600, test/num_examples=3581, test/ssim=0.732304, total_duration=4038.851532, train/loss=0.271366, train/ssim=0.735442, validation/loss=0.289293, validation/num_examples=3554, validation/ssim=0.715250
I1010 03:18:40.540861 140158288500480 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.460302, loss=0.339498
I1010 03:18:40.544565 140216147011392 pytorch_submission_base.py:86] 13000) loss = 0.339, grad_norm = 0.460
I1010 03:19:47.338902 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:19:49.290155 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:19:51.245393 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:19:53.192847 140216147011392 submission_runner.py:381] Time since start: 4125.28s, 	Step: 13249, 	{'train/ssim': 0.7464895248413086, 'train/loss': 0.2685727732522147, 'validation/ssim': 0.7241928109172763, 'validation/loss': 0.28715560646894345, 'validation/num_examples': 3554, 'test/ssim': 0.7413913329464535, 'test/loss': 0.2885199842157393, 'test/num_examples': 3581, 'score': 3588.502081632614, 'total_duration': 4125.282881736755, 'accumulated_submission_time': 3588.502081632614, 'accumulated_eval_time': 489.9494676589966, 'accumulated_logging_time': 2.9377171993255615}
I1010 03:19:53.211470 140158296893184 logging_writer.py:48] [13249] accumulated_eval_time=489.949468, accumulated_logging_time=2.937717, accumulated_submission_time=3588.502082, global_step=13249, preemption_count=0, score=3588.502082, test/loss=0.288520, test/num_examples=3581, test/ssim=0.741391, total_duration=4125.282882, train/loss=0.268573, train/ssim=0.746490, validation/loss=0.287156, validation/num_examples=3554, validation/ssim=0.724193
I1010 03:20:57.932577 140158288500480 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.156298, loss=0.277067
I1010 03:20:57.937338 140216147011392 pytorch_submission_base.py:86] 13500) loss = 0.277, grad_norm = 0.156
I1010 03:21:13.651915 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:21:15.496426 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:21:17.392202 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:21:19.260888 140216147011392 submission_runner.py:381] Time since start: 4211.35s, 	Step: 13558, 	{'train/ssim': 0.7451870782034737, 'train/loss': 0.2688079902103969, 'validation/ssim': 0.723133265334834, 'validation/loss': 0.28700902935622713, 'validation/num_examples': 3554, 'test/ssim': 0.7404320191418249, 'test/loss': 0.28838949408553827, 'test/num_examples': 3581, 'score': 3667.9507009983063, 'total_duration': 4211.350889444351, 'accumulated_submission_time': 3667.9507009983063, 'accumulated_eval_time': 495.5587031841278, 'accumulated_logging_time': 2.965099334716797}
I1010 03:21:19.280951 140158296893184 logging_writer.py:48] [13558] accumulated_eval_time=495.558703, accumulated_logging_time=2.965099, accumulated_submission_time=3667.950701, global_step=13558, preemption_count=0, score=3667.950701, test/loss=0.288389, test/num_examples=3581, test/ssim=0.740432, total_duration=4211.350889, train/loss=0.268808, train/ssim=0.745187, validation/loss=0.287009, validation/num_examples=3554, validation/ssim=0.723133
I1010 03:22:39.890968 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:22:41.900099 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:22:43.950352 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:22:45.978887 140216147011392 submission_runner.py:381] Time since start: 4298.07s, 	Step: 13864, 	{'train/ssim': 0.7454970223563058, 'train/loss': 0.26861725534711567, 'validation/ssim': 0.7233342657481008, 'validation/loss': 0.2868838162655635, 'validation/num_examples': 3554, 'test/ssim': 0.7406417305527436, 'test/loss': 0.28820446262609956, 'test/num_examples': 3581, 'score': 3747.5766203403473, 'total_duration': 4298.068925142288, 'accumulated_submission_time': 3747.5766203403473, 'accumulated_eval_time': 501.64697647094727, 'accumulated_logging_time': 2.994030714035034}
I1010 03:22:45.997547 140158288500480 logging_writer.py:48] [13864] accumulated_eval_time=501.646976, accumulated_logging_time=2.994031, accumulated_submission_time=3747.576620, global_step=13864, preemption_count=0, score=3747.576620, test/loss=0.288204, test/num_examples=3581, test/ssim=0.740642, total_duration=4298.068925, train/loss=0.268617, train/ssim=0.745497, validation/loss=0.286884, validation/num_examples=3554, validation/ssim=0.723334
I1010 03:23:20.150505 140158296893184 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.246200, loss=0.195650
I1010 03:23:20.155534 140216147011392 pytorch_submission_base.py:86] 14000) loss = 0.196, grad_norm = 0.246
I1010 03:24:06.518795 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:24:08.438479 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:24:10.407828 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:24:12.335970 140216147011392 submission_runner.py:381] Time since start: 4384.43s, 	Step: 14174, 	{'train/ssim': 0.7465243339538574, 'train/loss': 0.26850928579057964, 'validation/ssim': 0.7241201320255346, 'validation/loss': 0.2870658397942899, 'validation/num_examples': 3554, 'test/ssim': 0.7413347463173694, 'test/loss': 0.2884897478663432, 'test/num_examples': 3581, 'score': 3827.1162180900574, 'total_duration': 4384.426004648209, 'accumulated_submission_time': 3827.1162180900574, 'accumulated_eval_time': 507.46436762809753, 'accumulated_logging_time': 3.0210793018341064}
I1010 03:24:12.353406 140158288500480 logging_writer.py:48] [14174] accumulated_eval_time=507.464368, accumulated_logging_time=3.021079, accumulated_submission_time=3827.116218, global_step=14174, preemption_count=0, score=3827.116218, test/loss=0.288490, test/num_examples=3581, test/ssim=0.741335, total_duration=4384.426005, train/loss=0.268509, train/ssim=0.746524, validation/loss=0.287066, validation/num_examples=3554, validation/ssim=0.724120
I1010 03:25:32.928532 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:25:34.769398 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:25:36.683633 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:25:38.551463 140216147011392 submission_runner.py:381] Time since start: 4470.64s, 	Step: 14484, 	{'train/ssim': 0.7427704674857003, 'train/loss': 0.2697871242250715, 'validation/ssim': 0.7204860500492403, 'validation/loss': 0.28808318971581315, 'validation/num_examples': 3554, 'test/ssim': 0.7379321173162176, 'test/loss': 0.2896098563272305, 'test/num_examples': 3581, 'score': 3906.7040774822235, 'total_duration': 4470.641440629959, 'accumulated_submission_time': 3906.7040774822235, 'accumulated_eval_time': 513.087394952774, 'accumulated_logging_time': 3.046846628189087}
I1010 03:25:38.571337 140158296893184 logging_writer.py:48] [14484] accumulated_eval_time=513.087395, accumulated_logging_time=3.046847, accumulated_submission_time=3906.704077, global_step=14484, preemption_count=0, score=3906.704077, test/loss=0.289610, test/num_examples=3581, test/ssim=0.737932, total_duration=4470.641441, train/loss=0.269787, train/ssim=0.742770, validation/loss=0.288083, validation/num_examples=3554, validation/ssim=0.720486
I1010 03:25:40.934321 140158288500480 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.163885, loss=0.306764
I1010 03:25:40.937993 140216147011392 pytorch_submission_base.py:86] 14500) loss = 0.307, grad_norm = 0.164
I1010 03:26:59.133258 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:27:01.189069 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:27:03.222002 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:27:05.115738 140216147011392 submission_runner.py:381] Time since start: 4557.21s, 	Step: 14795, 	{'train/ssim': 0.7467835290091378, 'train/loss': 0.26850717408316477, 'validation/ssim': 0.7245136834156936, 'validation/loss': 0.28711073171844226, 'validation/num_examples': 3554, 'test/ssim': 0.7417798717406102, 'test/loss': 0.288411958295518, 'test/num_examples': 3581, 'score': 3986.2477135658264, 'total_duration': 4557.205749750137, 'accumulated_submission_time': 3986.2477135658264, 'accumulated_eval_time': 519.0699920654297, 'accumulated_logging_time': 3.075533628463745}
I1010 03:27:05.135998 140158296893184 logging_writer.py:48] [14795] accumulated_eval_time=519.069992, accumulated_logging_time=3.075534, accumulated_submission_time=3986.247714, global_step=14795, preemption_count=0, score=3986.247714, test/loss=0.288412, test/num_examples=3581, test/ssim=0.741780, total_duration=4557.205750, train/loss=0.268507, train/ssim=0.746784, validation/loss=0.287111, validation/num_examples=3554, validation/ssim=0.724514
I1010 03:27:57.715626 140158288500480 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.550820, loss=0.229998
I1010 03:27:57.719305 140216147011392 pytorch_submission_base.py:86] 15000) loss = 0.230, grad_norm = 0.551
I1010 03:28:25.791743 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:28:27.666268 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:28:29.587276 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:28:31.475123 140216147011392 submission_runner.py:381] Time since start: 4643.57s, 	Step: 15103, 	{'train/ssim': 0.7452718189784459, 'train/loss': 0.2685172898428781, 'validation/ssim': 0.723409967202448, 'validation/loss': 0.286828774713571, 'validation/num_examples': 3554, 'test/ssim': 0.7406030743856464, 'test/loss': 0.2881737149517418, 'test/num_examples': 3581, 'score': 4065.9376904964447, 'total_duration': 4643.565129995346, 'accumulated_submission_time': 4065.9376904964447, 'accumulated_eval_time': 524.7535533905029, 'accumulated_logging_time': 3.104548215866089}
I1010 03:28:31.495867 140158296893184 logging_writer.py:48] [15103] accumulated_eval_time=524.753553, accumulated_logging_time=3.104548, accumulated_submission_time=4065.937690, global_step=15103, preemption_count=0, score=4065.937690, test/loss=0.288174, test/num_examples=3581, test/ssim=0.740603, total_duration=4643.565130, train/loss=0.268517, train/ssim=0.745272, validation/loss=0.286829, validation/num_examples=3554, validation/ssim=0.723410
I1010 03:29:51.957264 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:29:53.782186 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:29:55.672131 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:29:57.572565 140216147011392 submission_runner.py:381] Time since start: 4729.66s, 	Step: 15411, 	{'train/ssim': 0.7468591417585101, 'train/loss': 0.26774283817836214, 'validation/ssim': 0.7242696114852982, 'validation/loss': 0.2864295388448755, 'validation/num_examples': 3554, 'test/ssim': 0.7415744554593688, 'test/loss': 0.2877630869192439, 'test/num_examples': 3581, 'score': 4145.391197681427, 'total_duration': 4729.662580251694, 'accumulated_submission_time': 4145.391197681427, 'accumulated_eval_time': 530.3691346645355, 'accumulated_logging_time': 3.1346733570098877}
I1010 03:29:57.592521 140158288500480 logging_writer.py:48] [15411] accumulated_eval_time=530.369135, accumulated_logging_time=3.134673, accumulated_submission_time=4145.391198, global_step=15411, preemption_count=0, score=4145.391198, test/loss=0.287763, test/num_examples=3581, test/ssim=0.741574, total_duration=4729.662580, train/loss=0.267743, train/ssim=0.746859, validation/loss=0.286430, validation/num_examples=3554, validation/ssim=0.724270
I1010 03:30:19.349145 140158296893184 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.194799, loss=0.332644
I1010 03:30:19.352525 140216147011392 pytorch_submission_base.py:86] 15500) loss = 0.333, grad_norm = 0.195
I1010 03:31:18.104547 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:31:20.135376 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:31:22.186223 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:31:24.229414 140216147011392 submission_runner.py:381] Time since start: 4816.32s, 	Step: 15720, 	{'train/ssim': 0.745774473462786, 'train/loss': 0.2684028829847063, 'validation/ssim': 0.7235231759109454, 'validation/loss': 0.2868624522435108, 'validation/num_examples': 3554, 'test/ssim': 0.7407758340460067, 'test/loss': 0.28827710485897795, 'test/num_examples': 3581, 'score': 4224.880594968796, 'total_duration': 4816.319433450699, 'accumulated_submission_time': 4224.880594968796, 'accumulated_eval_time': 536.4942610263824, 'accumulated_logging_time': 3.164893388748169}
I1010 03:31:24.248403 140158288500480 logging_writer.py:48] [15720] accumulated_eval_time=536.494261, accumulated_logging_time=3.164893, accumulated_submission_time=4224.880595, global_step=15720, preemption_count=0, score=4224.880595, test/loss=0.288277, test/num_examples=3581, test/ssim=0.740776, total_duration=4816.319433, train/loss=0.268403, train/ssim=0.745774, validation/loss=0.286862, validation/num_examples=3554, validation/ssim=0.723523
I1010 03:32:37.405510 140158296893184 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.284610, loss=0.313251
I1010 03:32:37.409789 140216147011392 pytorch_submission_base.py:86] 16000) loss = 0.313, grad_norm = 0.285
I1010 03:32:44.722111 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:32:46.680164 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:32:48.653668 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:32:50.588402 140216147011392 submission_runner.py:381] Time since start: 4902.68s, 	Step: 16027, 	{'train/ssim': 0.7457030841282436, 'train/loss': 0.2685463087899344, 'validation/ssim': 0.7235373956941122, 'validation/loss': 0.2868583133935794, 'validation/num_examples': 3554, 'test/ssim': 0.7408143538597808, 'test/loss': 0.2882149618319778, 'test/num_examples': 3581, 'score': 4304.353451967239, 'total_duration': 4902.678441524506, 'accumulated_submission_time': 4304.353451967239, 'accumulated_eval_time': 542.3609712123871, 'accumulated_logging_time': 3.192406415939331}
I1010 03:32:50.606748 140158288500480 logging_writer.py:48] [16027] accumulated_eval_time=542.360971, accumulated_logging_time=3.192406, accumulated_submission_time=4304.353452, global_step=16027, preemption_count=0, score=4304.353452, test/loss=0.288215, test/num_examples=3581, test/ssim=0.740814, total_duration=4902.678442, train/loss=0.268546, train/ssim=0.745703, validation/loss=0.286858, validation/num_examples=3554, validation/ssim=0.723537
I1010 03:34:11.234272 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:34:13.070650 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:34:14.965697 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:34:16.825205 140216147011392 submission_runner.py:381] Time since start: 4988.92s, 	Step: 16333, 	{'train/ssim': 0.7451808793204171, 'train/loss': 0.26800523485456196, 'validation/ssim': 0.7223644353193585, 'validation/loss': 0.28669811757548713, 'validation/num_examples': 3554, 'test/ssim': 0.7397627288554175, 'test/loss': 0.28804932663274924, 'test/num_examples': 3581, 'score': 4384.026094913483, 'total_duration': 4988.915132522583, 'accumulated_submission_time': 4384.026094913483, 'accumulated_eval_time': 547.9520001411438, 'accumulated_logging_time': 3.2197704315185547}
I1010 03:34:16.845721 140158296893184 logging_writer.py:48] [16333] accumulated_eval_time=547.952000, accumulated_logging_time=3.219770, accumulated_submission_time=4384.026095, global_step=16333, preemption_count=0, score=4384.026095, test/loss=0.288049, test/num_examples=3581, test/ssim=0.739763, total_duration=4988.915133, train/loss=0.268005, train/ssim=0.745181, validation/loss=0.286698, validation/num_examples=3554, validation/ssim=0.722364
I1010 03:34:59.072281 140158288500480 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.202093, loss=0.400278
I1010 03:34:59.076089 140216147011392 pytorch_submission_base.py:86] 16500) loss = 0.400, grad_norm = 0.202
I1010 03:35:37.306230 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:35:39.308098 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:35:41.367393 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:35:43.394693 140216147011392 submission_runner.py:381] Time since start: 5075.48s, 	Step: 16642, 	{'train/ssim': 0.7467686789376395, 'train/loss': 0.268008828163147, 'validation/ssim': 0.7243870105646807, 'validation/loss': 0.28655526714507423, 'validation/num_examples': 3554, 'test/ssim': 0.741610861796635, 'test/loss': 0.287915700376117, 'test/num_examples': 3581, 'score': 4463.493456125259, 'total_duration': 5075.484724998474, 'accumulated_submission_time': 4463.493456125259, 'accumulated_eval_time': 554.0406143665314, 'accumulated_logging_time': 3.2496731281280518}
I1010 03:35:43.413109 140158296893184 logging_writer.py:48] [16642] accumulated_eval_time=554.040614, accumulated_logging_time=3.249673, accumulated_submission_time=4463.493456, global_step=16642, preemption_count=0, score=4463.493456, test/loss=0.287916, test/num_examples=3581, test/ssim=0.741611, total_duration=5075.484725, train/loss=0.268009, train/ssim=0.746769, validation/loss=0.286555, validation/num_examples=3554, validation/ssim=0.724387
I1010 03:37:03.984390 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:37:05.927562 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:37:07.895629 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:37:09.822135 140216147011392 submission_runner.py:381] Time since start: 5161.91s, 	Step: 16950, 	{'train/ssim': 0.7458487919398716, 'train/loss': 0.2679550477436611, 'validation/ssim': 0.7235474251063942, 'validation/loss': 0.2863950541533308, 'validation/num_examples': 3554, 'test/ssim': 0.7408498739004468, 'test/loss': 0.28773135068329375, 'test/num_examples': 3581, 'score': 4543.084426403046, 'total_duration': 5161.912168264389, 'accumulated_submission_time': 4543.084426403046, 'accumulated_eval_time': 559.8785591125488, 'accumulated_logging_time': 3.276616334915161}
I1010 03:37:09.840665 140158288500480 logging_writer.py:48] [16950] accumulated_eval_time=559.878559, accumulated_logging_time=3.276616, accumulated_submission_time=4543.084426, global_step=16950, preemption_count=0, score=4543.084426, test/loss=0.287731, test/num_examples=3581, test/ssim=0.740850, total_duration=5161.912168, train/loss=0.267955, train/ssim=0.745849, validation/loss=0.286395, validation/num_examples=3554, validation/ssim=0.723547
I1010 03:37:21.493214 140158296893184 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.381571, loss=0.235092
I1010 03:37:21.497018 140216147011392 pytorch_submission_base.py:86] 17000) loss = 0.235, grad_norm = 0.382
I1010 03:38:30.443283 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:38:32.403836 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:38:34.384502 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:38:36.325480 140216147011392 submission_runner.py:381] Time since start: 5248.42s, 	Step: 17256, 	{'train/ssim': 0.7448173250470843, 'train/loss': 0.26873312677655903, 'validation/ssim': 0.7222584395443514, 'validation/loss': 0.287323101088782, 'validation/num_examples': 3554, 'test/ssim': 0.7397636151520176, 'test/loss': 0.28864522474300125, 'test/num_examples': 3581, 'score': 4622.705796718597, 'total_duration': 5248.415489912033, 'accumulated_submission_time': 4622.705796718597, 'accumulated_eval_time': 565.7610197067261, 'accumulated_logging_time': 3.304469347000122}
I1010 03:38:36.344508 140158288500480 logging_writer.py:48] [17256] accumulated_eval_time=565.761020, accumulated_logging_time=3.304469, accumulated_submission_time=4622.705797, global_step=17256, preemption_count=0, score=4622.705797, test/loss=0.288645, test/num_examples=3581, test/ssim=0.739764, total_duration=5248.415490, train/loss=0.268733, train/ssim=0.744817, validation/loss=0.287323, validation/num_examples=3554, validation/ssim=0.722258
I1010 03:39:40.131796 140158296893184 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.274380, loss=0.290122
I1010 03:39:40.135867 140216147011392 pytorch_submission_base.py:86] 17500) loss = 0.290, grad_norm = 0.274
I1010 03:39:57.029530 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:39:58.982765 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:40:00.944541 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:40:02.916613 140216147011392 submission_runner.py:381] Time since start: 5335.01s, 	Step: 17563, 	{'train/ssim': 0.7470796448843819, 'train/loss': 0.2674122708184378, 'validation/ssim': 0.7241617609559651, 'validation/loss': 0.28630331250879293, 'validation/num_examples': 3554, 'test/ssim': 0.7414779173066183, 'test/loss': 0.28767483223087126, 'test/num_examples': 3581, 'score': 4702.431264162064, 'total_duration': 5335.006631135941, 'accumulated_submission_time': 4702.431264162064, 'accumulated_eval_time': 571.6483502388, 'accumulated_logging_time': 3.3316338062286377}
I1010 03:40:02.934578 140158288500480 logging_writer.py:48] [17563] accumulated_eval_time=571.648350, accumulated_logging_time=3.331634, accumulated_submission_time=4702.431264, global_step=17563, preemption_count=0, score=4702.431264, test/loss=0.287675, test/num_examples=3581, test/ssim=0.741478, total_duration=5335.006631, train/loss=0.267412, train/ssim=0.747080, validation/loss=0.286303, validation/num_examples=3554, validation/ssim=0.724162
I1010 03:41:23.539136 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:41:25.462615 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:41:27.445991 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:41:29.383940 140216147011392 submission_runner.py:381] Time since start: 5421.47s, 	Step: 17872, 	{'train/ssim': 0.7452504294259208, 'train/loss': 0.2682562214987619, 'validation/ssim': 0.7233040401220456, 'validation/loss': 0.28660398879343696, 'validation/num_examples': 3554, 'test/ssim': 0.7405629865086568, 'test/loss': 0.28794627760882086, 'test/num_examples': 3581, 'score': 4782.042553663254, 'total_duration': 5421.473974704742, 'accumulated_submission_time': 4782.042553663254, 'accumulated_eval_time': 577.4933302402496, 'accumulated_logging_time': 3.358119487762451}
I1010 03:41:29.402785 140158296893184 logging_writer.py:48] [17872] accumulated_eval_time=577.493330, accumulated_logging_time=3.358119, accumulated_submission_time=4782.042554, global_step=17872, preemption_count=0, score=4782.042554, test/loss=0.287946, test/num_examples=3581, test/ssim=0.740563, total_duration=5421.473975, train/loss=0.268256, train/ssim=0.745250, validation/loss=0.286604, validation/num_examples=3554, validation/ssim=0.723304
I1010 03:42:01.810089 140158288500480 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.301900, loss=0.275785
I1010 03:42:01.814691 140216147011392 pytorch_submission_base.py:86] 18000) loss = 0.276, grad_norm = 0.302
I1010 03:42:49.889746 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:42:51.727348 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:42:53.638053 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:42:55.508257 140216147011392 submission_runner.py:381] Time since start: 5507.60s, 	Step: 18179, 	{'train/ssim': 0.7470302581787109, 'train/loss': 0.2679480825151716, 'validation/ssim': 0.7247494432989238, 'validation/loss': 0.2864546295492315, 'validation/num_examples': 3554, 'test/ssim': 0.7419618352502793, 'test/loss': 0.28781980990165107, 'test/num_examples': 3581, 'score': 4861.54571890831, 'total_duration': 5507.598253250122, 'accumulated_submission_time': 4861.54571890831, 'accumulated_eval_time': 583.1119477748871, 'accumulated_logging_time': 3.38723087310791}
I1010 03:42:55.530046 140158296893184 logging_writer.py:48] [18179] accumulated_eval_time=583.111948, accumulated_logging_time=3.387231, accumulated_submission_time=4861.545719, global_step=18179, preemption_count=0, score=4861.545719, test/loss=0.287820, test/num_examples=3581, test/ssim=0.741962, total_duration=5507.598253, train/loss=0.267948, train/ssim=0.747030, validation/loss=0.286455, validation/num_examples=3554, validation/ssim=0.724749
I1010 03:44:16.015017 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:44:17.933674 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:44:19.845033 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:44:21.743433 140216147011392 submission_runner.py:381] Time since start: 5593.83s, 	Step: 18483, 	{'train/ssim': 0.7471036911010742, 'train/loss': 0.26785273211342947, 'validation/ssim': 0.7244155875202237, 'validation/loss': 0.28654403557721053, 'validation/num_examples': 3554, 'test/ssim': 0.7417219215782952, 'test/loss': 0.2879024400154461, 'test/num_examples': 3581, 'score': 4941.012405872345, 'total_duration': 5593.833423614502, 'accumulated_submission_time': 4941.012405872345, 'accumulated_eval_time': 588.8404824733734, 'accumulated_logging_time': 3.4180259704589844}
I1010 03:44:21.764423 140158288500480 logging_writer.py:48] [18483] accumulated_eval_time=588.840482, accumulated_logging_time=3.418026, accumulated_submission_time=4941.012406, global_step=18483, preemption_count=0, score=4941.012406, test/loss=0.287902, test/num_examples=3581, test/ssim=0.741722, total_duration=5593.833424, train/loss=0.267853, train/ssim=0.747104, validation/loss=0.286544, validation/num_examples=3554, validation/ssim=0.724416
I1010 03:44:24.288846 140158296893184 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.384199, loss=0.268465
I1010 03:44:24.292479 140216147011392 pytorch_submission_base.py:86] 18500) loss = 0.268, grad_norm = 0.384
I1010 03:45:42.448645 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:45:44.433829 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:45:46.480689 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:45:48.511622 140216147011392 submission_runner.py:381] Time since start: 5680.60s, 	Step: 18793, 	{'train/ssim': 0.7472687448774066, 'train/loss': 0.2678966522216797, 'validation/ssim': 0.7246244878130276, 'validation/loss': 0.28676080140224924, 'validation/num_examples': 3554, 'test/ssim': 0.7418972719517942, 'test/loss': 0.28813720634948337, 'test/num_examples': 3581, 'score': 5020.715856313705, 'total_duration': 5680.601626873016, 'accumulated_submission_time': 5020.715856313705, 'accumulated_eval_time': 594.9036073684692, 'accumulated_logging_time': 3.44805908203125}
I1010 03:45:48.529952 140158288500480 logging_writer.py:48] [18793] accumulated_eval_time=594.903607, accumulated_logging_time=3.448059, accumulated_submission_time=5020.715856, global_step=18793, preemption_count=0, score=5020.715856, test/loss=0.288137, test/num_examples=3581, test/ssim=0.741897, total_duration=5680.601627, train/loss=0.267897, train/ssim=0.747269, validation/loss=0.286761, validation/num_examples=3554, validation/ssim=0.724624
I1010 03:46:41.189047 140158296893184 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.222571, loss=0.324591
I1010 03:46:41.192990 140216147011392 pytorch_submission_base.py:86] 19000) loss = 0.325, grad_norm = 0.223
I1010 03:47:09.052037 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:47:10.989459 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:47:12.964660 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:47:14.895740 140216147011392 submission_runner.py:381] Time since start: 5766.99s, 	Step: 19103, 	{'train/ssim': 0.7479588644845145, 'train/loss': 0.2675520862851824, 'validation/ssim': 0.7256016685644696, 'validation/loss': 0.28612731693162635, 'validation/num_examples': 3554, 'test/ssim': 0.7427907271013683, 'test/loss': 0.2874564282956053, 'test/num_examples': 3581, 'score': 5100.271804571152, 'total_duration': 5766.985763788223, 'accumulated_submission_time': 5100.271804571152, 'accumulated_eval_time': 600.7474706172943, 'accumulated_logging_time': 3.4747776985168457}
I1010 03:47:14.914507 140158288500480 logging_writer.py:48] [19103] accumulated_eval_time=600.747471, accumulated_logging_time=3.474778, accumulated_submission_time=5100.271805, global_step=19103, preemption_count=0, score=5100.271805, test/loss=0.287456, test/num_examples=3581, test/ssim=0.742791, total_duration=5766.985764, train/loss=0.267552, train/ssim=0.747959, validation/loss=0.286127, validation/num_examples=3554, validation/ssim=0.725602
I1010 03:48:35.468983 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:48:37.405064 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:48:39.366654 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:48:41.296981 140216147011392 submission_runner.py:381] Time since start: 5853.39s, 	Step: 19411, 	{'train/ssim': 0.7477327755519322, 'train/loss': 0.26737667833055767, 'validation/ssim': 0.7252926115380557, 'validation/loss': 0.28595418935420475, 'validation/num_examples': 3554, 'test/ssim': 0.7425309740208741, 'test/loss': 0.28725598891065696, 'test/num_examples': 3581, 'score': 5179.874560594559, 'total_duration': 5853.386994838715, 'accumulated_submission_time': 5179.874560594559, 'accumulated_eval_time': 606.575751543045, 'accumulated_logging_time': 3.502861738204956}
I1010 03:48:41.315217 140158296893184 logging_writer.py:48] [19411] accumulated_eval_time=606.575752, accumulated_logging_time=3.502862, accumulated_submission_time=5179.874561, global_step=19411, preemption_count=0, score=5179.874561, test/loss=0.287256, test/num_examples=3581, test/ssim=0.742531, total_duration=5853.386995, train/loss=0.267377, train/ssim=0.747733, validation/loss=0.285954, validation/num_examples=3554, validation/ssim=0.725293
I1010 03:49:03.199518 140158288500480 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.301632, loss=0.332932
I1010 03:49:03.203469 140216147011392 pytorch_submission_base.py:86] 19500) loss = 0.333, grad_norm = 0.302
I1010 03:50:02.064733 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:50:03.906478 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:50:05.808458 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:50:07.661438 140216147011392 submission_runner.py:381] Time since start: 5939.75s, 	Step: 19720, 	{'train/ssim': 0.7469757625034877, 'train/loss': 0.26691690513065885, 'validation/ssim': 0.7239631648538618, 'validation/loss': 0.28590320078388787, 'validation/num_examples': 3554, 'test/ssim': 0.741308634655997, 'test/loss': 0.2872242526747068, 'test/num_examples': 3581, 'score': 5259.533682107925, 'total_duration': 5939.751412153244, 'accumulated_submission_time': 5259.533682107925, 'accumulated_eval_time': 612.1727998256683, 'accumulated_logging_time': 3.5295193195343018}
I1010 03:50:07.683074 140158296893184 logging_writer.py:48] [19720] accumulated_eval_time=612.172800, accumulated_logging_time=3.529519, accumulated_submission_time=5259.533682, global_step=19720, preemption_count=0, score=5259.533682, test/loss=0.287224, test/num_examples=3581, test/ssim=0.741309, total_duration=5939.751412, train/loss=0.266917, train/ssim=0.746976, validation/loss=0.285903, validation/num_examples=3554, validation/ssim=0.723963
I1010 03:51:20.314410 140158288500480 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.263546, loss=0.211657
I1010 03:51:20.318175 140216147011392 pytorch_submission_base.py:86] 20000) loss = 0.212, grad_norm = 0.264
I1010 03:51:28.209668 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:51:30.214127 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:51:32.273195 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:51:34.310347 140216147011392 submission_runner.py:381] Time since start: 6026.40s, 	Step: 20029, 	{'train/ssim': 0.7463560104370117, 'train/loss': 0.26722730909075054, 'validation/ssim': 0.7237398386940771, 'validation/loss': 0.28585720974606077, 'validation/num_examples': 3554, 'test/ssim': 0.7411081952710485, 'test/loss': 0.28717789254485476, 'test/num_examples': 3581, 'score': 5339.068736076355, 'total_duration': 6026.400379896164, 'accumulated_submission_time': 5339.068736076355, 'accumulated_eval_time': 618.2736349105835, 'accumulated_logging_time': 3.560518980026245}
I1010 03:51:34.329503 140158296893184 logging_writer.py:48] [20029] accumulated_eval_time=618.273635, accumulated_logging_time=3.560519, accumulated_submission_time=5339.068736, global_step=20029, preemption_count=0, score=5339.068736, test/loss=0.287178, test/num_examples=3581, test/ssim=0.741108, total_duration=6026.400380, train/loss=0.267227, train/ssim=0.746356, validation/loss=0.285857, validation/num_examples=3554, validation/ssim=0.723740
I1010 03:52:55.018347 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:52:56.952545 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:52:58.903591 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:53:00.823560 140216147011392 submission_runner.py:381] Time since start: 6112.91s, 	Step: 20337, 	{'train/ssim': 0.7448810168675014, 'train/loss': 0.2684384754725865, 'validation/ssim': 0.723300399307998, 'validation/loss': 0.2868167875050559, 'validation/num_examples': 3554, 'test/ssim': 0.7403632970669854, 'test/loss': 0.28814525119554596, 'test/num_examples': 3581, 'score': 5418.749127864838, 'total_duration': 6112.913596153259, 'accumulated_submission_time': 5418.749127864838, 'accumulated_eval_time': 624.0790724754333, 'accumulated_logging_time': 3.5886647701263428}
I1010 03:53:00.842445 140158288500480 logging_writer.py:48] [20337] accumulated_eval_time=624.079072, accumulated_logging_time=3.588665, accumulated_submission_time=5418.749128, global_step=20337, preemption_count=0, score=5418.749128, test/loss=0.288145, test/num_examples=3581, test/ssim=0.740363, total_duration=6112.913596, train/loss=0.268438, train/ssim=0.744881, validation/loss=0.286817, validation/num_examples=3554, validation/ssim=0.723300
I1010 03:53:42.702556 140158296893184 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.495156, loss=0.278964
I1010 03:53:42.706160 140216147011392 pytorch_submission_base.py:86] 20500) loss = 0.279, grad_norm = 0.495
I1010 03:54:21.441308 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:54:23.387197 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:54:25.306334 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:54:27.165993 140216147011392 submission_runner.py:381] Time since start: 6199.26s, 	Step: 20644, 	{'train/ssim': 0.7476414952959333, 'train/loss': 0.26786511284964426, 'validation/ssim': 0.7247979416898214, 'validation/loss': 0.28677907416709164, 'validation/num_examples': 3554, 'test/ssim': 0.7420561235731988, 'test/loss': 0.2880790516571837, 'test/num_examples': 3581, 'score': 5498.319918632507, 'total_duration': 6199.255992412567, 'accumulated_submission_time': 5498.319918632507, 'accumulated_eval_time': 629.8040223121643, 'accumulated_logging_time': 3.61690354347229}
I1010 03:54:27.187510 140158288500480 logging_writer.py:48] [20644] accumulated_eval_time=629.804022, accumulated_logging_time=3.616904, accumulated_submission_time=5498.319919, global_step=20644, preemption_count=0, score=5498.319919, test/loss=0.288079, test/num_examples=3581, test/ssim=0.742056, total_duration=6199.255992, train/loss=0.267865, train/ssim=0.747641, validation/loss=0.286779, validation/num_examples=3554, validation/ssim=0.724798
I1010 03:55:47.860279 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:55:49.813316 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:55:51.872899 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:55:53.901954 140216147011392 submission_runner.py:381] Time since start: 6285.99s, 	Step: 20954, 	{'train/ssim': 0.746704237801688, 'train/loss': 0.2677987132753645, 'validation/ssim': 0.7242264025789603, 'validation/loss': 0.28662052701951146, 'validation/num_examples': 3554, 'test/ssim': 0.7415044380279601, 'test/loss': 0.2879053034352311, 'test/num_examples': 3581, 'score': 5578.015944719315, 'total_duration': 6285.991969347, 'accumulated_submission_time': 5578.015944719315, 'accumulated_eval_time': 635.8459193706512, 'accumulated_logging_time': 3.6472599506378174}
I1010 03:55:53.920989 140158296893184 logging_writer.py:48] [20954] accumulated_eval_time=635.845919, accumulated_logging_time=3.647260, accumulated_submission_time=5578.015945, global_step=20954, preemption_count=0, score=5578.015945, test/loss=0.287905, test/num_examples=3581, test/ssim=0.741504, total_duration=6285.991969, train/loss=0.267799, train/ssim=0.746704, validation/loss=0.286621, validation/num_examples=3554, validation/ssim=0.724226
I1010 03:56:04.262535 140158288500480 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.297875, loss=0.267323
I1010 03:56:04.266533 140216147011392 pytorch_submission_base.py:86] 21000) loss = 0.267, grad_norm = 0.298
I1010 03:57:14.347032 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:57:16.293863 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:57:18.249762 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:57:20.183502 140216147011392 submission_runner.py:381] Time since start: 6372.27s, 	Step: 21262, 	{'train/ssim': 0.7472938128880092, 'train/loss': 0.2669185059411185, 'validation/ssim': 0.7247526032507386, 'validation/loss': 0.2856641435596335, 'validation/num_examples': 3554, 'test/ssim': 0.7419476545046775, 'test/loss': 0.2869982811300091, 'test/num_examples': 3581, 'score': 5657.501625776291, 'total_duration': 6372.2735340595245, 'accumulated_submission_time': 5657.501625776291, 'accumulated_eval_time': 641.6825890541077, 'accumulated_logging_time': 3.6748082637786865}
I1010 03:57:20.202414 140158296893184 logging_writer.py:48] [21262] accumulated_eval_time=641.682589, accumulated_logging_time=3.674808, accumulated_submission_time=5657.501626, global_step=21262, preemption_count=0, score=5657.501626, test/loss=0.286998, test/num_examples=3581, test/ssim=0.741948, total_duration=6372.273534, train/loss=0.266919, train/ssim=0.747294, validation/loss=0.285664, validation/num_examples=3554, validation/ssim=0.724753
I1010 03:58:22.265350 140158288500480 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.410425, loss=0.247882
I1010 03:58:22.269403 140216147011392 pytorch_submission_base.py:86] 21500) loss = 0.248, grad_norm = 0.410
I1010 03:58:40.807067 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 03:58:42.754569 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 03:58:44.720449 140216147011392 spec.py:349] Evaluating on the test split.
I1010 03:58:46.646066 140216147011392 submission_runner.py:381] Time since start: 6458.74s, 	Step: 21567, 	{'train/ssim': 0.7468853678022113, 'train/loss': 0.266995940889631, 'validation/ssim': 0.7241129877866489, 'validation/loss': 0.2857368224513752, 'validation/num_examples': 3554, 'test/ssim': 0.7414209216175649, 'test/loss': 0.28708101350879645, 'test/num_examples': 3581, 'score': 5737.136184453964, 'total_duration': 6458.7360944747925, 'accumulated_submission_time': 5737.136184453964, 'accumulated_eval_time': 647.521933555603, 'accumulated_logging_time': 3.701996326446533}
I1010 03:58:46.665510 140158296893184 logging_writer.py:48] [21567] accumulated_eval_time=647.521934, accumulated_logging_time=3.701996, accumulated_submission_time=5737.136184, global_step=21567, preemption_count=0, score=5737.136184, test/loss=0.287081, test/num_examples=3581, test/ssim=0.741421, total_duration=6458.736094, train/loss=0.266996, train/ssim=0.746885, validation/loss=0.285737, validation/num_examples=3554, validation/ssim=0.724113
I1010 04:00:07.214698 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:00:09.152794 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:00:11.126195 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:00:13.058933 140216147011392 submission_runner.py:381] Time since start: 6545.15s, 	Step: 21876, 	{'train/ssim': 0.7475599561418805, 'train/loss': 0.2664895398276193, 'validation/ssim': 0.7243839880020752, 'validation/loss': 0.28561339542043296, 'validation/num_examples': 3554, 'test/ssim': 0.7417384885070512, 'test/loss': 0.2869405695860095, 'test/num_examples': 3581, 'score': 5816.7012441158295, 'total_duration': 6545.148970127106, 'accumulated_submission_time': 5816.7012441158295, 'accumulated_eval_time': 653.3664782047272, 'accumulated_logging_time': 3.729487657546997}
I1010 04:00:13.078936 140158288500480 logging_writer.py:48] [21876] accumulated_eval_time=653.366478, accumulated_logging_time=3.729488, accumulated_submission_time=5816.701244, global_step=21876, preemption_count=0, score=5816.701244, test/loss=0.286941, test/num_examples=3581, test/ssim=0.741738, total_duration=6545.148970, train/loss=0.266490, train/ssim=0.747560, validation/loss=0.285613, validation/num_examples=3554, validation/ssim=0.724384
I1010 04:00:44.118262 140158296893184 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.235574, loss=0.299343
I1010 04:00:44.122623 140216147011392 pytorch_submission_base.py:86] 22000) loss = 0.299, grad_norm = 0.236
I1010 04:01:33.628694 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:01:35.599071 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:01:37.564665 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:01:39.510265 140216147011392 submission_runner.py:381] Time since start: 6631.60s, 	Step: 22186, 	{'train/ssim': 0.747598375592913, 'train/loss': 0.2667443411690848, 'validation/ssim': 0.7247167446671005, 'validation/loss': 0.2856602279671673, 'validation/num_examples': 3554, 'test/ssim': 0.742061373176138, 'test/loss': 0.28696470412419717, 'test/num_examples': 3581, 'score': 5896.258113622665, 'total_duration': 6631.60024356842, 'accumulated_submission_time': 5896.258113622665, 'accumulated_eval_time': 659.2483315467834, 'accumulated_logging_time': 3.7586512565612793}
I1010 04:01:39.530241 140158288500480 logging_writer.py:48] [22186] accumulated_eval_time=659.248332, accumulated_logging_time=3.758651, accumulated_submission_time=5896.258114, global_step=22186, preemption_count=0, score=5896.258114, test/loss=0.286965, test/num_examples=3581, test/ssim=0.742061, total_duration=6631.600244, train/loss=0.266744, train/ssim=0.747598, validation/loss=0.285660, validation/num_examples=3554, validation/ssim=0.724717
I1010 04:03:00.121907 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:03:02.117884 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:03:04.107646 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:03:06.036477 140216147011392 submission_runner.py:381] Time since start: 6718.13s, 	Step: 22494, 	{'train/ssim': 0.7476109095982143, 'train/loss': 0.2668040990829468, 'validation/ssim': 0.7249833484278277, 'validation/loss': 0.2855746173160963, 'validation/num_examples': 3554, 'test/ssim': 0.7421855910534767, 'test/loss': 0.2869016066239354, 'test/num_examples': 3581, 'score': 5975.872817516327, 'total_duration': 6718.126484632492, 'accumulated_submission_time': 5975.872817516327, 'accumulated_eval_time': 665.1630713939667, 'accumulated_logging_time': 3.78808856010437}
I1010 04:03:06.055506 140158296893184 logging_writer.py:48] [22494] accumulated_eval_time=665.163071, accumulated_logging_time=3.788089, accumulated_submission_time=5975.872818, global_step=22494, preemption_count=0, score=5975.872818, test/loss=0.286902, test/num_examples=3581, test/ssim=0.742186, total_duration=6718.126485, train/loss=0.266804, train/ssim=0.747611, validation/loss=0.285575, validation/num_examples=3554, validation/ssim=0.724983
I1010 04:03:07.001192 140158288500480 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.325472, loss=0.257244
I1010 04:03:07.005858 140216147011392 pytorch_submission_base.py:86] 22500) loss = 0.257, grad_norm = 0.325
I1010 04:04:26.732515 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:04:28.699470 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:04:30.660171 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:04:32.598343 140216147011392 submission_runner.py:381] Time since start: 6804.69s, 	Step: 22802, 	{'train/ssim': 0.7485455785478864, 'train/loss': 0.2671150990894863, 'validation/ssim': 0.7257631695800506, 'validation/loss': 0.28609005010859245, 'validation/num_examples': 3554, 'test/ssim': 0.7429628049951131, 'test/loss': 0.2874554738223436, 'test/num_examples': 3581, 'score': 6055.535458803177, 'total_duration': 6804.688360452652, 'accumulated_submission_time': 6055.535458803177, 'accumulated_eval_time': 671.0289905071259, 'accumulated_logging_time': 3.8156802654266357}
I1010 04:04:32.618004 140158296893184 logging_writer.py:48] [22802] accumulated_eval_time=671.028991, accumulated_logging_time=3.815680, accumulated_submission_time=6055.535459, global_step=22802, preemption_count=0, score=6055.535459, test/loss=0.287455, test/num_examples=3581, test/ssim=0.742963, total_duration=6804.688360, train/loss=0.267115, train/ssim=0.748546, validation/loss=0.286090, validation/num_examples=3554, validation/ssim=0.725763
I1010 04:05:23.644298 140158288500480 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.216590, loss=0.272217
I1010 04:05:23.647855 140216147011392 pytorch_submission_base.py:86] 23000) loss = 0.272, grad_norm = 0.217
I1010 04:05:53.193857 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:05:55.124440 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:05:57.094180 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:05:59.026963 140216147011392 submission_runner.py:381] Time since start: 6891.12s, 	Step: 23110, 	{'train/ssim': 0.7476380211966378, 'train/loss': 0.26638807569231304, 'validation/ssim': 0.7247617396331598, 'validation/loss': 0.2854382413521736, 'validation/num_examples': 3554, 'test/ssim': 0.7420140585730243, 'test/loss': 0.28672707437037487, 'test/num_examples': 3581, 'score': 6135.137357711792, 'total_duration': 6891.117000579834, 'accumulated_submission_time': 6135.137357711792, 'accumulated_eval_time': 676.8624894618988, 'accumulated_logging_time': 3.843453884124756}
I1010 04:05:59.046888 140158296893184 logging_writer.py:48] [23110] accumulated_eval_time=676.862489, accumulated_logging_time=3.843454, accumulated_submission_time=6135.137358, global_step=23110, preemption_count=0, score=6135.137358, test/loss=0.286727, test/num_examples=3581, test/ssim=0.742014, total_duration=6891.117001, train/loss=0.266388, train/ssim=0.747638, validation/loss=0.285438, validation/num_examples=3554, validation/ssim=0.724762
I1010 04:07:19.568340 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:07:21.396508 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:07:23.293329 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:07:25.158494 140216147011392 submission_runner.py:381] Time since start: 6977.25s, 	Step: 23418, 	{'train/ssim': 0.7481451034545898, 'train/loss': 0.2670062950679234, 'validation/ssim': 0.7256499608715532, 'validation/loss': 0.28578363782445837, 'validation/num_examples': 3554, 'test/ssim': 0.7428976962833357, 'test/loss': 0.28712604419374826, 'test/num_examples': 3581, 'score': 6214.6651265621185, 'total_duration': 6977.248491764069, 'accumulated_submission_time': 6214.6651265621185, 'accumulated_eval_time': 682.4527862071991, 'accumulated_logging_time': 3.8714585304260254}
I1010 04:07:25.179886 140158288500480 logging_writer.py:48] [23418] accumulated_eval_time=682.452786, accumulated_logging_time=3.871459, accumulated_submission_time=6214.665127, global_step=23418, preemption_count=0, score=6214.665127, test/loss=0.287126, test/num_examples=3581, test/ssim=0.742898, total_duration=6977.248492, train/loss=0.267006, train/ssim=0.748145, validation/loss=0.285784, validation/num_examples=3554, validation/ssim=0.725650
I1010 04:07:45.242270 140158296893184 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.295492, loss=0.342715
I1010 04:07:45.245870 140216147011392 pytorch_submission_base.py:86] 23500) loss = 0.343, grad_norm = 0.295
I1010 04:08:45.737213 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:08:47.782186 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:08:49.863737 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:08:51.897293 140216147011392 submission_runner.py:381] Time since start: 7063.99s, 	Step: 23726, 	{'train/ssim': 0.7479040282113212, 'train/loss': 0.26656854152679443, 'validation/ssim': 0.7253206389367614, 'validation/loss': 0.28537140150182894, 'validation/num_examples': 3554, 'test/ssim': 0.7425822428703575, 'test/loss': 0.2866672152615366, 'test/num_examples': 3581, 'score': 6294.213047981262, 'total_duration': 7063.9873015880585, 'accumulated_submission_time': 6294.213047981262, 'accumulated_eval_time': 688.6130077838898, 'accumulated_logging_time': 3.901848077774048}
I1010 04:08:51.917761 140158288500480 logging_writer.py:48] [23726] accumulated_eval_time=688.613008, accumulated_logging_time=3.901848, accumulated_submission_time=6294.213048, global_step=23726, preemption_count=0, score=6294.213048, test/loss=0.286667, test/num_examples=3581, test/ssim=0.742582, total_duration=7063.987302, train/loss=0.266569, train/ssim=0.747904, validation/loss=0.285371, validation/num_examples=3554, validation/ssim=0.725321
I1010 04:10:03.442152 140158296893184 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.328799, loss=0.294344
I1010 04:10:03.445834 140216147011392 pytorch_submission_base.py:86] 24000) loss = 0.294, grad_norm = 0.329
I1010 04:10:12.542719 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:10:14.381603 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:10:16.277540 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:10:18.137097 140216147011392 submission_runner.py:381] Time since start: 7150.23s, 	Step: 24033, 	{'train/ssim': 0.7491746629987445, 'train/loss': 0.2659675393785749, 'validation/ssim': 0.7259814810336944, 'validation/loss': 0.2852673978703573, 'validation/num_examples': 3554, 'test/ssim': 0.7432080364466979, 'test/loss': 0.2865626322627234, 'test/num_examples': 3581, 'score': 6373.879824399948, 'total_duration': 7150.227107524872, 'accumulated_submission_time': 6373.879824399948, 'accumulated_eval_time': 694.2075641155243, 'accumulated_logging_time': 3.9307491779327393}
I1010 04:10:18.159547 140158288500480 logging_writer.py:48] [24033] accumulated_eval_time=694.207564, accumulated_logging_time=3.930749, accumulated_submission_time=6373.879824, global_step=24033, preemption_count=0, score=6373.879824, test/loss=0.286563, test/num_examples=3581, test/ssim=0.743208, total_duration=7150.227108, train/loss=0.265968, train/ssim=0.749175, validation/loss=0.285267, validation/num_examples=3554, validation/ssim=0.725981
I1010 04:11:38.864425 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:11:40.872560 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:11:42.927879 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:11:44.946673 140216147011392 submission_runner.py:381] Time since start: 7237.04s, 	Step: 24340, 	{'train/ssim': 0.748117310660226, 'train/loss': 0.2663300207683018, 'validation/ssim': 0.7252353202377603, 'validation/loss': 0.28534619058191474, 'validation/num_examples': 3554, 'test/ssim': 0.7424775235182212, 'test/loss': 0.2866532731342502, 'test/num_examples': 3581, 'score': 6453.605158567429, 'total_duration': 7237.03667473793, 'accumulated_submission_time': 6453.605158567429, 'accumulated_eval_time': 700.289989233017, 'accumulated_logging_time': 3.962435722351074}
I1010 04:11:44.966390 140158296893184 logging_writer.py:48] [24340] accumulated_eval_time=700.289989, accumulated_logging_time=3.962436, accumulated_submission_time=6453.605159, global_step=24340, preemption_count=0, score=6453.605159, test/loss=0.286653, test/num_examples=3581, test/ssim=0.742478, total_duration=7237.036675, train/loss=0.266330, train/ssim=0.748117, validation/loss=0.285346, validation/num_examples=3554, validation/ssim=0.725235
I1010 04:12:25.979499 140158288500480 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.230102, loss=0.332771
I1010 04:12:25.983283 140216147011392 pytorch_submission_base.py:86] 24500) loss = 0.333, grad_norm = 0.230
I1010 04:13:05.553872 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:13:07.492861 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:13:09.464016 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:13:11.398699 140216147011392 submission_runner.py:381] Time since start: 7323.49s, 	Step: 24647, 	{'train/ssim': 0.7490226200648716, 'train/loss': 0.2660079002380371, 'validation/ssim': 0.7261561027187676, 'validation/loss': 0.28506569333739273, 'validation/num_examples': 3554, 'test/ssim': 0.7434158389110933, 'test/loss': 0.28627056344465585, 'test/num_examples': 3581, 'score': 6533.202595472336, 'total_duration': 7323.488739013672, 'accumulated_submission_time': 6533.202595472336, 'accumulated_eval_time': 706.1351006031036, 'accumulated_logging_time': 3.99021053314209}
I1010 04:13:11.418604 140158296893184 logging_writer.py:48] [24647] accumulated_eval_time=706.135101, accumulated_logging_time=3.990211, accumulated_submission_time=6533.202595, global_step=24647, preemption_count=0, score=6533.202595, test/loss=0.286271, test/num_examples=3581, test/ssim=0.743416, total_duration=7323.488739, train/loss=0.266008, train/ssim=0.749023, validation/loss=0.285066, validation/num_examples=3554, validation/ssim=0.726156
I1010 04:14:31.956140 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:14:33.896059 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:14:35.867816 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:14:37.806568 140216147011392 submission_runner.py:381] Time since start: 7409.90s, 	Step: 24955, 	{'train/ssim': 0.7486442838396344, 'train/loss': 0.2661147969109671, 'validation/ssim': 0.7259156029078151, 'validation/loss': 0.2850989930470069, 'validation/num_examples': 3554, 'test/ssim': 0.7431618808468305, 'test/loss': 0.286351216435266, 'test/num_examples': 3581, 'score': 6612.728678226471, 'total_duration': 7409.896553039551, 'accumulated_submission_time': 6612.728678226471, 'accumulated_eval_time': 711.9858059883118, 'accumulated_logging_time': 4.018469333648682}
I1010 04:14:37.827011 140158288500480 logging_writer.py:48] [24955] accumulated_eval_time=711.985806, accumulated_logging_time=4.018469, accumulated_submission_time=6612.728678, global_step=24955, preemption_count=0, score=6612.728678, test/loss=0.286351, test/num_examples=3581, test/ssim=0.743162, total_duration=7409.896553, train/loss=0.266115, train/ssim=0.748644, validation/loss=0.285099, validation/num_examples=3554, validation/ssim=0.725916
I1010 04:14:47.739330 140158296893184 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.296574, loss=0.280076
I1010 04:14:47.743376 140216147011392 pytorch_submission_base.py:86] 25000) loss = 0.280, grad_norm = 0.297
I1010 04:15:58.287762 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:16:00.108924 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:16:02.054307 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:16:03.943790 140216147011392 submission_runner.py:381] Time since start: 7496.03s, 	Step: 25264, 	{'train/ssim': 0.7498152596609933, 'train/loss': 0.2655347926276071, 'validation/ssim': 0.726628446820484, 'validation/loss': 0.28489943522043826, 'validation/num_examples': 3554, 'test/ssim': 0.743795787445895, 'test/loss': 0.2862165334403798, 'test/num_examples': 3581, 'score': 6692.2029547691345, 'total_duration': 7496.03379368782, 'accumulated_submission_time': 6692.2029547691345, 'accumulated_eval_time': 717.6420066356659, 'accumulated_logging_time': 4.04834508895874}
I1010 04:16:03.965395 140158288500480 logging_writer.py:48] [25264] accumulated_eval_time=717.642007, accumulated_logging_time=4.048345, accumulated_submission_time=6692.202955, global_step=25264, preemption_count=0, score=6692.202955, test/loss=0.286217, test/num_examples=3581, test/ssim=0.743796, total_duration=7496.033794, train/loss=0.265535, train/ssim=0.749815, validation/loss=0.284899, validation/num_examples=3554, validation/ssim=0.726628
I1010 04:17:05.082324 140158296893184 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.268087, loss=0.247555
I1010 04:17:05.086389 140216147011392 pytorch_submission_base.py:86] 25500) loss = 0.248, grad_norm = 0.268
I1010 04:17:24.498320 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:17:26.301701 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:17:28.345655 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:17:30.380887 140216147011392 submission_runner.py:381] Time since start: 7582.47s, 	Step: 25572, 	{'train/ssim': 0.7491157395499093, 'train/loss': 0.2655557564326695, 'validation/ssim': 0.7260582129071117, 'validation/loss': 0.28473881006106677, 'validation/num_examples': 3554, 'test/ssim': 0.7433313680274365, 'test/loss': 0.28602485475644024, 'test/num_examples': 3581, 'score': 6771.736690759659, 'total_duration': 7582.470921754837, 'accumulated_submission_time': 6771.736690759659, 'accumulated_eval_time': 723.5248582363129, 'accumulated_logging_time': 4.078721284866333}
I1010 04:17:30.400783 140158288500480 logging_writer.py:48] [25572] accumulated_eval_time=723.524858, accumulated_logging_time=4.078721, accumulated_submission_time=6771.736691, global_step=25572, preemption_count=0, score=6771.736691, test/loss=0.286025, test/num_examples=3581, test/ssim=0.743331, total_duration=7582.470922, train/loss=0.265556, train/ssim=0.749116, validation/loss=0.284739, validation/num_examples=3554, validation/ssim=0.726058
I1010 04:18:51.080970 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:18:53.093205 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:18:55.075940 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:18:57.011603 140216147011392 submission_runner.py:381] Time since start: 7669.10s, 	Step: 25879, 	{'train/ssim': 0.7492758887154716, 'train/loss': 0.26557297366006033, 'validation/ssim': 0.7262558472847496, 'validation/loss': 0.28475136400007034, 'validation/num_examples': 3554, 'test/ssim': 0.7435339890655543, 'test/loss': 0.285983948759512, 'test/num_examples': 3581, 'score': 6851.406276702881, 'total_duration': 7669.1016047000885, 'accumulated_submission_time': 6851.406276702881, 'accumulated_eval_time': 729.4555983543396, 'accumulated_logging_time': 4.106729745864868}
I1010 04:18:57.031771 140158296893184 logging_writer.py:48] [25879] accumulated_eval_time=729.455598, accumulated_logging_time=4.106730, accumulated_submission_time=6851.406277, global_step=25879, preemption_count=0, score=6851.406277, test/loss=0.285984, test/num_examples=3581, test/ssim=0.743534, total_duration=7669.101605, train/loss=0.265573, train/ssim=0.749276, validation/loss=0.284751, validation/num_examples=3554, validation/ssim=0.726256
I1010 04:19:27.695563 140158288500480 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.314836, loss=0.281362
I1010 04:19:27.699140 140216147011392 pytorch_submission_base.py:86] 26000) loss = 0.281, grad_norm = 0.315
I1010 04:20:17.653319 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:20:19.627654 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:20:21.603214 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:20:23.536221 140216147011392 submission_runner.py:381] Time since start: 7755.63s, 	Step: 26187, 	{'train/ssim': 0.7499720709664481, 'train/loss': 0.26514412675585064, 'validation/ssim': 0.7265425785646454, 'validation/loss': 0.28472275269722497, 'validation/num_examples': 3554, 'test/ssim': 0.7437231111246858, 'test/loss': 0.28595378058677745, 'test/num_examples': 3581, 'score': 6931.0653030872345, 'total_duration': 7755.6260805130005, 'accumulated_submission_time': 6931.0653030872345, 'accumulated_eval_time': 735.338615655899, 'accumulated_logging_time': 4.135505676269531}
I1010 04:20:23.555805 140158296893184 logging_writer.py:48] [26187] accumulated_eval_time=735.338616, accumulated_logging_time=4.135506, accumulated_submission_time=6931.065303, global_step=26187, preemption_count=0, score=6931.065303, test/loss=0.285954, test/num_examples=3581, test/ssim=0.743723, total_duration=7755.626081, train/loss=0.265144, train/ssim=0.749972, validation/loss=0.284723, validation/num_examples=3554, validation/ssim=0.726543
I1010 04:21:43.985166 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:21:45.845100 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:21:47.754849 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:21:49.620092 140216147011392 submission_runner.py:381] Time since start: 7841.71s, 	Step: 26497, 	{'train/ssim': 0.7497331074305943, 'train/loss': 0.2651022842952183, 'validation/ssim': 0.7264929810600732, 'validation/loss': 0.2845141958774444, 'validation/num_examples': 3554, 'test/ssim': 0.7437115210922228, 'test/loss': 0.28576588570755374, 'test/num_examples': 3581, 'score': 7010.517087697983, 'total_duration': 7841.710106134415, 'accumulated_submission_time': 7010.517087697983, 'accumulated_eval_time': 740.9738247394562, 'accumulated_logging_time': 4.163392066955566}
I1010 04:21:49.643358 140158288500480 logging_writer.py:48] [26497] accumulated_eval_time=740.973825, accumulated_logging_time=4.163392, accumulated_submission_time=7010.517088, global_step=26497, preemption_count=0, score=7010.517088, test/loss=0.285766, test/num_examples=3581, test/ssim=0.743712, total_duration=7841.710106, train/loss=0.265102, train/ssim=0.749733, validation/loss=0.284514, validation/num_examples=3554, validation/ssim=0.726493
I1010 04:21:50.400604 140158296893184 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.215370, loss=0.354329
I1010 04:21:50.404181 140216147011392 pytorch_submission_base.py:86] 26500) loss = 0.354, grad_norm = 0.215
I1010 04:23:10.065548 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:23:12.072367 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:23:14.137803 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:23:16.173712 140216147011392 submission_runner.py:381] Time since start: 7928.26s, 	Step: 26806, 	{'train/ssim': 0.7503066062927246, 'train/loss': 0.26491570472717285, 'validation/ssim': 0.7270376605805079, 'validation/loss': 0.28440565840206455, 'validation/num_examples': 3554, 'test/ssim': 0.7442757511431862, 'test/loss': 0.28559104665901636, 'test/num_examples': 3581, 'score': 7089.949461936951, 'total_duration': 7928.2637367248535, 'accumulated_submission_time': 7089.949461936951, 'accumulated_eval_time': 747.0822746753693, 'accumulated_logging_time': 4.195976257324219}
I1010 04:23:16.194116 140158288500480 logging_writer.py:48] [26806] accumulated_eval_time=747.082275, accumulated_logging_time=4.195976, accumulated_submission_time=7089.949462, global_step=26806, preemption_count=0, score=7089.949462, test/loss=0.285591, test/num_examples=3581, test/ssim=0.744276, total_duration=7928.263737, train/loss=0.264916, train/ssim=0.750307, validation/loss=0.284406, validation/num_examples=3554, validation/ssim=0.727038
I1010 04:24:06.368942 140158296893184 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.220975, loss=0.277802
I1010 04:24:06.372499 140216147011392 pytorch_submission_base.py:86] 27000) loss = 0.278, grad_norm = 0.221
I1010 04:24:36.722956 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:24:38.672491 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:24:40.634377 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:24:42.559447 140216147011392 submission_runner.py:381] Time since start: 8014.65s, 	Step: 27116, 	{'train/ssim': 0.7501269749232701, 'train/loss': 0.26504877635410856, 'validation/ssim': 0.7268484069446398, 'validation/loss': 0.28449580289704385, 'validation/num_examples': 3554, 'test/ssim': 0.7440946739301173, 'test/loss': 0.2857025155006458, 'test/num_examples': 3581, 'score': 7169.514883041382, 'total_duration': 8014.649475097656, 'accumulated_submission_time': 7169.514883041382, 'accumulated_eval_time': 752.9190111160278, 'accumulated_logging_time': 4.225061655044556}
I1010 04:24:42.579591 140158288500480 logging_writer.py:48] [27116] accumulated_eval_time=752.919011, accumulated_logging_time=4.225062, accumulated_submission_time=7169.514883, global_step=27116, preemption_count=0, score=7169.514883, test/loss=0.285703, test/num_examples=3581, test/ssim=0.744095, total_duration=8014.649475, train/loss=0.265049, train/ssim=0.750127, validation/loss=0.284496, validation/num_examples=3554, validation/ssim=0.726848
I1010 04:24:47.823568 140216147011392 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I1010 04:24:49.801075 140216147011392 spec.py:333] Evaluating on the validation split.
I1010 04:24:51.774756 140216147011392 spec.py:349] Evaluating on the test split.
I1010 04:24:53.707913 140216147011392 submission_runner.py:381] Time since start: 8025.80s, 	Step: 27142, 	{'train/ssim': 0.7500604901994977, 'train/loss': 0.26499431473868235, 'validation/ssim': 0.7267776515018289, 'validation/loss': 0.2844626920975925, 'validation/num_examples': 3554, 'test/ssim': 0.7440094531031834, 'test/loss': 0.2856966523077527, 'test/num_examples': 3581, 'score': 7173.9355709552765, 'total_duration': 8025.797947645187, 'accumulated_submission_time': 7173.9355709552765, 'accumulated_eval_time': 758.8035817146301, 'accumulated_logging_time': 4.253333330154419}
I1010 04:24:53.727864 140158296893184 logging_writer.py:48] [27142] accumulated_eval_time=758.803582, accumulated_logging_time=4.253333, accumulated_submission_time=7173.935571, global_step=27142, preemption_count=0, score=7173.935571, test/loss=0.285697, test/num_examples=3581, test/ssim=0.744009, total_duration=8025.797948, train/loss=0.264994, train/ssim=0.750060, validation/loss=0.284463, validation/num_examples=3554, validation/ssim=0.726778
I1010 04:24:54.134432 140158288500480 logging_writer.py:48] [27142] global_step=27142, preemption_count=0, score=7173.935571
I1010 04:24:54.234758 140216147011392 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/fastmri_targets_check/nesterov_run_1/fastmri_pytorch/trial_1/checkpoint_27142.
I1010 04:24:55.354002 140216147011392 submission_runner.py:549] Tuning trial 1/1
I1010 04:24:55.354226 140216147011392 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.028609, beta1=0.981543, beta2=0.9978504782314613, warmup_steps=1357, decay_steps_factor=0.984398, end_factor=0.01, weight_decay=0.000576)
I1010 04:24:55.359743 140216147011392 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ssim': 0.2844876561846052, 'train/loss': 0.8128893715994698, 'validation/ssim': 0.2764096064183842, 'validation/loss': 0.8214740570703785, 'validation/num_examples': 3554, 'test/ssim': 0.2970230797088802, 'test/loss': 0.8232938654094527, 'test/num_examples': 3581, 'score': 87.99606323242188, 'total_duration': 318.74846863746643, 'accumulated_submission_time': 87.99606323242188, 'accumulated_eval_time': 230.33046627044678, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (303, {'train/ssim': 0.6871239798409599, 'train/loss': 0.3260907105037144, 'validation/ssim': 0.6650636551684721, 'validation/loss': 0.3454317813115504, 'validation/num_examples': 3554, 'test/ssim': 0.6836658809079168, 'test/loss': 0.34659116010498114, 'test/num_examples': 3581, 'score': 167.40180373191833, 'total_duration': 405.46679639816284, 'accumulated_submission_time': 167.40180373191833, 'accumulated_eval_time': 236.52397465705872, 'accumulated_logging_time': 0.03457927703857422, 'global_step': 303, 'preemption_count': 0}), (511, {'train/ssim': 0.7030360358101981, 'train/loss': 0.309774569102696, 'validation/ssim': 0.6823016044861424, 'validation/loss': 0.3276162477578081, 'validation/num_examples': 3554, 'test/ssim': 0.7008656936129922, 'test/loss': 0.3290475665840547, 'test/num_examples': 3581, 'score': 247.0248794555664, 'total_duration': 492.0938313007355, 'accumulated_submission_time': 247.0248794555664, 'accumulated_eval_time': 242.54185032844543, 'accumulated_logging_time': 0.07579469680786133, 'global_step': 511, 'preemption_count': 0}), (715, {'train/ssim': 0.7100772857666016, 'train/loss': 0.30013860974993023, 'validation/ssim': 0.6900099634654615, 'validation/loss': 0.31704710196697383, 'validation/num_examples': 3554, 'test/ssim': 0.7081830946968375, 'test/loss': 0.31867980530927115, 'test/num_examples': 3581, 'score': 326.9715027809143, 'total_duration': 578.9726641178131, 'accumulated_submission_time': 326.9715027809143, 'accumulated_eval_time': 248.48882460594177, 'accumulated_logging_time': 0.10609912872314453, 'global_step': 715, 'preemption_count': 0}), (925, {'train/ssim': 0.7171282087053571, 'train/loss': 0.29336401394435335, 'validation/ssim': 0.6966693558314575, 'validation/loss': 0.31036036914831877, 'validation/num_examples': 3554, 'test/ssim': 0.7143259482555502, 'test/loss': 0.312050852153728, 'test/num_examples': 3581, 'score': 406.6623592376709, 'total_duration': 666.492814540863, 'accumulated_submission_time': 406.6623592376709, 'accumulated_eval_time': 255.3394331932068, 'accumulated_logging_time': 0.13835358619689941, 'global_step': 925, 'preemption_count': 0}), (1225, {'train/ssim': 0.7261755807059151, 'train/loss': 0.28606932503836496, 'validation/ssim': 0.7053904106728335, 'validation/loss': 0.30326352953977914, 'validation/num_examples': 3554, 'test/ssim': 0.7225815283789444, 'test/loss': 0.305056267562308, 'test/num_examples': 3581, 'score': 486.0998725891113, 'total_duration': 752.815033197403, 'accumulated_submission_time': 486.0998725891113, 'accumulated_eval_time': 261.2199556827545, 'accumulated_logging_time': 0.1702287197113037, 'global_step': 1225, 'preemption_count': 0}), (1535, {'train/ssim': 0.7169576372419085, 'train/loss': 0.28833866119384766, 'validation/ssim': 0.6994413205191333, 'validation/loss': 0.30437538606367825, 'validation/num_examples': 3554, 'test/ssim': 0.7161155174444988, 'test/loss': 0.3060349094504852, 'test/num_examples': 3581, 'score': 565.7556653022766, 'total_duration': 839.4230749607086, 'accumulated_submission_time': 565.7556653022766, 'accumulated_eval_time': 267.1721751689911, 'accumulated_logging_time': 0.19649291038513184, 'global_step': 1535, 'preemption_count': 0}), (1845, {'train/ssim': 0.7319788251604352, 'train/loss': 0.28032668999263216, 'validation/ssim': 0.7118229047596019, 'validation/loss': 0.2970356079857203, 'validation/num_examples': 3554, 'test/ssim': 0.7287340630235968, 'test/loss': 0.29878882115461813, 'test/num_examples': 3581, 'score': 645.4322462081909, 'total_duration': 925.9867289066315, 'accumulated_submission_time': 645.4322462081909, 'accumulated_eval_time': 273.041535615921, 'accumulated_logging_time': 0.22025728225708008, 'global_step': 1845, 'preemption_count': 0}), (2151, {'train/ssim': 0.733088493347168, 'train/loss': 0.27777103015354704, 'validation/ssim': 0.7120736400666503, 'validation/loss': 0.2949073117438098, 'validation/num_examples': 3554, 'test/ssim': 0.7291287377172927, 'test/loss': 0.29670673999930186, 'test/num_examples': 3581, 'score': 724.9572834968567, 'total_duration': 1012.1303761005402, 'accumulated_submission_time': 724.9572834968567, 'accumulated_eval_time': 278.63119864463806, 'accumulated_logging_time': 0.24489331245422363, 'global_step': 2151, 'preemption_count': 0}), (2452, {'train/ssim': 0.733717509678432, 'train/loss': 0.2761002097811018, 'validation/ssim': 0.712606229336663, 'validation/loss': 0.29347729615837786, 'validation/num_examples': 3554, 'test/ssim': 0.7297807111316671, 'test/loss': 0.2951334612691113, 'test/num_examples': 3581, 'score': 802.7604639530182, 'total_duration': 1098.8546028137207, 'accumulated_submission_time': 802.7604639530182, 'accumulated_eval_time': 284.7772524356842, 'accumulated_logging_time': 2.008593797683716, 'global_step': 2452, 'preemption_count': 0}), (2761, {'train/ssim': 0.7319229670933315, 'train/loss': 0.27739221709115164, 'validation/ssim': 0.7104972362786649, 'validation/loss': 0.2946034068127462, 'validation/num_examples': 3554, 'test/ssim': 0.7279895057028414, 'test/loss': 0.29615737246055573, 'test/num_examples': 3581, 'score': 882.4432985782623, 'total_duration': 1185.344166278839, 'accumulated_submission_time': 882.4432985782623, 'accumulated_eval_time': 290.6179518699646, 'accumulated_logging_time': 2.0351693630218506, 'global_step': 2761, 'preemption_count': 0}), (3068, {'train/ssim': 0.7356076921735492, 'train/loss': 0.2748680966241019, 'validation/ssim': 0.7135930960273635, 'validation/loss': 0.2924621272905529, 'validation/num_examples': 3554, 'test/ssim': 0.7311548799218095, 'test/loss': 0.29407181429550056, 'test/num_examples': 3581, 'score': 962.1247160434723, 'total_duration': 1271.893544435501, 'accumulated_submission_time': 962.1247160434723, 'accumulated_eval_time': 296.4672088623047, 'accumulated_logging_time': 2.0600008964538574, 'global_step': 3068, 'preemption_count': 0}), (3376, {'train/ssim': 0.7398606709071568, 'train/loss': 0.27318453788757324, 'validation/ssim': 0.7182415912308314, 'validation/loss': 0.2912077294619619, 'validation/num_examples': 3554, 'test/ssim': 0.7352981119973471, 'test/loss': 0.2928267380139975, 'test/num_examples': 3581, 'score': 1041.7301037311554, 'total_duration': 1358.3521766662598, 'accumulated_submission_time': 1041.7301037311554, 'accumulated_eval_time': 302.3292465209961, 'accumulated_logging_time': 2.085174083709717, 'global_step': 3376, 'preemption_count': 0}), (3686, {'train/ssim': 0.7288509096418109, 'train/loss': 0.2772975819451468, 'validation/ssim': 0.7103600531531373, 'validation/loss': 0.2941648948038302, 'validation/num_examples': 3554, 'test/ssim': 0.726968287489528, 'test/loss': 0.2957079178193068, 'test/num_examples': 3581, 'score': 1121.4139938354492, 'total_duration': 1444.6579933166504, 'accumulated_submission_time': 1121.4139938354492, 'accumulated_eval_time': 307.9354507923126, 'accumulated_logging_time': 2.110544443130493, 'global_step': 3686, 'preemption_count': 0}), (3995, {'train/ssim': 0.7355198178972516, 'train/loss': 0.2743499619620187, 'validation/ssim': 0.7150340340549382, 'validation/loss': 0.2916196566588703, 'validation/num_examples': 3554, 'test/ssim': 0.7321354648448408, 'test/loss': 0.2932032095390603, 'test/num_examples': 3581, 'score': 1201.070006608963, 'total_duration': 1531.4433393478394, 'accumulated_submission_time': 1201.070006608963, 'accumulated_eval_time': 314.03532671928406, 'accumulated_logging_time': 2.13865327835083, 'global_step': 3995, 'preemption_count': 0}), (4302, {'train/ssim': 0.7404168673924038, 'train/loss': 0.27310214723859516, 'validation/ssim': 0.7188229536701604, 'validation/loss': 0.29081228897017447, 'validation/num_examples': 3554, 'test/ssim': 0.7358126412620427, 'test/loss': 0.2925010581017872, 'test/num_examples': 3581, 'score': 1280.64071059227, 'total_duration': 1617.88312458992, 'accumulated_submission_time': 1280.64071059227, 'accumulated_eval_time': 319.89588379859924, 'accumulated_logging_time': 2.1639182567596436, 'global_step': 4302, 'preemption_count': 0}), (4610, {'train/ssim': 0.7407129832676479, 'train/loss': 0.2717583349772862, 'validation/ssim': 0.7195261803381753, 'validation/loss': 0.28963235609304655, 'validation/num_examples': 3554, 'test/ssim': 0.7365850828373709, 'test/loss': 0.29113544548266895, 'test/num_examples': 3581, 'score': 1360.1626873016357, 'total_duration': 1704.3407008647919, 'accumulated_submission_time': 1360.1626873016357, 'accumulated_eval_time': 325.7730760574341, 'accumulated_logging_time': 2.1881043910980225, 'global_step': 4610, 'preemption_count': 0}), (4921, {'train/ssim': 0.7348274503435407, 'train/loss': 0.27647205761500765, 'validation/ssim': 0.7131163554709482, 'validation/loss': 0.29422208306221864, 'validation/num_examples': 3554, 'test/ssim': 0.7304922709482338, 'test/loss': 0.2956590692413083, 'test/num_examples': 3581, 'score': 1439.8118708133698, 'total_duration': 1790.817242860794, 'accumulated_submission_time': 1439.8118708133698, 'accumulated_eval_time': 331.63064885139465, 'accumulated_logging_time': 2.2127513885498047, 'global_step': 4921, 'preemption_count': 0}), (5227, {'train/ssim': 0.7347612380981445, 'train/loss': 0.27582904270717074, 'validation/ssim': 0.7149407467817952, 'validation/loss': 0.2928777983434159, 'validation/num_examples': 3554, 'test/ssim': 0.7320965359710975, 'test/loss': 0.2943809613672857, 'test/num_examples': 3581, 'score': 1519.4112832546234, 'total_duration': 1877.2939748764038, 'accumulated_submission_time': 1519.4112832546234, 'accumulated_eval_time': 337.5035355091095, 'accumulated_logging_time': 2.2394347190856934, 'global_step': 5227, 'preemption_count': 0}), (5534, {'train/ssim': 0.7369174957275391, 'train/loss': 0.2733560289655413, 'validation/ssim': 0.7157085463782006, 'validation/loss': 0.29101617455683737, 'validation/num_examples': 3554, 'test/ssim': 0.7329879458208252, 'test/loss': 0.2925143184624581, 'test/num_examples': 3581, 'score': 1598.9534223079681, 'total_duration': 1963.4597458839417, 'accumulated_submission_time': 1598.9534223079681, 'accumulated_eval_time': 343.1292185783386, 'accumulated_logging_time': 2.264946699142456, 'global_step': 5534, 'preemption_count': 0}), (5842, {'train/ssim': 0.7367862973894391, 'train/loss': 0.27314138412475586, 'validation/ssim': 0.7160769555430501, 'validation/loss': 0.2908402820215778, 'validation/num_examples': 3554, 'test/ssim': 0.7331502062753071, 'test/loss': 0.29232148077527226, 'test/num_examples': 3581, 'score': 1678.5088427066803, 'total_duration': 2050.116842508316, 'accumulated_submission_time': 1678.5088427066803, 'accumulated_eval_time': 349.21320366859436, 'accumulated_logging_time': 2.293583631515503, 'global_step': 5842, 'preemption_count': 0}), (6149, {'train/ssim': 0.7390224593026298, 'train/loss': 0.27243811743600027, 'validation/ssim': 0.7183370080367192, 'validation/loss': 0.2901400779161684, 'validation/num_examples': 3554, 'test/ssim': 0.7351779847197012, 'test/loss': 0.2915490391999441, 'test/num_examples': 3581, 'score': 1758.1025536060333, 'total_duration': 2136.3480501174927, 'accumulated_submission_time': 1758.1025536060333, 'accumulated_eval_time': 354.8616609573364, 'accumulated_logging_time': 2.3204290866851807, 'global_step': 6149, 'preemption_count': 0}), (6456, {'train/ssim': 0.742260047367641, 'train/loss': 0.27071663311549593, 'validation/ssim': 0.7203011241734665, 'validation/loss': 0.28857421875, 'validation/num_examples': 3554, 'test/ssim': 0.7377084978663432, 'test/loss': 0.2901110229662629, 'test/num_examples': 3581, 'score': 1837.7527384757996, 'total_duration': 2223.150191783905, 'accumulated_submission_time': 1837.7527384757996, 'accumulated_eval_time': 360.9774954319, 'accumulated_logging_time': 2.3480262756347656, 'global_step': 6456, 'preemption_count': 0}), (6763, {'train/ssim': 0.7443150111607143, 'train/loss': 0.27010384627750944, 'validation/ssim': 0.7219010902108539, 'validation/loss': 0.28841330163855866, 'validation/num_examples': 3554, 'test/ssim': 0.7391327083260611, 'test/loss': 0.2899065611582833, 'test/num_examples': 3581, 'score': 1917.3343152999878, 'total_duration': 2309.5701735019684, 'accumulated_submission_time': 1917.3343152999878, 'accumulated_eval_time': 366.80816197395325, 'accumulated_logging_time': 2.37479305267334, 'global_step': 6763, 'preemption_count': 0}), (7071, {'train/ssim': 0.7409170695713588, 'train/loss': 0.2706665311540876, 'validation/ssim': 0.7189129436022791, 'validation/loss': 0.28869148044017306, 'validation/num_examples': 3554, 'test/ssim': 0.7363670538737433, 'test/loss': 0.29013566882941216, 'test/num_examples': 3581, 'score': 1996.8287000656128, 'total_duration': 2395.994550704956, 'accumulated_submission_time': 1996.8287000656128, 'accumulated_eval_time': 372.70765590667725, 'accumulated_logging_time': 2.3995752334594727, 'global_step': 7071, 'preemption_count': 0}), (7380, {'train/ssim': 0.744725227355957, 'train/loss': 0.2702400003160749, 'validation/ssim': 0.7230981623918472, 'validation/loss': 0.28831218318048324, 'validation/num_examples': 3554, 'test/ssim': 0.7401801063774085, 'test/loss': 0.2897879678555222, 'test/num_examples': 3581, 'score': 2076.401529312134, 'total_duration': 2482.569599866867, 'accumulated_submission_time': 2076.401529312134, 'accumulated_eval_time': 378.6879720687866, 'accumulated_logging_time': 2.426312208175659, 'global_step': 7380, 'preemption_count': 0}), (7688, {'train/ssim': 0.7427986008780343, 'train/loss': 0.2706306832177298, 'validation/ssim': 0.7204409863885762, 'validation/loss': 0.28889488516460327, 'validation/num_examples': 3554, 'test/ssim': 0.7377329051111771, 'test/loss': 0.2903244159169052, 'test/num_examples': 3581, 'score': 2155.9663047790527, 'total_duration': 2568.992602586746, 'accumulated_submission_time': 2155.9663047790527, 'accumulated_eval_time': 384.56602334976196, 'accumulated_logging_time': 2.4513158798217773, 'global_step': 7688, 'preemption_count': 0}), (7996, {'train/ssim': 0.7406712940761021, 'train/loss': 0.27117202963147846, 'validation/ssim': 0.7190298618194289, 'validation/loss': 0.2891658510327272, 'validation/num_examples': 3554, 'test/ssim': 0.7364460024478149, 'test/loss': 0.2905314343496928, 'test/num_examples': 3581, 'score': 2235.521714925766, 'total_duration': 2655.379640340805, 'accumulated_submission_time': 2235.521714925766, 'accumulated_eval_time': 390.4260365962982, 'accumulated_logging_time': 2.4765377044677734, 'global_step': 7996, 'preemption_count': 0}), (8305, {'train/ssim': 0.7359423637390137, 'train/loss': 0.27750461442129953, 'validation/ssim': 0.7150202951340039, 'validation/loss': 0.2951681451577448, 'validation/num_examples': 3554, 'test/ssim': 0.7319636596577422, 'test/loss': 0.2967454302547298, 'test/num_examples': 3581, 'score': 2315.2203829288483, 'total_duration': 2741.678628206253, 'accumulated_submission_time': 2315.2203829288483, 'accumulated_eval_time': 396.0329291820526, 'accumulated_logging_time': 2.502425193786621, 'global_step': 8305, 'preemption_count': 0}), (8612, {'train/ssim': 0.7435787064688546, 'train/loss': 0.27013492584228516, 'validation/ssim': 0.7219633275226857, 'validation/loss': 0.2880846494761624, 'validation/num_examples': 3554, 'test/ssim': 0.7392561762601229, 'test/loss': 0.289554394613062, 'test/num_examples': 3581, 'score': 2394.7811086177826, 'total_duration': 2828.4357776641846, 'accumulated_submission_time': 2394.7811086177826, 'accumulated_eval_time': 402.1468152999878, 'accumulated_logging_time': 2.530094623565674, 'global_step': 8612, 'preemption_count': 0}), (8922, {'train/ssim': 0.7445036343165806, 'train/loss': 0.2693431888307844, 'validation/ssim': 0.7221773112162352, 'validation/loss': 0.2877102638807066, 'validation/num_examples': 3554, 'test/ssim': 0.7395213152968794, 'test/loss': 0.2891255975002618, 'test/num_examples': 3581, 'score': 2474.375424146652, 'total_duration': 2914.8790657520294, 'accumulated_submission_time': 2474.375424146652, 'accumulated_eval_time': 408.01150131225586, 'accumulated_logging_time': 2.5558865070343018, 'global_step': 8922, 'preemption_count': 0}), (9230, {'train/ssim': 0.74365234375, 'train/loss': 0.269491229738508, 'validation/ssim': 0.7217590984629995, 'validation/loss': 0.2875861327300665, 'validation/num_examples': 3554, 'test/ssim': 0.7390269663240017, 'test/loss': 0.28899057362206787, 'test/num_examples': 3581, 'score': 2553.899953842163, 'total_duration': 3001.2272350788116, 'accumulated_submission_time': 2553.899953842163, 'accumulated_eval_time': 413.8660433292389, 'accumulated_logging_time': 2.5818021297454834, 'global_step': 9230, 'preemption_count': 0}), (9539, {'train/ssim': 0.7448873519897461, 'train/loss': 0.2696940728596279, 'validation/ssim': 0.7227241202694148, 'validation/loss': 0.28791583248518393, 'validation/num_examples': 3554, 'test/ssim': 0.739924443896607, 'test/loss': 0.28938487334412527, 'test/num_examples': 3581, 'score': 2633.387009382248, 'total_duration': 3087.3191092014313, 'accumulated_submission_time': 2633.387009382248, 'accumulated_eval_time': 419.47976183891296, 'accumulated_logging_time': 2.6076576709747314, 'global_step': 9539, 'preemption_count': 0}), (9846, {'train/ssim': 0.7445345606122699, 'train/loss': 0.2717437744140625, 'validation/ssim': 0.7221051131867262, 'validation/loss': 0.29073562579136186, 'validation/num_examples': 3554, 'test/ssim': 0.7390494646223122, 'test/loss': 0.2922214656127827, 'test/num_examples': 3581, 'score': 2712.936015367508, 'total_duration': 3173.9999208450317, 'accumulated_submission_time': 2712.936015367508, 'accumulated_eval_time': 425.5468873977661, 'accumulated_logging_time': 2.6360297203063965, 'global_step': 9846, 'preemption_count': 0}), (10156, {'train/ssim': 0.7451366697038923, 'train/loss': 0.2704284531729562, 'validation/ssim': 0.7233288388743317, 'validation/loss': 0.28862735402671286, 'validation/num_examples': 3554, 'test/ssim': 0.7403834091821418, 'test/loss': 0.29010236453024646, 'test/num_examples': 3581, 'score': 2792.441019296646, 'total_duration': 3260.5476138591766, 'accumulated_submission_time': 2792.441019296646, 'accumulated_eval_time': 431.4342589378357, 'accumulated_logging_time': 2.67303729057312, 'global_step': 10156, 'preemption_count': 0}), (10469, {'train/ssim': 0.7449252264840263, 'train/loss': 0.2709404570715768, 'validation/ssim': 0.7238126549750281, 'validation/loss': 0.2888565192278946, 'validation/num_examples': 3554, 'test/ssim': 0.7407451545483106, 'test/loss': 0.29029073664610094, 'test/num_examples': 3581, 'score': 2872.029524087906, 'total_duration': 3347.007192850113, 'accumulated_submission_time': 2872.029524087906, 'accumulated_eval_time': 437.26291847229004, 'accumulated_logging_time': 2.699338436126709, 'global_step': 10469, 'preemption_count': 0}), (10779, {'train/ssim': 0.7457284927368164, 'train/loss': 0.2693310635430472, 'validation/ssim': 0.7233291136527504, 'validation/loss': 0.2877661297679551, 'validation/num_examples': 3554, 'test/ssim': 0.7407147477572605, 'test/loss': 0.2891107349880445, 'test/num_examples': 3581, 'score': 2951.710850954056, 'total_duration': 3433.288172483444, 'accumulated_submission_time': 2951.710850954056, 'accumulated_eval_time': 442.8781199455261, 'accumulated_logging_time': 2.7260024547576904, 'global_step': 10779, 'preemption_count': 0}), (11089, {'train/ssim': 0.7450030871800014, 'train/loss': 0.2688075133732387, 'validation/ssim': 0.7225435221537352, 'validation/loss': 0.28730048339019415, 'validation/num_examples': 3554, 'test/ssim': 0.7398610396013683, 'test/loss': 0.28871278781459436, 'test/num_examples': 3581, 'score': 3031.259932756424, 'total_duration': 3519.941817522049, 'accumulated_submission_time': 3031.259932756424, 'accumulated_eval_time': 448.96289348602295, 'accumulated_logging_time': 2.7537286281585693, 'global_step': 11089, 'preemption_count': 0}), (11400, {'train/ssim': 0.7452663694109235, 'train/loss': 0.26884065355573383, 'validation/ssim': 0.7228128736986494, 'validation/loss': 0.28721116323047097, 'validation/num_examples': 3554, 'test/ssim': 0.7401323827143256, 'test/loss': 0.2886229309746754, 'test/num_examples': 3581, 'score': 3110.8493280410767, 'total_duration': 3606.408946752548, 'accumulated_submission_time': 3110.8493280410767, 'accumulated_eval_time': 454.8260598182678, 'accumulated_logging_time': 2.7796761989593506, 'global_step': 11400, 'preemption_count': 0}), (11711, {'train/ssim': 0.7456013815743583, 'train/loss': 0.2692469528743199, 'validation/ssim': 0.7234306442784538, 'validation/loss': 0.2875077006651836, 'validation/num_examples': 3554, 'test/ssim': 0.7406137099448478, 'test/loss': 0.2889252944686366, 'test/num_examples': 3581, 'score': 3190.555389404297, 'total_duration': 3693.00421500206, 'accumulated_submission_time': 3190.555389404297, 'accumulated_eval_time': 460.725745677948, 'accumulated_logging_time': 2.8059895038604736, 'global_step': 11711, 'preemption_count': 0}), (12016, {'train/ssim': 0.744403498513358, 'train/loss': 0.2687079395566668, 'validation/ssim': 0.722090343846722, 'validation/loss': 0.28727425922486105, 'validation/num_examples': 3554, 'test/ssim': 0.7392697434157708, 'test/loss': 0.288701709107093, 'test/num_examples': 3581, 'score': 3270.0436816215515, 'total_duration': 3779.4352633953094, 'accumulated_submission_time': 3270.0436816215515, 'accumulated_eval_time': 466.5889983177185, 'accumulated_logging_time': 2.8333652019500732, 'global_step': 12016, 'preemption_count': 0}), (12326, {'train/ssim': 0.7438523428780692, 'train/loss': 0.2701730898448399, 'validation/ssim': 0.7217950257412422, 'validation/loss': 0.28835226648230866, 'validation/num_examples': 3554, 'test/ssim': 0.7389368367774365, 'test/loss': 0.2898826311500803, 'test/num_examples': 3581, 'score': 3349.625075817108, 'total_duration': 3865.857735157013, 'accumulated_submission_time': 3349.625075817108, 'accumulated_eval_time': 472.42758297920227, 'accumulated_logging_time': 2.859830617904663, 'global_step': 12326, 'preemption_count': 0}), (12636, {'train/ssim': 0.7447174617222377, 'train/loss': 0.2693060466221401, 'validation/ssim': 0.7227802437614308, 'validation/loss': 0.28751666531109316, 'validation/num_examples': 3554, 'test/ssim': 0.7399409426487015, 'test/loss': 0.2888523454407812, 'test/num_examples': 3581, 'score': 3429.299788951874, 'total_duration': 3952.412826061249, 'accumulated_submission_time': 3429.299788951874, 'accumulated_eval_time': 478.2607867717743, 'accumulated_logging_time': 2.8856914043426514, 'global_step': 12636, 'preemption_count': 0}), (12941, {'train/ssim': 0.7354416847229004, 'train/loss': 0.27136618750435965, 'validation/ssim': 0.7152502846704417, 'validation/loss': 0.28929300474597286, 'validation/num_examples': 3554, 'test/ssim': 0.7323037930222005, 'test/loss': 0.29060005415953993, 'test/num_examples': 3581, 'score': 3508.915581703186, 'total_duration': 4038.851531982422, 'accumulated_submission_time': 3508.915581703186, 'accumulated_eval_time': 484.09526443481445, 'accumulated_logging_time': 2.911729097366333, 'global_step': 12941, 'preemption_count': 0}), (13249, {'train/ssim': 0.7464895248413086, 'train/loss': 0.2685727732522147, 'validation/ssim': 0.7241928109172763, 'validation/loss': 0.28715560646894345, 'validation/num_examples': 3554, 'test/ssim': 0.7413913329464535, 'test/loss': 0.2885199842157393, 'test/num_examples': 3581, 'score': 3588.502081632614, 'total_duration': 4125.282881736755, 'accumulated_submission_time': 3588.502081632614, 'accumulated_eval_time': 489.9494676589966, 'accumulated_logging_time': 2.9377171993255615, 'global_step': 13249, 'preemption_count': 0}), (13558, {'train/ssim': 0.7451870782034737, 'train/loss': 0.2688079902103969, 'validation/ssim': 0.723133265334834, 'validation/loss': 0.28700902935622713, 'validation/num_examples': 3554, 'test/ssim': 0.7404320191418249, 'test/loss': 0.28838949408553827, 'test/num_examples': 3581, 'score': 3667.9507009983063, 'total_duration': 4211.350889444351, 'accumulated_submission_time': 3667.9507009983063, 'accumulated_eval_time': 495.5587031841278, 'accumulated_logging_time': 2.965099334716797, 'global_step': 13558, 'preemption_count': 0}), (13864, {'train/ssim': 0.7454970223563058, 'train/loss': 0.26861725534711567, 'validation/ssim': 0.7233342657481008, 'validation/loss': 0.2868838162655635, 'validation/num_examples': 3554, 'test/ssim': 0.7406417305527436, 'test/loss': 0.28820446262609956, 'test/num_examples': 3581, 'score': 3747.5766203403473, 'total_duration': 4298.068925142288, 'accumulated_submission_time': 3747.5766203403473, 'accumulated_eval_time': 501.64697647094727, 'accumulated_logging_time': 2.994030714035034, 'global_step': 13864, 'preemption_count': 0}), (14174, {'train/ssim': 0.7465243339538574, 'train/loss': 0.26850928579057964, 'validation/ssim': 0.7241201320255346, 'validation/loss': 0.2870658397942899, 'validation/num_examples': 3554, 'test/ssim': 0.7413347463173694, 'test/loss': 0.2884897478663432, 'test/num_examples': 3581, 'score': 3827.1162180900574, 'total_duration': 4384.426004648209, 'accumulated_submission_time': 3827.1162180900574, 'accumulated_eval_time': 507.46436762809753, 'accumulated_logging_time': 3.0210793018341064, 'global_step': 14174, 'preemption_count': 0}), (14484, {'train/ssim': 0.7427704674857003, 'train/loss': 0.2697871242250715, 'validation/ssim': 0.7204860500492403, 'validation/loss': 0.28808318971581315, 'validation/num_examples': 3554, 'test/ssim': 0.7379321173162176, 'test/loss': 0.2896098563272305, 'test/num_examples': 3581, 'score': 3906.7040774822235, 'total_duration': 4470.641440629959, 'accumulated_submission_time': 3906.7040774822235, 'accumulated_eval_time': 513.087394952774, 'accumulated_logging_time': 3.046846628189087, 'global_step': 14484, 'preemption_count': 0}), (14795, {'train/ssim': 0.7467835290091378, 'train/loss': 0.26850717408316477, 'validation/ssim': 0.7245136834156936, 'validation/loss': 0.28711073171844226, 'validation/num_examples': 3554, 'test/ssim': 0.7417798717406102, 'test/loss': 0.288411958295518, 'test/num_examples': 3581, 'score': 3986.2477135658264, 'total_duration': 4557.205749750137, 'accumulated_submission_time': 3986.2477135658264, 'accumulated_eval_time': 519.0699920654297, 'accumulated_logging_time': 3.075533628463745, 'global_step': 14795, 'preemption_count': 0}), (15103, {'train/ssim': 0.7452718189784459, 'train/loss': 0.2685172898428781, 'validation/ssim': 0.723409967202448, 'validation/loss': 0.286828774713571, 'validation/num_examples': 3554, 'test/ssim': 0.7406030743856464, 'test/loss': 0.2881737149517418, 'test/num_examples': 3581, 'score': 4065.9376904964447, 'total_duration': 4643.565129995346, 'accumulated_submission_time': 4065.9376904964447, 'accumulated_eval_time': 524.7535533905029, 'accumulated_logging_time': 3.104548215866089, 'global_step': 15103, 'preemption_count': 0}), (15411, {'train/ssim': 0.7468591417585101, 'train/loss': 0.26774283817836214, 'validation/ssim': 0.7242696114852982, 'validation/loss': 0.2864295388448755, 'validation/num_examples': 3554, 'test/ssim': 0.7415744554593688, 'test/loss': 0.2877630869192439, 'test/num_examples': 3581, 'score': 4145.391197681427, 'total_duration': 4729.662580251694, 'accumulated_submission_time': 4145.391197681427, 'accumulated_eval_time': 530.3691346645355, 'accumulated_logging_time': 3.1346733570098877, 'global_step': 15411, 'preemption_count': 0}), (15720, {'train/ssim': 0.745774473462786, 'train/loss': 0.2684028829847063, 'validation/ssim': 0.7235231759109454, 'validation/loss': 0.2868624522435108, 'validation/num_examples': 3554, 'test/ssim': 0.7407758340460067, 'test/loss': 0.28827710485897795, 'test/num_examples': 3581, 'score': 4224.880594968796, 'total_duration': 4816.319433450699, 'accumulated_submission_time': 4224.880594968796, 'accumulated_eval_time': 536.4942610263824, 'accumulated_logging_time': 3.164893388748169, 'global_step': 15720, 'preemption_count': 0}), (16027, {'train/ssim': 0.7457030841282436, 'train/loss': 0.2685463087899344, 'validation/ssim': 0.7235373956941122, 'validation/loss': 0.2868583133935794, 'validation/num_examples': 3554, 'test/ssim': 0.7408143538597808, 'test/loss': 0.2882149618319778, 'test/num_examples': 3581, 'score': 4304.353451967239, 'total_duration': 4902.678441524506, 'accumulated_submission_time': 4304.353451967239, 'accumulated_eval_time': 542.3609712123871, 'accumulated_logging_time': 3.192406415939331, 'global_step': 16027, 'preemption_count': 0}), (16333, {'train/ssim': 0.7451808793204171, 'train/loss': 0.26800523485456196, 'validation/ssim': 0.7223644353193585, 'validation/loss': 0.28669811757548713, 'validation/num_examples': 3554, 'test/ssim': 0.7397627288554175, 'test/loss': 0.28804932663274924, 'test/num_examples': 3581, 'score': 4384.026094913483, 'total_duration': 4988.915132522583, 'accumulated_submission_time': 4384.026094913483, 'accumulated_eval_time': 547.9520001411438, 'accumulated_logging_time': 3.2197704315185547, 'global_step': 16333, 'preemption_count': 0}), (16642, {'train/ssim': 0.7467686789376395, 'train/loss': 0.268008828163147, 'validation/ssim': 0.7243870105646807, 'validation/loss': 0.28655526714507423, 'validation/num_examples': 3554, 'test/ssim': 0.741610861796635, 'test/loss': 0.287915700376117, 'test/num_examples': 3581, 'score': 4463.493456125259, 'total_duration': 5075.484724998474, 'accumulated_submission_time': 4463.493456125259, 'accumulated_eval_time': 554.0406143665314, 'accumulated_logging_time': 3.2496731281280518, 'global_step': 16642, 'preemption_count': 0}), (16950, {'train/ssim': 0.7458487919398716, 'train/loss': 0.2679550477436611, 'validation/ssim': 0.7235474251063942, 'validation/loss': 0.2863950541533308, 'validation/num_examples': 3554, 'test/ssim': 0.7408498739004468, 'test/loss': 0.28773135068329375, 'test/num_examples': 3581, 'score': 4543.084426403046, 'total_duration': 5161.912168264389, 'accumulated_submission_time': 4543.084426403046, 'accumulated_eval_time': 559.8785591125488, 'accumulated_logging_time': 3.276616334915161, 'global_step': 16950, 'preemption_count': 0}), (17256, {'train/ssim': 0.7448173250470843, 'train/loss': 0.26873312677655903, 'validation/ssim': 0.7222584395443514, 'validation/loss': 0.287323101088782, 'validation/num_examples': 3554, 'test/ssim': 0.7397636151520176, 'test/loss': 0.28864522474300125, 'test/num_examples': 3581, 'score': 4622.705796718597, 'total_duration': 5248.415489912033, 'accumulated_submission_time': 4622.705796718597, 'accumulated_eval_time': 565.7610197067261, 'accumulated_logging_time': 3.304469347000122, 'global_step': 17256, 'preemption_count': 0}), (17563, {'train/ssim': 0.7470796448843819, 'train/loss': 0.2674122708184378, 'validation/ssim': 0.7241617609559651, 'validation/loss': 0.28630331250879293, 'validation/num_examples': 3554, 'test/ssim': 0.7414779173066183, 'test/loss': 0.28767483223087126, 'test/num_examples': 3581, 'score': 4702.431264162064, 'total_duration': 5335.006631135941, 'accumulated_submission_time': 4702.431264162064, 'accumulated_eval_time': 571.6483502388, 'accumulated_logging_time': 3.3316338062286377, 'global_step': 17563, 'preemption_count': 0}), (17872, {'train/ssim': 0.7452504294259208, 'train/loss': 0.2682562214987619, 'validation/ssim': 0.7233040401220456, 'validation/loss': 0.28660398879343696, 'validation/num_examples': 3554, 'test/ssim': 0.7405629865086568, 'test/loss': 0.28794627760882086, 'test/num_examples': 3581, 'score': 4782.042553663254, 'total_duration': 5421.473974704742, 'accumulated_submission_time': 4782.042553663254, 'accumulated_eval_time': 577.4933302402496, 'accumulated_logging_time': 3.358119487762451, 'global_step': 17872, 'preemption_count': 0}), (18179, {'train/ssim': 0.7470302581787109, 'train/loss': 0.2679480825151716, 'validation/ssim': 0.7247494432989238, 'validation/loss': 0.2864546295492315, 'validation/num_examples': 3554, 'test/ssim': 0.7419618352502793, 'test/loss': 0.28781980990165107, 'test/num_examples': 3581, 'score': 4861.54571890831, 'total_duration': 5507.598253250122, 'accumulated_submission_time': 4861.54571890831, 'accumulated_eval_time': 583.1119477748871, 'accumulated_logging_time': 3.38723087310791, 'global_step': 18179, 'preemption_count': 0}), (18483, {'train/ssim': 0.7471036911010742, 'train/loss': 0.26785273211342947, 'validation/ssim': 0.7244155875202237, 'validation/loss': 0.28654403557721053, 'validation/num_examples': 3554, 'test/ssim': 0.7417219215782952, 'test/loss': 0.2879024400154461, 'test/num_examples': 3581, 'score': 4941.012405872345, 'total_duration': 5593.833423614502, 'accumulated_submission_time': 4941.012405872345, 'accumulated_eval_time': 588.8404824733734, 'accumulated_logging_time': 3.4180259704589844, 'global_step': 18483, 'preemption_count': 0}), (18793, {'train/ssim': 0.7472687448774066, 'train/loss': 0.2678966522216797, 'validation/ssim': 0.7246244878130276, 'validation/loss': 0.28676080140224924, 'validation/num_examples': 3554, 'test/ssim': 0.7418972719517942, 'test/loss': 0.28813720634948337, 'test/num_examples': 3581, 'score': 5020.715856313705, 'total_duration': 5680.601626873016, 'accumulated_submission_time': 5020.715856313705, 'accumulated_eval_time': 594.9036073684692, 'accumulated_logging_time': 3.44805908203125, 'global_step': 18793, 'preemption_count': 0}), (19103, {'train/ssim': 0.7479588644845145, 'train/loss': 0.2675520862851824, 'validation/ssim': 0.7256016685644696, 'validation/loss': 0.28612731693162635, 'validation/num_examples': 3554, 'test/ssim': 0.7427907271013683, 'test/loss': 0.2874564282956053, 'test/num_examples': 3581, 'score': 5100.271804571152, 'total_duration': 5766.985763788223, 'accumulated_submission_time': 5100.271804571152, 'accumulated_eval_time': 600.7474706172943, 'accumulated_logging_time': 3.4747776985168457, 'global_step': 19103, 'preemption_count': 0}), (19411, {'train/ssim': 0.7477327755519322, 'train/loss': 0.26737667833055767, 'validation/ssim': 0.7252926115380557, 'validation/loss': 0.28595418935420475, 'validation/num_examples': 3554, 'test/ssim': 0.7425309740208741, 'test/loss': 0.28725598891065696, 'test/num_examples': 3581, 'score': 5179.874560594559, 'total_duration': 5853.386994838715, 'accumulated_submission_time': 5179.874560594559, 'accumulated_eval_time': 606.575751543045, 'accumulated_logging_time': 3.502861738204956, 'global_step': 19411, 'preemption_count': 0}), (19720, {'train/ssim': 0.7469757625034877, 'train/loss': 0.26691690513065885, 'validation/ssim': 0.7239631648538618, 'validation/loss': 0.28590320078388787, 'validation/num_examples': 3554, 'test/ssim': 0.741308634655997, 'test/loss': 0.2872242526747068, 'test/num_examples': 3581, 'score': 5259.533682107925, 'total_duration': 5939.751412153244, 'accumulated_submission_time': 5259.533682107925, 'accumulated_eval_time': 612.1727998256683, 'accumulated_logging_time': 3.5295193195343018, 'global_step': 19720, 'preemption_count': 0}), (20029, {'train/ssim': 0.7463560104370117, 'train/loss': 0.26722730909075054, 'validation/ssim': 0.7237398386940771, 'validation/loss': 0.28585720974606077, 'validation/num_examples': 3554, 'test/ssim': 0.7411081952710485, 'test/loss': 0.28717789254485476, 'test/num_examples': 3581, 'score': 5339.068736076355, 'total_duration': 6026.400379896164, 'accumulated_submission_time': 5339.068736076355, 'accumulated_eval_time': 618.2736349105835, 'accumulated_logging_time': 3.560518980026245, 'global_step': 20029, 'preemption_count': 0}), (20337, {'train/ssim': 0.7448810168675014, 'train/loss': 0.2684384754725865, 'validation/ssim': 0.723300399307998, 'validation/loss': 0.2868167875050559, 'validation/num_examples': 3554, 'test/ssim': 0.7403632970669854, 'test/loss': 0.28814525119554596, 'test/num_examples': 3581, 'score': 5418.749127864838, 'total_duration': 6112.913596153259, 'accumulated_submission_time': 5418.749127864838, 'accumulated_eval_time': 624.0790724754333, 'accumulated_logging_time': 3.5886647701263428, 'global_step': 20337, 'preemption_count': 0}), (20644, {'train/ssim': 0.7476414952959333, 'train/loss': 0.26786511284964426, 'validation/ssim': 0.7247979416898214, 'validation/loss': 0.28677907416709164, 'validation/num_examples': 3554, 'test/ssim': 0.7420561235731988, 'test/loss': 0.2880790516571837, 'test/num_examples': 3581, 'score': 5498.319918632507, 'total_duration': 6199.255992412567, 'accumulated_submission_time': 5498.319918632507, 'accumulated_eval_time': 629.8040223121643, 'accumulated_logging_time': 3.61690354347229, 'global_step': 20644, 'preemption_count': 0}), (20954, {'train/ssim': 0.746704237801688, 'train/loss': 0.2677987132753645, 'validation/ssim': 0.7242264025789603, 'validation/loss': 0.28662052701951146, 'validation/num_examples': 3554, 'test/ssim': 0.7415044380279601, 'test/loss': 0.2879053034352311, 'test/num_examples': 3581, 'score': 5578.015944719315, 'total_duration': 6285.991969347, 'accumulated_submission_time': 5578.015944719315, 'accumulated_eval_time': 635.8459193706512, 'accumulated_logging_time': 3.6472599506378174, 'global_step': 20954, 'preemption_count': 0}), (21262, {'train/ssim': 0.7472938128880092, 'train/loss': 0.2669185059411185, 'validation/ssim': 0.7247526032507386, 'validation/loss': 0.2856641435596335, 'validation/num_examples': 3554, 'test/ssim': 0.7419476545046775, 'test/loss': 0.2869982811300091, 'test/num_examples': 3581, 'score': 5657.501625776291, 'total_duration': 6372.2735340595245, 'accumulated_submission_time': 5657.501625776291, 'accumulated_eval_time': 641.6825890541077, 'accumulated_logging_time': 3.6748082637786865, 'global_step': 21262, 'preemption_count': 0}), (21567, {'train/ssim': 0.7468853678022113, 'train/loss': 0.266995940889631, 'validation/ssim': 0.7241129877866489, 'validation/loss': 0.2857368224513752, 'validation/num_examples': 3554, 'test/ssim': 0.7414209216175649, 'test/loss': 0.28708101350879645, 'test/num_examples': 3581, 'score': 5737.136184453964, 'total_duration': 6458.7360944747925, 'accumulated_submission_time': 5737.136184453964, 'accumulated_eval_time': 647.521933555603, 'accumulated_logging_time': 3.701996326446533, 'global_step': 21567, 'preemption_count': 0}), (21876, {'train/ssim': 0.7475599561418805, 'train/loss': 0.2664895398276193, 'validation/ssim': 0.7243839880020752, 'validation/loss': 0.28561339542043296, 'validation/num_examples': 3554, 'test/ssim': 0.7417384885070512, 'test/loss': 0.2869405695860095, 'test/num_examples': 3581, 'score': 5816.7012441158295, 'total_duration': 6545.148970127106, 'accumulated_submission_time': 5816.7012441158295, 'accumulated_eval_time': 653.3664782047272, 'accumulated_logging_time': 3.729487657546997, 'global_step': 21876, 'preemption_count': 0}), (22186, {'train/ssim': 0.747598375592913, 'train/loss': 0.2667443411690848, 'validation/ssim': 0.7247167446671005, 'validation/loss': 0.2856602279671673, 'validation/num_examples': 3554, 'test/ssim': 0.742061373176138, 'test/loss': 0.28696470412419717, 'test/num_examples': 3581, 'score': 5896.258113622665, 'total_duration': 6631.60024356842, 'accumulated_submission_time': 5896.258113622665, 'accumulated_eval_time': 659.2483315467834, 'accumulated_logging_time': 3.7586512565612793, 'global_step': 22186, 'preemption_count': 0}), (22494, {'train/ssim': 0.7476109095982143, 'train/loss': 0.2668040990829468, 'validation/ssim': 0.7249833484278277, 'validation/loss': 0.2855746173160963, 'validation/num_examples': 3554, 'test/ssim': 0.7421855910534767, 'test/loss': 0.2869016066239354, 'test/num_examples': 3581, 'score': 5975.872817516327, 'total_duration': 6718.126484632492, 'accumulated_submission_time': 5975.872817516327, 'accumulated_eval_time': 665.1630713939667, 'accumulated_logging_time': 3.78808856010437, 'global_step': 22494, 'preemption_count': 0}), (22802, {'train/ssim': 0.7485455785478864, 'train/loss': 0.2671150990894863, 'validation/ssim': 0.7257631695800506, 'validation/loss': 0.28609005010859245, 'validation/num_examples': 3554, 'test/ssim': 0.7429628049951131, 'test/loss': 0.2874554738223436, 'test/num_examples': 3581, 'score': 6055.535458803177, 'total_duration': 6804.688360452652, 'accumulated_submission_time': 6055.535458803177, 'accumulated_eval_time': 671.0289905071259, 'accumulated_logging_time': 3.8156802654266357, 'global_step': 22802, 'preemption_count': 0}), (23110, {'train/ssim': 0.7476380211966378, 'train/loss': 0.26638807569231304, 'validation/ssim': 0.7247617396331598, 'validation/loss': 0.2854382413521736, 'validation/num_examples': 3554, 'test/ssim': 0.7420140585730243, 'test/loss': 0.28672707437037487, 'test/num_examples': 3581, 'score': 6135.137357711792, 'total_duration': 6891.117000579834, 'accumulated_submission_time': 6135.137357711792, 'accumulated_eval_time': 676.8624894618988, 'accumulated_logging_time': 3.843453884124756, 'global_step': 23110, 'preemption_count': 0}), (23418, {'train/ssim': 0.7481451034545898, 'train/loss': 0.2670062950679234, 'validation/ssim': 0.7256499608715532, 'validation/loss': 0.28578363782445837, 'validation/num_examples': 3554, 'test/ssim': 0.7428976962833357, 'test/loss': 0.28712604419374826, 'test/num_examples': 3581, 'score': 6214.6651265621185, 'total_duration': 6977.248491764069, 'accumulated_submission_time': 6214.6651265621185, 'accumulated_eval_time': 682.4527862071991, 'accumulated_logging_time': 3.8714585304260254, 'global_step': 23418, 'preemption_count': 0}), (23726, {'train/ssim': 0.7479040282113212, 'train/loss': 0.26656854152679443, 'validation/ssim': 0.7253206389367614, 'validation/loss': 0.28537140150182894, 'validation/num_examples': 3554, 'test/ssim': 0.7425822428703575, 'test/loss': 0.2866672152615366, 'test/num_examples': 3581, 'score': 6294.213047981262, 'total_duration': 7063.9873015880585, 'accumulated_submission_time': 6294.213047981262, 'accumulated_eval_time': 688.6130077838898, 'accumulated_logging_time': 3.901848077774048, 'global_step': 23726, 'preemption_count': 0}), (24033, {'train/ssim': 0.7491746629987445, 'train/loss': 0.2659675393785749, 'validation/ssim': 0.7259814810336944, 'validation/loss': 0.2852673978703573, 'validation/num_examples': 3554, 'test/ssim': 0.7432080364466979, 'test/loss': 0.2865626322627234, 'test/num_examples': 3581, 'score': 6373.879824399948, 'total_duration': 7150.227107524872, 'accumulated_submission_time': 6373.879824399948, 'accumulated_eval_time': 694.2075641155243, 'accumulated_logging_time': 3.9307491779327393, 'global_step': 24033, 'preemption_count': 0}), (24340, {'train/ssim': 0.748117310660226, 'train/loss': 0.2663300207683018, 'validation/ssim': 0.7252353202377603, 'validation/loss': 0.28534619058191474, 'validation/num_examples': 3554, 'test/ssim': 0.7424775235182212, 'test/loss': 0.2866532731342502, 'test/num_examples': 3581, 'score': 6453.605158567429, 'total_duration': 7237.03667473793, 'accumulated_submission_time': 6453.605158567429, 'accumulated_eval_time': 700.289989233017, 'accumulated_logging_time': 3.962435722351074, 'global_step': 24340, 'preemption_count': 0}), (24647, {'train/ssim': 0.7490226200648716, 'train/loss': 0.2660079002380371, 'validation/ssim': 0.7261561027187676, 'validation/loss': 0.28506569333739273, 'validation/num_examples': 3554, 'test/ssim': 0.7434158389110933, 'test/loss': 0.28627056344465585, 'test/num_examples': 3581, 'score': 6533.202595472336, 'total_duration': 7323.488739013672, 'accumulated_submission_time': 6533.202595472336, 'accumulated_eval_time': 706.1351006031036, 'accumulated_logging_time': 3.99021053314209, 'global_step': 24647, 'preemption_count': 0}), (24955, {'train/ssim': 0.7486442838396344, 'train/loss': 0.2661147969109671, 'validation/ssim': 0.7259156029078151, 'validation/loss': 0.2850989930470069, 'validation/num_examples': 3554, 'test/ssim': 0.7431618808468305, 'test/loss': 0.286351216435266, 'test/num_examples': 3581, 'score': 6612.728678226471, 'total_duration': 7409.896553039551, 'accumulated_submission_time': 6612.728678226471, 'accumulated_eval_time': 711.9858059883118, 'accumulated_logging_time': 4.018469333648682, 'global_step': 24955, 'preemption_count': 0}), (25264, {'train/ssim': 0.7498152596609933, 'train/loss': 0.2655347926276071, 'validation/ssim': 0.726628446820484, 'validation/loss': 0.28489943522043826, 'validation/num_examples': 3554, 'test/ssim': 0.743795787445895, 'test/loss': 0.2862165334403798, 'test/num_examples': 3581, 'score': 6692.2029547691345, 'total_duration': 7496.03379368782, 'accumulated_submission_time': 6692.2029547691345, 'accumulated_eval_time': 717.6420066356659, 'accumulated_logging_time': 4.04834508895874, 'global_step': 25264, 'preemption_count': 0}), (25572, {'train/ssim': 0.7491157395499093, 'train/loss': 0.2655557564326695, 'validation/ssim': 0.7260582129071117, 'validation/loss': 0.28473881006106677, 'validation/num_examples': 3554, 'test/ssim': 0.7433313680274365, 'test/loss': 0.28602485475644024, 'test/num_examples': 3581, 'score': 6771.736690759659, 'total_duration': 7582.470921754837, 'accumulated_submission_time': 6771.736690759659, 'accumulated_eval_time': 723.5248582363129, 'accumulated_logging_time': 4.078721284866333, 'global_step': 25572, 'preemption_count': 0}), (25879, {'train/ssim': 0.7492758887154716, 'train/loss': 0.26557297366006033, 'validation/ssim': 0.7262558472847496, 'validation/loss': 0.28475136400007034, 'validation/num_examples': 3554, 'test/ssim': 0.7435339890655543, 'test/loss': 0.285983948759512, 'test/num_examples': 3581, 'score': 6851.406276702881, 'total_duration': 7669.1016047000885, 'accumulated_submission_time': 6851.406276702881, 'accumulated_eval_time': 729.4555983543396, 'accumulated_logging_time': 4.106729745864868, 'global_step': 25879, 'preemption_count': 0}), (26187, {'train/ssim': 0.7499720709664481, 'train/loss': 0.26514412675585064, 'validation/ssim': 0.7265425785646454, 'validation/loss': 0.28472275269722497, 'validation/num_examples': 3554, 'test/ssim': 0.7437231111246858, 'test/loss': 0.28595378058677745, 'test/num_examples': 3581, 'score': 6931.0653030872345, 'total_duration': 7755.6260805130005, 'accumulated_submission_time': 6931.0653030872345, 'accumulated_eval_time': 735.338615655899, 'accumulated_logging_time': 4.135505676269531, 'global_step': 26187, 'preemption_count': 0}), (26497, {'train/ssim': 0.7497331074305943, 'train/loss': 0.2651022842952183, 'validation/ssim': 0.7264929810600732, 'validation/loss': 0.2845141958774444, 'validation/num_examples': 3554, 'test/ssim': 0.7437115210922228, 'test/loss': 0.28576588570755374, 'test/num_examples': 3581, 'score': 7010.517087697983, 'total_duration': 7841.710106134415, 'accumulated_submission_time': 7010.517087697983, 'accumulated_eval_time': 740.9738247394562, 'accumulated_logging_time': 4.163392066955566, 'global_step': 26497, 'preemption_count': 0}), (26806, {'train/ssim': 0.7503066062927246, 'train/loss': 0.26491570472717285, 'validation/ssim': 0.7270376605805079, 'validation/loss': 0.28440565840206455, 'validation/num_examples': 3554, 'test/ssim': 0.7442757511431862, 'test/loss': 0.28559104665901636, 'test/num_examples': 3581, 'score': 7089.949461936951, 'total_duration': 7928.2637367248535, 'accumulated_submission_time': 7089.949461936951, 'accumulated_eval_time': 747.0822746753693, 'accumulated_logging_time': 4.195976257324219, 'global_step': 26806, 'preemption_count': 0}), (27116, {'train/ssim': 0.7501269749232701, 'train/loss': 0.26504877635410856, 'validation/ssim': 0.7268484069446398, 'validation/loss': 0.28449580289704385, 'validation/num_examples': 3554, 'test/ssim': 0.7440946739301173, 'test/loss': 0.2857025155006458, 'test/num_examples': 3581, 'score': 7169.514883041382, 'total_duration': 8014.649475097656, 'accumulated_submission_time': 7169.514883041382, 'accumulated_eval_time': 752.9190111160278, 'accumulated_logging_time': 4.225061655044556, 'global_step': 27116, 'preemption_count': 0}), (27142, {'train/ssim': 0.7500604901994977, 'train/loss': 0.26499431473868235, 'validation/ssim': 0.7267776515018289, 'validation/loss': 0.2844626920975925, 'validation/num_examples': 3554, 'test/ssim': 0.7440094531031834, 'test/loss': 0.2856966523077527, 'test/num_examples': 3581, 'score': 7173.9355709552765, 'total_duration': 8025.797947645187, 'accumulated_submission_time': 7173.9355709552765, 'accumulated_eval_time': 758.8035817146301, 'accumulated_logging_time': 4.253333330154419, 'global_step': 27142, 'preemption_count': 0})], 'global_step': 27142}
I1010 04:24:55.359937 140216147011392 submission_runner.py:552] Timing: 7173.9355709552765
I1010 04:24:55.360018 140216147011392 submission_runner.py:554] Total number of evals: 91
I1010 04:24:55.360072 140216147011392 submission_runner.py:555] ====================
I1010 04:24:55.360314 140216147011392 submission_runner.py:625] Final fastmri score: 7173.9355709552765
