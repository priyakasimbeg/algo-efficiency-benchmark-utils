torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_pytorch/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=8000 --torch_compile=true 2>&1 | tee -a /logs/criteo1tb_pytorch_09-14-2023-04-18-43.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-09-14 04:18:54.684930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-14 04:18:54.684949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0914 04:19:09.454598 139753231906624 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0914 04:19:09.454643 139641630480192 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0914 04:19:09.454656 140562951157568 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0914 04:19:09.454708 139933085345600 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0914 04:19:10.438786 140390639396672 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0914 04:19:10.438839 140000792557376 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0914 04:19:10.439098 140123318032192 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0914 04:19:10.445111 140088179025728 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0914 04:19:10.445773 140088179025728 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.449755 140000792557376 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.449778 140390639396672 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.449901 140123318032192 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.454012 139753231906624 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.454029 139641630480192 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.454084 139933085345600 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.454143 140562951157568 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0914 04:19:10.472281 140088179025728 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_pytorch/nadamw_run_0/criteo1tb_pytorch.
W0914 04:19:11.383852 140390639396672 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.383848 139753231906624 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.383851 140000792557376 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.383847 140123318032192 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.383867 140088179025728 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.383862 139933085345600 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.383915 140562951157568 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0914 04:19:11.384116 139641630480192 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0914 04:19:11.390452 140088179025728 submission_runner.py:500] Using RNG seed 1658882415
I0914 04:19:11.393176 140088179025728 submission_runner.py:509] --- Tuning run 1/1 ---
I0914 04:19:11.393291 140088179025728 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_pytorch/nadamw_run_0/criteo1tb_pytorch/trial_1.
I0914 04:19:11.393523 140088179025728 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_pytorch/nadamw_run_0/criteo1tb_pytorch/trial_1/hparams.json.
I0914 04:19:11.394367 140088179025728 submission_runner.py:185] Initializing dataset.
I0914 04:19:11.394485 140088179025728 submission_runner.py:192] Initializing model.
W0914 04:19:21.473024 139933085345600 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0914 04:19:21.473024 140123318032192 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0914 04:19:21.473018 139641630480192 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0914 04:19:21.473033 140390639396672 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0914 04:19:21.473036 140000792557376 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0914 04:19:21.473081 140088179025728 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0914 04:19:21.473116 139753231906624 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0914 04:19:21.473240 140088179025728 submission_runner.py:226] Initializing optimizer.
W0914 04:19:21.473145 140562951157568 submission_runner.py:209] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0914 04:19:21.473688 140088179025728 submission_runner.py:233] Initializing metrics bundle.
I0914 04:19:21.473784 140088179025728 submission_runner.py:251] Initializing checkpoint and logger.
I0914 04:19:21.474562 140088179025728 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0914 04:19:21.474672 140088179025728 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0914 04:19:21.885101 140088179025728 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_pytorch/nadamw_run_0/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0914 04:19:21.885998 140088179025728 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_pytorch/nadamw_run_0/criteo1tb_pytorch/trial_1/flags_0.json.
I0914 04:19:21.985440 140088179025728 submission_runner.py:285] Starting training loop.
I0914 04:19:27.534070 140050269456128 logging_writer.py:48] [0] global_step=0, grad_norm=4.288863, loss=0.409241
I0914 04:19:27.544421 140088179025728 pytorch_submission_base.py:86] 0) loss = 0.409, grad_norm = 4.289
I0914 04:19:27.855747 140088179025728 spec.py:320] Evaluating on the training split.
I0914 04:23:38.514743 140088179025728 spec.py:332] Evaluating on the validation split.
I0914 04:27:39.967077 140088179025728 spec.py:348] Evaluating on the test split.
I0914 04:31:39.438970 140088179025728 submission_runner.py:376] Time since start: 737.45s, 	Step: 1, 	{'train/loss': 0.4110136625299043, 'validation/loss': 0.4099512808988764, 'validation/num_examples': 89000000, 'test/loss': 0.4114890100309229, 'test/num_examples': 89274637, 'score': 5.560274600982666, 'total_duration': 737.4540328979492, 'accumulated_submission_time': 5.560274600982666, 'accumulated_eval_time': 731.5833909511566, 'accumulated_logging_time': 0}
I0914 04:31:39.459976 140023542195968 logging_writer.py:48] [1] accumulated_eval_time=731.583391, accumulated_logging_time=0, accumulated_submission_time=5.560275, global_step=1, preemption_count=0, score=5.560275, test/loss=0.411489, test/num_examples=89274637, total_duration=737.454033, train/loss=0.411014, validation/loss=0.409951, validation/num_examples=89000000
I0914 04:31:39.795902 139753231906624 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795913 140088179025728 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795900 139641630480192 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795913 140390639396672 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795927 140123318032192 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795910 139933085345600 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795938 140562951157568 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:39.795964 140000792557376 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0914 04:31:40.069765 140023533803264 logging_writer.py:48] [1] global_step=1, grad_norm=4.285904, loss=0.409173
I0914 04:31:40.073128 140088179025728 pytorch_submission_base.py:86] 1) loss = 0.409, grad_norm = 4.286
I0914 04:31:40.313926 140023542195968 logging_writer.py:48] [2] global_step=2, grad_norm=3.634463, loss=0.361471
I0914 04:31:40.317298 140088179025728 pytorch_submission_base.py:86] 2) loss = 0.361, grad_norm = 3.634
I0914 04:31:40.558039 140023533803264 logging_writer.py:48] [3] global_step=3, grad_norm=2.748933, loss=0.294673
I0914 04:31:40.561574 140088179025728 pytorch_submission_base.py:86] 3) loss = 0.295, grad_norm = 2.749
I0914 04:31:40.803238 140023542195968 logging_writer.py:48] [4] global_step=4, grad_norm=1.897129, loss=0.230921
I0914 04:31:40.806563 140088179025728 pytorch_submission_base.py:86] 4) loss = 0.231, grad_norm = 1.897
I0914 04:31:41.046174 140023533803264 logging_writer.py:48] [5] global_step=5, grad_norm=1.064842, loss=0.180867
I0914 04:31:41.049542 140088179025728 pytorch_submission_base.py:86] 5) loss = 0.181, grad_norm = 1.065
I0914 04:31:41.291283 140023542195968 logging_writer.py:48] [6] global_step=6, grad_norm=0.459706, loss=0.155901
I0914 04:31:41.294775 140088179025728 pytorch_submission_base.py:86] 6) loss = 0.156, grad_norm = 0.460
I0914 04:31:41.535747 140023533803264 logging_writer.py:48] [7] global_step=7, grad_norm=0.085780, loss=0.148372
I0914 04:31:41.539183 140088179025728 pytorch_submission_base.py:86] 7) loss = 0.148, grad_norm = 0.086
I0914 04:31:41.780464 140023542195968 logging_writer.py:48] [8] global_step=8, grad_norm=0.387715, loss=0.151694
I0914 04:31:41.784528 140088179025728 pytorch_submission_base.py:86] 8) loss = 0.152, grad_norm = 0.388
I0914 04:31:42.027105 140023533803264 logging_writer.py:48] [9] global_step=9, grad_norm=0.613754, loss=0.163853
I0914 04:31:42.030495 140088179025728 pytorch_submission_base.py:86] 9) loss = 0.164, grad_norm = 0.614
I0914 04:31:42.271463 140023542195968 logging_writer.py:48] [10] global_step=10, grad_norm=0.724001, loss=0.171164
I0914 04:31:42.275005 140088179025728 pytorch_submission_base.py:86] 10) loss = 0.171, grad_norm = 0.724
I0914 04:31:42.516668 140023533803264 logging_writer.py:48] [11] global_step=11, grad_norm=0.771040, loss=0.174751
I0914 04:31:42.520081 140088179025728 pytorch_submission_base.py:86] 11) loss = 0.175, grad_norm = 0.771
I0914 04:31:42.760543 140023542195968 logging_writer.py:48] [12] global_step=12, grad_norm=0.748576, loss=0.172391
I0914 04:31:42.764192 140088179025728 pytorch_submission_base.py:86] 12) loss = 0.172, grad_norm = 0.749
I0914 04:31:43.004104 140023533803264 logging_writer.py:48] [13] global_step=13, grad_norm=0.713320, loss=0.172190
I0914 04:31:43.007476 140088179025728 pytorch_submission_base.py:86] 13) loss = 0.172, grad_norm = 0.713
I0914 04:31:43.247018 140023542195968 logging_writer.py:48] [14] global_step=14, grad_norm=0.562422, loss=0.157485
I0914 04:31:43.250426 140088179025728 pytorch_submission_base.py:86] 14) loss = 0.157, grad_norm = 0.562
I0914 04:31:43.489501 140023533803264 logging_writer.py:48] [15] global_step=15, grad_norm=0.416007, loss=0.149862
I0914 04:31:43.492868 140088179025728 pytorch_submission_base.py:86] 15) loss = 0.150, grad_norm = 0.416
I0914 04:31:43.734179 140023542195968 logging_writer.py:48] [16] global_step=16, grad_norm=0.237976, loss=0.143641
I0914 04:31:43.737678 140088179025728 pytorch_submission_base.py:86] 16) loss = 0.144, grad_norm = 0.238
I0914 04:31:43.978521 140023533803264 logging_writer.py:48] [17] global_step=17, grad_norm=0.062419, loss=0.140465
I0914 04:31:43.981472 140088179025728 pytorch_submission_base.py:86] 17) loss = 0.140, grad_norm = 0.062
I0914 04:31:44.221531 140023542195968 logging_writer.py:48] [18] global_step=18, grad_norm=0.122469, loss=0.139414
I0914 04:31:44.225343 140088179025728 pytorch_submission_base.py:86] 18) loss = 0.139, grad_norm = 0.122
I0914 04:31:44.466194 140023533803264 logging_writer.py:48] [19] global_step=19, grad_norm=0.171504, loss=0.136173
I0914 04:31:44.469442 140088179025728 pytorch_submission_base.py:86] 19) loss = 0.136, grad_norm = 0.172
I0914 04:31:44.709139 140023542195968 logging_writer.py:48] [20] global_step=20, grad_norm=0.093771, loss=0.142718
I0914 04:31:44.712283 140088179025728 pytorch_submission_base.py:86] 20) loss = 0.143, grad_norm = 0.094
I0914 04:31:44.952603 140023533803264 logging_writer.py:48] [21] global_step=21, grad_norm=0.033879, loss=0.141841
I0914 04:31:44.955899 140088179025728 pytorch_submission_base.py:86] 21) loss = 0.142, grad_norm = 0.034
I0914 04:31:45.197274 140023542195968 logging_writer.py:48] [22] global_step=22, grad_norm=0.030703, loss=0.135190
I0914 04:31:45.200658 140088179025728 pytorch_submission_base.py:86] 22) loss = 0.135, grad_norm = 0.031
I0914 04:31:45.440224 140023533803264 logging_writer.py:48] [23] global_step=23, grad_norm=0.063612, loss=0.137340
I0914 04:31:45.443346 140088179025728 pytorch_submission_base.py:86] 23) loss = 0.137, grad_norm = 0.064
I0914 04:31:45.685469 140023542195968 logging_writer.py:48] [24] global_step=24, grad_norm=0.027064, loss=0.137042
I0914 04:31:45.688840 140088179025728 pytorch_submission_base.py:86] 24) loss = 0.137, grad_norm = 0.027
I0914 04:31:45.928400 140023533803264 logging_writer.py:48] [25] global_step=25, grad_norm=0.022729, loss=0.135327
I0914 04:31:45.931527 140088179025728 pytorch_submission_base.py:86] 25) loss = 0.135, grad_norm = 0.023
I0914 04:31:46.172660 140023542195968 logging_writer.py:48] [26] global_step=26, grad_norm=0.023275, loss=0.137913
I0914 04:31:46.175806 140088179025728 pytorch_submission_base.py:86] 26) loss = 0.138, grad_norm = 0.023
I0914 04:31:46.416298 140023533803264 logging_writer.py:48] [27] global_step=27, grad_norm=0.017230, loss=0.138007
I0914 04:31:46.419656 140088179025728 pytorch_submission_base.py:86] 27) loss = 0.138, grad_norm = 0.017
I0914 04:31:46.659938 140023542195968 logging_writer.py:48] [28] global_step=28, grad_norm=0.020445, loss=0.135548
I0914 04:31:46.663310 140088179025728 pytorch_submission_base.py:86] 28) loss = 0.136, grad_norm = 0.020
I0914 04:31:46.904717 140023533803264 logging_writer.py:48] [29] global_step=29, grad_norm=0.018902, loss=0.134993
I0914 04:31:46.908038 140088179025728 pytorch_submission_base.py:86] 29) loss = 0.135, grad_norm = 0.019
I0914 04:31:47.150591 140023542195968 logging_writer.py:48] [30] global_step=30, grad_norm=0.041444, loss=0.129505
I0914 04:31:47.154541 140088179025728 pytorch_submission_base.py:86] 30) loss = 0.130, grad_norm = 0.041
I0914 04:31:47.394065 140023533803264 logging_writer.py:48] [31] global_step=31, grad_norm=0.067788, loss=0.132191
I0914 04:31:47.397168 140088179025728 pytorch_submission_base.py:86] 31) loss = 0.132, grad_norm = 0.068
I0914 04:31:47.636198 140023542195968 logging_writer.py:48] [32] global_step=32, grad_norm=0.055046, loss=0.132268
I0914 04:31:47.639142 140088179025728 pytorch_submission_base.py:86] 32) loss = 0.132, grad_norm = 0.055
I0914 04:31:47.879672 140023533803264 logging_writer.py:48] [33] global_step=33, grad_norm=0.042813, loss=0.132822
I0914 04:31:47.883247 140088179025728 pytorch_submission_base.py:86] 33) loss = 0.133, grad_norm = 0.043
I0914 04:31:48.124634 140023542195968 logging_writer.py:48] [34] global_step=34, grad_norm=0.036482, loss=0.132519
I0914 04:31:48.128174 140088179025728 pytorch_submission_base.py:86] 34) loss = 0.133, grad_norm = 0.036
I0914 04:31:48.368415 140023533803264 logging_writer.py:48] [35] global_step=35, grad_norm=0.061389, loss=0.135178
I0914 04:31:48.371743 140088179025728 pytorch_submission_base.py:86] 35) loss = 0.135, grad_norm = 0.061
I0914 04:31:49.051131 140023542195968 logging_writer.py:48] [36] global_step=36, grad_norm=0.102064, loss=0.132660
I0914 04:31:49.054571 140088179025728 pytorch_submission_base.py:86] 36) loss = 0.133, grad_norm = 0.102
I0914 04:31:49.791541 140023533803264 logging_writer.py:48] [37] global_step=37, grad_norm=0.122981, loss=0.134429
I0914 04:31:49.795105 140088179025728 pytorch_submission_base.py:86] 37) loss = 0.134, grad_norm = 0.123
I0914 04:31:50.408863 140023542195968 logging_writer.py:48] [38] global_step=38, grad_norm=0.094187, loss=0.142868
I0914 04:31:50.412190 140088179025728 pytorch_submission_base.py:86] 38) loss = 0.143, grad_norm = 0.094
I0914 04:31:51.183413 140023533803264 logging_writer.py:48] [39] global_step=39, grad_norm=0.075321, loss=0.143670
I0914 04:31:51.186837 140088179025728 pytorch_submission_base.py:86] 39) loss = 0.144, grad_norm = 0.075
I0914 04:31:52.006655 140023542195968 logging_writer.py:48] [40] global_step=40, grad_norm=0.082157, loss=0.143363
I0914 04:31:52.009910 140088179025728 pytorch_submission_base.py:86] 40) loss = 0.143, grad_norm = 0.082
I0914 04:31:52.752862 140023533803264 logging_writer.py:48] [41] global_step=41, grad_norm=0.087309, loss=0.140597
I0914 04:31:52.756319 140088179025728 pytorch_submission_base.py:86] 41) loss = 0.141, grad_norm = 0.087
I0914 04:31:53.620293 140023542195968 logging_writer.py:48] [42] global_step=42, grad_norm=0.090400, loss=0.140577
I0914 04:31:53.623858 140088179025728 pytorch_submission_base.py:86] 42) loss = 0.141, grad_norm = 0.090
I0914 04:31:54.360623 140023533803264 logging_writer.py:48] [43] global_step=43, grad_norm=0.132498, loss=0.142934
I0914 04:31:54.363929 140088179025728 pytorch_submission_base.py:86] 43) loss = 0.143, grad_norm = 0.132
I0914 04:31:55.182628 140023542195968 logging_writer.py:48] [44] global_step=44, grad_norm=0.175705, loss=0.141609
I0914 04:31:55.186276 140088179025728 pytorch_submission_base.py:86] 44) loss = 0.142, grad_norm = 0.176
I0914 04:31:55.864733 140023533803264 logging_writer.py:48] [45] global_step=45, grad_norm=0.181638, loss=0.139098
I0914 04:31:55.868143 140088179025728 pytorch_submission_base.py:86] 45) loss = 0.139, grad_norm = 0.182
I0914 04:31:56.596075 140023542195968 logging_writer.py:48] [46] global_step=46, grad_norm=0.119957, loss=0.141585
I0914 04:31:56.599390 140088179025728 pytorch_submission_base.py:86] 46) loss = 0.142, grad_norm = 0.120
I0914 04:31:57.395936 140023533803264 logging_writer.py:48] [47] global_step=47, grad_norm=0.073268, loss=0.137829
I0914 04:31:57.399184 140088179025728 pytorch_submission_base.py:86] 47) loss = 0.138, grad_norm = 0.073
I0914 04:31:58.222562 140023542195968 logging_writer.py:48] [48] global_step=48, grad_norm=0.045783, loss=0.139719
I0914 04:31:58.225987 140088179025728 pytorch_submission_base.py:86] 48) loss = 0.140, grad_norm = 0.046
I0914 04:31:59.015979 140023533803264 logging_writer.py:48] [49] global_step=49, grad_norm=0.020553, loss=0.135192
I0914 04:31:59.019354 140088179025728 pytorch_submission_base.py:86] 49) loss = 0.135, grad_norm = 0.021
I0914 04:31:59.827670 140023542195968 logging_writer.py:48] [50] global_step=50, grad_norm=0.016873, loss=0.135756
I0914 04:31:59.831037 140088179025728 pytorch_submission_base.py:86] 50) loss = 0.136, grad_norm = 0.017
I0914 04:32:00.578089 140023533803264 logging_writer.py:48] [51] global_step=51, grad_norm=0.025947, loss=0.135415
I0914 04:32:00.581367 140088179025728 pytorch_submission_base.py:86] 51) loss = 0.135, grad_norm = 0.026
I0914 04:32:01.453587 140023542195968 logging_writer.py:48] [52] global_step=52, grad_norm=0.054106, loss=0.136501
I0914 04:32:01.457146 140088179025728 pytorch_submission_base.py:86] 52) loss = 0.137, grad_norm = 0.054
I0914 04:32:02.141784 140023533803264 logging_writer.py:48] [53] global_step=53, grad_norm=0.112471, loss=0.134323
I0914 04:32:02.145148 140088179025728 pytorch_submission_base.py:86] 53) loss = 0.134, grad_norm = 0.112
I0914 04:32:02.932836 140023542195968 logging_writer.py:48] [54] global_step=54, grad_norm=0.223201, loss=0.136208
I0914 04:32:02.936278 140088179025728 pytorch_submission_base.py:86] 54) loss = 0.136, grad_norm = 0.223
I0914 04:32:03.769047 140023533803264 logging_writer.py:48] [55] global_step=55, grad_norm=0.308437, loss=0.139251
I0914 04:32:03.772449 140088179025728 pytorch_submission_base.py:86] 55) loss = 0.139, grad_norm = 0.308
I0914 04:32:04.627052 140023542195968 logging_writer.py:48] [56] global_step=56, grad_norm=0.238399, loss=0.136826
I0914 04:32:04.630373 140088179025728 pytorch_submission_base.py:86] 56) loss = 0.137, grad_norm = 0.238
I0914 04:32:05.181208 140023533803264 logging_writer.py:48] [57] global_step=57, grad_norm=0.159091, loss=0.145560
I0914 04:32:05.185251 140088179025728 pytorch_submission_base.py:86] 57) loss = 0.146, grad_norm = 0.159
I0914 04:32:05.949064 140023542195968 logging_writer.py:48] [58] global_step=58, grad_norm=0.110249, loss=0.145350
I0914 04:32:05.952508 140088179025728 pytorch_submission_base.py:86] 58) loss = 0.145, grad_norm = 0.110
I0914 04:32:06.655478 140023533803264 logging_writer.py:48] [59] global_step=59, grad_norm=0.030002, loss=0.144437
I0914 04:32:06.658871 140088179025728 pytorch_submission_base.py:86] 59) loss = 0.144, grad_norm = 0.030
I0914 04:32:07.517406 140023542195968 logging_writer.py:48] [60] global_step=60, grad_norm=0.011935, loss=0.143117
I0914 04:32:07.520730 140088179025728 pytorch_submission_base.py:86] 60) loss = 0.143, grad_norm = 0.012
I0914 04:32:08.293804 140023533803264 logging_writer.py:48] [61] global_step=61, grad_norm=0.008605, loss=0.142163
I0914 04:32:08.297181 140088179025728 pytorch_submission_base.py:86] 61) loss = 0.142, grad_norm = 0.009
I0914 04:32:08.935398 140023542195968 logging_writer.py:48] [62] global_step=62, grad_norm=0.015640, loss=0.143502
I0914 04:32:08.938522 140088179025728 pytorch_submission_base.py:86] 62) loss = 0.144, grad_norm = 0.016
I0914 04:32:09.713242 140023533803264 logging_writer.py:48] [63] global_step=63, grad_norm=0.035660, loss=0.141361
I0914 04:32:09.716719 140088179025728 pytorch_submission_base.py:86] 63) loss = 0.141, grad_norm = 0.036
I0914 04:32:10.558694 140023542195968 logging_writer.py:48] [64] global_step=64, grad_norm=0.039992, loss=0.139288
I0914 04:32:10.561923 140088179025728 pytorch_submission_base.py:86] 64) loss = 0.139, grad_norm = 0.040
I0914 04:32:11.318704 140023533803264 logging_writer.py:48] [65] global_step=65, grad_norm=0.016261, loss=0.138789
I0914 04:32:11.321940 140088179025728 pytorch_submission_base.py:86] 65) loss = 0.139, grad_norm = 0.016
I0914 04:32:11.908193 140023542195968 logging_writer.py:48] [66] global_step=66, grad_norm=0.007418, loss=0.138656
I0914 04:32:11.911574 140088179025728 pytorch_submission_base.py:86] 66) loss = 0.139, grad_norm = 0.007
I0914 04:32:12.826271 140023533803264 logging_writer.py:48] [67] global_step=67, grad_norm=0.008656, loss=0.137195
I0914 04:32:12.829738 140088179025728 pytorch_submission_base.py:86] 67) loss = 0.137, grad_norm = 0.009
I0914 04:32:13.484149 140023542195968 logging_writer.py:48] [68] global_step=68, grad_norm=0.007776, loss=0.138739
I0914 04:32:13.487665 140088179025728 pytorch_submission_base.py:86] 68) loss = 0.139, grad_norm = 0.008
I0914 04:32:14.248445 140023533803264 logging_writer.py:48] [69] global_step=69, grad_norm=0.007300, loss=0.139428
I0914 04:32:14.252180 140088179025728 pytorch_submission_base.py:86] 69) loss = 0.139, grad_norm = 0.007
I0914 04:32:15.041129 140023542195968 logging_writer.py:48] [70] global_step=70, grad_norm=0.009920, loss=0.138651
I0914 04:32:15.044682 140088179025728 pytorch_submission_base.py:86] 70) loss = 0.139, grad_norm = 0.010
I0914 04:32:15.788650 140023533803264 logging_writer.py:48] [71] global_step=71, grad_norm=0.011490, loss=0.138385
I0914 04:32:15.792153 140088179025728 pytorch_submission_base.py:86] 71) loss = 0.138, grad_norm = 0.011
I0914 04:32:16.414130 140023542195968 logging_writer.py:48] [72] global_step=72, grad_norm=0.008978, loss=0.139654
I0914 04:32:16.417702 140088179025728 pytorch_submission_base.py:86] 72) loss = 0.140, grad_norm = 0.009
I0914 04:32:17.077865 140023533803264 logging_writer.py:48] [73] global_step=73, grad_norm=0.015882, loss=0.137066
I0914 04:32:17.081290 140088179025728 pytorch_submission_base.py:86] 73) loss = 0.137, grad_norm = 0.016
I0914 04:32:17.876186 140023542195968 logging_writer.py:48] [74] global_step=74, grad_norm=0.054991, loss=0.137353
I0914 04:32:17.879535 140088179025728 pytorch_submission_base.py:86] 74) loss = 0.137, grad_norm = 0.055
I0914 04:32:18.566779 140023533803264 logging_writer.py:48] [75] global_step=75, grad_norm=0.114497, loss=0.136821
I0914 04:32:18.570047 140088179025728 pytorch_submission_base.py:86] 75) loss = 0.137, grad_norm = 0.114
I0914 04:32:19.355893 140023542195968 logging_writer.py:48] [76] global_step=76, grad_norm=0.168740, loss=0.136235
I0914 04:32:19.359174 140088179025728 pytorch_submission_base.py:86] 76) loss = 0.136, grad_norm = 0.169
I0914 04:32:19.915064 140023533803264 logging_writer.py:48] [77] global_step=77, grad_norm=0.162755, loss=0.133176
I0914 04:32:19.918596 140088179025728 pytorch_submission_base.py:86] 77) loss = 0.133, grad_norm = 0.163
I0914 04:32:20.607166 140023542195968 logging_writer.py:48] [78] global_step=78, grad_norm=0.121490, loss=0.132212
I0914 04:32:20.610499 140088179025728 pytorch_submission_base.py:86] 78) loss = 0.132, grad_norm = 0.121
I0914 04:32:21.380432 140023533803264 logging_writer.py:48] [79] global_step=79, grad_norm=0.070207, loss=0.131826
I0914 04:32:21.383888 140088179025728 pytorch_submission_base.py:86] 79) loss = 0.132, grad_norm = 0.070
I0914 04:32:22.121175 140023542195968 logging_writer.py:48] [80] global_step=80, grad_norm=0.031167, loss=0.128572
I0914 04:32:22.124856 140088179025728 pytorch_submission_base.py:86] 80) loss = 0.129, grad_norm = 0.031
I0914 04:32:22.719576 140023533803264 logging_writer.py:48] [81] global_step=81, grad_norm=0.017185, loss=0.128995
I0914 04:32:22.723201 140088179025728 pytorch_submission_base.py:86] 81) loss = 0.129, grad_norm = 0.017
I0914 04:32:23.434470 140023542195968 logging_writer.py:48] [82] global_step=82, grad_norm=0.016351, loss=0.130403
I0914 04:32:23.437826 140088179025728 pytorch_submission_base.py:86] 82) loss = 0.130, grad_norm = 0.016
I0914 04:32:24.161522 140023533803264 logging_writer.py:48] [83] global_step=83, grad_norm=0.021161, loss=0.129715
I0914 04:32:24.165355 140088179025728 pytorch_submission_base.py:86] 83) loss = 0.130, grad_norm = 0.021
I0914 04:32:24.963531 140023542195968 logging_writer.py:48] [84] global_step=84, grad_norm=0.022398, loss=0.130774
I0914 04:32:24.967475 140088179025728 pytorch_submission_base.py:86] 84) loss = 0.131, grad_norm = 0.022
I0914 04:32:25.630116 140023533803264 logging_writer.py:48] [85] global_step=85, grad_norm=0.018691, loss=0.129785
I0914 04:32:25.633755 140088179025728 pytorch_submission_base.py:86] 85) loss = 0.130, grad_norm = 0.019
I0914 04:32:26.276214 140023542195968 logging_writer.py:48] [86] global_step=86, grad_norm=0.011848, loss=0.129938
I0914 04:32:26.279898 140088179025728 pytorch_submission_base.py:86] 86) loss = 0.130, grad_norm = 0.012
I0914 04:32:27.024620 140023533803264 logging_writer.py:48] [87] global_step=87, grad_norm=0.005919, loss=0.130638
I0914 04:32:27.028270 140088179025728 pytorch_submission_base.py:86] 87) loss = 0.131, grad_norm = 0.006
I0914 04:32:27.739232 140023542195968 logging_writer.py:48] [88] global_step=88, grad_norm=0.030776, loss=0.126049
I0914 04:32:27.742937 140088179025728 pytorch_submission_base.py:86] 88) loss = 0.126, grad_norm = 0.031
I0914 04:32:28.540267 140023533803264 logging_writer.py:48] [89] global_step=89, grad_norm=0.061567, loss=0.128727
I0914 04:32:28.543690 140088179025728 pytorch_submission_base.py:86] 89) loss = 0.129, grad_norm = 0.062
I0914 04:32:29.278981 140023542195968 logging_writer.py:48] [90] global_step=90, grad_norm=0.068706, loss=0.128444
I0914 04:32:29.282215 140088179025728 pytorch_submission_base.py:86] 90) loss = 0.128, grad_norm = 0.069
I0914 04:32:29.891376 140023533803264 logging_writer.py:48] [91] global_step=91, grad_norm=0.076139, loss=0.129115
I0914 04:32:29.894623 140088179025728 pytorch_submission_base.py:86] 91) loss = 0.129, grad_norm = 0.076
I0914 04:32:30.679595 140023542195968 logging_writer.py:48] [92] global_step=92, grad_norm=0.090090, loss=0.128189
I0914 04:32:30.683086 140088179025728 pytorch_submission_base.py:86] 92) loss = 0.128, grad_norm = 0.090
I0914 04:32:31.514453 140023533803264 logging_writer.py:48] [93] global_step=93, grad_norm=0.109328, loss=0.130609
I0914 04:32:31.517908 140088179025728 pytorch_submission_base.py:86] 93) loss = 0.131, grad_norm = 0.109
I0914 04:32:32.266835 140023542195968 logging_writer.py:48] [94] global_step=94, grad_norm=0.114867, loss=0.130400
I0914 04:32:32.270520 140088179025728 pytorch_submission_base.py:86] 94) loss = 0.130, grad_norm = 0.115
I0914 04:32:32.896100 140023533803264 logging_writer.py:48] [95] global_step=95, grad_norm=0.128116, loss=0.131158
I0914 04:32:32.899569 140088179025728 pytorch_submission_base.py:86] 95) loss = 0.131, grad_norm = 0.128
I0914 04:32:33.721141 140023542195968 logging_writer.py:48] [96] global_step=96, grad_norm=0.108866, loss=0.133644
I0914 04:32:33.724528 140088179025728 pytorch_submission_base.py:86] 96) loss = 0.134, grad_norm = 0.109
I0914 04:32:34.474268 140023533803264 logging_writer.py:48] [97] global_step=97, grad_norm=0.068143, loss=0.130573
I0914 04:32:34.477758 140088179025728 pytorch_submission_base.py:86] 97) loss = 0.131, grad_norm = 0.068
I0914 04:32:35.375125 140023542195968 logging_writer.py:48] [98] global_step=98, grad_norm=0.051591, loss=0.132442
I0914 04:32:35.379542 140088179025728 pytorch_submission_base.py:86] 98) loss = 0.132, grad_norm = 0.052
I0914 04:32:35.963330 140023533803264 logging_writer.py:48] [99] global_step=99, grad_norm=0.040634, loss=0.131623
I0914 04:32:35.966649 140088179025728 pytorch_submission_base.py:86] 99) loss = 0.132, grad_norm = 0.041
I0914 04:32:36.729930 140023542195968 logging_writer.py:48] [100] global_step=100, grad_norm=0.032193, loss=0.132428
I0914 04:32:36.733081 140088179025728 pytorch_submission_base.py:86] 100) loss = 0.132, grad_norm = 0.032
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0914 04:37:29.025543 140023533803264 logging_writer.py:48] [500] global_step=500, grad_norm=0.004511, loss=0.127251
I0914 04:37:29.028992 140088179025728 pytorch_submission_base.py:86] 500) loss = 0.127, grad_norm = 0.005
I0914 04:43:40.857644 140023542195968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.006409, loss=0.124103
I0914 04:43:40.861114 140088179025728 pytorch_submission_base.py:86] 1000) loss = 0.124, grad_norm = 0.006
I0914 04:49:47.006366 140023533803264 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.014728, loss=0.121360
I0914 04:49:47.010282 140088179025728 pytorch_submission_base.py:86] 1500) loss = 0.121, grad_norm = 0.015
I0914 04:51:40.292429 140088179025728 spec.py:320] Evaluating on the training split.
I0914 04:55:38.020184 140088179025728 spec.py:332] Evaluating on the validation split.
I0914 04:58:31.054480 140088179025728 spec.py:348] Evaluating on the test split.
I0914 05:01:37.440562 140088179025728 submission_runner.py:376] Time since start: 2535.46s, 	Step: 1658, 	{'train/loss': 0.12376263969120928, 'validation/loss': 0.12579106741573035, 'validation/num_examples': 89000000, 'test/loss': 0.128373638752516, 'test/num_examples': 89274637, 'score': 1204.9508435726166, 'total_duration': 2535.4556703567505, 'accumulated_submission_time': 1204.9508435726166, 'accumulated_eval_time': 1328.7317383289337, 'accumulated_logging_time': 0.028187274932861328}
I0914 05:01:37.455632 140023542195968 logging_writer.py:48] [1658] accumulated_eval_time=1328.731738, accumulated_logging_time=0.028187, accumulated_submission_time=1204.950844, global_step=1658, preemption_count=0, score=1204.950844, test/loss=0.128374, test/num_examples=89274637, total_duration=2535.455670, train/loss=0.123763, validation/loss=0.125791, validation/num_examples=89000000
I0914 05:05:35.449078 140023533803264 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.004772, loss=0.126914
I0914 05:05:35.452872 140088179025728 pytorch_submission_base.py:86] 2000) loss = 0.127, grad_norm = 0.005
I0914 05:11:43.323366 140023542195968 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.006215, loss=0.116840
I0914 05:11:43.329648 140088179025728 pytorch_submission_base.py:86] 2500) loss = 0.117, grad_norm = 0.006
I0914 05:17:55.448602 140023533803264 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.012298, loss=0.131639
I0914 05:17:55.452879 140088179025728 pytorch_submission_base.py:86] 3000) loss = 0.132, grad_norm = 0.012
I0914 05:21:37.985145 140088179025728 spec.py:320] Evaluating on the training split.
I0914 05:25:34.580047 140088179025728 spec.py:332] Evaluating on the validation split.
I0914 05:28:55.530987 140088179025728 spec.py:348] Evaluating on the test split.
I0914 05:32:05.216986 140088179025728 submission_runner.py:376] Time since start: 4363.23s, 	Step: 3299, 	{'train/loss': 0.12574493441899384, 'validation/loss': 0.12426114606741573, 'validation/num_examples': 89000000, 'test/loss': 0.12703421017550595, 'test/num_examples': 89274637, 'score': 2404.1292345523834, 'total_duration': 4363.232081651688, 'accumulated_submission_time': 2404.1292345523834, 'accumulated_eval_time': 1955.9636104106903, 'accumulated_logging_time': 0.050275325775146484}
I0914 05:32:05.232894 140023542195968 logging_writer.py:48] [3299] accumulated_eval_time=1955.963610, accumulated_logging_time=0.050275, accumulated_submission_time=2404.129235, global_step=3299, preemption_count=0, score=2404.129235, test/loss=0.127034, test/num_examples=89274637, total_duration=4363.232082, train/loss=0.125745, validation/loss=0.124261, validation/num_examples=89000000
I0914 05:34:19.830077 140023533803264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011363, loss=0.122474
I0914 05:34:19.833397 140088179025728 pytorch_submission_base.py:86] 3500) loss = 0.122, grad_norm = 0.011
I0914 05:40:33.392112 140023542195968 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.013194, loss=0.123246
I0914 05:40:33.396227 140088179025728 pytorch_submission_base.py:86] 4000) loss = 0.123, grad_norm = 0.013
I0914 05:46:40.591879 140023533803264 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.007322, loss=0.117750
I0914 05:46:40.595253 140088179025728 pytorch_submission_base.py:86] 4500) loss = 0.118, grad_norm = 0.007
I0914 05:52:05.972341 140088179025728 spec.py:320] Evaluating on the training split.
I0914 05:56:00.830268 140088179025728 spec.py:332] Evaluating on the validation split.
I0914 05:58:56.427096 140088179025728 spec.py:348] Evaluating on the test split.
I0914 06:01:53.160564 140088179025728 submission_runner.py:376] Time since start: 6151.18s, 	Step: 4949, 	{'train/loss': 0.12312465719673008, 'validation/loss': 0.12400551685393259, 'validation/num_examples': 89000000, 'test/loss': 0.12674648007809877, 'test/num_examples': 89274637, 'score': 3603.454144001007, 'total_duration': 6151.175648450851, 'accumulated_submission_time': 3603.454144001007, 'accumulated_eval_time': 2543.1519503593445, 'accumulated_logging_time': 0.07319808006286621}
I0914 06:01:53.174544 140023542195968 logging_writer.py:48] [4949] accumulated_eval_time=2543.151950, accumulated_logging_time=0.073198, accumulated_submission_time=3603.454144, global_step=4949, preemption_count=0, score=3603.454144, test/loss=0.126746, test/num_examples=89274637, total_duration=6151.175648, train/loss=0.123125, validation/loss=0.124006, validation/num_examples=89000000
I0914 06:02:14.162653 140023533803264 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.005454, loss=0.130875
I0914 06:02:14.165850 140088179025728 pytorch_submission_base.py:86] 5000) loss = 0.131, grad_norm = 0.005
I0914 06:08:13.732794 140023542195968 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.005108, loss=0.130357
I0914 06:08:13.736608 140088179025728 pytorch_submission_base.py:86] 5500) loss = 0.130, grad_norm = 0.005
I0914 06:14:23.934923 140023533803264 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.003602, loss=0.117437
I0914 06:14:23.938565 140088179025728 pytorch_submission_base.py:86] 6000) loss = 0.117, grad_norm = 0.004
I0914 06:20:32.141553 140023542195968 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.005124, loss=0.116879
I0914 06:20:32.145153 140088179025728 pytorch_submission_base.py:86] 6500) loss = 0.117, grad_norm = 0.005
I0914 06:21:54.119942 140088179025728 spec.py:320] Evaluating on the training split.
I0914 06:25:42.965591 140088179025728 spec.py:332] Evaluating on the validation split.
I0914 06:28:35.677708 140088179025728 spec.py:348] Evaluating on the test split.
I0914 06:31:33.331912 140088179025728 submission_runner.py:376] Time since start: 7931.35s, 	Step: 6614, 	{'train/loss': 0.12201123172674365, 'validation/loss': 0.12382551685393259, 'validation/num_examples': 89000000, 'test/loss': 0.12648277696161342, 'test/num_examples': 89274637, 'score': 4803.026340961456, 'total_duration': 7931.347029209137, 'accumulated_submission_time': 4803.026340961456, 'accumulated_eval_time': 3122.3641316890717, 'accumulated_logging_time': 0.09761786460876465}
I0914 06:31:33.350389 140023533803264 logging_writer.py:48] [6614] accumulated_eval_time=3122.364132, accumulated_logging_time=0.097618, accumulated_submission_time=4803.026341, global_step=6614, preemption_count=0, score=4803.026341, test/loss=0.126483, test/num_examples=89274637, total_duration=7931.347029, train/loss=0.122011, validation/loss=0.123826, validation/num_examples=89000000
I0914 06:36:04.265882 140023542195968 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.004303, loss=0.120556
I0914 06:36:04.269445 140088179025728 pytorch_submission_base.py:86] 7000) loss = 0.121, grad_norm = 0.004
I0914 06:42:13.026687 140023533803264 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.004896, loss=0.121237
I0914 06:42:13.034955 140088179025728 pytorch_submission_base.py:86] 7500) loss = 0.121, grad_norm = 0.005
I0914 06:48:18.131912 140088179025728 spec.py:320] Evaluating on the training split.
I0914 06:52:08.273653 140088179025728 spec.py:332] Evaluating on the validation split.
I0914 06:54:59.442869 140088179025728 spec.py:348] Evaluating on the test split.
I0914 06:57:47.221395 140088179025728 submission_runner.py:376] Time since start: 9505.24s, 	Step: 8000, 	{'train/loss': 0.12336901123136731, 'validation/loss': 0.12359693258426967, 'validation/num_examples': 89000000, 'test/loss': 0.12619659265598582, 'test/num_examples': 89274637, 'score': 5806.548750162125, 'total_duration': 9505.236487865448, 'accumulated_submission_time': 5806.548750162125, 'accumulated_eval_time': 3691.4538328647614, 'accumulated_logging_time': 0.12289547920227051}
I0914 06:57:47.236359 140023542195968 logging_writer.py:48] [8000] accumulated_eval_time=3691.453833, accumulated_logging_time=0.122895, accumulated_submission_time=5806.548750, global_step=8000, preemption_count=0, score=5806.548750, test/loss=0.126197, test/num_examples=89274637, total_duration=9505.236488, train/loss=0.123369, validation/loss=0.123597, validation/num_examples=89000000
I0914 06:57:47.528667 140023533803264 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5806.548750
I0914 06:57:58.213479 140088179025728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_pytorch/nadamw_run_0/criteo1tb_pytorch/trial_1/checkpoint_8000.
I0914 06:57:58.255705 140088179025728 submission_runner.py:540] Tuning trial 1/1
I0914 06:57:58.255925 140088179025728 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I0914 06:57:58.256645 140088179025728 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/loss': 0.4110136625299043, 'validation/loss': 0.4099512808988764, 'validation/num_examples': 89000000, 'test/loss': 0.4114890100309229, 'test/num_examples': 89274637, 'score': 5.560274600982666, 'total_duration': 737.4540328979492, 'accumulated_submission_time': 5.560274600982666, 'accumulated_eval_time': 731.5833909511566, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1658, {'train/loss': 0.12376263969120928, 'validation/loss': 0.12579106741573035, 'validation/num_examples': 89000000, 'test/loss': 0.128373638752516, 'test/num_examples': 89274637, 'score': 1204.9508435726166, 'total_duration': 2535.4556703567505, 'accumulated_submission_time': 1204.9508435726166, 'accumulated_eval_time': 1328.7317383289337, 'accumulated_logging_time': 0.028187274932861328, 'global_step': 1658, 'preemption_count': 0}), (3299, {'train/loss': 0.12574493441899384, 'validation/loss': 0.12426114606741573, 'validation/num_examples': 89000000, 'test/loss': 0.12703421017550595, 'test/num_examples': 89274637, 'score': 2404.1292345523834, 'total_duration': 4363.232081651688, 'accumulated_submission_time': 2404.1292345523834, 'accumulated_eval_time': 1955.9636104106903, 'accumulated_logging_time': 0.050275325775146484, 'global_step': 3299, 'preemption_count': 0}), (4949, {'train/loss': 0.12312465719673008, 'validation/loss': 0.12400551685393259, 'validation/num_examples': 89000000, 'test/loss': 0.12674648007809877, 'test/num_examples': 89274637, 'score': 3603.454144001007, 'total_duration': 6151.175648450851, 'accumulated_submission_time': 3603.454144001007, 'accumulated_eval_time': 2543.1519503593445, 'accumulated_logging_time': 0.07319808006286621, 'global_step': 4949, 'preemption_count': 0}), (6614, {'train/loss': 0.12201123172674365, 'validation/loss': 0.12382551685393259, 'validation/num_examples': 89000000, 'test/loss': 0.12648277696161342, 'test/num_examples': 89274637, 'score': 4803.026340961456, 'total_duration': 7931.347029209137, 'accumulated_submission_time': 4803.026340961456, 'accumulated_eval_time': 3122.3641316890717, 'accumulated_logging_time': 0.09761786460876465, 'global_step': 6614, 'preemption_count': 0}), (8000, {'train/loss': 0.12336901123136731, 'validation/loss': 0.12359693258426967, 'validation/num_examples': 89000000, 'test/loss': 0.12619659265598582, 'test/num_examples': 89274637, 'score': 5806.548750162125, 'total_duration': 9505.236487865448, 'accumulated_submission_time': 5806.548750162125, 'accumulated_eval_time': 3691.4538328647614, 'accumulated_logging_time': 0.12289547920227051, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0914 06:57:58.256743 140088179025728 submission_runner.py:543] Timing: 5806.548750162125
I0914 06:57:58.256804 140088179025728 submission_runner.py:545] Total number of evals: 6
I0914 06:57:58.256852 140088179025728 submission_runner.py:546] ====================
I0914 06:57:58.256932 140088179025728 submission_runner.py:614] Final criteo1tb score: 5806.548750162125
