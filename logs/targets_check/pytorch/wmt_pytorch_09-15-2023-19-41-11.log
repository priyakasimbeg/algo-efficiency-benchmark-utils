torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/wmt/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_pytorch/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=100000 --torch_compile=true 2>&1 | tee -a /logs/wmt_pytorch_09-15-2023-19-41-11.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-09-15 19:41:21.556529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-15 19:41:21.556529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0915 19:41:35.450580 139637639219008 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0915 19:41:35.450615 139958421567296 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0915 19:41:35.450644 140311716239168 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0915 19:41:35.451600 140589617661760 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0915 19:41:35.451765 140548286359360 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0915 19:41:35.451789 140119655679808 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0915 19:41:35.451828 140526367590208 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0915 19:41:35.452178 139742592976704 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0915 19:41:35.452281 140119655679808 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.452314 140548286359360 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.452336 140526367590208 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.452719 139742592976704 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.461555 139637639219008 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.461580 139958421567296 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.461604 140311716239168 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:35.462334 140589617661760 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0915 19:41:40.186454 140548286359360 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_pytorch/nadamw_run_0/wmt_pytorch.
W0915 19:41:41.160565 140119655679808 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.160566 140589617661760 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.160567 140548286359360 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.160565 140526367590208 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.160565 139637639219008 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.160565 139958421567296 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.161777 140311716239168 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0915 19:41:41.161888 139742592976704 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0915 19:41:41.166665 140548286359360 submission_runner.py:500] Using RNG seed 176746681
I0915 19:41:41.169416 140548286359360 submission_runner.py:509] --- Tuning run 1/1 ---
I0915 19:41:41.169538 140548286359360 submission_runner.py:514] Creating tuning directory at /experiment_runs/targets_check_pytorch/nadamw_run_0/wmt_pytorch/trial_1.
I0915 19:41:41.169771 140548286359360 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_pytorch/nadamw_run_0/wmt_pytorch/trial_1/hparams.json.
I0915 19:41:41.170731 140548286359360 submission_runner.py:185] Initializing dataset.
I0915 19:41:41.170864 140548286359360 submission_runner.py:192] Initializing model.
I0915 19:41:44.391056 140548286359360 submission_runner.py:223] Performing `torch.compile`.
I0915 19:41:44.657360 140548286359360 submission_runner.py:226] Initializing optimizer.
I0915 19:41:44.658810 140548286359360 submission_runner.py:233] Initializing metrics bundle.
I0915 19:41:44.658938 140548286359360 submission_runner.py:251] Initializing checkpoint and logger.
I0915 19:41:44.659579 140548286359360 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0915 19:41:44.659681 140548286359360 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0915 19:41:45.106958 140548286359360 submission_runner.py:272] Saving meta data to /experiment_runs/targets_check_pytorch/nadamw_run_0/wmt_pytorch/trial_1/meta_data_0.json.
I0915 19:41:45.107875 140548286359360 submission_runner.py:275] Saving flags to /experiment_runs/targets_check_pytorch/nadamw_run_0/wmt_pytorch/trial_1/flags_0.json.
I0915 19:41:45.186387 140548286359360 submission_runner.py:285] Starting training loop.
I0915 19:41:45.201820 140548286359360 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0915 19:41:45.206275 140548286359360 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 19:41:45.279474 140548286359360 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,976] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:47,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:48,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,007] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,009] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,010] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,010] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,013] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,013] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,014] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,021] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,022] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,026] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,027] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:48,156] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,156] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,157] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:48,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:48,439] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:48,439] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:48,440] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:52,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:52,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-15 19:41:53,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,356] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,357] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,357] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-15 19:41:53,395] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:53,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,714] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,714] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,771] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:41:53,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:53,804] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:53,805] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:53,805] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:53,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-15 19:41:54,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,401] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,401] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,402] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,402] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,403] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,413] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,429] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:54,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-15 19:41:54,520] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:54,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:41:54,542] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:41:54,544] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:54,544] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:54,544] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:54,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-15 19:41:55,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,037] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,038] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,038] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,040] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,042] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,051] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,053] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,060] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,118] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,119] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,119] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-15 19:41:55,187] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:55,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:41:55,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:55,241] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:55,242] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:55,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-15 19:41:55,512] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,513] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,513] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,513] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,513] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,513] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,514] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:55,515] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:56,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,327] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,328] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,328] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,330] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,333] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,340] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,341] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-15 19:41:56,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:56,364] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,371] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,381] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,383] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,387] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,388] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,388] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,388] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-09-15 19:41:56,389] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,390] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,390] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,390] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,390] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,391] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,391] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,391] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,391] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:56,397] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,397] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,398] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,399] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,399] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,399] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:56,411] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:56,412] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:56,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:56,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-15 19:41:57,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,267] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,268] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,269] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,283] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,284] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-15 19:41:57,302] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:57,310] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:41:57,316] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,330] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,331] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:41:57,337] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:41:57,407] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,407] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,410] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,411] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,411] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,411] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,412] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,414] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,414] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,420] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,421] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,421] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,426] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,432] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,436] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,437] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,462] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,462] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:41:57,477] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:41:57,477] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:41:57,477] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:41:57,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,911] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,911] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,918] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,930] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,946] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,953] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,955] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:57,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-15 19:41:57,990] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:41:59,208] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,209] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,254] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,259] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,263] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,281] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,328] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:41:59,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-15 19:41:59,337] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,112] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,112] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,118] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,118] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,119] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,119] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,119] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,120] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,120] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,129] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,131] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,131] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,131] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,137] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,138] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,142] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,142] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,142] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,147] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,148] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,149] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,169] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,182] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:00,186] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,190] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:00,192] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,192] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,192] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,196] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,199] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,204] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,210] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,233] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-15 19:42:00,255] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,637] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,641] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,646] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,651] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,655] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,673] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,687] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,695] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,695] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,696] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,696] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,699] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,701] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,701] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,702] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,704] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,704] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,705] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-15 19:42:00,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:00,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,725] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:00,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,740] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,741] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,741] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:00,776] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:00,776] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:00,776] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:00,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,935] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,942] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,943] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,948] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,958] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,963] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:00,980] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:00,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-15 19:42:01,014] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:01,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:01,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:01,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:02,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,019] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,072] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,123] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:02,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:02,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:02,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:02,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-15 19:42:02,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,488] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,489] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,510] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,519] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,535] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,538] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,539] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,571] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,572] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,572] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-15 19:42:02,576] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,623] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,623] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,624] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:02,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:02,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:02,679] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:02,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:02,713] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:02,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:02,722] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:02,724] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,724] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,724] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,751] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:02,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:02,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:02,788] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:02,812] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:02,822] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:02,822] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:02,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:02,846] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:02,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:02,868] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,868] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,868] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:02,901] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,901] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,901] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,930] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,932] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,959] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,959] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,959] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,960] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,961] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,961] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,962] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:02,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:02,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,976] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:02,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:02,988] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:02,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:02,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,004] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,004] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,005] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-15 19:42:03,019] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:03,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:03,047] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,047] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,048] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:03,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:03,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:03,098] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:03,100] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:03,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:03,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-15 19:42:03,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,139] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,140] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,140] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,141] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,143] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,144] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,144] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:03,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,164] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,170] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,171] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,171] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:03,179] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,187] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,187] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-15 19:42:03,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,188] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:03,202] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:03,206] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,222] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,222] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,222] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:03,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:03,229] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,231] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,231] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:03,233] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:03,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:03,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:03,249] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:03,257] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:03,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:03,272] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:03,277] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,277] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,277] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,279] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:03,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:03,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,282] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:03,286] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-15 19:42:03,292] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:03,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:03,310] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,310] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,310] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,310] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-15 19:42:03,315] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:03,323] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:03,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:03,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,325] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:03,363] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,363] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,364] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:03,367] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,367] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,367] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,509] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,513] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,519] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,535] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,551] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,562] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-15 19:42:03,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,601] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,602] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-15 19:42:03,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,726] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,733] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,736] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,751] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,764] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,766] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,768] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,775] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,779] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,782] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,782] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,790] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,791] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,791] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,791] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,792] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,792] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,798] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,807] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,808] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,810] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,816] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-15 19:42:03,820] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:03,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,824] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,825] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,825] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,836] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,837] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,837] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-15 19:42:03,846] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,850] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,860] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,871] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,871] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,872] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,876] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,876] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,877] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:03,920] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,921] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:03,929] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:03,933] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:03,946] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,956] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,961] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,962] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,962] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-15 19:42:03,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:03,964] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,971] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,972] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,972] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,975] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,977] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,977] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,977] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:03,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:03,978] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:03,982] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:03,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:03,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:03,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:03,988] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:03,988] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:03,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,006] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:04,011] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-15 19:42:04,022] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,024] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,024] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,024] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,024] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,035] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,041] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,055] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,059] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,059] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,059] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,068] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,071] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,074] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,083] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,093] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,093] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,102] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,102] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,112] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,112] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,113] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,113] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,119] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,121] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,122] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,122] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,122] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,126] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,126] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,126] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,126] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-15 19:42:04,139] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,142] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,142] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,143] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,153] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,161] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-15 19:42:04,167] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,181] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,211] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,211] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,225] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,225] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,226] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,364] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,371] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,373] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,389] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,415] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,422] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-15 19:42:04,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,447] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,462] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-15 19:42:04,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,579] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,586] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,587] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,605] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,606] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,610] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,616] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,628] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,629] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,635] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,636] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,636] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,636] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,637] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,641] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,641] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,642] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,643] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,644] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,644] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,659] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,661] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,661] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,662] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,662] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,684] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-15 19:42:04,684] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,685] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,687] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,692] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,694] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,694] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,694] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-15 19:42:04,717] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,717] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,718] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,745] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,745] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,745] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,772] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,781] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,784] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,788] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,794] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,799] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,802] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,806] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,812] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,814] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,814] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,814] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,818] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,818] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,818] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,821] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,823] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,823] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,823] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-15 19:42:04,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,835] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,841] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,841] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,841] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,842] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,854] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,860] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,861] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,861] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,862] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,869] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,873] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,875] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,875] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,875] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,876] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,882] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-15 19:42:04,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,895] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,895] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,896] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:04,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,899] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,900] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:04,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,920] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,923] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:04,932] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:04,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,935] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,935] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,935] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,935] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,943] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,948] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,948] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,950] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,950] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,950] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,954] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:04,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,957] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,957] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,957] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,959] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,968] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,975] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,975] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,975] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:04,977] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:04,985] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:04,991] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:04,993] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:04,994] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:04,994] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:04,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-15 19:42:04,998] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,010] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,011] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,011] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-15 19:42:05,021] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,027] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,028] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,028] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,074] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,074] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,075] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,184] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,185] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,193] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,211] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,227] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,245] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,262] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-15 19:42:05,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,318] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-15 19:42:05,397] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,400] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,406] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,426] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,431] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,436] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,440] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,441] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,454] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,454] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,455] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,457] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,457] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,457] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,458] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,460] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,470] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,476] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,484] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,484] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,484] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,491] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,497] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,497] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,497] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,507] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,516] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,517] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,518] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,518] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,533] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,534] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,534] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-15 19:42:05,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,538] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-15 19:42:05,566] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,568] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,596] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,596] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,596] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,598] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,612] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,632] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,633] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,633] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,633] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,637] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,638] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,638] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,638] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,641] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,641] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,641] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,656] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,665] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,668] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,669] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,673] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,674] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,674] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,674] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-15 19:42:05,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,693] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,697] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,697] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,698] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,698] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,701] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,711] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,711] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,720] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,724] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,734] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-15 19:42:05,736] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:05,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,747] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:05,757] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,760] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,767] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,768] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,772] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,772] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,772] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:05,773] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,775] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,775] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,776] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:05,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,784] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,795] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,799] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,799] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,800] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,806] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,808] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,808] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,809] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,833] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,835] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,836] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,836] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-09-15 19:42:05,839] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:05,843] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,845] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,846] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,846] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-09-15 19:42:05,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:05,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:05,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:05,912] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:05,912] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:05,913] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:05,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:05,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:05,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:06,003] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,010] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,011] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:06,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:06,037] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,043] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:06,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:06,070] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,080] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-09-15 19:42:06,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,146] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,217] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-09-15 19:42:06,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,226] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,228] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,247] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,254] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,257] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,259] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,259] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,272] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,272] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,273] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,279] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,282] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,284] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,284] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,284] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,285] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,287] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,289] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,294] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,309] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,309] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,310] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,314] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,314] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,314] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,317] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,343] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,343] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,343] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,349] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,350] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,350] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,360] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-09-15 19:42:06,363] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-09-15 19:42:06,393] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,403] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,409] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,418] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,418] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,419] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,422] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,447] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,454] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,454] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,454] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,460] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,461] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,464] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,482] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,482] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,487] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,488] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,488] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,488] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,488] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,495] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,495] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,496] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-09-15 19:42:06,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,509] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,519] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,519] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,523] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,524] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,525] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,525] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,529] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,529] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,532] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,541] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,548] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,548] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,550] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,553] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,554] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,556] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-09-15 19:42:06,560] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,567] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:06,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,572] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,577] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:06,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,584] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,585] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,585] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,587] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,593] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,594] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,594] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:06,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,599] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,599] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,600] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:06,601] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,601] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,601] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,608] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,610] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,618] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,623] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,623] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,623] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,628] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,629] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,629] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,658] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-09-15 19:42:06,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,660] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,661] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,661] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,661] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,661] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,662] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,662] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-09-15 19:42:06,683] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:06,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:06,731] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:06,733] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:06,734] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:06,734] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:06,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,819] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,828] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,836] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,858] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,865] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:06,896] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,896] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:06,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:06,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:06,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:06,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-09-15 19:42:06,968] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:06,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:06,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:07,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,031] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,039] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-09-15 19:42:07,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,050] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,073] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,077] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,077] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,081] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,084] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,088] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,088] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,108] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,108] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,112] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,116] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,139] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,144] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,147] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,164] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-09-15 19:42:07,183] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:07,188] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-09-15 19:42:07,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,215] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,222] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:07,242] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,242] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,242] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,246] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,246] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,247] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,256] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,263] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,267] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,276] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,278] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,279] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,283] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,283] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,284] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,304] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,314] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,314] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,314] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,324] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:07,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,326] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,335] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,346] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,346] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,346] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,346] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,347] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,347] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,348] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,357] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,358] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,378] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,384] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,384] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,385] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,393] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,393] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,394] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-09-15 19:42:07,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-09-15 19:42:07,413] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,414] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,415] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,425] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:07,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,492] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-09-15 19:42:07,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,846] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,848] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,864] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,868] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,879] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,879] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,879] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,880] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,880] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,881] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,885] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,897] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,897] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,898] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:07,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:07,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,918] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:07,926] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,926] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,927] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,928] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:07,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:07,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:07,950] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:07,963] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,963] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,964] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:07,987] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:07,987] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:07,988] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:07,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:08,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:08,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-09-15 19:42:08,020] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,053] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,053] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,053] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-09-15 19:42:08,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,195] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,204] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,210] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,226] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,226] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,227] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,234] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,241] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,241] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,245] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,256] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,264] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,265] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,271] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,276] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,276] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,277] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,280] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,282] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,295] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,306] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,309] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,310] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,310] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,311] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,316] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,324] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,324] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,325] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,330] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,331] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,331] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,338] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,339] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,339] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,340] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,346] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,346] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,346] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,364] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,364] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,364] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-09-15 19:42:08,372] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,374] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,374] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,375] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,375] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,394] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:42:08,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:42:08,400] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,400] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,404] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,404] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 32
[2023-09-15 19:42:08,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 32
[2023-09-15 19:42:08,428] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:08,437] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,437] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:08,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,455] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,461] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,479] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,490] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:08,491] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:08,491] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:08,499] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,515] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,534] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,566] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:08,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 33
[2023-09-15 19:42:08,650] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,089] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,094] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,112] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,118] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,135] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,135] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,135] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,138] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,149] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,150] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,158] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,158] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,158] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,199] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,204] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,217] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,218] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,218] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,222] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,224] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,225] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,225] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 33
[2023-09-15 19:42:09,231] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,257] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,257] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,257] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,265] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,274] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,293] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,293] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,293] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,321] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,323] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,323] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,323] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,337] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,354] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 34
[2023-09-15 19:42:09,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,383] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,386] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,386] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,394] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,401] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,403] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,405] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,405] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,405] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,407] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,407] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,411] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,414] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,419] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,419] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,420] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 34
[2023-09-15 19:42:09,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,434] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,435] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:09,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,444] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,444] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,444] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,447] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,456] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,462] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,464] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:09,470] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:09,476] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,476] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,476] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,481] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,487] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,497] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,497] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,497] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,497] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,504] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,505] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,511] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,513] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,514] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,514] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,515] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,527] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,533] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,533] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,533] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,533] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 35
[2023-09-15 19:42:09,536] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,543] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,553] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,553] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,554] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 35
[2023-09-15 19:42:09,558] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,560] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,560] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,561] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,568] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,585] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,585] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,666] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 36
[2023-09-15 19:42:09,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,691] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,746] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,746] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,746] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,748] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,763] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,771] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,778] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,778] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,781] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,781] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,787] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,788] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,802] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,802] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,803] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,805] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,805] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 36
[2023-09-15 19:42:09,807] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,808] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,814] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:09,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,837] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,837] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,837] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:09,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,842] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,843] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,843] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,843] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:09,857] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,857] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,858] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:09,862] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,862] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,863] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,863] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,886] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,887] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,888] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,892] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,897] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,903] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,915] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,917] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,920] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 37
[2023-09-15 19:42:09,921] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:09,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,939] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 37
[2023-09-15 19:42:09,945] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:09,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:09,968] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,971] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,971] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,971] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,971] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,971] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,972] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,975] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,977] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,977] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,978] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:09,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:09,996] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:09,997] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:09,997] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,093] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,124] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,150] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,177] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,208] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,213] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,213] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 38
[2023-09-15 19:42:10,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,232] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 38
[2023-09-15 19:42:10,307] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,338] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,338] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,360] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,365] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,366] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,366] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,367] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,370] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,393] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,397] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,399] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,399] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,400] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,426] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,427] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,429] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,429] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,436] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 39
[2023-09-15 19:42:10,446] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,453] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,453] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,458] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,460] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,463] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 39
[2023-09-15 19:42:10,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,472] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,487] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,487] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,488] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,489] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,489] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,489] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,491] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,492] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,493] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,502] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,508] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,508] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,538] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,546] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,547] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,562] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,563] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,584] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,585] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 40
[2023-09-15 19:42:10,591] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,592] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,604] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,607] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,627] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,627] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,628] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,630] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,630] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,635] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,635] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,635] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 40
[2023-09-15 19:42:10,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,645] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,645] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,645] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:10,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,655] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,655] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,656] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,667] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,667] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,667] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,668] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,668] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,669] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,670] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,670] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,671] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,675] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,676] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,676] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,680] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,683] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:10,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:10,689] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,691] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,695] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,698] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,698] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,718] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,719] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,720] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,726] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,729] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,735] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,746] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,746] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,747] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 41
[2023-09-15 19:42:10,748] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,749] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,753] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 41
[2023-09-15 19:42:10,772] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,783] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,786] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,787] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,787] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,800] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,816] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,825] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,860] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,861] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,861] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,862] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,862] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,892] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 42
[2023-09-15 19:42:10,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,906] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,907] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,907] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,907] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,908] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,910] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:10,920] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,928] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,928] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,928] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,937] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,938] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:10,943] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,944] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,949] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:10,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,954] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,954] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,955] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 42
[2023-09-15 19:42:10,963] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:10,968] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,968] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:10,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,983] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,983] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,983] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:10,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:10,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:10,988] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:10,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:10,992] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:10,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:10,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:10,994] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,994] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,994] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:10,995] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:10,995] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:10,995] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,002] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:11,007] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:11,008] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,008] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:11,011] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:11,014] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:11,038] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:11,041] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,043] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,045] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,046] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:11,048] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:11,054] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:11,064] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,064] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,064] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 43
[2023-09-15 19:42:11,068] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:11,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:11,077] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,087] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,089] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,089] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,090] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 43
[2023-09-15 19:42:11,090] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,116] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,116] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,123] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,123] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,124] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,129] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,129] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:11,143] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,143] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,143] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,230] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,284] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,302] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,326] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,350] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,359] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 44
[2023-09-15 19:42:11,371] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,377] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,441] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 44
[2023-09-15 19:42:11,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,485] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,497] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,501] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,502] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,502] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,515] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,528] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,542] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,557] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,562] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,572] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,575] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,575] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,575] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,577] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,580] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,583] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 45
[2023-09-15 19:42:11,589] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,600] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,600] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,601] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,607] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 45
[2023-09-15 19:42:11,614] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,620] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,622] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,622] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,623] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,636] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,636] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,637] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,637] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,642] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,642] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,642] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,645] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,649] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,649] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,650] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,680] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,680] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,680] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,695] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,710] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,718] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 46
[2023-09-15 19:42:11,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,738] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,739] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,740] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,740] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,741] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,753] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,755] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,762] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,775] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,776] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,781] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,784] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 46
[2023-09-15 19:42:11,785] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,785] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,791] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,791] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,791] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:11,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,801] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,802] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,802] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,803] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,814] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,815] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,816] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,821] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,821] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,822] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,822] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,822] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,822] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:11,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,825] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:11,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,837] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,853] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,853] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,854] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,861] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,864] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,865] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,865] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,880] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,881] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:11,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 47
[2023-09-15 19:42:11,887] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:11,891] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,892] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,892] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,903] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,906] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 47
[2023-09-15 19:42:11,910] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,911] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,911] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,912] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:11,930] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,930] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,930] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,934] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,935] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,935] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,937] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,938] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,938] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:11,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:11,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:11,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:11,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:11,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:11,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:11,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:11,994] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:11,994] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:11,995] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:12,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,016] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:12,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,025] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:12,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 48
[2023-09-15 19:42:12,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,039] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,051] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,053] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,060] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,060] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,061] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,068] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,069] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,069] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,070] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,076] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,084] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,088] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,090] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 48
[2023-09-15 19:42:12,097] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,097] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:12,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,112] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,113] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,115] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,121] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,129] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,130] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,130] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,130] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,132] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,132] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,133] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:12,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:12,143] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,144] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,144] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,144] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,144] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,144] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,152] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,157] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,174] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,180] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,188] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,191] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,196] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,197] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,198] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,198] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,199] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,202] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,204] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 49
[2023-09-15 19:42:12,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,206] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,214] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,222] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 49
[2023-09-15 19:42:12,229] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,231] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,231] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,237] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,248] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,248] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,249] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,276] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,277] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,277] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:12,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,368] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,435] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,441] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,466] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,482] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,498] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,509] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 50
[2023-09-15 19:42:12,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,528] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,577] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,606] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 50
[2023-09-15 19:42:12,608] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,620] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,637] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,637] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,637] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,649] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,656] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,680] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,682] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,688] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,694] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,709] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,710] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,711] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,711] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,713] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,719] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,719] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,719] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,723] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,726] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,741] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,742] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,742] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,743] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,745] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 51
[2023-09-15 19:42:12,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,748] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,755] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,756] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,756] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,756] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 51
[2023-09-15 19:42:12,770] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,770] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,771] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,772] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,778] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,784] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,784] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,785] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,807] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,808] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,808] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,814] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,815] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,816] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,816] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,851] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,860] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,869] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:12,873] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,890] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 52
[2023-09-15 19:42:12,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,891] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,895] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:12,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,896] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,896] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,897] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,906] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,906] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,906] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,907] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,920] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,923] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,924] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,929] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,935] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,935] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,936] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 52
[2023-09-15 19:42:12,947] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,949] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,950] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,950] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:12,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:12,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,958] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,963] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,964] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,964] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,965] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:12,969] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:12,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:12,981] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:12,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:12,983] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:12,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:12,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:12,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:12,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:12,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:12,992] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:12,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:12,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:12,994] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:13,006] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:13,008] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,010] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,010] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,011] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:13,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,017] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,021] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:13,023] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:13,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,034] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,034] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,035] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,045] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:13,046] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,046] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 53
[2023-09-15 19:42:13,049] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,059] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,059] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,060] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 53
[2023-09-15 19:42:13,072] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,074] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,074] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,074] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,093] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,100] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,100] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,100] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,129] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,129] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,176] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,188] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,188] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 54
[2023-09-15 19:42:13,196] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,197] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,206] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,206] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,211] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,220] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,221] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,221] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,222] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,223] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,223] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,226] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,232] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,232] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,236] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,242] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,242] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,242] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,251] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,251] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,251] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,260] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,264] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,264] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 54
[2023-09-15 19:42:13,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,265] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,267] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:13,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,282] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,289] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,289] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,294] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:13,305] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,309] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:13,310] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,310] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,311] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,311] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,313] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,317] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,322] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,328] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,329] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,334] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,337] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,342] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,353] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,361] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,365] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,370] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,370] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,370] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,370] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 55
[2023-09-15 19:42:13,372] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,381] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,381] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,381] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,387] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 55
[2023-09-15 19:42:13,395] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,403] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,404] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:13,406] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,406] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,406] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,414] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,416] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,417] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,417] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:13,446] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,446] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,447] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,501] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,607] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,616] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,620] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,621] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,640] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,651] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,665] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 56
[2023-09-15 19:42:13,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,682] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,711] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 56
[2023-09-15 19:42:13,771] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,771] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,772] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,819] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,830] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,833] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,838] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,851] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,852] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:13,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,855] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,864] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,865] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,869] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,877] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,881] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,881] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,881] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,884] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,893] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,893] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,894] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,895] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,895] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,895] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 57
[2023-09-15 19:42:13,899] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,900] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,900] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,901] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:13,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:13,907] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,909] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,914] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:13,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 57
[2023-09-15 19:42:13,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,926] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,926] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,930] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,931] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:13,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:13,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:13,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:13,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,951] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,951] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,960] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:13,961] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:13,961] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:13,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:13,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:13,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:13,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:13,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:14,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:14,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,009] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,017] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,032] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,032] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,039] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,039] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,043] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 58
[2023-09-15 19:42:14,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,050] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,060] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,060] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,060] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,061] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,062] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,062] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,070] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,074] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,076] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,076] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,076] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,076] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,077] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,078] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,078] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,079] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,082] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,082] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,082] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,090] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 58
[2023-09-15 19:42:14,100] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,104] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,106] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:14,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,121] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,136] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:14,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,139] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:14,142] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,143] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,143] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,143] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,144] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,156] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,159] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,163] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,165] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,166] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,171] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,173] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,173] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,173] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,179] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,188] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,188] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,189] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,189] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,192] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,192] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,193] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,195] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,195] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,199] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 59
[2023-09-15 19:42:14,203] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,208] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,209] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,209] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,217] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,217] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,217] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,222] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 59
[2023-09-15 19:42:14,227] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,267] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,267] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,267] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,286] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,326] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,335] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 60
[2023-09-15 19:42:14,348] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,353] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,358] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,361] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,364] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,370] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,377] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,379] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,379] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,380] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,398] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,398] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,399] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,402] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,403] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,403] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,404] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,404] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,410] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 60
[2023-09-15 19:42:14,415] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,415] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,417] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,417] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,417] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,424] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,424] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:14,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,439] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,454] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:14,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,458] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:14,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,462] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,463] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,465] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,477] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,481] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,486] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,487] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,488] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,500] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,509] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,514] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,514] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,515] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,518] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 61
[2023-09-15 19:42:14,521] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,532] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,534] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,534] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,535] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,540] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,540] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,540] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,540] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,540] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,541] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 61
[2023-09-15 19:42:14,545] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:14,554] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,554] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,554] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,561] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,562] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,562] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:14,595] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:14,595] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:14,596] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:14,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,640] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,749] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,770] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,775] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,777] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,789] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,797] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 62
[2023-09-15 19:42:14,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,832] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:14,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,854] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:14,886] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:14,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:14,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 62
[2023-09-15 19:42:14,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:14,963] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:14,985] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:14,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:14,988] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:14,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:15,003] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:15,007] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:15,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:15,011] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,017] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,020] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:15,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:15,036] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,044] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,073] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,074] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,074] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,211] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,257] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,257] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,257] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,335] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,336] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,336] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:15,417] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:15,448] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,460] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,461] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,462] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 63
[2023-09-15 19:42:15,467] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,478] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,478] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,479] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,481] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,481] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,481] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,483] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 63
[2023-09-15 19:42:15,498] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,514] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,519] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,521] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,521] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,521] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,527] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,528] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,528] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,572] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,582] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,598] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,600] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,604] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 64
[2023-09-15 19:42:15,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,617] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,617] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,617] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,634] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,634] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,634] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,641] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,642] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,644] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,644] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,645] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,648] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,648] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,648] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,663] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,663] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,663] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,666] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 64
[2023-09-15 19:42:15,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,676] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:15,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,680] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,703] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:15,706] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,710] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:15,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,712] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,712] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,723] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,726] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,732] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,732] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,733] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,735] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,748] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,750] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,763] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,764] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,764] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,764] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,765] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 65
[2023-09-15 19:42:15,772] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 65
[2023-09-15 19:42:15,796] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,800] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,825] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,825] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,825] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,826] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,855] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:15,911] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:15,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 66
[2023-09-15 19:42:15,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,921] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,922] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,927] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,928] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:15,935] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,940] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,943] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,943] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:15,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:15,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:15,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,968] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,968] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,975] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,976] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,977] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,977] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:15,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,987] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,987] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:15,988] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:15,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 66
[2023-09-15 19:42:15,990] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:15,990] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:15,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:15,991] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:15,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:16,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,007] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:16,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:16,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,029] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,031] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:16,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,035] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,035] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,036] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,040] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,048] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,051] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,061] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,071] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,074] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,083] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,083] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,084] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 67
[2023-09-15 19:42:16,095] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,107] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,107] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,108] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,113] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,114] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,115] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,117] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,117] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,118] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 67
[2023-09-15 19:42:16,119] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,123] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,123] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,124] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,126] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,127] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,127] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,170] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,234] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,305] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,321] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,343] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,348] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,356] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,359] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,361] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 68
[2023-09-15 19:42:16,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,405] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,450] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,480] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 68
[2023-09-15 19:42:16,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,502] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,502] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,503] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,538] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,557] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,564] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,576] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,577] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,581] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,588] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,590] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,590] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,591] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,607] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,607] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,608] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,609] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,609] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,610] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,614] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,616] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,616] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,616] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 69
[2023-09-15 19:42:16,623] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-09-15 19:42:16,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,629] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,629] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,629] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,630] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,630] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,630] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,637] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,637] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,637] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 69
[2023-09-15 19:42:16,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,673] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:16,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,677] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:16,677] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:16,678] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:16,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 70
[2023-09-15 19:42:16,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,837] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:16,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,957] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,967] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,967] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:16,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,977] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:16,979] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:16,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:16,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:16,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:17,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:17,010] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:17,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:17,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:17,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:17,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:17,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 70
[2023-09-15 19:42:17,029] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:17,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:17,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 70
[2023-09-15 19:42:17,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 70
[2023-09-15 19:42:17,375] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 69
[2023-09-15 19:42:17,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:17,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 69
[2023-09-15 19:42:18,136] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 68
[2023-09-15 19:42:18,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:18,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:18,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:18,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:18,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:18,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:19,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:19,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:19,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:19,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:19,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:19,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:19,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:19,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:19,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 68
[2023-09-15 19:42:19,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 67
[2023-09-15 19:42:19,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,411] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:19,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 67
[2023-09-15 19:42:19,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-09-15 19:42:20,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-09-15 19:42:20,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 66
[2023-09-15 19:42:20,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,855] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:20,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 66
[2023-09-15 19:42:20,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 65
[2023-09-15 19:42:20,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 65
[2023-09-15 19:42:20,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 64
[2023-09-15 19:42:21,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,591] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 64
[2023-09-15 19:42:21,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 63
[2023-09-15 19:42:21,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 63
[2023-09-15 19:42:21,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 62
[2023-09-15 19:42:21,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,768] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:21,768] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:21,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 62
[2023-09-15 19:42:21,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 61
[2023-09-15 19:42:21,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 61
[2023-09-15 19:42:21,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 60
[2023-09-15 19:42:21,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:21,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:21,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:21,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:21,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:21,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:21,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:21,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:21,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:21,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:21,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:21,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:21,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 60
[2023-09-15 19:42:21,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 59
[2023-09-15 19:42:21,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 59
[2023-09-15 19:42:21,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 58
[2023-09-15 19:42:21,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:21,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:21,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:21,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:22,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:22,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:22,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 58
[2023-09-15 19:42:22,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 57
[2023-09-15 19:42:22,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 57
[2023-09-15 19:42:22,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 56
[2023-09-15 19:42:22,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,127] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,157] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 56
[2023-09-15 19:42:22,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 55
[2023-09-15 19:42:22,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 55
[2023-09-15 19:42:22,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 54
[2023-09-15 19:42:22,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 54
[2023-09-15 19:42:22,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 53
[2023-09-15 19:42:22,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 53
[2023-09-15 19:42:22,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 52
[2023-09-15 19:42:22,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 52
[2023-09-15 19:42:22,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 51
[2023-09-15 19:42:22,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,550] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,550] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,552] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,552] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 51
[2023-09-15 19:42:22,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 50
[2023-09-15 19:42:22,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,665] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,665] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 50
[2023-09-15 19:42:22,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 49
[2023-09-15 19:42:22,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 49
[2023-09-15 19:42:22,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 48
[2023-09-15 19:42:22,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:22,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:22,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 48
[2023-09-15 19:42:22,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 47
[2023-09-15 19:42:22,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:22,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:22,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 47
[2023-09-15 19:42:22,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 46
[2023-09-15 19:42:22,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:22,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:22,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 46
[2023-09-15 19:42:22,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 45
[2023-09-15 19:42:22,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:22,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:22,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:22,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:22,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:22,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:22,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:22,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:22,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:22,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:22,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:22,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:22,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 45
[2023-09-15 19:42:22,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 44
[2023-09-15 19:42:22,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:22,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:22,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:22,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:22,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:22,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:23,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:23,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:23,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:23,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:23,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:23,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:23,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:23,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:23,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:23,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:23,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:23,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:23,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 44
[2023-09-15 19:42:23,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 43
[2023-09-15 19:42:23,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 43
[2023-09-15 19:42:23,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 42
[2023-09-15 19:42:23,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,147] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,208] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 42
[2023-09-15 19:42:23,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 41
[2023-09-15 19:42:23,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 41
[2023-09-15 19:42:23,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 40
[2023-09-15 19:42:23,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,323] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 40
[2023-09-15 19:42:23,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 39
[2023-09-15 19:42:23,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 39
[2023-09-15 19:42:23,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 38
[2023-09-15 19:42:23,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 38
[2023-09-15 19:42:23,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 37
[2023-09-15 19:42:23,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 37
[2023-09-15 19:42:23,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 36
[2023-09-15 19:42:23,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 36
[2023-09-15 19:42:23,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 35
[2023-09-15 19:42:23,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 35
[2023-09-15 19:42:23,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 34
[2023-09-15 19:42:23,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:23,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:23,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:23,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:23,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 34
[2023-09-15 19:42:23,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 33
[2023-09-15 19:42:23,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:23,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:23,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:23,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:23,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:23,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:23,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:23,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:23,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:23,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:23,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:23,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:23,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:23,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:23,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:23,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:23,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:23,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:23,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:23,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:24,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:24,004] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:24,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:24,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:24,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:24,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:24,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:24,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:24,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 33
[2023-09-15 19:42:24,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 28
[2023-09-15 19:42:24,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 28
[2023-09-15 19:42:24,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 27
[2023-09-15 19:42:24,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 27
[2023-09-15 19:42:24,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 26
[2023-09-15 19:42:24,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 26
[2023-09-15 19:42:24,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25
[2023-09-15 19:42:24,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25
[2023-09-15 19:42:24,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 24
[2023-09-15 19:42:24,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 24
[2023-09-15 19:42:24,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23
[2023-09-15 19:42:24,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,665] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23
[2023-09-15 19:42:24,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 22
[2023-09-15 19:42:24,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,745] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 22
[2023-09-15 19:42:24,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21
[2023-09-15 19:42:24,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:24,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:24,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:24,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:24,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:24,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:24,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:24,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:24,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21
[2023-09-15 19:42:24,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 20
[2023-09-15 19:42:24,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:24,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:24,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:24,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 20
[2023-09-15 19:42:24,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:24,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19
[2023-09-15 19:42:24,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:24,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:24,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:24,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:24,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:24,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:24,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:24,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:24,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:24,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:24,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:24,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:24,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:24,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:24,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:24,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:25,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:25,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:25,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19
[2023-09-15 19:42:25,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 18
[2023-09-15 19:42:25,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:25,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:25,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:25,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:25,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:25,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 18
[2023-09-15 19:42:25,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:25,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17
[2023-09-15 19:42:25,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:25,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:25,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:25,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:25,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17
[2023-09-15 19:42:25,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16
[2023-09-15 19:42:25,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16
[2023-09-15 19:42:25,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-09-15 19:42:25,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-09-15 19:42:25,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-09-15 19:42:25,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-09-15 19:42:25,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-09-15 19:42:25,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-09-15 19:42:25,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-09-15 19:42:25,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,453] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,466] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-09-15 19:42:25,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-09-15 19:42:25,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,606] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-09-15 19:42:25,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-09-15 19:42:25,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-09-15 19:42:25,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-09-15 19:42:25,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:25,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-09-15 19:42:25,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-09-15 19:42:25,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:25,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:25,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-09-15 19:42:25,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-09-15 19:42:25,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:25,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-09-15 19:42:25,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-09-15 19:42:25,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-09-15 19:42:25,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-09-15 19:42:26,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:26,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:26,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-09-15 19:42:26,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-09-15 19:42:26,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:26,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:26,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:26,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-09-15 19:42:26,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
I0915 19:42:26.528874 140508328335104 logging_writer.py:48] [0] global_step=0, grad_norm=4.642488, loss=11.108852
I0915 19:42:26.542961 140548286359360 pytorch_submission_base.py:86] 0) loss = 11.109, grad_norm = 4.642
I0915 19:42:27.263346 140548286359360 spec.py:320] Evaluating on the training split.
I0915 19:42:27.267148 140548286359360 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0915 19:42:27.271176 140548286359360 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 19:42:27.316090 140548286359360 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:27,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,713] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,714] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,715] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,715] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 71
[2023-09-15 19:42:27,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,733] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,735] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,737] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 71
[2023-09-15 19:42:27,738] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:27,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:42:27,757] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,760] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,762] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,764] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:27,764] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:27,764] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:27,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:27,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 72
[2023-09-15 19:42:28,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,040] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,040] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,042] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,047] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,053] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 72
[2023-09-15 19:42:28,068] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:28,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,097] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,101] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,101] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,101] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,101] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,102] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,111] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,111] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,116] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,117] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 73
[2023-09-15 19:42:28,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,381] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,381] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,386] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,390] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,390] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,394] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,396] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,402] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,403] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,403] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,404] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 73
[2023-09-15 19:42:28,405] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:28,407] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,407] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,408] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,408] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,408] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,408] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,411] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,411] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:28,420] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,421] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,421] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,428] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,428] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,428] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,428] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,429] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,429] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,429] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:28,430] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:28,430] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:28,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,453] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:28,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 74
[2023-09-15 19:42:29,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,119] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,134] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,136] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,139] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 74
[2023-09-15 19:42:29,165] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:29,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:29,183] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:29,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:42:29,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,238] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,238] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,238] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,238] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,238] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,238] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,238] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,239] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,239] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,239] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,239] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,239] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,255] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,256] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,265] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:42:29,285] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:29,285] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:29,286] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:29,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:29,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 75
[2023-09-15 19:42:30,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,178] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,193] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,205] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,206] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,208] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,208] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,208] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,212] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,212] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,212] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,215] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,216] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,220] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,230] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,230] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,231] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,233] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,238] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,242] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,242] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,242] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,243] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,248] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,248] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,248] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,250] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,254] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,254] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,256] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,263] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,264] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,264] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,265] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,289] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,291] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,291] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,295] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,299] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,299] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,300] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 75
[2023-09-15 19:42:30,316] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,330] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:30,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:30,333] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,333] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,333] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,340] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:30,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:30,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:30,384] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:30,384] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:30,385] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:30,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 77
[2023-09-15 19:42:30,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:30,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 77
[2023-09-15 19:42:30,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 78
[2023-09-15 19:42:31,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,049] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,057] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,072] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,077] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,077] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,078] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,085] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,086] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,086] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,087] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,096] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,098] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,104] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,113] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,113] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,113] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,115] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,115] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,116] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,159] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,160] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,161] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,186] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,193] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,193] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 78
[2023-09-15 19:42:31,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,196] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,198] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,204] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,204] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,204] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,206] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,213] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,220] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,220] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,220] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,221] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,229] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,229] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,230] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,232] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,232] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,232] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,233] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,234] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,234] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,234] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,234] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,236] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,238] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,241] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,243] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,251] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,253] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,253] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,254] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,256] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,257] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 79
[2023-09-15 19:42:31,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,272] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,272] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,273] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,282] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,283] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,284] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,284] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,287] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,287] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,288] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,289] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,290] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,290] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,297] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,305] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 79
[2023-09-15 19:42:31,317] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,324] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,339] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,343] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,343] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,344] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,354] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,354] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,354] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,362] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,407] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,408] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,408] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,477] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,485] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 81
[2023-09-15 19:42:31,492] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,497] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,497] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,498] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,502] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,502] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,503] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,517] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,517] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,517] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,526] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,526] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,527] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,548] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,548] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,549] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 81
[2023-09-15 19:42:31,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,575] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,578] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,581] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,595] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,603] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,605] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,605] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,605] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,605] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,607] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,613] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,614] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,615] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 82
[2023-09-15 19:42:31,615] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,615] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,622] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,625] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,625] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,628] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,628] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,628] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 82
[2023-09-15 19:42:31,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,631] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,635] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,635] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,636] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,636] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,636] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,637] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,638] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,638] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,638] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,639] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,643] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,645] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,652] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,655] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,657] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,658] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,659] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,659] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,660] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,666] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,666] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,666] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,673] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,676] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,678] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,678] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,679] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,684] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,686] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,686] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,686] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,688] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,688] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,688] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 83
[2023-09-15 19:42:31,694] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,719] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,719] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,720] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,721] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,724] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,725] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,725] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,732] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 83
[2023-09-15 19:42:31,740] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:31,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:31,775] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,775] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,775] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,775] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,783] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:31,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:31,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,831] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:31,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,879] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:31,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:31,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,903] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:31,903] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,903] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,903] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:31,909] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,914] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 85
[2023-09-15 19:42:31,914] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,927] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,928] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,928] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,933] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,933] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,933] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,936] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:31,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:31,939] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,945] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:31,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:31,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:31,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:31,970] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:31,970] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:31,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:31,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:31,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:31,985] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:31,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 85
[2023-09-15 19:42:31,991] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:31,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:31,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:31,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:32,002] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:32,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,005] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:32,010] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:32,015] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,019] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,019] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,019] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:32,021] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,021] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,026] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,027] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,029] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,030] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,030] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,030] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,030] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,030] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,037] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,041] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 86
[2023-09-15 19:42:32,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,043] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,044] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,045] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,049] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,049] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,050] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:32,052] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,052] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,053] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 86
[2023-09-15 19:42:32,056] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,056] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,056] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,057] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,061] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:32,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,071] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,083] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,084] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,084] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,086] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,086] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,093] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,097] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,097] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,097] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,101] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,101] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,101] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,107] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,109] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,109] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,109] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:32,114] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 87
[2023-09-15 19:42:32,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,138] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,138] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,145] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,146] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,148] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,148] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,148] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,156] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 87
[2023-09-15 19:42:32,167] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,199] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,199] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,200] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,251] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,295] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,303] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,309] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,319] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,319] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,326] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 89
[2023-09-15 19:42:32,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,333] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,333] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,333] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,336] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,349] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,349] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,350] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,350] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,351] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,360] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,360] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,361] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,372] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,396] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,397] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,397] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,399] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 89
[2023-09-15 19:42:32,406] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,413] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,426] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,429] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,430] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,431] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,433] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,434] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,441] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,442] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,447] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,447] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,448] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 90
[2023-09-15 19:42:32,456] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,460] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,464] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,465] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,465] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,466] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 90
[2023-09-15 19:42:32,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,476] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,476] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,476] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,481] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,483] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,483] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,485] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,486] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,486] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,487] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,488] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,488] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,499] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,500] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,500] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,501] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,502] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,502] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,511] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,513] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,514] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,514] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,515] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,518] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,518] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,518] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,523] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,530] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,531] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,531] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 91
[2023-09-15 19:42:32,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,534] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,566] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,567] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,567] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,567] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,568] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,576] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 91
[2023-09-15 19:42:32,579] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,613] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,614] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,614] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,621] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,621] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,621] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,665] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,709] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,721] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,727] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,735] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,735] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,739] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 93
[2023-09-15 19:42:32,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,753] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,753] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,753] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,754] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,765] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,765] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,767] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,767] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,780] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,788] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,813] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,813] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,813] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,814] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 93
[2023-09-15 19:42:32,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,834] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,843] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,844] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,847] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,847] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,851] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,854] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,860] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,863] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 94
[2023-09-15 19:42:32,868] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,868] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,869] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,869] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,869] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,869] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,870] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,870] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,877] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,879] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,879] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,879] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,879] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 94
[2023-09-15 19:42:32,881] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,882] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,887] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,888] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,889] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,892] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,894] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,894] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,895] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,895] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,899] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,899] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,900] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,902] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,912] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,912] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,913] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,923] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,923] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,923] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,928] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,928] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,929] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,930] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,931] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,933] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,933] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,934] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,936] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:32,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 95
[2023-09-15 19:42:32,946] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,946] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,947] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,957] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:32,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:32,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:32,979] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,979] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,979] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:32,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:32,984] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:32,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:32,985] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:32,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 95
[2023-09-15 19:42:32,991] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,991] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:32,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:32,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,004] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,011] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:33,023] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:33,025] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,025] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,025] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,032] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:33,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,035] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,036] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,058] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:33,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,075] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,120] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,130] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,147] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,152] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,153] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 97
[2023-09-15 19:42:33,156] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,161] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,184] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,203] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,226] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 97
[2023-09-15 19:42:33,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,243] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,243] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,244] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,259] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,264] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,276] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,276] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,276] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 98
[2023-09-15 19:42:33,280] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,281] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,281] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,281] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,281] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,281] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 98
[2023-09-15 19:42:33,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,294] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,294] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,296] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,297] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,301] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,302] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,306] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,315] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,318] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,319] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,328] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,328] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,329] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,348] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,350] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,351] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,351] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,356] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,356] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,358] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,358] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,359] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:33,372] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,372] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,372] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,384] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,384] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,384] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,414] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,420] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,420] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,420] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 99
[2023-09-15 19:42:33,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 99
[2023-09-15 19:42:33,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:42:33,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,457] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,458] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,458] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,493] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,494] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 100
[2023-09-15 19:42:33,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,690] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,718] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,726] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,730] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,732] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,754] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,767] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,786] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,786] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,787] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,794] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,822] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,822] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,822] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,851] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,851] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 100
[2023-09-15 19:42:33,878] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:33,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:33,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,902] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,902] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,902] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:33,944] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:33,944] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:33,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:33,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 101
[2023-09-15 19:42:34,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,074] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,086] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,098] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,106] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,125] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,127] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,128] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,135] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,136] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,136] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,139] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,139] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,140] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,154] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,155] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,184] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,196] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,209] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,215] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,216] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,221] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,223] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,224] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,226] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,227] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,227] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,228] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,228] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,247] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,247] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,255] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,255] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 101
[2023-09-15 19:42:34,259] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,259] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,259] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,259] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,260] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,260] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,261] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,262] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,262] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,262] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,262] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,263] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,264] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,267] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,270] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,272] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,273] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,273] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,274] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,279] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,281] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,281] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,281] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,282] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,286] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,286] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,287] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,290] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,294] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,294] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,297] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,297] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,297] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,299] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,306] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,306] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,307] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,307] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 102
[2023-09-15 19:42:34,322] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,336] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,336] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,336] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,336] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,337] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 102
[2023-09-15 19:42:34,364] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,369] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,369] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,369] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,371] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,371] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,372] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,374] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,374] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,374] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,378] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,395] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,400] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,403] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,408] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,408] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,408] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,409] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,414] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,414] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,415] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,415] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,421] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,428] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,431] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,431] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,431] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,434] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,451] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,454] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,456] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,457] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,457] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,457] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,461] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,461] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,465] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,466] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,467] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,467] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,468] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 104
[2023-09-15 19:42:34,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,482] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,483] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,483] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,487] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,506] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,506] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,507] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,508] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,509] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,509] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,509] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,519] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,522] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,522] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,522] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,527] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,531] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,549] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 104
[2023-09-15 19:42:34,549] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,561] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,562] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,562] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,570] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,574] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,574] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,574] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,584] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,584] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,593] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,614] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:34,637] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,638] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,638] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,688] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,700] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 106
[2023-09-15 19:42:34,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,727] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,728] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,731] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,756] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,757] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,757] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,757] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,757] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,760] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 106
[2023-09-15 19:42:34,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,797] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,799] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,807] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,814] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,828] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,832] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,832] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,833] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,838] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,840] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,842] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,842] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,842] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,842] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,843] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 107
[2023-09-15 19:42:34,849] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,850] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,850] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 107
[2023-09-15 19:42:34,860] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,865] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,865] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,865] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,866] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,866] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,867] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,867] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,868] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,870] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,870] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,871] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,871] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,872] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,873] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,873] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,874] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,875] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,876] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,876] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,877] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,878] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,879] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,879] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,879] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,886] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,891] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,895] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,895] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,895] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:34,905] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,905] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,906] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,907] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,908] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,908] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,908] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:34,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,914] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,929] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 108
[2023-09-15 19:42:34,939] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,942] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:34,942] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,943] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:34,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,946] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,950] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:34,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:34,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:34,965] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,968] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:34,972] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 108
[2023-09-15 19:42:34,976] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,976] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:34,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:34,980] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:34,982] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:34,982] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:34,982] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:34,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:34,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,989] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:34,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:34,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:34,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:34,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,000] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,000] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:35,002] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:35,006] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,006] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,007] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,008] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,010] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,011] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:35,012] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,012] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,013] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,014] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:35,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,015] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,017] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,017] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,017] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,020] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,021] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,023] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,038] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,041] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,041] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,042] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,047] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,048] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,048] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,049] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,051] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,051] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,051] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,051] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,051] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,052] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:35,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,056] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,057] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,057] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,067] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:35,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,086] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 110
[2023-09-15 19:42:35,088] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,088] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,088] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,090] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,092] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,092] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,093] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,101] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,107] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,108] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,108] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,108] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,115] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:35,118] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,147] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 110
[2023-09-15 19:42:35,147] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,153] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,153] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,154] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,158] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,158] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,158] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,161] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,183] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,183] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,183] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,205] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,273] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,275] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,285] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,300] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,300] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,300] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,301] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,302] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,302] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,309] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,310] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 112
[2023-09-15 19:42:35,315] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,336] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,336] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,337] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,341] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,341] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,342] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,358] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,358] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,358] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,384] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 112
[2023-09-15 19:42:35,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,412] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,412] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,414] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,415] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,416] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,416] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,416] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,416] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,416] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,416] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,416] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,421] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,428] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,429] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 113
[2023-09-15 19:42:35,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 113
[2023-09-15 19:42:35,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,451] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,456] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,456] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,457] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,458] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,458] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,458] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,458] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,458] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,460] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,465] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,474] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,476] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,477] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,478] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,478] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,482] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,482] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,483] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,486] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,487] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,487] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,487] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,491] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,494] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,501] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,512] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,513] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,513] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 114
[2023-09-15 19:42:35,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,529] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,532] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,532] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,532] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,540] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,550] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 114
[2023-09-15 19:42:35,561] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,566] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,567] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,568] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,568] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,581] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,581] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,581] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,582] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,582] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,582] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,582] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,582] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,582] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,584] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,592] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,598] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,599] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,599] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,606] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,616] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,616] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,616] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,618] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,619] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,620] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,624] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,624] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,625] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,627] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,632] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,633] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,633] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,633] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,633] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,634] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,634] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,634] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,634] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,634] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,635] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,649] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,649] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,650] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,657] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,667] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 116
[2023-09-15 19:42:35,671] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,672] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,672] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,675] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,676] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,676] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,677] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,702] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,703] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,703] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,703] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,704] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,704] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,711] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,711] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 116
[2023-09-15 19:42:35,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:35,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,746] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,746] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,746] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,753] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:35,763] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:35,764] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,764] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,765] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,772] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:35,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,798] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,799] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,799] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:35,816] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,816] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,816] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:35,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:35,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:35,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:35,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:35,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:35,880] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,880] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,880] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,881] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,882] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:35,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:35,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 118
[2023-09-15 19:42:35,898] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:35,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:35,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:35,921] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,921] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,921] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:35,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,925] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,926] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,931] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:35,958] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,959] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,959] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:35,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:35,961] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:35,963] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 118
[2023-09-15 19:42:35,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:35,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:35,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:35,985] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:35,990] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:35,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:35,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:35,997] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,997] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,997] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:35,997] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:35,998] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:35,998] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:35,999] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:36,002] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,004] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:36,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,006] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:36,007] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:36,017] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,018] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,018] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 119
[2023-09-15 19:42:36,023] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,030] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,031] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,031] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,032] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 119
[2023-09-15 19:42:36,032] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,032] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,037] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,037] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,038] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,039] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,041] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:36,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,043] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,043] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,050] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,051] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,051] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,051] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,054] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:36,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,065] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,065] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,065] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,072] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,072] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,076] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,077] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,077] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,077] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,079] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,079] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,080] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:36,087] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 120
[2023-09-15 19:42:36,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:36,101] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,115] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,127] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,130] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:36,133] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,137] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,138] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 120
[2023-09-15 19:42:36,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,161] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,163] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,163] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,163] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,165] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,165] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,165] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,170] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,171] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,171] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,172] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,172] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,172] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,172] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,176] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,179] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,181] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,183] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,189] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,199] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,205] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,206] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,206] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,206] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,207] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,211] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,211] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,213] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,213] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,213] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,214] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,215] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,216] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,216] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,216] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,216] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,217] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,218] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,226] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 122
[2023-09-15 19:42:36,253] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,253] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,253] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,256] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,259] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,261] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,261] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,261] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,261] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,269] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,291] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,303] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,304] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,304] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,305] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,305] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,305] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,305] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,307] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,312] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 122
[2023-09-15 19:42:36,312] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,329] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,339] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,342] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,342] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,343] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,348] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,350] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,355] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,355] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,356] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,393] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,393] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,394] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,398] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,398] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,398] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,464] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,464] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,464] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 124
[2023-09-15 19:42:36,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,484] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,484] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,510] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,510] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,511] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,511] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,511] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,511] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,534] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,539] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,542] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,546] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 124
[2023-09-15 19:42:36,549] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,559] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,560] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,561] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,561] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,561] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,573] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,575] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,576] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,576] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,576] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,577] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,579] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,584] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,586] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,592] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 125
[2023-09-15 19:42:36,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,604] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,604] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,604] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,610] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,610] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,611] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 125
[2023-09-15 19:42:36,612] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,613] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,613] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,617] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,625] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,627] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,627] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,627] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,628] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,630] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,630] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,631] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,635] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,643] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,644] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,644] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,644] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,644] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,645] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,645] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,661] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,661] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,662] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,666] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,666] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 126
[2023-09-15 19:42:36,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,681] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,681] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,681] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,683] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,689] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,707] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,709] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,718] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 126
[2023-09-15 19:42:36,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,728] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,730] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,730] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:36,731] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,742] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,742] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,742] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,744] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,744] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,745] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,750] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,752] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,755] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:36,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,759] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:36,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,761] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,761] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,761] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,765] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,767] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,767] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,769] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,781] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,788] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,794] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,796] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,796] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,796] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,796] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,797] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,797] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,802] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,802] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,810] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,810] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,815] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,831] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 128
[2023-09-15 19:42:36,837] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,850] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,851] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,851] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,851] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,854] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,856] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,856] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,857] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:36,877] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:36,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,886] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,886] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,893] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 128
[2023-09-15 19:42:36,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:36,902] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,904] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,905] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,905] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:36,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,925] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:36,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:36,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:36,929] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,929] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,930] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,932] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,937] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:36,938] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,938] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,938] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:36,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:36,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:36,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:36,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,977] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,980] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:36,982] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:36,982] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:36,982] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:36,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:36,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:36,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:36,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:37,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:37,016] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,018] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:37,018] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,027] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,043] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,043] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,045] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,045] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:37,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:37,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:37,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 130
[2023-09-15 19:42:37,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:37,071] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:37,088] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,098] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,098] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,098] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,098] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:37,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,115] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,124] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,125] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:37,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,132] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 130
[2023-09-15 19:42:37,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:37,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,146] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,147] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,159] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,159] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,159] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,160] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,160] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,161] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,168] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,168] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:37,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,180] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,185] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,185] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,186] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 131
[2023-09-15 19:42:37,188] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,195] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,195] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,195] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,197] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 131
[2023-09-15 19:42:37,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,217] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,217] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,217] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,218] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,218] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,224] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,224] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,224] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,230] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,231] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,232] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,236] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,236] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,239] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,242] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,251] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,251] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,252] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,264] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,266] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 132
[2023-09-15 19:42:37,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,278] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,279] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,286] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,293] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,293] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,296] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,302] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,302] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,302] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,304] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,310] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,313] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,313] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,314] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 132
[2023-09-15 19:42:37,322] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,328] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,328] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,329] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,329] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,329] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,329] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,329] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,330] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:42:37,336] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,336] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,336] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,337] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,337] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,337] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,339] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,340] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,340] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,347] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,351] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:42:37,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,359] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:42:37,360] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,360] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,360] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,369] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,373] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,373] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,374] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,377] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,380] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,380] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,380] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,380] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,380] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,381] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,383] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,386] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,388] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,394] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,397] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,397] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,398] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,409] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,412] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,433] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,437] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,437] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 134
[2023-09-15 19:42:37,441] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,447] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,447] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,448] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,469] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,469] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,469] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,471] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,476] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,498] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,498] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,499] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 134
[2023-09-15 19:42:37,500] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,505] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,506] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,508] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:42:37,514] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,519] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,519] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:42:37,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,534] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:42:37,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:42:37,537] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,537] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,537] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,545] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,556] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,556] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,556] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,589] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,589] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,590] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,619] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,619] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,620] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,620] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,659] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,667] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,668] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 136
[2023-09-15 19:42:37,676] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,677] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,678] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,678] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,678] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,694] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,696] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,696] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,696] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,725] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,730] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,743] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,744] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,744] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,745] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,745] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 136
[2023-09-15 19:42:37,751] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,762] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,784] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,795] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 137
[2023-09-15 19:42:37,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,810] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 137
[2023-09-15 19:42:37,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:42:37,837] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:42:37,839] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:42:37,839] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:42:37,840] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:42:37,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,843] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:42:37,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 138
[2023-09-15 19:42:37,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 138
[2023-09-15 19:42:37,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0915 19:42:41.521204 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 19:47:26.692166 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 19:47:26.695272 140548286359360 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 19:47:26.699700 140548286359360 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 19:47:26.737612 140548286359360 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
[2023-09-15 19:47:30,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,434] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,501] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,505] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,524] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,524] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,524] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,524] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,524] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,524] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:30,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,525] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,525] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,526] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,527] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,527] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,528] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,528] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,551] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,551] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 139
[2023-09-15 19:47:30,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,693] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,759] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,760] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,760] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,763] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,767] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,785] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,785] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,785] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,785] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,786] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,786] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,786] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,786] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,786] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,787] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,787] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,787] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,787] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,788] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,788] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,789] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,789] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,790] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 139
[2023-09-15 19:47:30,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:30,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_src_mask
[2023-09-15 19:47:30,817] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:30,818] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:30,818] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:30,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:30,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 140
[2023-09-15 19:47:31,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,012] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,020] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,026] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,026] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,027] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,065] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,065] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,066] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,072] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,074] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,080] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,080] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,080] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,081] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,081] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,081] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,081] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,081] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,082] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,083] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,084] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,084] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,086] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,086] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,087] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,087] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,087] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,087] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 140
[2023-09-15 19:47:31,091] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_src_mask>
[2023-09-15 19:47:31,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,102] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_src_mask> (RETURN_VALUE)
[2023-09-15 19:47:31,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 141
[2023-09-15 19:47:31,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,284] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,327] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,330] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,331] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,331] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,332] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,335] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,336] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 141
[2023-09-15 19:47:31,358] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:31,373] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,373] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,373] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,375] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,375] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,376] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,377] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,377] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,378] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,378] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,378] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,378] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,378] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,385] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,385] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,385] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,402] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,403] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 142
[2023-09-15 19:47:31,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,646] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,646] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,646] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,660] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,664] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,666] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,681] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,681] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,681] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,681] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,681] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,681] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,682] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,682] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,682] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,688] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,691] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,691] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,695] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,696] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 142
[2023-09-15 19:47:31,713] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:31,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,721] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:31,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:31,737] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:31,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:31,738] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:31,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 143
[2023-09-15 19:47:32,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,046] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,068] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,072] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,076] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,076] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,091] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,094] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,096] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,096] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,101] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,116] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,119] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,157] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 143
[2023-09-15 19:47:32,157] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:32,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,174] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,174] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,175] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:32,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,189] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,190] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,190] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,191] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,200] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing multi_head_attention_forward
[2023-09-15 19:47:32,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,214] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,214] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,214] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,221] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,221] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,222] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing multi_head_attention_forward (RETURN_VALUE)
[2023-09-15 19:47:32,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,274] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 144
[2023-09-15 19:47:32,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,829] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,848] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,848] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,856] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,864] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,866] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,866] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,886] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,886] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,887] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,887] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,887] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,887] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,887] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,887] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,897] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,897] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,897] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,902] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,904] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,905] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,905] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,906] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,907] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,915] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,915] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,915] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,924] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,925] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,926] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 144
[2023-09-15 19:47:32,935] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,935] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,939] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,939] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,943] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,943] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:32,950] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,950] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:32,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,951] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,952] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,952] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,952] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,953] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,959] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:32,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:32,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,973] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,974] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,974] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,976] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:32,978] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:32,978] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:32,978] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:32,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,003] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,004] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 146
[2023-09-15 19:47:33,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 146
[2023-09-15 19:47:33,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 147
[2023-09-15 19:47:33,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,722] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,739] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,752] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,752] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,753] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,754] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,754] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,767] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,767] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,768] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,788] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,789] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,798] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,798] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,798] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,817] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,818] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,818] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 147
[2023-09-15 19:47:33,827] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,837] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,838] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,838] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,839] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,843] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,846] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,852] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,853] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,855] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,855] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,855] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,868] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,869] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,870] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,872] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,872] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,872] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,873] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,873] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,873] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,875] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,875] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,875] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,879] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,882] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,882] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,882] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,888] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,889] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,890] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 148
[2023-09-15 19:47:33,892] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,896] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,903] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,916] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,917] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,925] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,925] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,925] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,926] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,926] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,926] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,927] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,929] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,929] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,930] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,935] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,936] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,936] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,938] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,938] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,938] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,939] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 148
[2023-09-15 19:47:33,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,942] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,942] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,942] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,946] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:33,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:33,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:33,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,969] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,969] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:33,975] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:33,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,977] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,977] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:33,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:33,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:33,990] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:33,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:33,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,027] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,029] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,029] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,030] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 150
[2023-09-15 19:47:34,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,148] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,150] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,154] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,168] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,172] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,172] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,173] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,174] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,174] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,174] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,178] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,179] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,179] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 150
[2023-09-15 19:47:34,184] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,184] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,184] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,185] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,193] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,199] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,213] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,215] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,215] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,222] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,237] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,238] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 151
[2023-09-15 19:47:34,238] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 151
[2023-09-15 19:47:34,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,252] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,255] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,257] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,259] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,259] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,265] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,276] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,285] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,286] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,286] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,287] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,289] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,289] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,289] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,291] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,293] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,293] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,294] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,294] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,295] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,300] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,300] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,300] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,303] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,309] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,309] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,310] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,312] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,312] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,312] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,312] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,316] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 152
[2023-09-15 19:47:34,321] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,322] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,328] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,329] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,329] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,329] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,330] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,330] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,339] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,340] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,340] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,340] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,343] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,343] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,344] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,353] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,354] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,354] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,357] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,364] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,366] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,366] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,366] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 152
[2023-09-15 19:47:34,367] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,382] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,382] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,382] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,388] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,402] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,410] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,410] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,410] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,411] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,454] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,454] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,455] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 154
[2023-09-15 19:47:34,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,552] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,560] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,566] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,571] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,574] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,581] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,585] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,586] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,586] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,590] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,591] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,592] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,592] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,592] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,597] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,597] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,597] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,602] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,606] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 154
[2023-09-15 19:47:34,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,617] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,617] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,618] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,627] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,627] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,628] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,632] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,657] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,658] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 155
[2023-09-15 19:47:34,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,665] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,672] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 155
[2023-09-15 19:47:34,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,676] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,679] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,683] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,690] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,696] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,697] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,699] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,699] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,699] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,706] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,706] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,706] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,706] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,706] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,706] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,707] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,707] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,712] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,712] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,713] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,714] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,720] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,722] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,731] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,734] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,734] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,738] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 156
[2023-09-15 19:47:34,741] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,741] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,741] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,743] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,749] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,751] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,752] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,763] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,765] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,765] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,771] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,772] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,772] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,773] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,780] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,785] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,787] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,787] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,788] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,788] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 156
[2023-09-15 19:47:34,790] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,793] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:34,811] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,822] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:34,822] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:34,824] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,824] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,824] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,825] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,825] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,825] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,831] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,832] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:34,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:34,876] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,876] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,877] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:34,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:34,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 158
[2023-09-15 19:47:34,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:34,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:34,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:34,974] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:34,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:34,982] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:34,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,990] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:34,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:34,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:34,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:34,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:34,998] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:34,998] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:34,999] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:35,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:35,008] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:35,009] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,009] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,013] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,014] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,014] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,014] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,023] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:35,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 158
[2023-09-15 19:47:35,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,038] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,038] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,038] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,039] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:35,041] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,047] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,070] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,071] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,078] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 159
[2023-09-15 19:47:35,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,090] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,094] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,096] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 159
[2023-09-15 19:47:35,098] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,102] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,104] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,112] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,112] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,112] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,119] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,120] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,120] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,123] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,123] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,124] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,127] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,128] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,128] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,128] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,131] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,136] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,139] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,151] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,151] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,152] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,152] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,154] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,154] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,154] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,154] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,159] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 160
[2023-09-15 19:47:35,161] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,163] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,163] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,163] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,175] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,175] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,176] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,180] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,180] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,180] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,185] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,185] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,186] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,193] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,203] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,203] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,203] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,205] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,206] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,206] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,206] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,207] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 160
[2023-09-15 19:47:35,210] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,236] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,236] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,244] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,245] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,245] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,252] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,296] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,296] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,375] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 162
[2023-09-15 19:47:35,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,397] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,400] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,406] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,406] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,407] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,421] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,421] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,421] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,423] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,424] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,428] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,431] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,432] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,447] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,447] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,447] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,448] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 162
[2023-09-15 19:47:35,449] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,467] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,486] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,492] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 163
[2023-09-15 19:47:35,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,502] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 163
[2023-09-15 19:47:35,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,519] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,520] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,521] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,526] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,527] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,529] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,532] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,532] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,534] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,535] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,535] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,536] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,536] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,537] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,539] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,544] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,545] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,547] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,560] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,560] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,561] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,561] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,561] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,561] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,566] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,567] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,567] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,567] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,570] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,570] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,572] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,573] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,573] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,575] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,576] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 164
[2023-09-15 19:47:35,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,588] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,589] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,589] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,589] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,609] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,609] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,610] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,613] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,613] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,613] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,619] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,620] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,620] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 164
[2023-09-15 19:47:35,629] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:35,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:35,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:35,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,663] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,664] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,664] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,665] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,673] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:35,716] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 166
[2023-09-15 19:47:35,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,809] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,809] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,818] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,818] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,834] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,834] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,842] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,852] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,863] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 166
[2023-09-15 19:47:35,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,897] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,906] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,907] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,913] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,913] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,913] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 167
[2023-09-15 19:47:35,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:35,931] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,932] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,932] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,932] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,932] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:35,934] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 167
[2023-09-15 19:47:35,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:35,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:35,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:35,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:35,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:35,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:35,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,953] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,957] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,957] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,957] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,959] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,959] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,959] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:35,963] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:35,966] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,967] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,967] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:35,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:35,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:35,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:35,978] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:35,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:35,980] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:35,988] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:35,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:35,988] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:35,988] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:35,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:35,990] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:35,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:36,007] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,007] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,008] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,008] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,009] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:36,027] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,027] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,031] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,032] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,032] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,034] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,034] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,035] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:36,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,042] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,042] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:36,048] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:36,067] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,067] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,067] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 168
[2023-09-15 19:47:36,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 168
[2023-09-15 19:47:36,087] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_tgt_and_memory_mask
[2023-09-15 19:47:36,104] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,141] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,141] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,141] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 169
[2023-09-15 19:47:36,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,357] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,368] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,372] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,373] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,373] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,380] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,380] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,381] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,383] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,394] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,394] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,395] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,396] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,396] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,396] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,411] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,411] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,416] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,427] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,434] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,435] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,440] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,440] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,441] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,516] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 169
[2023-09-15 19:47:36,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,527] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,532] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,539] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,540] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,540] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,545] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,545] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,545] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 170
[2023-09-15 19:47:36,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,636] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,646] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,648] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,652] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,654] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,654] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,663] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,663] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,663] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,664] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,664] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,664] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,686] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,691] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,694] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,696] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,696] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,697] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,707] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,708] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,708] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,709] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,730] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,732] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,732] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,732] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,739] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,755] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,756] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,756] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,757] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,760] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,763] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,763] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,763] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,771] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,790] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 170
[2023-09-15 19:47:36,792] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in make_tgt_and_memory_mask>
[2023-09-15 19:47:36,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,802] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,802] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,805] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,806] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in make_tgt_and_memory_mask> (RETURN_VALUE)
[2023-09-15 19:47:36,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,807] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,808] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,808] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,820] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,820] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,820] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,826] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,826] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,826] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,826] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 171
[2023-09-15 19:47:36,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,837] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 171
[2023-09-15 19:47:36,838] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:36,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:36,899] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,899] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,899] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:36,900] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:36,900] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:36,900] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 172
[2023-09-15 19:47:37,566] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,566] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,571] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,595] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,595] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,596] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,598] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,598] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,598] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,616] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,616] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,616] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,625] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,643] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,643] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,643] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,644] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,644] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,673] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,673] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,674] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,674] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,674] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,674] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,679] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,700] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,703] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,711] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 172
[2023-09-15 19:47:37,713] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,717] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,717] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,718] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,718] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,719] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,720] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,725] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,727] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,729] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,730] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,730] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,741] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,741] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,741] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,741] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,741] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,742] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,750] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,753] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,753] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,754] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,755] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,756] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,756] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,762] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,771] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 173
[2023-09-15 19:47:37,776] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,793] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,797] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,797] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,798] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,803] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,803] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,812] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,812] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,813] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 173
[2023-09-15 19:47:37,822] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,830] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,830] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:37,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,850] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,850] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,850] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:37,857] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:37,858] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,858] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,858] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,868] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,869] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:37,873] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:37,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,876] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:37,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:37,886] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,887] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,893] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:37,894] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,894] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,894] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,894] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,902] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:37,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:37,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:37,911] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,911] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,911] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:37,916] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:37,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,918] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,919] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,919] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:37,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,927] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:37,929] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,929] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:37,931] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,931] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 175
[2023-09-15 19:47:37,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:37,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:37,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:37,954] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:37,961] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:37,961] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:37,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:37,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:37,963] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,963] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,963] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,964] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,964] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,964] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:37,971] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:37,972] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,973] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,975] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:37,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:37,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:37,983] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,983] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,983] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:37,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:37,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:37,990] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,990] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,990] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:37,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,991] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:37,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:37,992] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:37,998] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,000] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,006] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:38,007] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,008] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 175
[2023-09-15 19:47:38,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,016] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,016] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,016] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,041] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,042] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,044] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,044] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,044] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,045] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,046] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,050] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,050] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,050] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,058] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,097] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,097] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,098] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,101] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,101] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,102] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,127] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 177
[2023-09-15 19:47:38,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,198] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,210] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,216] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,216] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,217] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,225] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,225] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,226] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,236] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,236] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,242] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,252] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 177
[2023-09-15 19:47:38,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,265] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,268] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,269] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,269] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,269] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,278] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,292] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,292] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,292] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,295] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,305] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 178
[2023-09-15 19:47:38,307] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,318] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 178
[2023-09-15 19:47:38,321] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,327] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,330] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,335] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,335] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,335] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,335] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,344] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,344] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,344] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,351] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,352] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,352] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,352] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,353] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,354] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,354] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,354] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,355] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,355] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,355] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,359] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,363] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,370] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,370] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,372] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,379] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,379] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,389] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,389] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 179
[2023-09-15 19:47:38,390] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,390] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,390] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,397] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,406] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,407] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,407] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,409] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,409] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,409] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,410] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,413] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,413] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,414] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,416] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,421] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,425] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,431] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 179
[2023-09-15 19:47:38,435] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,443] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,443] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,443] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,448] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,448] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,469] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,470] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,471] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,471] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,471] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,472] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,472] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,473] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,478] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,480] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,487] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,490] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,498] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,506] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,507] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,507] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,508] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,508] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,515] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,515] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,516] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,516] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,516] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,524] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,527] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,527] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,527] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,532] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,535] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,542] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,542] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 181
[2023-09-15 19:47:38,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,549] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,550] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,550] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,559] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,559] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,560] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,561] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,564] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,564] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,565] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,566] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,568] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,569] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,572] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,573] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,576] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,578] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,579] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,580] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,586] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,586] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,586] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,587] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,607] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 181
[2023-09-15 19:47:38,608] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:38,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,619] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,620] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,628] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,630] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,630] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,630] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,630] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,630] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,638] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,638] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,639] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,641] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,641] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:38,642] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:38,643] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,644] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,644] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,644] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,644] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,644] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,651] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,694] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:38,695] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,696] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,697] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,697] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 183
[2023-09-15 19:47:38,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,787] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,796] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,807] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,808] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,814] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,814] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,815] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,824] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,824] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,824] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,835] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,835] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,836] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 183
[2023-09-15 19:47:38,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,867] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,877] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,881] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,881] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,881] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,883] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,883] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,894] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,894] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,894] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:38,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:38,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 184
[2023-09-15 19:47:38,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:38,917] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 184
[2023-09-15 19:47:38,920] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,920] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,925] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,930] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,934] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,934] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,934] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,942] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,944] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,944] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,944] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,947] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,948] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,948] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,952] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,952] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,953] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,955] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,955] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,955] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:38,961] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,963] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:38,966] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,968] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,969] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,970] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:38,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:38,975] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:38,979] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,979] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,980] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:38,985] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 185
[2023-09-15 19:47:38,991] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:38,991] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:38,991] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:38,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:38,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:38,999] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:38,999] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,000] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,006] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,006] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,006] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,007] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,011] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,014] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,014] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,015] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,022] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:39,028] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 185
[2023-09-15 19:47:39,034] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,034] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,035] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,035] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,043] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,043] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,050] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,050] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,050] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,065] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,066] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,066] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,067] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,069] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,071] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,072] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,072] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,073] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,074] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,077] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,079] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,081] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,087] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,091] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,091] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,092] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,102] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,106] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,106] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,108] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,108] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,109] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,113] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,113] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,115] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,116] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,116] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,116] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,126] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,127] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,127] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,132] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,137] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,144] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 187
[2023-09-15 19:47:39,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,149] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,157] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,160] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,160] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,161] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,169] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,172] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,172] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,172] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,178] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,181] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,181] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,183] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,186] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,200] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 187
[2023-09-15 19:47:39,208] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,222] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,222] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,223] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,236] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,237] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,237] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,241] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,241] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,244] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,245] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,245] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,247] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,254] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,291] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,291] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,296] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,298] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,298] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,299] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 189
[2023-09-15 19:47:39,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,386] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,405] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,408] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,415] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,415] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,415] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,435] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,435] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,436] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,437] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,437] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,446] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 189
[2023-09-15 19:47:39,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,457] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,467] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,475] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,476] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,476] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,477] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,486] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,487] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,487] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,496] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,496] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,496] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,496] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 190
[2023-09-15 19:47:39,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,515] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 190
[2023-09-15 19:47:39,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,519] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,520] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,526] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,527] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,534] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,534] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,535] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,542] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,545] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,545] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,546] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,554] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,554] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,555] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,556] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,557] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,557] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,559] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,559] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,559] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,565] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,567] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,568] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,570] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,571] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,571] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,575] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,576] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,580] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 191
[2023-09-15 19:47:39,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,592] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,593] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,593] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,593] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,595] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,596] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,596] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,596] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,596] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,596] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,603] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,604] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,605] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,606] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,606] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,617] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,618] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,618] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,626] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,628] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,631] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,631] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,632] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 191
[2023-09-15 19:47:39,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,641] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,641] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,641] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:39,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,653] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,653] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,654] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,665] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,665] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,666] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:39,672] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,674] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:39,675] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,675] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,676] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,680] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,684] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,684] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,694] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,695] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,700] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,701] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,701] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,702] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,707] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,709] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,710] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,710] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,711] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,712] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,718] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,718] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,730] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,732] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,732] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,732] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,732] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,738] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,739] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,740] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,747] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 193
[2023-09-15 19:47:39,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,763] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,763] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,767] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,770] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,770] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,770] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,776] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,778] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,785] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,785] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,785] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,786] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,787] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,802] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,803] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,811] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 193
[2023-09-15 19:47:39,813] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,821] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:39,822] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,823] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,823] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,831] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,831] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,832] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:39,838] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,840] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,841] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,841] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,846] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,847] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:39,849] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,849] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:39,850] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,850] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:39,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,890] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,892] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,893] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,893] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:39,902] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:39,903] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:39,903] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:39,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:39,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:39,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:39,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:39,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:39,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 195
[2023-09-15 19:47:39,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:39,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:39,993] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:39,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:40,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,015] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,021] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,042] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:40,050] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,056] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 195
[2023-09-15 19:47:40,059] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,059] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,060] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,067] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,076] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,077] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,077] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,078] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,083] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,084] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,084] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,086] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,102] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,103] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 196
[2023-09-15 19:47:40,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,119] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,124] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 196
[2023-09-15 19:47:40,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,131] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,139] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,142] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,143] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,143] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,145] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,149] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,150] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,151] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,154] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,159] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,159] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,159] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,161] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,163] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,163] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,164] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,165] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,171] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,171] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,179] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,179] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,180] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,183] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,186] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,187] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,190] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 197
[2023-09-15 19:47:40,194] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,196] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,199] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,199] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,199] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,199] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,199] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,200] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,204] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,213] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,222] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,222] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,222] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,226] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,227] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,227] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,231] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,234] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,234] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,234] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,240] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,240] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,240] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 197
[2023-09-15 19:47:40,241] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,245] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,262] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,262] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,263] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,270] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,270] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,270] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,278] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,278] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,279] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,279] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,279] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,280] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,287] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,305] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,305] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,305] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,306] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,314] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,314] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,315] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,315] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,315] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,316] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,316] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,320] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,321] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,328] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,334] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,334] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,335] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,337] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,344] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 199
[2023-09-15 19:47:40,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,356] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,357] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,361] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,368] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,368] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,368] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,369] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,370] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,370] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,371] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,371] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,372] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,373] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,374] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,374] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,378] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,383] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,389] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,389] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,398] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,398] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,399] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,403] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,408] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,411] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,411] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 199
[2023-09-15 19:47:40,414] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,422] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,422] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,427] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,427] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,428] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,440] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,440] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,440] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,449] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,452] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,461] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,493] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,494] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,505] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,505] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,505] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 201
[2023-09-15 19:47:40,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,595] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,623] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,624] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,624] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,629] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,633] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,642] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,642] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,642] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,643] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,648] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 201
[2023-09-15 19:47:40,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,658] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,662] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,662] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,663] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,672] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,672] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,672] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,675] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,676] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,676] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,677] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,704] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,704] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,704] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,704] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 202
[2023-09-15 19:47:40,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,714] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 202
[2023-09-15 19:47:40,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,730] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,730] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,741] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,742] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,742] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,743] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,743] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,744] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,750] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,752] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,767] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,776] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,778] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,778] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,780] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,785] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,785] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,788] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,788] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,789] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,789] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,792] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 203
[2023-09-15 19:47:40,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,793] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,796] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,801] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,806] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,811] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,814] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,815] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,815] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,819] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,822] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,822] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,822] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,823] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,823] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,824] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,824] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,828] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,832] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 203
[2023-09-15 19:47:40,840] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _sa_block
[2023-09-15 19:47:40,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,856] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,856] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,857] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,861] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,861] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,861] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,863] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,869] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _sa_block>
[2023-09-15 19:47:40,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _sa_block> (RETURN_VALUE)
[2023-09-15 19:47:40,877] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,877] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,877] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,877] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,878] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,879] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,886] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,894] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,896] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,896] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,896] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,902] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,913] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,913] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,913] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,914] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,915] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,915] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,921] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,922] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,923] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,929] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,930] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,931] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,931] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,951] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,951] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 205
[2023-09-15 19:47:40,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,956] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,957] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,958] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,958] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,959] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,959] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,963] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,963] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,963] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,964] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:40,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,966] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,968] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,968] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,968] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,971] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:40,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:40,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:40,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:40,983] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,991] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:40,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:40,993] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:40,994] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:40,994] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:40,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:40,996] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,001] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,004] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:41,004] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,009] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,010] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,011] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,011] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,014] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 205
[2023-09-15 19:47:41,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,015] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,015] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,016] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:41,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _mha_block
[2023-09-15 19:47:41,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:41,032] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:41,033] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,033] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,033] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-15 19:47:41,041] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,043] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,046] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,046] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,046] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _mha_block>
[2023-09-15 19:47:41,049] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _mha_block> (RETURN_VALUE)
[2023-09-15 19:47:41,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,051] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,051] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,052] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,060] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,067] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,084] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,084] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,085] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,101] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,104] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 207
[2023-09-15 19:47:41,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,195] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,211] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,214] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,215] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,215] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,218] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,230] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,230] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,230] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,234] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,240] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,250] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,253] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,253] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,254] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 207
[2023-09-15 19:47:41,259] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,259] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,259] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,272] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,279] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,289] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,291] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,291] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,310] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 208
[2023-09-15 19:47:41,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 208
[2023-09-15 19:47:41,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,329] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,330] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,330] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-09-15 19:47:41,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-09-15 19:47:41,352] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-09-15 19:47:41,352] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-09-15 19:47:41,353] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-09-15 19:47:41,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 209
[2023-09-15 19:47:41,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,466] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,487] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,507] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,514] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,530] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,566] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,581] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-09-15 19:47:41,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 209
[2023-09-15 19:47:41,598] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0915 19:47:41.647943 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 19:52:21.236208 140548286359360 spec.py:348] Evaluating on the test split.
I0915 19:52:21.239169 140548286359360 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 19:52:21.243100 140548286359360 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0915 19:52:21.281836 140548286359360 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0915 19:52:25.151577 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 19:57:10.532746 140548286359360 submission_runner.py:376] Time since start: 925.35s, 	Step: 1, 	{'train/accuracy': 0.0006508186613687744, 'train/loss': 11.135978283208878, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.156982709451835, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.158046452849922, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 41.35803174972534, 'total_duration': 925.3467218875885, 'accumulated_submission_time': 41.35803174972534, 'accumulated_eval_time': 883.2694389820099, 'accumulated_logging_time': 0}
I0915 19:57:10.553211 140493530027776 logging_writer.py:48] [1] accumulated_eval_time=883.269439, accumulated_logging_time=0, accumulated_submission_time=41.358032, global_step=1, preemption_count=0, score=41.358032, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.158046, test/num_examples=3003, total_duration=925.346722, train/accuracy=0.000651, train/bleu=0.000000, train/loss=11.135978, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.156983, validation/num_examples=3000
I0915 19:57:11.057757 139637639219008 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.057781 140526367590208 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.057816 139742592976704 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.057822 140119655679808 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.057756 140548286359360 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.058073 140589617661760 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.058124 139958421567296 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.058388 140311716239168 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0915 19:57:11.882239 140493521635072 logging_writer.py:48] [1] global_step=1, grad_norm=4.714504, loss=11.109563
I0915 19:57:11.885904 140548286359360 pytorch_submission_base.py:86] 1) loss = 11.110, grad_norm = 4.715
I0915 19:57:12.326047 140493530027776 logging_writer.py:48] [2] global_step=2, grad_norm=4.697052, loss=11.078406
I0915 19:57:12.329437 140548286359360 pytorch_submission_base.py:86] 2) loss = 11.078, grad_norm = 4.697
I0915 19:57:12.773816 140493521635072 logging_writer.py:48] [3] global_step=3, grad_norm=4.619716, loss=11.029185
I0915 19:57:12.777294 140548286359360 pytorch_submission_base.py:86] 3) loss = 11.029, grad_norm = 4.620
I0915 19:57:13.220298 140493530027776 logging_writer.py:48] [4] global_step=4, grad_norm=4.412922, loss=10.961264
I0915 19:57:13.223984 140548286359360 pytorch_submission_base.py:86] 4) loss = 10.961, grad_norm = 4.413
I0915 19:57:13.659914 140493521635072 logging_writer.py:48] [5] global_step=5, grad_norm=4.374550, loss=10.866673
I0915 19:57:13.663351 140548286359360 pytorch_submission_base.py:86] 5) loss = 10.867, grad_norm = 4.375
I0915 19:57:14.105372 140493530027776 logging_writer.py:48] [6] global_step=6, grad_norm=4.177505, loss=10.762038
I0915 19:57:14.108732 140548286359360 pytorch_submission_base.py:86] 6) loss = 10.762, grad_norm = 4.178
I0915 19:57:14.546789 140493521635072 logging_writer.py:48] [7] global_step=7, grad_norm=3.996431, loss=10.649790
I0915 19:57:14.550806 140548286359360 pytorch_submission_base.py:86] 7) loss = 10.650, grad_norm = 3.996
I0915 19:57:14.990286 140493530027776 logging_writer.py:48] [8] global_step=8, grad_norm=3.742446, loss=10.527918
I0915 19:57:14.994050 140548286359360 pytorch_submission_base.py:86] 8) loss = 10.528, grad_norm = 3.742
I0915 19:57:15.442361 140493521635072 logging_writer.py:48] [9] global_step=9, grad_norm=3.489715, loss=10.396173
I0915 19:57:15.446223 140548286359360 pytorch_submission_base.py:86] 9) loss = 10.396, grad_norm = 3.490
I0915 19:57:15.892242 140493530027776 logging_writer.py:48] [10] global_step=10, grad_norm=3.206810, loss=10.280360
I0915 19:57:15.895637 140548286359360 pytorch_submission_base.py:86] 10) loss = 10.280, grad_norm = 3.207
I0915 19:57:16.335034 140493521635072 logging_writer.py:48] [11] global_step=11, grad_norm=2.882800, loss=10.152913
I0915 19:57:16.338891 140548286359360 pytorch_submission_base.py:86] 11) loss = 10.153, grad_norm = 2.883
I0915 19:57:16.778850 140493530027776 logging_writer.py:48] [12] global_step=12, grad_norm=2.644985, loss=10.015602
I0915 19:57:16.782593 140548286359360 pytorch_submission_base.py:86] 12) loss = 10.016, grad_norm = 2.645
I0915 19:57:17.218286 140493521635072 logging_writer.py:48] [13] global_step=13, grad_norm=2.365857, loss=9.898936
I0915 19:57:17.221936 140548286359360 pytorch_submission_base.py:86] 13) loss = 9.899, grad_norm = 2.366
I0915 19:57:17.664096 140493530027776 logging_writer.py:48] [14] global_step=14, grad_norm=2.159907, loss=9.772491
I0915 19:57:17.668033 140548286359360 pytorch_submission_base.py:86] 14) loss = 9.772, grad_norm = 2.160
I0915 19:57:18.110441 140493521635072 logging_writer.py:48] [15] global_step=15, grad_norm=1.921758, loss=9.670920
I0915 19:57:18.114243 140548286359360 pytorch_submission_base.py:86] 15) loss = 9.671, grad_norm = 1.922
I0915 19:57:18.553059 140493530027776 logging_writer.py:48] [16] global_step=16, grad_norm=1.714676, loss=9.590659
I0915 19:57:18.556317 140548286359360 pytorch_submission_base.py:86] 16) loss = 9.591, grad_norm = 1.715
I0915 19:57:18.998822 140493521635072 logging_writer.py:48] [17] global_step=17, grad_norm=1.535622, loss=9.508074
I0915 19:57:19.002534 140548286359360 pytorch_submission_base.py:86] 17) loss = 9.508, grad_norm = 1.536
I0915 19:57:19.442159 140493530027776 logging_writer.py:48] [18] global_step=18, grad_norm=1.412161, loss=9.422986
I0915 19:57:19.445425 140548286359360 pytorch_submission_base.py:86] 18) loss = 9.423, grad_norm = 1.412
I0915 19:57:19.880723 140493521635072 logging_writer.py:48] [19] global_step=19, grad_norm=1.280759, loss=9.356669
I0915 19:57:19.884094 140548286359360 pytorch_submission_base.py:86] 19) loss = 9.357, grad_norm = 1.281
I0915 19:57:20.327032 140493530027776 logging_writer.py:48] [20] global_step=20, grad_norm=1.181338, loss=9.273204
I0915 19:57:20.330677 140548286359360 pytorch_submission_base.py:86] 20) loss = 9.273, grad_norm = 1.181
I0915 19:57:20.768659 140493521635072 logging_writer.py:48] [21] global_step=21, grad_norm=1.078885, loss=9.248567
I0915 19:57:20.772503 140548286359360 pytorch_submission_base.py:86] 21) loss = 9.249, grad_norm = 1.079
I0915 19:57:21.211441 140493530027776 logging_writer.py:48] [22] global_step=22, grad_norm=0.998385, loss=9.157983
I0915 19:57:21.215543 140548286359360 pytorch_submission_base.py:86] 22) loss = 9.158, grad_norm = 0.998
I0915 19:57:21.657002 140493521635072 logging_writer.py:48] [23] global_step=23, grad_norm=0.920088, loss=9.086573
I0915 19:57:21.660511 140548286359360 pytorch_submission_base.py:86] 23) loss = 9.087, grad_norm = 0.920
I0915 19:57:22.094873 140493530027776 logging_writer.py:48] [24] global_step=24, grad_norm=0.840183, loss=9.056772
I0915 19:57:22.098397 140548286359360 pytorch_submission_base.py:86] 24) loss = 9.057, grad_norm = 0.840
I0915 19:57:22.539399 140493521635072 logging_writer.py:48] [25] global_step=25, grad_norm=0.771281, loss=8.997570
I0915 19:57:22.543186 140548286359360 pytorch_submission_base.py:86] 25) loss = 8.998, grad_norm = 0.771
I0915 19:57:22.981312 140493530027776 logging_writer.py:48] [26] global_step=26, grad_norm=0.724172, loss=8.967136
I0915 19:57:22.985335 140548286359360 pytorch_submission_base.py:86] 26) loss = 8.967, grad_norm = 0.724
I0915 19:57:23.424818 140493521635072 logging_writer.py:48] [27] global_step=27, grad_norm=0.672018, loss=8.935188
I0915 19:57:23.428552 140548286359360 pytorch_submission_base.py:86] 27) loss = 8.935, grad_norm = 0.672
I0915 19:57:23.870463 140493530027776 logging_writer.py:48] [28] global_step=28, grad_norm=0.625145, loss=8.898276
I0915 19:57:23.874019 140548286359360 pytorch_submission_base.py:86] 28) loss = 8.898, grad_norm = 0.625
I0915 19:57:24.314872 140493521635072 logging_writer.py:48] [29] global_step=29, grad_norm=0.579497, loss=8.862956
I0915 19:57:24.318243 140548286359360 pytorch_submission_base.py:86] 29) loss = 8.863, grad_norm = 0.579
I0915 19:57:24.756211 140493530027776 logging_writer.py:48] [30] global_step=30, grad_norm=0.544587, loss=8.817842
I0915 19:57:24.759572 140548286359360 pytorch_submission_base.py:86] 30) loss = 8.818, grad_norm = 0.545
I0915 19:57:25.199405 140493521635072 logging_writer.py:48] [31] global_step=31, grad_norm=0.498757, loss=8.794974
I0915 19:57:25.203238 140548286359360 pytorch_submission_base.py:86] 31) loss = 8.795, grad_norm = 0.499
I0915 19:57:25.643393 140493530027776 logging_writer.py:48] [32] global_step=32, grad_norm=0.475836, loss=8.740007
I0915 19:57:25.646742 140548286359360 pytorch_submission_base.py:86] 32) loss = 8.740, grad_norm = 0.476
I0915 19:57:26.092559 140493521635072 logging_writer.py:48] [33] global_step=33, grad_norm=0.440912, loss=8.739637
I0915 19:57:26.096503 140548286359360 pytorch_submission_base.py:86] 33) loss = 8.740, grad_norm = 0.441
I0915 19:57:26.537017 140493530027776 logging_writer.py:48] [34] global_step=34, grad_norm=0.416619, loss=8.720278
I0915 19:57:26.540316 140548286359360 pytorch_submission_base.py:86] 34) loss = 8.720, grad_norm = 0.417
I0915 19:57:26.977947 140493521635072 logging_writer.py:48] [35] global_step=35, grad_norm=0.386272, loss=8.705688
I0915 19:57:26.981296 140548286359360 pytorch_submission_base.py:86] 35) loss = 8.706, grad_norm = 0.386
I0915 19:57:27.416765 140493530027776 logging_writer.py:48] [36] global_step=36, grad_norm=0.356263, loss=8.718344
I0915 19:57:27.420077 140548286359360 pytorch_submission_base.py:86] 36) loss = 8.718, grad_norm = 0.356
I0915 19:57:27.857015 140493521635072 logging_writer.py:48] [37] global_step=37, grad_norm=0.346217, loss=8.686371
I0915 19:57:27.860426 140548286359360 pytorch_submission_base.py:86] 37) loss = 8.686, grad_norm = 0.346
I0915 19:57:28.297500 140493530027776 logging_writer.py:48] [38] global_step=38, grad_norm=0.331531, loss=8.692321
I0915 19:57:28.300883 140548286359360 pytorch_submission_base.py:86] 38) loss = 8.692, grad_norm = 0.332
I0915 19:57:28.743385 140493521635072 logging_writer.py:48] [39] global_step=39, grad_norm=0.313864, loss=8.627992
I0915 19:57:28.746923 140548286359360 pytorch_submission_base.py:86] 39) loss = 8.628, grad_norm = 0.314
I0915 19:57:29.186239 140493530027776 logging_writer.py:48] [40] global_step=40, grad_norm=0.289169, loss=8.627100
I0915 19:57:29.189509 140548286359360 pytorch_submission_base.py:86] 40) loss = 8.627, grad_norm = 0.289
I0915 19:57:29.626523 140493521635072 logging_writer.py:48] [41] global_step=41, grad_norm=0.274487, loss=8.627892
I0915 19:57:29.629840 140548286359360 pytorch_submission_base.py:86] 41) loss = 8.628, grad_norm = 0.274
I0915 19:57:30.068088 140493530027776 logging_writer.py:48] [42] global_step=42, grad_norm=0.265922, loss=8.572852
I0915 19:57:30.071636 140548286359360 pytorch_submission_base.py:86] 42) loss = 8.573, grad_norm = 0.266
I0915 19:57:30.514199 140493521635072 logging_writer.py:48] [43] global_step=43, grad_norm=0.256927, loss=8.559829
I0915 19:57:30.517544 140548286359360 pytorch_submission_base.py:86] 43) loss = 8.560, grad_norm = 0.257
I0915 19:57:30.954218 140493530027776 logging_writer.py:48] [44] global_step=44, grad_norm=0.243771, loss=8.559529
I0915 19:57:30.957473 140548286359360 pytorch_submission_base.py:86] 44) loss = 8.560, grad_norm = 0.244
I0915 19:57:31.405225 140493521635072 logging_writer.py:48] [45] global_step=45, grad_norm=0.234059, loss=8.541197
I0915 19:57:31.408516 140548286359360 pytorch_submission_base.py:86] 45) loss = 8.541, grad_norm = 0.234
I0915 19:57:31.851010 140493530027776 logging_writer.py:48] [46] global_step=46, grad_norm=0.226973, loss=8.526566
I0915 19:57:31.854432 140548286359360 pytorch_submission_base.py:86] 46) loss = 8.527, grad_norm = 0.227
I0915 19:57:32.291705 140493521635072 logging_writer.py:48] [47] global_step=47, grad_norm=0.244177, loss=8.560655
I0915 19:57:32.295003 140548286359360 pytorch_submission_base.py:86] 47) loss = 8.561, grad_norm = 0.244
I0915 19:57:32.736225 140493530027776 logging_writer.py:48] [48] global_step=48, grad_norm=0.217911, loss=8.516784
I0915 19:57:32.739448 140548286359360 pytorch_submission_base.py:86] 48) loss = 8.517, grad_norm = 0.218
I0915 19:57:33.175553 140493521635072 logging_writer.py:48] [49] global_step=49, grad_norm=0.205190, loss=8.520723
I0915 19:57:33.178937 140548286359360 pytorch_submission_base.py:86] 49) loss = 8.521, grad_norm = 0.205
I0915 19:57:33.620026 140493530027776 logging_writer.py:48] [50] global_step=50, grad_norm=0.196896, loss=8.492880
I0915 19:57:33.623492 140548286359360 pytorch_submission_base.py:86] 50) loss = 8.493, grad_norm = 0.197
I0915 19:57:34.064457 140493521635072 logging_writer.py:48] [51] global_step=51, grad_norm=0.195332, loss=8.495515
I0915 19:57:34.068706 140548286359360 pytorch_submission_base.py:86] 51) loss = 8.496, grad_norm = 0.195
I0915 19:57:34.504627 140493530027776 logging_writer.py:48] [52] global_step=52, grad_norm=0.190875, loss=8.493289
I0915 19:57:34.508179 140548286359360 pytorch_submission_base.py:86] 52) loss = 8.493, grad_norm = 0.191
I0915 19:57:34.949254 140493521635072 logging_writer.py:48] [53] global_step=53, grad_norm=0.191921, loss=8.486636
I0915 19:57:34.952646 140548286359360 pytorch_submission_base.py:86] 53) loss = 8.487, grad_norm = 0.192
I0915 19:57:35.393228 140493530027776 logging_writer.py:48] [54] global_step=54, grad_norm=0.186085, loss=8.480528
I0915 19:57:35.396622 140548286359360 pytorch_submission_base.py:86] 54) loss = 8.481, grad_norm = 0.186
I0915 19:57:35.833245 140493521635072 logging_writer.py:48] [55] global_step=55, grad_norm=0.180983, loss=8.503448
I0915 19:57:35.836608 140548286359360 pytorch_submission_base.py:86] 55) loss = 8.503, grad_norm = 0.181
I0915 19:57:36.278977 140493530027776 logging_writer.py:48] [56] global_step=56, grad_norm=0.169638, loss=8.504334
I0915 19:57:36.282592 140548286359360 pytorch_submission_base.py:86] 56) loss = 8.504, grad_norm = 0.170
I0915 19:57:36.721309 140493521635072 logging_writer.py:48] [57] global_step=57, grad_norm=0.176362, loss=8.438704
I0915 19:57:36.724856 140548286359360 pytorch_submission_base.py:86] 57) loss = 8.439, grad_norm = 0.176
I0915 19:57:37.160642 140493530027776 logging_writer.py:48] [58] global_step=58, grad_norm=0.168052, loss=8.464149
I0915 19:57:37.164539 140548286359360 pytorch_submission_base.py:86] 58) loss = 8.464, grad_norm = 0.168
I0915 19:57:37.605355 140493521635072 logging_writer.py:48] [59] global_step=59, grad_norm=0.163221, loss=8.485380
I0915 19:57:37.608624 140548286359360 pytorch_submission_base.py:86] 59) loss = 8.485, grad_norm = 0.163
I0915 19:57:38.044283 140493530027776 logging_writer.py:48] [60] global_step=60, grad_norm=0.163686, loss=8.444736
I0915 19:57:38.047506 140548286359360 pytorch_submission_base.py:86] 60) loss = 8.445, grad_norm = 0.164
I0915 19:57:38.488119 140493521635072 logging_writer.py:48] [61] global_step=61, grad_norm=0.173166, loss=8.469415
I0915 19:57:38.491456 140548286359360 pytorch_submission_base.py:86] 61) loss = 8.469, grad_norm = 0.173
I0915 19:57:38.931666 140493530027776 logging_writer.py:48] [62] global_step=62, grad_norm=0.167502, loss=8.435513
I0915 19:57:38.935013 140548286359360 pytorch_submission_base.py:86] 62) loss = 8.436, grad_norm = 0.168
I0915 19:57:39.371748 140493521635072 logging_writer.py:48] [63] global_step=63, grad_norm=0.164225, loss=8.422443
I0915 19:57:39.375178 140548286359360 pytorch_submission_base.py:86] 63) loss = 8.422, grad_norm = 0.164
I0915 19:57:39.817469 140493530027776 logging_writer.py:48] [64] global_step=64, grad_norm=0.168433, loss=8.423389
I0915 19:57:39.820833 140548286359360 pytorch_submission_base.py:86] 64) loss = 8.423, grad_norm = 0.168
I0915 19:57:40.269862 140493521635072 logging_writer.py:48] [65] global_step=65, grad_norm=0.166172, loss=8.423890
I0915 19:57:40.273524 140548286359360 pytorch_submission_base.py:86] 65) loss = 8.424, grad_norm = 0.166
I0915 19:57:40.709111 140493530027776 logging_writer.py:48] [66] global_step=66, grad_norm=0.159967, loss=8.425185
I0915 19:57:40.712447 140548286359360 pytorch_submission_base.py:86] 66) loss = 8.425, grad_norm = 0.160
I0915 19:57:41.153159 140493521635072 logging_writer.py:48] [67] global_step=67, grad_norm=0.170817, loss=8.360500
I0915 19:57:41.157115 140548286359360 pytorch_submission_base.py:86] 67) loss = 8.361, grad_norm = 0.171
I0915 19:57:41.594479 140493530027776 logging_writer.py:48] [68] global_step=68, grad_norm=0.171681, loss=8.353218
I0915 19:57:41.598206 140548286359360 pytorch_submission_base.py:86] 68) loss = 8.353, grad_norm = 0.172
I0915 19:57:42.035080 140493521635072 logging_writer.py:48] [69] global_step=69, grad_norm=0.167997, loss=8.411517
I0915 19:57:42.038547 140548286359360 pytorch_submission_base.py:86] 69) loss = 8.412, grad_norm = 0.168
I0915 19:57:42.477758 140493530027776 logging_writer.py:48] [70] global_step=70, grad_norm=0.168062, loss=8.372856
I0915 19:57:42.481280 140548286359360 pytorch_submission_base.py:86] 70) loss = 8.373, grad_norm = 0.168
I0915 19:57:42.917399 140493521635072 logging_writer.py:48] [71] global_step=71, grad_norm=0.175693, loss=8.381618
I0915 19:57:42.920707 140548286359360 pytorch_submission_base.py:86] 71) loss = 8.382, grad_norm = 0.176
I0915 19:57:43.364292 140493530027776 logging_writer.py:48] [72] global_step=72, grad_norm=0.180130, loss=8.377695
I0915 19:57:43.368114 140548286359360 pytorch_submission_base.py:86] 72) loss = 8.378, grad_norm = 0.180
I0915 19:57:43.806782 140493521635072 logging_writer.py:48] [73] global_step=73, grad_norm=0.170542, loss=8.325723
I0915 19:57:43.810423 140548286359360 pytorch_submission_base.py:86] 73) loss = 8.326, grad_norm = 0.171
I0915 19:57:44.247001 140493530027776 logging_writer.py:48] [74] global_step=74, grad_norm=0.174618, loss=8.328978
I0915 19:57:44.250219 140548286359360 pytorch_submission_base.py:86] 74) loss = 8.329, grad_norm = 0.175
I0915 19:57:44.691483 140493521635072 logging_writer.py:48] [75] global_step=75, grad_norm=0.171126, loss=8.351382
I0915 19:57:44.694934 140548286359360 pytorch_submission_base.py:86] 75) loss = 8.351, grad_norm = 0.171
I0915 19:57:45.134802 140493530027776 logging_writer.py:48] [76] global_step=76, grad_norm=0.163903, loss=8.306022
I0915 19:57:45.138106 140548286359360 pytorch_submission_base.py:86] 76) loss = 8.306, grad_norm = 0.164
I0915 19:57:45.572198 140493521635072 logging_writer.py:48] [77] global_step=77, grad_norm=0.162912, loss=8.301367
I0915 19:57:45.575562 140548286359360 pytorch_submission_base.py:86] 77) loss = 8.301, grad_norm = 0.163
I0915 19:57:46.015388 140493530027776 logging_writer.py:48] [78] global_step=78, grad_norm=0.185967, loss=8.320298
I0915 19:57:46.018717 140548286359360 pytorch_submission_base.py:86] 78) loss = 8.320, grad_norm = 0.186
I0915 19:57:46.457143 140493521635072 logging_writer.py:48] [79] global_step=79, grad_norm=0.164207, loss=8.289322
I0915 19:57:46.460681 140548286359360 pytorch_submission_base.py:86] 79) loss = 8.289, grad_norm = 0.164
I0915 19:57:46.896634 140493530027776 logging_writer.py:48] [80] global_step=80, grad_norm=0.158921, loss=8.300054
I0915 19:57:46.900126 140548286359360 pytorch_submission_base.py:86] 80) loss = 8.300, grad_norm = 0.159
I0915 19:57:47.339740 140493521635072 logging_writer.py:48] [81] global_step=81, grad_norm=0.159294, loss=8.274737
I0915 19:57:47.343123 140548286359360 pytorch_submission_base.py:86] 81) loss = 8.275, grad_norm = 0.159
I0915 19:57:47.779230 140493530027776 logging_writer.py:48] [82] global_step=82, grad_norm=0.176391, loss=8.256371
I0915 19:57:47.782488 140548286359360 pytorch_submission_base.py:86] 82) loss = 8.256, grad_norm = 0.176
I0915 19:57:48.225966 140493521635072 logging_writer.py:48] [83] global_step=83, grad_norm=0.167612, loss=8.294355
I0915 19:57:48.230245 140548286359360 pytorch_submission_base.py:86] 83) loss = 8.294, grad_norm = 0.168
I0915 19:57:48.668848 140493530027776 logging_writer.py:48] [84] global_step=84, grad_norm=0.160982, loss=8.242666
I0915 19:57:48.672654 140548286359360 pytorch_submission_base.py:86] 84) loss = 8.243, grad_norm = 0.161
I0915 19:57:49.110835 140493521635072 logging_writer.py:48] [85] global_step=85, grad_norm=0.173139, loss=8.263972
I0915 19:57:49.114931 140548286359360 pytorch_submission_base.py:86] 85) loss = 8.264, grad_norm = 0.173
I0915 19:57:49.558395 140493530027776 logging_writer.py:48] [86] global_step=86, grad_norm=0.165952, loss=8.254041
I0915 19:57:49.562127 140548286359360 pytorch_submission_base.py:86] 86) loss = 8.254, grad_norm = 0.166
I0915 19:57:50.004165 140493521635072 logging_writer.py:48] [87] global_step=87, grad_norm=0.156389, loss=8.246026
I0915 19:57:50.008240 140548286359360 pytorch_submission_base.py:86] 87) loss = 8.246, grad_norm = 0.156
I0915 19:57:50.453135 140493530027776 logging_writer.py:48] [88] global_step=88, grad_norm=0.165248, loss=8.205330
I0915 19:57:50.456583 140548286359360 pytorch_submission_base.py:86] 88) loss = 8.205, grad_norm = 0.165
I0915 19:57:50.900830 140493521635072 logging_writer.py:48] [89] global_step=89, grad_norm=0.157698, loss=8.178493
I0915 19:57:50.904306 140548286359360 pytorch_submission_base.py:86] 89) loss = 8.178, grad_norm = 0.158
I0915 19:57:51.346751 140493530027776 logging_writer.py:48] [90] global_step=90, grad_norm=0.160974, loss=8.245362
I0915 19:57:51.350192 140548286359360 pytorch_submission_base.py:86] 90) loss = 8.245, grad_norm = 0.161
I0915 19:57:51.786896 140493521635072 logging_writer.py:48] [91] global_step=91, grad_norm=0.183396, loss=8.238174
I0915 19:57:51.790338 140548286359360 pytorch_submission_base.py:86] 91) loss = 8.238, grad_norm = 0.183
I0915 19:57:52.234711 140493530027776 logging_writer.py:48] [92] global_step=92, grad_norm=0.156798, loss=8.201494
I0915 19:57:52.238190 140548286359360 pytorch_submission_base.py:86] 92) loss = 8.201, grad_norm = 0.157
I0915 19:57:52.676903 140493521635072 logging_writer.py:48] [93] global_step=93, grad_norm=0.166152, loss=8.154131
I0915 19:57:52.680711 140548286359360 pytorch_submission_base.py:86] 93) loss = 8.154, grad_norm = 0.166
I0915 19:57:53.115928 140493530027776 logging_writer.py:48] [94] global_step=94, grad_norm=0.156081, loss=8.203350
I0915 19:57:53.119661 140548286359360 pytorch_submission_base.py:86] 94) loss = 8.203, grad_norm = 0.156
I0915 19:57:53.561363 140493521635072 logging_writer.py:48] [95] global_step=95, grad_norm=0.158046, loss=8.160057
I0915 19:57:53.565412 140548286359360 pytorch_submission_base.py:86] 95) loss = 8.160, grad_norm = 0.158
I0915 19:57:54.001217 140493530027776 logging_writer.py:48] [96] global_step=96, grad_norm=0.154824, loss=8.171914
I0915 19:57:54.005331 140548286359360 pytorch_submission_base.py:86] 96) loss = 8.172, grad_norm = 0.155
I0915 19:57:54.445575 140493521635072 logging_writer.py:48] [97] global_step=97, grad_norm=0.171472, loss=8.200259
I0915 19:57:54.448915 140548286359360 pytorch_submission_base.py:86] 97) loss = 8.200, grad_norm = 0.171
I0915 19:57:54.890025 140493530027776 logging_writer.py:48] [98] global_step=98, grad_norm=0.160511, loss=8.165243
I0915 19:57:54.893550 140548286359360 pytorch_submission_base.py:86] 98) loss = 8.165, grad_norm = 0.161
I0915 19:57:55.330016 140493521635072 logging_writer.py:48] [99] global_step=99, grad_norm=0.164649, loss=8.155352
I0915 19:57:55.333316 140548286359360 pytorch_submission_base.py:86] 99) loss = 8.155, grad_norm = 0.165
I0915 19:57:55.773638 140493530027776 logging_writer.py:48] [100] global_step=100, grad_norm=0.167861, loss=8.162290
I0915 19:57:55.777193 140548286359360 pytorch_submission_base.py:86] 100) loss = 8.162, grad_norm = 0.168
I0915 20:00:49.162575 140493521635072 logging_writer.py:48] [500] global_step=500, grad_norm=0.521443, loss=5.657552
I0915 20:00:49.166228 140548286359360 pytorch_submission_base.py:86] 500) loss = 5.658, grad_norm = 0.521
I0915 20:04:25.772733 140493530027776 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.486322, loss=4.292418
I0915 20:04:25.777069 140548286359360 pytorch_submission_base.py:86] 1000) loss = 4.292, grad_norm = 0.486
I0915 20:08:02.597895 140493521635072 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.404390, loss=3.494610
I0915 20:08:02.601581 140548286359360 pytorch_submission_base.py:86] 1500) loss = 3.495, grad_norm = 0.404
I0915 20:11:11.685245 140548286359360 spec.py:320] Evaluating on the training split.
I0915 20:11:15.451502 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:14:05.482675 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 20:14:09.167392 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:17:00.014098 140548286359360 spec.py:348] Evaluating on the test split.
I0915 20:17:03.752101 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:19:35.932176 140548286359360 submission_runner.py:376] Time since start: 2270.75s, 	Step: 1935, 	{'train/accuracy': 0.4978049420162605, 'train/loss': 3.0021800916793047, 'train/bleu': 21.73098218033474, 'validation/accuracy': 0.49529454067525513, 'validation/loss': 3.011394775018289, 'validation/bleu': 17.356840239080277, 'validation/num_examples': 3000, 'test/accuracy': 0.49426529545058395, 'test/loss': 3.076523371680902, 'test/bleu': 15.678565306236928, 'test/num_examples': 3003, 'score': 880.4625396728516, 'total_duration': 2270.7462015151978, 'accumulated_submission_time': 880.4625396728516, 'accumulated_eval_time': 1387.516426563263, 'accumulated_logging_time': 0.030379533767700195}
I0915 20:19:35.950683 140493530027776 logging_writer.py:48] [1935] accumulated_eval_time=1387.516427, accumulated_logging_time=0.030380, accumulated_submission_time=880.462540, global_step=1935, preemption_count=0, score=880.462540, test/accuracy=0.494265, test/bleu=15.678565, test/loss=3.076523, test/num_examples=3003, total_duration=2270.746202, train/accuracy=0.497805, train/bleu=21.730982, train/loss=3.002180, validation/accuracy=0.495295, validation/bleu=17.356840, validation/loss=3.011395, validation/num_examples=3000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0915 20:20:05.339842 140493521635072 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.396520, loss=3.108572
I0915 20:20:05.343274 140548286359360 pytorch_submission_base.py:86] 2000) loss = 3.109, grad_norm = 0.397
I0915 20:23:42.027561 140493530027776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.241273, loss=2.812683
I0915 20:23:42.031230 140548286359360 pytorch_submission_base.py:86] 2500) loss = 2.813, grad_norm = 0.241
I0915 20:27:18.819622 140493521635072 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.231354, loss=2.603038
I0915 20:27:18.823562 140548286359360 pytorch_submission_base.py:86] 3000) loss = 2.603, grad_norm = 0.231
I0915 20:30:55.574191 140493530027776 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.175114, loss=2.427578
I0915 20:30:55.577855 140548286359360 pytorch_submission_base.py:86] 3500) loss = 2.428, grad_norm = 0.175
I0915 20:33:36.891757 140548286359360 spec.py:320] Evaluating on the training split.
I0915 20:33:40.635391 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:36:11.968867 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 20:36:15.646715 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:38:26.758625 140548286359360 spec.py:348] Evaluating on the test split.
I0915 20:38:30.494000 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:40:36.781005 140548286359360 submission_runner.py:376] Time since start: 3531.60s, 	Step: 3871, 	{'train/accuracy': 0.5657211483472492, 'train/loss': 2.3478211140340846, 'train/bleu': 26.325749844391794, 'validation/accuracy': 0.5775749835711894, 'validation/loss': 2.24222634406269, 'validation/bleu': 22.37134960329752, 'validation/num_examples': 3000, 'test/accuracy': 0.5799779210969729, 'test/loss': 2.2346400920922664, 'test/bleu': 21.045066845584646, 'test/num_examples': 3003, 'score': 1719.399917125702, 'total_duration': 3531.5950195789337, 'accumulated_submission_time': 1719.399917125702, 'accumulated_eval_time': 1807.405702829361, 'accumulated_logging_time': 0.060216426849365234}
I0915 20:40:36.797535 140493521635072 logging_writer.py:48] [3871] accumulated_eval_time=1807.405703, accumulated_logging_time=0.060216, accumulated_submission_time=1719.399917, global_step=3871, preemption_count=0, score=1719.399917, test/accuracy=0.579978, test/bleu=21.045067, test/loss=2.234640, test/num_examples=3003, total_duration=3531.595020, train/accuracy=0.565721, train/bleu=26.325750, train/loss=2.347821, validation/accuracy=0.577575, validation/bleu=22.371350, validation/loss=2.242226, validation/num_examples=3000
I0915 20:41:33.944072 140493530027776 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.174567, loss=2.484328
I0915 20:41:33.947458 140548286359360 pytorch_submission_base.py:86] 4000) loss = 2.484, grad_norm = 0.175
I0915 20:45:10.720617 140493521635072 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.161358, loss=2.304741
I0915 20:45:10.724290 140548286359360 pytorch_submission_base.py:86] 4500) loss = 2.305, grad_norm = 0.161
I0915 20:48:47.334408 140493530027776 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.137757, loss=2.137098
I0915 20:48:47.338206 140548286359360 pytorch_submission_base.py:86] 5000) loss = 2.137, grad_norm = 0.138
I0915 20:52:23.975271 140493521635072 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.175494, loss=2.279732
I0915 20:52:23.979016 140548286359360 pytorch_submission_base.py:86] 5500) loss = 2.280, grad_norm = 0.175
I0915 20:54:37.838318 140548286359360 spec.py:320] Evaluating on the training split.
I0915 20:54:41.580012 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:57:16.800335 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 20:57:20.456285 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 20:59:30.316841 140548286359360 spec.py:348] Evaluating on the test split.
I0915 20:59:34.042069 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:01:36.265869 140548286359360 submission_runner.py:376] Time since start: 4791.08s, 	Step: 5808, 	{'train/accuracy': 0.5918068596392914, 'train/loss': 2.1194177154930864, 'train/bleu': 27.87840393026368, 'validation/accuracy': 0.6071840398755131, 'validation/loss': 2.005180499931805, 'validation/bleu': 24.73060151531517, 'validation/num_examples': 3000, 'test/accuracy': 0.6141072569868108, 'test/loss': 1.9634082127709023, 'test/bleu': 23.361010994382085, 'test/num_examples': 3003, 'score': 2558.530163526535, 'total_duration': 4791.079871892929, 'accumulated_submission_time': 2558.530163526535, 'accumulated_eval_time': 2225.833331823349, 'accumulated_logging_time': 0.08784604072570801}
I0915 21:01:36.282975 140493530027776 logging_writer.py:48] [5808] accumulated_eval_time=2225.833332, accumulated_logging_time=0.087846, accumulated_submission_time=2558.530164, global_step=5808, preemption_count=0, score=2558.530164, test/accuracy=0.614107, test/bleu=23.361011, test/loss=1.963408, test/num_examples=3003, total_duration=4791.079872, train/accuracy=0.591807, train/bleu=27.878404, train/loss=2.119418, validation/accuracy=0.607184, validation/bleu=24.730602, validation/loss=2.005180, validation/num_examples=3000
I0915 21:03:00.629106 140493521635072 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.192989, loss=2.177813
I0915 21:03:00.633277 140548286359360 pytorch_submission_base.py:86] 6000) loss = 2.178, grad_norm = 0.193
I0915 21:06:37.312089 140493521635072 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.151126, loss=2.215014
I0915 21:06:37.316540 140548286359360 pytorch_submission_base.py:86] 6500) loss = 2.215, grad_norm = 0.151
I0915 21:10:14.158940 140502817015552 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.145777, loss=2.187649
I0915 21:10:14.163412 140548286359360 pytorch_submission_base.py:86] 7000) loss = 2.188, grad_norm = 0.146
I0915 21:13:50.803353 140493521635072 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.145540, loss=2.023890
I0915 21:13:50.807773 140548286359360 pytorch_submission_base.py:86] 7500) loss = 2.024, grad_norm = 0.146
I0915 21:15:37.185336 140548286359360 spec.py:320] Evaluating on the training split.
I0915 21:15:40.956320 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:17:58.387398 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 21:18:02.054866 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:20:06.958254 140548286359360 spec.py:348] Evaluating on the test split.
I0915 21:20:10.684687 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:22:06.558827 140548286359360 submission_runner.py:376] Time since start: 6021.37s, 	Step: 7745, 	{'train/accuracy': 0.6064239194789817, 'train/loss': 2.0064540209272668, 'train/bleu': 28.997343628428016, 'validation/accuracy': 0.6214926039354751, 'validation/loss': 1.8730328049249234, 'validation/bleu': 25.470898909959008, 'validation/num_examples': 3000, 'test/accuracy': 0.6278542792400209, 'test/loss': 1.8213873031782, 'test/bleu': 24.526511249281437, 'test/num_examples': 3003, 'score': 3397.5152928829193, 'total_duration': 6021.372844219208, 'accumulated_submission_time': 3397.5152928829193, 'accumulated_eval_time': 2615.20689702034, 'accumulated_logging_time': 0.11474442481994629}
I0915 21:22:06.576251 140502817015552 logging_writer.py:48] [7745] accumulated_eval_time=2615.206897, accumulated_logging_time=0.114744, accumulated_submission_time=3397.515293, global_step=7745, preemption_count=0, score=3397.515293, test/accuracy=0.627854, test/bleu=24.526511, test/loss=1.821387, test/num_examples=3003, total_duration=6021.372844, train/accuracy=0.606424, train/bleu=28.997344, train/loss=2.006454, validation/accuracy=0.621493, validation/bleu=25.470899, validation/loss=1.873033, validation/num_examples=3000
I0915 21:23:58.291637 140493521635072 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.157760, loss=2.119666
I0915 21:23:58.295299 140548286359360 pytorch_submission_base.py:86] 8000) loss = 2.120, grad_norm = 0.158
I0915 21:27:34.904175 140502817015552 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.222912, loss=2.010321
I0915 21:27:34.908002 140548286359360 pytorch_submission_base.py:86] 8500) loss = 2.010, grad_norm = 0.223
I0915 21:31:11.674537 140493521635072 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.185353, loss=2.094258
I0915 21:31:11.678062 140548286359360 pytorch_submission_base.py:86] 9000) loss = 2.094, grad_norm = 0.185
I0915 21:34:48.403626 140502817015552 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.143004, loss=2.012781
I0915 21:34:48.407323 140548286359360 pytorch_submission_base.py:86] 9500) loss = 2.013, grad_norm = 0.143
I0915 21:36:07.637327 140548286359360 spec.py:320] Evaluating on the training split.
I0915 21:36:11.410026 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:38:34.144274 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 21:38:37.805734 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:40:42.334880 140548286359360 spec.py:348] Evaluating on the test split.
I0915 21:40:46.060246 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:42:55.086644 140548286359360 submission_runner.py:376] Time since start: 7269.90s, 	Step: 9682, 	{'train/accuracy': 0.6133668948393489, 'train/loss': 1.9398628187678675, 'train/bleu': 29.83521448326555, 'validation/accuracy': 0.6335321322736234, 'validation/loss': 1.7975104539931308, 'validation/bleu': 26.513102722186588, 'validation/num_examples': 3000, 'test/accuracy': 0.6393701702399628, 'test/loss': 1.7417320318401022, 'test/bleu': 25.2091620892064, 'test/num_examples': 3003, 'score': 4236.662819385529, 'total_duration': 7269.9006135463715, 'accumulated_submission_time': 4236.662819385529, 'accumulated_eval_time': 3022.656242609024, 'accumulated_logging_time': 0.14206647872924805}
I0915 21:42:55.103133 140493521635072 logging_writer.py:48] [9682] accumulated_eval_time=3022.656243, accumulated_logging_time=0.142066, accumulated_submission_time=4236.662819, global_step=9682, preemption_count=0, score=4236.662819, test/accuracy=0.639370, test/bleu=25.209162, test/loss=1.741732, test/num_examples=3003, total_duration=7269.900614, train/accuracy=0.613367, train/bleu=29.835214, train/loss=1.939863, validation/accuracy=0.633532, validation/bleu=26.513103, validation/loss=1.797510, validation/num_examples=3000
I0915 21:45:13.997130 140502817015552 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.171907, loss=1.989380
I0915 21:45:14.001649 140548286359360 pytorch_submission_base.py:86] 10000) loss = 1.989, grad_norm = 0.172
I0915 21:48:50.662219 140493521635072 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.252439, loss=2.103708
I0915 21:48:50.666052 140548286359360 pytorch_submission_base.py:86] 10500) loss = 2.104, grad_norm = 0.252
I0915 21:52:27.431692 140502817015552 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.251994, loss=2.093699
I0915 21:52:27.436067 140548286359360 pytorch_submission_base.py:86] 11000) loss = 2.094, grad_norm = 0.252
I0915 21:56:04.025883 140493521635072 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.169920, loss=1.933287
I0915 21:56:04.030404 140548286359360 pytorch_submission_base.py:86] 11500) loss = 1.933, grad_norm = 0.170
I0915 21:56:55.881750 140548286359360 spec.py:320] Evaluating on the training split.
I0915 21:56:59.638519 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 21:59:29.252914 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 21:59:32.920113 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:01:45.436146 140548286359360 spec.py:348] Evaluating on the test split.
I0915 22:01:49.163169 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:03:50.193806 140548286359360 submission_runner.py:376] Time since start: 8525.01s, 	Step: 11619, 	{'train/accuracy': 0.6197812496428449, 'train/loss': 1.89744398379373, 'train/bleu': 30.243587393120052, 'validation/accuracy': 0.6409219972473993, 'validation/loss': 1.7486977067860288, 'validation/bleu': 26.96995823870777, 'validation/num_examples': 3000, 'test/accuracy': 0.6481436290744291, 'test/loss': 1.6893959531694847, 'test/bleu': 26.04726519570716, 'test/num_examples': 3003, 'score': 5075.5725355148315, 'total_duration': 8525.007823467255, 'accumulated_submission_time': 5075.5725355148315, 'accumulated_eval_time': 3436.968344449997, 'accumulated_logging_time': 0.16846919059753418}
I0915 22:03:50.210789 140502817015552 logging_writer.py:48] [11619] accumulated_eval_time=3436.968344, accumulated_logging_time=0.168469, accumulated_submission_time=5075.572536, global_step=11619, preemption_count=0, score=5075.572536, test/accuracy=0.648144, test/bleu=26.047265, test/loss=1.689396, test/num_examples=3003, total_duration=8525.007823, train/accuracy=0.619781, train/bleu=30.243587, train/loss=1.897444, validation/accuracy=0.640922, validation/bleu=26.969958, validation/loss=1.748698, validation/num_examples=3000
I0915 22:06:36.457857 140493521635072 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.185466, loss=1.998482
I0915 22:06:36.461500 140548286359360 pytorch_submission_base.py:86] 12000) loss = 1.998, grad_norm = 0.185
I0915 22:10:13.355427 140502817015552 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.161650, loss=2.051016
I0915 22:10:13.360083 140548286359360 pytorch_submission_base.py:86] 12500) loss = 2.051, grad_norm = 0.162
I0915 22:13:50.176555 140493521635072 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.156526, loss=1.987890
I0915 22:13:50.180255 140548286359360 pytorch_submission_base.py:86] 13000) loss = 1.988, grad_norm = 0.157
I0915 22:17:26.984948 140502817015552 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.167907, loss=2.028226
I0915 22:17:26.988966 140548286359360 pytorch_submission_base.py:86] 13500) loss = 2.028, grad_norm = 0.168
I0915 22:17:51.083706 140548286359360 spec.py:320] Evaluating on the training split.
I0915 22:17:54.853198 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:20:37.240698 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 22:20:40.904984 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:22:48.744088 140548286359360 spec.py:348] Evaluating on the test split.
I0915 22:22:52.471088 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:24:53.735158 140548286359360 submission_runner.py:376] Time since start: 9788.55s, 	Step: 13555, 	{'train/accuracy': 0.6268828344699601, 'train/loss': 1.8221518392859175, 'train/bleu': 30.193807832160665, 'validation/accuracy': 0.6434886114245328, 'validation/loss': 1.712724617487694, 'validation/bleu': 26.894849047664977, 'validation/num_examples': 3000, 'test/accuracy': 0.651978385915984, 'test/loss': 1.6516769798384754, 'test/bleu': 25.993052063563592, 'test/num_examples': 3003, 'score': 5914.606507778168, 'total_duration': 9788.549141168594, 'accumulated_submission_time': 5914.606507778168, 'accumulated_eval_time': 3859.6198971271515, 'accumulated_logging_time': 0.19640374183654785}
I0915 22:24:53.751999 140493521635072 logging_writer.py:48] [13555] accumulated_eval_time=3859.619897, accumulated_logging_time=0.196404, accumulated_submission_time=5914.606508, global_step=13555, preemption_count=0, score=5914.606508, test/accuracy=0.651978, test/bleu=25.993052, test/loss=1.651677, test/num_examples=3003, total_duration=9788.549141, train/accuracy=0.626883, train/bleu=30.193808, train/loss=1.822152, validation/accuracy=0.643489, validation/bleu=26.894849, validation/loss=1.712725, validation/num_examples=3000
I0915 22:28:07.721672 140502817015552 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.149076, loss=1.859911
I0915 22:28:07.725511 140548286359360 pytorch_submission_base.py:86] 14000) loss = 1.860, grad_norm = 0.149
I0915 22:31:44.474393 140493521635072 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.165565, loss=1.885981
I0915 22:31:44.478196 140548286359360 pytorch_submission_base.py:86] 14500) loss = 1.886, grad_norm = 0.166
I0915 22:35:21.350467 140502817015552 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.321365, loss=1.877138
I0915 22:35:21.355108 140548286359360 pytorch_submission_base.py:86] 15000) loss = 1.877, grad_norm = 0.321
I0915 22:38:54.537351 140548286359360 spec.py:320] Evaluating on the training split.
I0915 22:38:58.289841 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:41:45.095926 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 22:41:48.757472 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:44:00.415296 140548286359360 spec.py:348] Evaluating on the test split.
I0915 22:44:04.157996 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 22:46:14.220669 140548286359360 submission_runner.py:376] Time since start: 11069.03s, 	Step: 15491, 	{'train/accuracy': 0.6255072463768115, 'train/loss': 1.8254012148337595, 'train/bleu': 29.912893293052104, 'validation/accuracy': 0.6453980731795018, 'validation/loss': 1.6932260604332245, 'validation/bleu': 27.041283680647236, 'validation/num_examples': 3000, 'test/accuracy': 0.65558073325199, 'test/loss': 1.6255545145546453, 'test/bleu': 26.439915719755504, 'test/num_examples': 3003, 'score': 6753.552028656006, 'total_duration': 11069.034653186798, 'accumulated_submission_time': 6753.552028656006, 'accumulated_eval_time': 4299.303379774094, 'accumulated_logging_time': 0.22470474243164062}
I0915 22:46:14.237643 140493521635072 logging_writer.py:48] [15491] accumulated_eval_time=4299.303380, accumulated_logging_time=0.224705, accumulated_submission_time=6753.552029, global_step=15491, preemption_count=0, score=6753.552029, test/accuracy=0.655581, test/bleu=26.439916, test/loss=1.625555, test/num_examples=3003, total_duration=11069.034653, train/accuracy=0.625507, train/bleu=29.912893, train/loss=1.825401, validation/accuracy=0.645398, validation/bleu=27.041284, validation/loss=1.693226, validation/num_examples=3000
I0915 22:46:19.323981 140502817015552 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.197642, loss=1.894743
I0915 22:46:19.327427 140548286359360 pytorch_submission_base.py:86] 15500) loss = 1.895, grad_norm = 0.198
I0915 22:49:55.702555 140493521635072 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.168753, loss=1.828844
I0915 22:49:55.706598 140548286359360 pytorch_submission_base.py:86] 16000) loss = 1.829, grad_norm = 0.169
I0915 22:53:32.310412 140502817015552 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.162302, loss=1.952399
I0915 22:53:32.314973 140548286359360 pytorch_submission_base.py:86] 16500) loss = 1.952, grad_norm = 0.162
I0915 22:57:09.095312 140493521635072 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.218835, loss=1.945587
I0915 22:57:09.098897 140548286359360 pytorch_submission_base.py:86] 17000) loss = 1.946, grad_norm = 0.219
I0915 23:00:15.319386 140548286359360 spec.py:320] Evaluating on the training split.
I0915 23:00:19.061162 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:04:11.482054 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 23:04:15.135972 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:07:01.489045 140548286359360 spec.py:348] Evaluating on the test split.
I0915 23:07:05.207851 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:09:05.618680 140548286359360 submission_runner.py:376] Time since start: 12440.43s, 	Step: 17429, 	{'train/accuracy': 0.6272884243747927, 'train/loss': 1.8329014247978868, 'train/bleu': 30.486603052150425, 'validation/accuracy': 0.6499113464185193, 'validation/loss': 1.667441933454018, 'validation/bleu': 27.47786028943392, 'validation/num_examples': 3000, 'test/accuracy': 0.6568473650572308, 'test/loss': 1.5986300549067456, 'test/bleu': 26.58617215076298, 'test/num_examples': 3003, 'score': 7592.796184301376, 'total_duration': 12440.432694673538, 'accumulated_submission_time': 7592.796184301376, 'accumulated_eval_time': 4829.6028118133545, 'accumulated_logging_time': 0.2531464099884033}
I0915 23:09:05.635732 140502817015552 logging_writer.py:48] [17429] accumulated_eval_time=4829.602812, accumulated_logging_time=0.253146, accumulated_submission_time=7592.796184, global_step=17429, preemption_count=0, score=7592.796184, test/accuracy=0.656847, test/bleu=26.586172, test/loss=1.598630, test/num_examples=3003, total_duration=12440.432695, train/accuracy=0.627288, train/bleu=30.486603, train/loss=1.832901, validation/accuracy=0.649911, validation/bleu=27.477860, validation/loss=1.667442, validation/num_examples=3000
I0915 23:09:37.544184 140493521635072 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.226790, loss=1.935018
I0915 23:09:37.548004 140548286359360 pytorch_submission_base.py:86] 17500) loss = 1.935, grad_norm = 0.227
I0915 23:13:14.319967 140502817015552 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.199166, loss=1.844124
I0915 23:13:14.324362 140548286359360 pytorch_submission_base.py:86] 18000) loss = 1.844, grad_norm = 0.199
I0915 23:16:51.174904 140493521635072 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.170324, loss=1.968089
I0915 23:16:51.178600 140548286359360 pytorch_submission_base.py:86] 18500) loss = 1.968, grad_norm = 0.170
I0915 23:20:27.954442 140502817015552 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.212165, loss=1.864158
I0915 23:20:27.958801 140548286359360 pytorch_submission_base.py:86] 19000) loss = 1.864, grad_norm = 0.212
I0915 23:23:06.384022 140548286359360 spec.py:320] Evaluating on the training split.
I0915 23:23:10.122544 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:25:33.768794 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 23:25:37.405690 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:27:43.339061 140548286359360 spec.py:348] Evaluating on the test split.
I0915 23:27:47.058895 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:29:42.297065 140548286359360 submission_runner.py:376] Time since start: 13677.11s, 	Step: 19365, 	{'train/accuracy': 0.6406487698308303, 'train/loss': 1.7265259516510918, 'train/bleu': 31.542611233316112, 'validation/accuracy': 0.6512380503651536, 'validation/loss': 1.6504683373423763, 'validation/bleu': 27.456567025628075, 'validation/num_examples': 3000, 'test/accuracy': 0.6614490732670967, 'test/loss': 1.5820926660275405, 'test/bleu': 26.73234110881058, 'test/num_examples': 3003, 'score': 8431.73428940773, 'total_duration': 13677.111048460007, 'accumulated_submission_time': 8431.73428940773, 'accumulated_eval_time': 5225.515836715698, 'accumulated_logging_time': 0.27984142303466797}
I0915 23:29:42.314095 140493521635072 logging_writer.py:48] [19365] accumulated_eval_time=5225.515837, accumulated_logging_time=0.279841, accumulated_submission_time=8431.734289, global_step=19365, preemption_count=0, score=8431.734289, test/accuracy=0.661449, test/bleu=26.732341, test/loss=1.582093, test/num_examples=3003, total_duration=13677.111048, train/accuracy=0.640649, train/bleu=31.542611, train/loss=1.726526, validation/accuracy=0.651238, validation/bleu=27.456567, validation/loss=1.650468, validation/num_examples=3000
I0915 23:30:41.987249 140502817015552 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.191845, loss=1.863907
I0915 23:30:41.991428 140548286359360 pytorch_submission_base.py:86] 19500) loss = 1.864, grad_norm = 0.192
I0915 23:34:18.668581 140493521635072 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.215553, loss=1.837828
I0915 23:34:18.672852 140548286359360 pytorch_submission_base.py:86] 20000) loss = 1.838, grad_norm = 0.216
I0915 23:37:55.405194 140502817015552 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.184915, loss=1.848044
I0915 23:37:55.408809 140548286359360 pytorch_submission_base.py:86] 20500) loss = 1.848, grad_norm = 0.185
I0915 23:41:32.046823 140493521635072 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.178603, loss=1.825282
I0915 23:41:32.051497 140548286359360 pytorch_submission_base.py:86] 21000) loss = 1.825, grad_norm = 0.179
I0915 23:43:43.127669 140548286359360 spec.py:320] Evaluating on the training split.
I0915 23:43:46.875096 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:47:18.018749 140548286359360 spec.py:332] Evaluating on the validation split.
I0915 23:47:21.694931 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:49:39.788512 140548286359360 spec.py:348] Evaluating on the test split.
I0915 23:49:43.515708 140548286359360 workload.py:141] Translating evaluation dataset.
I0915 23:51:52.753515 140548286359360 submission_runner.py:376] Time since start: 15007.57s, 	Step: 21302, 	{'train/accuracy': 0.6382798359566523, 'train/loss': 1.758453294327216, 'train/bleu': 30.601670383946143, 'validation/accuracy': 0.6519324000942331, 'validation/loss': 1.6462875150339116, 'validation/bleu': 27.341726817968347, 'validation/num_examples': 3000, 'test/accuracy': 0.6647260472953344, 'test/loss': 1.5706432296205914, 'test/bleu': 26.979278595350486, 'test/num_examples': 3003, 'score': 9270.730333328247, 'total_duration': 15007.56753230095, 'accumulated_submission_time': 9270.730333328247, 'accumulated_eval_time': 5715.141753911972, 'accumulated_logging_time': 0.3067924976348877}
I0915 23:51:52.770862 140502817015552 logging_writer.py:48] [21302] accumulated_eval_time=5715.141754, accumulated_logging_time=0.306792, accumulated_submission_time=9270.730333, global_step=21302, preemption_count=0, score=9270.730333, test/accuracy=0.664726, test/bleu=26.979279, test/loss=1.570643, test/num_examples=3003, total_duration=15007.567532, train/accuracy=0.638280, train/bleu=30.601670, train/loss=1.758453, validation/accuracy=0.651932, validation/bleu=27.341727, validation/loss=1.646288, validation/num_examples=3000
I0915 23:53:19.614538 140493521635072 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.275166, loss=1.984537
I0915 23:53:19.618463 140548286359360 pytorch_submission_base.py:86] 21500) loss = 1.985, grad_norm = 0.275
I0915 23:56:56.568681 140502817015552 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.190285, loss=1.861757
I0915 23:56:56.572853 140548286359360 pytorch_submission_base.py:86] 22000) loss = 1.862, grad_norm = 0.190
I0916 00:00:33.295210 140493521635072 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.252426, loss=1.874230
I0916 00:00:33.299033 140548286359360 pytorch_submission_base.py:86] 22500) loss = 1.874, grad_norm = 0.252
I0916 00:04:10.120792 140502817015552 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.180026, loss=1.834849
I0916 00:04:10.124851 140548286359360 pytorch_submission_base.py:86] 23000) loss = 1.835, grad_norm = 0.180
I0916 00:05:53.662390 140548286359360 spec.py:320] Evaluating on the training split.
I0916 00:05:57.415413 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:08:28.091091 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 00:08:31.745382 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:10:38.905554 140548286359360 spec.py:348] Evaluating on the test split.
I0916 00:10:42.640448 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:12:43.794017 140548286359360 submission_runner.py:376] Time since start: 16258.61s, 	Step: 23238, 	{'train/accuracy': 0.6369830128718078, 'train/loss': 1.7659684792176318, 'train/bleu': 31.015807642183926, 'validation/accuracy': 0.6556893280926461, 'validation/loss': 1.6280676154046447, 'validation/bleu': 27.950156432722178, 'validation/num_examples': 3000, 'test/accuracy': 0.665667305792807, 'test/loss': 1.5500065365173437, 'test/bleu': 27.276721659909946, 'test/num_examples': 3003, 'score': 10109.776323080063, 'total_duration': 16258.608014583588, 'accumulated_submission_time': 10109.776323080063, 'accumulated_eval_time': 6125.273441314697, 'accumulated_logging_time': 0.3341820240020752}
I0916 00:12:43.812977 140493521635072 logging_writer.py:48] [23238] accumulated_eval_time=6125.273441, accumulated_logging_time=0.334182, accumulated_submission_time=10109.776323, global_step=23238, preemption_count=0, score=10109.776323, test/accuracy=0.665667, test/bleu=27.276722, test/loss=1.550007, test/num_examples=3003, total_duration=16258.608015, train/accuracy=0.636983, train/bleu=31.015808, train/loss=1.765968, validation/accuracy=0.655689, validation/bleu=27.950156, validation/loss=1.628068, validation/num_examples=3000
I0916 00:14:38.579910 140502817015552 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.207328, loss=1.794782
I0916 00:14:38.583952 140548286359360 pytorch_submission_base.py:86] 23500) loss = 1.795, grad_norm = 0.207
I0916 00:18:15.295467 140493521635072 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.179749, loss=1.816874
I0916 00:18:15.300079 140548286359360 pytorch_submission_base.py:86] 24000) loss = 1.817, grad_norm = 0.180
I0916 00:21:52.034311 140502817015552 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.178305, loss=1.809021
I0916 00:21:52.038703 140548286359360 pytorch_submission_base.py:86] 24500) loss = 1.809, grad_norm = 0.178
I0916 00:25:28.916504 140493521635072 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.193659, loss=1.792098
I0916 00:25:28.920408 140548286359360 pytorch_submission_base.py:86] 25000) loss = 1.792, grad_norm = 0.194
I0916 00:26:44.553192 140548286359360 spec.py:320] Evaluating on the training split.
I0916 00:26:48.299210 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:29:26.159068 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 00:29:29.819169 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:31:37.731000 140548286359360 spec.py:348] Evaluating on the test split.
I0916 00:31:41.462102 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:33:35.536963 140548286359360 submission_runner.py:376] Time since start: 17510.35s, 	Step: 25174, 	{'train/accuracy': 0.6599256505576209, 'train/loss': 1.5785911138118387, 'train/bleu': 32.28229505199461, 'validation/accuracy': 0.6561232966733208, 'validation/loss': 1.6102444870491377, 'validation/bleu': 27.92391550739818, 'validation/num_examples': 3000, 'test/accuracy': 0.6672709313810935, 'test/loss': 1.5376875617337749, 'test/bleu': 27.29301609084211, 'test/num_examples': 3003, 'score': 10948.71316242218, 'total_duration': 17510.35099887848, 'accumulated_submission_time': 10948.71316242218, 'accumulated_eval_time': 6536.257373809814, 'accumulated_logging_time': 0.3641328811645508}
I0916 00:33:35.554292 140502817015552 logging_writer.py:48] [25174] accumulated_eval_time=6536.257374, accumulated_logging_time=0.364133, accumulated_submission_time=10948.713162, global_step=25174, preemption_count=0, score=10948.713162, test/accuracy=0.667271, test/bleu=27.293016, test/loss=1.537688, test/num_examples=3003, total_duration=17510.350999, train/accuracy=0.659926, train/bleu=32.282295, train/loss=1.578591, validation/accuracy=0.656123, validation/bleu=27.923916, validation/loss=1.610244, validation/num_examples=3000
I0916 00:35:57.974815 140493521635072 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.217882, loss=1.731571
I0916 00:35:57.979091 140548286359360 pytorch_submission_base.py:86] 25500) loss = 1.732, grad_norm = 0.218
I0916 00:39:34.690097 140502817015552 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.189440, loss=1.702252
I0916 00:39:34.693882 140548286359360 pytorch_submission_base.py:86] 26000) loss = 1.702, grad_norm = 0.189
I0916 00:43:11.478340 140493521635072 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.192194, loss=1.854678
I0916 00:43:11.481809 140548286359360 pytorch_submission_base.py:86] 26500) loss = 1.855, grad_norm = 0.192
I0916 00:46:48.190589 140502817015552 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.168777, loss=1.832014
I0916 00:46:48.194817 140548286359360 pytorch_submission_base.py:86] 27000) loss = 1.832, grad_norm = 0.169
I0916 00:47:36.539395 140548286359360 spec.py:320] Evaluating on the training split.
I0916 00:47:40.289019 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:52:02.352252 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 00:52:06.009904 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:55:06.113304 140548286359360 spec.py:348] Evaluating on the test split.
I0916 00:55:09.840603 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 00:58:17.635736 140548286359360 submission_runner.py:376] Time since start: 18992.45s, 	Step: 27111, 	{'train/accuracy': 0.6404345340194397, 'train/loss': 1.721489065180103, 'train/bleu': 31.7286075932633, 'validation/accuracy': 0.6577847763821899, 'validation/loss': 1.602408641864329, 'validation/bleu': 28.129466292077105, 'validation/num_examples': 3000, 'test/accuracy': 0.6687583522166056, 'test/loss': 1.5291241793039336, 'test/bleu': 27.366074967561612, 'test/num_examples': 3003, 'score': 11787.881809949875, 'total_duration': 18992.449721574783, 'accumulated_submission_time': 11787.881809949875, 'accumulated_eval_time': 7177.353737592697, 'accumulated_logging_time': 0.3912367820739746}
I0916 00:58:17.653252 140493521635072 logging_writer.py:48] [27111] accumulated_eval_time=7177.353738, accumulated_logging_time=0.391237, accumulated_submission_time=11787.881810, global_step=27111, preemption_count=0, score=11787.881810, test/accuracy=0.668758, test/bleu=27.366075, test/loss=1.529124, test/num_examples=3003, total_duration=18992.449722, train/accuracy=0.640435, train/bleu=31.728608, train/loss=1.721489, validation/accuracy=0.657785, validation/bleu=28.129466, validation/loss=1.602409, validation/num_examples=3000
I0916 01:01:07.349011 140502817015552 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.209324, loss=1.821211
I0916 01:01:07.352509 140548286359360 pytorch_submission_base.py:86] 27500) loss = 1.821, grad_norm = 0.209
I0916 01:04:44.152625 140493521635072 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.192741, loss=1.964420
I0916 01:04:44.157081 140548286359360 pytorch_submission_base.py:86] 28000) loss = 1.964, grad_norm = 0.193
I0916 01:08:21.036439 140502817015552 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.189406, loss=1.813042
I0916 01:08:21.040299 140548286359360 pytorch_submission_base.py:86] 28500) loss = 1.813, grad_norm = 0.189
I0916 01:11:57.898518 140493521635072 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.199123, loss=1.829561
I0916 01:11:57.903177 140548286359360 pytorch_submission_base.py:86] 29000) loss = 1.830, grad_norm = 0.199
I0916 01:12:18.567887 140548286359360 spec.py:320] Evaluating on the training split.
I0916 01:12:22.347091 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:14:55.561345 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 01:14:59.210944 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:17:11.748858 140548286359360 spec.py:348] Evaluating on the test split.
I0916 01:17:15.475698 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:19:25.639213 140548286359360 submission_runner.py:376] Time since start: 20260.45s, 	Step: 29047, 	{'train/accuracy': 0.6409900426742532, 'train/loss': 1.7337724039829303, 'train/bleu': 31.65851214632639, 'validation/accuracy': 0.6601778031270537, 'validation/loss': 1.594835696395581, 'validation/bleu': 28.201814289752825, 'validation/num_examples': 3000, 'test/accuracy': 0.6684329789088373, 'test/loss': 1.5217239338213933, 'test/bleu': 27.511019094004567, 'test/num_examples': 3003, 'score': 12626.935340881348, 'total_duration': 20260.453236818314, 'accumulated_submission_time': 12626.935340881348, 'accumulated_eval_time': 7604.4251120090485, 'accumulated_logging_time': 0.41831302642822266}
I0916 01:19:25.657146 140502817015552 logging_writer.py:48] [29047] accumulated_eval_time=7604.425112, accumulated_logging_time=0.418313, accumulated_submission_time=12626.935341, global_step=29047, preemption_count=0, score=12626.935341, test/accuracy=0.668433, test/bleu=27.511019, test/loss=1.521724, test/num_examples=3003, total_duration=20260.453237, train/accuracy=0.640990, train/bleu=31.658512, train/loss=1.733772, validation/accuracy=0.660178, validation/bleu=28.201814, validation/loss=1.594836, validation/num_examples=3000
I0916 01:22:43.050792 140493521635072 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.185472, loss=1.790851
I0916 01:22:43.054691 140548286359360 pytorch_submission_base.py:86] 29500) loss = 1.791, grad_norm = 0.185
I0916 01:26:19.722371 140502817015552 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.227930, loss=1.756200
I0916 01:26:19.726624 140548286359360 pytorch_submission_base.py:86] 30000) loss = 1.756, grad_norm = 0.228
I0916 01:29:56.374474 140493521635072 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.170399, loss=1.779801
I0916 01:29:56.378068 140548286359360 pytorch_submission_base.py:86] 30500) loss = 1.780, grad_norm = 0.170
I0916 01:33:26.534189 140548286359360 spec.py:320] Evaluating on the training split.
I0916 01:33:30.276453 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:36:58.819639 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 01:37:02.486691 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:39:38.048859 140548286359360 spec.py:348] Evaluating on the test split.
I0916 01:39:41.782208 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:42:03.309706 140548286359360 submission_runner.py:376] Time since start: 21618.12s, 	Step: 30984, 	{'train/accuracy': 0.6439200655730135, 'train/loss': 1.7176848353747936, 'train/bleu': 31.60252316452521, 'validation/accuracy': 0.6615293052782979, 'validation/loss': 1.5832076632651797, 'validation/bleu': 28.104344130531995, 'validation/num_examples': 3000, 'test/accuracy': 0.673057928069258, 'test/loss': 1.5081017408924524, 'test/bleu': 27.706671646641407, 'test/num_examples': 3003, 'score': 13465.986572027206, 'total_duration': 21618.1237244606, 'accumulated_submission_time': 13465.986572027206, 'accumulated_eval_time': 8121.20067691803, 'accumulated_logging_time': 0.44565820693969727}
I0916 01:42:03.328196 140502817015552 logging_writer.py:48] [30984] accumulated_eval_time=8121.200677, accumulated_logging_time=0.445658, accumulated_submission_time=13465.986572, global_step=30984, preemption_count=0, score=13465.986572, test/accuracy=0.673058, test/bleu=27.706672, test/loss=1.508102, test/num_examples=3003, total_duration=21618.123724, train/accuracy=0.643920, train/bleu=31.602523, train/loss=1.717685, validation/accuracy=0.661529, validation/bleu=28.104344, validation/loss=1.583208, validation/num_examples=3000
I0916 01:42:11.382180 140493521635072 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.238402, loss=1.788149
I0916 01:42:11.385968 140548286359360 pytorch_submission_base.py:86] 31000) loss = 1.788, grad_norm = 0.238
I0916 01:45:48.184693 140502817015552 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.183685, loss=1.779099
I0916 01:45:48.189402 140548286359360 pytorch_submission_base.py:86] 31500) loss = 1.779, grad_norm = 0.184
I0916 01:49:24.887761 140493521635072 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.172670, loss=1.775088
I0916 01:49:24.892112 140548286359360 pytorch_submission_base.py:86] 32000) loss = 1.775, grad_norm = 0.173
I0916 01:53:01.444869 140502817015552 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.237345, loss=1.827540
I0916 01:53:01.449307 140548286359360 pytorch_submission_base.py:86] 32500) loss = 1.828, grad_norm = 0.237
I0916 01:56:04.164727 140548286359360 spec.py:320] Evaluating on the training split.
I0916 01:56:07.895974 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 01:59:16.309322 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 01:59:19.973737 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:01:29.040703 140548286359360 spec.py:348] Evaluating on the test split.
I0916 02:01:32.770059 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:03:32.406494 140548286359360 submission_runner.py:376] Time since start: 22907.22s, 	Step: 32921, 	{'train/accuracy': 0.647332079280602, 'train/loss': 1.6765122829303543, 'train/bleu': 31.908684782240226, 'validation/accuracy': 0.6618640810405326, 'validation/loss': 1.5747515723611611, 'validation/bleu': 28.266594137446134, 'validation/num_examples': 3000, 'test/accuracy': 0.6724885247806636, 'test/loss': 1.5000003631398524, 'test/bleu': 27.485078293296752, 'test/num_examples': 3003, 'score': 14304.972734451294, 'total_duration': 22907.22048640251, 'accumulated_submission_time': 14304.972734451294, 'accumulated_eval_time': 8569.442501783371, 'accumulated_logging_time': 0.4742164611816406}
I0916 02:03:32.424444 140493521635072 logging_writer.py:48] [32921] accumulated_eval_time=8569.442502, accumulated_logging_time=0.474216, accumulated_submission_time=14304.972734, global_step=32921, preemption_count=0, score=14304.972734, test/accuracy=0.672489, test/bleu=27.485078, test/loss=1.500000, test/num_examples=3003, total_duration=22907.220486, train/accuracy=0.647332, train/bleu=31.908685, train/loss=1.676512, validation/accuracy=0.661864, validation/bleu=28.266594, validation/loss=1.574752, validation/num_examples=3000
I0916 02:04:07.753606 140502817015552 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.280323, loss=1.837679
I0916 02:04:07.756999 140548286359360 pytorch_submission_base.py:86] 33000) loss = 1.838, grad_norm = 0.280
I0916 02:07:44.420395 140493521635072 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.183837, loss=1.818405
I0916 02:07:44.424976 140548286359360 pytorch_submission_base.py:86] 33500) loss = 1.818, grad_norm = 0.184
I0916 02:11:21.206308 140502817015552 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.190502, loss=1.729569
I0916 02:11:21.209961 140548286359360 pytorch_submission_base.py:86] 34000) loss = 1.730, grad_norm = 0.191
I0916 02:14:57.857602 140493521635072 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.190138, loss=1.707889
I0916 02:14:57.861220 140548286359360 pytorch_submission_base.py:86] 34500) loss = 1.708, grad_norm = 0.190
I0916 02:17:33.319030 140548286359360 spec.py:320] Evaluating on the training split.
I0916 02:17:37.058390 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:20:47.288385 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 02:20:50.946663 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:23:04.964753 140548286359360 spec.py:348] Evaluating on the test split.
I0916 02:23:08.695073 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:25:18.974458 140548286359360 submission_runner.py:376] Time since start: 24213.79s, 	Step: 34858, 	{'train/accuracy': 0.6455589929960552, 'train/loss': 1.6999612209750319, 'train/bleu': 31.683217066472352, 'validation/accuracy': 0.6652366368674908, 'validation/loss': 1.5713859910292496, 'validation/bleu': 28.525783784364737, 'validation/num_examples': 3000, 'test/accuracy': 0.6770669920399744, 'test/loss': 1.4881058080587997, 'test/bleu': 28.069393689092426, 'test/num_examples': 3003, 'score': 15144.050840854645, 'total_duration': 24213.788479566574, 'accumulated_submission_time': 15144.050840854645, 'accumulated_eval_time': 9035.09800195694, 'accumulated_logging_time': 0.5020933151245117}
I0916 02:25:18.992799 140502817015552 logging_writer.py:48] [34858] accumulated_eval_time=9035.098002, accumulated_logging_time=0.502093, accumulated_submission_time=15144.050841, global_step=34858, preemption_count=0, score=15144.050841, test/accuracy=0.677067, test/bleu=28.069394, test/loss=1.488106, test/num_examples=3003, total_duration=24213.788480, train/accuracy=0.645559, train/bleu=31.683217, train/loss=1.699961, validation/accuracy=0.665237, validation/bleu=28.525784, validation/loss=1.571386, validation/num_examples=3000
I0916 02:26:21.568883 140493521635072 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.208502, loss=1.662741
I0916 02:26:21.572593 140548286359360 pytorch_submission_base.py:86] 35000) loss = 1.663, grad_norm = 0.209
I0916 02:29:58.148846 140502817015552 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.183667, loss=1.808686
I0916 02:29:58.152930 140548286359360 pytorch_submission_base.py:86] 35500) loss = 1.809, grad_norm = 0.184
I0916 02:33:35.052268 140493521635072 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.192920, loss=1.754186
I0916 02:33:35.056984 140548286359360 pytorch_submission_base.py:86] 36000) loss = 1.754, grad_norm = 0.193
I0916 02:37:11.891250 140502817015552 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.191251, loss=1.734298
I0916 02:37:11.894717 140548286359360 pytorch_submission_base.py:86] 36500) loss = 1.734, grad_norm = 0.191
I0916 02:39:19.985234 140548286359360 spec.py:320] Evaluating on the training split.
I0916 02:39:23.723765 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:42:47.966480 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 02:42:51.624182 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:45:13.531117 140548286359360 spec.py:348] Evaluating on the test split.
I0916 02:45:17.251781 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 02:47:18.909673 140548286359360 submission_runner.py:376] Time since start: 25533.72s, 	Step: 36795, 	{'train/accuracy': 0.6474567661794925, 'train/loss': 1.6844318923021866, 'train/bleu': 31.50121315972364, 'validation/accuracy': 0.6638231392047216, 'validation/loss': 1.5592971568858414, 'validation/bleu': 28.369663348958092, 'validation/num_examples': 3000, 'test/accuracy': 0.6779850095868921, 'test/loss': 1.4803821865376794, 'test/bleu': 28.357257856642306, 'test/num_examples': 3003, 'score': 15983.212558031082, 'total_duration': 25533.723687171936, 'accumulated_submission_time': 15983.212558031082, 'accumulated_eval_time': 9514.022464036942, 'accumulated_logging_time': 0.5305404663085938}
I0916 02:47:18.928250 140493521635072 logging_writer.py:48] [36795] accumulated_eval_time=9514.022464, accumulated_logging_time=0.530540, accumulated_submission_time=15983.212558, global_step=36795, preemption_count=0, score=15983.212558, test/accuracy=0.677985, test/bleu=28.357258, test/loss=1.480382, test/num_examples=3003, total_duration=25533.723687, train/accuracy=0.647457, train/bleu=31.501213, train/loss=1.684432, validation/accuracy=0.663823, validation/bleu=28.369663, validation/loss=1.559297, validation/num_examples=3000
I0916 02:48:48.804765 140502817015552 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.205471, loss=1.731133
I0916 02:48:48.808473 140548286359360 pytorch_submission_base.py:86] 37000) loss = 1.731, grad_norm = 0.205
I0916 02:52:25.519771 140493521635072 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.214898, loss=1.761459
I0916 02:52:25.523898 140548286359360 pytorch_submission_base.py:86] 37500) loss = 1.761, grad_norm = 0.215
I0916 02:56:02.099032 140502817015552 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.215237, loss=1.781195
I0916 02:56:02.102653 140548286359360 pytorch_submission_base.py:86] 38000) loss = 1.781, grad_norm = 0.215
I0916 02:59:38.737216 140493521635072 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.170426, loss=1.719696
I0916 02:59:38.741306 140548286359360 pytorch_submission_base.py:86] 38500) loss = 1.720, grad_norm = 0.170
I0916 03:01:19.830524 140548286359360 spec.py:320] Evaluating on the training split.
I0916 03:01:23.587070 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:04:54.553409 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 03:04:58.207993 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:07:25.355259 140548286359360 spec.py:348] Evaluating on the test split.
I0916 03:07:29.075428 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:09:33.717416 140548286359360 submission_runner.py:376] Time since start: 26868.53s, 	Step: 38733, 	{'train/accuracy': 0.6518662253167447, 'train/loss': 1.63576664336263, 'train/bleu': 32.23173737476405, 'validation/accuracy': 0.6672948878501197, 'validation/loss': 1.5510907916516845, 'validation/bleu': 29.054450448406424, 'validation/num_examples': 3000, 'test/accuracy': 0.6802161408401604, 'test/loss': 1.4651368492533845, 'test/bleu': 28.3957710984702, 'test/num_examples': 3003, 'score': 16822.314371347427, 'total_duration': 26868.53143930435, 'accumulated_submission_time': 16822.314371347427, 'accumulated_eval_time': 10007.909393072128, 'accumulated_logging_time': 0.5586585998535156}
I0916 03:09:33.735838 140502817015552 logging_writer.py:48] [38733] accumulated_eval_time=10007.909393, accumulated_logging_time=0.558659, accumulated_submission_time=16822.314371, global_step=38733, preemption_count=0, score=16822.314371, test/accuracy=0.680216, test/bleu=28.395771, test/loss=1.465137, test/num_examples=3003, total_duration=26868.531439, train/accuracy=0.651866, train/bleu=32.231737, train/loss=1.635767, validation/accuracy=0.667295, validation/bleu=29.054450, validation/loss=1.551091, validation/num_examples=3000
I0916 03:11:30.490012 140493521635072 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.196034, loss=1.820790
I0916 03:11:30.494238 140548286359360 pytorch_submission_base.py:86] 39000) loss = 1.821, grad_norm = 0.196
I0916 03:15:07.360993 140502817015552 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.203677, loss=1.763841
I0916 03:15:07.365205 140548286359360 pytorch_submission_base.py:86] 39500) loss = 1.764, grad_norm = 0.204
I0916 03:18:44.200093 140493521635072 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.199416, loss=1.667117
I0916 03:18:44.204254 140548286359360 pytorch_submission_base.py:86] 40000) loss = 1.667, grad_norm = 0.199
I0916 03:22:21.088239 140502817015552 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.264499, loss=1.818912
I0916 03:22:21.091905 140548286359360 pytorch_submission_base.py:86] 40500) loss = 1.819, grad_norm = 0.264
I0916 03:23:34.556730 140548286359360 spec.py:320] Evaluating on the training split.
I0916 03:23:38.293565 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:27:27.697303 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 03:27:31.359617 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:30:34.275397 140548286359360 spec.py:348] Evaluating on the test split.
I0916 03:30:38.003092 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:33:22.677969 140548286359360 submission_runner.py:376] Time since start: 28297.49s, 	Step: 40669, 	{'train/accuracy': 0.6496135819421905, 'train/loss': 1.6628862407367477, 'train/bleu': 31.621890556436647, 'validation/accuracy': 0.6660921749265353, 'validation/loss': 1.5446853960273277, 'validation/bleu': 28.628090042409703, 'validation/num_examples': 3000, 'test/accuracy': 0.681610597873453, 'test/loss': 1.4537059329789088, 'test/bleu': 28.286168587522337, 'test/num_examples': 3003, 'score': 17661.329209566116, 'total_duration': 28297.49199128151, 'accumulated_submission_time': 17661.329209566116, 'accumulated_eval_time': 10596.03066778183, 'accumulated_logging_time': 0.5877997875213623}
I0916 03:33:22.696455 140493521635072 logging_writer.py:48] [40669] accumulated_eval_time=10596.030668, accumulated_logging_time=0.587800, accumulated_submission_time=17661.329210, global_step=40669, preemption_count=0, score=17661.329210, test/accuracy=0.681611, test/bleu=28.286169, test/loss=1.453706, test/num_examples=3003, total_duration=28297.491991, train/accuracy=0.649614, train/bleu=31.621891, train/loss=1.662886, validation/accuracy=0.666092, validation/bleu=28.628090, validation/loss=1.544685, validation/num_examples=3000
I0916 03:35:47.249737 140502817015552 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.180604, loss=1.702076
I0916 03:35:47.253372 140548286359360 pytorch_submission_base.py:86] 41000) loss = 1.702, grad_norm = 0.181
I0916 03:39:23.869546 140493521635072 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.190257, loss=1.675776
I0916 03:39:23.873760 140548286359360 pytorch_submission_base.py:86] 41500) loss = 1.676, grad_norm = 0.190
I0916 03:43:00.580127 140502817015552 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.219226, loss=1.807418
I0916 03:43:00.584592 140548286359360 pytorch_submission_base.py:86] 42000) loss = 1.807, grad_norm = 0.219
I0916 03:46:37.195619 140493521635072 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.203710, loss=1.683440
I0916 03:46:37.199060 140548286359360 pytorch_submission_base.py:86] 42500) loss = 1.683, grad_norm = 0.204
I0916 03:47:23.373597 140548286359360 spec.py:320] Evaluating on the training split.
I0916 03:47:27.154413 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:51:03.645139 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 03:51:07.287651 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:53:38.550746 140548286359360 spec.py:348] Evaluating on the test split.
I0916 03:53:42.268636 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 03:55:59.522919 140548286359360 submission_runner.py:376] Time since start: 29654.34s, 	Step: 42606, 	{'train/accuracy': 0.6480103341449752, 'train/loss': 1.6791106519427963, 'train/bleu': 32.725976031815065, 'validation/accuracy': 0.6679768384768943, 'validation/loss': 1.5388408660462982, 'validation/bleu': 28.837223580497827, 'validation/num_examples': 3000, 'test/accuracy': 0.6807971646040323, 'test/loss': 1.4510897826971123, 'test/bleu': 28.097397197877058, 'test/num_examples': 3003, 'score': 18500.204820394516, 'total_duration': 29654.336916446686, 'accumulated_submission_time': 18500.204820394516, 'accumulated_eval_time': 11112.180014133453, 'accumulated_logging_time': 0.6166443824768066}
I0916 03:55:59.541919 140502817015552 logging_writer.py:48] [42606] accumulated_eval_time=11112.180014, accumulated_logging_time=0.616644, accumulated_submission_time=18500.204820, global_step=42606, preemption_count=0, score=18500.204820, test/accuracy=0.680797, test/bleu=28.097397, test/loss=1.451090, test/num_examples=3003, total_duration=29654.336916, train/accuracy=0.648010, train/bleu=32.725976, train/loss=1.679111, validation/accuracy=0.667977, validation/bleu=28.837224, validation/loss=1.538841, validation/num_examples=3000
I0916 03:58:51.371924 140493521635072 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.183975, loss=1.774884
I0916 03:58:51.375633 140548286359360 pytorch_submission_base.py:86] 43000) loss = 1.775, grad_norm = 0.184
I0916 04:02:28.059406 140502817015552 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.184994, loss=1.667608
I0916 04:02:28.063944 140548286359360 pytorch_submission_base.py:86] 43500) loss = 1.668, grad_norm = 0.185
I0916 04:06:04.803059 140493521635072 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.196464, loss=1.699032
I0916 04:06:04.807591 140548286359360 pytorch_submission_base.py:86] 44000) loss = 1.699, grad_norm = 0.196
I0916 04:09:41.569381 140502817015552 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.193606, loss=1.794004
I0916 04:09:41.573451 140548286359360 pytorch_submission_base.py:86] 44500) loss = 1.794, grad_norm = 0.194
I0916 04:10:00.465229 140548286359360 spec.py:320] Evaluating on the training split.
I0916 04:10:04.224281 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:13:21.492413 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 04:13:25.162001 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:15:54.960752 140548286359360 spec.py:348] Evaluating on the test split.
I0916 04:15:58.688396 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:18:14.960066 140548286359360 submission_runner.py:376] Time since start: 30989.77s, 	Step: 44543, 	{'train/accuracy': 0.6614632462901184, 'train/loss': 1.5848266924536984, 'train/bleu': 32.99073407007707, 'validation/accuracy': 0.6697127127995933, 'validation/loss': 1.5274952030972957, 'validation/bleu': 29.023326963209804, 'validation/num_examples': 3000, 'test/accuracy': 0.6825867177967578, 'test/loss': 1.4426892684910813, 'test/bleu': 28.67403504200141, 'test/num_examples': 3003, 'score': 19339.33795952797, 'total_duration': 30989.77409505844, 'accumulated_submission_time': 19339.33795952797, 'accumulated_eval_time': 11606.674901247025, 'accumulated_logging_time': 0.6454687118530273}
I0916 04:18:14.979076 140493521635072 logging_writer.py:48] [44543] accumulated_eval_time=11606.674901, accumulated_logging_time=0.645469, accumulated_submission_time=19339.337960, global_step=44543, preemption_count=0, score=19339.337960, test/accuracy=0.682587, test/bleu=28.674035, test/loss=1.442689, test/num_examples=3003, total_duration=30989.774095, train/accuracy=0.661463, train/bleu=32.990734, train/loss=1.584827, validation/accuracy=0.669713, validation/bleu=29.023327, validation/loss=1.527495, validation/num_examples=3000
I0916 04:21:34.058972 140502817015552 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.205905, loss=1.794608
I0916 04:21:34.062646 140548286359360 pytorch_submission_base.py:86] 45000) loss = 1.795, grad_norm = 0.206
I0916 04:25:10.629686 140493521635072 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.206515, loss=1.666203
I0916 04:25:10.633018 140548286359360 pytorch_submission_base.py:86] 45500) loss = 1.666, grad_norm = 0.207
I0916 04:28:47.347339 140502817015552 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.180325, loss=1.695923
I0916 04:28:47.351456 140548286359360 pytorch_submission_base.py:86] 46000) loss = 1.696, grad_norm = 0.180
I0916 04:32:15.689210 140548286359360 spec.py:320] Evaluating on the training split.
I0916 04:32:19.438682 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:36:08.750411 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 04:36:12.394109 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:39:07.695304 140548286359360 spec.py:348] Evaluating on the test split.
I0916 04:39:11.420048 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:42:02.584556 140548286359360 submission_runner.py:376] Time since start: 32417.40s, 	Step: 46480, 	{'train/accuracy': 0.6528822342474548, 'train/loss': 1.6368912584265798, 'train/bleu': 32.26108818056441, 'validation/accuracy': 0.6696631163903733, 'validation/loss': 1.5237658824751088, 'validation/bleu': 28.805197147917227, 'validation/num_examples': 3000, 'test/accuracy': 0.682377549241764, 'test/loss': 1.4379119821625705, 'test/bleu': 28.351329898544645, 'test/num_examples': 3003, 'score': 20178.220480442047, 'total_duration': 32417.398582220078, 'accumulated_submission_time': 20178.220480442047, 'accumulated_eval_time': 12193.570276021957, 'accumulated_logging_time': 0.67494797706604}
I0916 04:42:02.603325 140493521635072 logging_writer.py:48] [46480] accumulated_eval_time=12193.570276, accumulated_logging_time=0.674948, accumulated_submission_time=20178.220480, global_step=46480, preemption_count=0, score=20178.220480, test/accuracy=0.682378, test/bleu=28.351330, test/loss=1.437912, test/num_examples=3003, total_duration=32417.398582, train/accuracy=0.652882, train/bleu=32.261088, train/loss=1.636891, validation/accuracy=0.669663, validation/bleu=28.805197, validation/loss=1.523766, validation/num_examples=3000
I0916 04:42:12.359448 140502817015552 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.188456, loss=1.708859
I0916 04:42:12.363066 140548286359360 pytorch_submission_base.py:86] 46500) loss = 1.709, grad_norm = 0.188
I0916 04:45:48.935541 140493521635072 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.172613, loss=1.666461
I0916 04:45:48.939837 140548286359360 pytorch_submission_base.py:86] 47000) loss = 1.666, grad_norm = 0.173
I0916 04:49:25.641707 140502817015552 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.206672, loss=1.670108
I0916 04:49:25.646100 140548286359360 pytorch_submission_base.py:86] 47500) loss = 1.670, grad_norm = 0.207
I0916 04:53:02.310606 140493521635072 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.183577, loss=1.613903
I0916 04:53:02.316158 140548286359360 pytorch_submission_base.py:86] 48000) loss = 1.614, grad_norm = 0.184
I0916 04:56:03.622516 140548286359360 spec.py:320] Evaluating on the training split.
I0916 04:56:07.346272 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 04:59:51.323697 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 04:59:54.975398 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:02:12.645219 140548286359360 spec.py:348] Evaluating on the test split.
I0916 05:02:16.394673 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:04:28.245871 140548286359360 submission_runner.py:376] Time since start: 33763.06s, 	Step: 48418, 	{'train/accuracy': 0.6519304754053311, 'train/loss': 1.648320852798388, 'train/bleu': 32.35196562593995, 'validation/accuracy': 0.6709774212347026, 'validation/loss': 1.515130682663575, 'validation/bleu': 29.078555145200127, 'validation/num_examples': 3000, 'test/accuracy': 0.6858288303991633, 'test/loss': 1.425182477775841, 'test/bleu': 28.760257903959555, 'test/num_examples': 3003, 'score': 21017.4000313282, 'total_duration': 33763.0598757267, 'accumulated_submission_time': 21017.4000313282, 'accumulated_eval_time': 12698.193695545197, 'accumulated_logging_time': 0.7032680511474609}
I0916 05:04:28.264648 140502817015552 logging_writer.py:48] [48418] accumulated_eval_time=12698.193696, accumulated_logging_time=0.703268, accumulated_submission_time=21017.400031, global_step=48418, preemption_count=0, score=21017.400031, test/accuracy=0.685829, test/bleu=28.760258, test/loss=1.425182, test/num_examples=3003, total_duration=33763.059876, train/accuracy=0.651930, train/bleu=32.351966, train/loss=1.648321, validation/accuracy=0.670977, validation/bleu=29.078555, validation/loss=1.515131, validation/num_examples=3000
I0916 05:05:04.852833 140493521635072 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.184026, loss=1.618402
I0916 05:05:04.856433 140548286359360 pytorch_submission_base.py:86] 48500) loss = 1.618, grad_norm = 0.184
I0916 05:08:41.618944 140502817015552 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.191346, loss=1.652551
I0916 05:08:41.623171 140548286359360 pytorch_submission_base.py:86] 49000) loss = 1.653, grad_norm = 0.191
I0916 05:12:18.392062 140493521635072 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.195111, loss=1.691076
I0916 05:12:18.396199 140548286359360 pytorch_submission_base.py:86] 49500) loss = 1.691, grad_norm = 0.195
I0916 05:15:55.108038 140502817015552 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.189736, loss=1.782501
I0916 05:15:55.112321 140548286359360 pytorch_submission_base.py:86] 50000) loss = 1.783, grad_norm = 0.190
I0916 05:18:29.304651 140548286359360 spec.py:320] Evaluating on the training split.
I0916 05:18:33.057424 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:22:45.086321 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 05:22:48.734411 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:25:37.883515 140548286359360 spec.py:348] Evaluating on the test split.
I0916 05:25:41.600604 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:28:05.721784 140548286359360 submission_runner.py:376] Time since start: 35180.54s, 	Step: 50355, 	{'train/accuracy': 0.6758551559954893, 'train/loss': 1.4852831404984566, 'train/bleu': 33.77617535096047, 'validation/accuracy': 0.6747839456423355, 'validation/loss': 1.5037334859456175, 'validation/bleu': 29.330393696235156, 'validation/num_examples': 3000, 'test/accuracy': 0.6859101737261054, 'test/loss': 1.418116139387601, 'test/bleu': 28.65569521589632, 'test/num_examples': 3003, 'score': 21856.62474298477, 'total_duration': 35180.53581738472, 'accumulated_submission_time': 21856.62474298477, 'accumulated_eval_time': 13274.610888957977, 'accumulated_logging_time': 0.731846809387207}
I0916 05:28:05.740703 140493521635072 logging_writer.py:48] [50355] accumulated_eval_time=13274.610889, accumulated_logging_time=0.731847, accumulated_submission_time=21856.624743, global_step=50355, preemption_count=0, score=21856.624743, test/accuracy=0.685910, test/bleu=28.655695, test/loss=1.418116, test/num_examples=3003, total_duration=35180.535817, train/accuracy=0.675855, train/bleu=33.776175, train/loss=1.485283, validation/accuracy=0.674784, validation/bleu=29.330394, validation/loss=1.503733, validation/num_examples=3000
I0916 05:29:09.686058 140502817015552 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.205645, loss=1.740991
I0916 05:29:09.689291 140548286359360 pytorch_submission_base.py:86] 50500) loss = 1.741, grad_norm = 0.206
I0916 05:32:46.401373 140493521635072 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.192565, loss=1.630033
I0916 05:32:46.405265 140548286359360 pytorch_submission_base.py:86] 51000) loss = 1.630, grad_norm = 0.193
I0916 05:36:23.062000 140502817015552 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.194377, loss=1.651362
I0916 05:36:23.066184 140548286359360 pytorch_submission_base.py:86] 51500) loss = 1.651, grad_norm = 0.194
I0916 05:39:59.750088 140493521635072 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.193555, loss=1.748823
I0916 05:39:59.753988 140548286359360 pytorch_submission_base.py:86] 52000) loss = 1.749, grad_norm = 0.194
I0916 05:42:06.518490 140548286359360 spec.py:320] Evaluating on the training split.
I0916 05:42:10.280807 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:45:38.736227 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 05:45:42.388250 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:48:00.112011 140548286359360 spec.py:348] Evaluating on the test split.
I0916 05:48:03.842407 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 05:50:28.218986 140548286359360 submission_runner.py:376] Time since start: 36523.03s, 	Step: 52292, 	{'train/accuracy': 0.6577692386866573, 'train/loss': 1.6074344057707484, 'train/bleu': 32.53646168910653, 'validation/accuracy': 0.6745979591077605, 'validation/loss': 1.4965689359090402, 'validation/bleu': 29.44271198290423, 'validation/num_examples': 3000, 'test/accuracy': 0.6883969554354773, 'test/loss': 1.4052153237754925, 'test/bleu': 29.14081296275224, 'test/num_examples': 3003, 'score': 22695.546543836594, 'total_duration': 36523.03301239014, 'accumulated_submission_time': 22695.546543836594, 'accumulated_eval_time': 13776.311428070068, 'accumulated_logging_time': 0.760345458984375}
I0916 05:50:28.238442 140502817015552 logging_writer.py:48] [52292] accumulated_eval_time=13776.311428, accumulated_logging_time=0.760345, accumulated_submission_time=22695.546544, global_step=52292, preemption_count=0, score=22695.546544, test/accuracy=0.688397, test/bleu=29.140813, test/loss=1.405215, test/num_examples=3003, total_duration=36523.033012, train/accuracy=0.657769, train/bleu=32.536462, train/loss=1.607434, validation/accuracy=0.674598, validation/bleu=29.442712, validation/loss=1.496569, validation/num_examples=3000
I0916 05:51:59.426270 140493521635072 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.206744, loss=1.701646
I0916 05:51:59.429663 140548286359360 pytorch_submission_base.py:86] 52500) loss = 1.702, grad_norm = 0.207
I0916 05:55:36.065538 140502817015552 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.206165, loss=1.678865
I0916 05:55:36.069150 140548286359360 pytorch_submission_base.py:86] 53000) loss = 1.679, grad_norm = 0.206
I0916 05:59:12.761052 140493521635072 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.215487, loss=1.672249
I0916 05:59:12.764931 140548286359360 pytorch_submission_base.py:86] 53500) loss = 1.672, grad_norm = 0.215
I0916 06:02:49.384606 140502817015552 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.194085, loss=1.723428
I0916 06:02:49.388530 140548286359360 pytorch_submission_base.py:86] 54000) loss = 1.723, grad_norm = 0.194
I0916 06:04:28.990956 140548286359360 spec.py:320] Evaluating on the training split.
I0916 06:04:32.737423 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:08:29.256623 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 06:08:32.902038 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:11:34.259447 140548286359360 spec.py:348] Evaluating on the test split.
I0916 06:11:37.980308 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:14:23.338213 140548286359360 submission_runner.py:376] Time since start: 37958.15s, 	Step: 54229, 	{'train/accuracy': 0.6601742519775307, 'train/loss': 1.6005024432534678, 'train/bleu': 32.528798784486355, 'validation/accuracy': 0.6759742594636148, 'validation/loss': 1.4859285687716208, 'validation/bleu': 29.386270567451255, 'validation/num_examples': 3000, 'test/accuracy': 0.6903608157573645, 'test/loss': 1.390614650514206, 'test/bleu': 29.13147483813223, 'test/num_examples': 3003, 'score': 23534.480858802795, 'total_duration': 37958.15223121643, 'accumulated_submission_time': 23534.480858802795, 'accumulated_eval_time': 14370.658740282059, 'accumulated_logging_time': 0.7891988754272461}
I0916 06:14:23.357549 140493521635072 logging_writer.py:48] [54229] accumulated_eval_time=14370.658740, accumulated_logging_time=0.789199, accumulated_submission_time=23534.480859, global_step=54229, preemption_count=0, score=23534.480859, test/accuracy=0.690361, test/bleu=29.131475, test/loss=1.390615, test/num_examples=3003, total_duration=37958.152231, train/accuracy=0.660174, train/bleu=32.528799, train/loss=1.600502, validation/accuracy=0.675974, validation/bleu=29.386271, validation/loss=1.485929, validation/num_examples=3000
I0916 06:16:21.744438 140502817015552 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.201749, loss=1.683130
I0916 06:16:21.748154 140548286359360 pytorch_submission_base.py:86] 54500) loss = 1.683, grad_norm = 0.202
I0916 06:19:58.406700 140493521635072 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.208609, loss=1.681256
I0916 06:19:58.410238 140548286359360 pytorch_submission_base.py:86] 55000) loss = 1.681, grad_norm = 0.209
I0916 06:23:35.110782 140502817015552 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.185434, loss=1.660500
I0916 06:23:35.114279 140548286359360 pytorch_submission_base.py:86] 55500) loss = 1.661, grad_norm = 0.185
I0916 06:27:11.769296 140493521635072 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.183128, loss=1.667850
I0916 06:27:11.773761 140548286359360 pytorch_submission_base.py:86] 56000) loss = 1.668, grad_norm = 0.183
I0916 06:28:24.294542 140548286359360 spec.py:320] Evaluating on the training split.
I0916 06:28:28.039439 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:31:37.943835 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 06:31:41.611586 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:34:29.173944 140548286359360 spec.py:348] Evaluating on the test split.
I0916 06:34:32.919046 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:37:09.040780 140548286359360 submission_runner.py:376] Time since start: 39323.85s, 	Step: 56167, 	{'train/accuracy': 0.660536354836873, 'train/loss': 1.6047413240904174, 'train/bleu': 32.80516408951384, 'validation/accuracy': 0.677462151740214, 'validation/loss': 1.4825181375618406, 'validation/bleu': 29.86055016844867, 'validation/num_examples': 3000, 'test/accuracy': 0.6911974899773401, 'test/loss': 1.3890211928417873, 'test/bleu': 29.26843379570543, 'test/num_examples': 3003, 'score': 24373.612773180008, 'total_duration': 39323.85476732254, 'accumulated_submission_time': 24373.612773180008, 'accumulated_eval_time': 14895.40498638153, 'accumulated_logging_time': 0.8188650608062744}
I0916 06:37:09.061000 140502817015552 logging_writer.py:48] [56167] accumulated_eval_time=14895.404986, accumulated_logging_time=0.818865, accumulated_submission_time=24373.612773, global_step=56167, preemption_count=0, score=24373.612773, test/accuracy=0.691197, test/bleu=29.268434, test/loss=1.389021, test/num_examples=3003, total_duration=39323.854767, train/accuracy=0.660536, train/bleu=32.805164, train/loss=1.604741, validation/accuracy=0.677462, validation/bleu=29.860550, validation/loss=1.482518, validation/num_examples=3000
I0916 06:39:34.467972 140493521635072 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.243868, loss=1.656101
I0916 06:39:34.472328 140548286359360 pytorch_submission_base.py:86] 56500) loss = 1.656, grad_norm = 0.244
I0916 06:43:11.030345 140502817015552 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.174870, loss=1.708243
I0916 06:43:11.033865 140548286359360 pytorch_submission_base.py:86] 57000) loss = 1.708, grad_norm = 0.175
I0916 06:46:47.632589 140493521635072 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.204478, loss=1.659253
I0916 06:46:47.636387 140548286359360 pytorch_submission_base.py:86] 57500) loss = 1.659, grad_norm = 0.204
I0916 06:50:24.319019 140502817015552 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.213627, loss=1.658526
I0916 06:50:24.322593 140548286359360 pytorch_submission_base.py:86] 58000) loss = 1.659, grad_norm = 0.214
I0916 06:51:10.030507 140548286359360 spec.py:320] Evaluating on the training split.
I0916 06:51:13.767910 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:55:01.307634 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 06:55:04.972028 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:57:28.803044 140548286359360 spec.py:348] Evaluating on the test split.
I0916 06:57:32.531184 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 06:59:45.988120 140548286359360 submission_runner.py:376] Time since start: 40680.80s, 	Step: 58105, 	{'train/accuracy': 0.6656157590123002, 'train/loss': 1.5523228678847991, 'train/bleu': 33.3060064493853, 'validation/accuracy': 0.6776605373770939, 'validation/loss': 1.4725700859257789, 'validation/bleu': 29.739731018351186, 'validation/num_examples': 3000, 'test/accuracy': 0.6916623089884376, 'test/loss': 1.3837500363139852, 'test/bleu': 29.4505696280244, 'test/num_examples': 3003, 'score': 25212.772010564804, 'total_duration': 40680.802152872086, 'accumulated_submission_time': 25212.772010564804, 'accumulated_eval_time': 15411.362670183182, 'accumulated_logging_time': 0.8487081527709961}
I0916 06:59:46.007637 140493521635072 logging_writer.py:48] [58105] accumulated_eval_time=15411.362670, accumulated_logging_time=0.848708, accumulated_submission_time=25212.772011, global_step=58105, preemption_count=0, score=25212.772011, test/accuracy=0.691662, test/bleu=29.450570, test/loss=1.383750, test/num_examples=3003, total_duration=40680.802153, train/accuracy=0.665616, train/bleu=33.306006, train/loss=1.552323, validation/accuracy=0.677661, validation/bleu=29.739731, validation/loss=1.472570, validation/num_examples=3000
I0916 07:02:38.288323 140502817015552 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.187341, loss=1.644784
I0916 07:02:38.292331 140548286359360 pytorch_submission_base.py:86] 58500) loss = 1.645, grad_norm = 0.187
I0916 07:06:14.887253 140493521635072 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.180609, loss=1.643570
I0916 07:06:14.890838 140548286359360 pytorch_submission_base.py:86] 59000) loss = 1.644, grad_norm = 0.181
I0916 07:09:51.513260 140502817015552 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.195616, loss=1.684878
I0916 07:09:51.517749 140548286359360 pytorch_submission_base.py:86] 59500) loss = 1.685, grad_norm = 0.196
I0916 07:13:28.232199 140493521635072 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.201367, loss=1.739635
I0916 07:13:28.236440 140548286359360 pytorch_submission_base.py:86] 60000) loss = 1.740, grad_norm = 0.201
I0916 07:13:46.687351 140548286359360 spec.py:320] Evaluating on the training split.
I0916 07:13:50.428792 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 07:17:25.516632 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 07:17:29.190541 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 07:20:06.375887 140548286359360 spec.py:348] Evaluating on the test split.
I0916 07:20:10.113564 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 07:22:47.519778 140548286359360 submission_runner.py:376] Time since start: 42062.33s, 	Step: 60042, 	{'train/accuracy': 0.66344400481679, 'train/loss': 1.5674051336085786, 'train/bleu': 32.90352663664884, 'validation/accuracy': 0.6789996404260331, 'validation/loss': 1.4669271382251925, 'validation/bleu': 29.466625662367147, 'validation/num_examples': 3000, 'test/accuracy': 0.6939283016675382, 'test/loss': 1.3731488946022892, 'test/bleu': 29.25067630613892, 'test/num_examples': 3003, 'score': 26051.63799715042, 'total_duration': 42062.333797454834, 'accumulated_submission_time': 26051.63799715042, 'accumulated_eval_time': 15952.195127725601, 'accumulated_logging_time': 0.8777682781219482}
I0916 07:22:47.538783 140502817015552 logging_writer.py:48] [60042] accumulated_eval_time=15952.195128, accumulated_logging_time=0.877768, accumulated_submission_time=26051.637997, global_step=60042, preemption_count=0, score=26051.637997, test/accuracy=0.693928, test/bleu=29.250676, test/loss=1.373149, test/num_examples=3003, total_duration=42062.333797, train/accuracy=0.663444, train/bleu=32.903527, train/loss=1.567405, validation/accuracy=0.679000, validation/bleu=29.466626, validation/loss=1.466927, validation/num_examples=3000
I0916 07:26:07.045121 140493521635072 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.200446, loss=1.662200
I0916 07:26:07.049520 140548286359360 pytorch_submission_base.py:86] 60500) loss = 1.662, grad_norm = 0.200
I0916 07:29:43.593153 140502817015552 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.189642, loss=1.688025
I0916 07:29:43.597248 140548286359360 pytorch_submission_base.py:86] 61000) loss = 1.688, grad_norm = 0.190
I0916 07:33:20.309338 140493521635072 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.201867, loss=1.593007
I0916 07:33:20.312953 140548286359360 pytorch_submission_base.py:86] 61500) loss = 1.593, grad_norm = 0.202
I0916 07:36:48.392029 140548286359360 spec.py:320] Evaluating on the training split.
I0916 07:36:52.127570 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 07:40:36.535006 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 07:40:40.190588 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 07:42:55.360773 140548286359360 spec.py:348] Evaluating on the test split.
I0916 07:42:59.087532 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 07:45:07.144146 140548286359360 submission_runner.py:376] Time since start: 43401.96s, 	Step: 61980, 	{'train/accuracy': 0.6672072174955748, 'train/loss': 1.5574134214583453, 'train/bleu': 33.13932775270849, 'validation/accuracy': 0.679334416188268, 'validation/loss': 1.4567734358532443, 'validation/bleu': 29.49959670083265, 'validation/num_examples': 3000, 'test/accuracy': 0.6967520771599558, 'test/loss': 1.3614961906629481, 'test/bleu': 29.544832346110113, 'test/num_examples': 3003, 'score': 26890.69024991989, 'total_duration': 43401.95816779137, 'accumulated_submission_time': 26890.69024991989, 'accumulated_eval_time': 16450.94729065895, 'accumulated_logging_time': 0.9066295623779297}
I0916 07:45:07.164113 140502817015552 logging_writer.py:48] [61980] accumulated_eval_time=16450.947291, accumulated_logging_time=0.906630, accumulated_submission_time=26890.690250, global_step=61980, preemption_count=0, score=26890.690250, test/accuracy=0.696752, test/bleu=29.544832, test/loss=1.361496, test/num_examples=3003, total_duration=43401.958168, train/accuracy=0.667207, train/bleu=33.139328, train/loss=1.557413, validation/accuracy=0.679334, validation/bleu=29.499597, validation/loss=1.456773, validation/num_examples=3000
I0916 07:45:16.942928 140493521635072 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.203136, loss=1.728673
I0916 07:45:16.947027 140548286359360 pytorch_submission_base.py:86] 62000) loss = 1.729, grad_norm = 0.203
I0916 07:48:53.770171 140502817015552 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.219615, loss=1.604364
I0916 07:48:53.773984 140548286359360 pytorch_submission_base.py:86] 62500) loss = 1.604, grad_norm = 0.220
I0916 07:52:30.616652 140493521635072 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.235991, loss=1.629354
I0916 07:52:30.620874 140548286359360 pytorch_submission_base.py:86] 63000) loss = 1.629, grad_norm = 0.236
I0916 07:56:07.249722 140502817015552 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.201694, loss=1.630459
I0916 07:56:07.253168 140548286359360 pytorch_submission_base.py:86] 63500) loss = 1.630, grad_norm = 0.202
I0916 07:59:08.186629 140548286359360 spec.py:320] Evaluating on the training split.
I0916 07:59:11.928750 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:03:29.830567 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 08:03:33.487510 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:06:24.307408 140548286359360 spec.py:348] Evaluating on the test split.
I0916 08:06:28.046478 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:09:27.310473 140548286359360 submission_runner.py:376] Time since start: 44862.12s, 	Step: 63917, 	{'train/accuracy': 0.6701691937342414, 'train/loss': 1.5221045324639764, 'train/bleu': 33.52928766069958, 'validation/accuracy': 0.6821738106161114, 'validation/loss': 1.4480898020483317, 'validation/bleu': 29.928367254077777, 'validation/num_examples': 3000, 'test/accuracy': 0.6966358724071815, 'test/loss': 1.352622596014177, 'test/bleu': 29.807237851771333, 'test/num_examples': 3003, 'score': 27729.889011621475, 'total_duration': 44862.12448811531, 'accumulated_submission_time': 27729.889011621475, 'accumulated_eval_time': 17070.071225881577, 'accumulated_logging_time': 0.9359610080718994}
I0916 08:09:27.330391 140493521635072 logging_writer.py:48] [63917] accumulated_eval_time=17070.071226, accumulated_logging_time=0.935961, accumulated_submission_time=27729.889012, global_step=63917, preemption_count=0, score=27729.889012, test/accuracy=0.696636, test/bleu=29.807238, test/loss=1.352623, test/num_examples=3003, total_duration=44862.124488, train/accuracy=0.670169, train/bleu=33.529288, train/loss=1.522105, validation/accuracy=0.682174, validation/bleu=29.928367, validation/loss=1.448090, validation/num_examples=3000
I0916 08:10:04.356929 140502817015552 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.192062, loss=1.636134
I0916 08:10:04.361288 140548286359360 pytorch_submission_base.py:86] 64000) loss = 1.636, grad_norm = 0.192
I0916 08:13:40.922779 140493521635072 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.191956, loss=1.641638
I0916 08:13:40.926702 140548286359360 pytorch_submission_base.py:86] 64500) loss = 1.642, grad_norm = 0.192
I0916 08:17:17.662469 140502817015552 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.199271, loss=1.569955
I0916 08:17:17.666174 140548286359360 pytorch_submission_base.py:86] 65000) loss = 1.570, grad_norm = 0.199
I0916 08:20:54.151404 140493521635072 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.202467, loss=1.541686
I0916 08:20:54.155999 140548286359360 pytorch_submission_base.py:86] 65500) loss = 1.542, grad_norm = 0.202
I0916 08:23:28.129784 140548286359360 spec.py:320] Evaluating on the training split.
I0916 08:23:31.877800 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:27:35.396909 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 08:27:39.060404 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:30:15.561857 140548286359360 spec.py:348] Evaluating on the test split.
I0916 08:30:19.277835 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:33:13.429267 140548286359360 submission_runner.py:376] Time since start: 46288.24s, 	Step: 65855, 	{'train/accuracy': 0.6707203840840636, 'train/loss': 1.5185212706932083, 'train/bleu': 33.26551608031923, 'validation/accuracy': 0.6816282501146917, 'validation/loss': 1.4448605139737882, 'validation/bleu': 29.926745217459516, 'validation/num_examples': 3000, 'test/accuracy': 0.6985183894021265, 'test/loss': 1.3473866640520598, 'test/bleu': 30.102687946614797, 'test/num_examples': 3003, 'score': 28568.889939785004, 'total_duration': 46288.24328684807, 'accumulated_submission_time': 28568.889939785004, 'accumulated_eval_time': 17655.37073612213, 'accumulated_logging_time': 0.9665215015411377}
I0916 08:33:13.449281 140502817015552 logging_writer.py:48] [65855] accumulated_eval_time=17655.370736, accumulated_logging_time=0.966522, accumulated_submission_time=28568.889940, global_step=65855, preemption_count=0, score=28568.889940, test/accuracy=0.698518, test/bleu=30.102688, test/loss=1.347387, test/num_examples=3003, total_duration=46288.243287, train/accuracy=0.670720, train/bleu=33.265516, train/loss=1.518521, validation/accuracy=0.681628, validation/bleu=29.926745, validation/loss=1.444861, validation/num_examples=3000
I0916 08:34:17.345216 140493521635072 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.208606, loss=1.584754
I0916 08:34:17.349112 140548286359360 pytorch_submission_base.py:86] 66000) loss = 1.585, grad_norm = 0.209
I0916 08:37:53.955005 140502817015552 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.205723, loss=1.591061
I0916 08:37:53.959128 140548286359360 pytorch_submission_base.py:86] 66500) loss = 1.591, grad_norm = 0.206
I0916 08:41:30.854183 140493521635072 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.200131, loss=1.658801
I0916 08:41:30.858812 140548286359360 pytorch_submission_base.py:86] 67000) loss = 1.659, grad_norm = 0.200
I0916 08:45:07.586644 140502817015552 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.216777, loss=1.508523
I0916 08:45:07.591569 140548286359360 pytorch_submission_base.py:86] 67500) loss = 1.509, grad_norm = 0.217
I0916 08:47:14.472303 140548286359360 spec.py:320] Evaluating on the training split.
I0916 08:47:18.221377 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:51:31.630657 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 08:51:35.309103 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:54:01.242086 140548286359360 spec.py:348] Evaluating on the test split.
I0916 08:54:04.979090 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 08:56:21.902658 140548286359360 submission_runner.py:376] Time since start: 47676.72s, 	Step: 67792, 	{'train/accuracy': 0.6686363532659533, 'train/loss': 1.5407497333508247, 'train/bleu': 33.666151112396506, 'validation/accuracy': 0.6826449765037012, 'validation/loss': 1.4397742665930986, 'validation/bleu': 29.82620958383268, 'validation/num_examples': 3000, 'test/accuracy': 0.6981116727674161, 'test/loss': 1.3458215312881296, 'test/bleu': 30.181832350093803, 'test/num_examples': 3003, 'score': 29408.087728977203, 'total_duration': 47676.716633081436, 'accumulated_submission_time': 29408.087728977203, 'accumulated_eval_time': 18202.801179409027, 'accumulated_logging_time': 0.9960479736328125}
I0916 08:56:21.922421 140493521635072 logging_writer.py:48] [67792] accumulated_eval_time=18202.801179, accumulated_logging_time=0.996048, accumulated_submission_time=29408.087729, global_step=67792, preemption_count=0, score=29408.087729, test/accuracy=0.698112, test/bleu=30.181832, test/loss=1.345822, test/num_examples=3003, total_duration=47676.716633, train/accuracy=0.668636, train/bleu=33.666151, train/loss=1.540750, validation/accuracy=0.682645, validation/bleu=29.826210, validation/loss=1.439774, validation/num_examples=3000
I0916 08:57:53.171455 140502817015552 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.189739, loss=1.525674
I0916 08:57:53.175489 140548286359360 pytorch_submission_base.py:86] 68000) loss = 1.526, grad_norm = 0.190
I0916 09:01:29.989230 140493521635072 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.224338, loss=1.559497
I0916 09:01:29.993688 140548286359360 pytorch_submission_base.py:86] 68500) loss = 1.559, grad_norm = 0.224
I0916 09:05:06.756823 140502817015552 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.205929, loss=1.598433
I0916 09:05:06.760516 140548286359360 pytorch_submission_base.py:86] 69000) loss = 1.598, grad_norm = 0.206
I0916 09:08:43.522827 140493521635072 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.259902, loss=1.606446
I0916 09:08:43.527057 140548286359360 pytorch_submission_base.py:86] 69500) loss = 1.606, grad_norm = 0.260
I0916 09:10:22.656431 140548286359360 spec.py:320] Evaluating on the training split.
I0916 09:10:26.423101 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 09:14:10.641541 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 09:14:14.306588 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 09:17:01.128235 140548286359360 spec.py:348] Evaluating on the test split.
I0916 09:17:04.848927 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 09:19:53.467740 140548286359360 submission_runner.py:376] Time since start: 49088.28s, 	Step: 69728, 	{'train/accuracy': 0.6783969842893427, 'train/loss': 1.4674473629770948, 'train/bleu': 33.896882991916954, 'validation/accuracy': 0.6841576669849103, 'validation/loss': 1.4302246329865718, 'validation/bleu': 30.053019980064374, 'validation/num_examples': 3000, 'test/accuracy': 0.6985532508279588, 'test/loss': 1.3396984486665504, 'test/bleu': 29.802057282650225, 'test/num_examples': 3003, 'score': 30247.01449728012, 'total_duration': 49088.2817606926, 'accumulated_submission_time': 30247.01449728012, 'accumulated_eval_time': 18773.612513303757, 'accumulated_logging_time': 1.0252406597137451}
I0916 09:19:53.487750 140502817015552 logging_writer.py:48] [69728] accumulated_eval_time=18773.612513, accumulated_logging_time=1.025241, accumulated_submission_time=30247.014497, global_step=69728, preemption_count=0, score=30247.014497, test/accuracy=0.698553, test/bleu=29.802057, test/loss=1.339698, test/num_examples=3003, total_duration=49088.281761, train/accuracy=0.678397, train/bleu=33.896883, train/loss=1.467447, validation/accuracy=0.684158, validation/bleu=30.053020, validation/loss=1.430225, validation/num_examples=3000
I0916 09:21:52.298852 140493521635072 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.305268, loss=1.540280
I0916 09:21:52.302926 140548286359360 pytorch_submission_base.py:86] 70000) loss = 1.540, grad_norm = 0.305
I0916 09:25:29.061732 140502817015552 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.217221, loss=1.592815
I0916 09:25:29.065745 140548286359360 pytorch_submission_base.py:86] 70500) loss = 1.593, grad_norm = 0.217
I0916 09:29:05.592411 140493521635072 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.195486, loss=1.579953
I0916 09:29:05.596319 140548286359360 pytorch_submission_base.py:86] 71000) loss = 1.580, grad_norm = 0.195
I0916 09:32:42.320624 140502817015552 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.209242, loss=1.566367
I0916 09:32:42.324347 140548286359360 pytorch_submission_base.py:86] 71500) loss = 1.566, grad_norm = 0.209
I0916 09:33:54.527863 140548286359360 spec.py:320] Evaluating on the training split.
I0916 09:33:58.271121 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 09:38:04.985806 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 09:38:08.650889 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 09:40:46.484189 140548286359360 spec.py:348] Evaluating on the test split.
I0916 09:40:50.225817 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 09:43:27.489750 140548286359360 submission_runner.py:376] Time since start: 50502.30s, 	Step: 71666, 	{'train/accuracy': 0.6763402823093039, 'train/loss': 1.484094790700736, 'train/bleu': 33.98225085368709, 'validation/accuracy': 0.6877534066533583, 'validation/loss': 1.4193172976776482, 'validation/bleu': 30.103169553896485, 'validation/num_examples': 3000, 'test/accuracy': 0.7012375806170472, 'test/loss': 1.3241120322758702, 'test/bleu': 30.08516299202726, 'test/num_examples': 3003, 'score': 31086.24572443962, 'total_duration': 50502.30377078056, 'accumulated_submission_time': 31086.24572443962, 'accumulated_eval_time': 19346.574458122253, 'accumulated_logging_time': 1.0552895069122314}
I0916 09:43:27.510425 140493521635072 logging_writer.py:48] [71666] accumulated_eval_time=19346.574458, accumulated_logging_time=1.055290, accumulated_submission_time=31086.245724, global_step=71666, preemption_count=0, score=31086.245724, test/accuracy=0.701238, test/bleu=30.085163, test/loss=1.324112, test/num_examples=3003, total_duration=50502.303771, train/accuracy=0.676340, train/bleu=33.982251, train/loss=1.484095, validation/accuracy=0.687753, validation/bleu=30.103170, validation/loss=1.419317, validation/num_examples=3000
I0916 09:45:53.419810 140502817015552 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.207458, loss=1.583623
I0916 09:45:53.423790 140548286359360 pytorch_submission_base.py:86] 72000) loss = 1.584, grad_norm = 0.207
I0916 09:49:30.183461 140493521635072 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.200134, loss=1.549569
I0916 09:49:30.187497 140548286359360 pytorch_submission_base.py:86] 72500) loss = 1.550, grad_norm = 0.200
I0916 09:53:06.939311 140502817015552 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.209145, loss=1.620557
I0916 09:53:06.943042 140548286359360 pytorch_submission_base.py:86] 73000) loss = 1.621, grad_norm = 0.209
I0916 09:56:43.839068 140493521635072 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.202666, loss=1.548261
I0916 09:56:43.843577 140548286359360 pytorch_submission_base.py:86] 73500) loss = 1.548, grad_norm = 0.203
I0916 09:57:28.270509 140548286359360 spec.py:320] Evaluating on the training split.
I0916 09:57:32.036049 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:01:30.845673 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 10:01:34.507770 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:03:49.181322 140548286359360 spec.py:348] Evaluating on the test split.
I0916 10:03:52.912614 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:06:18.340430 140548286359360 submission_runner.py:376] Time since start: 51873.15s, 	Step: 73602, 	{'train/accuracy': 0.670734622784349, 'train/loss': 1.5161990129893785, 'train/bleu': 34.0176450068542, 'validation/accuracy': 0.6872698416634636, 'validation/loss': 1.4138070591189198, 'validation/bleu': 30.301729899029556, 'validation/num_examples': 3000, 'test/accuracy': 0.7036895009005868, 'test/loss': 1.3104700482249725, 'test/bleu': 30.356615795898776, 'test/num_examples': 3003, 'score': 31925.20730447769, 'total_duration': 51873.154420137405, 'accumulated_submission_time': 31925.20730447769, 'accumulated_eval_time': 19876.64458489418, 'accumulated_logging_time': 1.0856199264526367}
I0916 10:06:18.360697 140502817015552 logging_writer.py:48] [73602] accumulated_eval_time=19876.644585, accumulated_logging_time=1.085620, accumulated_submission_time=31925.207304, global_step=73602, preemption_count=0, score=31925.207304, test/accuracy=0.703690, test/bleu=30.356616, test/loss=1.310470, test/num_examples=3003, total_duration=51873.154420, train/accuracy=0.670735, train/bleu=34.017645, train/loss=1.516199, validation/accuracy=0.687270, validation/bleu=30.301730, validation/loss=1.413807, validation/num_examples=3000
I0916 10:09:12.016470 140493521635072 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.204903, loss=1.533262
I0916 10:09:12.020045 140548286359360 pytorch_submission_base.py:86] 74000) loss = 1.533, grad_norm = 0.205
I0916 10:12:48.716236 140502817015552 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.219808, loss=1.652791
I0916 10:12:48.720581 140548286359360 pytorch_submission_base.py:86] 74500) loss = 1.653, grad_norm = 0.220
I0916 10:16:25.419265 140493521635072 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.224227, loss=1.511047
I0916 10:16:25.423207 140548286359360 pytorch_submission_base.py:86] 75000) loss = 1.511, grad_norm = 0.224
I0916 10:20:02.034105 140502817015552 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.201723, loss=1.535772
I0916 10:20:02.039165 140548286359360 pytorch_submission_base.py:86] 75500) loss = 1.536, grad_norm = 0.202
I0916 10:20:19.185216 140548286359360 spec.py:320] Evaluating on the training split.
I0916 10:20:22.927680 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:24:06.061379 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 10:24:09.744890 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:26:36.663488 140548286359360 spec.py:348] Evaluating on the test split.
I0916 10:26:40.392992 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:28:47.878851 140548286359360 submission_runner.py:376] Time since start: 53222.69s, 	Step: 75539, 	{'train/accuracy': 0.6856483481503376, 'train/loss': 1.4249545435813191, 'train/bleu': 34.9248659171008, 'validation/accuracy': 0.6876294156303083, 'validation/loss': 1.4081208114282526, 'validation/bleu': 30.243288374175677, 'validation/num_examples': 3000, 'test/accuracy': 0.7039683923072454, 'test/loss': 1.310944581227122, 'test/bleu': 30.28881401849146, 'test/num_examples': 3003, 'score': 32764.24068379402, 'total_duration': 53222.692863464355, 'accumulated_submission_time': 32764.24068379402, 'accumulated_eval_time': 20385.33827328682, 'accumulated_logging_time': 1.1155674457550049}
I0916 10:28:47.899601 140493521635072 logging_writer.py:48] [75539] accumulated_eval_time=20385.338273, accumulated_logging_time=1.115567, accumulated_submission_time=32764.240684, global_step=75539, preemption_count=0, score=32764.240684, test/accuracy=0.703968, test/bleu=30.288814, test/loss=1.310945, test/num_examples=3003, total_duration=53222.692863, train/accuracy=0.685648, train/bleu=34.924866, train/loss=1.424955, validation/accuracy=0.687629, validation/bleu=30.243288, validation/loss=1.408121, validation/num_examples=3000
I0916 10:32:08.890290 140502817015552 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.208203, loss=1.620303
I0916 10:32:08.894110 140548286359360 pytorch_submission_base.py:86] 76000) loss = 1.620, grad_norm = 0.208
I0916 10:35:45.637363 140493521635072 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.202300, loss=1.555179
I0916 10:35:45.641818 140548286359360 pytorch_submission_base.py:86] 76500) loss = 1.555, grad_norm = 0.202
I0916 10:39:22.389016 140502817015552 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.204582, loss=1.501670
I0916 10:39:22.393515 140548286359360 pytorch_submission_base.py:86] 77000) loss = 1.502, grad_norm = 0.205
I0916 10:42:48.937061 140548286359360 spec.py:320] Evaluating on the training split.
I0916 10:42:52.695381 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:46:53.442618 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 10:46:57.093456 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:49:30.497136 140548286359360 spec.py:348] Evaluating on the test split.
I0916 10:49:34.222875 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 10:52:06.296095 140548286359360 submission_runner.py:376] Time since start: 54621.11s, 	Step: 77476, 	{'train/accuracy': 0.6817491406187545, 'train/loss': 1.4452101629683771, 'train/bleu': 34.44427356101495, 'validation/accuracy': 0.6892784962368724, 'validation/loss': 1.3988072451054543, 'validation/bleu': 30.66482949169634, 'validation/num_examples': 3000, 'test/accuracy': 0.7053163674394283, 'test/loss': 1.300615612834815, 'test/bleu': 30.429071222834825, 'test/num_examples': 3003, 'score': 33603.42820382118, 'total_duration': 54621.11009454727, 'accumulated_submission_time': 33603.42820382118, 'accumulated_eval_time': 20942.697358608246, 'accumulated_logging_time': 1.145965814590454}
I0916 10:52:06.317822 140493521635072 logging_writer.py:48] [77476] accumulated_eval_time=20942.697359, accumulated_logging_time=1.145966, accumulated_submission_time=33603.428204, global_step=77476, preemption_count=0, score=33603.428204, test/accuracy=0.705316, test/bleu=30.429071, test/loss=1.300616, test/num_examples=3003, total_duration=54621.110095, train/accuracy=0.681749, train/bleu=34.444274, train/loss=1.445210, validation/accuracy=0.689278, validation/bleu=30.664829, validation/loss=1.398807, validation/num_examples=3000
I0916 10:52:17.850709 140502817015552 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.198077, loss=1.558975
I0916 10:52:17.854615 140548286359360 pytorch_submission_base.py:86] 77500) loss = 1.559, grad_norm = 0.198
I0916 10:55:54.548570 140493521635072 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.206080, loss=1.562289
I0916 10:55:54.552870 140548286359360 pytorch_submission_base.py:86] 78000) loss = 1.562, grad_norm = 0.206
I0916 10:59:31.318369 140502817015552 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.221842, loss=1.586788
I0916 10:59:31.322883 140548286359360 pytorch_submission_base.py:86] 78500) loss = 1.587, grad_norm = 0.222
I0916 11:03:08.182764 140493521635072 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.206803, loss=1.514647
I0916 11:03:08.186885 140548286359360 pytorch_submission_base.py:86] 79000) loss = 1.515, grad_norm = 0.207
I0916 11:06:07.022425 140548286359360 spec.py:320] Evaluating on the training split.
I0916 11:06:10.779828 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:09:59.639424 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 11:10:03.301352 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:12:40.248541 140548286359360 spec.py:348] Evaluating on the test split.
I0916 11:12:43.969846 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:15:12.009593 140548286359360 submission_runner.py:376] Time since start: 56006.82s, 	Step: 79412, 	{'train/accuracy': 0.6790285564220551, 'train/loss': 1.4710710656420913, 'train/bleu': 34.15271984650601, 'validation/accuracy': 0.6885097518939629, 'validation/loss': 1.3987671417589367, 'validation/bleu': 30.295707325996634, 'validation/num_examples': 3000, 'test/accuracy': 0.7071756434838185, 'test/loss': 1.2961548936726512, 'test/bleu': 30.575647800891755, 'test/num_examples': 3003, 'score': 34442.28955411911, 'total_duration': 56006.82362937927, 'accumulated_submission_time': 34442.28955411911, 'accumulated_eval_time': 21487.68458890915, 'accumulated_logging_time': 1.1779513359069824}
I0916 11:15:12.031348 140502817015552 logging_writer.py:48] [79412] accumulated_eval_time=21487.684589, accumulated_logging_time=1.177951, accumulated_submission_time=34442.289554, global_step=79412, preemption_count=0, score=34442.289554, test/accuracy=0.707176, test/bleu=30.575648, test/loss=1.296155, test/num_examples=3003, total_duration=56006.823629, train/accuracy=0.679029, train/bleu=34.152720, train/loss=1.471071, validation/accuracy=0.688510, validation/bleu=30.295707, validation/loss=1.398767, validation/num_examples=3000
I0916 11:15:51.283012 140493521635072 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.215131, loss=1.612385
I0916 11:15:51.286591 140548286359360 pytorch_submission_base.py:86] 79500) loss = 1.612, grad_norm = 0.215
I0916 11:19:27.925002 140502817015552 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.213814, loss=1.516883
I0916 11:19:27.928568 140548286359360 pytorch_submission_base.py:86] 80000) loss = 1.517, grad_norm = 0.214
I0916 11:23:04.525515 140493521635072 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.214650, loss=1.497677
I0916 11:23:04.530015 140548286359360 pytorch_submission_base.py:86] 80500) loss = 1.498, grad_norm = 0.215
I0916 11:26:41.156793 140502817015552 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.213542, loss=1.453140
I0916 11:26:41.161331 140548286359360 pytorch_submission_base.py:86] 81000) loss = 1.453, grad_norm = 0.214
I0916 11:29:13.084876 140548286359360 spec.py:320] Evaluating on the training split.
I0916 11:29:16.851590 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:33:02.533629 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 11:33:06.211254 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:35:33.495198 140548286359360 spec.py:348] Evaluating on the test split.
I0916 11:35:37.225802 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:37:57.147351 140548286359360 submission_runner.py:376] Time since start: 57371.96s, 	Step: 81350, 	{'train/accuracy': 0.69639723811043, 'train/loss': 1.3686391583870747, 'train/bleu': 35.66672404940048, 'validation/accuracy': 0.6909895723549615, 'validation/loss': 1.3908368502870392, 'validation/bleu': 30.731510654200974, 'validation/num_examples': 3000, 'test/accuracy': 0.707082679681599, 'test/loss': 1.2920591300621695, 'test/bleu': 30.729056093647316, 'test/num_examples': 3003, 'score': 35281.52217125893, 'total_duration': 57371.96135807037, 'accumulated_submission_time': 35281.52217125893, 'accumulated_eval_time': 22011.747089147568, 'accumulated_logging_time': 1.2102937698364258}
I0916 11:37:57.168007 140493521635072 logging_writer.py:48] [81350] accumulated_eval_time=22011.747089, accumulated_logging_time=1.210294, accumulated_submission_time=35281.522171, global_step=81350, preemption_count=0, score=35281.522171, test/accuracy=0.707083, test/bleu=30.729056, test/loss=1.292059, test/num_examples=3003, total_duration=57371.961358, train/accuracy=0.696397, train/bleu=35.666724, train/loss=1.368639, validation/accuracy=0.690990, validation/bleu=30.731511, validation/loss=1.390837, validation/num_examples=3000
I0916 11:39:03.347635 140502817015552 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.216674, loss=1.539786
I0916 11:39:03.351717 140548286359360 pytorch_submission_base.py:86] 81500) loss = 1.540, grad_norm = 0.217
I0916 11:42:40.095019 140493521635072 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.218873, loss=1.523346
I0916 11:42:40.098596 140548286359360 pytorch_submission_base.py:86] 82000) loss = 1.523, grad_norm = 0.219
I0916 11:46:16.860450 140502817015552 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.219590, loss=1.467344
I0916 11:46:16.865154 140548286359360 pytorch_submission_base.py:86] 82500) loss = 1.467, grad_norm = 0.220
I0916 11:49:53.669399 140493521635072 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.218480, loss=1.508242
I0916 11:49:53.673331 140548286359360 pytorch_submission_base.py:86] 83000) loss = 1.508, grad_norm = 0.218
I0916 11:51:57.897967 140548286359360 spec.py:320] Evaluating on the training split.
I0916 11:52:01.640383 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:55:56.787638 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 11:56:00.441135 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 11:58:22.573368 140548286359360 spec.py:348] Evaluating on the test split.
I0916 11:58:26.289943 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:00:45.938056 140548286359360 submission_runner.py:376] Time since start: 58740.75s, 	Step: 83286, 	{'train/accuracy': 0.6899125123424189, 'train/loss': 1.4051503945268549, 'train/bleu': 34.659869928608366, 'validation/accuracy': 0.6921178906647159, 'validation/loss': 1.3847799856790368, 'validation/bleu': 30.757435491047776, 'validation/num_examples': 3000, 'test/accuracy': 0.7088722328743245, 'test/loss': 1.2842437249433503, 'test/bleu': 30.874200324254378, 'test/num_examples': 3003, 'score': 36120.4275701046, 'total_duration': 58740.75207948685, 'accumulated_submission_time': 36120.4275701046, 'accumulated_eval_time': 22539.78720498085, 'accumulated_logging_time': 1.2403175830841064}
I0916 12:00:45.958650 140502817015552 logging_writer.py:48] [83286] accumulated_eval_time=22539.787205, accumulated_logging_time=1.240318, accumulated_submission_time=36120.427570, global_step=83286, preemption_count=0, score=36120.427570, test/accuracy=0.708872, test/bleu=30.874200, test/loss=1.284244, test/num_examples=3003, total_duration=58740.752079, train/accuracy=0.689913, train/bleu=34.659870, train/loss=1.405150, validation/accuracy=0.692118, validation/bleu=30.757435, validation/loss=1.384780, validation/num_examples=3000
I0916 12:02:19.742547 140493521635072 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.216421, loss=1.524094
I0916 12:02:19.746783 140548286359360 pytorch_submission_base.py:86] 83500) loss = 1.524, grad_norm = 0.216
I0916 12:05:56.441785 140502817015552 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.218047, loss=1.528091
I0916 12:05:56.445439 140548286359360 pytorch_submission_base.py:86] 84000) loss = 1.528, grad_norm = 0.218
I0916 12:09:33.190190 140493521635072 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.208073, loss=1.539773
I0916 12:09:33.194298 140548286359360 pytorch_submission_base.py:86] 84500) loss = 1.540, grad_norm = 0.208
I0916 12:13:09.799376 140502817015552 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.208690, loss=1.486102
I0916 12:13:09.803434 140548286359360 pytorch_submission_base.py:86] 85000) loss = 1.486, grad_norm = 0.209
I0916 12:14:46.702847 140548286359360 spec.py:320] Evaluating on the training split.
I0916 12:14:50.438317 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:18:34.941257 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 12:18:38.606716 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:21:06.349267 140548286359360 spec.py:348] Evaluating on the test split.
I0916 12:21:10.076420 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:23:22.155265 140548286359360 submission_runner.py:376] Time since start: 60096.97s, 	Step: 85223, 	{'train/accuracy': 0.6868093567518769, 'train/loss': 1.4139526005896537, 'train/bleu': 34.195216897693065, 'validation/accuracy': 0.6920186978462759, 'validation/loss': 1.3820907366306678, 'validation/bleu': 30.452985728097694, 'validation/num_examples': 3000, 'test/accuracy': 0.7091511242809831, 'test/loss': 1.2817079193539016, 'test/bleu': 30.728248298398086, 'test/num_examples': 3003, 'score': 36959.37769436836, 'total_duration': 60096.96929478645, 'accumulated_submission_time': 36959.37769436836, 'accumulated_eval_time': 23055.239689826965, 'accumulated_logging_time': 1.270453929901123}
I0916 12:23:22.176225 140493521635072 logging_writer.py:48] [85223] accumulated_eval_time=23055.239690, accumulated_logging_time=1.270454, accumulated_submission_time=36959.377694, global_step=85223, preemption_count=0, score=36959.377694, test/accuracy=0.709151, test/bleu=30.728248, test/loss=1.281708, test/num_examples=3003, total_duration=60096.969295, train/accuracy=0.686809, train/bleu=34.195217, train/loss=1.413953, validation/accuracy=0.692019, validation/bleu=30.452986, validation/loss=1.382091, validation/num_examples=3000
I0916 12:25:23.160990 140502817015552 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.217601, loss=1.554903
I0916 12:25:23.165358 140548286359360 pytorch_submission_base.py:86] 85500) loss = 1.555, grad_norm = 0.218
I0916 12:28:59.836350 140493521635072 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.214776, loss=1.500968
I0916 12:28:59.839990 140548286359360 pytorch_submission_base.py:86] 86000) loss = 1.501, grad_norm = 0.215
I0916 12:32:36.490412 140502817015552 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.212924, loss=1.496141
I0916 12:32:36.494224 140548286359360 pytorch_submission_base.py:86] 86500) loss = 1.496, grad_norm = 0.213
I0916 12:36:13.312837 140493521635072 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.217467, loss=1.482310
I0916 12:36:13.316629 140548286359360 pytorch_submission_base.py:86] 87000) loss = 1.482, grad_norm = 0.217
I0916 12:37:22.919865 140548286359360 spec.py:320] Evaluating on the training split.
I0916 12:37:26.676421 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:41:22.294965 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 12:41:25.948128 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:43:44.665999 140548286359360 spec.py:348] Evaluating on the test split.
I0916 12:43:48.390346 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 12:45:58.647950 140548286359360 submission_runner.py:376] Time since start: 61453.46s, 	Step: 87160, 	{'train/accuracy': 0.6854488458250214, 'train/loss': 1.4329901681390709, 'train/bleu': 34.80930256268224, 'validation/accuracy': 0.6923658727108157, 'validation/loss': 1.3809235742582238, 'validation/bleu': 30.66349984053672, 'validation/num_examples': 3000, 'test/accuracy': 0.7091976061820928, 'test/loss': 1.2777744792574517, 'test/bleu': 30.957561902975634, 'test/num_examples': 3003, 'score': 37798.30632042885, 'total_duration': 61453.46198105812, 'accumulated_submission_time': 37798.30632042885, 'accumulated_eval_time': 23570.96785211563, 'accumulated_logging_time': 1.301124095916748}
I0916 12:45:58.668789 140502817015552 logging_writer.py:48] [87160] accumulated_eval_time=23570.967852, accumulated_logging_time=1.301124, accumulated_submission_time=37798.306320, global_step=87160, preemption_count=0, score=37798.306320, test/accuracy=0.709198, test/bleu=30.957562, test/loss=1.277774, test/num_examples=3003, total_duration=61453.461981, train/accuracy=0.685449, train/bleu=34.809303, train/loss=1.432990, validation/accuracy=0.692366, validation/bleu=30.663500, validation/loss=1.380924, validation/num_examples=3000
I0916 12:48:27.069294 140493521635072 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.221808, loss=1.451457
I0916 12:48:27.073326 140548286359360 pytorch_submission_base.py:86] 87500) loss = 1.451, grad_norm = 0.222
I0916 12:52:03.768525 140502817015552 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.210557, loss=1.430132
I0916 12:52:03.772433 140548286359360 pytorch_submission_base.py:86] 88000) loss = 1.430, grad_norm = 0.211
I0916 12:55:40.565257 140493521635072 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.222598, loss=1.512470
I0916 12:55:40.568716 140548286359360 pytorch_submission_base.py:86] 88500) loss = 1.512, grad_norm = 0.223
I0916 12:59:17.144417 140502817015552 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.214625, loss=1.471926
I0916 12:59:17.147956 140548286359360 pytorch_submission_base.py:86] 89000) loss = 1.472, grad_norm = 0.215
I0916 12:59:59.425130 140548286359360 spec.py:320] Evaluating on the training split.
I0916 13:00:03.197070 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:04:04.255911 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 13:04:07.926423 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:06:46.660427 140548286359360 spec.py:348] Evaluating on the test split.
I0916 13:06:50.390810 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:09:19.798458 140548286359360 submission_runner.py:376] Time since start: 62854.61s, 	Step: 89097, 	{'train/accuracy': 0.6914659325533379, 'train/loss': 1.3901122318765773, 'train/bleu': 35.204091676264554, 'validation/accuracy': 0.6922666798923758, 'validation/loss': 1.3766441403392395, 'validation/bleu': 30.795370257766148, 'validation/num_examples': 3000, 'test/accuracy': 0.7091278833304282, 'test/loss': 1.2753284599965138, 'test/bleu': 30.907285027171604, 'test/num_examples': 3003, 'score': 38637.24909734726, 'total_duration': 62854.61246776581, 'accumulated_submission_time': 38637.24909734726, 'accumulated_eval_time': 24131.34121155739, 'accumulated_logging_time': 1.3332021236419678}
I0916 13:09:19.819566 140493521635072 logging_writer.py:48] [89097] accumulated_eval_time=24131.341212, accumulated_logging_time=1.333202, accumulated_submission_time=38637.249097, global_step=89097, preemption_count=0, score=38637.249097, test/accuracy=0.709128, test/bleu=30.907285, test/loss=1.275328, test/num_examples=3003, total_duration=62854.612468, train/accuracy=0.691466, train/bleu=35.204092, train/loss=1.390112, validation/accuracy=0.692267, validation/bleu=30.795370, validation/loss=1.376644, validation/num_examples=3000
I0916 13:12:15.543211 140502817015552 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.218881, loss=1.511204
I0916 13:12:15.546922 140548286359360 pytorch_submission_base.py:86] 89500) loss = 1.511, grad_norm = 0.219
I0916 13:15:52.464950 140493521635072 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.216674, loss=1.485571
I0916 13:15:52.469386 140548286359360 pytorch_submission_base.py:86] 90000) loss = 1.486, grad_norm = 0.217
I0916 13:19:29.220794 140502817015552 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.216843, loss=1.422269
I0916 13:19:29.225184 140548286359360 pytorch_submission_base.py:86] 90500) loss = 1.422, grad_norm = 0.217
I0916 13:23:06.064614 140493521635072 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.217165, loss=1.474994
I0916 13:23:06.068430 140548286359360 pytorch_submission_base.py:86] 91000) loss = 1.475, grad_norm = 0.217
I0916 13:23:20.627121 140548286359360 spec.py:320] Evaluating on the training split.
I0916 13:23:24.373621 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:27:04.541503 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 13:27:08.226124 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:29:38.321310 140548286359360 spec.py:348] Evaluating on the test split.
I0916 13:29:42.040149 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:32:19.219817 140548286359360 submission_runner.py:376] Time since start: 64234.03s, 	Step: 91033, 	{'train/accuracy': 0.6886444375906697, 'train/loss': 1.4099240873056669, 'train/bleu': 34.74568818265632, 'validation/accuracy': 0.6925270610407807, 'validation/loss': 1.3748779463366851, 'validation/bleu': 30.615161550600366, 'validation/num_examples': 3000, 'test/accuracy': 0.7104526175120562, 'test/loss': 1.2705817682005693, 'test/bleu': 30.919050079075177, 'test/num_examples': 3003, 'score': 39476.22729182243, 'total_duration': 64234.03384017944, 'accumulated_submission_time': 39476.22729182243, 'accumulated_eval_time': 24669.933961629868, 'accumulated_logging_time': 1.3639936447143555}
I0916 13:32:19.241056 140502817015552 logging_writer.py:48] [91033] accumulated_eval_time=24669.933962, accumulated_logging_time=1.363994, accumulated_submission_time=39476.227292, global_step=91033, preemption_count=0, score=39476.227292, test/accuracy=0.710453, test/bleu=30.919050, test/loss=1.270582, test/num_examples=3003, total_duration=64234.033840, train/accuracy=0.688644, train/bleu=34.745688, train/loss=1.409924, validation/accuracy=0.692527, validation/bleu=30.615162, validation/loss=1.374878, validation/num_examples=3000
I0916 13:35:42.848477 140493521635072 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.225844, loss=1.568476
I0916 13:35:42.852369 140548286359360 pytorch_submission_base.py:86] 91500) loss = 1.568, grad_norm = 0.226
I0916 13:39:19.670525 140502817015552 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.216497, loss=1.430801
I0916 13:39:19.675008 140548286359360 pytorch_submission_base.py:86] 92000) loss = 1.431, grad_norm = 0.216
I0916 13:42:56.354712 140493521635072 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.220659, loss=1.520070
I0916 13:42:56.358869 140548286359360 pytorch_submission_base.py:86] 92500) loss = 1.520, grad_norm = 0.221
I0916 13:46:20.217155 140548286359360 spec.py:320] Evaluating on the training split.
I0916 13:46:23.978146 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:50:05.599287 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 13:50:09.263800 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:52:35.992250 140548286359360 spec.py:348] Evaluating on the test split.
I0916 13:52:39.733372 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 13:55:01.706904 140548286359360 submission_runner.py:376] Time since start: 65596.52s, 	Step: 92970, 	{'train/accuracy': 0.6883505376587765, 'train/loss': 1.4091135712100438, 'train/bleu': 34.90095872598014, 'validation/accuracy': 0.6926882493707456, 'validation/loss': 1.3736237965121325, 'validation/bleu': 30.572436127072947, 'validation/num_examples': 3000, 'test/accuracy': 0.7108593341467666, 'test/loss': 1.2695729656905468, 'test/bleu': 30.87325884562185, 'test/num_examples': 3003, 'score': 40315.36530208588, 'total_duration': 65596.5208902359, 'accumulated_submission_time': 40315.36530208588, 'accumulated_eval_time': 25191.423709392548, 'accumulated_logging_time': 1.3946354389190674}
I0916 13:55:01.730963 140502817015552 logging_writer.py:48] [92970] accumulated_eval_time=25191.423709, accumulated_logging_time=1.394635, accumulated_submission_time=40315.365302, global_step=92970, preemption_count=0, score=40315.365302, test/accuracy=0.710859, test/bleu=30.873259, test/loss=1.269573, test/num_examples=3003, total_duration=65596.520890, train/accuracy=0.688351, train/bleu=34.900959, train/loss=1.409114, validation/accuracy=0.692688, validation/bleu=30.572436, validation/loss=1.373624, validation/num_examples=3000
I0916 13:55:15.878801 140493521635072 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.211004, loss=1.491063
I0916 13:55:15.882848 140548286359360 pytorch_submission_base.py:86] 93000) loss = 1.491, grad_norm = 0.211
I0916 13:58:52.550812 140502817015552 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.216473, loss=1.419142
I0916 13:58:52.555261 140548286359360 pytorch_submission_base.py:86] 93500) loss = 1.419, grad_norm = 0.216
I0916 14:02:29.069451 140493521635072 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.211470, loss=1.411705
I0916 14:02:29.073336 140548286359360 pytorch_submission_base.py:86] 94000) loss = 1.412, grad_norm = 0.211
I0916 14:06:05.707838 140502817015552 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.217019, loss=1.475675
I0916 14:06:05.712312 140548286359360 pytorch_submission_base.py:86] 94500) loss = 1.476, grad_norm = 0.217
I0916 14:09:02.791074 140548286359360 spec.py:320] Evaluating on the training split.
I0916 14:09:06.556770 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 14:13:15.777707 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 14:13:19.458340 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 14:16:05.874311 140548286359360 spec.py:348] Evaluating on the test split.
I0916 14:16:09.608299 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 14:18:50.536824 140548286359360 submission_runner.py:376] Time since start: 67025.35s, 	Step: 94908, 	{'train/accuracy': 0.6916007498514014, 'train/loss': 1.3918943392506058, 'train/bleu': 35.096711394481815, 'validation/accuracy': 0.6926510520638306, 'validation/loss': 1.3710974794174902, 'validation/bleu': 30.60419427220016, 'validation/num_examples': 3000, 'test/accuracy': 0.7109639184242635, 'test/loss': 1.2683152308116903, 'test/bleu': 30.868801544172666, 'test/num_examples': 3003, 'score': 41154.604830503464, 'total_duration': 67025.35080504417, 'accumulated_submission_time': 41154.604830503464, 'accumulated_eval_time': 25779.169449567795, 'accumulated_logging_time': 1.4308946132659912}
I0916 14:18:50.558667 140493521635072 logging_writer.py:48] [94908] accumulated_eval_time=25779.169450, accumulated_logging_time=1.430895, accumulated_submission_time=41154.604831, global_step=94908, preemption_count=0, score=41154.604831, test/accuracy=0.710964, test/bleu=30.868802, test/loss=1.268315, test/num_examples=3003, total_duration=67025.350805, train/accuracy=0.691601, train/bleu=35.096711, train/loss=1.391894, validation/accuracy=0.692651, validation/bleu=30.604194, validation/loss=1.371097, validation/num_examples=3000
I0916 14:19:31.541656 140502817015552 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.209589, loss=1.548794
I0916 14:19:31.545310 140548286359360 pytorch_submission_base.py:86] 95000) loss = 1.549, grad_norm = 0.210
I0916 14:23:08.137108 140493521635072 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.206787, loss=1.493415
I0916 14:23:08.140523 140548286359360 pytorch_submission_base.py:86] 95500) loss = 1.493, grad_norm = 0.207
I0916 14:26:44.713396 140502817015552 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.217423, loss=1.435801
I0916 14:26:44.717332 140548286359360 pytorch_submission_base.py:86] 96000) loss = 1.436, grad_norm = 0.217
I0916 14:30:21.279597 140493521635072 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.216321, loss=1.501195
I0916 14:30:21.283344 140548286359360 pytorch_submission_base.py:86] 96500) loss = 1.501, grad_norm = 0.216
I0916 14:32:51.414932 140548286359360 spec.py:320] Evaluating on the training split.
I0916 14:32:55.160320 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 14:36:40.922691 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 14:36:44.572650 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 14:39:18.760106 140548286359360 spec.py:348] Evaluating on the test split.
I0916 14:39:22.507848 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 14:42:08.710959 140548286359360 submission_runner.py:376] Time since start: 68423.52s, 	Step: 96846, 	{'train/accuracy': 0.6912699854651163, 'train/loss': 1.3916236522585848, 'train/bleu': 35.368876265285955, 'validation/accuracy': 0.693779370373585, 'validation/loss': 1.3705708081734882, 'validation/bleu': 30.902189213072816, 'validation/num_examples': 3000, 'test/accuracy': 0.7113590145836964, 'test/loss': 1.2673985750392192, 'test/bleu': 30.853806158294635, 'test/num_examples': 3003, 'score': 41993.62888288498, 'total_duration': 68423.52494740486, 'accumulated_submission_time': 41993.62888288498, 'accumulated_eval_time': 26336.465482711792, 'accumulated_logging_time': 1.4626400470733643}
I0916 14:42:08.732687 140502817015552 logging_writer.py:48] [96846] accumulated_eval_time=26336.465483, accumulated_logging_time=1.462640, accumulated_submission_time=41993.628883, global_step=96846, preemption_count=0, score=41993.628883, test/accuracy=0.711359, test/bleu=30.853806, test/loss=1.267399, test/num_examples=3003, total_duration=68423.524947, train/accuracy=0.691270, train/bleu=35.368876, train/loss=1.391624, validation/accuracy=0.693779, validation/bleu=30.902189, validation/loss=1.370571, validation/num_examples=3000
I0916 14:43:16.514797 140493521635072 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.216464, loss=1.528268
I0916 14:43:16.519295 140548286359360 pytorch_submission_base.py:86] 97000) loss = 1.528, grad_norm = 0.216
I0916 14:46:53.210054 140502817015552 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.220911, loss=1.584496
I0916 14:46:53.214010 140548286359360 pytorch_submission_base.py:86] 97500) loss = 1.584, grad_norm = 0.221
I0916 14:50:29.996731 140493521635072 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.216468, loss=1.435778
I0916 14:50:30.000535 140548286359360 pytorch_submission_base.py:86] 98000) loss = 1.436, grad_norm = 0.216
I0916 14:54:06.836856 140502817015552 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.227300, loss=1.466926
I0916 14:54:06.840754 140548286359360 pytorch_submission_base.py:86] 98500) loss = 1.467, grad_norm = 0.227
I0916 14:56:09.821935 140548286359360 spec.py:320] Evaluating on the training split.
I0916 14:56:13.582473 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 15:00:04.688811 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 15:00:08.359119 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 15:02:38.073629 140548286359360 spec.py:348] Evaluating on the test split.
I0916 15:02:41.819036 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 15:05:17.510350 140548286359360 submission_runner.py:376] Time since start: 69812.32s, 	Step: 98783, 	{'train/accuracy': 0.6911336878949043, 'train/loss': 1.3992832415677985, 'train/bleu': 35.3478765247788, 'validation/accuracy': 0.693655379350535, 'validation/loss': 1.3704470108864117, 'validation/bleu': 30.83141378400424, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938760095287, 'test/loss': 1.2670044775143803, 'test/bleu': 30.873594539411304, 'test/num_examples': 3003, 'score': 42832.89587807655, 'total_duration': 69812.32436609268, 'accumulated_submission_time': 42832.89587807655, 'accumulated_eval_time': 26884.15394926071, 'accumulated_logging_time': 1.493978500366211}
I0916 15:05:17.532185 140493521635072 logging_writer.py:48] [98783] accumulated_eval_time=26884.153949, accumulated_logging_time=1.493979, accumulated_submission_time=42832.895878, global_step=98783, preemption_count=0, score=42832.895878, test/accuracy=0.711394, test/bleu=30.873595, test/loss=1.267004, test/num_examples=3003, total_duration=69812.324366, train/accuracy=0.691134, train/bleu=35.347877, train/loss=1.399283, validation/accuracy=0.693655, validation/bleu=30.831414, validation/loss=1.370447, validation/num_examples=3000
I0916 15:06:52.668850 140502817015552 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.217117, loss=1.390601
I0916 15:06:52.672352 140548286359360 pytorch_submission_base.py:86] 99000) loss = 1.391, grad_norm = 0.217
I0916 15:10:29.246641 140493521635072 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.212621, loss=1.379153
I0916 15:10:29.250496 140548286359360 pytorch_submission_base.py:86] 99500) loss = 1.379, grad_norm = 0.213
I0916 15:14:06.154691 140548286359360 spec.py:320] Evaluating on the training split.
I0916 15:14:09.921342 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 15:18:06.289414 140548286359360 spec.py:332] Evaluating on the validation split.
I0916 15:18:09.964093 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 15:20:39.036326 140548286359360 spec.py:348] Evaluating on the test split.
I0916 15:20:42.768679 140548286359360 workload.py:141] Translating evaluation dataset.
I0916 15:23:20.404071 140548286359360 submission_runner.py:376] Time since start: 70895.22s, 	Step: 100000, 	{'train/accuracy': 0.6927951428967173, 'train/loss': 1.3887157006036752, 'train/bleu': 35.52138255068357, 'validation/accuracy': 0.69374217306667, 'validation/loss': 1.3705115249655926, 'validation/bleu': 30.811470111504, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938760095287, 'test/loss': 1.2671126024054384, 'test/bleu': 30.89441819156341, 'test/num_examples': 3003, 'score': 43359.921515226364, 'total_duration': 70895.21810030937, 'accumulated_submission_time': 43359.921515226364, 'accumulated_eval_time': 27438.403477430344, 'accumulated_logging_time': 1.525425910949707}
I0916 15:23:20.426115 140502817015552 logging_writer.py:48] [100000] accumulated_eval_time=27438.403477, accumulated_logging_time=1.525426, accumulated_submission_time=43359.921515, global_step=100000, preemption_count=0, score=43359.921515, test/accuracy=0.711394, test/bleu=30.894418, test/loss=1.267113, test/num_examples=3003, total_duration=70895.218100, train/accuracy=0.692795, train/bleu=35.521383, train/loss=1.388716, validation/accuracy=0.693742, validation/bleu=30.811470, validation/loss=1.370512, validation/num_examples=3000
I0916 15:23:20.892548 140493521635072 logging_writer.py:48] [100000] global_step=100000, preemption_count=0, score=43359.921515
I0916 15:23:23.511306 140548286359360 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_pytorch/nadamw_run_0/wmt_pytorch/trial_1/checkpoint_100000.
I0916 15:23:23.539013 140548286359360 submission_runner.py:540] Tuning trial 1/1
I0916 15:23:23.539238 140548286359360 submission_runner.py:541] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, beta1=0.9326607383586145, beta2=0.9955159689799007, warmup_steps=1999, weight_decay=0.08121616522670176, label_smoothing=0.0)
I0916 15:23:23.540999 140548286359360 submission_runner.py:542] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006508186613687744, 'train/loss': 11.135978283208878, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.156982709451835, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.158046452849922, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 41.35803174972534, 'total_duration': 925.3467218875885, 'accumulated_submission_time': 41.35803174972534, 'accumulated_eval_time': 883.2694389820099, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1935, {'train/accuracy': 0.4978049420162605, 'train/loss': 3.0021800916793047, 'train/bleu': 21.73098218033474, 'validation/accuracy': 0.49529454067525513, 'validation/loss': 3.011394775018289, 'validation/bleu': 17.356840239080277, 'validation/num_examples': 3000, 'test/accuracy': 0.49426529545058395, 'test/loss': 3.076523371680902, 'test/bleu': 15.678565306236928, 'test/num_examples': 3003, 'score': 880.4625396728516, 'total_duration': 2270.7462015151978, 'accumulated_submission_time': 880.4625396728516, 'accumulated_eval_time': 1387.516426563263, 'accumulated_logging_time': 0.030379533767700195, 'global_step': 1935, 'preemption_count': 0}), (3871, {'train/accuracy': 0.5657211483472492, 'train/loss': 2.3478211140340846, 'train/bleu': 26.325749844391794, 'validation/accuracy': 0.5775749835711894, 'validation/loss': 2.24222634406269, 'validation/bleu': 22.37134960329752, 'validation/num_examples': 3000, 'test/accuracy': 0.5799779210969729, 'test/loss': 2.2346400920922664, 'test/bleu': 21.045066845584646, 'test/num_examples': 3003, 'score': 1719.399917125702, 'total_duration': 3531.5950195789337, 'accumulated_submission_time': 1719.399917125702, 'accumulated_eval_time': 1807.405702829361, 'accumulated_logging_time': 0.060216426849365234, 'global_step': 3871, 'preemption_count': 0}), (5808, {'train/accuracy': 0.5918068596392914, 'train/loss': 2.1194177154930864, 'train/bleu': 27.87840393026368, 'validation/accuracy': 0.6071840398755131, 'validation/loss': 2.005180499931805, 'validation/bleu': 24.73060151531517, 'validation/num_examples': 3000, 'test/accuracy': 0.6141072569868108, 'test/loss': 1.9634082127709023, 'test/bleu': 23.361010994382085, 'test/num_examples': 3003, 'score': 2558.530163526535, 'total_duration': 4791.079871892929, 'accumulated_submission_time': 2558.530163526535, 'accumulated_eval_time': 2225.833331823349, 'accumulated_logging_time': 0.08784604072570801, 'global_step': 5808, 'preemption_count': 0}), (7745, {'train/accuracy': 0.6064239194789817, 'train/loss': 2.0064540209272668, 'train/bleu': 28.997343628428016, 'validation/accuracy': 0.6214926039354751, 'validation/loss': 1.8730328049249234, 'validation/bleu': 25.470898909959008, 'validation/num_examples': 3000, 'test/accuracy': 0.6278542792400209, 'test/loss': 1.8213873031782, 'test/bleu': 24.526511249281437, 'test/num_examples': 3003, 'score': 3397.5152928829193, 'total_duration': 6021.372844219208, 'accumulated_submission_time': 3397.5152928829193, 'accumulated_eval_time': 2615.20689702034, 'accumulated_logging_time': 0.11474442481994629, 'global_step': 7745, 'preemption_count': 0}), (9682, {'train/accuracy': 0.6133668948393489, 'train/loss': 1.9398628187678675, 'train/bleu': 29.83521448326555, 'validation/accuracy': 0.6335321322736234, 'validation/loss': 1.7975104539931308, 'validation/bleu': 26.513102722186588, 'validation/num_examples': 3000, 'test/accuracy': 0.6393701702399628, 'test/loss': 1.7417320318401022, 'test/bleu': 25.2091620892064, 'test/num_examples': 3003, 'score': 4236.662819385529, 'total_duration': 7269.9006135463715, 'accumulated_submission_time': 4236.662819385529, 'accumulated_eval_time': 3022.656242609024, 'accumulated_logging_time': 0.14206647872924805, 'global_step': 9682, 'preemption_count': 0}), (11619, {'train/accuracy': 0.6197812496428449, 'train/loss': 1.89744398379373, 'train/bleu': 30.243587393120052, 'validation/accuracy': 0.6409219972473993, 'validation/loss': 1.7486977067860288, 'validation/bleu': 26.96995823870777, 'validation/num_examples': 3000, 'test/accuracy': 0.6481436290744291, 'test/loss': 1.6893959531694847, 'test/bleu': 26.04726519570716, 'test/num_examples': 3003, 'score': 5075.5725355148315, 'total_duration': 8525.007823467255, 'accumulated_submission_time': 5075.5725355148315, 'accumulated_eval_time': 3436.968344449997, 'accumulated_logging_time': 0.16846919059753418, 'global_step': 11619, 'preemption_count': 0}), (13555, {'train/accuracy': 0.6268828344699601, 'train/loss': 1.8221518392859175, 'train/bleu': 30.193807832160665, 'validation/accuracy': 0.6434886114245328, 'validation/loss': 1.712724617487694, 'validation/bleu': 26.894849047664977, 'validation/num_examples': 3000, 'test/accuracy': 0.651978385915984, 'test/loss': 1.6516769798384754, 'test/bleu': 25.993052063563592, 'test/num_examples': 3003, 'score': 5914.606507778168, 'total_duration': 9788.549141168594, 'accumulated_submission_time': 5914.606507778168, 'accumulated_eval_time': 3859.6198971271515, 'accumulated_logging_time': 0.19640374183654785, 'global_step': 13555, 'preemption_count': 0}), (15491, {'train/accuracy': 0.6255072463768115, 'train/loss': 1.8254012148337595, 'train/bleu': 29.912893293052104, 'validation/accuracy': 0.6453980731795018, 'validation/loss': 1.6932260604332245, 'validation/bleu': 27.041283680647236, 'validation/num_examples': 3000, 'test/accuracy': 0.65558073325199, 'test/loss': 1.6255545145546453, 'test/bleu': 26.439915719755504, 'test/num_examples': 3003, 'score': 6753.552028656006, 'total_duration': 11069.034653186798, 'accumulated_submission_time': 6753.552028656006, 'accumulated_eval_time': 4299.303379774094, 'accumulated_logging_time': 0.22470474243164062, 'global_step': 15491, 'preemption_count': 0}), (17429, {'train/accuracy': 0.6272884243747927, 'train/loss': 1.8329014247978868, 'train/bleu': 30.486603052150425, 'validation/accuracy': 0.6499113464185193, 'validation/loss': 1.667441933454018, 'validation/bleu': 27.47786028943392, 'validation/num_examples': 3000, 'test/accuracy': 0.6568473650572308, 'test/loss': 1.5986300549067456, 'test/bleu': 26.58617215076298, 'test/num_examples': 3003, 'score': 7592.796184301376, 'total_duration': 12440.432694673538, 'accumulated_submission_time': 7592.796184301376, 'accumulated_eval_time': 4829.6028118133545, 'accumulated_logging_time': 0.2531464099884033, 'global_step': 17429, 'preemption_count': 0}), (19365, {'train/accuracy': 0.6406487698308303, 'train/loss': 1.7265259516510918, 'train/bleu': 31.542611233316112, 'validation/accuracy': 0.6512380503651536, 'validation/loss': 1.6504683373423763, 'validation/bleu': 27.456567025628075, 'validation/num_examples': 3000, 'test/accuracy': 0.6614490732670967, 'test/loss': 1.5820926660275405, 'test/bleu': 26.73234110881058, 'test/num_examples': 3003, 'score': 8431.73428940773, 'total_duration': 13677.111048460007, 'accumulated_submission_time': 8431.73428940773, 'accumulated_eval_time': 5225.515836715698, 'accumulated_logging_time': 0.27984142303466797, 'global_step': 19365, 'preemption_count': 0}), (21302, {'train/accuracy': 0.6382798359566523, 'train/loss': 1.758453294327216, 'train/bleu': 30.601670383946143, 'validation/accuracy': 0.6519324000942331, 'validation/loss': 1.6462875150339116, 'validation/bleu': 27.341726817968347, 'validation/num_examples': 3000, 'test/accuracy': 0.6647260472953344, 'test/loss': 1.5706432296205914, 'test/bleu': 26.979278595350486, 'test/num_examples': 3003, 'score': 9270.730333328247, 'total_duration': 15007.56753230095, 'accumulated_submission_time': 9270.730333328247, 'accumulated_eval_time': 5715.141753911972, 'accumulated_logging_time': 0.3067924976348877, 'global_step': 21302, 'preemption_count': 0}), (23238, {'train/accuracy': 0.6369830128718078, 'train/loss': 1.7659684792176318, 'train/bleu': 31.015807642183926, 'validation/accuracy': 0.6556893280926461, 'validation/loss': 1.6280676154046447, 'validation/bleu': 27.950156432722178, 'validation/num_examples': 3000, 'test/accuracy': 0.665667305792807, 'test/loss': 1.5500065365173437, 'test/bleu': 27.276721659909946, 'test/num_examples': 3003, 'score': 10109.776323080063, 'total_duration': 16258.608014583588, 'accumulated_submission_time': 10109.776323080063, 'accumulated_eval_time': 6125.273441314697, 'accumulated_logging_time': 0.3341820240020752, 'global_step': 23238, 'preemption_count': 0}), (25174, {'train/accuracy': 0.6599256505576209, 'train/loss': 1.5785911138118387, 'train/bleu': 32.28229505199461, 'validation/accuracy': 0.6561232966733208, 'validation/loss': 1.6102444870491377, 'validation/bleu': 27.92391550739818, 'validation/num_examples': 3000, 'test/accuracy': 0.6672709313810935, 'test/loss': 1.5376875617337749, 'test/bleu': 27.29301609084211, 'test/num_examples': 3003, 'score': 10948.71316242218, 'total_duration': 17510.35099887848, 'accumulated_submission_time': 10948.71316242218, 'accumulated_eval_time': 6536.257373809814, 'accumulated_logging_time': 0.3641328811645508, 'global_step': 25174, 'preemption_count': 0}), (27111, {'train/accuracy': 0.6404345340194397, 'train/loss': 1.721489065180103, 'train/bleu': 31.7286075932633, 'validation/accuracy': 0.6577847763821899, 'validation/loss': 1.602408641864329, 'validation/bleu': 28.129466292077105, 'validation/num_examples': 3000, 'test/accuracy': 0.6687583522166056, 'test/loss': 1.5291241793039336, 'test/bleu': 27.366074967561612, 'test/num_examples': 3003, 'score': 11787.881809949875, 'total_duration': 18992.449721574783, 'accumulated_submission_time': 11787.881809949875, 'accumulated_eval_time': 7177.353737592697, 'accumulated_logging_time': 0.3912367820739746, 'global_step': 27111, 'preemption_count': 0}), (29047, {'train/accuracy': 0.6409900426742532, 'train/loss': 1.7337724039829303, 'train/bleu': 31.65851214632639, 'validation/accuracy': 0.6601778031270537, 'validation/loss': 1.594835696395581, 'validation/bleu': 28.201814289752825, 'validation/num_examples': 3000, 'test/accuracy': 0.6684329789088373, 'test/loss': 1.5217239338213933, 'test/bleu': 27.511019094004567, 'test/num_examples': 3003, 'score': 12626.935340881348, 'total_duration': 20260.453236818314, 'accumulated_submission_time': 12626.935340881348, 'accumulated_eval_time': 7604.4251120090485, 'accumulated_logging_time': 0.41831302642822266, 'global_step': 29047, 'preemption_count': 0}), (30984, {'train/accuracy': 0.6439200655730135, 'train/loss': 1.7176848353747936, 'train/bleu': 31.60252316452521, 'validation/accuracy': 0.6615293052782979, 'validation/loss': 1.5832076632651797, 'validation/bleu': 28.104344130531995, 'validation/num_examples': 3000, 'test/accuracy': 0.673057928069258, 'test/loss': 1.5081017408924524, 'test/bleu': 27.706671646641407, 'test/num_examples': 3003, 'score': 13465.986572027206, 'total_duration': 21618.1237244606, 'accumulated_submission_time': 13465.986572027206, 'accumulated_eval_time': 8121.20067691803, 'accumulated_logging_time': 0.44565820693969727, 'global_step': 30984, 'preemption_count': 0}), (32921, {'train/accuracy': 0.647332079280602, 'train/loss': 1.6765122829303543, 'train/bleu': 31.908684782240226, 'validation/accuracy': 0.6618640810405326, 'validation/loss': 1.5747515723611611, 'validation/bleu': 28.266594137446134, 'validation/num_examples': 3000, 'test/accuracy': 0.6724885247806636, 'test/loss': 1.5000003631398524, 'test/bleu': 27.485078293296752, 'test/num_examples': 3003, 'score': 14304.972734451294, 'total_duration': 22907.22048640251, 'accumulated_submission_time': 14304.972734451294, 'accumulated_eval_time': 8569.442501783371, 'accumulated_logging_time': 0.4742164611816406, 'global_step': 32921, 'preemption_count': 0}), (34858, {'train/accuracy': 0.6455589929960552, 'train/loss': 1.6999612209750319, 'train/bleu': 31.683217066472352, 'validation/accuracy': 0.6652366368674908, 'validation/loss': 1.5713859910292496, 'validation/bleu': 28.525783784364737, 'validation/num_examples': 3000, 'test/accuracy': 0.6770669920399744, 'test/loss': 1.4881058080587997, 'test/bleu': 28.069393689092426, 'test/num_examples': 3003, 'score': 15144.050840854645, 'total_duration': 24213.788479566574, 'accumulated_submission_time': 15144.050840854645, 'accumulated_eval_time': 9035.09800195694, 'accumulated_logging_time': 0.5020933151245117, 'global_step': 34858, 'preemption_count': 0}), (36795, {'train/accuracy': 0.6474567661794925, 'train/loss': 1.6844318923021866, 'train/bleu': 31.50121315972364, 'validation/accuracy': 0.6638231392047216, 'validation/loss': 1.5592971568858414, 'validation/bleu': 28.369663348958092, 'validation/num_examples': 3000, 'test/accuracy': 0.6779850095868921, 'test/loss': 1.4803821865376794, 'test/bleu': 28.357257856642306, 'test/num_examples': 3003, 'score': 15983.212558031082, 'total_duration': 25533.723687171936, 'accumulated_submission_time': 15983.212558031082, 'accumulated_eval_time': 9514.022464036942, 'accumulated_logging_time': 0.5305404663085938, 'global_step': 36795, 'preemption_count': 0}), (38733, {'train/accuracy': 0.6518662253167447, 'train/loss': 1.63576664336263, 'train/bleu': 32.23173737476405, 'validation/accuracy': 0.6672948878501197, 'validation/loss': 1.5510907916516845, 'validation/bleu': 29.054450448406424, 'validation/num_examples': 3000, 'test/accuracy': 0.6802161408401604, 'test/loss': 1.4651368492533845, 'test/bleu': 28.3957710984702, 'test/num_examples': 3003, 'score': 16822.314371347427, 'total_duration': 26868.53143930435, 'accumulated_submission_time': 16822.314371347427, 'accumulated_eval_time': 10007.909393072128, 'accumulated_logging_time': 0.5586585998535156, 'global_step': 38733, 'preemption_count': 0}), (40669, {'train/accuracy': 0.6496135819421905, 'train/loss': 1.6628862407367477, 'train/bleu': 31.621890556436647, 'validation/accuracy': 0.6660921749265353, 'validation/loss': 1.5446853960273277, 'validation/bleu': 28.628090042409703, 'validation/num_examples': 3000, 'test/accuracy': 0.681610597873453, 'test/loss': 1.4537059329789088, 'test/bleu': 28.286168587522337, 'test/num_examples': 3003, 'score': 17661.329209566116, 'total_duration': 28297.49199128151, 'accumulated_submission_time': 17661.329209566116, 'accumulated_eval_time': 10596.03066778183, 'accumulated_logging_time': 0.5877997875213623, 'global_step': 40669, 'preemption_count': 0}), (42606, {'train/accuracy': 0.6480103341449752, 'train/loss': 1.6791106519427963, 'train/bleu': 32.725976031815065, 'validation/accuracy': 0.6679768384768943, 'validation/loss': 1.5388408660462982, 'validation/bleu': 28.837223580497827, 'validation/num_examples': 3000, 'test/accuracy': 0.6807971646040323, 'test/loss': 1.4510897826971123, 'test/bleu': 28.097397197877058, 'test/num_examples': 3003, 'score': 18500.204820394516, 'total_duration': 29654.336916446686, 'accumulated_submission_time': 18500.204820394516, 'accumulated_eval_time': 11112.180014133453, 'accumulated_logging_time': 0.6166443824768066, 'global_step': 42606, 'preemption_count': 0}), (44543, {'train/accuracy': 0.6614632462901184, 'train/loss': 1.5848266924536984, 'train/bleu': 32.99073407007707, 'validation/accuracy': 0.6697127127995933, 'validation/loss': 1.5274952030972957, 'validation/bleu': 29.023326963209804, 'validation/num_examples': 3000, 'test/accuracy': 0.6825867177967578, 'test/loss': 1.4426892684910813, 'test/bleu': 28.67403504200141, 'test/num_examples': 3003, 'score': 19339.33795952797, 'total_duration': 30989.77409505844, 'accumulated_submission_time': 19339.33795952797, 'accumulated_eval_time': 11606.674901247025, 'accumulated_logging_time': 0.6454687118530273, 'global_step': 44543, 'preemption_count': 0}), (46480, {'train/accuracy': 0.6528822342474548, 'train/loss': 1.6368912584265798, 'train/bleu': 32.26108818056441, 'validation/accuracy': 0.6696631163903733, 'validation/loss': 1.5237658824751088, 'validation/bleu': 28.805197147917227, 'validation/num_examples': 3000, 'test/accuracy': 0.682377549241764, 'test/loss': 1.4379119821625705, 'test/bleu': 28.351329898544645, 'test/num_examples': 3003, 'score': 20178.220480442047, 'total_duration': 32417.398582220078, 'accumulated_submission_time': 20178.220480442047, 'accumulated_eval_time': 12193.570276021957, 'accumulated_logging_time': 0.67494797706604, 'global_step': 46480, 'preemption_count': 0}), (48418, {'train/accuracy': 0.6519304754053311, 'train/loss': 1.648320852798388, 'train/bleu': 32.35196562593995, 'validation/accuracy': 0.6709774212347026, 'validation/loss': 1.515130682663575, 'validation/bleu': 29.078555145200127, 'validation/num_examples': 3000, 'test/accuracy': 0.6858288303991633, 'test/loss': 1.425182477775841, 'test/bleu': 28.760257903959555, 'test/num_examples': 3003, 'score': 21017.4000313282, 'total_duration': 33763.0598757267, 'accumulated_submission_time': 21017.4000313282, 'accumulated_eval_time': 12698.193695545197, 'accumulated_logging_time': 0.7032680511474609, 'global_step': 48418, 'preemption_count': 0}), (50355, {'train/accuracy': 0.6758551559954893, 'train/loss': 1.4852831404984566, 'train/bleu': 33.77617535096047, 'validation/accuracy': 0.6747839456423355, 'validation/loss': 1.5037334859456175, 'validation/bleu': 29.330393696235156, 'validation/num_examples': 3000, 'test/accuracy': 0.6859101737261054, 'test/loss': 1.418116139387601, 'test/bleu': 28.65569521589632, 'test/num_examples': 3003, 'score': 21856.62474298477, 'total_duration': 35180.53581738472, 'accumulated_submission_time': 21856.62474298477, 'accumulated_eval_time': 13274.610888957977, 'accumulated_logging_time': 0.731846809387207, 'global_step': 50355, 'preemption_count': 0}), (52292, {'train/accuracy': 0.6577692386866573, 'train/loss': 1.6074344057707484, 'train/bleu': 32.53646168910653, 'validation/accuracy': 0.6745979591077605, 'validation/loss': 1.4965689359090402, 'validation/bleu': 29.44271198290423, 'validation/num_examples': 3000, 'test/accuracy': 0.6883969554354773, 'test/loss': 1.4052153237754925, 'test/bleu': 29.14081296275224, 'test/num_examples': 3003, 'score': 22695.546543836594, 'total_duration': 36523.03301239014, 'accumulated_submission_time': 22695.546543836594, 'accumulated_eval_time': 13776.311428070068, 'accumulated_logging_time': 0.760345458984375, 'global_step': 52292, 'preemption_count': 0}), (54229, {'train/accuracy': 0.6601742519775307, 'train/loss': 1.6005024432534678, 'train/bleu': 32.528798784486355, 'validation/accuracy': 0.6759742594636148, 'validation/loss': 1.4859285687716208, 'validation/bleu': 29.386270567451255, 'validation/num_examples': 3000, 'test/accuracy': 0.6903608157573645, 'test/loss': 1.390614650514206, 'test/bleu': 29.13147483813223, 'test/num_examples': 3003, 'score': 23534.480858802795, 'total_duration': 37958.15223121643, 'accumulated_submission_time': 23534.480858802795, 'accumulated_eval_time': 14370.658740282059, 'accumulated_logging_time': 0.7891988754272461, 'global_step': 54229, 'preemption_count': 0}), (56167, {'train/accuracy': 0.660536354836873, 'train/loss': 1.6047413240904174, 'train/bleu': 32.80516408951384, 'validation/accuracy': 0.677462151740214, 'validation/loss': 1.4825181375618406, 'validation/bleu': 29.86055016844867, 'validation/num_examples': 3000, 'test/accuracy': 0.6911974899773401, 'test/loss': 1.3890211928417873, 'test/bleu': 29.26843379570543, 'test/num_examples': 3003, 'score': 24373.612773180008, 'total_duration': 39323.85476732254, 'accumulated_submission_time': 24373.612773180008, 'accumulated_eval_time': 14895.40498638153, 'accumulated_logging_time': 0.8188650608062744, 'global_step': 56167, 'preemption_count': 0}), (58105, {'train/accuracy': 0.6656157590123002, 'train/loss': 1.5523228678847991, 'train/bleu': 33.3060064493853, 'validation/accuracy': 0.6776605373770939, 'validation/loss': 1.4725700859257789, 'validation/bleu': 29.739731018351186, 'validation/num_examples': 3000, 'test/accuracy': 0.6916623089884376, 'test/loss': 1.3837500363139852, 'test/bleu': 29.4505696280244, 'test/num_examples': 3003, 'score': 25212.772010564804, 'total_duration': 40680.802152872086, 'accumulated_submission_time': 25212.772010564804, 'accumulated_eval_time': 15411.362670183182, 'accumulated_logging_time': 0.8487081527709961, 'global_step': 58105, 'preemption_count': 0}), (60042, {'train/accuracy': 0.66344400481679, 'train/loss': 1.5674051336085786, 'train/bleu': 32.90352663664884, 'validation/accuracy': 0.6789996404260331, 'validation/loss': 1.4669271382251925, 'validation/bleu': 29.466625662367147, 'validation/num_examples': 3000, 'test/accuracy': 0.6939283016675382, 'test/loss': 1.3731488946022892, 'test/bleu': 29.25067630613892, 'test/num_examples': 3003, 'score': 26051.63799715042, 'total_duration': 42062.333797454834, 'accumulated_submission_time': 26051.63799715042, 'accumulated_eval_time': 15952.195127725601, 'accumulated_logging_time': 0.8777682781219482, 'global_step': 60042, 'preemption_count': 0}), (61980, {'train/accuracy': 0.6672072174955748, 'train/loss': 1.5574134214583453, 'train/bleu': 33.13932775270849, 'validation/accuracy': 0.679334416188268, 'validation/loss': 1.4567734358532443, 'validation/bleu': 29.49959670083265, 'validation/num_examples': 3000, 'test/accuracy': 0.6967520771599558, 'test/loss': 1.3614961906629481, 'test/bleu': 29.544832346110113, 'test/num_examples': 3003, 'score': 26890.69024991989, 'total_duration': 43401.95816779137, 'accumulated_submission_time': 26890.69024991989, 'accumulated_eval_time': 16450.94729065895, 'accumulated_logging_time': 0.9066295623779297, 'global_step': 61980, 'preemption_count': 0}), (63917, {'train/accuracy': 0.6701691937342414, 'train/loss': 1.5221045324639764, 'train/bleu': 33.52928766069958, 'validation/accuracy': 0.6821738106161114, 'validation/loss': 1.4480898020483317, 'validation/bleu': 29.928367254077777, 'validation/num_examples': 3000, 'test/accuracy': 0.6966358724071815, 'test/loss': 1.352622596014177, 'test/bleu': 29.807237851771333, 'test/num_examples': 3003, 'score': 27729.889011621475, 'total_duration': 44862.12448811531, 'accumulated_submission_time': 27729.889011621475, 'accumulated_eval_time': 17070.071225881577, 'accumulated_logging_time': 0.9359610080718994, 'global_step': 63917, 'preemption_count': 0}), (65855, {'train/accuracy': 0.6707203840840636, 'train/loss': 1.5185212706932083, 'train/bleu': 33.26551608031923, 'validation/accuracy': 0.6816282501146917, 'validation/loss': 1.4448605139737882, 'validation/bleu': 29.926745217459516, 'validation/num_examples': 3000, 'test/accuracy': 0.6985183894021265, 'test/loss': 1.3473866640520598, 'test/bleu': 30.102687946614797, 'test/num_examples': 3003, 'score': 28568.889939785004, 'total_duration': 46288.24328684807, 'accumulated_submission_time': 28568.889939785004, 'accumulated_eval_time': 17655.37073612213, 'accumulated_logging_time': 0.9665215015411377, 'global_step': 65855, 'preemption_count': 0}), (67792, {'train/accuracy': 0.6686363532659533, 'train/loss': 1.5407497333508247, 'train/bleu': 33.666151112396506, 'validation/accuracy': 0.6826449765037012, 'validation/loss': 1.4397742665930986, 'validation/bleu': 29.82620958383268, 'validation/num_examples': 3000, 'test/accuracy': 0.6981116727674161, 'test/loss': 1.3458215312881296, 'test/bleu': 30.181832350093803, 'test/num_examples': 3003, 'score': 29408.087728977203, 'total_duration': 47676.716633081436, 'accumulated_submission_time': 29408.087728977203, 'accumulated_eval_time': 18202.801179409027, 'accumulated_logging_time': 0.9960479736328125, 'global_step': 67792, 'preemption_count': 0}), (69728, {'train/accuracy': 0.6783969842893427, 'train/loss': 1.4674473629770948, 'train/bleu': 33.896882991916954, 'validation/accuracy': 0.6841576669849103, 'validation/loss': 1.4302246329865718, 'validation/bleu': 30.053019980064374, 'validation/num_examples': 3000, 'test/accuracy': 0.6985532508279588, 'test/loss': 1.3396984486665504, 'test/bleu': 29.802057282650225, 'test/num_examples': 3003, 'score': 30247.01449728012, 'total_duration': 49088.2817606926, 'accumulated_submission_time': 30247.01449728012, 'accumulated_eval_time': 18773.612513303757, 'accumulated_logging_time': 1.0252406597137451, 'global_step': 69728, 'preemption_count': 0}), (71666, {'train/accuracy': 0.6763402823093039, 'train/loss': 1.484094790700736, 'train/bleu': 33.98225085368709, 'validation/accuracy': 0.6877534066533583, 'validation/loss': 1.4193172976776482, 'validation/bleu': 30.103169553896485, 'validation/num_examples': 3000, 'test/accuracy': 0.7012375806170472, 'test/loss': 1.3241120322758702, 'test/bleu': 30.08516299202726, 'test/num_examples': 3003, 'score': 31086.24572443962, 'total_duration': 50502.30377078056, 'accumulated_submission_time': 31086.24572443962, 'accumulated_eval_time': 19346.574458122253, 'accumulated_logging_time': 1.0552895069122314, 'global_step': 71666, 'preemption_count': 0}), (73602, {'train/accuracy': 0.670734622784349, 'train/loss': 1.5161990129893785, 'train/bleu': 34.0176450068542, 'validation/accuracy': 0.6872698416634636, 'validation/loss': 1.4138070591189198, 'validation/bleu': 30.301729899029556, 'validation/num_examples': 3000, 'test/accuracy': 0.7036895009005868, 'test/loss': 1.3104700482249725, 'test/bleu': 30.356615795898776, 'test/num_examples': 3003, 'score': 31925.20730447769, 'total_duration': 51873.154420137405, 'accumulated_submission_time': 31925.20730447769, 'accumulated_eval_time': 19876.64458489418, 'accumulated_logging_time': 1.0856199264526367, 'global_step': 73602, 'preemption_count': 0}), (75539, {'train/accuracy': 0.6856483481503376, 'train/loss': 1.4249545435813191, 'train/bleu': 34.9248659171008, 'validation/accuracy': 0.6876294156303083, 'validation/loss': 1.4081208114282526, 'validation/bleu': 30.243288374175677, 'validation/num_examples': 3000, 'test/accuracy': 0.7039683923072454, 'test/loss': 1.310944581227122, 'test/bleu': 30.28881401849146, 'test/num_examples': 3003, 'score': 32764.24068379402, 'total_duration': 53222.692863464355, 'accumulated_submission_time': 32764.24068379402, 'accumulated_eval_time': 20385.33827328682, 'accumulated_logging_time': 1.1155674457550049, 'global_step': 75539, 'preemption_count': 0}), (77476, {'train/accuracy': 0.6817491406187545, 'train/loss': 1.4452101629683771, 'train/bleu': 34.44427356101495, 'validation/accuracy': 0.6892784962368724, 'validation/loss': 1.3988072451054543, 'validation/bleu': 30.66482949169634, 'validation/num_examples': 3000, 'test/accuracy': 0.7053163674394283, 'test/loss': 1.300615612834815, 'test/bleu': 30.429071222834825, 'test/num_examples': 3003, 'score': 33603.42820382118, 'total_duration': 54621.11009454727, 'accumulated_submission_time': 33603.42820382118, 'accumulated_eval_time': 20942.697358608246, 'accumulated_logging_time': 1.145965814590454, 'global_step': 77476, 'preemption_count': 0}), (79412, {'train/accuracy': 0.6790285564220551, 'train/loss': 1.4710710656420913, 'train/bleu': 34.15271984650601, 'validation/accuracy': 0.6885097518939629, 'validation/loss': 1.3987671417589367, 'validation/bleu': 30.295707325996634, 'validation/num_examples': 3000, 'test/accuracy': 0.7071756434838185, 'test/loss': 1.2961548936726512, 'test/bleu': 30.575647800891755, 'test/num_examples': 3003, 'score': 34442.28955411911, 'total_duration': 56006.82362937927, 'accumulated_submission_time': 34442.28955411911, 'accumulated_eval_time': 21487.68458890915, 'accumulated_logging_time': 1.1779513359069824, 'global_step': 79412, 'preemption_count': 0}), (81350, {'train/accuracy': 0.69639723811043, 'train/loss': 1.3686391583870747, 'train/bleu': 35.66672404940048, 'validation/accuracy': 0.6909895723549615, 'validation/loss': 1.3908368502870392, 'validation/bleu': 30.731510654200974, 'validation/num_examples': 3000, 'test/accuracy': 0.707082679681599, 'test/loss': 1.2920591300621695, 'test/bleu': 30.729056093647316, 'test/num_examples': 3003, 'score': 35281.52217125893, 'total_duration': 57371.96135807037, 'accumulated_submission_time': 35281.52217125893, 'accumulated_eval_time': 22011.747089147568, 'accumulated_logging_time': 1.2102937698364258, 'global_step': 81350, 'preemption_count': 0}), (83286, {'train/accuracy': 0.6899125123424189, 'train/loss': 1.4051503945268549, 'train/bleu': 34.659869928608366, 'validation/accuracy': 0.6921178906647159, 'validation/loss': 1.3847799856790368, 'validation/bleu': 30.757435491047776, 'validation/num_examples': 3000, 'test/accuracy': 0.7088722328743245, 'test/loss': 1.2842437249433503, 'test/bleu': 30.874200324254378, 'test/num_examples': 3003, 'score': 36120.4275701046, 'total_duration': 58740.75207948685, 'accumulated_submission_time': 36120.4275701046, 'accumulated_eval_time': 22539.78720498085, 'accumulated_logging_time': 1.2403175830841064, 'global_step': 83286, 'preemption_count': 0}), (85223, {'train/accuracy': 0.6868093567518769, 'train/loss': 1.4139526005896537, 'train/bleu': 34.195216897693065, 'validation/accuracy': 0.6920186978462759, 'validation/loss': 1.3820907366306678, 'validation/bleu': 30.452985728097694, 'validation/num_examples': 3000, 'test/accuracy': 0.7091511242809831, 'test/loss': 1.2817079193539016, 'test/bleu': 30.728248298398086, 'test/num_examples': 3003, 'score': 36959.37769436836, 'total_duration': 60096.96929478645, 'accumulated_submission_time': 36959.37769436836, 'accumulated_eval_time': 23055.239689826965, 'accumulated_logging_time': 1.270453929901123, 'global_step': 85223, 'preemption_count': 0}), (87160, {'train/accuracy': 0.6854488458250214, 'train/loss': 1.4329901681390709, 'train/bleu': 34.80930256268224, 'validation/accuracy': 0.6923658727108157, 'validation/loss': 1.3809235742582238, 'validation/bleu': 30.66349984053672, 'validation/num_examples': 3000, 'test/accuracy': 0.7091976061820928, 'test/loss': 1.2777744792574517, 'test/bleu': 30.957561902975634, 'test/num_examples': 3003, 'score': 37798.30632042885, 'total_duration': 61453.46198105812, 'accumulated_submission_time': 37798.30632042885, 'accumulated_eval_time': 23570.96785211563, 'accumulated_logging_time': 1.301124095916748, 'global_step': 87160, 'preemption_count': 0}), (89097, {'train/accuracy': 0.6914659325533379, 'train/loss': 1.3901122318765773, 'train/bleu': 35.204091676264554, 'validation/accuracy': 0.6922666798923758, 'validation/loss': 1.3766441403392395, 'validation/bleu': 30.795370257766148, 'validation/num_examples': 3000, 'test/accuracy': 0.7091278833304282, 'test/loss': 1.2753284599965138, 'test/bleu': 30.907285027171604, 'test/num_examples': 3003, 'score': 38637.24909734726, 'total_duration': 62854.61246776581, 'accumulated_submission_time': 38637.24909734726, 'accumulated_eval_time': 24131.34121155739, 'accumulated_logging_time': 1.3332021236419678, 'global_step': 89097, 'preemption_count': 0}), (91033, {'train/accuracy': 0.6886444375906697, 'train/loss': 1.4099240873056669, 'train/bleu': 34.74568818265632, 'validation/accuracy': 0.6925270610407807, 'validation/loss': 1.3748779463366851, 'validation/bleu': 30.615161550600366, 'validation/num_examples': 3000, 'test/accuracy': 0.7104526175120562, 'test/loss': 1.2705817682005693, 'test/bleu': 30.919050079075177, 'test/num_examples': 3003, 'score': 39476.22729182243, 'total_duration': 64234.03384017944, 'accumulated_submission_time': 39476.22729182243, 'accumulated_eval_time': 24669.933961629868, 'accumulated_logging_time': 1.3639936447143555, 'global_step': 91033, 'preemption_count': 0}), (92970, {'train/accuracy': 0.6883505376587765, 'train/loss': 1.4091135712100438, 'train/bleu': 34.90095872598014, 'validation/accuracy': 0.6926882493707456, 'validation/loss': 1.3736237965121325, 'validation/bleu': 30.572436127072947, 'validation/num_examples': 3000, 'test/accuracy': 0.7108593341467666, 'test/loss': 1.2695729656905468, 'test/bleu': 30.87325884562185, 'test/num_examples': 3003, 'score': 40315.36530208588, 'total_duration': 65596.5208902359, 'accumulated_submission_time': 40315.36530208588, 'accumulated_eval_time': 25191.423709392548, 'accumulated_logging_time': 1.3946354389190674, 'global_step': 92970, 'preemption_count': 0}), (94908, {'train/accuracy': 0.6916007498514014, 'train/loss': 1.3918943392506058, 'train/bleu': 35.096711394481815, 'validation/accuracy': 0.6926510520638306, 'validation/loss': 1.3710974794174902, 'validation/bleu': 30.60419427220016, 'validation/num_examples': 3000, 'test/accuracy': 0.7109639184242635, 'test/loss': 1.2683152308116903, 'test/bleu': 30.868801544172666, 'test/num_examples': 3003, 'score': 41154.604830503464, 'total_duration': 67025.35080504417, 'accumulated_submission_time': 41154.604830503464, 'accumulated_eval_time': 25779.169449567795, 'accumulated_logging_time': 1.4308946132659912, 'global_step': 94908, 'preemption_count': 0}), (96846, {'train/accuracy': 0.6912699854651163, 'train/loss': 1.3916236522585848, 'train/bleu': 35.368876265285955, 'validation/accuracy': 0.693779370373585, 'validation/loss': 1.3705708081734882, 'validation/bleu': 30.902189213072816, 'validation/num_examples': 3000, 'test/accuracy': 0.7113590145836964, 'test/loss': 1.2673985750392192, 'test/bleu': 30.853806158294635, 'test/num_examples': 3003, 'score': 41993.62888288498, 'total_duration': 68423.52494740486, 'accumulated_submission_time': 41993.62888288498, 'accumulated_eval_time': 26336.465482711792, 'accumulated_logging_time': 1.4626400470733643, 'global_step': 96846, 'preemption_count': 0}), (98783, {'train/accuracy': 0.6911336878949043, 'train/loss': 1.3992832415677985, 'train/bleu': 35.3478765247788, 'validation/accuracy': 0.693655379350535, 'validation/loss': 1.3704470108864117, 'validation/bleu': 30.83141378400424, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938760095287, 'test/loss': 1.2670044775143803, 'test/bleu': 30.873594539411304, 'test/num_examples': 3003, 'score': 42832.89587807655, 'total_duration': 69812.32436609268, 'accumulated_submission_time': 42832.89587807655, 'accumulated_eval_time': 26884.15394926071, 'accumulated_logging_time': 1.493978500366211, 'global_step': 98783, 'preemption_count': 0}), (100000, {'train/accuracy': 0.6927951428967173, 'train/loss': 1.3887157006036752, 'train/bleu': 35.52138255068357, 'validation/accuracy': 0.69374217306667, 'validation/loss': 1.3705115249655926, 'validation/bleu': 30.811470111504, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938760095287, 'test/loss': 1.2671126024054384, 'test/bleu': 30.89441819156341, 'test/num_examples': 3003, 'score': 43359.921515226364, 'total_duration': 70895.21810030937, 'accumulated_submission_time': 43359.921515226364, 'accumulated_eval_time': 27438.403477430344, 'accumulated_logging_time': 1.525425910949707, 'global_step': 100000, 'preemption_count': 0})], 'global_step': 100000}
I0916 15:23:23.541159 140548286359360 submission_runner.py:543] Timing: 43359.921515226364
I0916 15:23:23.541210 140548286359360 submission_runner.py:545] Total number of evals: 53
I0916 15:23:23.541277 140548286359360 submission_runner.py:546] ====================
I0916 15:23:23.541481 140548286359360 submission_runner.py:614] Final wmt score: 43359.921515226364
